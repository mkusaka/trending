<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-03T01:42:35Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ScalaOtus/scala-dev-mooc-2023-03</title>
    <updated>2023-04-03T01:42:35Z</updated>
    <id>tag:github.com,2023-04-03:/ScalaOtus/scala-dev-mooc-2023-03</id>
    <link href="https://github.com/ScalaOtus/scala-dev-mooc-2023-03" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>twitter/tormenta</title>
    <updated>2023-04-03T01:42:35Z</updated>
    <id>tag:github.com,2023-04-03:/twitter/tormenta</id>
    <link href="https://github.com/twitter/tormenta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala extensions for Storm&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Tormenta&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://travis-ci.org/twitter/tormenta&#34;&gt;&lt;img src=&#34;https://secure.travis-ci.org/twitter/tormenta.png&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/twitter/tormenta&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/twitter/tormenta/develop.svg?maxAge=3600&#34; alt=&#34;Codecov branch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://index.scala-lang.org/twitter/tormenta/tormenta-core&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/twitter/tormenta/tormenta-core/latest.svg?color=orange&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/tormenta?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/tormenta.svg?sanitize=true&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Scala extensions for the &lt;a href=&#34;https://github.com/nathanmarz/storm&#34;&gt;Storm&lt;/a&gt; distributed computation system. Tormenta adds a type-safe wrapper over Storm&#39;s Kafka spout. This type safety allows the user to push mapping and filtering transformations down to the level of the spout itself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.twitter.tormenta.scheme._&#xA;import com.twitter.tormenta.spout._&#xA;&#xA;// produces strings:&#xA;val scheme: Scheme[String] = Scheme { bytes =&amp;gt; Some(new String(bytes)) }&#xA;&#xA;// produces integers w/ string length:&#xA;val mappedScheme: Scheme[Int] = scheme.map(_.length)&#xA;&#xA;// filters out all tuples less than 5:&#xA;val filteredScheme: Scheme[Int] = mappedScheme.filter(_ &amp;gt; 5)&#xA;&#xA;// produces lengths for input strings &amp;gt; length of 5&#xA;val spout: KafkaSpout[Int] = new KafkaSpout(filteredScheme, zkHost, brokerZkPath, topic, appID, zkRoot)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use a &lt;code&gt;Spout[T]&lt;/code&gt; in a Storm topology, call the &lt;code&gt;getSpout&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;topologyBuilder.setSpout(&#34;spoutName&#34;, spout.getSpout, 10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you&#39;re cooking with gas.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;To learn more and find links to tutorials and information around the web, check out the &lt;a href=&#34;https://github.com/twitter/tormenta/wiki&#34;&gt;Tormenta Wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest ScalaDocs are hosted on Tormenta&#39;s &lt;a href=&#34;http://twitter.github.io/tormenta&#34;&gt;Github Project Page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Discussion occurs primarily on the &lt;a href=&#34;https://groups.google.com/forum/#!forum/tormenta-user&#34;&gt;Tormenta mailing list&lt;/a&gt;. Issues should be reported on the &lt;a href=&#34;https://github.com/twitter/tormenta/issues&#34;&gt;GitHub issue tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get Involved + Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests and bug reports are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;We use a lightweight form of project governence inspired by the one used by Apache projects. Please see &lt;a href=&#34;https://github.com/twitter/analytics-infra-governance#contributing-and-committership&#34;&gt;Contributing and Committership&lt;/a&gt; for our code of conduct and our pull request review process. The TL;DR is send us a pull request, iterate on the feedback + discussion, and get a +1 from a &lt;a href=&#34;https://raw.githubusercontent.com/twitter/tormenta/develop/COMMITTERS.md&#34;&gt;Committer&lt;/a&gt; in order to get your PR accepted.&lt;/p&gt; &#xA;&lt;p&gt;The current list of active committers (who can +1 a pull request) can be found here: &lt;a href=&#34;https://raw.githubusercontent.com/twitter/tormenta/develop/COMMITTERS.md&#34;&gt;Committers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A list of contributors to the project can be found here: &lt;a href=&#34;https://github.com/twitter/tormenta/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maven&lt;/h2&gt; &#xA;&lt;p&gt;Tormenta modules are available on Maven Central. The current groupid and version for all modules is, respectively, &lt;code&gt;&#34;com.twitter&#34;&lt;/code&gt; and &lt;code&gt;0.12.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Current published artifacts are&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-core_2.12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-core_2.11&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-core_2.10&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-kafka_2.12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-kafka_2.11&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-kafka_2.10&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-twitter_2.12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-twitter_2.11&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tormenta-twitter_2.10&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The suffix denotes the scala version.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Oscar Boykin &lt;a href=&#34;https://twitter.com/posco&#34;&gt;https://twitter.com/posco&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sam Ritchie &lt;a href=&#34;https://twitter.com/sritchie&#34;&gt;https://twitter.com/sritchie&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2017 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>filodb/FiloDB</title>
    <updated>2023-04-03T01:42:35Z</updated>
    <id>tag:github.com,2023-04-03:/filodb/FiloDB</id>
    <link href="https://github.com/filodb/FiloDB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Distributed Prometheus time series database&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FiloDB&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/velvia/FiloDB?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/velvia/FiloDB&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/filodb/FiloDB&#34;&gt;&lt;img src=&#34;https://travis-ci.org/filodb/FiloDB.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Distributed, Prometheus-compatible, real-time, in-memory, massively scalable, multi-schema time series / event / operational database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    _______ __      ____  ____ &#xA;   / ____(_) /___  / __ \/ __ )&#xA;  / /_  / / / __ \/ / / / __  |&#xA; / __/ / / / /_/ / /_/ / /_/ / &#xA;/_/   /_/_/\____/_____/_____/  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/Dantat.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt; &lt;em&gt;generated with &lt;a href=&#34;https://github.com/thlorenz/doctoc&#34;&gt;DocToc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#use-cases&#34;&gt;Use Cases&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#anti-use-cases&#34;&gt;Anti-use-cases&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#end-to-end-kafka-developer-setup&#34;&gt;End to End Kafka Developer Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#using-the-gateway-to-stream-application-metrics&#34;&gt;Using the Gateway to stream Application Metrics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#multiple-servers-using-consul&#34;&gt;Multiple Servers using Consul&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#local-scale-testing&#34;&gt;Local Scale Testing&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#understanding-the-filodb-data-model&#34;&gt;Understanding the FiloDB Data Model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#prometheus-filodb-schema-for-operational-metrics&#34;&gt;Prometheus FiloDB Schema for Operational Metrics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#traditional-multi-column-schema&#34;&gt;Traditional, Multi-Column Schema&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#data-modelling-and-performance-considerations&#34;&gt;Data Modelling and Performance Considerations&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#sharding&#34;&gt;Sharding&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#querying-filodb&#34;&gt;Querying FiloDB&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#filodb-promql-extensions&#34;&gt;FiloDB PromQL Extensions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#first-class-histogram-support&#34;&gt;First-Class Histogram Support&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#using-the-filodb-http-api&#34;&gt;Using the FiloDB HTTP API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#grafana-setup&#34;&gt;Grafana setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#using-the-cli&#34;&gt;Using the CLI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#cli-options&#34;&gt;CLI Options&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#configuring-the-cli&#34;&gt;Configuring the CLI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#current-status&#34;&gt;Current Status&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#deploying&#34;&gt;Deploying&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#monitoring-and-metrics&#34;&gt;Monitoring and Metrics&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#metrics-sinks&#34;&gt;Metrics Sinks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#metrics-configuration&#34;&gt;Metrics Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#code-walkthrough&#34;&gt;Code Walkthrough&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#building-and-testing&#34;&gt;Building and Testing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#debugging-serialization-and-queries&#34;&gt;Debugging serialization and queries&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#other-debugging-tips&#34;&gt;Other debugging tips&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#benchmarking&#34;&gt;Benchmarking&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#you-can-help&#34;&gt;You can help!&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;FiloDB is an open-source distributed, real-time, in-memory, massively scalable, multi-schema time series / event / operational database with Prometheus query support and some Spark support as well.&lt;/p&gt; &#xA;&lt;p&gt;The normal configuration for real-time ingestion is deployment as stand-alone processes in a cluster, ingesting directly from Apache Kafka. The processes form a cluster using peer-to-peer Akka Cluster technology.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Massively Scalable&lt;/strong&gt; - designed to ingest many millions of entities, sharded across multiple processes, with distributed querying built in&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prometheus PromQL Support&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tag-based Indexing&lt;/strong&gt; - Support for indexing and fast querying over flexible tags for each time series/partition, just like Prometheus&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient&lt;/strong&gt; - holds a huge amount of data in-memory thanks to columnar compression techniques&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low-latency&lt;/strong&gt; - designed for highly concurrent, low-latency workloads such as dashboards and alerting&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real Time&lt;/strong&gt; - data immediately available for querying once ingested&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fault Tolerant&lt;/strong&gt; - designed for dual-datacenter operation with strong recoverability and no single point of failure. explain explain&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Schema and Multi-Stream&lt;/strong&gt; - easily segregate and prioritize different classes of metrics and data. Easily support different types of events.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Off Heap&lt;/strong&gt; - intelligent memory management minimizes garbage collection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://velvia.github.io/presentations/2015-filodb-spark-streaming/#/&#34;&gt;Overview presentation&lt;/a&gt; -- see the docs folder for design docs.&lt;/p&gt; &#xA;&lt;p&gt;To compile the .mermaid source files to .png&#39;s, install the &lt;a href=&#34;http://knsv.github.io/mermaid/mermaidCLI.html&#34;&gt;Mermaid CLI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Use Cases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Real-time operational metrics storage, querying, dashboards, visibility&lt;/li&gt; &#xA; &lt;li&gt;Distributed tracing (ie Zipkin like) storage&lt;/li&gt; &#xA; &lt;li&gt;Low-latency real-time ad-hoc application metric debugging&lt;/li&gt; &#xA; &lt;li&gt;Real-time event storage and querying&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Anti-use-cases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Heavily transactional, update-oriented workflows&lt;/li&gt; &#xA; &lt;li&gt;OLAP / Analytics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oracle.com/java/technologies/javase-downloads.html#JDK11&#34;&gt;Java 11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.scala-sbt.org/&#34;&gt;SBT&lt;/a&gt; to build&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cassandra.apache.org/&#34;&gt;Apache Cassandra&lt;/a&gt; 2.x or 3.x (We prefer using &lt;a href=&#34;https://github.com/pcmanus/ccm&#34;&gt;CCM&lt;/a&gt; for local testing) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For testing, install a single node C* cluster, like this: &lt;code&gt;ccm create v39_single -v 3.9 -n 1 -s&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt; 0.10.x or above&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark (2.0)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the project and cd into the project directory,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ git clone https://github.com/filodb/FiloDB.git&#xA;$ cd FiloDB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It is recommended you use the last stable released version.&lt;/li&gt; &#xA;   &lt;li&gt;To build, run &lt;code&gt;filo-cli&lt;/code&gt; (see below) and also &lt;code&gt;sbt spark/assembly&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Follow the instructions below to set up an end to end local environment.&lt;/p&gt; &#xA;&lt;h2&gt;End to End Kafka Developer Setup&lt;/h2&gt; &#xA;&lt;p&gt;This section describes how you can run an end-to-end test locally on a Macbook by ingesting time series data into FiloDB In Memory Store, and querying from it using PromQL.&lt;/p&gt; &#xA;&lt;p&gt;Use your favorite package manager to install and set up pre-requisite infrastructure. Kafka 0.10.2+ or 0.11 can be used.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install kafka&#xA;brew services start zookeeper&#xA;brew services start kafka&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may see this error from kafka log if you use an M1 chip Mac.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;/opt/homebrew/Cellar/kafka/3.3.1_1/libexec/bin/kafka-run-class.sh: line 342: /opt/homebrew/@@HOMEBREW_JAVA@@/bin/java: No such file or directory&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To resolve the issue, you may run brew bottle to get the installation file and reinstall kafka through it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew bottle --skip-relocation kafka&#xA;brew reinstall `ls kafka*bottle*`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a new Kafka topic with 4 partitions. This is where time series data will be ingested for FiloDB to consume&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 4 --topic timeseries-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and start Cassandra 2.1 or more recent versions (Cassandra 3 and above recommended).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bin/cassandra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should install Cassandra using a tool which you&#39;re the most familiar with.&lt;/p&gt; &#xA;&lt;p&gt;For instance, one easy way to install it is via &lt;code&gt;brew&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cassandra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are working on an Apple M1 laptop, you may need to apply workaround mentioned in &lt;a href=&#34;https://stackoverflow.com/questions/68315912/how-to-start-cassandra-on-a-m1-macbook&#34;&gt;here&lt;/a&gt; to move past the JNA issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp lib/cassandra/jna-5.10.0.jar /opt/homebrew/Cellar/cassandra/4.0.7/libexec/jna-5.6.0.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start Cassandra&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew services start cassandra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the required projects&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt standalone/assembly cli/assembly gateway/assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First initialize the keyspaces and tables in Cassandra.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./scripts/schema-create.sh filodb_admin filodb filodb_downsample prometheus 4 1,5 &amp;gt; /tmp/ddl.cql&#xA;cqlsh -f /tmp/ddl.cql&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Verify that tables were created in &lt;code&gt;filodb&lt;/code&gt;, &lt;code&gt;filodb_downsample&lt;/code&gt; and &lt;code&gt;filodb-admin&lt;/code&gt; keyspaces using &lt;code&gt;cqlsh&lt;/code&gt;: First type &lt;code&gt;cqlsh&lt;/code&gt; to start the cassandra cli. Then check the keyspaces by entering &lt;code&gt;DESCRIBE keyspaces&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script below brings up the FiloDB Dev Standalone server, and then sets up the prometheus dataset (NOTE: if you previously started FiloDB and have not cleared the metadata, then the -s is not needed as FiloDB will recover previous ingestion configs from Cassandra. This script targets directly towards the &lt;code&gt;develop&lt;/code&gt; branch.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filodb-dev-start.sh -o 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;o&lt;/code&gt; argument is the ordinal of the filodb server. This is used to determine which shards are assigned. Note that the above script starts the server with configuration at &lt;code&gt;conf/timeseries-filodb-server.conf&lt;/code&gt;. This config file refers to the following datasets that will be loaded on bootstrap:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;conf/timeseries-dev-source.conf&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For queries to work properly you&#39;ll want to start a second server to serve all the shards:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filodb-dev-start.sh -o 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To quickly verify that both servers are up and set up for ingestion, do this (the output below was formatted using &lt;code&gt;| jq &#39;.&#39;&lt;/code&gt;, ports may vary):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl localhost:8080/api/v1/cluster/prometheus/status&#xA;&#xA;{&#xA;  &#34;status&#34;: &#34;success&#34;,&#xA;  &#34;data&#34;: [&#xA;    {&#xA;      &#34;shard&#34;: 0,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka://filo-standalone&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 1,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka://filo-standalone&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 2,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka.tcp://filo-standalone@127.0.0.1:57749&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 3,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka.tcp://filo-standalone@127.0.0.1:57749&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also check the server logs at &lt;code&gt;logs/filodb-server-N.log&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Now run the time series generator. This will ingest 20 time series (the default) with 100 samples each into the Kafka topic with current timestamps. The required argument is the path to the source config. Use &lt;code&gt;--help&lt;/code&gt; for all the options.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./dev-gateway.sh --gen-gauge-data conf/timeseries-dev-source.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Check logs/gateway-server.log for logs.&lt;/p&gt; &#xA;&lt;p&gt;At this point, you should be able to confirm such a message in the server logs: &lt;code&gt;KAMON counter name=memstore-rows-ingested count=4999&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now you are ready to query FiloDB for the ingested data. The following command should return matching subset of the data that was ingested by the producer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dfilodb.v2-cluster-enabled=true --host 127.0.0.1 --dataset prometheus --promql &#39;heap_usage0{_ws_=&#34;demo&#34;, _ns_=&#34;App-2&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also look at Cassandra to check for persisted data. Look at the tables in &lt;code&gt;filodb&lt;/code&gt; and &lt;code&gt;filodb-admin&lt;/code&gt; keyspaces.&lt;/p&gt; &#xA;&lt;p&gt;If the above does not work, try the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Delete the Kafka topic and re-create it. Note that Kafka topic deletion might not happen until the server is stopped and restarted&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before you remove a topic, update server.properties from configuration(conf) folder and have delete.topic.enable property set to true: &lt;code&gt;delete.topic.enable=true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run below kafka-topics.sh command with “–delete” option to remove “timeseries-dev” and &#34;timeseries-dev-ds-1m&#34;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt; kafka-topics.sh --bootstrap-server localhost:9092 --topic timeseries-dev --delete  &#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;code&gt;./filodb-dev-stop.sh&lt;/code&gt; and restart filodb instances like above&lt;/li&gt; &#xA; &lt;li&gt;Re-run &lt;code&gt;./dev-gateway.sh --gen-gauge-data&lt;/code&gt;. You can check consumption via running the &lt;code&gt;TestConsumer&lt;/code&gt;, like this: &lt;code&gt;java -Xmx4G -Dconfig.file=conf/timeseries-filodb-server.conf -cp standalone/target/scala-2.12/standalone-assembly-0.8-SNAPSHOT.jar filodb.kafka.TestConsumer conf/timeseries-dev-source.conf&lt;/code&gt;. Also, the &lt;code&gt;memstore_rows_ingested&lt;/code&gt; metric which is logged to &lt;code&gt;logs/filodb-server-N.log&lt;/code&gt; should become nonzero.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To stop the dev server. Note that this will stop all the FiloDB servers if multiple are running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filodb-dev-stop.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the Gateway to stream Application Metrics&lt;/h3&gt; &#xA;&lt;p&gt;FiloDB includes a Gateway server that listens to application metrics and data on a TCP port, converts the data to its internal format, shards it properly and sends it Kafka.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;STATUS&lt;/strong&gt;: Currently the only supported format is &lt;a href=&#34;https://docs.influxdata.com/influxdb/v1.6/write_protocols/line_protocol_tutorial/&#34;&gt;Influx Line Protocol&lt;/a&gt;. The only tested configuration is using Telegraf with a Prometheus endpoint source and a socket writer using ILP protocol.&lt;/p&gt; &#xA;&lt;p&gt;The following will scrape metrics from FiloDB using its Prometheus metrics endpoint, and forward it to Kafka to be queried by FiloDB itself :)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure the above steps are followed for setting up and starting FiloDB, configuring datasets and Kafka topics.&lt;/li&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;https://github.com/influxdata/telegraf&#34;&gt;Telegraf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start the FiloDB gateway: &lt;code&gt;./dev-gateway.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start Telegraf using the config file &lt;code&gt;conf/telegraf.conf&lt;/code&gt; : &lt;code&gt;telegraf --config conf/telegraf.conf&lt;/code&gt;. This config file scrapes from a Prom endpoint at port 9095 and forwards it using ILP format to a TCP socket at 8007, which is the gateway default&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, metrics from the application having a Prom endpoint at port 9095 will be streamed into Kafka and FiloDB.&lt;/p&gt; &#xA;&lt;p&gt;Querying the total number of ingesting time series for the last 5 minutes, every 10 seconds:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dfilodb.v2-cluster-enabled=true  --host 127.0.0.1 --dataset prometheus --promql &#39;sum(num_ingesting_partitions{_ws_=&#34;local_test&#34;,_ns_=&#34;filodb&#34;})&#39; --minutes 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that histograms are ingested using FiloDB&#39;s optimized histogram format, which leads to very large savings in space. For example, querying the 90%-tile for the size of chunks written to Cassandra, last 5 minutes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dfilodb.v2-cluster-enabled=true  --host 127.0.0.1 --dataset prometheus --promql &#39;histogram_quantile(0.9, sum(rate(chunk_bytes_per_call{_ws_=&#34;local_test&#34;,_ns_=&#34;filodb&#34;}[3m])))&#39; --minutes 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is how you display the raw histogram data for the same:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dfilodb.v2-cluster-enabled=true --host 127.0.0.1 --dataset prometheus --promql &#39;chunk_bytes_per_call{_ws_=&#34;local_test&#34;,_ns_=&#34;filodb&#34;}&#39; --minutes 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downsample Filo Cluster&lt;/h3&gt; &#xA;&lt;p&gt;To bring up local cluster for serving downsampled data&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filodb-dev-start.sh -o 0 -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Subsequent servers. Change log file suffix with the &lt;code&gt;-l&lt;/code&gt; option for each server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filodb-dev-start.sh -o 1 -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you had run the unit test &lt;code&gt;DownsamplerMainSpec&lt;/code&gt; which populates data into the downsample dataset, you can query downsample results by visiting the following URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#34;http://localhost:9080/promql/prometheus/api/v1/query_range?query=my_counter\{_ws_=&#39;my_ws&#39;,_ns_=&#39;my_ns&#39;\}&amp;amp;start=74372801&amp;amp;end=74373042&amp;amp;step=10&amp;amp;verbose=true&amp;amp;spread=2&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Local Scale Testing&lt;/h4&gt; &#xA;&lt;p&gt;Follow the same steps as in original setup, but do this first to clear out existing metadata:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./filo-cli -Dconfig.file=conf/timeseries-filodb-server.conf --command clearMetadata&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the steps to create the dataset etc. Create a different Kafka topic with 128 partitions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 128 --topic timeseries-perf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify server config to load the &lt;code&gt;conf/timeseries-128shards-source.conf&lt;/code&gt; dataset instead of the default one.&lt;/p&gt; &#xA;&lt;p&gt;Start two servers as follows. This will not start ingestion yet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./filodb-dev-start.sh -o 0&#xA;./filodb-dev-start.sh -o 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now if you curl the cluster status you should see 128 shards which are slowly turning active: &lt;code&gt;curl http://127.0.0.1:8080/api/v1/cluster/timeseries/status | jq &#39;.&#39;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generate records:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./dev-gateway.sh --gen-gauge-data -p 5000 conf/timeseries-128shards-source.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Understanding the FiloDB Data Model&lt;/h2&gt; &#xA;&lt;p&gt;FiloDB is designed to scale to ingest and query millions of discrete time series. A single time series consists of data points that contain the same &lt;strong&gt;partition key&lt;/strong&gt;. Successive data points are appended. Each data point must contain a timestamp. Examples of time series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Individual operational metrics&lt;/li&gt; &#xA; &lt;li&gt;Data from a single IoT device&lt;/li&gt; &#xA; &lt;li&gt;Events from a single application, device, or endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;strong&gt;partition key&lt;/strong&gt; differentiates time series and also controls distribution of time series across the cluster. For more information on sharding, see the sharding section below. Components of a partition key, including individual key/values of &lt;code&gt;MapColumn&lt;/code&gt;s, are indexed and used for filtering in queries.&lt;/p&gt; &#xA;&lt;p&gt;The data points use a configurable schema consisting of multiple columns. Each column definition consists of &lt;code&gt;name:columntype&lt;/code&gt;, with optional parameters. For examples, see the examples below, or see the introductory walk-through above where two datasets are created.&lt;/p&gt; &#xA;&lt;p&gt;A single partition key schema is used for a running FiloDB cluster, though multiple data schemas may be supported. These schemas are defined in the config file - see the &lt;code&gt;partition-schema&lt;/code&gt; and &lt;code&gt;schemas&lt;/code&gt; sections of &lt;code&gt;filodb-defaults.conf&lt;/code&gt;. The CLI command &lt;code&gt;validateSchemas&lt;/code&gt; may be run to verify schemas defined in config files, as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dconfig.file=conf/timeseries-filodb-server.conf --command validateSchemas&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset Configuration&lt;/h3&gt; &#xA;&lt;p&gt;THIS IS IMPORTANT TO READ AND UNDERSTAND.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;dataset&#34; ingests one stream or Kafka topic of raw time series data, and is also the unit of isolation. Each dataset contains its own offheap memory, and can have independent data retention and ingestion properties.&lt;/p&gt; &#xA;&lt;p&gt;Datasets are setup and loaded into the server via configuration files referred to by application.conf loaded by the server. See &lt;code&gt;conf/timeseries-dev-source.conf&lt;/code&gt; for an example. It is important to note that some aspects of the dataset, like its column definition are immutable. This is primarily because the data columns are used to populate persistent cassandra store. Once created, it should not be changed. If altered, it may render cassandra unreadable.&lt;/p&gt; &#xA;&lt;p&gt;However, some aspects of the dataset which relate to runtime configuration are mutable. For example, amount of chunk and buffer memory allocated, flush interval etc. That said, these are advanced settings and one needs to know implications of the configuration before changing them. If a rolling bounce is being done, one needs to remember that part of the cluster could be with the old config and the rest could have new config applied.&lt;/p&gt; &#xA;&lt;h3&gt;Prometheus FiloDB Schema for Operational Metrics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Partition key = &lt;code&gt;metric:string,tags:map&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Columns: &lt;code&gt;timestamp:ts,value:double:detectDrops=true&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The above is the classic Prometheus-compatible schema. It supports indexing on any tag. Thus standard Prometheus queries that filter by a tag such as &lt;code&gt;hostname&lt;/code&gt; or &lt;code&gt;datacenter&lt;/code&gt; for example would work fine. Note that the Prometheus metric name, which in Prometheus data is a label/tag with the key &lt;code&gt;__name__&lt;/code&gt;, is separated out to an explicit metric column.&lt;/p&gt; &#xA;&lt;p&gt;Note that in the Prometheus data model, more complex metrics such as histograms are represented as individual time series. This has some simplicity benefits, but does use up more time series and incur extra I/O overhead when transmitting raw data records.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: &lt;code&gt;detectDrops=true&lt;/code&gt; allows for proper and efficient rate calculation on Prometheus counters.&lt;/p&gt; &#xA;&lt;h3&gt;Traditional, Multi-Column Schema&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s say that one had a metrics client, such as CodaHale metrics, which pre-aggregates percentiles and sends them along with the metric. If we used the Prometheus schema, each percentile would wind up in its own time series. This is fine, but incurs significant overhead as the partition key has to then be sent with each percentile over the wire. Instead we can have a schema which includes all the percentiles together when sending the data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Partition key = &lt;code&gt;metricName:string,tags:map&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Columns: &lt;code&gt;timestamp:ts,min:double,max:double,p50:double,p90:double,p95:double,p99:double,p999:double&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Modelling and Performance Considerations&lt;/h3&gt; &#xA;&lt;p&gt;For more information on memory configuration, please have a look at the &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/ingestion.md&#34;&gt;ingestion guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Choosing Partition Keys&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FiloDB is designed to efficiently ingest a huge number of individual time series - depending on available memory, one million or more time series per node is achievable. Here are some pointers on choosing them:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is better to have smaller time series, as the indexing and filtering operations are designed to work on units of time series, and not samples within each time series.&lt;/li&gt; &#xA; &lt;li&gt;The default partition key consists of a metric name, and tags represented as a &lt;code&gt;MapColumn&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Each time series does take up both heap and offheap memory, and memory is likely the main limiting factor. The amount of configured memory limits the number of actively ingesting time series possible at any moment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sharding&lt;/h3&gt; &#xA;&lt;p&gt;All data for a single time series, which belong to the same partition key, are routed to the same shard, or Kafka partition, and one shard must fit completely into the memory of a single node.&lt;/p&gt; &#xA;&lt;p&gt;The number of shards in each dataset is preconfigured in the source config. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/ingestion.md&#34;&gt;ingestion doc&lt;/a&gt; for more information on configuration.&lt;/p&gt; &#xA;&lt;p&gt;Metrics are routed to shards based on factors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Shard keys, which can be for example an application and the metric name, which define a group of shards to use for that application. This allows limiting queries to a subset of shards for lower latency. Currently &lt;code&gt;_ws_&lt;/code&gt;, &lt;code&gt;_ns_&lt;/code&gt; labels are mandatory to calculate shard key along with &lt;code&gt;_metric_&lt;/code&gt; column.&lt;/li&gt; &#xA; &lt;li&gt;The rest of the tags or components of a partition key are then used to compute which shard within the group of shards to assign to.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Querying FiloDB&lt;/h2&gt; &#xA;&lt;p&gt;FiloDB can be queried using the &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/querying/basics/&#34;&gt;Prometheus Query Language&lt;/a&gt; through its HTTP API or through its CLI.&lt;/p&gt; &#xA;&lt;h3&gt;FiloDB PromQL Extensions&lt;/h3&gt; &#xA;&lt;p&gt;FiloDB supports computing Z-score queries. It represents the distance between the raw score and the population mean in units of the standard deviation. This can be used for anomaly detection.&lt;/p&gt; &#xA;&lt;p&gt;Since FiloDB supports multiple schemas, with possibly more than one value column per schema, there needs to be a way to specify the target column to query. This is done using a &lt;code&gt;::columnName&lt;/code&gt; suffix in the metric name, like this request which pulls out the &#34;min&#34; column:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http_req_timer::min{_ws_=&#34;demo&#34;, _ns_=&#34;foo&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default if &lt;code&gt;::col&lt;/code&gt; suffix is not specified then the &lt;code&gt;value-column&lt;/code&gt; option of each data schema is used.&lt;/p&gt; &#xA;&lt;p&gt;Some special functions exist to aid debugging and for other purposes:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;function&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;_filodb_chunkmeta_all&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(CLI Only) Returns chunk metadata fields for all chunks matching the time range and filter criteria - ID, # rows, start and end time, as well as the number of bytes and type of encoding used for a particular column.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;histogram_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Extract a bucket from a HistogramColumn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;histogram_max_quantile&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;More accurate &lt;code&gt;histogram_quantile&lt;/code&gt; when the max is known&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;hist_to_prom_vectors&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Convert a histogram to a set of equivalent Prometheus-style bucket time series&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Example of debugging chunk metadata using the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli --host 127.0.0.1 --dataset prometheus --promql &#39;_filodb_chunkmeta_all(heap_usage0{_ws_=&#34;demo&#34;,_ns_=&#34;App-0&#34;})&#39; --start XX --end YY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is also a special filter, &lt;code&gt;_type_=&#34;gauge&#34;&lt;/code&gt;, to filter on only a particular type of metric or schema. Normally, this is not necessary unless a user changes the type of metric in their application, for example from a gauge to a histogram. The types are found in the configuration &lt;code&gt;schemas&lt;/code&gt; section, and by default are &lt;code&gt;gauge&lt;/code&gt;, &lt;code&gt;prom-counter&lt;/code&gt;, &lt;code&gt;prom-histogram&lt;/code&gt;, and &lt;code&gt;untyped&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;First-Class Histogram Support&lt;/h3&gt; &#xA;&lt;p&gt;One major difference FiloDB has from the Prometheus data model is that FiloDB supports histograms as a first-class entity. In Prometheus, histograms are stored with each bucket in its own time series differentiated by the &lt;code&gt;le&lt;/code&gt; tag. In FiloDB, there is a &lt;code&gt;HistogramColumn&lt;/code&gt; which stores all the buckets together for significantly improved compression, especially over the wire during ingestion, as well as significantly faster query speeds (up to two orders of magnitude). There is no &#34;le&#34; tag or individual time series for each bucket. Here are the differences users need to be aware of when using &lt;code&gt;HistogramColumn&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There is no need to append &lt;code&gt;_bucket&lt;/code&gt; to the metric name.&lt;/li&gt; &#xA; &lt;li&gt;To compute quantiles: &lt;code&gt;histogram_quantile(0.7, sum(rate(http_req_latency{app=&#34;foo&#34;}[5m])))&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;To extract a bucket: &lt;code&gt;http_req_latency{app=&#34;foo&#34;,_bucket_=&#34;100.0&#34;}&lt;/code&gt; (&lt;code&gt;_bucket_&lt;/code&gt; is a special filter that translates to the &lt;code&gt;histogram_bucket&lt;/code&gt; function for bucket extraction)&lt;/li&gt; &#xA; &lt;li&gt;Sum over multiple Histogram time series: &lt;code&gt;sum(rate(http_req_latency{app=&#34;foo&#34;}[5m]))&lt;/code&gt; - you could then compute quantile over the sum.&lt;/li&gt; &#xA; &lt;li&gt;To convert a &lt;code&gt;HistogramColumn&lt;/code&gt; data back to Prometheus-style time series for each bucket, use the &lt;code&gt;hist_to_prom_vectors&lt;/code&gt; function &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NOTE: Do NOT use &lt;code&gt;by (le)&lt;/code&gt; when summing &lt;code&gt;HistogramColumns&lt;/code&gt;. This is not appropriate as the &#34;le&#34; tag is not used. FiloDB knows how to sum multiple histograms together correctly without grouping tricks.&lt;/li&gt; &#xA;   &lt;li&gt;FiloDB prevents many incorrect histogram aggregations in Prometheus when using &lt;code&gt;HistogramColumn&lt;/code&gt;, such as handling of multiple histogram schemas across time series and across time.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FiloDB offers an improved accuracy &lt;code&gt;histogram_max_quantile&lt;/code&gt; function designed to work with a max column from the source. If clients are able to send the max value captured during a window, then we can report more accurate upper quantiles (ie 99%, 99.9%, etc.) that do not suffer from clipping.&lt;/p&gt; &#xA;&lt;h3&gt;Step &amp;amp; Lookback PromQL Improvements&lt;/h3&gt; &#xA;&lt;p&gt;The value provided within square brackets in the Range Vector Selector expression of the PromQL expression is commonly known as &#34;range&#34; or &#34;lookback&#34;. It indicates how far behind each instant the query engine will lookback to normalize the value for any given instant.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Problem 1:&lt;/strong&gt; PromQL has an important gotcha on range value. Hard coded lookback can yield to queries with unintended effects. Lets consider the expression &lt;code&gt;rate(foo[5m])&lt;/code&gt;. The result of this query would have completely different meanings with different step values. Since step parameter typically is not controlled by user (as in Grafana), it can have silent negative effects. For example, if user expands the time range and hardcoded lookback becomes smaller than step, the same query will no longer do what the user originally intended to do - the lookback will not include enough samples for an effective query.&lt;/p&gt; &#xA;&lt;p&gt;To alleviate this, we introduce a new range notation where lookback can be specified as a multiple of step. The notation &lt;code&gt;[1i]&lt;/code&gt; would cause lookback to be 1 times step. Notation &lt;code&gt;[2i]&lt;/code&gt; would make lookback as 2 times step and so on. This feature is in alignment with &lt;a href=&#34;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/MetricsQL&#34;&gt;MetricsQL&lt;/a&gt; approach to solving this.&lt;/p&gt; &#xA;&lt;p&gt;This notation will help users who do not know what range value to use in their queries. The &lt;code&gt;[1i]&lt;/code&gt; should be a good default for all queries. 2 or more times step can be used when smoothing is needed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Problem 2:&lt;/strong&gt; There is yet another gotcha that we solve. If step == lookback, lookback windows in consecutive instants become adjacent and non-overlapping. In the case of cumulative counters, where rate or increase is used, this can still yield to wrong results. The increase or decrease between the adjacent windows would be omitted from the query.&lt;/p&gt; &#xA;&lt;p&gt;The solution that we have adopted to solve this is to extend the lookback window by the publish interval of the time series. The padding is done if the publish interval is supplied. The publish interval is supplied as an internal &lt;code&gt;_step_&lt;/code&gt; tag within the time series tags. The intent here is to make it better over time, by making it a first class column in the partition key schema.&lt;/p&gt; &#xA;&lt;p&gt;Important to note that the lookback padding happens &lt;strong&gt;only&lt;/strong&gt; if step multiple notation was used in the PromQL expression. If a fixed range was used in the query, that value will be honored as such. This way, backward compatibility will always be retained.&lt;/p&gt; &#xA;&lt;p&gt;Examples&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sum_over_time(foo[5i])&lt;/code&gt; with step = 10s will be equivalent to &lt;code&gt;sum_over_time(foo[50s])&lt;/code&gt; , thus lookback = 5 * 10s&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sum_over_time(foo[1i])&lt;/code&gt; with step = 10s will be equivalent to &lt;code&gt;sum_over_time(foo[10s])&lt;/code&gt; , thus lookback = 1 * 10s&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rate(foo[1i])&lt;/code&gt; with step = 60s and publishInterval = 10s will be equivalent to &lt;code&gt;rate(foo[70s])&lt;/code&gt;, thus lookback = 1 * 60s + 10s. Publish interval is padded when both counter functions and step-factor notation is used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sum_over_time(foo[2.5i])&lt;/code&gt; - decimal scale can also be used. Here lookback will be 2.5 times the step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;foo offset 2i&lt;/code&gt; - offset can also use step multiple notation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using the FiloDB HTTP API&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/http_api.md&#34;&gt;HTTP API&lt;/a&gt; doc.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#39;localhost:8080/promql/timeseries/api/v1/query?query=memstore_rows_ingested_total{_ws_=&#34;demo&#34;, _ns_=&#34;filodb&#34;}[5m]&amp;amp;time=1568756529&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;data&#34;: {&#xA;    &#34;resultType&#34;: &#34;matrix&#34;,&#xA;    &#34;result&#34;: [&#xA;      {&#xA;        &#34;metric&#34;: {&#xA;          &#34;host&#34;: &#34;MacBook-Pro-229.local&#34;,&#xA;          &#34;shard&#34;: &#34;1&#34;,&#xA;          &#34;__name__&#34;: &#34;memstore_rows_ingested_total&#34;,&#xA;          &#34;dataset&#34;: &#34;prometheus&#34;,&#xA;          &#34;_ws_&#34;: &#34;demo&#34;,&#xA;          &#34;_ns_&#34;: &#34;filodb&#34;&#xA;        },&#xA;        &#34;values&#34;: [&#xA;          [&#xA;            1539908420,&#xA;            &#34;252.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908430,&#xA;            &#34;252.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908440,&#xA;            &#34;252.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908450,&#xA;            &#34;252.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908460,&#xA;            &#34;252.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908470,&#xA;            &#34;360.0&#34;&#xA;          ]&#xA;        ]&#xA;      },&#xA;      {&#xA;        &#34;metric&#34;: {&#xA;          &#34;host&#34;: &#34;MacBook-Pro-229.local&#34;,&#xA;          &#34;shard&#34;: &#34;0&#34;,&#xA;          &#34;__name__&#34;: &#34;memstore_rows_ingested_total&#34;,&#xA;          &#34;dataset&#34;: &#34;prometheus&#34;,&#xA;          &#34;_ws_&#34;: &#34;demo&#34;,&#xA;          &#34;_ns_&#34;: &#34;filodb&#34;&#xA;        },&#xA;        &#34;values&#34;: [&#xA;          [&#xA;            1539908420,&#xA;            &#34;462.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908430,&#xA;            &#34;462.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908440,&#xA;            &#34;462.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908450,&#xA;            &#34;462.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908460,&#xA;            &#34;462.0&#34;&#xA;          ],&#xA;          [&#xA;            1539908470,&#xA;            &#34;660.0&#34;&#xA;          ]&#xA;        ]&#xA;      }&#xA;    ]&#xA;  },&#xA;  &#34;status&#34;: &#34;success&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The HTTP API can also be used to quickly check on the cluster and shard status:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl localhost:8080/api/v1/cluster/timeseries/status | jq &#39;.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;status&#34;: &#34;success&#34;,&#xA;  &#34;data&#34;: [&#xA;    {&#xA;      &#34;shard&#34;: 0,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka://filo-standalone&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 1,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka://filo-standalone&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 2,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka.tcp://filo-standalone@127.0.0.1:52519&#34;&#xA;    },&#xA;    {&#xA;      &#34;shard&#34;: 3,&#xA;      &#34;status&#34;: &#34;ShardStatusActive&#34;,&#xA;      &#34;address&#34;: &#34;akka.tcp://filo-standalone@127.0.0.1:52519&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grafana setup&lt;/h3&gt; &#xA;&lt;p&gt;Since FiloDB exposes a Prometheus-compatible HTTP API, it is possible to set up FiloDB as a Grafana data source.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the data source type to &#34;Prometheus&#34;&lt;/li&gt; &#xA; &lt;li&gt;In the HTTP URL box, enter in the FiloDB HTTP URL (usually the load balancer for all the FiloDB endpoints). Be sure to append &lt;code&gt;/promql/timeseries/&lt;/code&gt;, where you would put the name of the dataset instead of &#34;timeseries&#34; if it is not called timeseries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using the CLI&lt;/h3&gt; &#xA;&lt;p&gt;The CLI is now primarily used to interact with standalone FiloDB servers, including querying, getting status, and as a way to initialize dataset definitions and do admin tasks. The following examples use the FiloDB/Gateway/Telegraf setup from the section &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/#using-the-gateway-to-stream-application-metrics&#34;&gt;Using the Gateway to stream Application Metrics&lt;/a&gt; -- but be sure to start a second FiloDB server, using &lt;code&gt;./filodb-dev-start.sh -l 2 -p&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;indexnames&lt;/strong&gt; command lists all of the indexed tag keys or column names, based on the partition key or Prometheus key/value tags that define time series:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli &#39;-Dakka.remote.netty.tcp.hostname=127.0.0.1&#39; --host 127.0.0.1 --dataset prometheus --command indexnames&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;le&#xA;host&#xA;shard&#xA;__name__&#xA;dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;strong&gt;indexvalues&lt;/strong&gt; command lists the top values (as well as their cardinality) in specific shards for any given tag key or column name. Here we list the top metrics (for Prometheus schema, which uses the tag &lt;code&gt;__name__&lt;/code&gt;) in shard 0:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli &#39;-Dakka.remote.netty.tcp.hostname=127.0.0.1&#39; --host 127.0.0.1 --dataset prometheus --command indexvalues --indexName __name__ --shards 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;             chunk_bytes_per_call_bucket  10&#xA;                  chunks_per_call_bucket  10&#xA;       kafka_container_size_bytes_bucket  10&#xA;        num_samples_per_container_bucket  10&#xA;       blockstore_blocks_reclaimed_total  1&#xA;                  blockstore_free_blocks  1&#xA;blockstore_time_ordered_blocks_reclaimed_total  1&#xA;                  blockstore_used_blocks  1&#xA;             memstore_data_dropped_total  1&#xA;memstore_encoded_bytes_allocated_bytes_total  1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, let&#39;s query a particular metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli &#39;-Dakka.remote.netty.tcp.hostname=127.0.0.1&#39; --host 127.0.0.1 --dataset prometheus --promql &#39;memstore_rows_ingested_total{_ws_=&#34;demo&#34;, _ns_=&#34;filodb&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Sending query command to server for prometheus with options QueryOptions(&amp;lt;function1&amp;gt;,16,60,100,None)...&#xA;Query Plan:&#xA;PeriodicSeries(RawSeries(IntervalSelector(List(1539908042000),List(1539908342000)),List(ColumnFilter(app,Equals(filodb)), ColumnFilter(__name__,Equals(memstore_rows_ingested_total))),List()),1539908342000,10000,1539908342000)&#xA;/shard:1/b2[[__name__: memstore_rows_ingested_total, app: filodb, dataset: prometheus, host: MacBook-Pro-229.local, shard: 1]]&#xA;  2018-10-18T17:19:02.000-07:00 (1s ago) 1539908342000  36.0&#xA;&#xA;/shard:3/b2[[__name__: memstore_rows_ingested_total, app: filodb, dataset: prometheus, host: MacBook-Pro-229.local, shard: 0]]&#xA;  2018-10-18T17:19:02.000-07:00 (2s ago) 1539908342000  66.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Options&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;filo-cli&lt;/code&gt; accepts arguments and options as key-value pairs, specified like &lt;code&gt;--limit=100&lt;/code&gt; For quick help run it with no arguments. A subset of useful options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;key&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dataset&lt;/td&gt; &#xA;   &lt;td&gt;It is required for all the operations. Its value should be the name of the dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;host&lt;/td&gt; &#xA;   &lt;td&gt;The hostname or IP address of the FiloDB standalone host&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;port&lt;/td&gt; &#xA;   &lt;td&gt;The port number of the FiloDB standalone host. Defaults to 2552.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;start&lt;/td&gt; &#xA;   &lt;td&gt;The start of the query timerange in seconds since epoch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;step&lt;/td&gt; &#xA;   &lt;td&gt;The step size in seconds of the PromQL query. Successive windows occur at every step seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;stop&lt;/td&gt; &#xA;   &lt;td&gt;The end of the query timerange in seconds since epoch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;minutes&lt;/td&gt; &#xA;   &lt;td&gt;A shortcut to set the start at N minutes ago, and the stop at current time. Should specify a step also.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;chunks&lt;/td&gt; &#xA;   &lt;td&gt;Either &#34;memory&#34; or &#34;buffers&#34; to select either all the in-memory chunks or the write buffers only. Should specify a step also.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;database&lt;/td&gt; &#xA;   &lt;td&gt;Specifies the &#34;database&#34; the dataset should operate in. For Cassandra, this is the keyspace. If not specified, uses config value.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;limit&lt;/td&gt; &#xA;   &lt;td&gt;Limits the number of time series in the output&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sampleLimit&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of output samples in the query result. An exception is thrown if the output returns more results than this.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shards&lt;/td&gt; &#xA;   &lt;td&gt;(EXPERT) overrides the automatic shard calculation by passing in a comma-separated list of specific shards to query. Very useful to debug sharding issues.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;everyNSeconds&lt;/td&gt; &#xA;   &lt;td&gt;Repeats the query every (argument) seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;timeoutSeconds&lt;/td&gt; &#xA;   &lt;td&gt;The number of seconds for the network timeout&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Configuring the CLI&lt;/h3&gt; &#xA;&lt;p&gt;Using the CLI to initialize Cassandra clusters and datasets necessitates passing in the right configuration. The easiest is to pass in a config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dconfig.file=conf/timeseries-filodb-server.conf --command init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also set the &lt;code&gt;FILO_CONFIG_FILE&lt;/code&gt; environment var instead, but any &lt;code&gt;-Dfilodb.config.file&lt;/code&gt; args passed in takes precedence.&lt;/p&gt; &#xA;&lt;p&gt;Individual configuration params may also be changed by passing them on the command line. They must be the first arguments passed in. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./filo-cli -Dfilodb.cassandra.keyspace=mykeyspace --command init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All &lt;code&gt;-D&lt;/code&gt; config options must be passed before any other arguments.&lt;/p&gt; &#xA;&lt;p&gt;You may also configure CLI logging by copying &lt;code&gt;cli/src/main/resources/logback.xml&lt;/code&gt; to your deploy folder, customizing it, and passing on the command line &lt;code&gt;-Dlogback.configurationFile=/path/to/filo-cli-logback.xml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also change the logging directory by setting the FILO_LOG_DIR environment variable before calling the CLI.&lt;/p&gt; &#xA;&lt;h3&gt;Debugging Binary Vectors and Binary Records&lt;/h3&gt; &#xA;&lt;p&gt;Following command can be used to formulate BR for a partKey using Prometheus filter. Use this to formulate CQL to issue a cassandra query&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./filo-cli --command promFilterToPartKeyBR --promql &#34;myMetricName{_ws_=&#39;myWs&#39;,_ns_=&#39;myNs&#39;}&#34; --schema prom-counter&#xA;0x2c0000000f1712000000200000004b8b36940c006d794d65747269634e616d650e00c104006d794e73c004006d795773&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following command can be used to decode partKey BR into tag/value pairs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./filo-cli --command partKeyBrAsString --hexPk 0x2C0000000F1712000000200000004B8B36940C006D794D65747269634E616D650E00C104006D794E73C004006D795773&#xA;b2[schema=prom-counter  _metric_=myMetricName,tags={_ns_: myNs, _ws_: myWs}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following command can be used to decode a ChunkSetInfo read from Cassandra&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./filo-cli  --command decodeChunkInfo --hexChunkInfo 0x12e8253a267ea2db060000005046fc896e0100005046fc896e010000&#xA;ChunkSetInfo id=-2620393330526787566 numRows=6 startTime=1574272801000 endTime=1574273042000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following command can be used to decode a Binary Vector. Valid vector types are &lt;code&gt;d&lt;/code&gt; for double, &lt;code&gt;i&lt;/code&gt; for integer &lt;code&gt;l&lt;/code&gt; for long, &lt;code&gt;h&lt;/code&gt; for histogram and &lt;code&gt;s&lt;/code&gt; for string&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./filo-cli  --command decodeVector  --vectorType d --hexVector 0x1b000000080800000300000000000000010000000700000006080400109836&#xA;DoubleLongWrapDataReader$&#xA;3.0,5.0,13.0,15.0,13.0,11.0,9.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Current Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FiloDB Standalone&lt;/td&gt; &#xA;   &lt;td&gt;Stable, tested at scale&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gateway&lt;/td&gt; &#xA;   &lt;td&gt;Experimental&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cassandra&lt;/td&gt; &#xA;   &lt;td&gt;Stable, works with C-2.x and 3.x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kafka&lt;/td&gt; &#xA;   &lt;td&gt;Stable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark&lt;/td&gt; &#xA;   &lt;td&gt;Deprecated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;FiloDB PromQL Support: currently FiloDB supports about 60% of PromQL. We are working to add more support regularly.&lt;/p&gt; &#xA;&lt;h2&gt;Deploying&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt standalone/assembly&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt cli/assembly&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt gateway/assembly&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy and modify &lt;code&gt;conf/timeseries-filodb-server.conf&lt;/code&gt;, deploy it&lt;/li&gt; &#xA; &lt;li&gt;Create a source config. See &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/ingestion.md&#34;&gt;ingestion docs&lt;/a&gt; as well as &lt;code&gt;conf/timeseries-128shards-source.conf&lt;/code&gt; as examples.&lt;/li&gt; &#xA; &lt;li&gt;Run the cli jar as the filo CLI command line tool and initialize keyspaces if using Cassandra: &lt;code&gt;filo-cli-*.jar --command init&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create datasets&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/akka-bootstrapper.md&#34;&gt;Akka Bootstrapper&lt;/a&gt; for different methods of bootstrapping FiloDB clusters&lt;/li&gt; &#xA; &lt;li&gt;Start the gateway server(s)&lt;/li&gt; &#xA; &lt;li&gt;Run the CLI setup command to set up the standalone nodes to start ingesting a dataset from Kafka&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: The setup command only has to be run the first time you start up the standalone servers. After that, the setup is persisted to Cassandra so that on startup, FiloDB nodes will automatically start ingestion from that dataset.&lt;/p&gt; &#xA;&lt;p&gt;Recommended flags:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-XX:MaxInlineLevel=20&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Monitoring and Metrics&lt;/h2&gt; &#xA;&lt;p&gt;FiloDB uses &lt;a href=&#34;http://kamon.io&#34;&gt;Kamon&lt;/a&gt; for metrics and Akka/Futures/async tracing. Not only does this give us summary statistics, but this also gives us Zipkin-style tracing of the ingestion write path in production, which can give a much richer picture than just stats.&lt;/p&gt; &#xA;&lt;h3&gt;Metrics Sinks&lt;/h3&gt; &#xA;&lt;p&gt;Kamon metrics sinks are configured using the config key &lt;code&gt;kamon.reporters&lt;/code&gt;. For an example see &lt;code&gt;conf/timeseries-filodb-server.conf&lt;/code&gt;. Simply list the sinks/reporters to enable in that key. See the Kamon &lt;a href=&#34;https://kamon.io/documentation/1.x/reporters/prometheus/&#34;&gt;docs for Reporters&lt;/a&gt; for more info and config options on each one. Here are some possible values for reporters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;kamon.prometheus.PrometheusReporter&lt;/code&gt; - this exposes a Prometheus read endpoint at port 9095 by default. Easily connect this to a Prometheus server, or feed the metrics back into FiloDB itself or many other sinks using Influx &lt;a href=&#34;https://github.com/influxdata/telegraf&#34;&gt;Telegraf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;kamon.zipkin.ZipkinReporter&lt;/code&gt; - reports trace spans to a Zipkin server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;filodb.coordinator.KamonMetricsLogReporter&lt;/code&gt; - this is part of the coordinator module and will log all metrics (including segment trace metrics) at every Kamon tick interval, which defaults to 10 seconds. Which metrics to log including pattern matching on names can be configured.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;filodb.coordinator.KamonSpanLogReporter&lt;/code&gt; - logs traces. Super useful to debug timing and flow for queries and chunk writes. Logging every trace might be really expensive; you can toggle tracing probability via the &lt;code&gt;kamon.trace&lt;/code&gt; config section - see &lt;code&gt;conf/timeseries-filodb-server.conf&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Metrics Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Kamon has many configurable options. To get more detailed traces on the write / segment append path, for example, here is how you might pass to &lt;code&gt;spark-submit&lt;/code&gt; or &lt;code&gt;spark-shell&lt;/code&gt; options to set detailed tracing on and to trace 3% of all segment appends:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--driver-java-options &#39;-XX:+UseG1GC -XX:MaxGCPauseMillis=500 -Dkamon.trace.level-of-detail=simple-trace -Dkamon.trace.random-sampler.chance=3&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To change the metrics flush interval, you can set &lt;code&gt;kamon.metric.tick-interval&lt;/code&gt; and &lt;code&gt;kamon.statsd.flush-interval&lt;/code&gt;. The statsd flush-interval must be equal to or greater than the tick-interval.&lt;/p&gt; &#xA;&lt;p&gt;Methods of configuring Kamon (except for the metrics logger):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The best way to configure Kamon is to pass this Java property: &lt;code&gt;-Dkamon.config-provider=filodb.coordinator.KamonConfigProvider&lt;/code&gt;. This lets you configure Kamon through the same mechanisms as the rest of FiloDB: &lt;code&gt;-Dfilo.config.file&lt;/code&gt; for example, and the configuration is automatically passed to each executor/worker. Otherwise:&lt;/li&gt; &#xA; &lt;li&gt;Passing Java options on the command line with &lt;code&gt;-D&lt;/code&gt;, or for Spark, &lt;code&gt;--driver-java-options&lt;/code&gt; and &lt;code&gt;--executor-java-options&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Passing options in a config file and using &lt;code&gt;-Dconfig.file&lt;/code&gt;. NOTE: &lt;code&gt;-Dfilo.config.file&lt;/code&gt; will not work because Kamon uses a different initialization stack. Need to be done for both drivers and executors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code Walkthrough&lt;/h2&gt; &#xA;&lt;p&gt;Please go to the &lt;a href=&#34;https://raw.githubusercontent.com/filodb/FiloDB/develop/doc/architecture.md&#34;&gt;architecture&lt;/a&gt; doc.&lt;/p&gt; &#xA;&lt;h2&gt;Building and Testing&lt;/h2&gt; &#xA;&lt;p&gt;Run the tests with &lt;code&gt;sbt test&lt;/code&gt;, or for continuous development, &lt;code&gt;sbt ~test&lt;/code&gt;. Noisy cassandra logs can be seen in &lt;code&gt;filodb-test.log&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The docs use &lt;a href=&#34;https://github.com/knsv/mermaid&#34;&gt;mermaid&lt;/a&gt; and &lt;a href=&#34;https://github.com/thlorenz/doctoc&#34;&gt;doctoc&lt;/a&gt;. On a Mac, to install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install yarn&#xA;yarn global add mermaid.cli&#xA;yarn global add doctoc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multi-JVM tests output a separate log file per process, in the &lt;code&gt;logs&lt;/code&gt; dir under &lt;code&gt;multijvm-nodeN-test.log&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some useful environment vars:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;LOG_AKKA_TO_CONSOLE&lt;/code&gt; - define this to have noisy Akka Cluster logs output to the console&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MAYBE_MULTI_JVM&lt;/code&gt; - enable multi-JVM tests for the Kafka and Standalone modules. These require both Cassandra and Kafka to be up and running.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Debugging serialization and queries&lt;/h3&gt; &#xA;&lt;p&gt;Right now both Java and Kryo serialization are used for Akka messaging. Kryo is used for query result serialization. If there are mysterious hangs, or other potentially serialization-related bugs, here is where to investigate:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;SerializationSpec&lt;/code&gt; in coordinator.client module, especially if changes have been done to the Akka configuration. This test uses the Akka serialization module to ensure settings and serializers work correctly.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;MAYBE_MULTI_JVM&lt;/code&gt; to true and run &lt;code&gt;cassandra/test&lt;/code&gt; and &lt;code&gt;standalone/test&lt;/code&gt;. They test multi-node communication for both ingestion and querying.&lt;/li&gt; &#xA; &lt;li&gt;Set the following log levels to trace queries and serialization in detail: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;filodb.coordinator.queryengine&lt;/code&gt; to DEBUG - especially useful to see how &lt;code&gt;ExecPlans&lt;/code&gt; get distributed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;com.esotericsoftware.minlog&lt;/code&gt; to DEBUG or TRACE -- detailed overall serialization&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;com.esotericsoftware.kryo.io&lt;/code&gt; to TRACE - detailed &lt;code&gt;BinaryRecord&lt;/code&gt; / &lt;code&gt;BinaryVector&lt;/code&gt; serde debugging&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;In particular, enable the above when running the CLI with the standalone FiloDB process to do PromQL queries.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To dynamically change the log level, you can use the &lt;code&gt;/admin/loglevel&lt;/code&gt; HTTP API (per host). Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -d &#39;trace&#39; http://localhost:8080/admin/loglevel/com.esotericsoftware.minlog&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other debugging tips&lt;/h3&gt; &#xA;&lt;p&gt;To debug raw record ingestion and data mismatches:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;trace-filters&lt;/code&gt; source config setting... see &lt;code&gt;timeseries-dev-source.conf&lt;/code&gt; and &lt;code&gt;TimeSeriesShard&lt;/code&gt; &lt;code&gt;tracedPartFilters&lt;/code&gt;. This will log every sample for time series matching criteria set in trace-filters.&lt;/li&gt; &#xA; &lt;li&gt;Use the filtering capability in &lt;code&gt;filodb.kafka.TestConsumer&lt;/code&gt; to print out raw records received from Kafka.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Benchmarking&lt;/h3&gt; &#xA;&lt;p&gt;To run benchmarks, from within SBT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd jmh&#xA;jmh:run -i 5 -wi 5 -f3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Typically, one might run a specific benchmark. This is how we run the query benchmark, with options for inlining and maximizing performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jmh:run -i 15 -wi 10 -f3 -jvmArgsAppend -XX:MaxInlineLevel=20 -jvmArgsAppend -Xmx4g -jvmArgsAppend -XX:MaxInlineSize=99 filodb.jmh.QueryInMemoryBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get the huge variety of JMH options by running &lt;code&gt;jmh:run -help&lt;/code&gt;. For good profiling, there are options such as &lt;code&gt;-prof jmh.extras.JFR&lt;/code&gt; as well as &lt;code&gt;perfasm&lt;/code&gt; / &lt;code&gt;dtraceasm&lt;/code&gt; options. If you would like really good profiling analysis, including memory/heap allocation, I would suggest running one fork with many more iterations, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jmh:run -i 1000 -wi 10 -f1 -jvmArgsAppend -XX:MaxInlineLevel=20 -jvmArgsAppend -Xmx4g -jvmArgsAppend -XX:MaxInlineSize=99 filodb.jmh.QueryInMemoryBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should last a good 15 minutes at least. While it is running, fire up JMC (java Mission Control) and flight record the &#34;jmh.ForkMain&#34; process for 15 minutes. This gives you excellent CPU as well as memory allocation analysis.&lt;/p&gt; &#xA;&lt;p&gt;Another good option is generating a FlameGraph: &lt;code&gt;-prof jmh.extras.Async:dir=/tmp/filodbprofile&lt;/code&gt;. Be sure to read the instructions for setting up FlameGraph profiling. You can also run a stack profiler with an option like &lt;code&gt; -prof stack:lines=4;detailLine=true&lt;/code&gt;, but the analysis is not as good as JMC or Async Profiler/FlameGraph/&lt;/p&gt; &#xA;&lt;p&gt;There is also a script, &lt;code&gt;run_benchmarks.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For running basic continuous profiling in a test environment, a simple profiler can be enabled. It periodically writes a report of the top called methods, as a percentage over a sampling interval. Methods which simply indicate that threads are blocked are excluded. See the profiler section in the filodb-defaults.conf file, and copy this section to a local configuration file.&lt;/p&gt; &#xA;&lt;h3&gt;Gatling Performance Tests&lt;/h3&gt; &#xA;&lt;p&gt;Use this section to setup performance tests locally to compare query throughput and latencies to do query optimization.&lt;/p&gt; &#xA;&lt;p&gt;Setup Cassandra schema&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./scripts/schema-create.sh filodb_admin filodb filodb_downsample promperf 8 1,5 &amp;gt; /tmp/ddl.cql&#xA;cqlsh -f /tmp/ddl.cql&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setup Kafka Topic&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 8 --topic prom-perf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before each clear the cassandra tables to ensure same baseline for apples-to-apples comparison:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./scripts/schema-truncate.sh filodb_admin filodb filodb_downsample promperf 8 1,5 &amp;gt; /tmp/ddl.cql&#xA;cqlsh -f /tmp/ddl.cql&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start FiloDB Perf Server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;java -agentpath:/Applications/YourKit-Java-Profiler-2020.9.app/Contents/Resources/bin/mac/libyjpagent.dylib=port=10001,listen=localhost \&#xA;  -Xmx4G \&#xA;  -Dconfig.file=conf/promperf-filodb-server.conf -Dlogback.configurationFile=conf/logback-perf.xml \&#xA;  -Dkamon.environment.service=filodb-local1 -Dfilodb.cluster-discovery.localhost-ordinal=0 \&#xA;  &amp;lt;classpath&amp;gt; \&#xA;  filodb.standalone.FiloServer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Produce Metrics. Generate 5000 tme series, each with 180 samples, at 60s publish interval for 2 metrics to the FiloDb dataset indicated by &lt;code&gt;prom-perf-source.conf&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;java -Dlogback.configurationFile=conf/logback-dev.xml -Dconfig.file=conf/promperf-filodb-server.conf \&#xA;   -Dkamon.prometheus.embedded-server.port=9097 \&#xA;   &amp;lt;classpath&amp;gt; \&#xA;   filodb.gateway.GatewayServer --gen-gauge-data \&#xA;   -p 5000 -n 180 -i 60 -m 2 conf/promperf-source.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update QueryRangeSimulation.Configuration code with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query Start time by looking at the output of above data generator&lt;/li&gt; &#xA; &lt;li&gt;Select the query you want to run the load test on&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then run &lt;code&gt;GatlingDriver&lt;/code&gt; from your IDE, or run Gatling via SBT with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt gatling/gatling:testOnly filodb.gatling.QueryRangeSimulation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you added profiler arguments to the FiloDB command, you can profile the code when the gatling job is running&lt;/p&gt; &#xA;&lt;h2&gt;You can help!&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome!&lt;/p&gt;</summary>
  </entry>
</feed>