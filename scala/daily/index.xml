<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-19T01:37:38Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>awslabs/spark-sql-kinesis-connector</title>
    <updated>2025-04-19T01:37:38Z</updated>
    <id>tag:github.com,2025-04-19:/awslabs/spark-sql-kinesis-connector</id>
    <link href="https://github.com/awslabs/spark-sql-kinesis-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spark Structured Streaming Kinesis Data Streams connector supports both GetRecords and SubscribeToShard (Enhanced Fan-Out, EFO)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Kinesis Data Streams Connector for Spark Structured Streaming&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of Amazon Kinesis Data Streams connector in Spark Structured Streaming with support to both GetRecords and SubscribeToShard (Enhanced Fan-Out, EFO) consumer types.&lt;/p&gt; &#xA;&lt;h2&gt;Developer Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone SparkSqlKinesisConnector from the source repository on GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/awslabs/spark-sql-kinesis-connector.git&#xA;cd spark-sql-kinesis-connector&#xA;&#xA;mvn clean install -DskipTests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create &lt;code&gt;target/spark-streaming-sql-kinesis-connector_2.12-&amp;lt;kineisis-connector-version&amp;gt;-SNAPSHOT.jar&lt;/code&gt; file which contains the connector and its shaded dependencies. The jar file will also be installed to local maven repository.&lt;/p&gt; &#xA;&lt;p&gt;After the jar file is installed in local Maven repository, configure your project pom.xml (use Maven as an example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-streaming-sql-kinesis-connector_2.12&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;${kinesis-connector-version}&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Current version is tested with Spark 3.2 and above.&lt;/p&gt; &#xA;&lt;h3&gt;Public jar file&lt;/h3&gt; &#xA;&lt;p&gt;For easier access, there is a public jar file available at S3. For example, for version 1.0.0, the file path to jar file is &lt;code&gt;s3://awslabs-code-us-east-1/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.0.0.jar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run with &lt;code&gt;spark-submit&lt;/code&gt;, include the jar file as below (version 1.0.0 as an example)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--jars s3://awslabs-code-us-east-1/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.0.0.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The jar file can also be downloaded at &lt;code&gt;https://awslabs-code-us-east-1.s3.amazonaws.com/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.0.0.jar&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Change the jar file name based on version, e.g. version 1.1.0 is spark-streaming-sql-kinesis-connector_2.12-1.1.0.jar&lt;/p&gt; &#xA;&lt;h2&gt;How to use it&lt;/h2&gt; &#xA;&lt;h3&gt;Code Examples&lt;/h3&gt; &#xA;&lt;h5&gt;Configure Kinesis Source with GetRecords consumer type&lt;/h5&gt; &#xA;&lt;p&gt;Consume data from Kinesis using GetRecords consumer type which is default consumer type.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val kinesis = spark&#xA;              .readStream&#xA;              .format(&#34;aws-kinesis&#34;)&#xA;              .option(&#34;kinesis.region&#34;, &#34;us-east-2&#34;)&#xA;              .option(&#34;kinesis.streamName&#34;, &#34;teststream&#34;)&#xA;              .option(&#34;kinesis.consumerType&#34;, &#34;GetRecords&#34;)&#xA;              .option(&#34;kinesis.endpointUrl&#34;, endpointUrl)&#xA;              .option(&#34;kinesis.startingposition&#34;, &#34;LATEST&#34;)&#xA;              .load&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following policy definition should be added to the IAM role&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;KdsStreamSubscribeToShardPolicy&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#xA;                &#34;kinesis:DescribeStreamSummary&#34;,&#xA;                &#34;kinesis:ListShards&#34;,&#xA;                &#34;kinesis:GetShardIterator&#34;,&#xA;                &#34;kinesis:GetRecords&#34;&#xA;            ],&#xA;            &#34;Resource&#34;: [&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;account-id&amp;gt;:stream/&amp;lt;kinesis-stream-name&amp;gt;&#34;,&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;account-id&amp;gt;:stream/&amp;lt;Kinesis-stream-name&amp;gt;/*&#34;&#xA;            ]&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Configure Kinesis Source with SubscribeToShard consumer type&lt;/h5&gt; &#xA;&lt;p&gt;Consume data from Kinesis using SubscribeToShard(EFO) consumer type (&lt;strong&gt;Please be aware that EFO may incur extra AWS costs&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val kinesis = spark&#xA;              .readStream&#xA;              .format(&#34;aws-kinesis&#34;)&#xA;              .option(&#34;kinesis.region&#34;, &#34;us-east-2&#34;)&#xA;              .option(&#34;kinesis.streamName&#34;, &#34;teststream&#34;)&#xA;              .option(&#34;kinesis.consumerType&#34;, &#34;SubscribeToShard&#34;)&#xA;              .option(&#34;kinesis.endpointUrl&#34;, endpointUrl)&#xA;              .option(&#34;kinesis.startingposition&#34;, &#34;LATEST&#34;)&#xA;              .option(&#34;kinesis.consumerName&#34;, &#34;TestConsumer&#34;)&#xA;              .load()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following policy definition should be added to the IAM role&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;KdsStreamSubscribeToShardPolicy&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#xA;                &#34;kinesis:SubscribeToShard&#34;,&#xA;                &#34;kinesis:DescribeStreamSummary&#34;,&#xA;                &#34;kinesis:ListShards&#34;,&#xA;                &#34;kinesis:DescribeStreamConsumer&#34;,&#xA;                &#34;kinesis:GetShardIterator&#34;,&#xA;                &#34;kinesis:GetRecords&#34;,&#xA;                &#34;kinesis:ListStreamConsumers&#34;,&#xA;                &#34;kinesis:RegisterStreamConsumer&#34;,&#xA;                &#34;kinesis:DeregisterStreamConsumer&#34;&#xA;            ],&#xA;            &#34;Resource&#34;: [&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;account-id&amp;gt;:stream/&amp;lt;kinesis-stream-name&amp;gt;&#34;,&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;account-id&amp;gt;:stream/&amp;lt;Kinesis-stream-name&amp;gt;/*&#34;&#xA;            ]&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Check Schema&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;kinesis.printSchema&#xA;root&#xA;|-- data: binary (nullable = true)&#xA;|-- streamName: string (nullable = true)&#xA;|-- partitionKey: string (nullable = true)&#xA;|-- sequenceNumber: string (nullable = true)&#xA;|-- approximateArrivalTimestamp: timestamp (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Start Query&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt; // Cast data into string and group by data column&#xA; val query = kinesis&#xA;            .selectExpr(&#34;CAST(data AS STRING)&#34;).as[(String)]&#xA;            .groupBy(&#34;data&#34;).count()&#xA;            .writeStream&#xA;            .format(&#34;console&#34;)&#xA;            .outputMode(&#34;complete&#34;) &#xA;            .start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Gracefully Shutdown Query&lt;/h5&gt; &#xA;&lt;p&gt;Below is an example on how to ensure the query is gracefully shutdown before the streaming driver process is stopped.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// add query stop to system shutdown hook&#xA;sys.addShutdownHook {&#xA;  query.stop()&#xA;}&#xA;&#xA;// wait for the signal to stop query&#xA;waitForQueryStop(query, writeToDir)&#xA;&#xA;def waitForQueryStop(query: StreamingQuery, path: String): Unit = {&#xA;    val stopLockPath = new Path(path, &#34;STOP_LOCK&#34;)&#xA;    val fileContext = FileContext.getFileContext(stopLockPath.toUri, new Configuration())&#xA;&#xA;    while (query.isActive) {&#xA;      // Stop the query when &#34;STOP_LOCK&#34; file is found&#xA;      if (fileContext.util().exists(stopLockPath)) {&#xA;        query.stop()&#xA;        fileContext.delete(stopLockPath, false)&#xA;      }&#xA;&#xA;      Thread.sleep(500)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Even gracefully shutdown is implemented, there is no guarantee of consumer deregistration success, especially in the event that an application is terminated brutally.&lt;/p&gt; &#xA;&lt;h5&gt;Using the Kinesis Sink&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Cast data into string and group by data column&#xA;kinesis&#xA;  .selectExpr(&#34;CAST(rand() AS STRING) as partitionKey&#34;,&#34;CAST(data AS STRING)&#34;).as[(String,String)]&#xA;  .groupBy(&#34;data&#34;).count()&#xA;  .writeStream&#xA;  .format(&#34;aws-kinesis&#34;)&#xA;  .outputMode(&#34;append&#34;)&#xA;  .option(&#34;kinesis.region&#34;, &#34;us-east-1&#34;)&#xA;  .option(&#34;kinesis.streamName&#34;, &#34;sparkSinkTest&#34;)&#xA;  .option(&#34;kinesis.endpointUrl&#34;, &#34;https://kinesis.us-east-1.amazonaws.com&#34;)&#xA;  .option(&#34;checkpointLocation&#34;, &#34;/path/to/checkpoint&#34;)&#xA;  .start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Kinesis Connector Metadata storage&lt;/h3&gt; &#xA;&lt;p&gt;By default, Kinesis Connector&#39;s metadata is stored under the same HDFS/S3 folder of checkpoint location .&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to save the metadata in DynamoDB by specifying the options as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;      .option(&#34;kinesis.metadataCommitterType&#34;, &#34;DYNAMODB&#34;)&#xA;      .option(&#34;kinesis.dynamodb.tableName&#34;, &#34;kinesisTestMetadata&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use DynamoDB, following policy definition should be added to the IAM role running the job&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;KDSConnectorAccess&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#xA;                &#34;dynamodb:CreateTable&#34;,&#xA;                &#34;dynamodb:DescribeTable&#34;,&#xA;                &#34;dynamodb:DeleteItem&#34;,&#xA;                &#34;dynamodb:PutItem&#34;,&#xA;                &#34;dynamodb:GetItem&#34;,&#xA;                &#34;dynamodb:Scan&#34;,&#xA;                &#34;dynamodb:Query&#34;&#xA;            ],&#xA;            &#34;Resource&#34;: &#34;arn:aws:dynamodb:&amp;lt;region&amp;gt;:&amp;lt;account-number&amp;gt;:table/&amp;lt;tablename&amp;gt;&#34;&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;EFO Consumer Registration/Deregistration&lt;/h3&gt; &#xA;&lt;p&gt;The Spark application need to provide a &lt;code&gt;kinesis.consumerName&lt;/code&gt; when it is using Kinesis Enhanced Fan Out. &lt;strong&gt;Each application must have a unique stream consumer name.&lt;/strong&gt; Kinesis Connector registers the stream consumer automatically when the application starts. If a consumer with the same &lt;code&gt;kinesis.consumerName&lt;/code&gt; already exists, the connector reuses it.&lt;/p&gt; &#xA;&lt;p&gt;The Stream consumer is deregistered when the application is &lt;strong&gt;shutdown gracefully&lt;/strong&gt; with query &lt;code&gt;stop()&lt;/code&gt; called. There is no guarantee of deregistration success, especially in the event that an application is terminated brutally. The stream consumer will be reused when the application restarts. Note that The stream consumers remain registered may &lt;strong&gt;incur extra AWS costs&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Avoid race conditions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Speculative execution should to be disabled (by default, &lt;em&gt;spark.speculation&lt;/em&gt; is turned off on EMR) to avoid Spark running two tasks for the same shard at the same time which will create race conditions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For the same reason, If two jobs need to read from the same Kinesis stream at the same time, the Spark application should cache the dataframe. Here is an example of caching dataframe in scala. Although &lt;code&gt;batchDF.count&lt;/code&gt; and &lt;code&gt;batchDF.write&lt;/code&gt; will start two jobs, &lt;code&gt;batchDF.persist()&lt;/code&gt; ensures the application will only read from Kinesis stream once. &lt;code&gt;batchDF.unpersist()&lt;/code&gt; releases the cache once the processing is done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val batchProcessor: (DataFrame, Long) =&amp;gt; Unit = (batchDF: DataFrame, batchId: Long) =&amp;gt; {&#xA;      val now = System.currentTimeMillis()&#xA;      val writeToDirNow = s&#34;${writeToDir}/${now}&#34;&#xA;      batchDF.persist()&#xA;      if (batchDF.count() &amp;gt; 0) {&#xA;        batchDF.write&#xA;          .format(&#34;csv&#34;)&#xA;          .mode(SaveMode.Append)&#xA;          .save(writeToDirNow)&#xA;      }&#xA;      batchDF.unpersist()&#xA;    }&#xA;&#xA;    val inputDf = reader.load()&#xA;      .selectExpr(&#34;CAST(data AS STRING)&#34;)&#xA;&#xA;    val query = inputDf&#xA;      .writeStream&#xA;      .queryName(&#34;KinesisDataConsumerForeachBatch&#34;)&#xA;      .foreachBatch {batchProcessor}&#xA;      .option(&#34;checkpointLocation&#34;, checkpointDir)&#xA;      .trigger(Trigger.ProcessingTime(&#34;15 seconds&#34;))&#xA;      .start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same applies if you want to write the output of a streaming query to multiple locations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val query = inputDf&#xA;      .writeStream&#xA;      .foreachBatch { (batchDF: DataFrame, batchId: Long) =&amp;gt;&#xA;        batchDF.persist()&#xA;        batchDF.write.format(...).save(...)  // location 1&#xA;        batchDF.write.format(...).save(...)  // location 2&#xA;        batchDF.unpersist()&#xA;      }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Credential Provider&lt;/h3&gt; &#xA;&lt;p&gt;Kinesis Connector uses the default credentials provider chain to supply credentials that are used in your application.It looks for credentials in this order:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Java System Properties - aws.accessKeyId and aws.secretAccessKey&lt;/li&gt; &#xA; &lt;li&gt;Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY&lt;/li&gt; &#xA; &lt;li&gt;Web Identity Token credentials from system properties or environment variables&lt;/li&gt; &#xA; &lt;li&gt;Credential profiles file at the default location (~/.aws/credentials) shared by all AWS SDKs and the AWS CLI&lt;/li&gt; &#xA; &lt;li&gt;Credentials delivered through the Amazon EC2 container service if AWS_CONTAINER_CREDENTIALS_RELATIVE_URI&#34; environment variable is set and security manager has permission to access the variable,&lt;/li&gt; &#xA; &lt;li&gt;Instance profile credentials delivered through the Amazon EC2 metadata service&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials-chain.html&#34;&gt;AWS SDK for Java 2.x developer Guide&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Cross Account Access using AssumeRole&lt;/h3&gt; &#xA;&lt;p&gt;There are scenarios where customers follow a multi-account approach resulting in Kinesis Data Streams and Spark consumer applications operating in different accounts.&lt;/p&gt; &#xA;&lt;p&gt;The steps to access a Kinesis data stream in one account from a Spark structured streaming application in another account are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 1 – Create AWS Identity and Access Management (IAM) role in Account A to access the Kinesis data stream with trust relationship with Account B.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Attach below policy to the role in Account A&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;KdsStreamPolicy&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#xA;                &#34;kinesis:SubscribeToShard&#34;,&#xA;                &#34;kinesis:DescribeStreamSummary&#34;,&#xA;                &#34;kinesis:ListShards&#34;,&#xA;                &#34;kinesis:DescribeStreamConsumer&#34;,&#xA;                &#34;kinesis:GetShardIterator&#34;,&#xA;                &#34;kinesis:GetRecords&#34;,&#xA;                &#34;kinesis:ListStreamConsumers&#34;,&#xA;                &#34;kinesis:RegisterStreamConsumer&#34;,&#xA;                &#34;kinesis:DeregisterStreamConsumer&#34;&#xA;            ],&#xA;            &#34;Resource&#34;: [&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;AccountA-id&amp;gt;:stream/&amp;lt;SparkConnectorTestStream&amp;gt;&#34;,&#xA;                &#34;arn:aws:kinesis:*:&amp;lt;AccountA-id&amp;gt;:stream/&amp;lt;SparkConnectorTestStream&amp;gt;/*&#34;&#xA;            ]&#xA;        }&#xA;    ]&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Trust policy of the role in Account A&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;&#x9;&#34;Statement&#34;: [&#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;&#34;Sid&#34;: &#34;AccountATrust&#34;,&#xA;&#x9;&#x9;&#x9;&#34;Effect&#34;: &#34;Allow&#34;,&#xA;&#x9;&#x9;&#x9;&#34;Principal&#34;: {&#34;AWS&#34;:[&#34;arn:aws:iam::&amp;lt;AccountB-id&amp;gt;:root&#34;]},&#xA;&#x9;&#x9;&#x9;&#34;Action&#34;: &#34;sts:AssumeRole&#34;&#xA;&#x9;&#x9;}&#xA;&#x9;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 2 – Create IAM role in Account B to assume the role in Account A. This role is used to run the Spark application.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;add below permission&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Sid&#34;: &#34;AssumeRoleInSourceAccount&#34;,&#xA;  &#34;Effect&#34;: &#34;Allow&#34;,&#xA;  &#34;Action&#34;: &#34;sts:AssumeRole&#34;,&#xA;  &#34;Resource&#34;: &#34;&amp;lt;RoleArnInAccountA&amp;gt;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 3 – Configure Kinesis connector to assume the role in Account A to read Kinesis data stream in Account A as below&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  .option(&#34;kinesis.stsRoleArn&#34;, &#34;RoleArnInAccountA&#34;)&#xA;  .option(&#34;kinesis.stsSessionName&#34;, &#34;StsSessionName&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cross Account Access using Access Key&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s also possible to access cross account Kinesis data stream using user&#39;s AWS credentials. The user is in Kinesis account (Account A) and needs to have access permission to Kinesis as above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  .option(&#34;kinesis.awsAccessKeyId&#34;, &#34;awsAccessKeyId&#34;)&#xA;  .option(&#34;kinesis.awsSecretKey&#34;, &#34;awsSecretKey&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Using permanent credentials are not recommended due to security concerns.&lt;/p&gt; &#xA;&lt;h2&gt;Known Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speculative execution is not supported.&lt;/li&gt; &#xA; &lt;li&gt;Trigger.AvailableNow is not supported.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html#continuous-processing&#34;&gt;Continuous Processing&lt;/a&gt; is not supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Kinesis Source Configuration&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.endpointUrl&lt;/td&gt; &#xA;   &lt;td&gt;required, no default value&lt;/td&gt; &#xA;   &lt;td&gt;Endpoint URL for Kinesis Stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.region&lt;/td&gt; &#xA;   &lt;td&gt;inferred value from &lt;code&gt;kinesis.endpointUrl&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Region running the Kinesis connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.streamName&lt;/td&gt; &#xA;   &lt;td&gt;required, no default value&lt;/td&gt; &#xA;   &lt;td&gt;Name of the stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.consumerType&lt;/td&gt; &#xA;   &lt;td&gt;GetRecords&lt;/td&gt; &#xA;   &lt;td&gt;Consumer type. Possible values are &#34;GetRecords&#34;, &#34;SubscribeToShard&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.failOnDataLoss&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;Fail the streaming job if any active shard is missing or expired&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.maxFetchRecordsPerShard&lt;/td&gt; &#xA;   &lt;td&gt;100,000&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of records to fetch per shard per microbatch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.maxFetchTimePerShardSec&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Maximum time in seconds to fetch records per shard per microbatch. If kinesis.maxFetchTimePerShardSec is not explicitly defined, the decision to conclude the current task is based on the value of kinesis.maxFetchRecordsPerShard. However, if kinesis.maxFetchTimePerShardSec is defined, the current task is terminated when either kinesis.maxFetchRecordsPerShard or kinesis.maxFetchTimePerShardSec is reached first. maxFetchTimePerShardSec must be no less than 10 seconds to make sure the fetch can be progressing.&lt;br&gt; Note: If a shard is idle (no new data) for more than 10s, the task terminates even if neither maxFetchTimePerShard nor maxFetchRecordsPerShard reached.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.startingPosition&lt;/td&gt; &#xA;   &lt;td&gt;LATEST&lt;/td&gt; &#xA;   &lt;td&gt;Starting Position in Kinesis to fetch data from. Possible values are &#34;LATEST&#34;, &#34;TRIM_HORIZON&#34;, &#34;EARLIEST&#34; (alias for TRIM_HORIZON), or &#34;AT_TIMESTAMP YYYY-MM-DDTHH:MM:SSZ&#34; (e.g. 2023-08-30T19:00:05Z, 2023-08-30T19:00:05-08:00)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.describeShardInterval&lt;/td&gt; &#xA;   &lt;td&gt;1s&lt;/td&gt; &#xA;   &lt;td&gt;Minimum Interval between two ListShards API calls to get latest shards. Possible values are time values such as 50s, 100ms.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.minBatchesToRetain&lt;/td&gt; &#xA;   &lt;td&gt;same as &lt;code&gt;spark.sql.streaming.minBatchesToRetain&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The minimum number of batches of kinesis metadata that must be retained and made recoverable.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.checkNewRecordThreads&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;Number of threads in Spark driver to check if there are new records in Kinesis stream.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.metadataCommitterType&lt;/td&gt; &#xA;   &lt;td&gt;HDFS&lt;/td&gt; &#xA;   &lt;td&gt;Where to save Kinesis connector metadata. Possible values are &#34;HDFS&#34;, &#34;DYNAMODB&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.metadataPath&lt;/td&gt; &#xA;   &lt;td&gt;Same as &lt;code&gt;checkpointLocation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a path to HDFS or S3. Only valid when &lt;code&gt;kinesis.metadataCommitterType&lt;/code&gt; is HDFS.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.metadataNumRetries&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Maximum Number of retries for metadata requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.metadataRetryIntervalsMs&lt;/td&gt; &#xA;   &lt;td&gt;1000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Wait time before retrying metadata requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.metadataMaxRetryIntervalMs&lt;/td&gt; &#xA;   &lt;td&gt;10000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Max wait time between 2 retries of metadata requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.clientNumRetries&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Maximum Number of retries for Kinesis API requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.clientRetryIntervalsMs&lt;/td&gt; &#xA;   &lt;td&gt;1000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Wait time before retrying Kinesis requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.clientMaxRetryIntervalMs&lt;/td&gt; &#xA;   &lt;td&gt;10000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Max wait time between 2 retries of Kinesis requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.consumerName&lt;/td&gt; &#xA;   &lt;td&gt;Required when &lt;code&gt;kinesis.consumerType&lt;/code&gt; is &#34;SubscribeToShard&#34;&lt;/td&gt; &#xA;   &lt;td&gt;Kinesis stream Enhance Fan Out consumer name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.stsRoleArn&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;AWS STS Role ARN for Kinesis operations describe, read record, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.stsSessionName&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;AWS STS Session name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.stsEndpointUrl&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;AWS STS Endpoint URL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.awsAccessKeyId&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;awsAccessKeyId for Kinesis operations describe, read record, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.awsSecretKey&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;awsSecretKey for Kinesis operations describe, read record, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.kinesisRegion&lt;/td&gt; &#xA;   &lt;td&gt;inferred value from &lt;code&gt;kinesis.endpointUrl&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Region the Kinesis stream belongs to&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.credentialProviderClass&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Custom connector credential provider implements &lt;code&gt;org.apache.spark.sql.connector.kinesis.ConnectorAwsCredentialsProvider&lt;/code&gt;. If the implementation returns temporary credentials, it is responsible for refreshing the credentials before they expire.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.credentialProviderParam&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Custom connector credential provider&#39;s input parameter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.dynamodb.tableName&lt;/td&gt; &#xA;   &lt;td&gt;Required when when &lt;code&gt;kinesis.metadataCommitterType&lt;/code&gt; is &#34;DYNAMODB&#34;&lt;/td&gt; &#xA;   &lt;td&gt;Dynamodb tableName&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.subscribeToShard.timeoutSec&lt;/td&gt; &#xA;   &lt;td&gt;60 (seconds)&lt;/td&gt; &#xA;   &lt;td&gt;Timeout waiting for subscribeToShard finish&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.subscribeToShard.maxRetries&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;Max retries of subscribeToShard request&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.getRecords.numberOfRecordsPerFetch&lt;/td&gt; &#xA;   &lt;td&gt;10,000&lt;/td&gt; &#xA;   &lt;td&gt;Maximum Number of records to fetch per getRecords API call&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.getRecords.fetchIntervalMs&lt;/td&gt; &#xA;   &lt;td&gt;200 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Minimum interval of two getRecords API calls&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Kinesis Sink Configuration&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.endpointUrl&lt;/td&gt; &#xA;   &lt;td&gt;required, no default value&lt;/td&gt; &#xA;   &lt;td&gt;Endpoint URL for Kinesis Stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.region&lt;/td&gt; &#xA;   &lt;td&gt;inferred value from endpoint url&lt;/td&gt; &#xA;   &lt;td&gt;Region running the Kinesis connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.streamName&lt;/td&gt; &#xA;   &lt;td&gt;required, no default value&lt;/td&gt; &#xA;   &lt;td&gt;Name of the stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.sink.flushWaitTimeMs&lt;/td&gt; &#xA;   &lt;td&gt;100 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Wait time while flushing records to Kinesis on Task End&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.sink.recordMaxBufferedTimeMs&lt;/td&gt; &#xA;   &lt;td&gt;1000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Specify the maximum buffered time of a record&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.sink.maxConnections&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Specify the maximum connections to Kinesis&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.sink.aggregationEnabled&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;Specify if records should be aggregated before sending them to Kinesis&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kinesis.sink.recordTtl&lt;/td&gt; &#xA;   &lt;td&gt;30000 (milliseconds)&lt;/td&gt; &#xA;   &lt;td&gt;Records not successfully written to Kinesis within this time are dropped&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/spark-sql-kinesis-connector/main/*CONTRIBUTING.md#security-issue-notifications*&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This connector would not have been possible without reference implemetation of &lt;a href=&#34;https://github.com/qubole/kinesis-sql&#34;&gt;Kinesis Connector&lt;/a&gt;, &lt;a href=&#34;https://github.com/apache/flink-connector-aws&#34;&gt;Apache Flink AWS Connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/aws-samples/spark-streaming-sql-s3-connector&#34;&gt;spark-streaming-sql-s3-connector&lt;/a&gt; and &lt;a href=&#34;https://github.com/awslabs/amazon-kinesis-client&#34;&gt;Kinesis Client Library&lt;/a&gt;. Structure of some part of the code is influenced by the excellent work done by various Apache Spark Contributors.&lt;/p&gt;</summary>
  </entry>
</feed>