<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-12T01:45:02Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>project-sunbird/knowledge-platform-jobs</title>
    <updated>2023-01-12T01:45:02Z</updated>
    <id>tag:github.com,2023-01-12:/project-sunbird/knowledge-platform-jobs</id>
    <link href="https://github.com/project-sunbird/knowledge-platform-jobs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Background and pipeline jobs of Knowledge Platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sunbird-knowledge-platform-jobs&lt;/h1&gt; &#xA;&lt;p&gt;Background and pipeline jobs of Knowledge Platform&lt;/p&gt; &#xA;&lt;h2&gt;Knowledge-platform-jobs local setup&lt;/h2&gt; &#xA;&lt;p&gt;This readme file contains the instruction to set up and run the knowledge-platform-jobs in local machine.&lt;/p&gt; &#xA;&lt;h3&gt;System Requirements:&lt;/h3&gt; &#xA;&lt;h3&gt;Prerequisites:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 11&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prepare folders for database data and logs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir -p ~/sunbird-dbs/neo4j ~/sunbird-dbs/cassandra ~/sunbird-dbs/redis ~/sunbird-dbs/es ~/sunbird-dbs/kafka&#xA;export sunbird_dbs_path=~/sunbird-dbs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Elasticsearch database setup in docker:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --name sunbird_es -d -p 9200:9200 -p 9300:9300 \&#xA;-v $sunbird_dbs_path/es/data:/usr/share/elasticsearch/data \&#xA;-v $sunbird_dbs_path/es/logs://usr/share/elasticsearch/logs \&#xA;-v $sunbird_dbs_path/es/backups:/opt/elasticsearch/backup \&#xA; -e &#34;discovery.type=single-node&#34; docker.elastic.co/elasticsearch/elasticsearch:6.8.22&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--name - Name your container (avoids generic id)&lt;/p&gt; &#xA; &lt;p&gt;-p - Specify container ports to expose&lt;/p&gt; &#xA; &lt;p&gt;Using the -p option with ports 7474 and 7687 allows us to expose and listen for traffic on both the HTTP and Bolt ports. Having the HTTP port means we can connect to our database with Neo4j Browser, and the Bolt port means efficient and type-safe communication requests between other layers and the database.&lt;/p&gt; &#xA; &lt;p&gt;-d - This detaches the container to run in the background, meaning we can access the container separately and see into all of its processes.&lt;/p&gt; &#xA; &lt;p&gt;-v - The next several lines start with the -v option. These lines define volumes we want to bind in our local directory structure so we can access certain files locally.&lt;/p&gt; &#xA; &lt;p&gt;--env - Set config as environment variables for Neo4j database&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Neo4j database setup in docker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, we need to get the neo4j image from docker hub using the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull neo4j:3.3.0 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;We need to create the neo4j instance, By using the below command we can create the same and run in a container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --name sunbird_neo4j -p7474:7474 -p7687:7687 -d \&#xA;    -v $sunbird_dbs_path/neo4j/data:/var/lib/neo4j/data \&#xA;-v $sunbird_dbs_path/neo4j/logs:/var/lib/neo4j/logs \&#xA;-v $sunbird_dbs_path/neo4j/plugins:/var/lib/neo4j/plugins \&#xA;--env NEO4J_dbms_connector_https_advertised__address=&#34;localhost:7473&#34; \&#xA;--env NEO4J_dbms_connector_http_advertised__address=&#34;localhost:7474&#34; \&#xA;--env NEO4J_dbms_connector_bolt_advertised__address=&#34;localhost:7687&#34; \&#xA;--env NEO4J_AUTH=none \&#xA;neo4j:3.3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--name - Name your container (avoids generic id)&lt;/p&gt; &#xA; &lt;p&gt;-p - Specify container ports to expose&lt;/p&gt; &#xA; &lt;p&gt;Using the -p option with ports 7474 and 7687 allows us to expose and listen for traffic on both the HTTP and Bolt ports. Having the HTTP port means we can connect to our database with Neo4j Browser, and the Bolt port means efficient and type-safe communication requests between other layers and the database.&lt;/p&gt; &#xA; &lt;p&gt;-d - This detaches the container to run in the background, meaning we can access the container separately and see into all of its processes.&lt;/p&gt; &#xA; &lt;p&gt;-v - The next several lines start with the -v option. These lines define volumes we want to bind in our local directory structure so we can access certain files locally.&lt;/p&gt; &#xA; &lt;p&gt;--env - Set config as environment variables for Neo4j database&lt;/p&gt; &#xA; &lt;p&gt;Using Docker on Windows will also need a couple of additional configurations because the default 0.0.0.0 address that is resolved with the above command does not translate to localhost in Windows. We need to add environment variables to our command above to set the advertised addresses.&lt;/p&gt; &#xA; &lt;p&gt;By default, Neo4j requires authentication and requires us to first login with neo4j/neo4j and set a new password. We will skip this password reset by initializing the authentication none when we create the Docker container using the --env NEO4J_AUTH=none.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Load seed data to neo4j using the instructions provided in the &lt;a href=&#34;https://raw.githubusercontent.com/project-sunbird/knowledge-platform-jobs/master/master-data/loading-seed-data.md#loading-seed-data-to-neo4j-database&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Verify whether neo4j is running or not by accessing neo4j browser(&lt;a href=&#34;http://localhost:7474/browser&#34;&gt;http://localhost:7474/browser&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To SSH to neo4j docker container, run the below command.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it sunbird_neo4j bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Redis database setup in docker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;we need to get the redis image from docker hub using the below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull redis:6.0.8 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;We need to create the redis instance, By using the below command we can create the same and run in a container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --name sunbird_redis -d -p 6379:6379 redis:6.0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To SSH to redis docker container, run the below command&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it sunbird_redis bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;cassandra database setup in docker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;we need to get the cassandra image and can be done using the below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull cassandra:3.11.8 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;We need to create the cassandra instance, By using the below command we can create the same and run in a container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --name sunbird_cassandra -d -p 9042:9042 \&#xA;-v $sunbird_dbs_path/cassandra/data:/var/lib/cassandra \&#xA;-v $sunbird_dbs_path/cassandra/logs:/opt/cassandra/logs \&#xA;-v $sunbird_dbs_path/cassandra/backups:/mnt/backups \&#xA;--network bridge cassandra:3.11.8 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For network, we can use the existing network or create a new network using the following command and use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker network create sunbird_db_network&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To start cassandra cypher shell run the below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it sunbird_cassandra cqlsh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;To ssh to cassandra docker container, run the below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it sunbird_cassandra /bin/bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Load seed data to cassandra using the instructions provided in the &lt;a href=&#34;https://raw.githubusercontent.com/project-sunbird/knowledge-platform-jobs/master/master-data/loading-seed-data.md#loading-seed-data-to-cassandra-database&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running kafka using docker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Kafka stores information about the cluster and consumers into Zookeeper. ZooKeeper acts as a coordinator between them. we need to run two services(zookeeper &amp;amp; kafka), Prepare your docker-compose.yml file using the following reference.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;version: &#39;3&#39;&#xA;&#xA;services:&#xA;  zookeeper:&#xA;    image: &#39;wurstmeister/zookeeper:latest&#39;&#xA;    container_name: zookeeper&#xA;    ports:&#xA;      - &#34;2181:2181&#34;    &#xA;    environment:&#xA;      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:2181     &#xA;    &#xA;  kafka:&#xA;    image: &#39;wurstmeister/kafka:2.11-1.0.1&#39;&#xA;    container_name: kafka&#xA;    ports:&#xA;      - &#34;9092:9092&#34;&#xA;    environment:&#xA;      - KAFKA_BROKER_ID=1&#xA;      - KAFKA_LISTENERS=PLAINTEXT://:9092&#xA;      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092&#xA;      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181      &#xA;      - ALLOW_PLAINTEXT_LISTENER=yes&#xA;    depends_on:&#xA;      - zookeeper  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Go to the path where docker-compose.yml placed and run the below command to create and run the containers (zookeeper &amp;amp; kafka).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker-compose -f docker-compose.yml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To start kafka docker container shell, run the below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it kafka sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to path /opt/kafka/bin, where we will have executable files to perform operations(creating topics, running producers and consumers, etc). Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test_topic &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Steps to start a job in debug or development mode using IntelliJ:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to downloaded repository folder and run below command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mvn clean install -DskipTests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Open the project in IntelliJ.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the target job folder (Example: ../knowledge-platform-jobs/publish-pipeline/content-publish) and edit the &#39;pom.xml&#39; to add below dependency.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;flink-clients_${scala.version}&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;${flink.version}&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Comment &#34;provided&#34; scope from flink-streaming-scala_${scala.version} artifact dependency in the job&#39;s &#39;pom.xml&#39;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;flink-streaming-scala_${scala.version}&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;${flink.version}&amp;lt;/version&amp;gt;&#xA;&amp;lt;!--            &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Comment the default flink StreamExecutionEnvironment in the job&#39;s StreamTask file (Example: ContentPublishStreamTask.scala) and add code to create local StreamExecutionEnvironment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;//    implicit val env: StreamExecutionEnvironment = FlinkUtil.getExecutionContext(config)&#xA;      implicit val env: StreamExecutionEnvironment = StreamExecutionEnvironment.createLocalEnvironment()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Save cloud storage related environment variables in StreamTask environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Start all databases, zookeper and kafka containers in docker&lt;/li&gt; &#xA; &lt;li&gt;Run the StreamTask (Normal or Debug)&lt;/li&gt; &#xA; &lt;li&gt;Open a terminal, connect to kafka docker container and produce the target job topic.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it kafka_container_id sh&#xA;kafka-console-producer.sh --broker-list kafka:9092 --topic sunbirddev.publish.job.request&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Steps for running jobs in Flink locally:-&lt;/h2&gt; &#xA;&lt;h3&gt;Running flink :&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the Apache flink&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://dlcdn.apache.org/flink/flink-1.12.7/flink-1.12.7-bin-scala_2.12.tgz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Extract the downloaded folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tar xzf flink-1.12.7-bin-scala_2.12.tgz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Change the directory &amp;amp; Start the flink cluster.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd flink-1.12.7&#xA;./bin/start-cluster.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Open web view to check jobmanager and taskmanager&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;localhost:8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setting up Cloud storage connection:&lt;/h3&gt; &#xA;&lt;p&gt;Setup cloud storage specific variables as environment variables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export cloud_storage_type=  #values can be &#39;aws&#39; or &#39;azure&#39;&#xA;&#xA;For AWS Cloud Storage connectivity: &#xA;export aws_storage_key=&#xA;export aws_storage_secret=&#xA;export aws_storage_container=&#xA;&#xA;For Azure Cloud Storage connectivity:&#xA;export azure_storage_key=&#xA;export azure_storage_secret=&#xA;export azure_storage_container=&#xA;&#xA;export content_youtube_apikey= #key to fetch metadata of youtube videos&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running job in Flink:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to the required job folder (Example: ../knowledge-platform-jobs/publish-pipeline/content-publish) and run the below maven command to build the application.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mvn clean install -DskipTests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start all databases, zookeper and kafka containers in docker&lt;/li&gt; &#xA; &lt;li&gt;Start flink (if not started) and submit the job to flink. Example:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd flink-1.12.7&#xA;./bin/start-cluster.sh&#xA;./bin/flink run -m localhost:8081 /user/test/workspace/knowledge-platform-jobs/publish-pipeline/content-publish/target/content-publish-1.0.0.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Open a terminal, connect to kafka docker container and produce the target job topic.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker exec -it kafka_container_id sh&#xA;kafka-console-producer.sh --broker-list kafka:9092 --topic sunbirddev.publish.job.request&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ovotech/natchez-extras</title>
    <updated>2023-01-12T01:45:02Z</updated>
    <id>tag:github.com,2023-01-12:/ovotech/natchez-extras</id>
    <link href="https://github.com/ovotech/natchez-extras" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Integrations between Natchez, Doobie, HTTP4s, Log4cats and Datadog. Formerly called effect-utils.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Natchez Extras&lt;/h1&gt; &#xA;&lt;p&gt;This repository consists of a number of additional integrations for &lt;a href=&#34;https://github.com/tpolecat/natchez&#34;&gt;Natchez&lt;/a&gt;, primarily to assist with integrating Natchez &amp;amp; Datadog. Separate to the Natchez integrations but included here for simplicity is a module to send metrics to Datadog over UDP with FS2.&lt;/p&gt; &#xA;&lt;h2&gt;Cats Effect versions&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re using Cats Effect 2.x you should use versions 4.x.x of this library, for Cats Effect 3 use versions 5.0.0 and up.&lt;/p&gt; &#xA;&lt;h2&gt;http4s dependencies&lt;/h2&gt; &#xA;&lt;p&gt;http4s currently has a stable release (&lt;code&gt;0.23.*&lt;/code&gt;) and a milestone release (&lt;code&gt;1.0.0-M*&lt;/code&gt;). Originally the modules within this library were moved to depend on &lt;code&gt;1.0.0-M*&lt;/code&gt; when ported to Cats Effect 3 but the milestone releases don&#39;t guarantee binary compatibility. (See issue #66).&lt;/p&gt; &#xA;&lt;p&gt;As such there are two versions of natchez-extras-datadog and natchez-extras-http4s:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;natchez-extras-datadog-stable&lt;/code&gt; / &lt;code&gt;natchez-extras-http4s-stable&lt;/code&gt; which depend on http4s &lt;code&gt;0.23.*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;natchez-extras-datadog&lt;/code&gt; / &lt;code&gt;natchez-extras-http4s&lt;/code&gt; which depend on http4s &lt;code&gt;1.0.0-M*&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is recommended that you use the stable variants to avoid binary compatibility issues when upgrading http4s but the unstable versions will continue to exist just in case they&#39;re relied upon.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from effect-utils&lt;/h2&gt; &#xA;&lt;p&gt;For historical reasons prior to 4.0.0 this repository was called &lt;code&gt;effect-utils&lt;/code&gt;. If you&#39;re upgrading your dependencies the renamings are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;com.ovoenergy.effect&#34; % &#34;datadog-metrics&#34;  =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-dogstatsd&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-datadog&#34;  =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-datadog-stable&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-doobie&#34;   =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-doobie&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-slf4j&#34;    =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-slf4j&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-combine&#34;  =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-combine&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-fs2&#34;      =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-fs2&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-testkit&#34;  =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-testkit&#34;&#xA;&#34;com.ovoenergy.effect&#34; % &#34;natchez-http4s&#34;   =&amp;gt; &#34;com.ovoenergy&#34; % &#34;natchez-extras-http4s-stable&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other significant changes are the &lt;code&gt;Datadog&lt;/code&gt; metrics object being renamed to &lt;code&gt;Dogstatsd&lt;/code&gt; and the modules having their code moved into a subpackage under &lt;code&gt;com.ovoenergy.natchez.extras&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.ovoenergy.effect.Combine // effect-utils&#xA;import com.ovoenergy.natchez.extras.combine.Combine // natchez-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is to ensure that each module has an isolated package and so can define, for example, a &lt;code&gt;syntax&lt;/code&gt; object without affecting anything else.&lt;/p&gt; &#xA;&lt;h2&gt;Current modules&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/&#34;&gt;Dogstatsd&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-dogstatsd/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module allows you to send Metrics and Events to the Datadog agent over UDP with FS2.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-datadog.html&#34;&gt;Datadog&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-datadog-stable/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module integrates Natchez with Datadog. It uses HTTP4s and does not depend on the Java Datadog library.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-doobie.html&#34;&gt;Doobie&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-doobie/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module integrates Natchez with Doobie so you can trace which DB queries are being run and for how long.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-http4s.html&#34;&gt;HTTP4S&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-http4s-stable/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module integrates Natchez with the HTTP4S client and provides middleware to trace both inbound and outbound HTTP requests. It aims to be as configurable as possible so can be configured for use with tracing platforms other than Datadog.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-slf4j.html&#34;&gt;Sl4fj&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-slf4j/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module provides an &lt;code&gt;Slf4j&lt;/code&gt; integration with Natchez that logs whenever spans get started or completed. This is mainly useful when running applications locally or integrating with existing logging platforms.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-combine.html&#34;&gt;Combine&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-combine/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module allows two Natchez &lt;code&gt;EntryPoint&lt;/code&gt;s to be combined so that they&#39;ll both be used. For example if you want to log spans with the above Slf4j integration as well as submitting them to Datadog.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-fs2.html&#34;&gt;FS2&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-fs2/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module provides an &lt;code&gt;AllocatedSpan&lt;/code&gt; that can be manually submitted, for use in FS2 streams where the &lt;code&gt;Resource&lt;/code&gt; based model of Natchez isn&#39;t a good fit if you want to have one trace per stream item.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-testkit.html&#34;&gt;Testkit&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-testkit/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module provides a &lt;code&gt;TestEntrypoint&lt;/code&gt; backed by a &lt;code&gt;Ref&lt;/code&gt; which can be useful in unit tests.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ovotech.github.io/natchez-extras/docs/natchez-log4cats.html&#34;&gt;Log4cats&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://index.scala-lang.org/ovotech/natchez-extras/natchez-extras-log4cats/latest.svg?sanitize=true&#34; alt=&#34;latest version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This module provides a &lt;code&gt;TracedLogger&lt;/code&gt; for &lt;code&gt;log4cats&lt;/code&gt; that will automatically add trace &amp;amp; span IDs to your log lines so that they&#39;re linked in the Datadog UI.&lt;/p&gt; &#xA;&lt;h2&gt;Notes for maintainers&lt;/h2&gt; &#xA;&lt;p&gt;To create a release, push a tag to master of the format &lt;code&gt;x.y.z&lt;/code&gt;. See the &lt;a href=&#34;https://semver.org/&#34;&gt;semantic versioning guide&lt;/a&gt; for details of how to choose a version number.&lt;/p&gt;</summary>
  </entry>
</feed>