<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-16T01:35:22Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>projectglow/glow</title>
    <updated>2024-02-16T01:35:22Z</updated>
    <id>tag:github.com,2024-02-16:/projectglow/glow</id>
    <link href="https://github.com/projectglow/glow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source toolkit for large-scale genomic analysis&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/projectglow/glow/main/static/glow_logo_horiz_color.png&#34; width=&#34;300px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; An open-source toolkit for large-scale genomic analyes &lt;br&gt; &lt;a href=&#34;https://glow.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;strong&gt;Explore the docs »&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/projectglow/glow/issues&#34;&gt;Issues&lt;/a&gt; · &lt;a href=&#34;https://groups.google.com/forum/#!forum/proj-glow&#34;&gt;Mailing list&lt;/a&gt; · &lt;a href=&#34;https://join.slack.com/t/proj-glow/shared_invite/enQtNzkwNDE4MzMwMTk5LTE2M2JiMjQ1ZDgyYWNkZTFiY2QyYWE0NGI2YWY3ODY3NmEwNmU5OGQzODcxMDBlYzY2YmYzOGM1YTcyYTRhYjA&#34;&gt;Slack&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Glow is an open-source toolkit to enable bioinformatics at biobank-scale and beyond.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/projectglow/glow&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/projectglow/glow.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://glow.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/glow/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/projectglow/databricks-glow&#34;&gt;&lt;img src=&#34;https://raster.shields.io/docker/pulls/projectglow/databricks-glow.svg?sanitize=true&#34; alt=&#34;Docker Hub Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/glow.py/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/glow.py.svg?sanitize=true&#34; alt=&#34;PyPi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mvnrepository.com/artifact/io.projectglow&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/io.projectglow/glow-spark3_2.12.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/projectglow/glow&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/projectglow/glow/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/glow&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/glow.svg?sanitize=true&#34; alt=&#34;Conda Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/212904926&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/212904926.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Easy to get started&lt;/h1&gt; &#xA;&lt;p&gt;The toolkit includes building blocks to perform common analyses right away:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load VCF, BGEN, and Plink files into distributed DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Perform quality control and data manipulation with built-in functions&lt;/li&gt; &#xA; &lt;li&gt;Variant normalization and liftOver&lt;/li&gt; &#xA; &lt;li&gt;Perform genome-wide association studies&lt;/li&gt; &#xA; &lt;li&gt;Integrate with Spark ML libraries for population stratification&lt;/li&gt; &#xA; &lt;li&gt;Parallelize command line tools to scale existing workflows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Built to scale&lt;/h1&gt; &#xA;&lt;p&gt;Glow makes genomic data work with Spark, the leading engine for working with large structured datasets. It fits natively into the ecosystem of tools that have enabled thousands of organizations to scale their workflows. Glow bridges the gap between bioinformatics and the Spark ecosystem.&lt;/p&gt; &#xA;&lt;h1&gt;Flexible&lt;/h1&gt; &#xA;&lt;p&gt;Glow works with datasets in common file formats like VCF, BGEN, and Plink as well as high-performance big data standards. You can write queries using the native Spark SQL APIs in Python, SQL, R, Java, and Scala. The same APIs allow you to bring your genomic data together with other datasets such as electronic health records, real world evidence, and medical images. Glow makes it easy to parallelize existing tools and libraries implemented as command line tools or Pandas functions.&lt;/p&gt; &#xA;&lt;h1&gt;Building and Testing&lt;/h1&gt; &#xA;&lt;p&gt;This project is built using &lt;a href=&#34;https://www.scala-sbt.org/1.0/docs/Setup.html&#34;&gt;sbt&lt;/a&gt; and Java 8.&lt;/p&gt; &#xA;&lt;p&gt;To build and run Glow, you must &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;install conda&lt;/a&gt; and activate the environment in &lt;code&gt;python/environment.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f python/environment.yml&#xA;conda activate glow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the environment file changes, you must update the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env update -f python/environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start an sbt shell using the &lt;code&gt;sbt&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;FYI&lt;/strong&gt;: The following SBT projects are built on Spark 3.2.1/Scala 2.12.8 by default. To change the Spark version and Scala version, set the environment variables &lt;code&gt;SPARK_VERSION&lt;/code&gt; and &lt;code&gt;SCALA_VERSION&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To compile the main code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run all Scala tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;core/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test a specific suite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;core/testOnly *VCFDataSourceSuite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run all Python tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These tests will run with the same Spark classpath as the Scala tests.&lt;/p&gt; &#xA;&lt;p&gt;To test a specific Python test file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python/pytest python/test_render_template.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using the &lt;code&gt;pytest&lt;/code&gt; key, all arguments are passed directly to the &lt;a href=&#34;https://docs.pytest.org/en/latest/usage.html&#34;&gt;pytest runner&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run documentation tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docs/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the Scala, Python and documentation tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run Scala tests against the staged Maven artifact with the current stable version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;stagedRelease/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing code on a Databricks cluster&lt;/h2&gt; &#xA;&lt;p&gt;To test your changes on a Databricks cluster, build and install Python and Scala artifacts.&lt;/p&gt; &#xA;&lt;p&gt;To build an uber jar (Glow + dependencies) with your changes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sbt core/assembly&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The uber jar will be at a path like &lt;code&gt;glow/core/target/${scala_version}/${artifact-name}-assembly-${version}-SNAPSHOT.jar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build a wheel with the Python code:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Activate the Glow dev conda environment (&lt;code&gt;conda activate glow&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;python&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The wheel file will be at a path like &lt;code&gt;python/dist/glow.py-${version}-py3-none-any.whl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then &lt;a href=&#34;https://docs.databricks.com/libraries/index.html&#34;&gt;install these libraries on a Databricks cluster&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Tips&lt;/h2&gt; &#xA;&lt;p&gt;If you use IntelliJ, you&#39;ll want to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download library and SBT sources; use SBT shell for imports and build from &lt;a href=&#34;https://www.jetbrains.com/help/idea/sbt.html&#34;&gt;IntelliJ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set up &lt;a href=&#34;https://scalameta.org/scalafmt/docs/installation.html&#34;&gt;scalafmt on save&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run Python unit tests from inside IntelliJ, you must:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the &#34;Terminal&#34; tab in IntelliJ&lt;/li&gt; &#xA; &lt;li&gt;Activate the glow conda environment (&lt;code&gt;conda activate glow&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Start an sbt shell from inside the terminal (&lt;code&gt;sbt&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &#34;sbt shell&#34; tab in IntelliJ will NOT work since it does not use the glow conda environment.&lt;/p&gt; &#xA;&lt;p&gt;To test or testOnly in remote debug mode with IntelliJ IDEA set the remote debug configuration in IntelliJ to &#39;Attach to remote JVM&#39; mode and a specific port number (here the default port number 5005 is used) and then modify the definition of options in groupByHash function in build.sbt to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;val options = ForkOptions().withRunJVMOptions(Vector(&#34;-Xmx1024m&#34;)).withRunJVMOptions(Vector(&#34;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>