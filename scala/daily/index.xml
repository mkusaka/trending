<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-01T01:44:05Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>starlake-ai/starlake</title>
    <updated>2023-10-01T01:44:05Z</updated>
    <id>tag:github.com,2023-10-01:/starlake-ai/starlake</id>
    <link href="https://github.com/starlake-ai/starlake" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Starlake is a Spark Based On Premise and Cloud ELT/ETL Framework for Batch &amp; Stream Processing&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/starlake-ai/starlake/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;a href=&#34;https://scala-steward.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Scala_Steward-helping-blue.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=&#34; alt=&#34;Scala Steward badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/starlake-ai/starlake&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/starlake-ai/starlake/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codacy.com/gh/starlake-ai/starlake/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=starlake-ai/starlake&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/569178d6936842808702e72c30d74643&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://starlake-ai.github.io/starlake/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-passing-green.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/ai.starlake/starlake-spark3_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/ai.starlake/starlake-spark3_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central Starlake Spark 3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://starlakeai.slack.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-join-blue.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;About Starlake&lt;/h1&gt; &#xA;&lt;p&gt;Complete documentation available &lt;a href=&#34;https://starlake-ai.github.io/starlake/index.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is Starlake ?&lt;/h1&gt; &#xA;&lt;p&gt;Starlake is a configuration only Extract, Load and Transform engine. The workflow below is a typical use case :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extract your data as a set of Fixed Position, DSV (Delimiter-separated values) or JSON or XML files&lt;/li&gt; &#xA; &lt;li&gt;Define or infer the structure of each POSITION/DSV/JSON/XML file with a schema using YAML syntax&lt;/li&gt; &#xA; &lt;li&gt;Configure the loading process&lt;/li&gt; &#xA; &lt;li&gt;Start watching your data being available as Tables in your warehouse.&lt;/li&gt; &#xA; &lt;li&gt;Build aggregates using SQL, Jinja and YAML configuration files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You may use Sytarlake for Extract, Load and Transform steps or any combination of these steps.&lt;/p&gt; &#xA;&lt;h2&gt;Data Extraction&lt;/h2&gt; &#xA;&lt;p&gt;Starlake provides a fast way to extract, in full or incrementally, tables from your database.&lt;/p&gt; &#xA;&lt;p&gt;Using parallel load through a JDBC connection and configuring the incremental fields in the schema, you may extract your data incrementally. Once copied to the cloud provider of your choice, the data is available for further processing by the Load and Transform steps.&lt;/p&gt; &#xA;&lt;h2&gt;Data Loading&lt;/h2&gt; &#xA;&lt;p&gt;Usually, data loading is done by writing hand made custom parsers that transform input files into datasets of records.&lt;/p&gt; &#xA;&lt;p&gt;Starlake aims at automating this parsing task by making data loading purely declarative.&lt;/p&gt; &#xA;&lt;p&gt;The major benefits the Starlake data loader bring to your warehouse are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Eliminates manual coding for data loading&lt;/li&gt; &#xA; &lt;li&gt;Assign metadata to each dataset&lt;/li&gt; &#xA; &lt;li&gt;Expose data loading metrics and history&lt;/li&gt; &#xA; &lt;li&gt;Transform text files to strongly typed records without coding&lt;/li&gt; &#xA; &lt;li&gt;Support semantic types by allowing you to set type constraints on the incoming data&lt;/li&gt; &#xA; &lt;li&gt;Apply privacy to specific fields&lt;/li&gt; &#xA; &lt;li&gt;Apply security at load time&lt;/li&gt; &#xA; &lt;li&gt;Preview your data lifecycle and publish in SVG format&lt;/li&gt; &#xA; &lt;li&gt;Support multiple data sources and sinks&lt;/li&gt; &#xA; &lt;li&gt;Starlake is a very, very simple piece of software to administer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Transformation&lt;/h2&gt; &#xA;&lt;p&gt;Simply write standard SQL et describe how you want the result to be stored in a YAML description file. The major benefits Starlake bring to your Data transformation jobs are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Write transformations in regular SQL of python scripts&lt;/li&gt; &#xA; &lt;li&gt;Use Jinja2 to augment your SQL scripts and make them easier to read and maintain&lt;/li&gt; &#xA; &lt;li&gt;Describe where and how the result is stored using YML description files&lt;/li&gt; &#xA; &lt;li&gt;Apply security to the target table&lt;/li&gt; &#xA; &lt;li&gt;Preview your data lifecycle and publish in SVG format&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Starlake Data Pipeline automates the loading and parsing of files and their ingestion into a warehouse where datasets become available as strongly typed records.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/workflow.png&#34; alt=&#34;Complete Starlake Data Pipeline Workflow&#34; title=&#34;Complete Starlake Data Pipeline Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The figure above describes how Starlake implements the &lt;code&gt;Extract Load Transform (ELT)&lt;/code&gt; Data Pipeline steps. Starlake may be used indistinctly for all or any of these steps.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;extract&lt;/code&gt; step allows to export selective data from an existing SQL database to a set of CSV files.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;load&lt;/code&gt; step allows you to load text files, to ingest POSITION/CSV/JSON/XML files as strong typed records stored as parquet files or DWH tables (eq. Google BigQuery) or whatever sink you configured&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;transform&lt;/code&gt; step allows to join loaded data and save them as parquet files, DWH tables or Elasticsearch indices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Load Transform steps support multiple configurations for inputs and outputs as illustrated in the figure below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/anywhere.png&#34; alt=&#34;Anywhere&#34; title=&#34;Anywhere&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Starlake Data Pipeline steps are described below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Landing Area : In this optional step, files with predefined filename patterns are stored on a local filesystem in a predefined folder hierarchy&lt;/li&gt; &#xA; &lt;li&gt;Pending Area : Files associated with a schema are imported into this area.&lt;/li&gt; &#xA; &lt;li&gt;Accepted Area : Pending files are parsed against their schema and records are rejected or accepted and made available in Bigquery/Snowflake/Databricks/Hive/... tables or parquet files in a cloud bucket.&lt;/li&gt; &#xA; &lt;li&gt;Business Area : Tables (Hive / BigQuery / Parquet files / ...) in the working area may be joined to provide a holistic view of the data through the definition of transformations.&lt;/li&gt; &#xA; &lt;li&gt;Data visualization : parquet files / tables may be exposed in data warehouses or elasticsearch indices through an indexing definition&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Input file schemas, ingestion rules, transformation and indexing definitions used in the steps above are all defined in YAML files.&lt;/p&gt; &#xA;&lt;h3&gt;BigQuery Data Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/elt-gcp-bq.png&#34; alt=&#34;Bigquery Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Azure Databricks Data Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/elt-azure-databricks.png&#34; alt=&#34;Azure Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;On Premise Data Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/elt-onpremise.png&#34; alt=&#34;On Premise Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Google Cloud Storage Data Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/starlake-ai/starlake/master/docs/static/img/guide/elt-gcp-gcs.png&#34; alt=&#34;Cloud Storage Workflow&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dhgarrette/low-resource-pos-tagging-2013</title>
    <updated>2023-10-01T01:44:05Z</updated>
    <id>tag:github.com,2023-10-01:/dhgarrette/low-resource-pos-tagging-2013</id>
    <link href="https://github.com/dhgarrette/low-resource-pos-tagging-2013" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ANNOUNCEMENT: New Version Available&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;The code here has been completely rewritten to be significantly cleaner and faster. For the new version, see:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/dhgarrette/low-resource-pos-tagging-2014/&#34;&gt;https://github.com/dhgarrette/low-resource-pos-tagging-2014/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For the old version, see the &lt;a href=&#34;https://github.com/dhgarrette/low-resource-pos-tagging-2013/raw/master/README_old.md&#34;&gt;original readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Dan Garrette (&lt;a href=&#34;mailto:dhg@cs.utexas.edu&#34;&gt;dhg@cs.utexas.edu&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
</feed>