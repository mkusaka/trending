<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-17T01:36:04Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mspnp/azure-databricks-streaming-analytics</title>
    <updated>2024-09-17T01:36:04Z</updated>
    <id>tag:github.com,2024-09-17:/mspnp/azure-databricks-streaming-analytics</id>
    <link href="https://github.com/mspnp/azure-databricks-streaming-analytics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stream processing with Azure Databricks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stream processing with Azure Databricks&lt;/h1&gt; &#xA;&lt;p&gt;This reference architecture shows an end-to-end &lt;a href=&#34;https://docs.microsoft.com/azure/architecture/data-guide/big-data/real-time-processing&#34;&gt;stream processing&lt;/a&gt; pipeline. This type of pipeline has four stages: ingest, process, store, and analysis and reporting. For this reference architecture, the pipeline ingests data from two sources, performs a join on related records from each stream, enriches the result, and calculates an average in real time. The results are stored for further analysis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mspnp/architecture-center/raw/master/docs/reference-architectures/data/images/stream-processing-databricks.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: A taxi company collects data about each taxi trip. For this scenario, we assume there are two separate devices sending data. The taxi has a meter that sends information about each ride â€” the duration, distance, and pickup and dropoff locations. A separate device accepts payments from customers and sends data about fares. To spot ridership trends, the taxi company wants to calculate the average tip per mile driven, in real time, for each neighborhood.&lt;/p&gt; &#xA;&lt;h2&gt;Deploy the solution&lt;/h2&gt; &#xA;&lt;p&gt;A deployment for this reference architecture is available on &lt;a href=&#34;https://github.com/mspnp/azure-databricks-streaming-analytics&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone, fork, or download this GitHub repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; to run the data generator.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest&#34;&gt;Azure CLI 2.0&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://docs.microsoft.com/azure/databricks/dev-tools/cli/&#34;&gt;Databricks CLI&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;From a command prompt, bash prompt, or PowerShell prompt, sign into your Azure account as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;az login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Optional - Install a Java IDE, with the following resources:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;JDK 1.8&lt;/li&gt; &#xA;   &lt;li&gt;Scala SDK 2.12&lt;/li&gt; &#xA;   &lt;li&gt;Maven 3.6.3&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: Instructions are included for building via a docker container if you do not want to install a Java IDE.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Download the New York City taxi and neighborhood data files&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a directory named &lt;code&gt;DataFile&lt;/code&gt; in the root of the cloned Github repository in your local file system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open a web browser and navigate to &lt;a href=&#34;https://uofi.app.box.com/v/NYCtaxidata/folder/2332219935&#34;&gt;https://uofi.app.box.com/v/NYCtaxidata/folder/2332219935&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Download&lt;/strong&gt; button on this page to download a zip file of all the taxi data for that year.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extract the zip file to the &lt;code&gt;DataFile&lt;/code&gt; directory.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: This zip file contains other zip files. Don&#39;t extract the child zip files.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;The directory structure should look like the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/DataFile&#xA;    /FOIL2013&#xA;        trip_data_1.zip&#xA;        trip_data_2.zip&#xA;        trip_data_3.zip&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open a web browser and navigate to &lt;a href=&#34;https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html#ti1400387013&#34;&gt;https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html#ti1400387013&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under the section &lt;strong&gt;County Subdivisions&lt;/strong&gt; click the dropdown an select &lt;strong&gt;New York&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the &lt;strong&gt;cb_2019_36_cousub_500k.zip&lt;/strong&gt; file from your browser&#39;s &lt;strong&gt;downloads&lt;/strong&gt; directory to the &lt;code&gt;DataFile&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deploy the Azure resources&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;From a shell or Windows Command Prompt, run the following command and follow the sign-in prompt:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;az login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the folder named &lt;code&gt;azure&lt;/code&gt; in the GitHub repository directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd azure&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following commands to deploy the Azure resources:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export resourceGroup=&#39;[Resource group name]&#39;&#xA;export resourceLocation=&#39;[Region]&#39;&#xA;export eventHubNamespace=&#39;[Event Hubs namespace name]&#39;&#xA;export databricksWorkspaceName=&#39;[Azure Databricks workspace name]&#39;&#xA;export cosmosDatabaseAccount=&#39;[Cosmos DB database name]&#39;&#xA;export logAnalyticsWorkspaceName=&#39;[Log Analytics workspace name]&#39;&#xA;export logAnalyticsWorkspaceRegion=&#39;[Log Analytics region]&#39;&#xA;&#xA;# Create a resource group&#xA;az group create --name $resourceGroup --location $resourceLocation&#xA;&#xA;# Deploy resources&#xA;az deployment group create --resource-group $resourceGroup \&#xA; --template-file ./deployresources.json --parameters \&#xA; eventHubNamespace=$eventHubNamespace \&#xA;    databricksWorkspaceName=$databricksWorkspaceName \&#xA; cosmosDatabaseAccount=$cosmosDatabaseAccount \&#xA; logAnalyticsWorkspaceName=$logAnalyticsWorkspaceName \&#xA; logAnalyticsWorkspaceRegion=$logAnalyticsWorkspaceRegion&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The output of the deployment is written to the console once complete. Search the output for the following JSON:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;&#34;outputs&#34;: {&#xA;        &#34;cosmosDb&#34;: {&#xA;          &#34;type&#34;: &#34;Object&#34;,&#xA;          &#34;value&#34;: {&#xA;            &#34;hostName&#34;: &amp;lt;value&amp;gt;,&#xA;            &#34;secret&#34;: &amp;lt;value&amp;gt;,&#xA;            &#34;username&#34;: &amp;lt;value&amp;gt;&#xA;          }&#xA;        },&#xA;        &#34;eventHubs&#34;: {&#xA;          &#34;type&#34;: &#34;Object&#34;,&#xA;          &#34;value&#34;: {&#xA;            &#34;taxi-fare-eh&#34;: &amp;lt;value&amp;gt;,&#xA;            &#34;taxi-ride-eh&#34;: &amp;lt;value&amp;gt;&#xA;          }&#xA;        },&#xA;        &#34;logAnalytics&#34;: {&#xA;          &#34;type&#34;: &#34;Object&#34;,&#xA;          &#34;value&#34;: {&#xA;            &#34;secret&#34;: &amp;lt;value&amp;gt;,&#xA;            &#34;workspaceId&#34;: &amp;lt;value&amp;gt;&#xA;          }&#xA;        }&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These values are the secrets that will be added to Databricks secrets in upcoming sections. Keep them secure until you add them in those sections.&lt;/p&gt; &#xA;&lt;h3&gt;Add a Cassandra table to the Cosmos DB Account&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In the Azure portal, navigate to the resource group created in the &lt;strong&gt;deploy the Azure resources&lt;/strong&gt; section above. Click on &lt;strong&gt;Azure Cosmos DB Account&lt;/strong&gt;. Create a table with the Cassandra API.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;overview&lt;/strong&gt; blade, click &lt;strong&gt;add table&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When the &lt;strong&gt;add table&lt;/strong&gt; blade opens, enter &lt;code&gt;newyorktaxi&lt;/code&gt; in the &lt;strong&gt;Keyspace name&lt;/strong&gt; text box.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;enter CQL command to create the table&lt;/strong&gt; section, enter &lt;code&gt;neighborhoodstats&lt;/code&gt; in the text box beside &lt;code&gt;newyorktaxi&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the text box below, enter the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(neighborhood text, window_end timestamp, number_of_rides bigint, total_fare_amount double, total_tip_amount double, average_fare_amount double, average_tip_amount double, primary key(neighborhood, window_end))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;Table throughput&lt;/strong&gt; section confirm that &lt;code&gt;Autoscale&lt;/code&gt; is selected and that value &lt;code&gt;4000&lt;/code&gt; is in the &lt;strong&gt;Table Max RU/s&lt;/strong&gt; text box.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Add the Databricks secrets using the Databricks CLI&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tip: Make sure you have authenticated your Databricks CLI configuration. The simplest method in bash is to run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DATABRICKS_AAD_TOKEN=$(az account get-access-token --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d | jq .accessToken --raw-output)&#xA;databricks configure --aad-token --host &amp;lt;enter Databricks Workspace URL from Portal&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The resource GUID (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) is a fixed value. For other options see &lt;a href=&#34;https://docs.microsoft.com/azure/databricks/dev-tools/cli/#--set-up-authentication&#34;&gt;Set up authentication&lt;/a&gt; in the Azure Databricks documentation. If you see a JSONDecodeError error when running a command, your token has exired and you can refresh by running the commands above again.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, enter the secrets for EventHub:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Using the &lt;strong&gt;Azure Databricks CLI&lt;/strong&gt; installed in step 4 of the prerequisites, create the Azure Databricks secret scope:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks secrets create-scope --scope &#34;azure-databricks-job&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the secret for the taxi ride EventHub:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks secrets put --scope &#34;azure-databricks-job&#34; --key &#34;taxi-ride&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once executed, this command opens the vi editor. Enter the &lt;strong&gt;taxi-ride-eh&lt;/strong&gt; value from the &lt;strong&gt;eventHubs&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section. Save and exit vi (if in edit mode hit ESC, then type &#34;:wq&#34;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the secret for the taxi fare EventHub:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks secrets put --scope &#34;azure-databricks-job&#34; --key &#34;taxi-fare&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once executed, this command opens the vi editor. Enter the &lt;strong&gt;taxi-fare-eh&lt;/strong&gt; value from the &lt;strong&gt;eventHubs&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section. Save and exit vi (if in edit mode hit ESC, then type &#34;:wq&#34;).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Next, enter the secrets for Cosmos DB:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Using the &lt;strong&gt;Azure Databricks CLI&lt;/strong&gt;, add the secret for the Cosmos DB user name:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks secrets put --scope azure-databricks-job --key &#34;cassandra-username&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once executed, this command opens the vi editor. Enter the &lt;strong&gt;username&lt;/strong&gt; value from the &lt;strong&gt;CosmosDb&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section. Save and exit vi (if in edit mode hit ESC, then type &#34;:wq&#34;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Next, add the secret for the Cosmos DB password:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks secrets put --scope azure-databricks-job --key &#34;cassandra-password&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once executed, this command opens the vi editor. Enter the &lt;strong&gt;secret&lt;/strong&gt; value from the &lt;strong&gt;CosmosDb&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section. Save and exit vi (if in edit mode hit ESC, then type &#34;:wq&#34;).&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: If using an &lt;a href=&#34;https://docs.azuredatabricks.net/user-guide/secrets/secret-scopes.html#azure-key-vault-backed-scopes&#34;&gt;Azure Key Vault-backed secret scope&lt;/a&gt;, the scope must be named &lt;strong&gt;azure-databricks-job&lt;/strong&gt; and the secrets must have the exact same names as those above.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Add the Census Neighborhoods data file to the Databricks file system&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a directory in the Databricks file system:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dbfs mkdirs dbfs:/azure-databricks-job&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the DataFile folder and enter the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dbfs cp cb_2020_36_cousub_500k.zip dbfs:/azure-databricks-job/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: The filename may change if you obtain a shapefile for a different year.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Build the .jar files for the Databricks job&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;To build the jars using a docker container from a bash prompt change to the &lt;strong&gt;azure&lt;/strong&gt; directory and run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -v `pwd`:/streaming_azuredatabricks_azure -v ~/.m2:/root/.m2 maven:3.6.3-jdk-8 mvn -f /streaming_azuredatabricks_azure/pom.xml package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: Alternately, use your Java IDE to import the Maven project file named &lt;strong&gt;pom.xml&lt;/strong&gt; located in the &lt;strong&gt;azure&lt;/strong&gt; directory. Perform a clean build.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The outputs of the build is a file named &lt;strong&gt;azure-databricks-job-1.0-SNAPSHOT.jar&lt;/strong&gt; in the &lt;strong&gt;./AzureDataBricksJob/target&lt;/strong&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Create a Databricks cluster&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In the Databricks workspace, click &lt;strong&gt;Compute&lt;/strong&gt;, then click &lt;strong&gt;Create cluster&lt;/strong&gt;. Enter the cluster name you created in step 3 of the &lt;strong&gt;configure custom logging for the Databricks job&lt;/strong&gt; section above.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Standard&lt;/strong&gt; for &lt;strong&gt;Cluster Mode&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;strong&gt;Databricks runtime version&lt;/strong&gt; to &lt;strong&gt;7.3 Extended Support (Scala 2.12, Apache Spark 3.0.1)&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deselect &lt;strong&gt;Enable autoscaling&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;strong&gt;Worker Type&lt;/strong&gt; to &lt;strong&gt;Standard_DS3_v2&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;strong&gt;Workers&lt;/strong&gt; to &lt;strong&gt;2&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;strong&gt;Driver Type&lt;/strong&gt; to &lt;strong&gt;Same as worker&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Optional - Configure Azure Log Analytics&lt;/h4&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;Follow the instructions in &lt;a href=&#34;https://github.com/mspnp/spark-monitoring&#34;&gt;Monitoring Azure Databricks&lt;/a&gt; to build the monitoring library and upload the resulting library files to your workspace.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Click on &lt;strong&gt;Advanced Options&lt;/strong&gt; then &lt;strong&gt;Init Scripts&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Enter &lt;strong&gt;dbfs:/databricks/spark-monitoring/spark-monitoring.sh&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Add&lt;/strong&gt; button.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Create Cluster&lt;/strong&gt; button.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install dependent libraries on cluster&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In the Databricks user interface, click on the &lt;strong&gt;home&lt;/strong&gt; button.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on &lt;strong&gt;Compute&lt;/strong&gt; in the navigtation menu on the left then click on the cluster you created in the &lt;strong&gt;Create a Databricks cluster&lt;/strong&gt; step.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on &lt;strong&gt;Libraries&lt;/strong&gt;, then click &lt;strong&gt;Install New&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;Library Source&lt;/strong&gt; control, select &lt;strong&gt;Maven&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under the &lt;strong&gt;Maven Coordinates&lt;/strong&gt; text box, enter &lt;code&gt;com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Install&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Repeat steps 3 - 6 for the &lt;code&gt;com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.0.1&lt;/code&gt; Maven coordinate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Repeat steps 3 - 5 for the &lt;code&gt;org.geotools:gt-shapefile:23.0&lt;/code&gt; Maven coordinate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enter &lt;code&gt;https://repo.osgeo.org/repository/release/&lt;/code&gt; in the &lt;strong&gt;Repository&lt;/strong&gt; text box.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Create a Databricks job&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the &lt;strong&gt;azure-databricks-job-1.0-SNAPSHOT.jar&lt;/strong&gt; file to the Databricks file system by entering the following command in the &lt;strong&gt;Databricks CLI&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;databricks fs cp --overwrite AzureDataBricksJob/target/azure-databricks-job-1.0-SNAPSHOT.jar dbfs:/azure-databricks-job/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the Databricks workspace, click &#34;Jobs&#34;, &#34;create job&#34;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enter a job name.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;Task&lt;/strong&gt; area, change &lt;strong&gt;Type&lt;/strong&gt; to &lt;code&gt;JAR&lt;/code&gt; and Enter &lt;code&gt;com.microsoft.pnp.TaxiCabReader&lt;/code&gt; in the &lt;strong&gt;Main Class&lt;/strong&gt; field.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under &lt;strong&gt;Dependent Libraries&lt;/strong&gt; click &lt;strong&gt;Add&lt;/strong&gt;, this opens the &lt;strong&gt;Add dependent library&lt;/strong&gt; dialog box.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change &lt;strong&gt;Library Source&lt;/strong&gt; to &lt;strong&gt;DBFS/ADLS&lt;/strong&gt;, confirm that Library Type is &lt;strong&gt;Jar&lt;/strong&gt; and enter &lt;code&gt;dbfs:/azure-databricks-job/azure-databricks-job-1.0-SNAPSHOT.jar&lt;/code&gt; in the &lt;strong&gt;File Path&lt;/strong&gt; text box and select &lt;strong&gt;Add&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;Parameters&lt;/strong&gt; field, enter the following (replace &lt;strong&gt;&amp;lt;Cosmos DB Cassandra host name&amp;gt;&lt;/strong&gt; with a value from above):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;[&#34;-n&#34;,&#34;jar:file:/dbfs/azure-databricks-job/cb_2020_36_cousub_500k.zip!/cb_2020_36_cousub_500k.shp&#34;,&#34;--taxi-ride-consumer-group&#34;,&#34;taxi-ride-eh-cg&#34;,&#34;--taxi-fare-consumer-group&#34;,&#34;taxi-fare-eh-cg&#34;,&#34;--window-interval&#34;,&#34;1 hour&#34;,&#34;--cassandra-host&#34;,&#34;&amp;lt;Cosmos DB Cassandra host name&amp;gt;&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under &lt;strong&gt;Cluster&lt;/strong&gt;, click the drop down arrow and select the cluster created the &lt;strong&gt;Create a Databricks cluster&lt;/strong&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click Create&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;Runs&lt;/strong&gt; tab and click &lt;strong&gt;Run Now&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Run the data generator&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the directory &lt;code&gt;onprem&lt;/code&gt; in the GitHub repository.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ../onprem&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update the values in the file &lt;strong&gt;main.env&lt;/strong&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;RIDE_EVENT_HUB=[Connection string for the taxi-ride event hub]&#xA;FARE_EVENT_HUB=[Connection string for the taxi-fare event hub]&#xA;RIDE_DATA_FILE_PATH=/DataFile/FOIL2013&#xA;MINUTES_TO_LEAD=0&#xA;PUSH_RIDE_DATA_FIRST=false&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The connection string for the taxi-ride event hub is the &lt;strong&gt;taxi-ride-eh&lt;/strong&gt; value from the &lt;strong&gt;eventHubs&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section. The connection string for the taxi-fare event hub the &lt;strong&gt;taxi-fare-eh&lt;/strong&gt; value from the &lt;strong&gt;eventHubs&lt;/strong&gt; output section in step 4 of the &lt;em&gt;deploy the Azure resources&lt;/em&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command to build the Docker image.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --no-cache -t dataloader .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate back to the repository root directory.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command to run the Docker image.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v `pwd`/DataFile:/DataFile --env-file=onprem/main.env dataloader:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output should look like the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Created 10000 records for TaxiFare&#xA;Created 10000 records for TaxiRide&#xA;Created 20000 records for TaxiFare&#xA;Created 20000 records for TaxiRide&#xA;Created 30000 records for TaxiFare&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hit CTRL+C to cancel the generation of data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Verify the solution is running&lt;/h3&gt; &#xA;&lt;p&gt;To verify the Databricks job is running correctly, open the Azure portal and navigate to the Cosmos DB database. Open the &lt;strong&gt;Data Explorer&lt;/strong&gt; blade and examine the data in the &lt;strong&gt;neighborhoodstats&lt;/strong&gt; table, you should see results similar to:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;average_fare _amount&lt;/th&gt; &#xA;   &lt;th&gt;average_tip _amount&lt;/th&gt; &#xA;   &lt;th&gt;neighborhood&lt;/th&gt; &#xA;   &lt;th&gt;number_of_rides&lt;/th&gt; &#xA;   &lt;th&gt;total_fare _amount&lt;/th&gt; &#xA;   &lt;th&gt;total_tip _amount&lt;/th&gt; &#xA;   &lt;th&gt;window_end&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10.5&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;Bronx&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;10.5&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1/1/2013 8:02:00 AM +00:00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12.67&lt;/td&gt; &#xA;   &lt;td&gt;2.6&lt;/td&gt; &#xA;   &lt;td&gt;Brooklyn&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;1/1/2013 8:02:00 AM +00:00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14.98&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;Manhattan&lt;/td&gt; &#xA;   &lt;td&gt;52&lt;/td&gt; &#xA;   &lt;td&gt;779&lt;/td&gt; &#xA;   &lt;td&gt;37.83&lt;/td&gt; &#xA;   &lt;td&gt;1/1/2013 8:02:00 AM +00:00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[1] Donovan, Brian; Work, Dan (2016): New York City Taxi Trip Data (2010-2013). University of Illinois at Urbana-Champaign. &lt;a href=&#34;https://doi.org/10.13012/J8PN93H8&#34;&gt;https://doi.org/10.13012/J8PN93H8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>