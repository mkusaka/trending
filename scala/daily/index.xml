<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-02T01:54:13Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gatling/gatling</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/gatling/gatling</id>
    <link href="https://github.com/gatling/gatling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modern Load Testing as Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gatling &lt;a href=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;What is Gatling ?&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is a load test tool. It officially supports HTTP, WebSocket, Server-Sent-Events and JMS.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finding fancy GUIs not that convenient for describing load tests, what you want is a friendly expressive DSL?&lt;/li&gt; &#xA; &lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt; &#xA; &lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Gatling is for you!&lt;/p&gt; &#xA;&lt;h2&gt;Underlying technologies&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is developed in Scala and built upon :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netty.io&#34;&gt;Netty&lt;/a&gt; for non blocking HTTP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://akka.io&#34;&gt;Akka&lt;/a&gt; for virtual users orchestration ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions, help?&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://gatling.io/docs/current/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://groups.google.com/forum/#!forum/gatling&#34;&gt;Gatling User Group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Found a real bug? Raise an &lt;a href=&#34;https://github.com/gatling/gatling/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Partners&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Takima&#34; src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/logo-takima-1-nom-bas.png&#34; width=&#34;80&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/highsoft_logo.png&#34; alt=&#34;Highsoft AS&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>twitter/finagle</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/twitter/finagle</id>
    <link href="https://github.com/twitter/finagle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fault tolerant, protocol-agnostic RPC system&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/twitter/finagle/raw/develop/doc/src/sphinx/_static/logo_medium.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Finagle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/finagle/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/finagle/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/finagle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/finagle/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;https://twitter.github.io/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/finagle/&#34;&gt;https://github.com/twitter/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://finagle.github.io/blog/&#34;&gt;https://finagle.github.io/blog/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols.&lt;/p&gt; &#xA;&lt;p&gt;For extensive documentation, please see the &lt;a href=&#34;https://twitter.github.io/finagle/guide/&#34;&gt;user guide&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/docs/com/twitter/finagle&#34;&gt;API documentation&lt;/a&gt; websites. Documentation improvements are always welcome, so please send patches our way.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The following are a few of the companies that are using Finagle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://foursquare.com/&#34;&gt;Foursquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ing.nl&#34;&gt;ING Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinterest.com/&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://soundcloud.com/&#34;&gt;SoundCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tumblr.com/&#34;&gt;Tumblr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a more complete list, please see &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/ADOPTERS.md&#34;&gt;our adopter page&lt;/a&gt;. If your organization is using Finagle, consider adding a link there and sending us a pull request!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2010 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stripe/bonsai</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/stripe/bonsai</id>
    <link href="https://github.com/stripe/bonsai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Beautiful trees, without the landscaping.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bonsai&lt;/h1&gt; &#xA;&lt;p&gt;Beautiful trees, without the landscaping. Bonsai is a Scala library for transforming arbitrary tree structures into read-only versions that take up a fraction of the space.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Bonsai compresses trees in 2 ways: by using significantly less space to store the tree structure itself (tree compression), and by encoding the node labels in a memory efficient structure (label compression).&lt;/p&gt; &#xA;&lt;h3&gt;What is a &#34;Tree&#34;?&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai works over arbitrary trees, so it assumes a fairly generic interface for interacting with trees. In Bonsai a tree;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;has 0 or 1 root nodes&lt;/li&gt; &#xA; &lt;li&gt;each node has 0 or more children&lt;/li&gt; &#xA; &lt;li&gt;each node has a label attached to it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The actual type of the node is unimportant. What is important is the node labels and the relationships between the nodes (parent, child, sibling, etc). This structure is enough to describe most of the types of trees you are familiar with.&lt;/p&gt; &#xA;&lt;p&gt;Bonsai encodes this notion of trees with the &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/TreeOps.scala&#34;&gt;TreeOps type class&lt;/a&gt;. Here is a truncated version of the type class:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;trait TreeOps[Tree, Label] {&#xA;&#xA;  /** The type of the nodes in the tree. */&#xA;  type Node&#xA;&#xA;  /**&#xA;   * Returns the root node of the tree.&#xA;   */&#xA;  def root(t: Tree): Option[Node]&#xA;&#xA;  /**&#xA;   * Returns all the direct children of the given node. The order may or may&#xA;   * not matter. TreeOps does not provide any guarantees here.&#xA;   */&#xA;  def children(node: Node): Iterable[Node]&#xA;&#xA;  /**&#xA;   * Returns the label attached to the given node.&#xA;   */&#xA;  def label(node: Node): Label&#xA;&#xA;  ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The type &lt;code&gt;T&lt;/code&gt; is our actual tree type. The &lt;code&gt;Node&lt;/code&gt; type is the way we reference internal nodes in our tree &lt;code&gt;T&lt;/code&gt;. The actual type of &lt;code&gt;Node&lt;/code&gt; isn&#39;t important, however, and is mostly an implementation detail. The important bit is the &lt;code&gt;Label&lt;/code&gt; type, which is the user-facing data associated with each node.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;bonsai-example&lt;/code&gt; subproject has &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-example/src/main/scala/com/stripe/bonsai/example/Huffman.scala&#34;&gt;an example of a Huffman tree&lt;/a&gt;. A Huffman tree is used to store a Huffman coding for decoding a compressed message (a bitstring). We decode the bitstring, bit-by-bit, using the tree.&lt;/p&gt; &#xA;&lt;p&gt;Starting at the root of the tree, we follow the left child if the current bit is a 0 and the right child if it is a 1. We continue until reaching a leaf node, at which poitn we output the symbol associated with it, then start back at the beginning of the tree. When we&#39;ve exhausted the entire bitstring, we&#39;ll have our decoded message.&lt;/p&gt; &#xA;&lt;p&gt;Here is how we may implement a Huffman tree in Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sealed trait HuffmanTree[+A]&#xA;case class Branch[+A](zero: HuffmanTree[A], one: HuffmanTree[A]) extends HuffmanTree[A]&#xA;case class Leaf[+A](value: A) extends HuffmanTree[A]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is how we would implement its &lt;code&gt;TreeOps&lt;/code&gt; instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.stripe.bonsai.TreeOps&#xA;&#xA;object HuffmanTree {&#xA;  implicit def huffmanTreeOps[A]: TreeOps[HuffmanTree[A], Option[A]] =&#xA;    new TreeOps[HuffmanTree[A], Option[A]] {&#xA;      type Node = HuffmanTree[A]&#xA;&#xA;      def root(tree: HuffmanTree[A]): Option[HuffmanTree[A]] = Some(tree)&#xA;      def children(tree: HuffmanTree[A]): Iterable[HuffmanTree[A]] = tree match {&#xA;        case Branch(l, r) =&amp;gt; l :: r :: Nil&#xA;        case _ =&amp;gt; Nil&#xA;      }&#xA;      def label(tree: HuffmanTree[A]): Option[A] = tree match {&#xA;        case Leaf(value) =&amp;gt; Some(value)&#xA;        case _ =&amp;gt; None&#xA;      }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As long as we are careful to implement all our operations on a Huffman tree by using its more generic &lt;code&gt;TreeOps&lt;/code&gt; interface, rather than &lt;code&gt;HuffmanTree&lt;/code&gt; directly, we can then swap out the actual tree data structure, without affecting the code using it.&lt;/p&gt; &#xA;&lt;p&gt;For example, below we implement a &lt;code&gt;decode&lt;/code&gt; operation as an implicit class using just &lt;code&gt;TreeOps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import scala.collection.immutable.BitSet&#xA;&#xA;implicit class HuffmanTreeOps[T, A](tree: T)(implicit treeOps: TreeOps[T, Option[A]]) {&#xA;  // Importing treeOps gives us some useful methods on `tree`&#xA;  import treeOps._&#xA;&#xA;  def decode(bits: BitSet, len: Int): Vector[A] = {&#xA;    val root = tree.root.get&#xA;    val (_, result) = (0 until len)&#xA;      .foldLeft((root, Vector.empty[A])) { case ((node, acc), i) =&amp;gt;&#xA;        node.label match {&#xA;          case Some(value) =&amp;gt; (root, acc :+ value)&#xA;          case None if bits(i) =&amp;gt; (node.children.head, acc)&#xA;          case None =&amp;gt; (node.children.iterator.drop(1).next, acc)&#xA;        }&#xA;      }&#xA;    result&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The goal of this indirection through &lt;code&gt;TreeOps&lt;/code&gt; is to let us use a compressed version of the &lt;code&gt;tree&lt;/code&gt; instead of an actual &lt;code&gt;HuffmanTree&lt;/code&gt;, which will see below.&lt;/p&gt; &#xA;&lt;h3&gt;Tree Compression&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai&#39;s tree compression is based off of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Succinct_data_structure&#34; title=&#34;Succinct Data Structures&#34;&gt;succinct data structure&lt;/a&gt; for binary trees. Bonsai supports k-ary trees by first transforming the original tree into a &lt;a href=&#34;https://en.wikipedia.org/wiki/Left-child_right-sibling_binary_tree&#34;&gt;left-child right-sibling tree&lt;/a&gt;, which preserves all the relationships from the original tree, but ensures we have at most 2 children per node. You can read more about the details of the actual compression algorithm used in &lt;a href=&#34;http://www.dcc.uchile.cl/~gnavarro/algoritmos/ps/wea05.pdf&#34;&gt;&#34;Practical Implementation of Rank and Select Queries&#34;&lt;/a&gt;. &lt;strong&gt;The upshot is that we can store the entire structure of a tree in only ~2.73bits per node.&lt;/strong&gt; This replaces the normal strategy of using JVM objects for nodes and references to store the relationships.&lt;/p&gt; &#xA;&lt;p&gt;We actually compress trees by transforming them into &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Tree.scala&#34;&gt;Bonsai &lt;code&gt;Tree&lt;/code&gt;s&lt;/a&gt;. Bonsai&#39;s &lt;code&gt;Tree&lt;/code&gt; constructor takes any arbitrary tree &lt;code&gt;T&lt;/code&gt; that has a &lt;code&gt;TreeOps[T]&lt;/code&gt; available and will return a &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Tree.scala&#34;&gt;&lt;code&gt;Tree&lt;/code&gt;&lt;/a&gt; with the same structure and labels (and &lt;code&gt;Label&lt;/code&gt; type) as the original tree. However, the entire structure and labels of the tree will have been compressed, so this new tree requires significantly less space.&lt;/p&gt; &#xA;&lt;p&gt;In the example in &lt;code&gt;bonsai-example&lt;/code&gt;, we use the Huffman encoding described above to construct a simple Huffman tree for the printable ASCII characters (0x20 -&amp;gt; 0x7E) and compress it using Bonsai&#39;s &lt;code&gt;Tree&lt;/code&gt;. The result is a &lt;strong&gt;11x reduction&lt;/strong&gt; in memory requirements. Since our &lt;code&gt;decode&lt;/code&gt; operation was implemented using &lt;code&gt;TreeOps&lt;/code&gt;, we can use this compressed tree just as we would&#39;ve used the original tree.&lt;/p&gt; &#xA;&lt;p&gt;This example is a bit contrived, since the trees are small to begin with, but you can imagine that applying this to a large random forest yields great results.&lt;/p&gt; &#xA;&lt;h3&gt;Label Compression&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai provides a &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Layout.scala&#34;&gt;Layout&lt;/a&gt; type class, along with some simple combinators, for describing how to (de)serialize your labels. At the lowest level are a set of Layout &#34;primitives&#34; that can encode simple data types into compact data structures. The combinators then allow more complex structures to be described (tuples, &lt;code&gt;Either&lt;/code&gt;, mappings to case classes, etc), without adding much, if any, overhead.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of a &lt;code&gt;Layout&lt;/code&gt; for some &lt;code&gt;Widget&lt;/code&gt; type we made up:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.stripe.bonsai.Layout&#xA;&#xA;sealed trait Widget&#xA;case class Sprocket(radius: Int, weight: Option[Double]) extends Widget&#xA;case class Doodad(length: Int, width: Int, weight: Option[Double]) extends Widget&#xA;&#xA;object Widget {&#xA;  implicit val WidgetLayout: Layout[Widget] = {&#xA;    Layout[Either[(Int, Option[Double]), ((Int, Int), Option[Double])]].transform(&#xA;      {&#xA;        case Left((r, wt)) =&amp;gt; Sprocket(r, wt)&#xA;        case Right(((l, w), wt)) =&amp;gt; Doodad(l, w, wt)&#xA;      },&#xA;      {&#xA;        case Sprocket(r, wt) =&amp;gt; Left((r, wt))&#xA;        case Doodad(l, w, wt) =&amp;gt; Right(((l, w), wt))&#xA;      }&#xA;    )&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can see the &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-example/src/main/scala/com/stripe/bonsai/example/Widget.scala&#34;&gt;full Widget code/example&lt;/a&gt; in the &lt;code&gt;bonsai-example&lt;/code&gt; sub project. In that example, we compress a &lt;code&gt;Vector[Option[Widget]]&lt;/code&gt; using the layout and end up with over a &lt;strong&gt;6x reduction&lt;/strong&gt; in memory requirements.&lt;/p&gt; &#xA;&lt;p&gt;Currently, Bonsai focuses mainly on compressing the overhead of the structure your data requires (eg options, eithers, tuples), rather than the data itself. This will likely change in future releases, and we&#39;ll support better compression for primitive types, as well as things like dictionary encoding for all types.&lt;/p&gt; &#xA;&lt;h1&gt;Using Bonsai in SBT or Maven&lt;/h1&gt; &#xA;&lt;p&gt;Bonsai is published on sonatype. To use it in your SBT project, you can add the following to your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;com.stripe&#34; %% &#34;bonsai&#34; % &#34;0.3.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Miscellaneous&lt;/h1&gt; &#xA;&lt;p&gt;Bonsai is Open Source and available under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;For more help, feel free to contact the authors or create an issue.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/chisel3</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/chipsalliance/chisel3</id>
    <link href="https://github.com/chipsalliance/chisel3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chisel 3: A Modern Hardware Design Language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/chisel_logo.svg?sanitize=true&#34; alt=&#34;Chisel 3&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Upcoming Events&lt;/h2&gt; &#xA;&lt;h3&gt;Chisel Dev Meeting&lt;/h3&gt; &#xA;&lt;p&gt;Chisel/FIRRTL development meetings happen every Monday and Tuesday from 1100--1200 PT.&lt;/p&gt; &#xA;&lt;p&gt;Call-in info and meeting notes are available &lt;a href=&#34;https://docs.google.com/document/d/1BLP2DYt59DqI-FgFCcjw8Ddl4K-WU0nHmQu0sZ_wAGo/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chisel Community Conference 2021, Shanghai, China.&lt;/h3&gt; &#xA;&lt;p&gt;CCC is an annual gathering of Chisel community enthusiasts and technical exchange workshop. This year with the support of the Chisel development community and RISC-V World Conference China 2021 Committee, we have brought together designers and developers with hands-on experience in Chisel from home and abroad to share cutting-edge results and experiences from both the open source community as well as industry.&lt;br&gt; English translated recordings version will be updated soon.&lt;br&gt; Looking forward to CCC 2022! See you then!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/chipsalliance/chisel3.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/chipsalliance/chisel3/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/tag/chipsalliance/chisel3.svg?include_prereleases&amp;amp;sort=semver&#34; alt=&#34;GitHub tag (latest SemVer)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt; is a hardware design language that facilitates &lt;strong&gt;advanced circuit generation and design reuse for both ASIC and FPGA digital logic designs&lt;/strong&gt;. Chisel adds hardware construction primitives to the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala&lt;/a&gt; programming language, providing designers with the power of a modern programming language to write complex, parameterizable circuit generators that produce synthesizable Verilog. This generator methodology enables the creation of re-usable components and libraries, such as the FIFO queue and arbiters in the &lt;a href=&#34;https://www.chisel-lang.org/api/latest/#chisel3.util.package&#34;&gt;Chisel Standard Library&lt;/a&gt;, raising the level of abstraction in design while retaining fine-grained control.&lt;/p&gt; &#xA;&lt;p&gt;For more information on the benefits of Chisel see: &lt;a href=&#34;https://stackoverflow.com/questions/53007782/what-benefits-does-chisel-offer-over-classic-hardware-description-languages&#34;&gt;&#34;What benefits does Chisel offer over classic Hardware Description Languages?&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chisel is powered by &lt;a href=&#34;https://github.com/chipsalliance/firrtl&#34;&gt;FIRRTL (Flexible Intermediate Representation for RTL)&lt;/a&gt;, a hardware compiler framework that performs optimizations of Chisel-generated circuits and supports custom user-defined circuit transformations.&lt;/p&gt; &#xA;&lt;h2&gt;What does Chisel code look like?&lt;/h2&gt; &#xA;&lt;p&gt;Consider an FIR filter that implements a convolution operation, as depicted in this block diagram:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/fir_filter.svg?sanitize=true&#34; width=&#34;512&#34;&gt; &#xA;&lt;p&gt;While Chisel provides similar base primitives as synthesizable Verilog, and &lt;em&gt;could&lt;/em&gt; be used as such:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// 3-point moving sum implemented in the style of a FIR filter&#xA;class MovingSum3(bitWidth: Int) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;&#xA;  val z1 = RegNext(io.in)&#xA;  val z2 = RegNext(z1)&#xA;&#xA;  io.out := (io.in * 1.U) + (z1 * 1.U) + (z2 * 1.U)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the power of Chisel comes from the ability to create generators, such as an FIR filter that is defined by the list of coefficients:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Generalized FIR filter parameterized by the convolution coefficients&#xA;class FirFilter(bitWidth: Int, coeffs: Seq[UInt]) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;  // Create the serial-in, parallel-out shift register&#xA;  val zs = Reg(Vec(coeffs.length, UInt(bitWidth.W)))&#xA;  zs(0) := io.in&#xA;  for (i &amp;lt;- 1 until coeffs.length) {&#xA;    zs(i) := zs(i-1)&#xA;  }&#xA;&#xA;  // Do the multiplies&#xA;  val products = VecInit.tabulate(coeffs.length)(i =&amp;gt; zs(i) * coeffs(i))&#xA;&#xA;  // Sum up the products&#xA;  io.out := products.reduce(_ + _)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use and re-use them across designs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val movingSum3Filter = Module(new FirFilter(8, Seq(1.U, 1.U, 1.U)))  // same 3-point moving sum filter as before&#xA;val delayFilter = Module(new FirFilter(8, Seq(0.U, 1.U)))  // 1-cycle delay as a FIR filter&#xA;val triangleFilter = Module(new FirFilter(8, Seq(1.U, 2.U, 3.U, 2.U, 1.U)))  // 5-point FIR filter with a triangle impulse response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above can be converted to Verilog using &lt;code&gt;ChiselStage&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import chisel3.stage.{ChiselStage, ChiselGeneratorAnnotation}&#xA;&#xA;(new chisel3.stage.ChiselStage).execute(&#xA;  Array(&#34;-X&#34;, &#34;verilog&#34;),&#xA;  Seq(ChiselGeneratorAnnotation(() =&amp;gt; new FirFilter(8, Seq(1.U, 1.U, 1.U)))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you may generate some Verilog directly for inspection:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val verilogString = chisel3.emitVerilog(new FirFilter(8, Seq(0.U, 1.U)))&#xA;println(verilogString)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Bootcamp Interactive Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://mybinder.org/v2/gh/freechipsproject/chisel-bootcamp/master&#34;&gt;&lt;strong&gt;online Chisel Bootcamp&lt;/strong&gt;&lt;/a&gt; is the recommended way to get started with and learn Chisel. &lt;strong&gt;No setup is required&lt;/strong&gt; (it runs in the browser), nor does it assume any prior knowledge of Scala.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/ucb-bar/chisel-tutorial&#34;&gt;&lt;strong&gt;classic Chisel tutorial&lt;/strong&gt;&lt;/a&gt; contains small exercises and runs on your computer.&lt;/p&gt; &#xA;&lt;h3&gt;A Textbook on Chisel&lt;/h3&gt; &#xA;&lt;p&gt;If you like a textbook to learn Chisel and also a bit of digital design in general, you may be interested in reading &lt;a href=&#34;http://www.imm.dtu.dk/~masca/chisel-book.html&#34;&gt;&lt;strong&gt;Digital Design with Chisel&lt;/strong&gt;&lt;/a&gt;. It is available in English, Chinese, Japanese, and Vietnamese.&lt;/p&gt; &#xA;&lt;h3&gt;Build Your Own Chisel Projects&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt; for how to set up your environment to build Chisel locally.&lt;/p&gt; &#xA;&lt;p&gt;When you&#39;re ready to build your own circuits in Chisel, &lt;strong&gt;we recommend starting from the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;Chisel Template&lt;/a&gt; repository&lt;/strong&gt;, which provides a pre-configured project, example design, and testbench. Follow the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template README&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you insist on setting up your own project from scratch, your project needs to depend on both the chisel3-plugin (Scalac plugin) and the chisel3 library. For example, in SBT this could be expressed as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// build.sbt&#xA;scalaVersion := &#34;2.13.7&#34;&#xA;addCompilerPlugin(&#34;edu.berkeley.cs&#34; % &#34;chisel3-plugin&#34; % &#34;3.5.0&#34; cross CrossVersion.full)&#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.5.0&#34;&#xA;// We also recommend using chiseltest for writing unit tests &#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chiseltest&#34; % &#34;0.5.0&#34; % &#34;test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Guide For New Contributors&lt;/h3&gt; &#xA;&lt;p&gt;If you are trying to make a contribution to this project, please read &lt;a href=&#34;https://github.com/Burnleydev1/chisel3/raw/recent_PR/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Design Verification&lt;/h3&gt; &#xA;&lt;p&gt;These simulation-based verification tools are available for Chisel:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-testers&#34;&gt;&lt;strong&gt;iotesters&lt;/strong&gt;&lt;/a&gt;, specifically &lt;a href=&#34;https://github.com/freechipsproject/chisel-testers/wiki/Using%20the%20PeekPokeTester&#34;&gt;PeekPokeTester&lt;/a&gt;, provides constructs (&lt;code&gt;peek&lt;/code&gt;, &lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;expect&lt;/code&gt;) similar to a non-synthesizable Verilog testbench.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/chisel-testers2&#34;&gt;&lt;strong&gt;testers2&lt;/strong&gt;&lt;/a&gt; is an in-development replacement for PeekPokeTester, providing the same base constructs but with a streamlined interface and concurrency support with &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;join&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Useful Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-cheatsheet/releases/latest/download/chisel_cheatsheet.pdf&#34;&gt;&lt;strong&gt;Cheat Sheet&lt;/strong&gt;&lt;/a&gt;, a 2-page reference of the base Chisel syntax and libraries&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org/api/latest/chisel3/index.html&#34;&gt;&lt;strong&gt;ScalaDoc&lt;/strong&gt;&lt;/a&gt;, a listing, description, and examples of the functionality exposed by Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;strong&gt;Gitter&lt;/strong&gt;&lt;/a&gt;, where you can ask questions or discuss anything Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; (&lt;a href=&#34;https://github.com/freechipsproject/www.chisel-lang.org/&#34;&gt;source&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scastie.scala-lang.org/9ga9i2DvQymKlA5JjS1ieA&#34;&gt;&lt;strong&gt;Scastie (3.5.0)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.asic-world.com/verilog/veritut.html&#34;&gt;&lt;strong&gt;asic-world&lt;/strong&gt;&lt;/a&gt; If you aren&#39;t familiar with verilog, this is a good tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are migrating from Chisel2, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/chisel3-vs-chisel2.html&#34;&gt;the migration guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Types Overview&lt;/h3&gt; &#xA;&lt;p&gt;These are the base data types for defining circuit components:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/type_hierarchy.svg?sanitize=true&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributor Documentation&lt;/h2&gt; &#xA;&lt;p&gt;This section describes how to get started contributing to Chisel itself, including how to test your version locally against other projects that pull in Chisel using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Library-Dependencies.html&#34;&gt;sbt&#39;s managed dependencies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Useful Resources for Contributors&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/#useful-resources&#34;&gt;Useful Resources&lt;/a&gt; for users are also helpful for contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/114YihixFBPCfUnv1inqAL8UjsiWfcNWdPHX7SeqlRQc&#34;&gt;&lt;strong&gt;Chisel Breakdown Slides&lt;/strong&gt;&lt;/a&gt;, an introductory talk about Chisel&#39;s internals&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compiling and Testing Chisel&lt;/h3&gt; &#xA;&lt;p&gt;You must first install required dependencies to build Chisel locally, please see &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Clone and build the Chisel library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/chipsalliance/chisel3.git&#xA;cd chisel3&#xA;sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to run the following unit tests, you will need several tools on your &lt;code&gt;PATH&lt;/code&gt;, namely &lt;a href=&#34;https://www.veripool.org/verilator/&#34;&gt;verilator&lt;/a&gt;, &lt;a href=&#34;http://www.clifford.at/yosys/&#34;&gt;yosys&lt;/a&gt;, &lt;a href=&#34;https://github.com/chipsalliance/espresso&#34;&gt;espresso&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Z3Prover/z3&#34;&gt;z3&lt;/a&gt;. Check that each is installed on your &lt;code&gt;PATH&lt;/code&gt; by running &lt;code&gt;which verilator&lt;/code&gt; and so on.&lt;/p&gt; &#xA;&lt;p&gt;If the compilation succeeded and the dependencies noted above are installed, you can then run the included unit tests by invoking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Projects Against Local Chisel&lt;/h3&gt; &#xA;&lt;p&gt;To use the development version of Chisel (&lt;code&gt;master&lt;/code&gt; branch), you will need to build from source and &lt;code&gt;publishLocal&lt;/code&gt;. The repository version can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/build.sbt&#34;&gt;build.sbt&lt;/a&gt; file. As of the time of writing it was:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version := &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To publish your version of Chisel to the local Ivy (sbt&#39;s dependency manager) repository, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The compiled version gets placed in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;. If you need to un-publish your local copy of Chisel, remove the directory generated in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to have your projects use this version of Chisel, you should update the &lt;code&gt;libraryDependencies&lt;/code&gt; setting in your project&#39;s build.sbt file to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building Chisel with FIRRTL in the same SBT Project&lt;/h3&gt; &#xA;&lt;p&gt;While we recommend using the library dependency approach as described above, it is possible to build Chisel and FIRRTL in a single SBT project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This only works for the &#34;main&#34; configuration; you cannot build the Chisel tests this way because &lt;code&gt;treadle&lt;/code&gt; is only supported as a library dependency.&lt;/li&gt; &#xA; &lt;li&gt;Do not &lt;code&gt;publishLocal&lt;/code&gt; when building this way. The published artifact will be missing the FIRRTL dependency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This works by using &lt;a href=&#34;http://eed3si9n.com/hot-source-dependencies-using-sbt-sriracha&#34;&gt;sbt-sriracha&lt;/a&gt;, an SBT plugin for toggling between source and library dependencies. It provides two JVM system properties that, when set, will tell SBT to include FIRRTL as a source project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.sourcemode&lt;/code&gt; - when set to true, SBT will look for FIRRTL in the workspace&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.workspace&lt;/code&gt; - sets the root directory of the workspace&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From root of this repo&#xA;git clone git@github.com:chipsalliance/firrtl.git&#xA;sbt -Dsbt.sourcemode=true -Dsbt.workspace=$PWD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is primarily useful for building projects that themselves want to include Chisel as a source dependency. As an example, see &lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;Rocket Chip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chisel3 Architecture Overview&lt;/h3&gt; &#xA;&lt;p&gt;The Chisel3 compiler consists of these main parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The frontend&lt;/strong&gt;, &lt;code&gt;chisel3.*&lt;/code&gt;, which is the publicly visible &#34;API&#34; of Chisel and what is used in Chisel RTL. These just add data to the...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Builder&lt;/strong&gt;, &lt;code&gt;chisel3.internal.Builder&lt;/code&gt;, which maintains global state (like the currently open Module) and contains commands, generating...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The intermediate data structures&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.*&lt;/code&gt;, which are syntactically very similar to Firrtl. Once the entire circuit has been elaborated, the top-level object (a &lt;code&gt;Circuit&lt;/code&gt;) is then passed to...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Firrtl emitter&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.Emitter&lt;/code&gt;, which turns the intermediate data structures into a string that can be written out into a Firrtl file for further processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also included is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The standard library&lt;/strong&gt; of circuit generators, &lt;code&gt;chisel3.util.*&lt;/code&gt;. These contain commonly used interfaces and constructors (like &lt;code&gt;Decoupled&lt;/code&gt;, which wraps a signal with a ready-valid pair) as well as fully parameterizable circuit generators (like arbiters and multiplexors).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chisel Stage&lt;/strong&gt;, &lt;code&gt;chisel3.stage.*&lt;/code&gt;, which contains compilation and test functions that are invoked in the standard Verilog generation and simulation testing infrastructure. These can also be used as part of custom flows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chisel Sub-Projects&lt;/h3&gt; &#xA;&lt;p&gt;Chisel consists of 4 Scala projects; each is its own separate compilation unit:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/core&#34;&gt;&lt;code&gt;core&lt;/code&gt;&lt;/a&gt; is the bulk of the source code of Chisel, depends on &lt;code&gt;macros&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main&#34;&gt;&lt;code&gt;src/main&lt;/code&gt;&lt;/a&gt; is the &#34;main&#34; that brings it all together and includes a &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main/scala/chisel3/util&#34;&gt;&lt;code&gt;util&lt;/code&gt;&lt;/a&gt; library, which depends on &lt;code&gt;core&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/plugin&#34;&gt;&lt;code&gt;plugin&lt;/code&gt;&lt;/a&gt; is the compiler plugin, no internal dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/macros&#34;&gt;&lt;code&gt;macros&lt;/code&gt;&lt;/a&gt; is most of the macros used in Chisel, no internal dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code that touches lots of APIs that are private to the &lt;code&gt;chisel3&lt;/code&gt; package should belong in &lt;code&gt;core&lt;/code&gt;, while code that is pure Chisel should belong in &lt;code&gt;src/main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Which version should I use?&lt;/h3&gt; &#xA;&lt;p&gt;We encourage Chisel users (as opposed to Chisel developers), to use the latest release version of Chisel. This &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template&lt;/a&gt; repository is kept up-to-date, depending on the most recent version of Chisel. The recommended version is also captured near the top of this README, and in the &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases&#34;&gt;Github releases&lt;/a&gt; section of this repo. If you encounter an issue with a released version of Chisel, please file an issue on GitHub mentioning the Chisel version and provide a simple test case (if possible). Try to reproduce the issue with the associated latest minor release (to verify that the issue hasn&#39;t been addressed).&lt;/p&gt; &#xA;&lt;p&gt;For more information on our versioning policy and what versions of the various Chisel ecosystem projects work together, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&#34;&gt;Chisel Project Versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re developing a Chisel library (or &lt;code&gt;chisel3&lt;/code&gt; itself), you&#39;ll probably want to work closer to the tip of the development trunk. By default, the master branches of the chisel repositories are configured to build and publish their version of the code as &lt;code&gt;Z.Y-SNAPSHOT&lt;/code&gt;. Updated SNAPSHOTs are publised on every push to master. You are encouraged to do your development against the latest SNAPSHOT, but note that neither API nor ABI compatibility is guaranteed so your code may break at any time.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark™&lt;/a&gt;: This connector allows Apache Spark™ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table’s metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/sql-spark-connector</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/microsoft/sql-spark-connector</id>
    <link href="https://github.com/microsoft/sql-spark-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark Connector for SQL Server and Azure SQL&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/sql-spark-connector/master/sql-spark-connector-icon.svg?sanitize=true&#34; alt=&#34;Apache Spark Connector for SQL Server and Azure SQL&#34; width=&#34;150&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Apache Spark Connector for SQL Server and Azure SQL&lt;/h1&gt; &#xA;&lt;p&gt;Born out of Microsoft’s SQL Server Big Data Clusters investments, the Apache Spark Connector for SQL Server and Azure SQL is a high-performance connector that enables you to use transactional data in big data analytics and persists results for ad-hoc queries or reporting. The connector allows you to use any SQL database, on-premises or in the cloud, as an input data source or output data sink for Spark jobs.&lt;/p&gt; &#xA;&lt;p&gt;This library contains the source code for the Apache Spark Connector for SQL Server and Azure SQL.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; is a unified analytics engine for large-scale data processing.&lt;/p&gt; &#xA;&lt;p&gt;There are three version sets of the connector available through Maven, a 2.4.x, a 3.0.x and a 3.1.x compatible version. All versions can be found &lt;a href=&#34;https://search.maven.org/search?q=spark-mssql-connector&#34;&gt;here&lt;/a&gt; and can be imported using the coordinates below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Maven Coordinate&lt;/th&gt; &#xA;   &lt;th&gt;Scala Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4.x compatible connnector&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;com.microsoft.azure:spark-mssql-connector:1.0.2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 3.0.x compatible connnector&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;com.microsoft.azure:spark-mssql-connector_2.12:1.1.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 3.1.x compatible connnector&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;com.microsoft.azure:spark-mssql-connector_2.12:1.2.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Current Releases&lt;/h2&gt; &#xA;&lt;p&gt;The latest Spark 2.4.x compatible connector is on v1.0.2.&lt;/p&gt; &#xA;&lt;p&gt;The latest Spark 3.0.x compatible connector is on v1.1.0.&lt;/p&gt; &#xA;&lt;p&gt;The latest Spark 3.1.x compatible connector is on v1.2.0.&lt;/p&gt; &#xA;&lt;p&gt;For main changes from previous releases and known issues please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/sql-spark-connector/master/docs/CHANGELIST.md&#34;&gt;CHANGELIST&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for all Spark bindings (Scala, Python, R)&lt;/li&gt; &#xA; &lt;li&gt;Basic authentication and Active Directory (AD) Key Tab support&lt;/li&gt; &#xA; &lt;li&gt;Reordered DataFrame write support&lt;/li&gt; &#xA; &lt;li&gt;Support for write to SQL Server Single instance and Data Pool in SQL Server Big Data Clusters&lt;/li&gt; &#xA; &lt;li&gt;Reliable connector support for Sql Server Single Instance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Versions Supported&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apache Spark&lt;/td&gt; &#xA;   &lt;td&gt;2.4.x, 3.0.x, 3.1.x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Microsoft JDBC Driver for SQL Server&lt;/td&gt; &#xA;   &lt;td&gt;8.4.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Microsoft SQL Server&lt;/td&gt; &#xA;   &lt;td&gt;SQL Server 2008 or later&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Azure SQL Databases&lt;/td&gt; &#xA;   &lt;td&gt;Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Azure Synapse (Azure SQL DW) use is not tested with this connector. While it may work, there may be unintended consequences.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Supported Options&lt;/h3&gt; &#xA;&lt;p&gt;The Apache Spark Connector for SQL Server and Azure SQL supports the options defined here: &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html&#34;&gt;SQL DataSource JDBC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition following options are supported&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;reliabilityLevel&lt;/td&gt; &#xA;   &lt;td&gt;&#34;BEST_EFFORT&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&#34;BEST_EFFORT&#34; or &#34;NO_DUPLICATES&#34;. &#34;NO_DUPLICATES&#34; implements an reliable insert in executor restart scenarios&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dataPoolDataSource&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;none implies the value is not set and the connector should write to SQl Server Single Instance. Set this value to data source name to write a Data Pool Table in Big Data Cluster&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;isolationLevel&lt;/td&gt; &#xA;   &lt;td&gt;&#34;READ_COMMITTED&#34;&lt;/td&gt; &#xA;   &lt;td&gt;Specify the isolation level&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tableLock&lt;/td&gt; &#xA;   &lt;td&gt;&#34;false&#34;&lt;/td&gt; &#xA;   &lt;td&gt;Implements an insert with TABLOCK option to improve write performance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;schemaCheckEnabled&lt;/td&gt; &#xA;   &lt;td&gt;&#34;true&#34;&lt;/td&gt; &#xA;   &lt;td&gt;Disables strict dataframe and sql table schema check when set to false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Other &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/connect/jdbc/using-bulk-copy-with-the-jdbc-driver?view=sql-server-2017#sqlserverbulkcopyoptions&#34;&gt;Bulk api options&lt;/a&gt; can be set as options on the dataframe and will be passed to bulkcopy apis on write&lt;/p&gt; &#xA;&lt;h2&gt;Performance comparison&lt;/h2&gt; &#xA;&lt;p&gt;Apache Spark Connector for SQL Server and Azure SQL is up to 15x faster than generic JDBC connector for writing to SQL Server. Note performance characteristics vary on type, volume of data, options used and may show run to run variations. The following performance results are the time taken to overwrite a sql table with 143.9M rows in a spark dataframe. The spark dataframe is constructed by reading store_sales HDFS table generated using &lt;a href=&#34;https://github.com/databricks/spark-sql-perf&#34;&gt;spark TPCDS Benchmark&lt;/a&gt;. Time to read store_sales to dataframe is excluded. The results are averaged over 3 runs. &lt;em&gt;Note: The following results were achieved using the Apache Spark 2.4.5 compatible connector. These numbers are not a guarantee of performance.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector Type&lt;/th&gt; &#xA;   &lt;th&gt;Options&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Time to write&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JDBCConnector&lt;/td&gt; &#xA;   &lt;td&gt;Default&lt;/td&gt; &#xA;   &lt;td&gt;Generic JDBC connector with default options&lt;/td&gt; &#xA;   &lt;td&gt;1385s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sql-spark-connector&lt;/td&gt; &#xA;   &lt;td&gt;BEST_EFFORT&lt;/td&gt; &#xA;   &lt;td&gt;Best effort sql-spark-connector with default options&lt;/td&gt; &#xA;   &lt;td&gt;580s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sql-spark-connector&lt;/td&gt; &#xA;   &lt;td&gt;NO_DUPLICATES&lt;/td&gt; &#xA;   &lt;td&gt;Reliable sql-spark-connector&lt;/td&gt; &#xA;   &lt;td&gt;709s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sql-spark-connector&lt;/td&gt; &#xA;   &lt;td&gt;BEST_EFFORT + tabLock=true&lt;/td&gt; &#xA;   &lt;td&gt;Best effort sql-spark-connector with table lock enabled&lt;/td&gt; &#xA;   &lt;td&gt;72s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sql-spark-connector&lt;/td&gt; &#xA;   &lt;td&gt;NO_DUPLICATES + tabLock=true&lt;/td&gt; &#xA;   &lt;td&gt;Reliable sql-spark-connector with table lock enabled&lt;/td&gt; &#xA;   &lt;td&gt;198s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Config&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Spark config : &lt;code&gt;num_executors = 20&lt;/code&gt;, &lt;code&gt;executor_memory = &#39;1664m&#39;&lt;/code&gt;, &lt;code&gt;executor_cores = 2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Data Gen config : &lt;code&gt;scale_factor=50&lt;/code&gt;, &lt;code&gt;partitioned_tables=true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Data file Store_sales with number of of rows 143,997,590&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Environment&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/big-data-cluster/release-notes-big-data-cluster?view=sql-server-ver15&#34;&gt;SQL Server Big Data Cluster&lt;/a&gt; CU5&lt;/li&gt; &#xA; &lt;li&gt;Master + 6 nodes&lt;/li&gt; &#xA; &lt;li&gt;Each node gen 5 server, 512GB Ram, 4TB NVM per node, NIC 10GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Commonly Faced Issues&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;java.lang.NoClassDefFoundError: com/microsoft/aad/adal4j/AuthenticationException&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This issue arises from using an older version of the mssql driver (which is now included in this connector) in your hadoop environment. If you are coming from using the previous Azure SQL Connector and have manually installed drivers onto that cluster for AAD compatibility, you will need to remove those drivers.&lt;/p&gt; &#xA;&lt;p&gt;Steps to fix the issue:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you are using a generic Hadoop environment, check and remove the mssql jar: &lt;code&gt;rm $HADOOP_HOME/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar&lt;/code&gt;. If you are using Databricks, add a global or cluster init script to remove old versions of the mssql driver from the &lt;code&gt;/databricks/jars&lt;/code&gt; folder, or add this line to an existing script: &lt;code&gt;rm /databricks/jars/*mssql*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the &lt;code&gt;adal4j&lt;/code&gt; and &lt;code&gt;mssql&lt;/code&gt; packages, I used Maven, but anyway should work. DO NOT install the SQL spark connector this way.&lt;/li&gt; &#xA; &lt;li&gt;Add the driver class to your connection configuration:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;connectionProperties = {&#xA;  &#34;Driver&#34;: &#34;com.microsoft.sqlserver.jdbc.SQLServerDriver&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information and explanation, visit the closed &lt;a href=&#34;https://github.com/microsoft/sql-spark-connector/issues/26&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;The Apache Spark Connector for SQL Server and Azure SQL is based on the Spark DataSourceV1 API and SQL Server Bulk API and uses the same interface as the built-in JDBC Spark-SQL connector. This allows you to easily integrate the connector and migrate your existing Spark jobs by simply updating the format parameter with &lt;code&gt;com.microsoft.sqlserver.jdbc.spark&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To include the connector in your projects download this repository and build the jar using SBT.&lt;/p&gt; &#xA;&lt;h3&gt;Migrating from Legacy Azure SQL Connector for Spark&lt;/h3&gt; &#xA;&lt;h4&gt;Receiving &lt;code&gt;java.lang.NoClassDefFoundError&lt;/code&gt; when trying to use the new connector with Azure Databricks?&lt;/h4&gt; &#xA;&lt;p&gt;If you are migrating from the previous Azure SQL Connector for Spark and have manually installed drivers onto that cluster for AAD compatibility, you will most likely need to remove those custom drivers, restore the previous drivers that ship by default with Databricks, uninstall the previous connector, and restart your cluster. You may be better off spinning up a new cluster.&lt;/p&gt; &#xA;&lt;p&gt;With this new connector, you should be able to simply install onto a cluster (new or existing cluster that hasn&#39;t had its drivers modified) or a cluster which previously used modified drivers for the older Azure SQL Connector for Spark provided the modified drivers were removed and the previous default drivers restored.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/microsoft/sql-spark-connector/issues/26&#34;&gt;Issue #26&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Executing custom SQL through the connector&lt;/h3&gt; &#xA;&lt;p&gt;The previous Azure SQL Connector for Spark provided the ability to execute custom SQL code like DML or DDL statements through the connector. This functionality is out-of-scope of this connector since it is based on the DataSource APIs. This functionality is readily provided by libraries like pyodbc or you can use the standard java sql interfaces as well.&lt;/p&gt; &#xA;&lt;p&gt;You can read the closed issue and view community provided alternatives in &lt;a href=&#34;https://github.com/microsoft/sql-spark-connector/issues/21&#34;&gt;Issue #21&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Write to a new SQL Table&lt;/h3&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;Important: using the &lt;code&gt;overwrite&lt;/code&gt; mode will first DROP the table if it already exists in the database by default. Please use this option with due care to avoid unexpected data loss!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;When using mode &lt;code&gt;overwrite&lt;/code&gt; if you do not use the option &lt;code&gt;truncate&lt;/code&gt;, on recreation of the table indexes will be lost. For example a columnstore table would now be a heap. If you want to maintain existing indexing please also specify option &lt;code&gt;truncate&lt;/code&gt; with value true. i.e &lt;code&gt;.option(&#34;truncate&#34;,true)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;server_name = &#34;jdbc:sqlserver://{SERVER_ADDR}&#34;&#xA;database_name = &#34;database_name&#34;&#xA;url = server_name + &#34;;&#34; + &#34;databaseName=&#34; + database_name + &#34;;&#34;&#xA;&#xA;table_name = &#34;table_name&#34;&#xA;username = &#34;username&#34;&#xA;password = &#34;password123!#&#34; # Please specify password here&#xA;&#xA;try:&#xA;  df.write \&#xA;    .format(&#34;com.microsoft.sqlserver.jdbc.spark&#34;) \&#xA;    .mode(&#34;overwrite&#34;) \&#xA;    .option(&#34;url&#34;, url) \&#xA;    .option(&#34;dbtable&#34;, table_name) \&#xA;    .option(&#34;user&#34;, username) \&#xA;    .option(&#34;password&#34;, password) \&#xA;    .save()&#xA;except ValueError as error :&#xA;    print(&#34;Connector write failed&#34;, error)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Append to SQL Table&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:&#xA;  df.write \&#xA;    .format(&#34;com.microsoft.sqlserver.jdbc.spark&#34;) \&#xA;    .mode(&#34;append&#34;) \&#xA;    .option(&#34;url&#34;, url) \&#xA;    .option(&#34;dbtable&#34;, table_name) \&#xA;    .option(&#34;user&#34;, username) \&#xA;    .option(&#34;password&#34;, password) \&#xA;    .save()&#xA;except ValueError as error :&#xA;    print(&#34;Connector write failed&#34;, error)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Specifying the isolation level&lt;/h3&gt; &#xA;&lt;p&gt;This connector by default uses &lt;code&gt;READ_COMMITTED&lt;/code&gt; isolation level when performing the bulk insert into the database. If you wish to override this to another isolation level, please use the &lt;code&gt;mssqlIsolationLevel&lt;/code&gt; option as shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    .option(&#34;mssqlIsolationLevel&#34;, &#34;READ_UNCOMMITTED&#34;) \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Read from SQL Table&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jdbcDF = spark.read \&#xA;        .format(&#34;com.microsoft.sqlserver.jdbc.spark&#34;) \&#xA;        .option(&#34;url&#34;, url) \&#xA;        .option(&#34;dbtable&#34;, table_name) \&#xA;        .option(&#34;user&#34;, username) \&#xA;        .option(&#34;password&#34;, password).load()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Azure Active Directory Authentication&lt;/h3&gt; &#xA;&lt;h4&gt;Python Example with Service Principal&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;context = adal.AuthenticationContext(authority)&#xA;token = context.acquire_token_with_client_credentials(resource_app_id_url, service_principal_id, service_principal_secret)&#xA;access_token = token[&#34;accessToken&#34;]&#xA;&#xA;jdbc_db = spark.read \&#xA;        .format(&#34;com.microsoft.sqlserver.jdbc.spark&#34;) \&#xA;        .option(&#34;url&#34;, url) \&#xA;        .option(&#34;dbtable&#34;, table_name) \&#xA;        .option(&#34;accessToken&#34;, access_token) \&#xA;        .option(&#34;encrypt&#34;, &#34;true&#34;) \&#xA;        .option(&#34;hostNameInCertificate&#34;, &#34;*.database.windows.net&#34;) \&#xA;        .load()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Python Example with Active Directory Password&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jdbc_df = spark.read \&#xA;        .format(&#34;com.microsoft.sqlserver.jdbc.spark&#34;) \&#xA;        .option(&#34;url&#34;, url) \&#xA;        .option(&#34;dbtable&#34;, table_name) \&#xA;        .option(&#34;authentication&#34;, &#34;ActiveDirectoryPassword&#34;) \&#xA;        .option(&#34;user&#34;, user_name) \&#xA;        .option(&#34;password&#34;, password) \&#xA;        .option(&#34;encrypt&#34;, &#34;true&#34;) \&#xA;        .option(&#34;hostNameInCertificate&#34;, &#34;*.database.windows.net&#34;) \&#xA;        .load()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A required dependency must be installed in order to authenticate using Active Directory.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;Scala,&lt;/strong&gt; the &lt;code&gt;com.microsoft.aad.adal4j&lt;/code&gt; artifact will need to be installed.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;Python,&lt;/strong&gt; the &lt;code&gt;adal&lt;/code&gt; library will need to be installed. This is available via pip.&lt;/p&gt; &#xA;&lt;p&gt;Please check the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/sql-spark-connector/master/samples&#34;&gt;sample notebooks&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;The Apache Spark Connector for Azure SQL and SQL Server is an open source project. This connector does not come with any Microsoft support. For issues with or questions about the connector, please create an Issue in this project repository. The connector community is active and monitoring submissions.&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Visit the Connector project in the &lt;strong&gt;Projects&lt;/strong&gt; tab to see needed / planned items. Feel free to make an issue and start contributing!&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-xml</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/databricks/spark-xml</id>
    <link href="https://github.com/databricks/spark-xml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;XML data source for Spark SQL and DataFrames&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XML Data Source for Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/databricks/spark-xml&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/databricks/spark-xml/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;A library for parsing and querying XML data with &lt;a href=&#34;https://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;, for Spark SQL and DataFrames. The structure and test tools are mostly copied from &lt;a href=&#34;https://github.com/databricks/spark-csv&#34;&gt;CSV Data Source for Spark&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This package supports to process format-free XML files in a distributed way, unlike JSON datasource in Spark restricts in-line JSON format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compatible with Spark 3.0 and later with Scala 2.12, and also Spark 3.2 and later with Scala 2.12 or 2.13. Scala 2.11 and Spark 2 support ended with version 0.13.0.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-xml_2.12&#xA;version: 0.14.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.14.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading XML files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: Location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowTag&lt;/code&gt;: The row tag of your xml files to treat as a row. For example, in this xml &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;book&lt;/code&gt;. Default is &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;samplingRatio&lt;/code&gt;: Sampling ratio for inferring schema (0.0 ~ 1). Default is 1. Possible types are &lt;code&gt;StructType&lt;/code&gt;, &lt;code&gt;ArrayType&lt;/code&gt;, &lt;code&gt;StringType&lt;/code&gt;, &lt;code&gt;LongType&lt;/code&gt;, &lt;code&gt;DoubleType&lt;/code&gt;, &lt;code&gt;BooleanType&lt;/code&gt;, &lt;code&gt;TimestampType&lt;/code&gt; and &lt;code&gt;NullType&lt;/code&gt;, unless user provides a schema for this.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;excludeAttribute&lt;/code&gt; : Whether you want to exclude attributes in elements or not. Default is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;treatEmptyValuesAsNulls&lt;/code&gt; : (DEPRECATED: use &lt;code&gt;nullValue&lt;/code&gt; set to &lt;code&gt;&#34;&#34;&lt;/code&gt;) Whether you want to treat whitespaces as a null value. Default is false&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: The mode for dealing with corrupt records during parsing. Default is &lt;code&gt;PERMISSIVE&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt; : &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;When it encounters a corrupted record, it sets all fields to &lt;code&gt;null&lt;/code&gt; and puts the malformed string into a new field configured by &lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;When it encounters a field of the wrong datatype, it sets the offending field to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt; : ignores the whole corrupted records.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt; : throws an exception when it meets corrupted records.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: if &lt;code&gt;true&lt;/code&gt;, attempts to infer an appropriate type for each resulting DataFrame column, like a boolean, numeric or date type. If &lt;code&gt;false&lt;/code&gt;, all resulting columns are of string type. Default is &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;: The name of new field where malformed strings are stored. Default is &lt;code&gt;_corrupt_record&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;attributePrefix&lt;/code&gt;: The prefix for attributes so that we can differentiate attributes and elements. This will be the prefix for field names. Default is &lt;code&gt;_&lt;/code&gt;. Can be empty, but only for reading XML.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valueTag&lt;/code&gt;: The tag used for the value when there are attributes in the element having no child. Default is &lt;code&gt;_VALUE&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: Defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignoreSurroundingSpaces&lt;/code&gt;: Defines whether or not surrounding whitespaces from values being read should be skipped. Default is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wildcardColName&lt;/code&gt;: Name of a column existing in the provided schema which is interpreted as a &#39;wildcard&#39;. It must have type string or array of strings. It will match any XML child element that is not otherwise matched by the schema. The XML of the child becomes the string value of the column. If an array, then all unmatched elements will be returned as an array of strings. As its name implies, it is meant to emulate XSD&#39;s &lt;code&gt;xs:any&lt;/code&gt; type. Default is &lt;code&gt;xs_any&lt;/code&gt;. New in 0.11.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowValidationXSDPath&lt;/code&gt;: Path to an XSD file that is used to validate the XML for each row individually. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided, or inferred. Note that if the same local path is not already also visible on the executors in the cluster, then the XSD and any others it depends on should be added to the Spark executors with &lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@addFile(path:String):Unit&#34;&gt;&lt;code&gt;SparkContext.addFile&lt;/code&gt;&lt;/a&gt;. In this case, to use local XSD &lt;code&gt;/foo/bar.xsd&lt;/code&gt;, call &lt;code&gt;addFile(&#34;/foo/bar.xsd&#34;)&lt;/code&gt; and pass just &lt;code&gt;&#34;bar.xsd&#34;&lt;/code&gt; as &lt;code&gt;rowValidationXSDPath&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignoreNamespace&lt;/code&gt;: If true, namespaces prefixes on XML elements and attributes are ignored. Tags &lt;code&gt;&amp;lt;abc:author&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;def:author&amp;gt;&lt;/code&gt; would, for example, be treated as if both are just &lt;code&gt;&amp;lt;author&amp;gt;&lt;/code&gt;. Note that, at the moment, namespaces cannot be ignored on the &lt;code&gt;rowTag&lt;/code&gt; element, only its children. Note that XML parsing is in general not namespace-aware even if &lt;code&gt;false&lt;/code&gt;. Defaults to &lt;code&gt;false&lt;/code&gt;. New in 0.11.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timestampFormat&lt;/code&gt;: Specifies an additional timestamp format that will be tried when parsing values as &lt;code&gt;TimestampType&lt;/code&gt; columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to try several formats, including &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_INSTANT&#34;&gt;ISO_INSTANT&lt;/a&gt;, including variations with offset timezones or no timezone (defaults to UTC). New in 0.12.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: Specifies an additional timestamp format that will be tried when parsing values as &lt;code&gt;DateType&lt;/code&gt; columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_DATE&#34;&gt;ISO_DATE&lt;/a&gt;. New in 0.12.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: Location to write files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowTag&lt;/code&gt;: The row tag of your xml files to treat as a row. For example, in &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;book&lt;/code&gt;. Default is &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rootTag&lt;/code&gt;: The root tag of your xml files to treat as the root. For example, in &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;books&lt;/code&gt;. It can include basic attributes by specifying a value like &lt;code&gt;books foo=&#34;bar&#34;&lt;/code&gt; (as of 0.11.0). Default is &lt;code&gt;ROWS&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;declaration&lt;/code&gt;: Content of XML declaration to write at the start of every output XML file, before the &lt;code&gt;rootTag&lt;/code&gt;. For example, a value of &lt;code&gt;foo&lt;/code&gt; causes &lt;code&gt;&amp;lt;?xml foo?&amp;gt;&lt;/code&gt; to be written. Set to empty string to suppress. Defaults to &lt;code&gt;version=&#34;1.0&#34; encoding=&#34;UTF-8&#34; standalone=&#34;yes&#34;&lt;/code&gt;. New in 0.14.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: The value to write &lt;code&gt;null&lt;/code&gt; value. Default is string &lt;code&gt;null&lt;/code&gt;. When this is &lt;code&gt;null&lt;/code&gt;, it does not write attributes and elements for fields.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;attributePrefix&lt;/code&gt;: The prefix for attributes so that we can differentiating attributes and elements. This will be the prefix for field names. Default is &lt;code&gt;_&lt;/code&gt;. Cannot be empty for writing XML.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valueTag&lt;/code&gt;: The tag used for the value when there are attributes in the element having no child. Default is &lt;code&gt;_VALUE&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;compression&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timestampFormat&lt;/code&gt;: Controls the format used to write &lt;code&gt;TimestampType&lt;/code&gt; format columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_INSTANT&#34;&gt;ISO_INSTANT&lt;/a&gt;. New in 0.12.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: Controls the format used to write &lt;code&gt;DateType&lt;/code&gt; format columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_DATE&#34;&gt;ISO_DATE&lt;/a&gt;. New in 0.12.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently it supports the shortened name usage. You can use just &lt;code&gt;xml&lt;/code&gt; instead of &lt;code&gt;com.databricks.spark.xml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;XSD Support&lt;/h3&gt; &#xA;&lt;p&gt;Per above, the XML for individual rows can be validated against an XSD using &lt;code&gt;rowValidationXSDPath&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The utility &lt;code&gt;com.databricks.spark.xml.util.XSDToSchema&lt;/code&gt; can be used to extract a Spark DataFrame schema from &lt;em&gt;some&lt;/em&gt; XSD files. It supports only simple, complex and sequence types, only basic XSD functionality, and is experimental.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.util.XSDToSchema&#xA;import java.nio.file.Paths&#xA;&#xA;val schema = XSDToSchema.read(Paths.get(&#34;/path/to/your.xsd&#34;))&#xA;val df = spark.read.schema(schema)....xml(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parsing Nested XML&lt;/h3&gt; &#xA;&lt;p&gt;Although primarily used to convert (portions of) large XML documents into a &lt;code&gt;DataFrame&lt;/code&gt;, &lt;code&gt;spark-xml&lt;/code&gt; can also parse XML in a string-valued column in an existing DataFrame with &lt;code&gt;from_xml&lt;/code&gt;, in order to add it as a new column with parsed results as a struct.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.functions.from_xml&#xA;import com.databricks.spark.xml.schema_of_xml&#xA;import spark.implicits._&#xA;val df = ... /// DataFrame with XML in column &#39;payload&#39; &#xA;val payloadSchema = schema_of_xml(df.select(&#34;payload&#34;).as[String])&#xA;val parsed = df.withColumn(&#34;parsed&#34;, from_xml($&#34;payload&#34;, payloadSchema))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This can convert arrays of strings containing XML to arrays of parsed structs. Use &lt;code&gt;schema_of_xml_array&lt;/code&gt; instead&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;com.databricks.spark.xml.from_xml_string&lt;/code&gt; is an alternative that operates on a String directly instead of a column, for use in UDFs&lt;/li&gt; &#xA; &lt;li&gt;If you use &lt;code&gt;DROPMALFORMED&lt;/code&gt; mode with &lt;code&gt;from_xml&lt;/code&gt;, then XML values that do not parse correctly will result in a &lt;code&gt;null&lt;/code&gt; value for the column. No rows will be dropped.&lt;/li&gt; &#xA; &lt;li&gt;If you use &lt;code&gt;PERMISSIVE&lt;/code&gt; mode with &lt;code&gt;from_xml&lt;/code&gt; et al, which is the default, then the parse mode will actually instead default to &lt;code&gt;DROPMALFORMED&lt;/code&gt;. If however you include a column in the schema for &lt;code&gt;from_xml&lt;/code&gt; that matches the &lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;, then &lt;code&gt;PERMISSIVE&lt;/code&gt; mode will still output malformed records to that column in the resulting struct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pyspark notes&lt;/h4&gt; &#xA;&lt;p&gt;The functions above are exposed in the Scala API only, at the moment, as there is no separate Python package for &lt;code&gt;spark-xml&lt;/code&gt;. They can be accessed from Pyspark by manually declaring some helper functions that call into the JVM-based API from Python. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql.column import Column, _to_java_column&#xA;from pyspark.sql.types import _parse_datatype_json_string&#xA;&#xA;def ext_from_xml(xml_column, schema, options={}):&#xA;    java_column = _to_java_column(xml_column.cast(&#39;string&#39;))&#xA;    java_schema = spark._jsparkSession.parseDataType(schema.json())&#xA;    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)&#xA;    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(&#xA;        java_column, java_schema, scala_map)&#xA;    return Column(jc)&#xA;&#xA;def ext_schema_of_xml_df(df, options={}):&#xA;    assert len(df.columns) == 1&#xA;&#xA;    scala_options = spark._jvm.PythonUtils.toScalaMap(options)&#xA;    java_xml_module = getattr(getattr(&#xA;        spark._jvm.com.databricks.spark.xml, &#34;package$&#34;), &#34;MODULE$&#34;)&#xA;    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)&#xA;    return _parse_datatype_json_string(java_schema.json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Structure Conversion&lt;/h2&gt; &#xA;&lt;p&gt;Due to the structure differences between &lt;code&gt;DataFrame&lt;/code&gt; and XML, there are some conversion rules from XML data to &lt;code&gt;DataFrame&lt;/code&gt; and from &lt;code&gt;DataFrame&lt;/code&gt; to XML data. Note that handling attributes can be disabled with the option &lt;code&gt;excludeAttribute&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Conversion from XML to &lt;code&gt;DataFrame&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Attributes&lt;/strong&gt;: Attributes are converted as fields with the heading prefix, &lt;code&gt;attributePrefix&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;one myOneAttrib=&#34;AAAA&#34;&amp;gt;&#xA;    &amp;lt;two&amp;gt;two&amp;lt;/two&amp;gt;&#xA;    &amp;lt;three&amp;gt;three&amp;lt;/three&amp;gt;&#xA;&amp;lt;/one&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root&#xA; |-- _myOneAttrib: string (nullable = true)&#xA; |-- two: string (nullable = true)&#xA; |-- three: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Value in an element that has no child elements but attributes&lt;/strong&gt;: The value is put in a separate field, &lt;code&gt;valueTag&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;one&amp;gt;&#xA;    &amp;lt;two myTwoAttrib=&#34;BBBBB&#34;&amp;gt;two&amp;lt;/two&amp;gt;&#xA;    &amp;lt;three&amp;gt;three&amp;lt;/three&amp;gt;&#xA;&amp;lt;/one&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root&#xA; |-- two: struct (nullable = true)&#xA; |    |-- _VALUE: string (nullable = true)&#xA; |    |-- _myTwoAttrib: string (nullable = true)&#xA; |-- three: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Conversion from &lt;code&gt;DataFrame&lt;/code&gt; to XML&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Element as an array in an array&lt;/strong&gt;: Writing a XML file from &lt;code&gt;DataFrame&lt;/code&gt; having a field &lt;code&gt;ArrayType&lt;/code&gt; with its element as &lt;code&gt;ArrayType&lt;/code&gt; would have an additional nested field for the element. This would not happen in reading and writing XML data but writing a &lt;code&gt;DataFrame&lt;/code&gt; read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a &lt;code&gt;DataFrame&lt;/code&gt; read from other sources is possible to have a different structure.&lt;/p&gt; &lt;p&gt;&lt;code&gt;DataFrame&lt;/code&gt; with a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; |-- a: array (nullable = true)&#xA; |    |-- element: array (containsNull = true)&#xA; |    |    |-- element: string (containsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with data below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;+------------------------------------+&#xA;|                                   a|&#xA;+------------------------------------+&#xA;|[WrappedArray(aa), WrappedArray(bb)]|&#xA;+------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a XML file below:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;a&amp;gt;&#xA;    &amp;lt;item&amp;gt;aa&amp;lt;/item&amp;gt;&#xA;&amp;lt;/a&amp;gt;&#xA;&amp;lt;a&amp;gt;&#xA;    &amp;lt;item&amp;gt;bb&amp;lt;/item&amp;gt;&#xA;&amp;lt;/a&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;These examples use a XML file available for download &lt;a href=&#34;https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;XML data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE books&#xA;USING com.databricks.spark.xml&#xA;OPTIONS (path &#34;books.xml&#34;, rowTag &#34;book&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL. In this case, we do not infer schema.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string)&#xA;USING com.databricks.spark.xml&#xA;OPTIONS (path &#34;books.xml&#34;, rowTag &#34;book&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;Import &lt;code&gt;com.databricks.spark.xml._&lt;/code&gt; to get implicits that add the &lt;code&gt;.xml(...)&lt;/code&gt; method to &lt;code&gt;DataFrame&lt;/code&gt;. You can also use &lt;code&gt;.format(&#34;xml&#34;)&lt;/code&gt; and &lt;code&gt;.load(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SparkSession&#xA;import com.databricks.spark.xml._&#xA;&#xA;val spark = SparkSession.builder().getOrCreate()&#xA;val df = spark.read&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;books.xml&#34;)&#xA;&#xA;val selectedData = df.select(&#34;author&#34;, &#34;_id&#34;)&#xA;selectedData.write&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;newbooks.xml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType}&#xA;import com.databricks.spark.xml._&#xA;&#xA;val spark = SparkSession.builder().getOrCreate()&#xA;val customSchema = StructType(Array(&#xA;  StructField(&#34;_id&#34;, StringType, nullable = true),&#xA;  StructField(&#34;author&#34;, StringType, nullable = true),&#xA;  StructField(&#34;description&#34;, StringType, nullable = true),&#xA;  StructField(&#34;genre&#34;, StringType, nullable = true),&#xA;  StructField(&#34;price&#34;, DoubleType, nullable = true),&#xA;  StructField(&#34;publish_date&#34;, StringType, nullable = true),&#xA;  StructField(&#34;title&#34;, StringType, nullable = true)))&#xA;&#xA;&#xA;val df = spark.read&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .schema(customSchema)&#xA;  .xml(&#34;books.xml&#34;)&#xA;&#xA;val selectedData = df.select(&#34;author&#34;, &#34;_id&#34;)&#xA;selectedData.write&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;newbooks.xml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SparkSession;&#xA;&#xA;SparkSession spark = SparkSession.builder().getOrCreate();&#xA;DataFrame df = spark.read()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .load(&#34;books.xml&#34;);&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .save(&#34;newbooks.xml&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SparkSession;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SparkSession spark = SparkSession.builder().getOrCreate();&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;  new StructField(&#34;_id&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;author&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;description&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;genre&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;price&#34;, DataTypes.DoubleType, true, Metadata.empty()),&#xA;  new StructField(&#34;publish_date&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;title&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = spark.read()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .schema(customSchema)&#xA;  .load(&#34;books.xml&#34;);&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .save(&#34;newbooks.xml&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;spark = SparkSession.builder.getOrCreate()&#xA;&#xA;df = spark.read.format(&#39;xml&#39;).options(rowTag=&#39;book&#39;).load(&#39;books.xml&#39;)&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;, rootTag=&#39;books&#39;) \&#xA;    .save(&#39;newbooks.xml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;from pyspark.sql.types import *&#xA;&#xA;spark = SparkSession.builder.getOrCreate()&#xA;customSchema = StructType([&#xA;    StructField(&#34;_id&#34;, StringType(), True),&#xA;    StructField(&#34;author&#34;, StringType(), True),&#xA;    StructField(&#34;description&#34;, StringType(), True),&#xA;    StructField(&#34;genre&#34;, StringType(), True),&#xA;    StructField(&#34;price&#34;, DoubleType(), True),&#xA;    StructField(&#34;publish_date&#34;, StringType(), True),&#xA;    StructField(&#34;title&#34;, StringType(), True)])&#xA;&#xA;df = spark.read \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;) \&#xA;    .load(&#39;books.xml&#39;, schema = customSchema)&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;, rootTag=&#39;books&#39;) \&#xA;    .save(&#39;newbooks.xml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;Automatically infer schema (data types)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;sparkR.session(&#34;local[4]&#34;, sparkPackages = c(&#34;com.databricks:spark-xml_2.12:0.14.0&#34;))&#xA;&#xA;df &amp;lt;- read.df(&#34;books.xml&#34;, source = &#34;xml&#34;, rowTag = &#34;book&#34;)&#xA;&#xA;# In this case, `rootTag` is set to &#34;ROWS&#34; and `rowTag` is set to &#34;ROW&#34;.&#xA;write.df(df, &#34;newbooks.csv&#34;, &#34;xml&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;sparkR.session(&#34;local[4]&#34;, sparkPackages = c(&#34;com.databricks:spark-xml_2.12:0.14.0&#34;))&#xA;customSchema &amp;lt;- structType(&#xA;  structField(&#34;_id&#34;, &#34;string&#34;),&#xA;  structField(&#34;author&#34;, &#34;string&#34;),&#xA;  structField(&#34;description&#34;, &#34;string&#34;),&#xA;  structField(&#34;genre&#34;, &#34;string&#34;),&#xA;  structField(&#34;price&#34;, &#34;double&#34;),&#xA;  structField(&#34;publish_date&#34;, &#34;string&#34;),&#xA;  structField(&#34;title&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(&#34;books.xml&#34;, source = &#34;xml&#34;, schema = customSchema, rowTag = &#34;book&#34;)&#xA;&#xA;# In this case, `rootTag` is set to &#34;ROWS&#34; and `rowTag` is set to &#34;ROW&#34;.&#xA;write.df(df, &#34;newbooks.csv&#34;, &#34;xml&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hadoop InputFormat&lt;/h2&gt; &#xA;&lt;p&gt;The library contains a Hadoop input format for reading XML files by a start tag and an end tag. This is similar with &lt;a href=&#34;https://github.com/apache/mahout/raw/9d14053c80a1244bdf7157ab02748a492ae9868a/integration/src/main/java/org/apache/mahout/text/wikipedia/XmlInputFormat.java&#34;&gt;XmlInputFormat.java&lt;/a&gt; in &lt;a href=&#34;https://mahout.apache.org&#34;&gt;Mahout&lt;/a&gt; but supports to read compressed files, different encodings and read elements including attributes, which you may make direct use of as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.XmlInputFormat&#xA;import org.apache.spark.SparkContext&#xA;import org.apache.hadoop.io.{LongWritable, Text}&#xA;&#xA;val sc: SparkContext = _&#xA;&#xA;// This will detect the tags including attributes&#xA;sc.hadoopConfiguration.set(XmlInputFormat.START_TAG_KEY, &#34;&amp;lt;book&amp;gt;&#34;)&#xA;sc.hadoopConfiguration.set(XmlInputFormat.END_TAG_KEY, &#34;&amp;lt;/book&amp;gt;&#34;)&#xA;&#xA;val records = sc.newAPIHadoopFile(&#xA;  &#34;path&#34;,&#xA;  classOf[XmlInputFormat],&#xA;  classOf[LongWritable],&#xA;  classOf[Text])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;https://www.scala-sbt.org/&#34;&gt;SBT&lt;/a&gt;. To build a JAR file simply run &lt;code&gt;sbt package&lt;/code&gt; from the project root.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project was initially created by &lt;a href=&#34;https://github.com/HyukjinKwon&#34;&gt;HyukjinKwon&lt;/a&gt; and donated to &lt;a href=&#34;https://databricks.com&#34;&gt;Databricks&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ergoplatform/ergo</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/ergoplatform/ergo</id>
    <link href="https://github.com/ergoplatform/ergo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ergo protocol description &amp; reference client implementation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ergo&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the reference implementation of the Ergo Platform protocol, which is an alternative to the &lt;a href=&#34;https://bitcoin.org/bitcoin.pdf&#34;&gt;Bitcoin protocol&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ergo Platform website: &lt;a href=&#34;https://ergoplatform.org/&#34;&gt;https://ergoplatform.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Differences from Bitcoin&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Powerful contracts in the multi-stage extended UTXO model (see &lt;a href=&#34;https://ergoplatform.org/docs/ErgoScript.pdf&#34;&gt;ErgoScript whitepaper&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Memory-hard Proof-of-Work function &lt;a href=&#34;https://docs.ergoplatform.com/ErgoPow.pdf&#34;&gt;Autolykos2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support for stateless clients (asymmetric, based on &lt;a href=&#34;https://eprint.iacr.org/2016/994&#34;&gt;https://eprint.iacr.org/2016/994&lt;/a&gt;), &lt;a href=&#34;https://eprint.iacr.org/2017/963.pdf&#34;&gt;NiPoPoWs&lt;/a&gt;, hybrid modes&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ScorexFoundation/sigmastate-interpreter&#34;&gt;Alternative transactional language&lt;/a&gt;, which is more powerful that Bitcoin Script but also safe against heavy validation attacks&lt;/li&gt; &#xA; &lt;li&gt;Alternative fee model with &lt;a href=&#34;https://fc18.ifca.ai/bitcoin/papers/bitcoin18-final18.pdf&#34;&gt;mandatory storage-rent component&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Specifications&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://ergoplatform.org/docs/whitepaper.pdf&#34;&gt;White Paper&lt;/a&gt; with a brief description is available. A Yellow Paper with detailed specification is underway and will be available shortly. At the moment, there are &lt;a href=&#34;https://github.com/ergoplatform/ergo/tree/master/papers/yellow&#34;&gt;drafts of the Yellow Paper&lt;/a&gt; available, and currently the reference implementation code should be considered as the specification.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can check our &lt;a href=&#34;https://github.com/ergoplatform/ergo/wiki/Set-up-a-full-node&#34;&gt;Setup A Full Node&lt;/a&gt; wiki page to learn how to manually setup and configure a node.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively you can run the prepared &lt;a href=&#34;https://raw.githubusercontent.com/ergoplatform/ergo/master/ergo-installer.sh&#34;&gt;ergo-installer.sh&lt;/a&gt; script. With this script you&#39;ll have the latest Ergo node installed without any hassle (only availalbe for Linux distributions):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -s https://raw.githubusercontent.com/ergoplatform/ergo/master/ergo-installer.sh | sh -s -- --api-key=&amp;lt;YOUR_API_KEY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build from source&lt;/h2&gt; &#xA;&lt;p&gt;In order to build the Ergo node from sources you need JDK (&amp;gt;= 1.8) and SBT to be &lt;a href=&#34;https://docs.scala-lang.org/getting-started/sbt-track/getting-started-with-scala-and-sbt-on-the-command-line.html&#34;&gt;installed&lt;/a&gt; on your machine.&lt;/p&gt; &#xA;&lt;p&gt;In order to simply get a single jar run: &lt;code&gt;sbt assembly&lt;/code&gt; - assembly would appear in &lt;code&gt;target/scala-2.12/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;If you want to create a package for a specific platform with launching scripts the one of the following packager commands could be chosen (depending on desired system type you want to build for):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;universal:packageBin&lt;/code&gt; - Generates a universal zip file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;universal:packageZipTarball&lt;/code&gt; - Generates a universal tgz file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;debian:packageBin&lt;/code&gt; - Generates a deb&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker:publishLocal&lt;/code&gt; - Builds a Docker image using the local Docker server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rpm:packageBin&lt;/code&gt; - Generates an rpm&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;universal:packageOsxDmg&lt;/code&gt; - Generates a DMG file with the same contents as the universal zip/tgz.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;windows:packageBin&lt;/code&gt; - Generates an MSI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The final build command should look like: &lt;code&gt;sbt &amp;lt;packager_command&amp;gt;&lt;/code&gt;, example: &lt;code&gt;sbt universal:packageBin&lt;/code&gt;. A resulted package could be found in the &lt;code&gt;target/scala-2.12/&amp;lt;platform_type&amp;gt;&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Running the node&lt;/h2&gt; &#xA;&lt;p&gt;The node can be started in a couple different ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In case you only have a jar: &lt;code&gt;java -jar /path/to/ergo-&amp;lt;version&amp;gt;.jar --&amp;lt;networkId&amp;gt; -c /path/to/local.conf&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Using start script from sbt-native-packager: &lt;code&gt;sh /path/to/bin/ergo --&amp;lt;networkId&amp;gt; -c /path/to/local.conf&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Available &lt;code&gt;networkId&lt;/code&gt; options: &lt;code&gt;mainnet&lt;/code&gt;, &lt;code&gt;testnet&lt;/code&gt;, &lt;code&gt;devnet&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;UI&lt;/h2&gt; &#xA;&lt;p&gt;Node UI (graphical interface) could be accessed at &lt;code&gt;&amp;lt;node_ip&amp;gt;:&amp;lt;api_port&amp;gt;/panel&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/ergoplatform/static-data/raw/master/img/node_ui.png&#34; align=&#34;right&#34;&gt; &#xA;&lt;h2&gt;Docker Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ergo has officially supported Docker package. To run last Ergo version in mainnet as a console application with logs printed to console:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo docker run --rm -p 9030:9030 -p 127.0.0.1:9053:9053 -v /path/on/host/to/ergo/data:/home/ergo/.ergo ergoplatform/ergo --mainnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will connect to Ergo mainnet with default config and open port &lt;code&gt;9030&lt;/code&gt; globally and &lt;code&gt;9053&lt;/code&gt; locally on the host system. All data will be stored in your host directory &lt;code&gt;/path/on/host/to/ergo/data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run specific Ergo version &lt;code&gt;&amp;lt;VERSION&amp;gt;&lt;/code&gt; as a service with custom config &lt;code&gt;/path/on/host/system/to/myergo.conf&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo docker run -d \&#xA;    -p 9030:9030 \&#xA;    -p 127.0.0.1:9053:9053 \&#xA;    -v /path/on/host/to/ergo/data:/home/ergo/.ergo \&#xA;    -v /path/on/host/system/to/myergo.conf:/etc/myergo.conf \&#xA;    -e MAX_HEAP=3G \&#xA;    ergoplatform/ergo:&amp;lt;VERSION&amp;gt; --&amp;lt;networkId&amp;gt; -c /etc/myergo.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available versions can be found on &lt;a href=&#34;https://hub.docker.com/r/ergoplatform/ergo/tags&#34;&gt;Ergo Docker image page&lt;/a&gt;, for example, &lt;code&gt;v4.0.31&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This will connect to the Ergo mainnet or testnet following your configuration passed in &lt;code&gt;myergo.conf&lt;/code&gt; and network flag &lt;code&gt;--&amp;lt;networkId&amp;gt;&lt;/code&gt;. Every default config value would be overwritten with corresponding value in &lt;code&gt;myergo.conf&lt;/code&gt;. &lt;code&gt;MAX_HEAP&lt;/code&gt; variable can be used to control how much memory can the node consume.&lt;/p&gt; &#xA;&lt;p&gt;This command also would store your data in &lt;code&gt;/path/on/host/to/ergo/data&lt;/code&gt; on host system, and open ports &lt;code&gt;9030&lt;/code&gt; (node communication) globally and &lt;code&gt;9053&lt;/code&gt; (REST API) locally on host system. The &lt;code&gt;/path/on/host/to/ergo/data&lt;/code&gt; directory must has &lt;code&gt;777&lt;/code&gt; permissions or has owner/group numeric id equal to &lt;code&gt;9052&lt;/code&gt; to be writable by container, as &lt;code&gt;ergo&lt;/code&gt; user inside Docker image (please refer to &lt;a href=&#34;https://raw.githubusercontent.com/ergoplatform/ergo/master/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Ergo node works normally behind NAT, so you can keep closed your &lt;code&gt;9030&lt;/code&gt; port, hence other nodes could not discover and connect to yours one, only your node could initiate connections.&lt;/p&gt; &#xA;&lt;p&gt;It is also a good practice to keep closed REST API port &lt;code&gt;9053&lt;/code&gt;, and connect to your node from inside another container in the same Docker network (this case not covered by this short quick start manual).&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;There are three kinds of tests:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Unit and property tests, run them with &lt;code&gt;sbt test&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;Integration tests, they require for Docker to be installed, then run &lt;code&gt;sudo sbt it:test&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Bootstrapping tests, very slow as they are checking that the node is indeed catching up with the main network in different regimes, they require for Docker too, run as &lt;code&gt;sudo sbt it2:test&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Open project in IDE&lt;/h2&gt; &#xA;&lt;p&gt;Your can use &lt;a href=&#34;https://www.jetbrains.com/idea/&#34;&gt;IntelliJ IDEA&lt;/a&gt; (Community or Ultimate edition) or &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;VSCode&lt;/a&gt; + &lt;a href=&#34;https://scalameta.org/metals/&#34;&gt;Metals&lt;/a&gt;. Before opening the project in IDE make sure it can be built with sbt. You may need to fix dependency resolution errors first.&lt;/p&gt; &#xA;&lt;p&gt;After that you can open the project folder in Idea (File / Open) which will run Project Import Wizard. The wizard will use SBT configuration (build.sbt file) to generate Idea&#39;s project configuration files. You can open &lt;code&gt;File / Project Structure...&lt;/code&gt; dialog to see project configuration. If everything is successful you can compile the project in IDE.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Ergo is open-source and open movement, always in need for testers and developers! Please feel free to discuss development in &lt;a href=&#34;https://discord.gg/kj7s7nb&#34;&gt;Ergo Discord&lt;/a&gt;, #development channel.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ergoplatform/ergo/master/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Dan-M/dumbsterweb</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/Dan-M/dumbsterweb</id>
    <link href="https://github.com/Dan-M/dumbsterweb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Lift web app wrapping Dumbster, the fake Smtp Server&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>OpenXiangShan/XiangShan</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/OpenXiangShan/XiangShan</id>
    <link href="https://github.com/OpenXiangShan/XiangShan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source high-performance RISC-V processor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiangShan&lt;/h1&gt; &#xA;&lt;p&gt;XiangShan (香山) is an open-source high-performance RISC-V processor project.&lt;/p&gt; &#xA;&lt;p&gt;中文说明&lt;a href=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/readme.zh-cn.md&#34;&gt;在此&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2020-2022 by Institute of Computing Technology, Chinese Academy of Sciences.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2020-2022 by Peng Cheng Laboratory.&lt;/p&gt; &#xA;&lt;h2&gt;Docs and slides&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan-doc&#34;&gt;XiangShan-doc&lt;/a&gt; is our official documentation repository. It contains design spec., technical slides, tutorial and more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Micro-architecture documentation of XiangShan has been published. Please check out &lt;a href=&#34;https://xiangshan-doc.readthedocs.io&#34;&gt;https://xiangshan-doc.readthedocs.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Follow us&lt;/h2&gt; &#xA;&lt;p&gt;Wechat/微信：香山开源处理器&lt;/p&gt; &#xA;&lt;div align=&#34;left&#34;&gt;&#xA; &lt;img width=&#34;340&#34; height=&#34;117&#34; src=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/wechat.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;Zhihu/知乎：&lt;a href=&#34;https://www.zhihu.com/people/openxiangshan&#34;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Weibo/微博：&lt;a href=&#34;https://weibo.com/u/7706264932&#34;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can contact us through &lt;a href=&#34;mailto:xiangshan-all@ict.ac.cn&#34;&gt;our mail list&lt;/a&gt;. All mails from this list will be archived to &lt;a href=&#34;https://www.mail-archive.com/xiangshan-all@ict.ac.cn/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The first stable micro-architecture of XiangShan is called Yanqihu (雁栖湖) on this &lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan/tree/yanqihu&#34;&gt;branch&lt;/a&gt;, which has been developed since June 2020. The current version of XiangShan, also known as Nanhu (南湖), is still under development on the master branch.&lt;/p&gt; &#xA;&lt;p&gt;The micro-architecture overview of Nanhu (南湖) is shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/xs-arch-nanhu.svg?sanitize=true&#34; alt=&#34;xs-arch-nanhu&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sub-directories Overview&lt;/h2&gt; &#xA;&lt;p&gt;Some of the key directories are shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── src&#xA;│   └── main/scala         # design files&#xA;│       ├── device         # virtual device for simulation&#xA;│       ├── system         # SoC wrapper&#xA;│       ├── top            # top module&#xA;│       ├── utils          # utilization code&#xA;│       ├── xiangshan      # main design code&#xA;│       └── xstransforms   # some useful firrtl transforms&#xA;├── scripts                # scripts for agile development&#xA;├── fudian                 # floating unit submodule of XiangShan&#xA;├── huancun                # L2/L3 cache submodule of XiangShan&#xA;├── difftest               # difftest co-simulation framework&#xA;└── ready-to-run           # pre-built simulation images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;IDE Support&lt;/h2&gt; &#xA;&lt;h3&gt;bsp&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;make bsp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;IDEA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;make idea&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generate Verilog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make verilog&lt;/code&gt; to generate verilog code. The output file is &lt;code&gt;build/XSTop.v&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run Programs by Simulation&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;NEMU_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&#34;https://github.com/OpenXiangShan/NEMU&#34;&gt;NEMU project&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;NOOP_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the XiangShan project.&lt;/li&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;AM_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&#34;https://github.com/OpenXiangShan/nexus-am&#34;&gt;AM project&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;mill&lt;/code&gt;. Refer to &lt;a href=&#34;https://com-lihaoyi.github.io/mill/mill/Intro_to_Mill.html#_installation&#34;&gt;the Manual section in this guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone this project and run &lt;code&gt;make init&lt;/code&gt; to initialize submodules.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run with simulator&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://verilator.org/guide/latest/&#34;&gt;Verilator&lt;/a&gt;, the open-source Verilog simulator.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make emu&lt;/code&gt; to build the C++ simulator &lt;code&gt;./build/emu&lt;/code&gt; with Verilator.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;./build/emu --help&lt;/code&gt; for run-time arguments of the simulator.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; and &lt;code&gt;verilator.mk&lt;/code&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make emu CONFIG=MinimalConfig EMU_THREADS=2 -j10&#xA;./build/emu -b 0 -e 0 -i ./ready-to-run/coremark-2-iteration.bin --diff ./ready-to-run/riscv64-nemu-interpreter-so&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting Guide&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan/wiki/Troubleshooting-Guide&#34;&gt;Troubleshooting Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;In the development of XiangShan, some sub-modules from the open-source community are employed. All relevant usage is listed below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Sub-module&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;   &lt;th&gt;Detail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;L2 Cache/LLC&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ucb-bar/block-inclusivecache-sifive&#34;&gt;Sifive block-inclusivecache&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our new L2/L3 design are inspired by Sifive&#39;s &lt;code&gt;block-inclusivecache&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Diplomacy/TileLink&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;Rocket-chip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We reused the Diplomacy framework and TileLink utility that exist in rocket-chip to negotiate bus.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We are grateful for the support of the open-source community and encourage other open-source projects to reuse our code within the scope of the &lt;a href=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/LICENSE&#34;&gt;license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linkedin/feathr</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/linkedin/feathr</id>
    <link href="https://github.com/linkedin/feathr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Feathr – An Enterprise-Grade, High Performance Feature Store&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Feathr – An Enterprise-Grade, High Performance Feature Store&lt;/h1&gt; &#xA;&lt;h2&gt;What is Feathr?&lt;/h2&gt; &#xA;&lt;p&gt;Feathr is the feature store that is used in production in LinkedIn for many years and was open sourced in April 2022. Read our announcement on &lt;a href=&#34;https://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m&#34;&gt;Open Sourcing Feathr&lt;/a&gt; and &lt;a href=&#34;https://azure.microsoft.com/en-us/blog/feathr-linkedin-s-feature-store-is-now-available-on-azure/&#34;&gt;Feathr on Azure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Feathr lets you:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Define features&lt;/strong&gt; based on raw data sources (batch and streaming) using pythonic APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Register and get features by names&lt;/strong&gt; during model training and model inferencing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Share features&lt;/strong&gt; across your team and company.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Feathr automatically computes your feature values and joins them to your training data, using point-in-time-correct semantics to avoid data leakage, and supports materializing and deploying your features for use online in production.&lt;/p&gt; &#xA;&lt;h2&gt;🌟 Feathr Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable with built-in optimizations.&lt;/strong&gt; For example, based on some internal use case, Feathr can process billions of rows and PB scale data with built-in optimizations such as bloom filters and salted joins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich support for point-in-time joins and aggregations:&lt;/strong&gt; Feathr has high performant built-in operators designed for Feature Store, including time-based aggregation, sliding window joins, look-up features, all with point-in-time correctness.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly customizable user-defined functions (UDFs)&lt;/strong&gt; with native PySpark and Spark SQL support to lower the learning curve for data scientists.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pythonic APIs&lt;/strong&gt; to access everything with low learning curve; Integrated with model building so data scientists can be productive from day one.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich type system&lt;/strong&gt; including support for embeddings for advanced machine learning/deep learning scenarios. One of the common use cases is to build embeddings for customer profiles, and those embeddings can be reused across an organization in all the machine learning applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Native cloud integration&lt;/strong&gt; with simplified and scalable architecture, which is illustrated in the next section.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature sharing and reuse made easy:&lt;/strong&gt; Feathr has built-in feature registry so that features can be easily shared across different teams and boost team productivity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📓 Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more details on Feathr, read our &lt;a href=&#34;https://linkedin.github.io/feathr/&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For Python API references, read the &lt;a href=&#34;https://feathr.readthedocs.io/&#34;&gt;Python API Reference&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For technical talks on Feathr, see the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/docs/talks/Feathr%20Feature%20Store%20Talk.pdf&#34;&gt;slides here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠️ Install Feathr Client Locally&lt;/h2&gt; &#xA;&lt;p&gt;If you want to install Feathr client in a python environment, use this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install feathr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use the latest code from GitHub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/linkedin/feathr.git#subdirectory=feathr_project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;☁️ Running Feathr on Cloud&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please read the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/docs/quickstart_databricks.md&#34;&gt;Quick Start Guide for Feathr on Databricks&lt;/a&gt; to run Feathr with Databricks.&lt;/li&gt; &#xA; &lt;li&gt;Please read the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/docs/quickstart.md&#34;&gt;Quick Start Guide for Feathr on Azure Synapse&lt;/a&gt; to run Feathr with Azure Synapse.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔡 Feathr Examples&lt;/h2&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://linkedin.github.io/feathr/concepts/feathr-capabilities.html&#34;&gt;Feathr Capabilities&lt;/a&gt; for more examples. Below are a few selected ones:&lt;/p&gt; &#xA;&lt;h3&gt;Rich UDF Support&lt;/h3&gt; &#xA;&lt;p&gt;Feathr has highly customizable UDFs with native PySpark and Spark SQL integration to lower learning curve for data scientists:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def add_new_dropoff_and_fare_amount_column(df: DataFrame):&#xA;    df = df.withColumn(&#34;f_day_of_week&#34;, dayofweek(&#34;lpep_dropoff_datetime&#34;))&#xA;    df = df.withColumn(&#34;fare_amount_cents&#34;, df.fare_amount.cast(&#39;double&#39;) * 100)&#xA;    return df&#xA;&#xA;batch_source = HdfsSource(name=&#34;nycTaxiBatchSource&#34;,&#xA;                        path=&#34;abfss://feathrazuretest3fs@feathrazuretest3storage.dfs.core.windows.net/demo_data/green_tripdata_2020-04.csv&#34;,&#xA;                        preprocessing=add_new_dropoff_and_fare_amount_column,&#xA;                        event_timestamp_column=&#34;new_lpep_dropoff_datetime&#34;,&#xA;                        timestamp_format=&#34;yyyy-MM-dd HH:mm:ss&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Defining Window Aggregation Features&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;agg_features = [Feature(name=&#34;f_location_avg_fare&#34;,&#xA;                        key=location_id,                          # Query/join key of the feature(group)&#xA;                        feature_type=FLOAT,&#xA;                        transform=WindowAggTransformation(        # Window Aggregation transformation&#xA;                            agg_expr=&#34;cast_float(fare_amount)&#34;,&#xA;                            agg_func=&#34;AVG&#34;,                       # Apply average aggregation over the window&#xA;                            window=&#34;90d&#34;)),                       # Over a 90-day window&#xA;                ]&#xA;&#xA;agg_anchor = FeatureAnchor(name=&#34;aggregationFeatures&#34;,&#xA;                           source=batch_source,&#xA;                           features=agg_features)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Define features on top of other features - Derived Features&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute a new feature(a.k.a. derived feature) on top of an existing feature&#xA;derived_feature = DerivedFeature(name=&#34;f_trip_time_distance&#34;,&#xA;                                 feature_type=FLOAT,&#xA;                                 key=trip_key,&#xA;                                 input_features=[f_trip_distance, f_trip_time_duration],&#xA;                                 transform=&#34;f_trip_distance * f_trip_time_duration&#34;)&#xA;&#xA;# Another example to compute embedding similarity&#xA;user_embedding = Feature(name=&#34;user_embedding&#34;, feature_type=DENSE_VECTOR, key=user_key)&#xA;item_embedding = Feature(name=&#34;item_embedding&#34;, feature_type=DENSE_VECTOR, key=item_key)&#xA;&#xA;user_item_similarity = DerivedFeature(name=&#34;user_item_similarity&#34;,&#xA;                                      feature_type=FLOAT,&#xA;                                      key=[user_key, item_key],&#xA;                                      input_features=[user_embedding, item_embedding],&#xA;                                      transform=&#34;cosine_similarity(user_embedding, item_embedding)&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Define Streaming Features&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://linkedin.github.io/feathr/how-to-guides/streaming-source-ingestion.html&#34;&gt;Streaming Source Ingestion Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Point in Time Joins&lt;/h3&gt; &#xA;&lt;p&gt;Read &lt;a href=&#34;https://linkedin.github.io/feathr/concepts/point-in-time-join.html&#34;&gt;Point-in-time Correctness and Point-in-time Join in Feathr&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Running Feathr Examples&lt;/h3&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/feathr_project/feathrcli/data/feathr_user_workspace/nyc_driver_demo.ipynb&#34;&gt;quick start Jupyter Notebook&lt;/a&gt; to try it out. There is also a companion &lt;a href=&#34;https://linkedin.github.io/feathr/quickstart.html&#34;&gt;quick start guide&lt;/a&gt; containing a bit more explanation on the notebook.&lt;/p&gt; &#xA;&lt;h2&gt;🗣️ Tech Talks on Feathr&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gZg01UKQMTY&#34;&gt;Introduction to Feathr - Beginner&#39;s guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mybuild.microsoft.com/en-US/sessions/5bdff7d5-23e6-4f0d-9175-da8325d05c2a?source=sessions&#34;&gt;Document Intelligence using Azure Feature Store (Feathr) and SynapseML &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⚙️ Cloud Integrations&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feathr component&lt;/th&gt; &#xA;   &lt;th&gt;Cloud Integrations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Offline store – Object Store&lt;/td&gt; &#xA;   &lt;td&gt;Azure Blob Storage, Azure ADLS Gen2, AWS S3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Offline store – SQL&lt;/td&gt; &#xA;   &lt;td&gt;Azure SQL DB, Azure Synapse Dedicated SQL Pools, Azure SQL in VM, Snowflake&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Streaming Source&lt;/td&gt; &#xA;   &lt;td&gt;Kafka, EventHub&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Online store&lt;/td&gt; &#xA;   &lt;td&gt;Azure Cache for Redis&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Feature Registry&lt;/td&gt; &#xA;   &lt;td&gt;Azure Purview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Compute Engine&lt;/td&gt; &#xA;   &lt;td&gt;Azure Synapse Spark Pools, Databricks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Machine Learning Platform&lt;/td&gt; &#xA;   &lt;td&gt;Azure Machine Learning, Jupyter Notebook&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;File Format&lt;/td&gt; &#xA;   &lt;td&gt;Parquet, ORC, Avro, Delta Lake&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🚀 Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;For a complete roadmap with esitmated dates, please &lt;a href=&#34;https://github.com/linkedin/feathr/milestones?direction=asc&amp;amp;sort=title&amp;amp;state=open&#34;&gt;visit this page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Private Preview release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Public Preview release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Future release &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support streaming&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support common data sources&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support online transformation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature versioning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature monitoring&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature store UI &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Lineage&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Search&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature data deletion and retention&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👨‍👨‍👦‍👦 Community Guidelines&lt;/h2&gt; &#xA;&lt;p&gt;Build for the community and build by the community. Check out &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/CONTRIBUTING.md&#34;&gt;Community Guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📢 Slack Channel&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://feathrai.slack.com&#34;&gt;Slack channel&lt;/a&gt; for questions and discussions (or click the &lt;a href=&#34;https://join.slack.com/t/feathrai/shared_invite/zt-19dcbquwl-zKiJGYTak6Psw2GbUYtT2g&#34;&gt;invitation link&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linkerd/linkerd</title>
    <updated>2022-06-02T01:54:13Z</updated>
    <id>tag:github.com,2022-06-02:/linkerd/linkerd</id>
    <link href="https://github.com/linkerd/linkerd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Old repo for Linkerd 1.x. See the linkerd2 repo for Linkerd 2.x.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/9226/33582867-3e646e02-d90c-11e7-85a2-2e238737e859.png&#34; alt=&#34;linkerd&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/linkerd/linkerd.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/linkerd/linkerd&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/linkerd/linkerd/tree/main.svg?style=shield&amp;amp;circle-token=06d80fc52dbaeaac316d09b7ad4ada6f7d2bf31f&#34; alt=&#34;Circle CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://slack.linkerd.io&#34;&gt;&lt;img src=&#34;https://slack.linkerd.io/badge.svg?sanitize=true&#34; alt=&#34;Slack Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/buoyantio/linkerd/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/buoyantio/linkerd.svg?sanitize=true&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1445&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1445/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This repo is for the 1.x version of Linkerd. Feature development is now happening in the &lt;a href=&#34;https://github.com/linkerd/linkerd2&#34;&gt;linkerd2&lt;/a&gt; repo. This repo is currently only used for periodic maintenance releases of Linkerd 1.x.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Linkerd 1.x (pronounced &#34;linker-DEE&#34;) acts as a transparent HTTP/gRPC/thrift/etc proxy, and can usually be dropped into existing applications with a minimum of configuration, regardless of what language they&#39;re written in. It works with many common protocols and service discovery backends, including scheduled environments like Nomad, Mesos and Kubernetes.&lt;/p&gt; &#xA;&lt;p&gt;Linkerd is built on top of &lt;a href=&#34;https://netty.io/&#34;&gt;Netty&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;Finagle&lt;/a&gt;, a production-tested RPC framework used by high-traffic companies like Twitter, Pinterest, Tumblr, PagerDuty, and others.&lt;/p&gt; &#xA;&lt;p&gt;Linkerd is hosted by the Cloud Native Computing Foundation (&lt;a href=&#34;https://www.cncf.io/about&#34;&gt;CNCF&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Want to try it?&lt;/h2&gt; &#xA;&lt;p&gt;We distribute binaries which you can download from the &lt;a href=&#34;https://github.com/linkerd/linkerd/releases&#34;&gt;Linkerd releases page&lt;/a&gt;. We also publish Docker images for each release, which you can find on &lt;a href=&#34;https://hub.docker.com/r/buoyantio/linkerd/&#34;&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For instructions on how to configure and run Linkerd, see the &lt;a href=&#34;https://linkerd.io/1/&#34;&gt;1.x user documentation on linkerd.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Working in this repo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/BUILD.md&#34;&gt;BUILD.md&lt;/a&gt; includes general information on how to work in this repo. Additionally, there are documents on how to build several of the application subprojects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/linkerd/README.md&#34;&gt;linkerd&lt;/a&gt; -- produces &lt;code&gt;linkerd&lt;/code&gt; router artifacts&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/namerd/README.md&#34;&gt;namerd&lt;/a&gt; -- produces &lt;code&gt;namerd&lt;/code&gt; service discovery artifacts&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/grpc/README.md&#34;&gt;grpc&lt;/a&gt; -- produces the &lt;code&gt;protoc-gen-io.buoyant.grpc&lt;/code&gt; code generator&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We &lt;span&gt;❤️&lt;/span&gt; pull requests! See &lt;a href=&#34;https://raw.githubusercontent.com/linkerd/linkerd/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for info on contributing changes.&lt;/p&gt; &#xA;&lt;h2&gt;Related Repos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd2&#34;&gt;linkerd2&lt;/a&gt;: The main repo for Linkerd 2.x and where current development is happening.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd-examples&#34;&gt;linkerd-examples&lt;/a&gt;: A variety of configuration examples and explanations&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd-tcp&#34;&gt;linkerd-tcp&lt;/a&gt;: A lightweight TCP/TLS load balancer that uses Namerd&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd-viz&#34;&gt;linkerd-viz&lt;/a&gt;: Zero-configuration service dashboard for Linkerd&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd-zipkin&#34;&gt;linkerd-zipkin&lt;/a&gt;: &lt;a href=&#34;https://github.com/openzipkin/zipkin&#34;&gt;Zipkin&lt;/a&gt; tracing plugins&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/linkerd/namerctl&#34;&gt;namerctl&lt;/a&gt;: A commandline utility for controlling Namerd&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project is for everyone. We ask that our users and contributors take a few minutes to review our &lt;a href=&#34;https://github.com/linkerd/linkerd/wiki/Linkerd-code-of-conduct&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2018, Linkerd Authors. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use these files except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://www.apache.org/licenses/LICENSE-2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;!-- references --&gt;</summary>
  </entry>
</feed>