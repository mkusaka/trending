<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-21T01:34:10Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aws/sagemaker-spark</title>
    <updated>2024-10-21T01:34:10Z</updated>
    <id>tag:github.com,2024-10-21:/aws/sagemaker-spark</id>
    <link href="https://github.com/aws/sagemaker-spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Spark library for Amazon SageMaker.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img alt=&#34;SageMaker&#34; src=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/branding/icon/sagemaker-banner.png&#34; height=&#34;100&#34;&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;SageMaker Spark&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/aws/sagemaker-spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/aws/sagemaker-spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark is an open source Spark library for &lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;Amazon SageMaker&lt;/a&gt;. With SageMaker Spark you construct Spark ML &lt;code&gt;Pipeline&lt;/code&gt;s using Amazon SageMaker stages. These pipelines interleave native Spark ML stages and stages that interact with SageMaker training and model hosting.&lt;/p&gt; &#xA;&lt;p&gt;With SageMaker Spark, you can train on Amazon SageMaker from Spark &lt;code&gt;DataFrame&lt;/code&gt;s using &lt;strong&gt;Amazon-provided ML algorithms&lt;/strong&gt; like K-Means clustering or XGBoost, and make predictions on &lt;code&gt;DataFrame&lt;/code&gt;s against SageMaker endpoints hosting your trained models, and, if you have &lt;strong&gt;your own ML algorithms&lt;/strong&gt; built into SageMaker compatible Docker containers, you can use SageMaker Spark to train and infer on &lt;code&gt;DataFrame&lt;/code&gt;s with your own algorithms -- &lt;strong&gt;all at Spark scale.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#getting-sagemaker-spark&#34;&gt;Getting SageMaker Spark&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#scala&#34;&gt;Scala&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#running-sagemaker-spark&#34;&gt;Running SageMaker Spark&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#running-sagemaker-spark-applications-with-spark-shell-or-spark-submit&#34;&gt;Running SageMaker Spark Applications with spark-shell or &lt;code&gt;spark-submit&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#running-sagemaker-spark-applications-on-emr&#34;&gt;Running SageMaker Spark Applications on EMR&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#python&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#s3-filesystem-schemes&#34;&gt;S3 FileSystem Schemes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#getting-started-k-means-clustering-on-sagemaker-with-sagemaker-spark-sdk&#34;&gt;Getting Started: K-Means Clustering on SageMaker with SageMaker Spark SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#example-using-sagemaker-spark-with-any-sagemaker-algorithm&#34;&gt;Example: Using SageMaker Spark with Any SageMaker Algorithm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#example-using-sagemakerestimator-and-sagemakermodel-in-a-spark-pipeline&#34;&gt;Example: Using SageMakerEstimator and SageMakerModel in a Spark Pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#example-using-multiple-sagemakerestimators-and-sagemakermodels-in-a-spark-pipeline&#34;&gt;Example: Using Multiple SageMakerEstimators and SageMakerModels in a Spark Pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#example-creating-a-sagemakermodel&#34;&gt;Example: Creating a SageMakerModel&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#sagemakermodel-from-an-endpoint&#34;&gt;SageMakerModel From an Endpoint&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#sagemakermodel-from-model-data-in-s3&#34;&gt;SageMakerModel From Model Data in S3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#sagemakermodel-from-a-previously-completed-training-job&#34;&gt;SageMakerModel From a Previously Completed Training Job&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#example-tearing-down-amazon-sagemaker-endpoints&#34;&gt;Example: Tearing Down Amazon SageMaker Endpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#configuring-an-iam-role&#34;&gt;Configuring an IAM Role&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#sagemaker-spark-in-depth&#34;&gt;SageMaker Spark: In-Depth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#the-amazon-record-format&#34;&gt;The Amazon Record format&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#serializing-and-deserializing-for-inference&#34;&gt;Serializing and Deserializing for Inference&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting SageMaker Spark&lt;/h2&gt; &#xA;&lt;h3&gt;Scala&lt;/h3&gt; &#xA;&lt;p&gt;SageMaker Spark for Scala is available in the Maven central repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sagemaker-spark_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;spark_2.2.0-1.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, if your project depends on Spark 2.1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sagemaker-spark_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;spark_2.1.1-1.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also build SageMaker Spark from source. See &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/sagemaker-spark-sdk&#34;&gt;sagemaker-spark-sdk&lt;/a&gt; for more on building SageMaker Spark from source.&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/sagemaker-pyspark-sdk&#34;&gt;sagemaker-pyspark-sdk&lt;/a&gt; for more on installing and running SageMaker PySpark.&lt;/p&gt; &#xA;&lt;h2&gt;Running SageMaker Spark&lt;/h2&gt; &#xA;&lt;p&gt;SageMaker Spark depends on hadoop-aws-2.8.1. To run Spark applications that depend on SageMaker Spark, you need to build Spark with Hadoop 2.8. However, if you are running Spark applications on EMR, you can use Spark built with Hadoop 2.7.&lt;/p&gt; &#xA;&lt;p&gt;Apache Spark currently distributes binaries built against Hadoop-2.7, but not 2.8. See the &lt;a href=&#34;https://spark.apache.org/docs/2.2.0/hadoop-provided.html&#34;&gt;Spark documentation&lt;/a&gt; for more on building Spark with Hadoop 2.8.&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark needs to be added to both the driver and executor classpaths.&lt;/p&gt; &#xA;&lt;h3&gt;Running SageMaker Spark Applications with &lt;code&gt;spark-shell&lt;/code&gt; or &lt;code&gt;spark-submit&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can submit SageMaker Spark and the AWS Java Client as dependencies with the &#34;--jars&#34; flag, or take a dependency on SageMaker Spark in Maven using the &#34;--package&#34; flag.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Hadoop-2.8. &lt;a href=&#34;https://hadoop.apache.org/docs/r2.8.0/&#34;&gt;https://hadoop.apache.org/docs/r2.8.0/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build Spark 2.2 with Hadoop-2.8. The &lt;a href=&#34;https://spark.apache.org/docs/2.2.0/hadoop-provided.html&#34;&gt;Spark documentation&lt;/a&gt; has guidance on building Spark with your own Hadoop installation.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;spark-shell&lt;/code&gt; or &lt;code&gt;spark-submit&lt;/code&gt; with the &lt;code&gt;--packages&lt;/code&gt; flag:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark-shell --packages com.amazonaws:sagemaker-spark_2.11:spark_2.2.0-1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running SageMaker Spark Applications on EMR&lt;/h3&gt; &#xA;&lt;p&gt;You can run SageMaker Spark applications on an EMR cluster just like any other Spark application by submitting your Spark application jar and the SageMaker Spark dependency jars with the --jars or --packages flags.&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark is pre-installed on EMR releases since 5.11.0. You can run your SageMaker Spark application on EMR by submitting your Spark application jar and any additional dependencies your Spark application uses.&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark applications have also been verified to be compatible with EMR-5.6.0 (which runs Spark 2.1) and EMR-5-8.0 (which runs Spark 2.2). When submitting your Spark application to an earlier EMR release, use the &lt;code&gt;--packages&lt;/code&gt; flag to depend on a recent version of the AWS Java SDK:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark-submit&#xA;  --packages com.amazonaws:aws-java-sdk:1.11.613 \&#xA;  --deploy-mode cluster \&#xA;  --conf spark.driver.userClassPathFirst=true \&#xA;  --conf spark.executor.userClassPathFirst=true \&#xA;  --jars SageMakerSparkApplicationJar.jar,...&#xA;  ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark.driver.userClassPathFirst=true&lt;/code&gt; and &lt;code&gt;spark.executor.userClassPathFirst=true&lt;/code&gt; properties are required so that the Spark cluster will use the AWS Java SDK dependencies with SageMaker, rather than the AWS Java SDK installed on these earlier EMR clusters.&lt;/p&gt; &#xA;&lt;p&gt;For more on running Spark application on EMR, see the &lt;a href=&#34;http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html&#34;&gt;EMR Documentation&lt;/a&gt; on submitting a step.&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/sagemaker-pyspark-sdk&#34;&gt;sagemaker-pyspark-sdk&lt;/a&gt; for more on installing and running SageMaker PySpark.&lt;/p&gt; &#xA;&lt;h3&gt;S3 FileSystem Schemes&lt;/h3&gt; &#xA;&lt;p&gt;EMR allows you to read and write data using the EMR FileSystem (EMRFS), accessed through Spark with &#34;s3://&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.read.format(&#34;libsvm&#34;).load(&#34;s3://my-bucket/my-prefix&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In other execution environments, you can use the S3A schema to use the S3A FileSystem &#34;s3a://&#34; to read and write data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.read.format(&#34;libsvm&#34;).load(&#34;s3a://my-bucket/my-prefix&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the code examples in this README, we use &#34;s3://&#34; to use the &lt;a href=&#34;http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html&#34;&gt;EMRFS&lt;/a&gt;, or &#34;s3a://&#34; to use the &lt;a href=&#34;https://wiki.apache.org/hadoop/AmazonS3&#34;&gt;S3A system&lt;/a&gt;, which is recommended over &#34;s3n://&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;API Documentation&lt;/h3&gt; &#xA;&lt;p&gt;You can view the &lt;a href=&#34;https://aws.github.io/sagemaker-spark/&#34;&gt;Scala API Documentation for SageMaker Spark here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can view the &lt;a href=&#34;http://sagemaker-pyspark.readthedocs.io/en/latest/&#34;&gt;PySpark API Documentation for SageMaker Spark here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started: K-Means Clustering on SageMaker with SageMaker Spark SDK&lt;/h2&gt; &#xA;&lt;p&gt;This example walks through using SageMaker Spark to train on a Spark DataFrame using a SageMaker-provided algorithm, host the resulting model on SageMaker Spark, and making predictions on a Spark DataFrame using that hosted model.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll cluster handwritten digits in the MNIST dataset, which we&#39;ve made available in LibSVM format at &lt;code&gt;s3://sagemaker-sample-data-us-east-1/spark/mnist/train/mnist_train.libsvm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can start a Spark shell with SageMaker Spark&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark-shell --packages com.amazonaws:sagemaker-spark_2.11:spark_2.1.1-1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create your Spark Session and load your training and test data into DataFrames:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val spark = SparkSession.builder.getOrCreate&#xA;&#xA;// load mnist data as a dataframe from libsvm. replace this region with your own.&#xA;val region = &#34;us-east-1&#34;&#xA;val trainingData = spark.read.format(&#34;libsvm&#34;)&#xA;  .option(&#34;numFeatures&#34;, &#34;784&#34;)&#xA;  .load(s&#34;s3://sagemaker-sample-data-$region/spark/mnist/train/&#34;)&#xA;&#xA;val testData = spark.read.format(&#34;libsvm&#34;)&#xA;  .option(&#34;numFeatures&#34;, &#34;784&#34;)&#xA;  .load(s&#34;s3://sagemaker-sample-data-$region/spark/mnist/test/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;DataFrame&lt;/code&gt; consists of a column named &#34;label&#34; of Doubles, indicating the digit for each example, and a column named &#34;features&#34; of Vectors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;trainingData.show&#xA;&#xA;+-----+--------------------+&#xA;|label|            features|&#xA;+-----+--------------------+&#xA;|  5.0|(784,[152,153,154...|&#xA;|  0.0|(784,[127,128,129...|&#xA;|  4.0|(784,[160,161,162...|&#xA;|  1.0|(784,[158,159,160...|&#xA;|  9.0|(784,[208,209,210...|&#xA;|  2.0|(784,[155,156,157...|&#xA;|  1.0|(784,[124,125,126...|&#xA;|  3.0|(784,[151,152,153...|&#xA;|  1.0|(784,[152,153,154...|&#xA;|  4.0|(784,[134,135,161...|&#xA;|  3.0|(784,[123,124,125...|&#xA;|  5.0|(784,[216,217,218...|&#xA;|  3.0|(784,[143,144,145...|&#xA;|  6.0|(784,[72,73,74,99...|&#xA;|  1.0|(784,[151,152,153...|&#xA;|  7.0|(784,[211,212,213...|&#xA;|  2.0|(784,[151,152,153...|&#xA;|  8.0|(784,[159,160,161...|&#xA;|  6.0|(784,[100,101,102...|&#xA;|  9.0|(784,[209,210,211...|&#xA;+-----+--------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Construct a &lt;code&gt;KMeansSageMakerEstimator&lt;/code&gt;, which extends &lt;code&gt;SageMakerEstimator&lt;/code&gt;, which is a Spark &lt;code&gt;Estimator&lt;/code&gt;. You need to pass in an Amazon SageMaker-compatible IAM Role that Amazon SageMaker will use to make AWS service calls on your behalf (or configure SageMaker Spark to &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#configuring-iam-role-and-s3-buckets&#34;&gt;get this from Spark Config&lt;/a&gt;). Consult the API Documentation for a complete list of parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In this example, we are setting the &#34;k&#34; and &#34;feature_dim&#34; hyperparameters, corresponding to the number of clusters we want and to the number of dimensions in our training dataset, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#xA;// Replace this IAM Role ARN with your own.&#xA;val roleArn = &#34;arn:aws:iam::account-id:role/rolename&#34;&#xA;&#xA;val estimator = new KMeansSageMakerEstimator(&#xA;  sagemakerRole = IAMRole(roleArn),&#xA;  trainingInstanceType = &#34;ml.p2.xlarge&#34;,&#xA;  trainingInstanceCount = 1,&#xA;  endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;  endpointInitialInstanceCount = 1)&#xA;  .setK(10).setFeatureDim(784)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To train and host your model, call &lt;code&gt;fit()&lt;/code&gt; on your training &lt;code&gt;DataFrame&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val model = estimator.fit(trainingData)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What happens in this call to &lt;code&gt;fit()&lt;/code&gt;?&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;SageMaker Spark serializes your &lt;code&gt;DataFrame&lt;/code&gt; and uploads the serialized training data to S3. For the K-Means algorithm, SageMaker Spark converts the &lt;code&gt;DataFrame&lt;/code&gt; to the &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#the-amazon-record-format&#34;&gt;Amazon Record format&lt;/a&gt;. SageMaker Spark will create an S3 bucket for you that your IAM role can access if you do not provide an S3 Bucket in the constructor.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;SageMaker Spark sends a &lt;code&gt;CreateTrainingJobRequest&lt;/code&gt; to Amazon SageMaker to run a Training Job with one &lt;code&gt;p2.xlarge&lt;/code&gt; on the data in S3, configured with the values you pass in to the &lt;code&gt;SageMakerEstimator&lt;/code&gt;, and polls for completion of the Training Job. In this example, we are sending a CreateTrainingJob request to run a k-means clustering Training Job on Amazon SageMaker on serialized data we uploaded from your &lt;code&gt;DataFrame&lt;/code&gt;. When training completes, the Amazon SageMaker service puts a serialized model in an S3 bucket you own (or the default bucket created by SageMaker Spark).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After training completes, SageMaker Spark sends a &lt;code&gt;CreateModelRequest&lt;/code&gt;, a &lt;code&gt;CreateEndpointConfigRequest&lt;/code&gt;, and a &lt;code&gt;CreateEndpointRequest&lt;/code&gt; and polls for completion, each configured with the values you pass in to the SageMakerEstimator. This Endpoint will initially be backed by one &lt;code&gt;c4.xlarge&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make inferences using the Endpoint hosting our model, call &lt;code&gt;transform()&lt;/code&gt; on the &lt;code&gt;SageMakerModel&lt;/code&gt; returned by &lt;code&gt;fit()&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val transformedData = model.transform(testData)&#xA;transformedData.show&#xA;+-----+--------------------+-------------------+---------------+&#xA;|label|            features|distance_to_cluster|closest_cluster|&#xA;+-----+--------------------+-------------------+---------------+&#xA;|  5.0|(784,[152,153,154...|  1767.897705078125|            4.0|&#xA;|  0.0|(784,[127,128,129...|  1392.157470703125|            5.0|&#xA;|  4.0|(784,[160,161,162...| 1671.5711669921875|            9.0|&#xA;|  1.0|(784,[158,159,160...| 1182.6082763671875|            6.0|&#xA;|  9.0|(784,[208,209,210...| 1390.4002685546875|            0.0|&#xA;|  2.0|(784,[155,156,157...|  1713.988037109375|            1.0|&#xA;|  1.0|(784,[124,125,126...| 1246.3016357421875|            2.0|&#xA;|  3.0|(784,[151,152,153...|  1753.229248046875|            4.0|&#xA;|  1.0|(784,[152,153,154...|  978.8394165039062|            2.0|&#xA;|  4.0|(784,[134,135,161...|  1623.176513671875|            3.0|&#xA;|  3.0|(784,[123,124,125...|  1533.863525390625|            4.0|&#xA;|  5.0|(784,[216,217,218...|  1469.357177734375|            6.0|&#xA;|  3.0|(784,[143,144,145...|  1736.765869140625|            4.0|&#xA;|  6.0|(784,[72,73,74,99...|   1473.69384765625|            8.0|&#xA;|  1.0|(784,[151,152,153...|    944.88720703125|            2.0|&#xA;|  7.0|(784,[211,212,213...| 1285.9071044921875|            3.0|&#xA;|  2.0|(784,[151,152,153...| 1635.0125732421875|            1.0|&#xA;|  8.0|(784,[159,160,161...| 1436.3162841796875|            6.0|&#xA;|  6.0|(784,[100,101,102...| 1499.7366943359375|            7.0|&#xA;|  9.0|(784,[209,210,211...| 1364.6319580078125|            6.0|&#xA;+-----+--------------------+-------------------+---------------+&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this call to &lt;code&gt;transform()&lt;/code&gt;, the &lt;code&gt;SageMakerModel&lt;/code&gt; serializes chunks of the input &lt;code&gt;DataFrame&lt;/code&gt; and sends them to the Endpoint using the SageMakerRuntime &lt;code&gt;InvokeEndpoint&lt;/code&gt; API. The &lt;code&gt;SageMakerModel&lt;/code&gt; deserializes the Endpoint&#39;s responses, which contain predictions, and appends the prediction columns to the input &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example: Using SageMaker Spark with Any SageMaker Algorithm&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;SageMakerEstimator&lt;/code&gt; is an &lt;code&gt;org.apache.spark.ml.Estimator&lt;/code&gt; that trains a model on Amazon SageMaker.&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark provides several classes that extend &lt;code&gt;SageMakerEstimator&lt;/code&gt; to run particular algorithms, like &lt;code&gt;KMeansSageMakerEstimator&lt;/code&gt; to run the SageMaker-provided k-means algorithm, or &lt;code&gt;XGBoostSageMakerEstimator&lt;/code&gt; to run the SageMaker-provided XGBoost algorithm. These classes are just &lt;code&gt;SageMakerEstimator&lt;/code&gt;s with certain default values passed in. You can use SageMaker Spark with any algorithm that runs on Amazon SageMaker by creating a SageMakerEstimator.&lt;/p&gt; &#xA;&lt;p&gt;Instead of creating a KMeansSageMakerEstimator, you can create an equivalent SageMakerEstimator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val estimator = new SageMakerEstimator(&#xA;  trainingImage =&#xA;    &#34;382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1&#34;,&#xA;  modelImage =&#xA;    &#34;382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1&#34;,&#xA;  requestRowSerializer = new ProtobufRequestRowSerializer(),&#xA;  responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(),&#xA;  hyperParameters = Map(&#34;k&#34; -&amp;gt; &#34;10&#34;, &#34;feature_dim&#34; -&amp;gt; &#34;784&#34;),&#xA;  sagemakerRole = IAMRole(roleArn),&#xA;  trainingInstanceType = &#34;ml.p2.xlarge&#34;,&#xA;  trainingInstanceCount = 1,&#xA;  endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;  endpointInitialInstanceCount = 1,&#xA;  trainingSparkDataFormat = &#34;sagemaker&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;trainingImage&lt;/code&gt; identifies the Docker registry path to the training image containing your custom code. In this case, this points to the us-east-1 k-means image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;modelImage&lt;/code&gt; identifies the Docker registry path to the image containing inference code. Amazon SageMaker k-means uses the same image to train and to host trained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;requestRowSerializer&lt;/code&gt; implements &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.RequestRowSerializer&lt;/code&gt;. A &lt;code&gt;RequestRowSerializer&lt;/code&gt; serializes &lt;code&gt;org.apache.spark.sql.Row&lt;/code&gt;s in the input &lt;code&gt;DataFrame&lt;/code&gt; to send them to the model hosted in Amazon SageMaker for inference. This is passed to the SageMakerModel returned by &lt;code&gt;fit&lt;/code&gt;. In this case, we pass in a &lt;code&gt;RequestRowSerializer&lt;/code&gt; that serializes &lt;code&gt;Row&lt;/code&gt;s to the Amazon Record protobuf format. See &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#serializing-and-deserializing-for-inference&#34;&gt;Serializing and Deserializing for Inference&lt;/a&gt; for more information on how SageMaker Spark makes inferences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;responseRowDeserializer&lt;/code&gt; Implements &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.ResponseRowDeserializer&lt;/code&gt;. A &lt;code&gt;ResponseRowDeserializer&lt;/code&gt; deserializes responses containing predictions from the Endpoint back into columns in a &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;hyperParameters&lt;/code&gt; is a &lt;code&gt;Map[String, String]&lt;/code&gt; that the &lt;code&gt;trainingImage&lt;/code&gt; will use to set training hyperparameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trainingSparkDataFormat&lt;/code&gt; specifies the data format that Spark uses when uploading training data from a &lt;code&gt;DataFrame&lt;/code&gt; to S3.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SageMaker Spark needs the trainingSparkDataFormat to tell Spark how to write the DataFrame to S3 for the &lt;code&gt;trainingImage&lt;/code&gt; to train on. In this example, &#34;sagemaker&#34; tells Spark to write the data as RecordIO-encoded &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#the-amazon-record-format&#34;&gt;Amazon Records&lt;/a&gt;, but your own algorithm may take another data format. You can pass in any format that Spark supports as long as your &lt;code&gt;trainingImage&lt;/code&gt; can train using that data format, such as &#34;csv&#34;, &#34;parquet&#34;, &#34;com.databricks.spark.csv&#34;, or &#34;libsvm.&#34;&lt;/p&gt; &#xA;&lt;p&gt;SageMaker Spark also needs a &lt;code&gt;RequestRowSerializer&lt;/code&gt; to serialize Spark &lt;code&gt;Row&lt;/code&gt;s to a data format the &lt;code&gt;modelImage&lt;/code&gt; can deserialize, and a &lt;code&gt;ResponseRowDeserializer&lt;/code&gt; to deserialize responses that contain predictions from the &lt;code&gt;modelImage&lt;/code&gt; back into Spark &lt;code&gt;Row&lt;/code&gt;s. See &lt;a href=&#34;https://raw.githubusercontent.com/aws/sagemaker-spark/master/#serializing-and-deserializing-for-inference&#34;&gt;Serializing and Deserializing for Inference&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Example: Using SageMakerEstimator and SageMakerModel in a Spark Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;SageMakerEstimator&lt;/code&gt;s and &lt;code&gt;SageMakerModel&lt;/code&gt;s can be used in &lt;code&gt;Pipeline&lt;/code&gt;s. In this example, we run &lt;code&gt;org.apache.spark.ml.feature.PCA&lt;/code&gt; on our Spark cluster, then train and infer using Amazon SageMaker&#39;s K-Means on the output column from &lt;code&gt;PCA&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val pcaEstimator = new PCA()&#xA;  .setInputCol(&#34;features&#34;)&#xA;  .setOutputCol(&#34;projectedFeatures&#34;)&#xA;  .setK(50)&#xA;&#xA;val kMeansSageMakerEstimator = new KMeansSageMakerEstimator(&#xA;  sagemakerRole = IAMRole(roleArn),&#xA;  requestRowSerializer =&#xA;    new ProtobufRequestRowSerializer(featuresColumnName = &#34;projectedFeatures&#34;),&#xA;  trainingSparkDataFormatOptions = Map(&#34;featuresColumnName&#34; -&amp;gt; &#34;projectedFeatures&#34;),&#xA;  trainingInstanceType = &#34;ml.p2.xlarge&#34;,&#xA;  trainingInstanceCount = 1,&#xA;  endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;  endpointInitialInstanceCount = 1)&#xA;  .setK(10).setFeatureDim(50)&#xA;&#xA;val pipeline = new Pipeline().setStages(Array(pcaEstimator, kMeansSageMakerEstimator))&#xA;&#xA;// train&#xA;val pipelineModel = pipeline.fit(trainingData)&#xA;&#xA;val transformedData = pipelineModel.transform(testData)&#xA;transformedData.show()&#xA;&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;|label|            features|   projectedFeatures|distance_to_cluster|closest_cluster|&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;|  5.0|(784,[152,153,154...|[880.731433034386...|     1500.470703125|            0.0|&#xA;|  0.0|(784,[127,128,129...|[1768.51722024166...|      1142.18359375|            4.0|&#xA;|  4.0|(784,[160,161,162...|[704.949236329314...|  1386.246826171875|            9.0|&#xA;|  1.0|(784,[158,159,160...|[-42.328192193771...| 1277.0736083984375|            5.0|&#xA;|  9.0|(784,[208,209,210...|[374.043902028333...|   1211.00927734375|            3.0|&#xA;|  2.0|(784,[155,156,157...|[941.267714528850...|  1496.157958984375|            8.0|&#xA;|  1.0|(784,[124,125,126...|[30.2848596410594...| 1327.6766357421875|            5.0|&#xA;|  3.0|(784,[151,152,153...|[1270.14374062052...| 1570.7674560546875|            0.0|&#xA;|  1.0|(784,[152,153,154...|[-112.10792566485...|     1037.568359375|            5.0|&#xA;|  4.0|(784,[134,135,161...|[452.068280676606...| 1165.1236572265625|            3.0|&#xA;|  3.0|(784,[123,124,125...|[610.596447285397...|  1325.953369140625|            7.0|&#xA;|  5.0|(784,[216,217,218...|[142.959601818422...| 1353.4930419921875|            5.0|&#xA;|  3.0|(784,[143,144,145...|[1036.71862533658...| 1460.4315185546875|            7.0|&#xA;|  6.0|(784,[72,73,74,99...|[996.740157435754...| 1159.8631591796875|            2.0|&#xA;|  1.0|(784,[151,152,153...|[-107.26076167417...|   960.963623046875|            5.0|&#xA;|  7.0|(784,[211,212,213...|[619.771820430940...|   1245.13623046875|            6.0|&#xA;|  2.0|(784,[151,152,153...|[850.152101817161...|  1304.437744140625|            8.0|&#xA;|  8.0|(784,[159,160,161...|[370.041887230547...| 1192.4781494140625|            0.0|&#xA;|  6.0|(784,[100,101,102...|[546.674328209335...|    1277.0908203125|            2.0|&#xA;|  9.0|(784,[209,210,211...|[-29.259112927426...| 1245.8182373046875|            6.0|&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;requestRowSerializer = new ProtobufRequestRowSerializer(featuresColumnName = &#34;projectedFeatures&#34;)&lt;/code&gt; tells the &lt;code&gt;SageMakerModel&lt;/code&gt; returned by &lt;code&gt;fit()&lt;/code&gt; to infer on the features in the &#34;projectedFeatures&#34; column&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trainingSparkDataFormatOptions = Map(&#34;featuresColumnName&#34; -&amp;gt; &#34;projectedFeatures&#34;)&lt;/code&gt; tells the &lt;code&gt;SageMakerProtobufWriter&lt;/code&gt; that Spark is using to write the &lt;code&gt;DataFrame&lt;/code&gt; as format &#34;sagemaker&#34; to serialize the &#34;projectedFeatures&#34; column when writing Amazon Records for training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example: Using Multiple SageMakerEstimators and SageMakerModels in a Spark Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;We can use multiple &lt;code&gt;SageMakerEstimator&lt;/code&gt;s and &lt;code&gt;SageMakerModel&lt;/code&gt;s in a pipeline. Here, we use SageMaker&#39;s PCA algorithm to reduce a dataset with 50 dimensions to a dataset with 20 dimensions, then use SageMaker&#39;s K-Means algorithm to train on the 20-dimension data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val pcaEstimator = new PCASageMakerEstimator(sagemakerRole = IAMRole(sagemakerRole),&#xA;  trainingInstanceType = &#34;ml.p2.xlarge&#34;,&#xA;  trainingInstanceCount = 1,&#xA;  endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;  endpointInitialInstanceCount = 1&#xA;  responseRowDeserializer = new PCAProtobufResponseRowDeserializer(&#xA;    projectionColumnName = &#34;projectionDim20&#34;),&#xA;  trainingInputS3DataPath = S3DataPath(trainingBucket, inputPrefix),&#xA;  trainingOutputS3DataPath = S3DataPath(trainingBucket, outputPrefix),&#xA;  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM)&#xA;  .setNumComponents(20).setFeatureDim(50)&#xA;&#xA;val kmeansEstimator = new KMeansSageMakerEstimator(sagemakerRole = IAMRole(sagemakerRole),&#xA;  trainingInstanceType = &#34;ml.p2.xlarge&#34;,&#xA;  trainingInstanceCount = 1,&#xA;  endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;  endpointInitialInstanceCount = 1&#xA;  trainingSparkDataFormatOptions = Map(&#34;featuresColumnName&#34; -&amp;gt; &#34;projectionDim20&#34;),&#xA;  requestRowSerializer = new ProtobufRequestRowSerializer(&#xA;    featuresColumnName = &#34;projectionDim20&#34;),&#xA;  responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(),&#xA;  trainingInputS3DataPath = S3DataPath(trainingBucket, inputPrefix),&#xA;  trainingOutputS3DataPath = S3DataPath(trainingBucket, outputPrefix),&#xA;  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM)&#xA;  .setK(10).setFeatureDim(20)&#xA;&#xA;val pipeline = new Pipeline().setStages(Array(pcaEstimator, kmeansEstimator))&#xA;&#xA;val model = pipeline.fit(dataset)&#xA;&#xA;// For expediency, transforming the training dataset:&#xA;val transformedData = model.transform(dataset)&#xA;transformedData.show()&#xA;&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;|label|            features|     projectionDim20|distance_to_cluster|closest_cluster|&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;|  1.0|[-0.7927307,-11.2...|[5.50362682342529...|  45.03189468383789|            1.0|&#xA;|  1.0|[-3.762671,-5.853...|[-2.1558122634887...|  41.79889678955078|            1.0|&#xA;|  1.0|[-2.0988898,-2.40...|[4.53881502151489...| 50.824703216552734|            1.0|&#xA;|  1.0|[-2.81075,-3.6481...|[0.97894239425659...|  52.78211975097656|            1.0|&#xA;|  1.0|[-2.14356,-4.0369...|[2.25758934020996...|  48.99141311645508|            1.0|&#xA;|  1.0|[-5.3773708,-15.3...|[-3.2523036003112...|  21.99374771118164|            1.0|&#xA;|  1.0|[-1.0369565,-16.5...|[-17.643878936767...| 29.127044677734375|            3.0|&#xA;|  1.0|[-2.019725,-3.226...|[1.41068196296691...|   51.7830696105957|            1.0|&#xA;|  1.0|[-4.3821997,-0.98...|[-0.8335087299346...| 53.921058654785156|            1.0|&#xA;|  1.0|[-7.075208,-34.31...|[11.4329795837402...|  35.12031173706055|            3.0|&#xA;|  1.0|[-3.90454,-4.8401...|[-1.4304646253585...|  50.00594711303711|            1.0|&#xA;|  1.0|[0.9607103,-13.50...|[1.13785743713378...|  28.71956443786621|            1.0|&#xA;|  1.0|[-4.5025017,-15.2...|[2.66747045516967...| 25.419822692871094|            1.0|&#xA;|  1.0|[0.041773,-27.148...|[7.58121681213378...| 30.303693771362305|            3.0|&#xA;|  1.0|[-10.1477266,-39....|[-12.086886405944...|   35.9030647277832|            2.0|&#xA;|  1.0|[-3.09143,-6.4892...|[1.79180252552032...|  39.34271240234375|            1.0|&#xA;|  1.0|[-13.5285917,-32....|[7.62783145904541...| 35.040035247802734|            2.0|&#xA;|  1.0|[-4.189806,-16.04...|[1.41141772270202...| 25.123626708984375|            1.0|&#xA;|  1.0|[-12.77831508,-62...|[0.11281073093414...|  63.91242599487305|            2.0|&#xA;|  1.0|[-9.3934507,-12.5...|[-9.4945802688598...| 20.913305282592773|            1.0|&#xA;+-----+--------------------+--------------------+-------------------+---------------+&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;responseRowDeserializer = new PCAProtobufResponseRowDeserializer( projectionColumnName = &#34;projectionDim20&#34;)&lt;/code&gt; tells the &lt;code&gt;SageMakerModel&lt;/code&gt; attached to the PCA endpoint to deserialize responses (which contain the lower-dimensional projections of the features vectors) into the column named &#34;projectionDim20&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM&lt;/code&gt; tells the &lt;code&gt;SageMakerEstimator&lt;/code&gt; to delay SageMaker Endpoint creation until it is needed to transform a &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trainingSparkDataFormatOptions = Map(&#34;featuresColumnName&#34; -&amp;gt; &#34;projectionDim20&#34;), requestRowSerializer = new ProtobufRequestRowSerializer( featuresColumnName = &#34;projectionDim20&#34;)&lt;/code&gt; these lines tell the &lt;code&gt;KMeansSageMakerEstimator&lt;/code&gt; to respectively train and infer on the features in the &#34;projectionDim20&#34; column.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example: Creating a SageMakerModel&lt;/h2&gt; &#xA;&lt;p&gt;SageMaker Spark supports attaching &lt;code&gt;SageMakerModel&lt;/code&gt;s to an existing SageMaker endpoint, or to an Endpoint created by reference to model data in S3, or to a previously completed Training Job.&lt;/p&gt; &#xA;&lt;p&gt;This allows you to use SageMaker Spark just for model hosting and inference on Spark-scale &lt;code&gt;DataFrame&lt;/code&gt;s without running a new Training Job.&lt;/p&gt; &#xA;&lt;h3&gt;SageMakerModel From an Endpoint&lt;/h3&gt; &#xA;&lt;p&gt;You can attach a &lt;code&gt;SageMakerModel&lt;/code&gt; to an endpoint that has already been created. Supposing an endpoint with name &#34;my-endpoint-name&#34; is already in service and hosting a SageMaker K-Means model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val model = SageMakerModel&#xA;  .fromEndpoint(endpointName = &#34;my-endpoint-name&#34;,&#xA;                requestRowSerializer = new ProtobufRequestRowSerializer(&#xA;                  featuresColumnName = &#34;MyFeaturesColumn&#34;),&#xA;                responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(&#xA;                  distanceToClusterColumnName = &#34;DistanceToCluster&#34;,&#xA;                  closestClusterColumnName = &#34;ClusterLabel&#34;&#xA;                ))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This &lt;code&gt;SageMakerModel&lt;/code&gt; will, upon a call to &lt;code&gt;transform()&lt;/code&gt;, serialize the column named &#34;MyFeaturesColumn&#34; for inference, and append the columns &#34;DistanceToCluster&#34; and &#34;ClusterLabel&#34; to the &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SageMakerModel From Model Data in S3&lt;/h3&gt; &#xA;&lt;p&gt;You can create a SageMakerModel and an Endpoint by referring directly to your model data in S3:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val model = SageMakerModel&#xA;  .fromModelS3Path(modelPath = &#34;s3://my-model-bucket/my-model-data/model.tar.gz&#34;,&#xA;                   modelExecutionRoleARN = &#34;arn:aws:iam::account-id:role/rolename&#34;&#xA;                   modelImage = 382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1&#34;,&#xA;                   endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;                   endpointInitialInstanceCount = 1&#xA;                   requestRowSerializer = new ProtobufRequestRowSerializer(),&#xA;                   responseRowDeserializer = new KMeansProtobufResponseRowDeserializer()&#xA;                  )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SageMakerModel From a Previously Completed Training Job&lt;/h3&gt; &#xA;&lt;p&gt;You can create a SageMakerModel and an Endpoint by referring to a previously-completed training job:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val model = SageMakerModel&#xA;  .fromTrainingJob(trainingJobName = &#34;my-training-job-name&#34;,&#xA;                   modelExecutionRoleARN = &#34;arn:aws:iam::account-id:role/rolename&#34;&#xA;                   modelImage = 382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1&#34;,&#xA;                   endpointInstanceType = &#34;ml.c4.xlarge&#34;,&#xA;                   endpointInitialInstanceCount = 1&#xA;                   requestRowSerializer = new ProtobufRequestRowSerializer(),&#xA;                   responseRowDeserializer = new KMeansProtobufResponseRowDeserializer()&#xA;                  )&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example: Tearing Down Amazon SageMaker Endpoints&lt;/h2&gt; &#xA;&lt;p&gt;SageMaker Spark provides a utility for deleting Endpoints created by a SageMakerModel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val sagemakerClient = AmazonSageMakerClientBuilder.defaultClient&#xA;val cleanup = new SageMakerResourceCleanup(sagemakerClient)&#xA;cleanup.deleteResources(model.getCreatedResources)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuring an IAM Role&lt;/h2&gt; &#xA;&lt;p&gt;SageMaker Spark allows you to add your IAM Role ARN to your Spark Config so that you don&#39;t have to keep passing in &lt;code&gt;IAMRole(&#34;arn:aws:iam::account-id:role/rolename&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Add an entry to your Spark Config with key &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.sagemakerrole&lt;/code&gt; whose value is your Amazon SageMaker-compatible IAM Role. &lt;code&gt;SageMakerEstimator&lt;/code&gt; will look for this role if it is not supplied in the constructor.&lt;/p&gt; &#xA;&lt;h2&gt;SageMaker Spark: In-Depth&lt;/h2&gt; &#xA;&lt;h3&gt;The Amazon Record format&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;KMeansSageMakerEstimator&lt;/code&gt;, &lt;code&gt;PCASageMakerEstimator&lt;/code&gt;, and &lt;code&gt;LinearLearnerSageMakerEstimator&lt;/code&gt; all serialize &lt;code&gt;DataFrame&lt;/code&gt;s to the Amazon Record protobuf format with each Record encoded in &lt;a href=&#34;https://mxnet.incubator.apache.org/architecture/note_data_loading.html&#34;&gt;RecordIO&lt;/a&gt;. They do this by passing in &#34;sagemaker&#34; to the &lt;code&gt;trainingSparkDataFormat&lt;/code&gt; constructor argument, which configures Spark to use the &lt;code&gt;SageMakerProtobufWriter&lt;/code&gt; to serialize Spark &lt;code&gt;DataFrame&lt;/code&gt;s.&lt;/p&gt; &#xA;&lt;p&gt;Writing a &lt;code&gt;DataFrame&lt;/code&gt; using the &#34;sagemaker&#34; format serializes a column named &#34;label&#34;, expected to contain &lt;code&gt;Double&lt;/code&gt;s, and a column named &#34;features&#34;, expected to contain a Sparse or Dense &lt;code&gt;org.apache.mllib.linalg.Vector&lt;/code&gt;. If the features column contains a &lt;code&gt;SparseVector&lt;/code&gt;, SageMaker Spark sparsely-encodes the &lt;code&gt;Vector&lt;/code&gt; into the Amazon Record. If the features column contains a &lt;code&gt;DenseVector&lt;/code&gt;, SageMaker Spark densely-encodes the &lt;code&gt;Vector&lt;/code&gt; into the Amazon Record.&lt;/p&gt; &#xA;&lt;p&gt;You can choose which columns the &lt;code&gt;SageMakerEstimator&lt;/code&gt; chooses as its &#34;label&#34; and &#34;features&#34; columns by passing in a &lt;code&gt;trainingSparkDataFormatOptions&lt;/code&gt; &lt;code&gt;Map[String, String]&lt;/code&gt; with keys &#34;labelColumnName&#34; and &#34;featuresColumnName&#34; and with values corresponding to the names of your chosen label and features columns.&lt;/p&gt; &#xA;&lt;p&gt;You can also write Amazon Records using SageMaker Spark by using the &#34;sagemaker&#34; format directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;myDataFrame.write&#xA;    .format(&#34;sagemaker&#34;)&#xA;    .option(&#34;labelColumnName&#34;, &#34;myLabelColumn&#34;)&#xA;    .option(&#34;featuresColumnName&#34;, &#34;myFeaturesColumn&#34;)&#xA;    .save(&#34;s3://my-s3-bucket/my-s3-prefix&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;SageMakerEstimator&lt;/code&gt; deletes the RecordIO-encoded Amazon Records in S3 following training on Amazon SageMaker. You can choose to allow the data to persist in S3 by passing in &lt;code&gt;deleteStagingDataAfterTraining = true&lt;/code&gt; to &lt;code&gt;SageMakerEstimator&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aws.amazon.com/sagemaker/latest/dg/cdf-training.html&#34;&gt;AWS Documentation on Amazon Records&lt;/a&gt; for more information on Amazon Records.&lt;/p&gt; &#xA;&lt;h3&gt;Serializing and Deserializing for Inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;SageMakerEstimator.fit()&lt;/code&gt; returns a &lt;code&gt;SageMakerModel&lt;/code&gt;, which transforms a &lt;code&gt;DataFrame&lt;/code&gt; by calling &lt;code&gt;InvokeEndpoint&lt;/code&gt; on an Amazon SageMaker Endpoint. &lt;code&gt;InvokeEndpointRequest&lt;/code&gt;s carry serialized &lt;code&gt;Row&lt;/code&gt;s as their payload.&lt;code&gt;Row&lt;/code&gt;s in the &lt;code&gt;DataFrame&lt;/code&gt; are serialized for predictions against an Endpoint using a &lt;code&gt;RequestRowSerializer&lt;/code&gt;. Responses from an Endpoint containing predictions are deserialized into Spark &lt;code&gt;Row&lt;/code&gt;s and appended as columns in a &lt;code&gt;DataFrame&lt;/code&gt; using a &lt;code&gt;ResponseRowDeserializer.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Internally, &lt;code&gt;SageMakerModel.transform&lt;/code&gt; calls &lt;code&gt;mapPartitions&lt;/code&gt; to distribute the work of serializing Spark &lt;code&gt;Row&lt;/code&gt;s, constructing and sending &lt;code&gt;InvokeEndpointRequest&lt;/code&gt;s to an Endpoint, and deserializing &lt;code&gt;InvokeEndpointResponse&lt;/code&gt;s across a Spark cluster. Because each &lt;code&gt;InvokeEndpointRequest&lt;/code&gt; can carry only 5MB, each Spark partition creates a &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator&lt;/code&gt; to iterate over its partition, sending prediction requests to the Endpoint in 5MB increments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;RequestRowSerializer.serializeRow()&lt;/code&gt; converts a &lt;code&gt;Row&lt;/code&gt; to an &lt;code&gt;Array[Byte]&lt;/code&gt;. The &lt;code&gt;RequestBatchIterator&lt;/code&gt; appends these byte arrays to form the request body of an &lt;code&gt;InvokeEndpointRequest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, the &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.ProtobufRequestRowSerializer&lt;/code&gt; creates one RecordIO-encoded Amazon Record per input row by serializing the &#34;features&#34; column in each row, and wrapping each Amazon Record in the RecordIO header.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ResponseRowDeserializer.deserializeResponse()&lt;/code&gt; converts an &lt;code&gt;Array[Byte]&lt;/code&gt; containing predictions from an Endpoint to an &lt;code&gt;Iterator[Row]&lt;/code&gt;to appends columns containing these predictions to the &lt;code&gt;DataFrame&lt;/code&gt; being transformed by the &lt;code&gt;SageMakerModel&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For comparison, SageMaker&#39;s XGBoost uses LibSVM-formatted data for inference (as well as training), and responds with a comma-delimited list of predictions. Accordingly, SageMaker Spark uses &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.LibSVMRequestRowSerializer&lt;/code&gt; to serialize rows into LibSVM-formatted data, and uses &lt;code&gt;com.amazonaws.services.sagemaker.sparksdk.transformation.XGBoostCSVResponseRowDeserializer&lt;/code&gt; to deserialize the response into a column of predictions.&lt;/p&gt; &#xA;&lt;p&gt;To support your own model image&#39;s data formats for inference, you can implement your own &lt;code&gt;RequestRowSerializer&lt;/code&gt; and &lt;code&gt;ResponseRowDeserializer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SageMaker Spark is licensed under &lt;a href=&#34;https://github.com/aws/sagemaker-spark/LICENSE.txt&#34;&gt;Apache-2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snowplow-incubator/snowplow-micro</title>
    <updated>2024-10-21T01:34:10Z</updated>
    <id>tag:github.com,2024-10-21:/snowplow-incubator/snowplow-micro</id>
    <link href="https://github.com/snowplow-incubator/snowplow-micro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Standalone application to automate testing of trackers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Snowplow Micro&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/snowplow/snowplow-micro&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/snowplow/snowplow-micro?sort=semver&#34; alt=&#34;Docker Image Version (latest semver)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/snowplow/snowplow-micro&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/snowplow/snowplow-micro&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-micro/actions&#34;&gt;&lt;img src=&#34;https://github.com/snowplow-incubator/snowplow-micro/actions/workflows/test.yml/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.snowplow.io/limited-use-license-1.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Snowplow--Limited--Use-blue.svg?style=flat&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Snowplow Micro is a lightweight version of the Snowplow pipeline. Its great for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting familiar with Snowplow&lt;/li&gt; &#xA; &lt;li&gt;Debugging and testing, including automated testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Just like a real Snowplow pipeline, Micro receives, validates and enriches events sent by your tracking code.&lt;/p&gt; &#xA;&lt;p&gt;Learn more in the documentation: &lt;a href=&#34;https://docs.snowplow.io/docs/getting-started-with-micro/basic-usage/&#34;&gt;https://docs.snowplow.io/docs/getting-started-with-micro/basic-usage/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Maintainer quick start&lt;/h2&gt; &#xA;&lt;p&gt;First, install &lt;a href=&#34;https://git-scm.com/&#34;&gt;Git&lt;/a&gt;, &lt;a href=&#34;https://www.scala-sbt.org/&#34;&gt;sbt&lt;/a&gt; and &lt;a href=&#34;https://www.npmjs.com/&#34;&gt;npm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then clone the repository and publish the Collector dependency locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;git clone git@github.com:snowplow-incubator/snowplow-micro.git&#xA;cd snowplow-micro&#xA;&#xA;git clone --branch 3.2.0 --depth 1 git@github.com:snowplow/stream-collector.git&#xA;cd stream-collector&#xA;sbt +publishLocal &amp;amp;&amp;amp; cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build a Docker image for local testing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ui&#xA;npm build &amp;amp;&amp;amp; cd ..&#xA;sbt docker:publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the Docker image name in the output.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2019-present Snowplow Analytics Ltd. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;https://docs.snowplow.io/limited-use-license-1.0&#34;&gt;Snowplow Limited Use License Agreement&lt;/a&gt;. &lt;em&gt;(If you are uncertain how it applies to your use case, check our answers to &lt;a href=&#34;https://docs.snowplow.io/docs/contributing/limited-use-license-faq/&#34;&gt;frequently asked questions&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>