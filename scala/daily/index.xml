<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-08T01:41:03Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>joomcode/trace-analysis</title>
    <updated>2023-11-08T01:41:03Z</updated>
    <id>tag:github.com,2023-11-08:/joomcode/trace-analysis</id>
    <link href="https://github.com/joomcode/trace-analysis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Library for performance bottleneck detection and optimization efficiency prediction&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;trace-analysis&lt;/h1&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;trace-analysis&lt;/code&gt; is a library for performance bottleneck detection and optimization efficiency prediction.&lt;/p&gt; &#xA;&lt;p&gt;Given dataframe &lt;a href=&#34;https://github.com/opentracing/specification/raw/master/specification.md&#34;&gt;OpenTracing&lt;/a&gt;-compatible trace-analysis calculates latency distribution among all encountered spans.&lt;/p&gt; &#xA;&lt;p&gt;Also, trace-analysis allows to simulate optimization effects on historical traces, so you can estimate optimization potential before implementation.&lt;/p&gt; &#xA;&lt;p&gt;Optimization simulation is a key feature of this library but as it requires using sophisticated tree processing algorithms (see &lt;a href=&#34;https://raw.githubusercontent.com/joomcode/trace-analysis/main/#optimization-analysis-explained&#34;&gt;Optimization Analysis Explained&lt;/a&gt;) we have to deal with recursive data structures during Spark job execution.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;The latest release is available on Maven Central. Currently, we support &lt;a href=&#34;https://search.maven.org/artifact/com.joom.tracing/trace-analysis_2.12&#34;&gt;Scala 2.12&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;implementation group: &#39;com.joom.tracing&#39;, name: &#39;trace-analysis_2.12&#39;, version: &#39;0.1.0&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and &lt;a href=&#34;https://search.maven.org/artifact/com.joom.tracing/trace-analysis_2.13&#34;&gt;Scala 2.13&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;implementation group: &#39;com.joom.tracing&#39;, name: &#39;trace-analysis_2.13&#39;, version: &#39;0.1.0&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The most common approach to analyze Jaeger traces is to use Jaeger UI. But this approach has many issues such as&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Traces may become very long, and it&#39;s difficult to detect latency dominators without special tooling.&lt;/li&gt; &#xA; &lt;li&gt;On higher percentiles (p95, p99) there may be many operations with high latency (database/cache queries, third party service requests, etc.). But it is not clear how to estimate total impact of each such operation on the latency percentile.&lt;/li&gt; &#xA; &lt;li&gt;Jaeger UI does not let us analyze subtraces. We have to open the whole trace and then look for operations of interest.&lt;/li&gt; &#xA; &lt;li&gt;There is no tooling to estimate optimizations effect before implementation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This library solves all the issues mentioned above by analyzing big corpora of Jaeger traces using Spark capabilities.&lt;/p&gt; &#xA;&lt;p&gt;Usage of this library can be separated into two steps.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Hot spot analysis. On this stage we investigate which operations take most of the time.&lt;/li&gt; &#xA; &lt;li&gt;Potential optimization analysis. On this stage we simulate potential optimizations and estimate their effect on historical traces.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Hot Spot Analysis&lt;/h2&gt; &#xA;&lt;p&gt;First of all we should read the corpus of traces we want to analyze. For example, it may be all the traces about &lt;code&gt;HTTP GET /dispatch&lt;/code&gt; request.&lt;/p&gt; &#xA;&lt;p&gt;For example, we can read test span dump from file &lt;code&gt;lib/src/test/resources/test_data.json&lt;/code&gt; (spans created with &lt;a href=&#34;https://github.com/jaegertracing/jaeger/raw/main/examples/hotrod/README.md&#34;&gt;hotrod&lt;/a&gt;) into variable &lt;code&gt;spanDF&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SparkSession&#xA;import com.joom.trace.analysis.spark.spanSchema&#xA;&#xA;val spark = SparkSession.builder()&#xA;  .master(&#34;local[1]&#34;)&#xA;  .appName(&#34;Test&#34;)&#xA;  .getOrCreate()&#xA;&#xA;&#xA;val spanDF = spark&#xA;  .read&#xA;  .schema(spanSchema)&#xA;  .json(&#34;lib/src/test/resources/test_data.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then we have to define queries to particular operations inside this trace corpus. It may be root operation (&lt;code&gt;HTTP GET /dispatch&lt;/code&gt;) or we may want to see latency distribution inside heavy subtraces. Let&#39;s assume that we want to check 2 operations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Root operation &lt;code&gt;HTTP GET /dispatch&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Heavy subtrace &lt;code&gt;FindDriverIDs&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;So we create &lt;code&gt;TraceAnalysisQuery&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val query = HotSpotAnalysis.TraceAnalysisQuery(&#xA;   TraceSelector(&#34;HTTP GET /dispatch&#34;),&#xA;   Seq(&#xA;      OperationAnalysisQuery(&#34;HTTP GET /dispatch&#34;),&#xA;      OperationAnalysisQuery(&#34;FindDriverIDs&#34;)&#xA;   )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The only thing left is to calculate span durations distribution&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val durations = HotSpotAnalysis.getSpanDurations(spanDF, Seq(query))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here &lt;code&gt;durations&lt;/code&gt; is a map of dataframes with scheme&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(&#xA;  &#34;operation_name&#34;: name of the operation; &#xA;  &#34;duration&#34;: total operations duration [microseconds], &#xA;  &#34;count&#34;: total operation count&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joomcode/trace-analysis/main/resources/hot_spot_result.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Further we can, for example, investigate the longest or most frequent operations.&lt;/p&gt; &#xA;&lt;h2&gt;Optimization Analysis&lt;/h2&gt; &#xA;&lt;p&gt;Now, when we checked latency distribution among different spans we may want to optimize particular heavy operation. For example &lt;code&gt;FindDriverIDs&lt;/code&gt;. But it may take weeks and even months to refactor existing code base. So it will be very convenient if we could estimate optimization impact before optimization implementation.&lt;/p&gt; &#xA;&lt;p&gt;And comes &lt;code&gt;OptimizationAnalysis&lt;/code&gt;!&lt;/p&gt; &#xA;&lt;p&gt;But before getting our hands dirty we should catch basic idea behind this optimization potential estimation.&lt;/p&gt; &#xA;&lt;h3&gt;Optimization Analysis Explained&lt;/h3&gt; &#xA;&lt;p&gt;Every Opentracing trace is a tree-like structure with spans in the nodes. Each span has a name, start time, end time and (except root spans) reference to its parent. So the basic idea is to artificially change duration of spans matching some condition and get a new span duration.&lt;/p&gt; &#xA;&lt;p&gt;But when calculating updated trace duration we should take into account order of span execution (sequential/parallel). There can be two extreme cases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Sequential execution - total trace duration will be reduced by the same absolute value as optimized span.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joomcode/trace-analysis/main/resources/optimization_seq.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Parallel execution (non-critical path) - total trace duration will be unchanged because our optimization does not affect critical path.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joomcode/trace-analysis/main/resources/optimization_parallel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To handle all this cases correctly we need information about span execution order; that&#39;s why in the code we deal with trees not with Spark Rows.&lt;/p&gt; &#xA;&lt;p&gt;Now, when we grasped basic understanding of optimization analysis algorithm (it is mostly implemented inside &lt;code&gt;SpanModifier&lt;/code&gt; class) let&#39;s apply it to our dataset.&lt;/p&gt; &#xA;&lt;h3&gt;Optimization Analysis Applied&lt;/h3&gt; &#xA;&lt;p&gt;Like before we need to load historical traces. We have to preprocess them with &lt;code&gt;getTraceDataset&lt;/code&gt; method and store in variable &lt;code&gt;traceDS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val traceDS = getTraceDataset(spanDF)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Suppose we want to simulate effect of 50% latency decrease of &lt;code&gt;FindDriverIDs&lt;/code&gt; operation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val optimization = FractionOptimization(&#34;FindDriverIDs&#34;, 0.5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Particularly we are interested in p50 and p90&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val percentiles = Seq(Percentile(&#34;p50&#34;, 0.5), Percentile(&#34;p90&#34;, 0.9))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we create optimized durations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val optimizedDurations = OptimizationAnalysis.calculateOptimizedTracesDurations(&#xA;  traceDS,&#xA;  Seq(optimization),&#xA;  percentiles&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here &lt;code&gt;optimizedDurations&lt;/code&gt; is a dataframe with schema.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(&#xA;   &#34;optimization_name&#34;: name of the optimization,&#xA;   &#34;percentile&#34;: percentile of the request latency&#xA;   &#34;duration&#34;: latency [microseconds] &#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is a special optimization with name &lt;code&gt;&#34;none&#34;&lt;/code&gt; which indicates non-optimized latency&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joomcode/trace-analysis/main/resources/optimization_analysis_result.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now we have estimation of our optimization potential!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alduk/ScalazTest</title>
    <updated>2023-11-08T01:41:03Z</updated>
    <id>tag:github.com,2023-11-08:/alduk/ScalazTest</id>
    <link href="https://github.com/alduk/ScalazTest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Samples from &lt;a href=&#34;http://eed3si9n.com/learning-scalaz-day1&#34;&gt;http://eed3si9n.com/learning-scalaz-day1&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>