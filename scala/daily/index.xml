<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-11T01:37:33Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cognitedata/cognite-sdk-scala</title>
    <updated>2022-11-11T01:37:33Z</updated>
    <id>tag:github.com,2022-11-11:/cognitedata/cognite-sdk-scala</id>
    <link href="https://github.com/cognitedata/cognite-sdk-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cognite Scala SDK&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cognite Scala SDK&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Under development, not recommended for production use cases&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the Cognite Scala SDK for developers working with &lt;a href=&#34;https://cognite.com/cognite/cognite-data-fusion/developers/&#34;&gt;Cognite Data Fusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://mvnrepository.com/artifact/com.cognite/cognite-sdk-scala&#34;&gt;Maven Repository&lt;/a&gt; to see the available versions, and how to include it as a dependency for sbt, Maven, Gradle, and other build tools.&lt;/p&gt; &#xA;&lt;p&gt;Authentication is specified using an implicit &lt;code&gt;Auth&lt;/code&gt; parameter, which by default looks for an API key to use in the &lt;code&gt;COGNITE_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;GenericClient&lt;/code&gt; requires a backend for &lt;a href=&#34;https://github.com/softwaremill/sttp&#34;&gt;sttp&lt;/a&gt;, which can use any effects wrapper (for example, &lt;a href=&#34;https://github.com/typelevel/cats-effect&#34;&gt;cats-effect&lt;/a&gt;, but users who do not want to use an effect wrapper can use &lt;code&gt;Client&lt;/code&gt; to easily create a client using an identity effect.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Create a simple client, specifying an application name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.cognite.sdk.scala.v1._&#xA;&#xA;val c = Client(&#34;scala-sdk-examples&#34;, &#34;https://api.cognitedata.com&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a client using the cats-effect &lt;code&gt;IO&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.cognite.sdk.scala.v1._&#xA;import com.cognite.sdk.scala.common._&#xA;import java.util.concurrent.Executors&#xA;import scala.concurrent._&#xA;import cats.effect.IO&#xA;import com.softwaremill.sttp.asynchttpclient.cats.AsyncHttpClientCatsBackend&#xA;&#xA;implicit val cs = IO.contextShift(ExecutionContext.fromExecutor(Executors.newCachedThreadPool()))&#xA;implicit val sttpBackend = AsyncHttpClientCatsBackend[cats.effect.IO]()&#xA;val auth = ApiKeyAuth(your API key comes here) // you can get the key at https://openindustrialdata.com/&#xA;val c = new GenericClient[IO, Nothing](&#34;scala-sdk-examples&#34;, projectName=&#34;publicdata&#34;, auth)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following examples will use &lt;code&gt;Client&lt;/code&gt; with the identity effect. Add &lt;code&gt;.unsafeRunSync&lt;/code&gt; or similar to the end of the examples if you use &lt;code&gt;IO&lt;/code&gt; or another effect wrapper.&lt;/p&gt; &#xA;&lt;h3&gt;Getting started with Ammonite&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://ammonite.io/&#34;&gt;Ammonite&lt;/a&gt; for a better interactive Scala environment, and load the SDK directly like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import $ivy.`com.cognite::cognite-sdk-scala:0.1.2`&#xA;import com.cognite.sdk.scala.v1._&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Discover time series&lt;/h3&gt; &#xA;&lt;p&gt;List and filter endpoints use Stream from &lt;a href=&#34;https://github.com/functional-streams-for-scala/fs2&#34;&gt;fs2&lt;/a&gt;, which loads more data as required. You can use &lt;code&gt;.compile.toList&lt;/code&gt; to convert it to a list, but note that this could end up fetching a lot of data unless you limit it, for example by using &lt;code&gt;.take(25)&lt;/code&gt; as we do here.&lt;/p&gt; &#xA;&lt;p&gt;For the next examples, you will need to supply ids for the time series that you want to retrieve. You can find some ids by listing the available time series:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val timeSeriesList = c.timeSeries.list().take(25).compile.toList&#xA;val timeSeriesId = timeSeriesList.head.id&#xA;print(c.timeSeries.retrieveById(timeSeriesId))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieve data points&lt;/h3&gt; &#xA;&lt;p&gt;If you have a time series ID you can retrieve its data points:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val dataPoints = c.dataPoints.queryById(&#xA;      timeSeriesId,&#xA;      inclusiveStart=Instant.ofEpochMilli(0),&#xA;      exclusiveEnd=Instant.now())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also possible to query aggregate values using a time series ID. Possible aggregate values can be found in the API &lt;a href=&#34;https://docs.cognite.com/api/v1/#operation/getMultiTimeSeriesDatapoints&#34;&gt;documentation&lt;/a&gt; You must also specify a granularity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val aggregates = Seq(&#34;count&#34;, &#34;average&#34;, &#34;max&#34;)&#xA;c.dataPoints.queryAggregatesById(timeSeriesId, Instant.ofEpochMilli(0L), Instant.now(), granularity=&#34;1d&#34;, aggregates)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need only the last data point for a time series or group of timeseries, you can retrieve these using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val latestPoints: Map[Long, Option[DataPoint]] = c.dataPoints.getLatestDataPointsByIds(Seq(timeSeriesId))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This returns a map from each of the time series IDs specified in the function call to the latest data point for that time series, if it exists, or None if it does not.&lt;/p&gt; &#xA;&lt;p&gt;There are analogous functions available to execute these queries using external IDs instead of IDs and returning string valued data points rather than numeric valued data points.&lt;/p&gt; &#xA;&lt;h3&gt;Insert and Delete Data&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to insert and delete numeric data points for specified time series. To insert data, you must specify the time series for which to insert data and the data points to insert:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val testDataPoints = (startTime to endTime by 1000).map(t =&amp;gt; DataPoint(Instant.ofEpochMilli(t), math.random()))&#xA;c.dataPoints.insertById(timeSeriesId, testDataPoints)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To delete data, you must specify the time series and the range of times for which to delete data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;c.dataPoints.deleteRangeById(timeSeriesId, Instant.ofEpochMilli(0L), Instant.now())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create an asset hierarchy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val root = AssetCreate(name = &#34;root&#34;, externalId = Some(&#34;1&#34;))&#xA;val child = AssetCreate(name = &#34;child&#34;, externalId = Some(&#34;2&#34;), parentExternalId = Some(&#34;1&#34;))&#xA;val descendant = AssetCreate(name = &#34;descendant&#34;, externalId = Some(&#34;3&#34;), parentExternalId = Some(&#34;2&#34;))&#xA;val createdAssets = c.assets.create(Seq(root, child, descendant))&#xA;&#xA;// clean up the assets we created&#xA;c.assets.deleteByExternalIds(Seq(&#34;1&#34;, &#34;2&#34;, &#34;3&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Print an asset subtree using filter&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def children(parents: Seq[Asset]): List[Asset] =&#xA;  c.assets.filter(AssetsFilter(parentIds = Some(parents.map(_.id)))).compile.toList&#xA;&#xA;def printSubTree(parents: Seq[Asset], prefix: String = &#34;&#34;): Unit = {&#xA;  if (parents.nonEmpty) {&#xA;    val assetsUnderThisLevel = children(parents)&#xA;    parents.foreach { p =&amp;gt;&#xA;      println(prefix + p.name)&#xA;      val assetsUnderThisAsset = assetsUnderThisLevel.filter(_.parentId.exists(pid =&amp;gt; pid == p.id))&#xA;      printSubTree(assetsUnderThisAsset, prefix + &#34;  &#34;)&#xA;    }&#xA;  }&#xA;}&#xA;&#xA;val rootId = 2780934754068396L&#xA;printSubTree(c.assets.retrieveByIds(Seq(rootId)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Find all events in an asset subtree&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def eventsForAssets(parents: Seq[Asset]): List[Event] =&#xA;  c.events.filter(EventsFilter(assetIds = Some(parents.map(_.id)))).compile.toList&#xA;&#xA;def eventsInSubTree(parents: Seq[Asset]): Seq[Event] = {&#xA;  if (parents.nonEmpty) {&#xA;    val assetsUnderThisLevel = children(parents)&#xA;    eventsForAssets(parents) ++ parents.flatMap { p =&amp;gt;&#xA;      val assetsUnderThisAsset = assetsUnderThisLevel.filter(_.parentId.exists(pid =&amp;gt; pid == p.id))&#xA;      eventsInSubTree(assetsUnderThisAsset)&#xA;    }&#xA;  } else {&#xA;    Seq.empty[Event]&#xA;  }&#xA;}&#xA;&#xA;eventsInSubTree(c.assets.retrieveByIds(Seq(rootId))).foreach(event =&amp;gt; println(event.`type`))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3D&lt;/h2&gt; &#xA;&lt;p&gt;To list 3D models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val models = c.threeDModels.list().take(10).compile.toList&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can create, update, delete, and retrieve models based on their ID. To access model revisions for a specific model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val modelId = modelIds.head.id&#xA;val revisions = c.threeDRevisions(modelId).list().take(10).compile.toList&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also create, update, delete, and retrieve revisions based on their IDs. You can also list all the nodes in a particular revision, as well as the ancestors of a particular node (including the node itself):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val revisionId = revisionIds.head.id&#xA;val nodes = c.threeDNodes(modelId, revisionId).list().listWithLimit(10).compile.toList&#xA;val nodeId = nodeIds.head.id&#xA;val ancestorNodes = c.threeDNodes(modelId, revisionId).ancestors(nodeId).compile.toList&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running tests&lt;/h2&gt; &#xA;&lt;p&gt;To be able to run most tests locally, these environment variables need to be set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#.env&#xA;&#xA;TEST_AAD_TENANT=&#34;a valid azure ad tenant id&#34;&#xA;TEST_CLIENT_ID=&#34;the id of a valid client credential, belonging to the given tenant, and with access to the playground project&#34;&#xA;TEST_CLIENT_SECRET=&#34;a valid client secret for the given client id&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some tests for API Keys features are still in place, to run these tests the aditional environment variable &lt;code&gt;TEST_API_KEY&lt;/code&gt; is needed, this should contain a valid API key for the playground project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cognitedata/spark-jdbc-sink</title>
    <updated>2022-11-11T01:37:33Z</updated>
    <id>tag:github.com,2022-11-11:/cognitedata/spark-jdbc-sink</id>
    <link href="https://github.com/cognitedata/spark-jdbc-sink" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Spark metrics sink for databases using JDBC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This repository has been archived and will no longer be maintained or updated.&lt;/p&gt; &#xA;&lt;h1&gt;spark-jdbc-sink&lt;/h1&gt; &#xA;&lt;p&gt;A Spark metrics sink that uses the JDBC reporter from &lt;a href=&#34;https://github.com/wso2/carbon-metrics&#34;&gt;WSO2 Carbon Metrics&lt;/a&gt; to write metrics to databases using JDBC.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Configure the sink using Spark&#39;s &lt;code&gt;metrics.properties&lt;/code&gt;. Here&#39;s an example using all available configuration options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;*.sink.jdbc.class=org.apache.spark.metrics.sink.JdbcSink&#xA;*.sink.jdbc.period=1&#xA;*.sink.jdbc.unit=seconds&#xA;*.sink.jdbc.user=spark&#xA;*.sink.jdbc.password=sparkmetrics&#xA;*.sink.jdbc.driver=org.postgresql.Driver&#xA;*.sink.jdbc.url=jdbc:postgresql://localhost/spark&#xA;*.sink.jdbc.filter=OnlyMyMetrics&#xA;*.sink.jdbc.name=MyMetrics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;Example database schemas for common databases are included in the &lt;a href=&#34;https://github.com/wso2/carbon-metrics/tree/master/features/org.wso2.carbon.metrics.jdbc.core.feature/resources/sql&#34;&gt;carbon-metrics&lt;/a&gt; repository.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cognitedata/cdp-spark-datasource</title>
    <updated>2022-11-11T01:37:33Z</updated>
    <id>tag:github.com,2022-11-11:/cognitedata/cdp-spark-datasource</id>
    <link href="https://github.com/cognitedata/cdp-spark-datasource" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spark data source for Cognite Data Fusion&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/com.cognite.spark.datasource/cdf-spark-datasource_2.13?label=version&#34; alt=&#34;Maven Central&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Spark Data Source&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/cognitedata/cdp-spark-datasource&#34;&gt;Cognite Spark Data Source&lt;/a&gt; lets you use &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Spark&lt;/a&gt; to read and write data from and to &lt;a href=&#34;https://docs.cognite.com/dev/&#34;&gt;Cognite Data Fusion&lt;/a&gt; (CDF).&lt;/p&gt; &#xA;&lt;p&gt;Reads and writes are done in parallel using asynchronous calls.&lt;/p&gt; &#xA;&lt;p&gt;The instructions below explain how to read from, and write to, the different resource types in CDF.&lt;/p&gt; &#xA;&lt;p&gt;This repository also contains &lt;code&gt;cdf_dump&lt;/code&gt; command line tool for reading data from CDF locally. There is a &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/cdf_dump/&#34;&gt;separate documentation page for it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In this article&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#spark-data-source&#34;&gt;Spark Data Source&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#read-and-write-to-cdf&#34;&gt;Read and write to CDF&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#common-options&#34;&gt;Common options&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#read-data&#34;&gt;Read data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#write-data&#34;&gt;Write data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#delete-data&#34;&gt;Delete data&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#asset-hierarchy-builder-beta&#34;&gt;Asset hierarchy builder (beta)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#options&#34;&gt;Options&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#setup-for-python&#34;&gt;Setup for Python&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#example-scala&#34;&gt;Example (Scala)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#example-python&#34;&gt;Example (Python)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#schemas&#34;&gt;Schemas&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#assets-schema&#34;&gt;Assets schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#events-schema&#34;&gt;Events schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#files-schema&#34;&gt;Files schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#data-points-schema&#34;&gt;Data points schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#string-data-points-schema&#34;&gt;String data points schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#time-series-schema&#34;&gt;Time series schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#asset-hierarchy-schema&#34;&gt;Asset Hierarchy schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#sequences-schema&#34;&gt;Sequences schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#sequence-rows-schema&#34;&gt;Sequence rows schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#labels-schema&#34;&gt;Labels schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#relationships-schema&#34;&gt;Relationships schema&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#data-sets-schema&#34;&gt;Data sets schema&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#examples-by-resource-types&#34;&gt;Examples by resource types&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#assets&#34;&gt;Assets&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#time-series&#34;&gt;Time series&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#data-points&#34;&gt;Data points&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#numerical-data-points&#34;&gt;Numerical data points&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#string-data-points&#34;&gt;String data points&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#events&#34;&gt;Events&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#files-metadata&#34;&gt;Files metadata&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#3d-models-and-revisions-metadata&#34;&gt;3D models and revisions metadata&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#sequences&#34;&gt;Sequences&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#sequence-rows&#34;&gt;Sequence rows&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#raw-tables&#34;&gt;RAW tables&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#labels&#34;&gt;Labels&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#relationships&#34;&gt;Relationships&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#data-sets&#34;&gt;Data sets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#comprehensive-examples&#34;&gt;Comprehensive example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#build-the-project-with-sbt&#34;&gt;Build the project with sbt&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#set-up&#34;&gt;Set up&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#run-the-tests&#34;&gt;Run the tests&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#run-the-project-locally-with-spark-shell&#34;&gt;Run the project locally with spark-shell&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Add the Spark data source to your cluster. We recommend using the &lt;a href=&#34;https://mvnrepository.com/artifact/com.cognite.spark.datasource/cdf-spark-datasource&#34;&gt;latest version&lt;/a&gt;. If using &lt;code&gt;spark-submit&lt;/code&gt; or &lt;code&gt;spark-shell&lt;/code&gt; or &lt;code&gt;pyspark&lt;/code&gt;, you can use &lt;code&gt;--packages com.cognite.spark.datasource:cdf-spark-datasource_2.12:&amp;lt;latest-release&amp;gt;&lt;/code&gt; (change to 2.13 if you&#39;re using Scala 2.13). If you&#39;re using Databricks, add the Maven coordinate &lt;code&gt;com.cognite.spark.datasource:cdf-spark-datasource_2.12:&amp;lt;latest-release&amp;gt;&lt;/code&gt; as a library to your cluster.&lt;/p&gt; &#xA;&lt;p&gt;You can also use &lt;code&gt;spark.jars.packages&lt;/code&gt; to include this data source using the same coordinate. See the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;official documentation&lt;/a&gt; for more information. Note that you should &lt;em&gt;not&lt;/em&gt; use &lt;code&gt;--jars&lt;/code&gt; or &lt;code&gt;spark.jars&lt;/code&gt;, as those options will not download and add the dependencies of our Spark data source.&lt;/p&gt; &#xA;&lt;p&gt;Then, try it out!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;your-api-key&#34;)&#xA;  .option(&#34;type&#34;, &#34;assets&#34;)&#xA;  .load()&#xA;df.count&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Read and write to CDF&lt;/h2&gt; &#xA;&lt;p&gt;The Cognite Spark Data Source lets you read data from and write data to these resource types: &lt;strong&gt;assets&lt;/strong&gt;, &lt;strong&gt;time series&lt;/strong&gt;, &lt;strong&gt;data points&lt;/strong&gt;, &lt;strong&gt;events&lt;/strong&gt;, and &lt;strong&gt;RAW tables&lt;/strong&gt;. For &lt;strong&gt;files&lt;/strong&gt; and &lt;strong&gt;3D models&lt;/strong&gt;, you can read &lt;strong&gt;metadata&lt;/strong&gt; .&lt;/p&gt; &#xA;&lt;h3&gt;Common options&lt;/h3&gt; &#xA;&lt;p&gt;Some options are common to all resource types. To set the options, use &lt;code&gt;spark.read.format(&#34;cognite.spark.v1&#34;).option(&#34;nameOfOption&#34;, &#34;value&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The common options are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;apiKey&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The CDF &lt;a href=&#34;https://doc.cognitedata.com/dev/guides/iam/authentication.html#api-keys&#34;&gt;API key&lt;/a&gt; for authorization.&lt;/td&gt; &#xA;   &lt;td&gt;Yes, if you don&#39;t specify a &lt;code&gt;bearerToken&lt;/code&gt; or the options for the &lt;code&gt;native tokens&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;bearerToken&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The CDF &lt;a href=&#34;https://doc.cognitedata.com/dev/guides/iam/authentication.html#tokens&#34;&gt;token&lt;/a&gt; for authorization.&lt;/td&gt; &#xA;   &lt;td&gt;Yes, if you don&#39;t specify an &lt;code&gt;apiKey&lt;/code&gt; or the options for the &lt;code&gt;native tokens&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;project&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The CDF project. By default it&#39;s inferred from the API key.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The Cognite Data Fusion resource type. See below for more &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#examples-by-resource-types&#34;&gt;resource type examples&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;maxRetries&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The maximum number of retries to be made when a request fails. Default: 10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;maxRetryDelay&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The maximum number of seconds to wait between retrying requests. Default: 30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;limitPerPartition&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The number of items to fetch for this resource type to create the DataFrame. Note that this is different from the SQL &lt;code&gt;SELECT * FROM ... LIMIT 1000&lt;/code&gt; limit. This option specifies the limit for items to fetch from CDF &lt;em&gt;per partition&lt;/em&gt;, &lt;em&gt;before&lt;/em&gt; filtering and other transformations are applied to limit the number of results. Not supported by data points.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;batchSize&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The maximum number of items to read/write per API call.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;baseUrl&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Address of the CDF API. For example might be changed to &lt;a href=&#34;https://greenfield.cognitedata.com&#34;&gt;https://greenfield.cognitedata.com&lt;/a&gt;. By default it is set to &lt;a href=&#34;https://api.cognitedata.com&#34;&gt;https://api.cognitedata.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;collectMetrics&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; - if Spark metrics should be collected about number of reads, inserts, updates and deletes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metricsPrefix&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Common prefix for all collected metrics. Might be useful when working with multiple connections.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;partitions&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of &lt;a href=&#34;https://docs.cognite.com/dev/concepts/pagination/#parallel-retrieval&#34;&gt;CDF partitions&lt;/a&gt; to use. By default it&#39;s 200.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;parallelismPerPartition&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How many parallel request should run for one Spark partition. Number of Spark partitions = &lt;code&gt;partitions&lt;/code&gt; / &lt;code&gt;parallelismPerPartition&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;applicationName&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Identifies the application making requests by including a &lt;code&gt;X-CDP-App&lt;/code&gt; header. Defaults to &lt;code&gt;com.cognite.spark.datasource-(version)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clientTag&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If set, will be included as a &lt;code&gt;X-CDP-ClientTag&lt;/code&gt; header in requests. This is typically used to group sets of requests as belonging to some definition of a job or workload for debugging.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To autenticate using OIDC tokens set all of these options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;tokenUri&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Token uri to request a token from. (Example: &lt;code&gt;https://login.microsoftonline.com/&amp;lt;Directory (tenant) ID&amp;gt;/oauth2/v2.0/token&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clientId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Application (client) ID associated with the target CDF project.&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clientSecret&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Client secret for the application.&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;project&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The CDF project.&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;scopes&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The scopes needed for the user. Required for AAD setup.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;audience&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The audience needed for token retrieval, supported for the custom Aize and AKSO OAuth2 setup.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Read data&lt;/h3&gt; &#xA;&lt;p&gt;To read from CDF resource types, you need to specify: an &lt;strong&gt;API-key&lt;/strong&gt; or a &lt;strong&gt;bearertoken&lt;/strong&gt; and the &lt;strong&gt;resource type&lt;/strong&gt; you want to read from. To read from a table you also need to specify the database and table names.&lt;/p&gt; &#xA;&lt;h4&gt;Filter pushdown&lt;/h4&gt; &#xA;&lt;p&gt;For some fields, filters are pushed down to the API. For example, if you read events with a filter on asset IDs, only the IDs that satisfy the filter are read from CDF, as opposed to reading all events and &lt;strong&gt;then&lt;/strong&gt; applying the filter. This happens automatically, but note that filters are only pushed down when Spark reads data from CDF, and not when working on a DataFrame that is already in memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;externalId&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; always support this filter, for most resources it&#39;s also possible to filter by &lt;code&gt;externalId&lt;/code&gt; prefix. For example query &lt;code&gt;where externalId LIKE &#39;my-id-prefix-%&#39;&lt;/code&gt; will only fetch items with the matching id. Note that the filter only works on prefix, not any substring, so querying &lt;code&gt;where externalId LIKE &#39;%-something-%&#39;&lt;/code&gt; will download all items from CDF.&lt;/p&gt; &#xA;&lt;p&gt;Generally, filtering works on anything that &lt;a href=&#34;https://docs.cognite.com/api/v1/#operation/listAssets&#34;&gt;CDF API can filter on&lt;/a&gt;. If it does not work for something you&#39;d expect to work, feel free to report it to Cognite. However, due to limitations in Spark we cannot filter on metadata field. See &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#schemas&#34;&gt;Schema section&lt;/a&gt; for more details on which filters are supported.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;equality&lt;/strong&gt; means that &lt;code&gt;column = &#39;x&#39;&lt;/code&gt;, &lt;code&gt;column IN (&#39;x&#39;, &#39;y&#39;)&lt;/code&gt; and similar are supported&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;comparison&lt;/strong&gt; means that &lt;code&gt;column &amp;lt;= 10&lt;/code&gt;, &lt;code&gt;column &amp;gt; 10&lt;/code&gt; and similar are supported&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In practice, it&#39;s often useful to filter on &lt;code&gt;datasetId&lt;/code&gt;. Usually, one query only needs items from a single dataset, so it&#39;s an easy way to improve run-time.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Note that filter pushdown works even with more complex predicates with AND and OR operators.&lt;/p&gt; &#xA;&lt;h3&gt;Write data&lt;/h3&gt; &#xA;&lt;p&gt;You can write to CDF with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;insertInto&lt;/code&gt; - checks that all fields are present and in the correct order. Can be more convenient when you&#39;re working with Spark SQL tables.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;save&lt;/code&gt; - gives you control over how to handle potential collisions with existing data, and allows you to update a subset of fields in a row.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;.insertInto()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To write to a resource using the insert into pattern, you&#39;ll need to register a DataFrame that was read from the resource, as a temporary view. You also need write access to the project and resources. In the examples below, replace &lt;code&gt;myApiKey&lt;/code&gt; with your own API key.&lt;/p&gt; &#xA;&lt;p&gt;Your schema must match that of the target exactly. To ensure this, copy the schema from the DataFrame you read into with &lt;code&gt;sourceDf.select(destinationDf.columns.map(col):_*)&lt;/code&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#time-series&#34;&gt;time series example below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;.insertInto()&lt;/code&gt; does upserts (updates existing rows and inserts new rows) for events, assets, and time series. &lt;code&gt;.insertInto()&lt;/code&gt; upserts use &lt;code&gt;externalId&lt;/code&gt; (ignoring &lt;code&gt;id&lt;/code&gt;), attempting to create a row with &lt;code&gt;externalId&lt;/code&gt; if set, and if a row with the given &lt;code&gt;externalId&lt;/code&gt; already exists it will be updated.&lt;/p&gt; &#xA;&lt;p&gt;Data points also have upsert behavior, but based on the timestamp.&lt;/p&gt; &#xA;&lt;p&gt;For RAW tables, &lt;code&gt;.insertInto()&lt;/code&gt; does inserts and throws an error if one or more rows already exist. For files, &lt;code&gt;insertInto()&lt;/code&gt; only supports updating existing files.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;.save()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We currently support writing with &lt;code&gt;.save()&lt;/code&gt; for assets, events, and time series. You&#39;ll need to provide an API key and the resource type you want to write to. You can also use &lt;code&gt;.option(&#34;onconflict&#34;, value)&lt;/code&gt; to specify the desired behavior when rows in your Dataframe are present in CDF.&lt;/p&gt; &#xA;&lt;p&gt;The valid options for onconflict are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;abort&lt;/code&gt; - tries to insert all rows in the Dataframe. Throws an error if the resource item already exists, and no more rows will be written.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;update&lt;/code&gt; - looks for all rows in the Dataframe in CDF and tries to update them. If one or more rows do not exist, no more rows are updated and an error is thrown. Supports partial updates.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;upsert&lt;/code&gt; - updates rows that already exist, and inserts new rows. In this mode inserted rows with &lt;code&gt;id&lt;/code&gt; set will always attempt to update the target row with such an &lt;code&gt;id&lt;/code&gt;. If &lt;code&gt;id&lt;/code&gt; is null, or not present, and &lt;code&gt;externalId&lt;/code&gt; is not null, it will attempt to create a row with the given &lt;code&gt;externalId&lt;/code&gt;. If such a row already exists, that row will be updated to the values present in the row being inserted.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Multiple rows with the same &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;externalId&lt;/code&gt; are allowed for upserts, but the order in which they are applied is undefined and we currently only guarantee that at least one upsert will be made for each &lt;code&gt;externalId&lt;/code&gt;, and at least one update will be made for each &lt;code&gt;id&lt;/code&gt; set.&lt;/p&gt; &#xA;&lt;p&gt;This is based on the assumption that upserts for the same &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;externalId&lt;/code&gt; will have the same values. If you have a use case where this is not the case, please let us know.&lt;/p&gt; &#xA;&lt;h4&gt;Handling NULLs and empty fields&lt;/h4&gt; &#xA;&lt;p&gt;By default, the Spark Datasource ignores &lt;code&gt;NULL&lt;/code&gt;s when updating: nothing is written to CDF when there is a &lt;code&gt;NULL&lt;/code&gt; in the field. This can be controlled by setting the &lt;code&gt;ignoreNullFields&lt;/code&gt; option which defaults to &lt;code&gt;&#34;true&#34;&lt;/code&gt; when using &lt;code&gt;.save()&lt;/code&gt; writes. This is usually useful for ignoring the columns which are irrelevant for a task, however it makes it impossible to null a field in CDF. When the &lt;code&gt;ignoreNullFields&lt;/code&gt; option is set to &lt;code&gt;false&lt;/code&gt;, NULLs are written to CDF (when possible). Fields which are not specified are still ignored. See an example of using &lt;code&gt;.save()&lt;/code&gt; under &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#events&#34;&gt;Events below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Delete data&lt;/h3&gt; &#xA;&lt;p&gt;We currently support deleting with &lt;code&gt;.save()&lt;/code&gt; for assets, events and time series.&lt;/p&gt; &#xA;&lt;p&gt;You need to provide an API key and specify the resource type, and then specify &lt;code&gt;delete&lt;/code&gt; as the &lt;code&gt;onconflict&lt;/code&gt; option like this: &lt;code&gt;.option(&#34;onconflict&#34;, &#34;delete&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See an example for using &lt;code&gt;.save()&lt;/code&gt; to delete under &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#time-series&#34;&gt;Time Series below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Assets and events will ignore existing ids on deletes. If you prefer to abort the job when attempting to delete an unknown id, use &lt;code&gt;.option(&#34;ignoreUnknownIds&#34;, &#34;false&#34;)&lt;/code&gt; for those resources types.&lt;/p&gt; &#xA;&lt;p&gt;Expected schema for delete of Time series, Assets, Events or Files is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Expected schema for delete of Datapoints or String Datapoints is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt; Yes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;inclusiveBegin&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;exclusiveBegin&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;inclusiveEnd&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;exclusiveEnd&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;One of &lt;code&gt;id&lt;/code&gt; &amp;amp; &lt;code&gt;externalId&lt;/code&gt;, &lt;code&gt;inclusiveBegin&lt;/code&gt; &amp;amp; &lt;code&gt;exclusiveBegin&lt;/code&gt; and &lt;code&gt;inclusiveEnd&lt;/code&gt; &amp;amp; &lt;code&gt;exclusiveEnd&lt;/code&gt; must be specified. Data points are deleted by a range and both bound must be specified. To delete a single data point, set &lt;code&gt;inclusiveBegin&lt;/code&gt; and &lt;code&gt;inclusiveEnd&lt;/code&gt; to the same value. To delete a range between two points, set &lt;code&gt;exclusiveBegin&lt;/code&gt; to the first point and &lt;code&gt;exclusiveEnd&lt;/code&gt; to the second one; this will not delete the boundaries, but everything between them.&lt;/p&gt; &#xA;&lt;h2&gt;Asset hierarchy builder (beta)&lt;/h2&gt; &#xA;&lt;p&gt;Note: The asset hierarchy builder is currently in beta, and has not been sufficiently tested to be used on production data.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;.option(&#34;type&#34;, &#34;assethierarchy&#34;)&lt;/code&gt; lets you write new asset hierarchies, or update existing ones, using the Spark Data Source. The asset hierarchy builder can ingest entire hierarchies of nodes connected through the &lt;code&gt;externalId&lt;/code&gt;/&lt;code&gt;parentExternalId&lt;/code&gt; relationship. If input contains an update to data that already exists, i.e there&#39;s a match on &lt;code&gt;externalId&lt;/code&gt; and there&#39;s a change to one of the other fields, the asset will be updated. There&#39;s also an option to delete assets from CDF that are not referenced in the input data.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Root assets are denoted by setting their &lt;code&gt;parentExternalId&lt;/code&gt; to the empty string &lt;code&gt;&#34;&#34;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The input data must not have loops, to ensure all asset hierarchies are fully connected.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;externalId&lt;/code&gt; can not be the empty string &lt;code&gt;&#34;&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Options&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;deleteMissingAssets&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether or not you would like assets under the root to be deleted if they&#39;re not present in the input data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;subtrees&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ingest&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Controls what should happen with subtrees without a root node in the input data. &lt;code&gt;ingest&lt;/code&gt; says they will be processed and loaded into CDF, &lt;code&gt;ignore&lt;/code&gt; will ignore all of them and &lt;code&gt;error&lt;/code&gt; will stop the execution and raise an error (nothing will be ingested).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;batchSize&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1000&lt;/td&gt; &#xA;   &lt;td&gt;The number of assets to write per API call.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Setup for Python&lt;/h3&gt; &#xA;&lt;p&gt;You may want to set up a Jupyter notebook with &lt;code&gt;pySpark&lt;/code&gt; running.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download spark version &lt;code&gt;2.4.5&lt;/code&gt; &lt;a href=&#34;https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the instructions given &lt;a href=&#34;https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes&#34;&gt;here&lt;/a&gt;, except that your Spark version will be &lt;code&gt;2.4.5&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start your Jupyter notebook with the following command (instead of &lt;code&gt;pyspark&lt;/code&gt; as in the link above):&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyspark --packages com.cognite.spark.datasource:cdf-spark-datasource_2.11:1.2.18&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example (Scala)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val assetHierarchySchema = Seq(&#34;externalId&#34;, &#34;parentExternalId&#34;, &#34;source&#34;, &#34;description&#34;, &#34;name&#34;, &#34;metadata&#34;)&#xA;&#xA;// Manually create some assets that satisfy the requirements of the asset hierarchy builder&#xA;val assetHierarchy = Seq(&#xA;  (&#34;root_asset&#34;, &#34;&#34;, &#34;manual_input&#34;, &#34;root_asset&#34;, Some(&#34;This is the root asset&#34;), Map(&#34;asset_depth&#34; -&amp;gt; &#34;0&#34;)),&#xA;  (&#34;first_child&#34;, &#34;root_asset&#34;, &#34;manual_input&#34;, &#34;first_child&#34;, Some(&#34;This is the first_child&#34;), Map(&#34;asset_depth&#34; -&amp;gt; &#34;1&#34;)),&#xA;  (&#34;second_child&#34;, &#34;root_asset&#34;, &#34;manual_input&#34;, &#34;second_child&#34;, Some(&#34;This is the second_child&#34;), Map(&#34;asset_depth&#34; -&amp;gt; &#34;1&#34;)),&#xA;  (&#34;grandchild&#34;, &#34;first_child&#34;, &#34;manual_input&#34;, &#34;grandchild&#34;, Some(&#34;This is the child of first_child&#34;), Map(&#34;asset_depth&#34; -&amp;gt; &#34;2&#34;))&#xA;)&#xA;&#xA;val assetHierarchyDataFrame = spark&#xA;  .sparkContext&#xA;  .parallelize(assetHierarchy)&#xA;  .toDF(assetHierarchySchema:_*)&#xA;&#xA;// Validate that the schema is as expected&#xA;assetHierarchyDataFrame.printSchema()&#xA;&#xA;// Insert the assets with the asset hierarchy builder&#xA;assetHierarchyDataFrame.write&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;assethierarchy&#34;)&#xA;  .save()&#xA;&#xA;// Have a look at your new asset hierarchy&#xA;spark.read&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;assets&#34;)&#xA;  .load()&#xA;  .where(&#34;source = &#39;manual_input&#39;&#34;)&#xA;  .show()&#xA;&#xA;// Delete everything but the root using the deleteMissingAssets flag&#xA;spark&#xA;  .sparkContext&#xA;  .parallelize(Seq(Seq(&#34;root_asset&#34;, &#34;&#34;, &#34;manual_input&#34;, &#34;root_asset&#34;, Some(&#34;This is the root asset&#34;), None)))&#xA;  .toDF(assetHierarchySchema)&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;assethierarchy&#34;)&#xA;  .option(&#34;deleteMissingAssets&#34;, &#34;true&#34;)&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example (Python)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assetHierarchySchema = [&#34;externalId&#34;, &#34;parentExternalId&#34;, &#34;source&#34;, &#34;description&#34;, &#34;name&#34;, &#34;metadata&#34;]&#xA;&#xA;# Manually create some assets that satisfy the requirements of the asset hierarchy builder&#xA;assetHierarchy = [&#xA;  [&#34;root_asset&#34;, &#34;&#34;, &#34;manual_input&#34;, &#34;root_asset&#34;, &#34;This is the root asset&#34;, {&#34;asset_depth&#34; : &#34;0&#34;}],&#xA;  [&#34;first_child&#34;, &#34;root_asset&#34;, &#34;manual_input&#34;, &#34;first_child&#34;, &#34;This is the first_child&#34;, {&#34;asset_depth&#34; : &#34;1&#34;}],&#xA;  [&#34;second_child&#34;, &#34;root_asset&#34;, &#34;manual_input&#34;, &#34;second_child&#34;, &#34;This is the second_child&#34;, {&#34;asset_depth&#34; : &#34;1&#34;}],&#xA;  [&#34;grandchild&#34;, &#34;first_child&#34;, &#34;manual_input&#34;, &#34;grandchild&#34;, &#34;This is the child of first_child&#34;, {&#34;asset_depth&#34; : &#34;2&#34;}]&#xA;]&#xA;&#xA;assetHierarchyDataFrame = spark.sparkContext.parallelize(assetHierarchy).toDF(assetHierarchySchema)&#xA;&#xA;# Validate that the schema is as expected&#xA;assetHierarchyDataFrame.printSchema()&#xA;&#xA;# Insert the assets with the asset hierarchy builder&#xA;assetHierarchyDataFrame.write \&#xA;    .format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;assethierarchy&#34;) \&#xA;    .save()&#xA;&#xA;# Have a look at your new asset hierarchy&#xA;spark.read \&#xA;  .format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, myApiKey) \&#xA;  .option(&#34;type&#34;, &#34;assets&#34;) \&#xA;  .load() \&#xA;  .where(&#34;source = &#39;manual_input&#39;&#34;) \&#xA;  .show()&#xA;&#xA;# Delete everything but the root using the deleteMissingAssets flag&#xA;spark \&#xA;    .sparkContext \&#xA;    .parallelize([[&#34;root_asset&#34;, &#34;&#34;, &#34;manual_input&#34;, &#34;root_asset&#34;, &#34;This is the root asset&#34;, {&#34;&#34;:&#34;&#34;}]]) \&#xA;    .toDF(assetHierarchySchema) \&#xA;    .write \&#xA;    .format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;assethierarchy&#34;) \&#xA;    .option(&#34;deleteMissingAssets&#34;, &#34;true&#34;) \&#xA;    .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Schemas&lt;/h2&gt; &#xA;&lt;p&gt;Spark DataFrames have schemas, with typing and names for columns. When writing to a resource in CDF using the &lt;code&gt;insertInto&lt;/code&gt;-pattern you have to match the schema exactly (see &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#%60.insertInto()%60&#34;&gt;.insertInto()&lt;/a&gt; for a tip about this).&lt;/p&gt; &#xA;&lt;p&gt;The schemas mirror the CDF API as closely as possible.&lt;/p&gt; &#xA;&lt;h3&gt;Assets schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality, prefix (&lt;code&gt;LIKE &#39;xyz%&#39;&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;parentId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;source&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;createdTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;rootId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;aggregates&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, long)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Events schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;startTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;endTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;subtype&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;assetIds&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(long)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;source&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality, prefix (&lt;code&gt;LIKE &#39;xyz%&#39;&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;createdTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Files schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;source&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality, prefix (&lt;code&gt;LIKE &#39;xyz%&#39;&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;mimeType&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;assetIds&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(long)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;uploaded&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;boolean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;uploadedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;createdTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sourceCreatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sourceModifiedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;securityCategories&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(long)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;uploadUrl&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Data points schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;value&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;double&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;aggregation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;granularity&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;String data points schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;value&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Time series schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;isString&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;boolean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;unit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;assetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;isStep&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;boolean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;securityCategories&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(long)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality, prefix (&lt;code&gt;LIKE &#39;xyz%&#39;&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;createdTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Asset Hierarchy schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;parentExternalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;source&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sequences schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;assetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;columns&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(SequenceColumn)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;code&gt;columns&lt;/code&gt; field is an array of &lt;code&gt;SequenceColumn&lt;/code&gt;s, which are rows with the following fields:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;valueType&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sequence rows schema&lt;/h3&gt; &#xA;&lt;p&gt;The schema of the &lt;code&gt;sequencerows&lt;/code&gt; relation matches the sequence that is specified in the &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;externalId&lt;/code&gt; option. Apart from the sequence columns, you need a non-nullable &lt;code&gt;rowNumber&lt;/code&gt; column of type &lt;code&gt;long&lt;/code&gt;. You also need the &lt;code&gt;externalId&lt;/code&gt; column, which allows you to write to multiple sequences as long as they have the same schema as the &lt;code&gt;externalId&lt;/code&gt; or &lt;code&gt;id&lt;/code&gt; passed with the &lt;code&gt;.option()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Labels schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Relationships schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sourceExternalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sourceType&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;targetExternalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;targetType&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;startTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;endTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;confidence&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;double&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;comparison&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;labels&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;array(string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dataSetId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Data sets schema&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Column name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Nullable&lt;/th&gt; &#xA;   &lt;th&gt;Filter pushdown &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/#filter-pushdown&#34;&gt;?&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;externalId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality, prefix (&lt;code&gt;LIKE &#39;xyz%&#39;&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;map(string, string)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;writeProtected&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lastUpdatedTime&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;timestamp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;comparison, equality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Examples by resource types&lt;/h2&gt; &#xA;&lt;h3&gt;Assets&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about assets &lt;a href=&#34;https://doc.cognitedata.com/dev/concepts/resource_types/assets.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Read assets from your project into a DataFrame&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA; .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA; .option(&#34;type&#34;, &#34;assets&#34;)&#xA; .load()&#xA;&#xA;// Register your assets in a temporary view&#xA;df.createTempView(&#34;assets&#34;)&#xA;&#xA;// Create a new asset and write to CDF&#xA;// Note that parentId, asset type IDs, and asset type field IDs have to exist.&#xA;val assetColumns = Seq(&#34;externalId&#34;, &#34;name&#34;, &#34;parentId&#34;, &#34;description&#34;, &#34;metadata&#34;, &#34;source&#34;,&#xA;&#34;id&#34;, &#34;createdTime&#34;, &#34;lastupdatedTime&#34;)&#xA;val someAsset = Seq(&#xA;(&#34;Some external ID&#34;, &#34;asset name&#34;, &#34;This is another asset&#34;, Map(&#34;sourceSystem&#34;-&amp;gt;&#34;MySparkJob&#34;), &#34;some source&#34;,&#xA;99L, 0L, 0L))&#xA;&#xA;val someAssetDf = spark&#xA;  .sparkContext&#xA;  .parallelize(someAsset)&#xA;  .toDF(assetColumns:_*)&#xA;&#xA;// Write the new asset to CDF, ensuring correct schema by borrowing the schema of the df from CDF&#xA;spark&#xA;  .sqlContext&#xA;  .createDataFrame(someAssetDf.rdd, df.schema)&#xA;  .write&#xA;  .insertInto(&#34;assets&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read assets from your project into a DataFrame&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;assets&#34;) \&#xA;    .load()&#xA;&#xA;# Register your assets in a temporary view&#xA;df.createTempView(&#34;assets&#34;)&#xA;&#xA;# Create a new asset and write to CDF&#xA;# Note that parentId, asset type IDs, and asset type field IDs have to exist. You might want to change the columns here as per your requirements&#xA;assetColumns = [&#34;externalId&#34;, &#34;name&#34;, &#34;parentId&#34;, &#34;description&#34;, &#34;metadata&#34;, &#34;source&#34;, &#34;id&#34;, &#34;createdTime&#34;, &#34;lastupdatedTime&#34;, &#34;rootId&#34;, &#34;aggregates&#34;, &#34;dataSetId&#34;, &#34;parentExternalId&#34;]&#xA;&#xA;someAsset = [[&#34;Some external ID&#34;, &#34;asset name&#34;, 0, &#34;This is another asset&#34;, {&#34;sourceSystem&#34;: &#34;MySparkJob&#34;}, &#34;some source&#34;, 99, 0, 0, &#34;&#34;, &#34;&#34;, &#34;&#34;, &#34;&#34;]]&#xA;&#xA;someAssetDf = spark.sparkContext \&#xA;    .parallelize(someAsset) \&#xA;    .toDF(assetColumns)&#xA;&#xA;&#xA;# Write the new asset to CDF, ensuring correct schema by borrowing the schema of the df from CDF&#xA;spark.createDataFrame(someAssetDf.rdd, df.schema) \&#xA;    .write \&#xA;    .insertInto(&#34;assets&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Time series&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about time series &lt;a href=&#34;https://doc.cognitedata.com/dev/concepts/resource_types/timeseries.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Get all the time series from your project&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;timeseries&#34;)&#xA;  .load()&#xA;df.createTempView(&#34;timeseries&#34;)&#xA;&#xA;// Read some new time series data from a csv file&#xA;val timeSeriesDf = spark.read.format(&#34;csv&#34;)&#xA;  .option(&#34;header&#34;, &#34;true&#34;)&#xA;  .load(&#34;timeseries.csv&#34;)&#xA;&#xA;// Ensure correct schema by copying the columns in the DataFrame read from the project.&#xA;// Note that the time series must already exist in the project before data can be written to it, based on the ´name´ column.&#xA;timeSeriesDf.select(df.columns.map(col):_*)&#xA;  .write&#xA;  .insertInto(&#34;timeseries&#34;)&#xA;&#xA;// Delete all time series you just created&#xA;timeSeriesDf&#xA;  .write&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;timeseries&#34;)&#xA;  .option(&#34;onconflict&#34;, &#34;delete&#34;)&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Get all the time series from your project&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;timeseries&#34;) \&#xA;    .load()&#xA;&#xA;df.createTempView(&#34;timeseries&#34;)&#xA;&#xA;# Read some new time series data from a csv file&#xA;timeSeriesDf = spark.read.format(&#34;csv&#34;) \&#xA;    .option(&#34;header&#34;, &#34;true&#34;) \&#xA;    .load(&#34;timeseries.csv&#34;)&#xA;&#xA;# Ensure correct schema by copying the columns in the DataFrame read from the project.&#xA;# Note that the time series must already exist in the project before data can be written to it, based on the ´name´ column.&#xA;timeSeriesDf.select(df.columns.map(col)) \&#xA;    .write \&#xA;    .insertInto(&#34;timeseries&#34;)&#xA;&#xA;# Delete all time series you just created&#xA;timeSeriesDf \&#xA;    .write \&#xA;    .format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;timeseries&#34;) \&#xA;    .option(&#34;onconflict&#34;, &#34;delete&#34;) \&#xA;    .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data points&lt;/h3&gt; &#xA;&lt;p&gt;Data points are always related to a time series. To read data points you need to filter by a valid time series id, otherwise an empty DataFrame is returned. &lt;strong&gt;Important&lt;/strong&gt;: Be careful when using caching with this resource type. If you cache the result of a filter and then apply another filter, you do not trigger more data to be read from CDF and end up with an empty DataFrame.&lt;/p&gt; &#xA;&lt;h4&gt;Numerical data points&lt;/h4&gt; &#xA;&lt;p&gt;To read numerical data points from CDF, use the &lt;code&gt;.option(&#34;type&#34;, &#34;datapoints&#34;)&lt;/code&gt; option. For numerical data points you can also request aggregated data by filtering by &lt;strong&gt;aggregation&lt;/strong&gt; and &lt;strong&gt;granularity&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;aggregation&lt;/code&gt;: Numerical data points can be aggregated to reduce the amount of data transferred in query responses and improve performance. You can specify one or more aggregates (for example average, minimum and maximum) and also the time granularity for the aggregates (for example 1h for one hour). If the aggregate option is NULL, or not set, data points return the raw time series data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;granularity&lt;/code&gt;: Aggregates are aligned to the start time modulo of the granularity unit. For example, if you ask for daily average temperatures since Monday afternoon last week, the first aggregated data point contains averages for the whole of Monday, the second for Tuesday, etc.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Get the datapoints from publicdata&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;publicdataApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;datapoints&#34;)&#xA;  .load()&#xA;&#xA;// Create the view to enable SQL syntax&#xA;df.createTempView(&#34;datapoints&#34;)&#xA;&#xA;// Read the raw datapoints from the VAL_23-FT-92537-04:X.Value time series.&#xA;val timeseriesId = 3385857257491234L&#xA;val timeseries = spark.sql(s&#34;select * from datapoints where id = $timeseriesId&#34;)&#xA;&#xA;// Read aggregate data from the same time series&#xA;val timeseriesAggregated = spark.sql(s&#34;select * from datapoints where id = $timeseriesId&#34; +&#xA;s&#34;and aggregation = &#39;min&#39; and granularity = &#39;1d&#39;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the datapoints from publicdata&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;datapoints&#34;) \&#xA;    .load()&#xA;&#xA;# Create the view to enable SQL syntax&#xA;df.createTempView(&#34;datapoints&#34;)&#xA;&#xA;# Read the raw datapoints from the VAL_23-FT-92537-04:X.Value time series.&#xA;timeseriesId = 3385857257491234&#xA;&#xA;&#xA;query = &#34;select * from datapoints where id = %d&#34; % timeseriesId&#xA;timeseries = spark.sql(query)&#xA;&#xA;# Read aggregate data from the same time series&#xA;timeseriesAggregated = spark.sql(&#34;select * from datapoints where id = %d&#34; %timeseriesId  +&#xA;&#34; and aggregation = &#39;min&#39; and granularity = &#39;1d&#39;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;String data points&lt;/h4&gt; &#xA;&lt;p&gt;To read string data points from CDF, provide the &lt;code&gt;.option(&#34;type&#34;, &#34;stringdatapoints&#34;)&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Get the datapoints from publicdata&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;publicdataApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;stringdatapoints&#34;)&#xA;  .load()&#xA;&#xA;// Create the view to enable SQL syntax&#xA;df.createTempView(&#34;stringdatapoints&#34;)&#xA;&#xA;// Read the raw datapoints from the VAL_23-PIC-96153:MODE time series.&#xA;val timeseriesId = 6536948395539605L&#xA;val timeseries = spark.sql(s&#34;select * from stringdatapoints where id = $timeseriesId&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Get the datapoints from publicdata&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;stringdatapoints&#34;) \&#xA;    .load()&#xA;&#xA;# Create the view to enable SQL syntax&#xA;df.createTempView(&#34;stringdatapoints&#34;)&#xA;&#xA;# Read the raw datapoints from the VAL_23-PIC-96153:MODE time series.&#xA;timeseriesId = 6536948395539605&#xA;timeseries = spark.sql(&#34;select * from stringdatapoints where id = %d&#34; % timeseriesId)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Events&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about events &lt;a href=&#34;https://doc.cognitedata.com/dev/concepts/resource_types/events.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Read events from `publicdata`&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;publicdataApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;events&#34;)&#xA;  .load()&#xA;&#xA;// Insert the events in your own project using .save()&#xA;import org.apache.spark.sql.functions._&#xA;df.withColumn(&#34;source&#34;, lit(&#34;publicdata&#34;))&#xA;  .write.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;onconflict&#34;, &#34;abort&#34;)&#xA;  .save()&#xA;&#xA;// Get a reference to the events in your project&#xA;val myProjectDf = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;events&#34;)&#xA;  .load()&#xA;myProjectDf.createTempView(&#34;events&#34;)&#xA;&#xA;// Update the description of all events from Open Industrial Data&#xA;spark.sql(&#34;&#34;&#34;&#xA; |select &#39;Manually copied data from publicdata&#39; as description,&#xA; |id,&#xA; |from events&#xA; |where source = &#39;publicdata&#39;&#xA;&#34;&#34;&#34;.stripMargin)&#xA;.write.format(&#34;cognite.spark.v1&#34;)&#xA;.option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;.option(&#34;onconflict&#34;, &#34;update&#34;)&#xA;.save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read events from `publicdata`&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;events&#34;) \&#xA;    .load()&#xA;&#xA;# Insert the events in your own project using .save()&#xA;from pyspark.sql.functions import lit&#xA;df.withColumn(&#34;source&#34;, lit(&#34;publicdata&#34;)) \&#xA;    .write.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;events&#34;) \&#xA;    .option(&#34;onconflict&#34;, &#34;abort&#34;) \&#xA;    .save()&#xA;&#xA;# Get a reference to the events in your project&#xA;myProjectDf = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;events&#34;) \&#xA;    .load()&#xA;myProjectDf.createTempView(&#34;events&#34;)&#xA;&#xA;# Update the description of all events from Open Industrial Data&#xA;spark.sql(&#xA;    &#34;select &#39;Manually copied data from publicdata&#39; as description,&#34; \&#xA;    &#34; id,&#34; \&#xA;    &#34; from events&#34; \&#xA;    &#34; where source = &#39;publicdata&#39;&#34;) \&#xA;    .write.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;onconflict&#34;, &#34;update&#34;) \&#xA;    .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Files metadata&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about files &lt;a href=&#34;https://doc.cognitedata.com/dev/concepts/resource_types/files.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Read files metadata from publicdata&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;files&#34;)&#xA;  .load()&#xA;&#xA;df.groupBy(&#34;fileType&#34;).count().show()&#xA;&#xA;// Register your files in a temporary view&#xA;df.createTempView(&#34;files&#34;)&#xA;&#xA;&#xA;// Insert the files in your own project using .save()&#xA;spark.sql(s&#34;&#34;&#34;&#xA;      |select &#39;example-externalId&#39; as externalId,&#xA;      |&#39;example-name&#39; as name,&#xA;      |&#39;text&#39; as source&#34;&#34;&#34;)&#xA;  .write.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;files&#34;)&#xA;  .option(&#34;onconflict&#34;, &#34;abort&#34;)&#xA;  .save()&#xA;&#xA;//You can also insert using insertInto(). But you need to make sure the the schema is matched correctly.&#xA; spark.sql(s&#34;&#34;&#34;&#xA;                |select &#34;name-using-insertInto()&#34; as name,&#xA;                |null as id,&#xA;                |&#39;text&#39; as source,&#xA;                |&#39;externalId-using-insertInto()&#39; as externalId,&#xA;                |null as mimeType,&#xA;                |null as metadata,&#xA;                |null as assetIds,&#xA;                |null as datasetId,&#xA;                |null as sourceCreatedTime,&#xA;                |null as sourceModifiedTime,&#xA;                |null as securityCategories,&#xA;                |null as uploaded,&#xA;                |null as createdTime,&#xA;                |null as lastUpdatedTime,&#xA;                |null as uploadedTime,&#xA;                |null as uploadUrl&#xA;     &#34;&#34;&#34;.stripMargin)&#xA;      .select(df.columns.map(col):_*)&#xA;      .write&#xA;      .insertInto(&#34;files&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read files metadata from publicdata&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, myApiKey) \&#xA;  .option(&#34;type&#34;, &#34;files&#34;) \&#xA;  .load()&#xA;&#xA;df.groupBy(&#34;fileType&#34;).count().show()&#xA;&#xA;# Register your files in a temporary view&#xA;df.createTempView(&#34;files&#34;)&#xA;&#xA;# Insert the files in your own project using .save()&#xA;spark.sql(&#xA;    &#34;select &#39;example-externalId&#39; as externalId,&#34; \&#xA;    &#34; &#39;example-name&#39; as name,&#34; \&#xA;    &#34; &#39;text&#39; as source&#34;) \&#xA;  .write.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;) \&#xA;  .option(&#34;type&#34;, &#34;files&#34;) \&#xA;  .option(&#34;onconflict&#34;, &#34;abort&#34;) \&#xA;  .save()&#xA;&#xA;# You can also insert data using insertInto(). But you need to make sure the the schema is matched correctly.&#xA;# The example using insertInto() is given above in Scala example.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3D models and revisions metadata&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about 3D models and revisions &lt;a href=&#34;https://doc.cognitedata.com/dev/concepts/resource_types/3dmodels.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that the Open Industrial Data project does not have any 3D models. To test this example, you need a project with existing 3D models. There are four options for listing metadata about 3D models: &lt;code&gt;3dmodels&lt;/code&gt;, &lt;code&gt;3dmodelrevisions&lt;/code&gt;, &lt;code&gt;3dmodelrevisionmappings&lt;/code&gt; and &lt;code&gt;3dmodelrevisionnodes&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Read 3D models metadata from a project with 3D models and revisions&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;apiKeyToProjectWith3dModels&#34;)&#xA;  .option(&#34;type&#34;, &#34;3dmodels&#34;)&#xA;  .load()&#xA;&#xA;df.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read 3D models metadata from a project with 3D models and revisions&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, &#34;apiKeyToProjectWith3dModels&#34;) \&#xA;    .option(&#34;type&#34;, &#34;3dmodels&#34;) \&#xA;    .load()&#xA;&#xA;df.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Sequences&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about sequences &lt;a href=&#34;https://docs.cognite.com/dev/concepts/resource_types/sequences.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// List all sequences&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, myApiKey)&#xA;  .option(&#34;type&#34;, &#34;sequences&#34;)&#xA;  .load()&#xA;&#xA;// Create new sequence using Spark SQL&#xA;spark.sql(&#34;&#34;&#34;&#xA; |select &#39;c|$key&#39; as externalId,&#xA; |&#39;c seq&#39; as name,&#xA; |&#39;Sequence C detailed description&#39; as description,&#xA; |array(&#xA; |  named_struct(&#xA; |    &#39;metadata&#39;, map(&#39;foo&#39;, &#39;bar&#39;, &#39;nothing&#39;, NULL),&#xA; |    &#39;name&#39;, &#39;column 1&#39;,&#xA; |    &#39;externalId&#39;, &#39;c_col1&#39;,&#xA; |    &#39;valueType&#39;, &#39;STRING&#39;&#xA; |  )&#xA; |) as columns&#xA;&#34;&#34;&#34;.stripMargin)&#xA;.write.format(&#34;cognite.spark.v1&#34;)&#xA;.option(&#34;apiKey&#34;, myApiKey)&#xA;.option(&#34;type&#34;, &#34;sequences&#34;)&#xA;.option(&#34;onconflict&#34;, &#34;abort&#34;)&#xA;.save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# List all sequences&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;sequences&#34;) \&#xA;    .load()&#xA;&#xA;# Create new sequence using Spark SQL&#xA;spark.sql(&#xA;    &#34;select &#39;c|$key&#39; as externalId,&#34; \&#xA;    &#34; &#39;c seq&#39; as name,&#34; \&#xA;    &#34; &#39;Sequence C detailed description&#39; as description,&#34; \&#xA;    &#34; array(&#34; \&#xA;    &#34;   named_struct(&#34; \&#xA;    &#34;     &#39;metadata&#39;, map(&#39;foo&#39;, &#39;bar&#39;, &#39;nothing&#39;, NULL),&#34; \&#xA;    &#34;     &#39;name&#39;, &#39;column 1&#39;,&#34; \&#xA;    &#34;     &#39;externalId&#39;, &#39;c_col1&#39;,&#34; \&#xA;    &#34;     &#39;valueType&#39;, &#39;STRING&#39;&#34; \&#xA;    &#34;   )&#34; \&#xA;    &#34; ) as columns&#34;) \&#xA;    .write.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;sequences&#34;) \&#xA;    .option(&#34;onconflict&#34;, &#34;abort&#34;) \&#xA;    .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Sequence Rows&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about sequences &lt;a href=&#34;https://docs.cognite.com/dev/concepts/resource_types/sequences.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;One of two additional options must be specified:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;id&lt;/code&gt;: Cognite internal id of the sequence that is read or written to&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;externalId&lt;/code&gt;: the external Id of the sequence that is read or written to&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example. See Python example below.&#xA;&#xA;// Read sequence rows&#xA;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, myApiKey)&#xA;  .option(&#34;type&#34;, &#34;sequencerows&#34;)&#xA;  .option(&#34;id&#34;, sequenceId) // or you can use the &#34;externalId&#34; option&#xA;  .load()&#xA;&#xA;// Insert the rows into another sequence using .save()&#xA;import org.apache.spark.sql.functions._&#xA;import org.apache.spark.sql.functions.lit&#xA;&#xA;df&#xA;  .withColumn(&#34;externalId&#34;, lit(&#34;my-sequence&#34;)) // Required when writing to support writing to multiple sequences&#xA;  .write.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, myApiKey)&#xA;  .option(&#34;type&#34;, &#34;sequencerows&#34;)&#xA;  .option(&#34;onconflict&#34;, &#34;upsert&#34;)&#xA;  .option(&#34;externalId&#34;, &#34;my-sequence&#34;)&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read sequence rows&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;sequencerows&#34;) \&#xA;    .option(&#34;id&#34;, sequenceId) \&#xA;    .load()&#xA;&#xA;# Insert the rows into another sequence using .save()&#xA;from pyspark.sql.functions import lit&#xA;df \&#xA;    .withColumn(&#34;externalId&#34;, &#34;my-sequence&#34;) \  # Required when writing to support writing to multiple sequences&#xA;    .write.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;sequencerows&#34;) \&#xA;    .option(&#34;onconflict&#34;, &#34;upsert&#34;) \&#xA;    .option(&#34;externalId&#34;, &#34;my-sequence&#34;) \&#xA;    .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Labels&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about labels &lt;a href=&#34;https://docs.cognite.com/dev/concepts/resource_types/labels.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that labels can not be updated, but can only be read, created, or deleted. If you want to change a label, you can first delete it, and then recreate it with the same external id, but the new label will have a different Cognite internal id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read labels&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;labels&#34;) \&#xA;    .load()&#xA;&#xA;df.show()&#xA;&#xA;&#xA;# Write labels&#xA;spark.sql(&#xA;    &#34;select &#39;label-externalId&#39; as externalId,&#34; \&#xA;    &#34; &#39;new-label&#39; as name,&#34; \&#xA;    &#34; &#39;text&#39; as description&#34;) \&#xA;  .write.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;) \&#xA;  .option(&#34;type&#34;, &#34;labels&#34;) \&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Relationships&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about relationships &lt;a href=&#34;https://docs.cognite.com/api/v1/#tag/Relationships&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that relationships can&#39;t be updated, but can only be read, created, or deleted. If you want to change a relationship, you can first delete it, and then recreate it with the same external id. &lt;code&gt;externalId&lt;/code&gt;, &lt;code&gt;sourceExternalId&lt;/code&gt;, &lt;code&gt;sourceType&lt;/code&gt;, &lt;code&gt;targetExternalId&lt;/code&gt;, &lt;code&gt;targetType&lt;/code&gt; can&#39;t be null.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Scala Example&#xA;&#xA;// Read relationships&#xA;df = spark.read&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, myApiKey)&#xA;  .option(&#34;type&#34;, &#34;relationships&#34;)&#xA;  .load()&#xA;&#xA;df.show()&#xA;&#xA;&#xA;// Write relationships&#xA;spark&#xA;  .sql(&#xA;    s&#34;&#34;&#34;select &#39;relationships_external_id&#39; as externalId,&#xA;       |&#39;my_asset1&#39; as sourceExternalId,&#xA;       |&#39;asset&#39; as sourceType,&#xA;       |&#39;my_asset2&#39; as targetExternalId,&#xA;       |&#39;asset&#39; as targetType,&#xA;       | array(&#39;scala-sdk-relationships-test-label1&#39;) as labels,&#xA;       | 0.7 as confidence,&#xA;       | cast(from_unixtime(0) as timestamp) as startTime,&#xA;       | cast(from_unixtime(1) as timestamp) as endTime&#34;&#34;&#34;.stripMargin)&#xA;  .write&#xA;  .format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;type&#34;, &#34;relationships&#34;)&#xA;  .option(&#34;apiKey&#34;, myApiKey)&#xA;  .save() &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;relationships&#34;) \&#xA;    .load()&#xA;&#xA;df.show()&#xA;&#xA;&#xA;# Get a reference to the relationships in your project&#xA;myProjectDf = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;relationships&#34;) \&#xA;    .load()&#xA;myProjectDf.createTempView(&#34;relationships&#34;)&#xA;&#xA;&#xA;# Insert the relationships in your own project using .save()&#xA;spark.sql(&#xA;    &#34;select &#39;relationships_external_id&#39; as externalId, \&#xA;      &#39;my_asset1&#39; as sourceExternalId, \&#xA;      &#39;asset&#39; as sourceType, \&#xA;      &#39;my_asset2&#39; as targetExternalId, \&#xA;      &#39;asset&#39; as targetType, \&#xA;      array(&#39;scala-sdk-relationships-test-label1&#39;) as labels, \&#xA;      0.7 as confidence, \&#xA;      cast(from_unixtime(0) as timestamp) as startTime, \&#xA;      cast(from_unixtime(1) as timestamp) as endTime&#34;) \&#xA;  .write.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;) \&#xA;  .option(&#34;type&#34;, &#34;relationships&#34;) \&#xA;  .option(&#34;onconflict&#34;, &#34;abort&#34;) \&#xA;  .save()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data sets&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about labels &lt;a href=&#34;https://docs.cognite.com/api/v1/#tag/Data-sets&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that data sets can be read, created and updated, but not deleted.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# Read data sets&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;    .option(&#34;apiKey&#34;, myApiKey) \&#xA;    .option(&#34;type&#34;, &#34;datasets&#34;) \&#xA;    .load()&#xA;&#xA;df.show()&#xA;&#xA;&#xA;# Write labels&#xA;spark.sql(&#xA;    &#34;select &#39;new-datasets&#39; as externalId,&#34; \&#xA;    &#34; &#39;New Dataset&#39; as name,&#34; \&#xA;    &#34; &#39;text&#39; as description&#34;) \&#xA;    &#34; null as metadata&#34;) \&#xA;    &#34; true as writeProtected&#34;) \&#xA;  .write.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;) \&#xA;  .option(&#34;type&#34;, &#34;datasets&#34;) \&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;RAW tables&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about RAW tables &lt;a href=&#34;https://doc.cognitedata.com/api/v1/#tag/Raw&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;RAW tables are organized in databases and tables that you need to provide as options to the DataFrameReader. &lt;code&gt;publicdata&lt;/code&gt; does not contain any RAW tables so you&#39;ll need access to a project with raw table data.&lt;/p&gt; &#xA;&lt;p&gt;Two additional options are required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;database&lt;/code&gt;: The name of the database in Cognite Data Fusion&#39;s RAW storage to use. The database must exist, and will not be created if it does not.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;table&lt;/code&gt;: The name of the table in Cognite Data Fusion&#39;s RAW storage to use. The table must exist in the database specified in the &lt;code&gt;database&lt;/code&gt; option, and will not be created if it does not.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optionally, you can have Spark infer the DataFrame schema with the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;inferSchema&lt;/code&gt;: Set this to &lt;code&gt;&#34;true&#34;&lt;/code&gt; to enable schema inference. You can also use the inferred schema can also be used for inserting new rows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;inferSchemaLimit&lt;/code&gt;: The number of rows to use for inferring the schema of the table. The default is to read all rows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;collectSchemaInferenceMetrics&lt;/code&gt;: Whether metrics should be collected about the read operations for schema inference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;rawEnsureParent&lt;/code&gt;: When set to true, the parent database and table will be creates if it does not exists already.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, &#34;myApiKey&#34;)&#xA;  .option(&#34;type&#34;, &#34;raw&#34;)&#xA;  .option(&#34;database&#34;, &#34;database-name&#34;) // a RAW database from your project&#xA;  .option(&#34;table&#34;, &#34;table-name&#34;) // name of a table in &#34;database-name&#34;&#xA;  .load()&#xA;df.createTempView(&#34;tablename&#34;)&#xA;&#xA;// Insert some new values&#xA;spark.sql(&#34;&#34;&#34;insert into tablename values (&#34;key&#34;, &#34;values&#34;)&#34;&#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python Example&#xA;&#xA;# database-name -&amp;gt; a RAW database from your project&#xA;# table-name -&amp;gt; name of a table in &#34;database-name&#34;&#xA;df = spark.read.format(&#34;cognite.spark.v1&#34;) \&#xA;  .option(&#34;apiKey&#34;, myApiKey) \&#xA;  .option(&#34;type&#34;, &#34;raw&#34;) \&#xA;  .option(&#34;database&#34;, &#34;database-name&#34;) \&#xA;  .option(&#34;table&#34;, &#34;table-name&#34;) \&#xA;  .load()&#xA;&#xA;df.createTempView(&#34;tablename&#34;)&#xA;&#xA;# Insert some new values&#xA;spark.sql(&#34;insert into tablename values (&#39;key&#39;, &#39;values&#39;)&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details on how to insert a dataframe into a RAW table, see the &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/docs/assets-raw-backup.py&#34;&gt;example, where we show how to backup assets to a RAW table&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comprehensive examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cognitedata/cdp-spark-datasource/raw/master/docs/public_transport_example.ipynb&#34;&gt;GTFS public transportation dataset&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shows how to create assets, asset hierarchies, events and RAW from CSV files.&lt;/li&gt; &#xA;   &lt;li&gt;Shows how to query events and assets using Spark SQL.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cognitedata/cdp-spark-datasource/raw/master/docs/assets-raw-backup.py&#34;&gt;Asset backup to RAW&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shows how to create RAW rows from existing CDF resources.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build the project with sbt&lt;/h2&gt; &#xA;&lt;p&gt;The project runs read-only integration tests against the Open Industrial Data project. Navigate to &lt;a href=&#34;https://openindustrialdata.com/&#34;&gt;https://openindustrialdata.com/&lt;/a&gt; to get an API key and store it in the &lt;code&gt;TEST_API_KEY_READ&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;To run the write integration tests, you&#39;ll also need to set the &lt;code&gt;TEST_API_KEY_WRITE&lt;/code&gt; environment variable to an API key for a project where you have write access.&lt;/p&gt; &#xA;&lt;p&gt;If you are using the SBT shell in IntelliJ or similar and want to get it to pick up environment variables from a file, you can create a file in this directory named &lt;code&gt;.env&lt;/code&gt; containing environment variables, one per line, of the format &lt;code&gt;ENVIRONMENT_VARIABLE_NAME=value&lt;/code&gt;. See &lt;a href=&#34;https://github.com/mefellows/sbt-dotenv&#34;&gt;sbt-dotenv&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Set up&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;First run &lt;code&gt;sbt compile&lt;/code&gt; to generate Scala sources for protobuf.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have set &lt;code&gt;TEST_API_KEY_WRITE&lt;/code&gt;, run the Python files &lt;code&gt;scripts/createThreeDData.py&lt;/code&gt; and &lt;code&gt;scripts/createFilesMetaData.py&lt;/code&gt; (You need to install the cognite-sdk-python and set the &lt;code&gt;PROJECT&lt;/code&gt; and &lt;code&gt;TEST_API_KEY_WRITE&lt;/code&gt; environment variables).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This uploads a 3D model to your project that you can use for testing.&lt;/p&gt; &#xA;&lt;h3&gt;Run the tests&lt;/h3&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;all tests&lt;/strong&gt;, run &lt;code&gt;sbt test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;groups of tests&lt;/strong&gt;, enter sbt shell mode &lt;code&gt;sbt&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;only the read-only tests&lt;/strong&gt;, run &lt;code&gt;sbt&amp;gt; testOnly -- -n ReadTest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;only the write tests&lt;/strong&gt;, run &lt;code&gt;sbt&amp;gt; testOnly -- -n WriteTest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;all tests except the write tests&lt;/strong&gt;, run &lt;code&gt;sbt&amp;gt; testOnly -- -l WriteTest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To &lt;strong&gt;skip the read/write tests in assembly&lt;/strong&gt;, add &lt;code&gt;test in assembly := {}&lt;/code&gt; to build.sbt, or run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows: &lt;code&gt;sbt &#34;set test in assembly := {}&#34; assembly&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Linux/macos: &lt;code&gt;sbt &#39;set test in assembly := {}&#39; assembly&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run the project locally with spark-shell&lt;/h2&gt; &#xA;&lt;p&gt;To download the spark data source, simply add the Maven coordinates for the package using the &lt;code&gt;--packages&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;Get an API-key for the Open Industrial Data project at &lt;a href=&#34;https://openindustrialdata.com&#34;&gt;https://openindustrialdata.com&lt;/a&gt; and run the following commands (replace &amp;lt;release&amp;gt; with the release you&#39;d like, for example 1.2.0):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;$&amp;gt; spark-shell --packages com.cognite.spark.datasource:cdf-spark-datasource_2.11:&amp;lt;latest-release&amp;gt;&#xA;scala&amp;gt; val apiKey=&#34;secret-key-you-have&#34;&#xA;scala&amp;gt; val df = spark.read.format(&#34;cognite.spark.v1&#34;)&#xA;  .option(&#34;apiKey&#34;, apiKey)&#xA;  .option(&#34;batchSize&#34;, &#34;1000&#34;)&#xA;  .option(&#34;limitPerPartition&#34;, &#34;1000&#34;)&#xA;  .option(&#34;type&#34;, &#34;assets&#34;)&#xA;  .load()&#xA;&#xA;df: org.apache.spark.sql.DataFrame = [name: string, parentId: bigint ... 3 more fields]&#xA;&#xA;scala&amp;gt; df.count&#xA;res0: Long = 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you&#39;re on an older version than &lt;code&gt;1.1.0&lt;/code&gt; you&#39;ll need to use the old name, &lt;code&gt;cdp-spark-datasource&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run cdf_dump locally&lt;/h2&gt; &#xA;&lt;p&gt;To run &lt;code&gt;cdf_dump&lt;/code&gt; from the sources, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt &#34;cdfdump/run --help&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/cognitedata/cdp-spark-datasource/master/cdf_dump&#34;&gt;./cdf_dump&lt;/a&gt; for more details.&lt;/p&gt;</summary>
  </entry>
</feed>