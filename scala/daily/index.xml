<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-31T01:52:29Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>playframework/playframework</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/playframework/playframework</id>
    <link href="https://github.com/playframework/playframework" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Play Framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Play Framework - The High Velocity Web Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/playframework&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/playframework?label=follow&amp;amp;style=flat&amp;amp;logo=twitter&amp;amp;color=brightgreen&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/g5s2vtZ4Fa&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/931647755942776882?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/playframework/playframework/discussions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/discussions/playframework/playframework?&amp;amp;logo=github&amp;amp;color=brightgreen&#34; alt=&#34;GitHub Discussions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/tags/playframework&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=stackoverflow&amp;amp;logo=stackoverflow&amp;amp;logoColor=fe7a16&amp;amp;color=brightgreen&amp;amp;message=playframework&#34; alt=&#34;StackOverflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCRp6QDm5SDjbIuisUpxV9cg&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/views/UCRp6QDm5SDjbIuisUpxV9cg?label=watch&amp;amp;logo=youtube&amp;amp;style=flat&amp;amp;color=brightgreen&amp;amp;logoColor=ff0000&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.twitch.tv/playframework&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitch/status/playframework?logo=twitch&amp;amp;logoColor=white&amp;amp;color=brightgreen&amp;amp;label=live%20stream&#34; alt=&#34;Twitch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/playframework&#34;&gt;&lt;img src=&#34;https://img.shields.io/opencollective/all/playframework?label=financial%20contributors&amp;amp;logo=open-collective&#34; alt=&#34;OpenCollective&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/playframework/playframework/actions/workflows/build-test.yml&#34;&gt;&lt;img src=&#34;https://github.com/playframework/playframework/actions/workflows/build-test.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mvnrepository.com/artifact/com.typesafe.play/play_2.13&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/com.typesafe.play/play_2.13.svg?logo=apache-maven&#34; alt=&#34;Maven&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/playframework/playframework&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/repo-size/playframework/playframework.svg?logo=git&#34; alt=&#34;Repository size&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://scala-steward.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Scala_Steward-helping-blue.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=&#34; alt=&#34;Scala Steward badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mergify.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://api.mergify.com/v1/badges/playframework/playframework&amp;amp;style=flat&#34; alt=&#34;Mergify Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Play Framework combines productivity and performance making it easy to build scalable web applications with Java and Scala. Play is developer friendly with a &#34;just hit refresh&#34; workflow and built-in testing support. With Play, applications scale predictably due to a stateless and non-blocking architecture. By being RESTful by default, including assets compilers, JSON &amp;amp; WebSocket support, Play is a perfect fit for modern web &amp;amp; mobile applications.&lt;/p&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com&#34;&gt;www.playframework.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/download&#34;&gt;Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/documentation/latest/Installing&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/documentation/latest/NewApplication&#34;&gt;Create a new application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/documentation/latest/ScalaHome&#34;&gt;Play for Scala developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/documentation/latest/JavaHome&#34;&gt;Play for Java developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/documentation/latest/BuildingFromSource&#34;&gt;Build from source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/playframework/playframework/issues&#34;&gt;Search or create issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/tagged/playframework&#34;&gt;Get help&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playframework.com/contributing&#34;&gt;Contribute&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsors &amp;amp; Backers&lt;/h2&gt; &#xA;&lt;p&gt;If you find Play useful for work, please consider asking your company to support this Open Source project by &lt;a href=&#34;https://www.playframework.com/sponsors&#34;&gt;becoming a sponsor&lt;/a&gt;.&lt;br&gt; You can also individually sponsor the project by &lt;a href=&#34;https://www.playframework.com/sponsors&#34;&gt;becoming a backer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://opencollective.com/playframework&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://opencollective.com/playframework/donate/button@2x.png?color=blue&#34; width=&#34;250&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Thank you to our premium sponsors!&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://informaticon.com/&#34;&gt;&lt;img src=&#34;https://www.playframework.com/assets/images/home/sponsors/61220b8306493af6a21b7db17de7f4b2-informaticon-logo-full-color.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://cedarlakeventures.com/&#34;&gt;&lt;img src=&#34;https://www.playframework.com/assets/images/home/sponsors/bec2b526c9ce52c051f9089a10044867-cedar-lake-ventures.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://iterable.com/&#34;&gt;&lt;img src=&#34;https://www.playframework.com/assets/images/home/sponsors/61ddb4c3665b621e6672181f97196748-iterable.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pronto.net/&#34;&gt;&lt;img src=&#34;https://www.playframework.com/assets/images/home/sponsors/c77b1d664f10a1c9cb19b97c6d8bd204-pronto-software.png&#34; width=&#34;250&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://civiform.us/&#34;&gt;&lt;img src=&#34;https://www.playframework.com/assets/images/home/sponsors/cb047b3782866c962c4d6a35b056b809-civiform.png&#34; width=&#34;250&#34;&gt; &lt;/a&gt;&#xA;&lt;/div&gt;&#xA;&lt;a href=&#34;https://civiform.us/&#34;&gt; &lt;h3&gt;Thank you to all our backers!&lt;/h3&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://civiform.us/&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://opencollective.com/playframework#section-contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/playframework/organizations.svg?width=890&amp;amp;button=false&amp;amp;avatarHeight=46&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/playframework#section-contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/playframework/individuals.svg?width=890&amp;amp;button=false&amp;amp;avatarHeight=46&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (C) Lightbend Inc. (&lt;a href=&#34;https://www.lightbend.com&#34;&gt;https://www.lightbend.com&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this project except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rtyley/bfg-repo-cleaner</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/rtyley/bfg-repo-cleaner</id>
    <link href="https://github.com/rtyley/bfg-repo-cleaner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Removes large or troublesome blobs like git-filter-branch does, but faster. And written in Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BFG Repo-Cleaner &lt;a href=&#34;https://travis-ci.com/rtyley/bfg-repo-cleaner&#34;&gt;&lt;img src=&#34;https://travis-ci.com/rtyley/bfg-repo-cleaner.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Removes large or troublesome blobs like git-filter-branch does, but faster - and written in Scala&lt;/em&gt; - &lt;a href=&#34;https://j.mp/fund-bfg&#34;&gt;Fund the BFG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bfg --strip-blobs-bigger-than 1M --replace-text banned.txt repo.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The BFG is a simpler, faster (&lt;a href=&#34;https://docs.google.com/spreadsheet/ccc?key=0AsR1d5Zpes8HdER3VGU1a3dOcmVHMmtzT2dsS2xNenc&#34;&gt;10 - 720x&lt;/a&gt; faster) alternative to &lt;code&gt;git-filter-branch&lt;/code&gt; for cleansing bad data out of your Git repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removing &lt;strong&gt;Crazy Big Files&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Removing &lt;strong&gt;Passwords, Credentials&lt;/strong&gt; &amp;amp; other &lt;strong&gt;Private data&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main documentation for The BFG is here : &lt;strong&gt;&lt;a href=&#34;https://rtyley.github.io/bfg-repo-cleaner/&#34;&gt;https://rtyley.github.io/bfg-repo-cleaner/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>twitter/twitter-server</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/twitter/twitter-server</id>
    <link href="https://github.com/twitter/twitter-server" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Twitter-Server defines a template from which services at Twitter are built&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TwitterServer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/twitter-server/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/twitter-server/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/twitter-server&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/twitter-server/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/twitter-server/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/twitter-server_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/twitter-server_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TwitterServer defines a template from which servers at Twitter are built. It provides common application components such as an administrative HTTP server, tracing, stats, etc. These features are wired in correctly for use in production at Twitter.&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://twitter.github.io/twitter-server/&#34;&gt;user guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/twitter-server_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/twitter-server/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/twitter-server/&#34;&gt;https://twitter.github.io/twitter-server/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/twitter-server/&#34;&gt;https://github.com/twitter/twitter-server/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of TwitterServer, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/twitter-server/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2013 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenXiangShan/XiangShan</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/OpenXiangShan/XiangShan</id>
    <link href="https://github.com/OpenXiangShan/XiangShan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source high-performance RISC-V processor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiangShan&lt;/h1&gt; &#xA;&lt;p&gt;XiangShan (香山) is an open-source high-performance RISC-V processor project.&lt;/p&gt; &#xA;&lt;p&gt;中文说明&lt;a href=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/readme.zh-cn.md&#34;&gt;在此&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2020-2022 by Institute of Computing Technology, Chinese Academy of Sciences.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2020-2022 by Peng Cheng Laboratory.&lt;/p&gt; &#xA;&lt;h2&gt;Docs and slides&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan-doc&#34;&gt;XiangShan-doc&lt;/a&gt; is our official documentation repository. It contains design spec., technical slides, tutorial and more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Micro-architecture documentation of XiangShan has been published. Please check out &lt;a href=&#34;https://xiangshan-doc.readthedocs.io&#34;&gt;https://xiangshan-doc.readthedocs.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Follow us&lt;/h2&gt; &#xA;&lt;p&gt;Wechat/微信：香山开源处理器&lt;/p&gt; &#xA;&lt;div align=&#34;left&#34;&gt;&#xA; &lt;img width=&#34;340&#34; height=&#34;117&#34; src=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/wechat.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;Zhihu/知乎：&lt;a href=&#34;https://www.zhihu.com/people/openxiangshan&#34;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Weibo/微博：&lt;a href=&#34;https://weibo.com/u/7706264932&#34;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can contact us through &lt;a href=&#34;mailto:xiangshan-all@ict.ac.cn&#34;&gt;our mail list&lt;/a&gt;. All mails from this list will be archived to &lt;a href=&#34;https://www.mail-archive.com/xiangshan-all@ict.ac.cn/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The first stable micro-architecture of XiangShan is called Yanqihu (雁栖湖) on this &lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan/tree/yanqihu&#34;&gt;branch&lt;/a&gt;, which has been developed since June 2020. The current version of XiangShan, also known as Nanhu (南湖), is still under development on the master branch.&lt;/p&gt; &#xA;&lt;p&gt;The micro-architecture overview of Nanhu (南湖) is shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/xs-arch-nanhu.svg?sanitize=true&#34; alt=&#34;xs-arch-nanhu&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sub-directories Overview&lt;/h2&gt; &#xA;&lt;p&gt;Some of the key directories are shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── src&#xA;│   └── main/scala         # design files&#xA;│       ├── device         # virtual device for simulation&#xA;│       ├── system         # SoC wrapper&#xA;│       ├── top            # top module&#xA;│       ├── utils          # utilization code&#xA;│       ├── xiangshan      # main design code&#xA;│       └── xstransforms   # some useful firrtl transforms&#xA;├── scripts                # scripts for agile development&#xA;├── fudian                 # floating unit submodule of XiangShan&#xA;├── huancun                # L2/L3 cache submodule of XiangShan&#xA;├── difftest               # difftest co-simulation framework&#xA;└── ready-to-run           # pre-built simulation images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;IDE Support&lt;/h2&gt; &#xA;&lt;h3&gt;bsp&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;make bsp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;IDEA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;make idea&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generate Verilog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make verilog&lt;/code&gt; to generate verilog code. The output file is &lt;code&gt;build/XSTop.v&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run Programs by Simulation&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;NEMU_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&#34;https://github.com/OpenXiangShan/NEMU&#34;&gt;NEMU project&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;NOOP_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the XiangShan project.&lt;/li&gt; &#xA; &lt;li&gt;Set environment variable &lt;code&gt;AM_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&#34;https://github.com/OpenXiangShan/nexus-am&#34;&gt;AM project&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;mill&lt;/code&gt;. Refer to &lt;a href=&#34;https://com-lihaoyi.github.io/mill/mill/Intro_to_Mill.html#_installation&#34;&gt;the Manual section in this guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone this project and run &lt;code&gt;make init&lt;/code&gt; to initialize submodules.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run with simulator&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://verilator.org/guide/latest/&#34;&gt;Verilator&lt;/a&gt;, the open-source Verilog simulator.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make emu&lt;/code&gt; to build the C++ simulator &lt;code&gt;./build/emu&lt;/code&gt; with Verilator.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;./build/emu --help&lt;/code&gt; for run-time arguments of the simulator.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; and &lt;code&gt;verilator.mk&lt;/code&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make emu CONFIG=MinimalConfig EMU_THREADS=2 -j10&#xA;./build/emu -b 0 -e 0 -i ./ready-to-run/coremark-2-iteration.bin --diff ./ready-to-run/riscv64-nemu-interpreter-so&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting Guide&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan/wiki/Troubleshooting-Guide&#34;&gt;Troubleshooting Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;In the development of XiangShan, some sub-modules from the open-source community are employed. All relevant usage is listed below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Sub-module&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;   &lt;th&gt;Detail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;L2 Cache/LLC&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ucb-bar/block-inclusivecache-sifive&#34;&gt;Sifive block-inclusivecache&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our new L2/L3 design are inspired by Sifive&#39;s &lt;code&gt;block-inclusivecache&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Diplomacy/TileLink&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;Rocket-chip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We reused the Diplomacy framework and TileLink utility that exist in rocket-chip to negotiate bus.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We are grateful for the support of the open-source community and encourage other open-source projects to reuse our code within the scope of the &lt;a href=&#34;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/LICENSE&#34;&gt;license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/SynapseML</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/microsoft/SynapseML</id>
    <link href="https://github.com/microsoft/SynapseML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple and Distributed Machine Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://mmlspark.azureedge.net/icons/mmlspark.svg?sanitize=true&#34; alt=&#34;SynapseML&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Synapse Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;SynapseML (previously MMLSpark) is an open source library to simplify the creation of scalable machine learning pipelines. SynapseML builds on &lt;a href=&#34;https://github.com/apache/spark&#34;&gt;Apache Spark&lt;/a&gt; and SparkML to enable new kinds of machine learning, analytics, and model deployment workflows. SynapseML adds many deep learning and data science tools to the Spark ecosystem, including seamless integration of Spark Machine Learning pipelines with the &lt;a href=&#34;https://onnx.ai&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt;, &lt;a href=&#34;https://github.com/Microsoft/LightGBM&#34;&gt;LightGBM&lt;/a&gt;, &lt;a href=&#34;https://azure.microsoft.com/en-us/services/cognitive-services/&#34;&gt;The Cognitive Services&lt;/a&gt;, &lt;a href=&#34;https://vowpalwabbit.org/&#34;&gt;Vowpal Wabbit&lt;/a&gt;, and &lt;a href=&#34;http://www.opencv.org/&#34;&gt;OpenCV&lt;/a&gt;. These tools enable powerful and highly-scalable predictive and analytical models for a variety of datasources.&lt;/p&gt; &#xA;&lt;p&gt;SynapseML also brings new networking capabilities to the Spark Ecosystem. With the HTTP on Spark project, users can embed &lt;strong&gt;any&lt;/strong&gt; web service into their SparkML models. For production grade deployment, the Spark Serving project enables high throughput, sub-millisecond latency web services, backed by your Spark cluster.&lt;/p&gt; &#xA;&lt;p&gt;SynapseML requires Scala 2.12, Spark 3.2+, and Python 3.6+. See the API documentation &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.9.5/scala/index.html#package&#34;&gt;for Scala&lt;/a&gt; and &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.9.5/pyspark/index.html&#34;&gt;for PySpark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topics&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Build&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://msdata.visualstudio.com/A365/_build/latest?definitionId=17563&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://msdata.visualstudio.com/A365/_apis/build/status/microsoft.SynapseML?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/Microsoft/SynapseML&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/Microsoft/SynapseML/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Version&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/Microsoft/SynapseML/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-0.9.5-blue&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Microsoft/SynapseML/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-notes-blue&#34; alt=&#34;Release Notes&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#sbt&#34;&gt;&lt;img src=&#34;https://mmlspark.blob.core.windows.net/icons/badges/master_version3.svg?sanitize=true&#34; alt=&#34;Snapshot Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Docs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.9.5/scala/index.html#package&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=scala&amp;amp;color=blue&amp;amp;logo=scala&#34; alt=&#34;Scala Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.9.5/pyspark/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=python&amp;amp;color=blue&amp;amp;logo=python&#34; alt=&#34;PySpark Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.08744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/academic-paper-7fdcf7&#34; alt=&#34;Academic Paper&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Support&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://gitter.im/Microsoft/MMLSpark?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/MMLSpark.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;mailto:synapseml-support@microsoft.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mail-synapseml--support-brightgreen&#34; alt=&#34;Mail&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Binder&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/microsoft/SynapseML/master?labpath=notebooks%2Ffeatures&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#documentation-and-examples&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#setup-and-installation&#34;&gt;Setup and installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#papers&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#learn-more&#34;&gt;Learn More&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#contributing--feedback&#34;&gt;Contributing &amp;amp; feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#other-relevant-projects&#34;&gt;Other relevant projects&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/vw-blue-dark-orange.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/decision_tree_recolor.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/mmlspark_serving_recolor.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/vw/about/&#34;&gt;&lt;strong&gt;Vowpal Wabbit on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/cognitive_services/CognitiveServices%20-%20Overview/&#34;&gt;&lt;strong&gt;The Cognitive Services for Big Data&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/&#34;&gt;&lt;strong&gt;LightGBM on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/spark_serving/about/&#34;&gt;&lt;strong&gt;Spark Serving&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fast, Sparse, and Effective Text Analytics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Leverage the Microsoft Cognitive Services at Unprecedented Scales in your existing SparkML pipelines&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Train Gradient Boosted Machines with LightGBM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Serve any Spark Computation as a Web Service with Sub-Millisecond Latency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/microservice_recolor.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/onnxai-ar21_crop.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/scales.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/bindings.png&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/cognitive_services/CognitiveServices%20-%20Overview/#arbitrary-web-apis&#34;&gt;&lt;strong&gt;HTTP on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/onnx/about/&#34;&gt;&lt;strong&gt;ONNX on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Model%20Interpretation%20on%20Spark/&#34;&gt;&lt;strong&gt;Responsible AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/reference/developer-readme/#packagepython&#34;&gt;&lt;strong&gt;Spark Binding Autogeneration&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;An Integration Between Spark and the HTTP Protocol, enabling Distributed Microservice Orchestration&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Distributed and Hardware Accelerated Model Inference on Spark&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Understand Opaque-box Models and Measure Dataset Biases&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Automatically Generate Spark bindings for PySpark and SparklyR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/isolation forest 3.svg&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/cyberml.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/conditional_knn.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/documentation/estimators/estimators_core/#isolationforest&#34;&gt;&lt;strong&gt;Isolation Forest on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/SynapseML/raw/master/notebooks/features/other/CyberML%20-%20Anomalous%20Access%20Detection.ipynb&#34;&gt;&lt;strong&gt;CyberML&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/other/ConditionalKNN%20-%20Exploring%20Art%20Across%20Cultures/&#34;&gt;&lt;strong&gt;Conditional KNN&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Distributed Nonlinear Outlier Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Machine Learning Tools for Cyber Security&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scalable KNN Models with Conditional Queries&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documentation and Examples&lt;/h2&gt; &#xA;&lt;p&gt;For quickstarts, documentation, demos, and examples please see our &lt;a href=&#34;https://aka.ms/spark&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup and installation&lt;/h2&gt; &#xA;&lt;p&gt;First select the correct platform that you are installing SynapseML into:&lt;/p&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#synapse-analytics&#34;&gt;Synapse Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#databricks&#34;&gt;Databricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#python-standalone&#34;&gt;Python Standalone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#spark-submit&#34;&gt;Spark Submit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#apache-livy-and-hdinsight&#34;&gt;Apachy Livy and HDInsight&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#r-beta&#34;&gt;R (Beta)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#building-from-source&#34;&gt;Building from Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;h3&gt;Synapse Analytics&lt;/h3&gt; &#xA;&lt;p&gt;In Azure Synapse notebooks please place the following in the first cell of your notebook.&lt;/p&gt; &#xA;&lt;p&gt;For Spark 3.2 Pools:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;  &#34;name&#34;: &#34;synapseml&#34;,&#xA;  &#34;conf&#34;: {&#xA;      &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.9.5&#34;,&#xA;      &#34;spark.jars.repositories&#34;: &#34;https://mmlspark.azureedge.net/maven&#34;,&#xA;      &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12&#34;,&#xA;      &#34;spark.yarn.user.classpath.first&#34;: &#34;true&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Spark 3.1 Pools:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;  &#34;name&#34;: &#34;synapseml&#34;,&#xA;  &#34;conf&#34;: {&#xA;      &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.9.5-13-d1b51517-SNAPSHOT&#34;,&#xA;      &#34;spark.jars.repositories&#34;: &#34;https://mmlspark.azureedge.net/maven&#34;,&#xA;      &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12&#34;,&#xA;      &#34;spark.yarn.user.classpath.first&#34;: &#34;true&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install at the pool level instead of the notebook level &lt;a href=&#34;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/how-to-set-spark-pyspark-custom-configs-in-synapse-workspace/ba-p/2114434&#34;&gt;add the spark properties listed above to the pool configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Databricks&lt;/h3&gt; &#xA;&lt;p&gt;To install SynapseML on the &lt;a href=&#34;http://community.cloud.databricks.com&#34;&gt;Databricks cloud&lt;/a&gt;, create a new &lt;a href=&#34;https://docs.databricks.com/user-guide/libraries.html#libraries-from-maven-pypi-or-spark-packages&#34;&gt;library from Maven coordinates&lt;/a&gt; in your workspace.&lt;/p&gt; &#xA;&lt;p&gt;For the coordinates use: &lt;code&gt;com.microsoft.azure:synapseml_2.12:0.9.5&lt;/code&gt; with the resolver: &lt;code&gt;https://mmlspark.azureedge.net/maven&lt;/code&gt;. Ensure this library is attached to your target cluster(s).&lt;/p&gt; &#xA;&lt;p&gt;Finally, ensure that your Spark cluster has at least Spark 3.2 and Scala 2.12. If you encounter Netty dependency issues please use DBR 10.1.&lt;/p&gt; &#xA;&lt;p&gt;You can use SynapseML in both your Scala and PySpark notebooks. To get started with our example notebooks import the following databricks archive:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;https://mmlspark.blob.core.windows.net/dbcs/SynapseMLExamplesv0.9.5.dbc&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python Standalone&lt;/h3&gt; &#xA;&lt;p&gt;To try out SynapseML on a Python (or Conda) installation you can get Spark installed via pip with &lt;code&gt;pip install pyspark&lt;/code&gt;. You can then use &lt;code&gt;pyspark&lt;/code&gt; as in the above example, or from python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyspark&#xA;spark = pyspark.sql.SparkSession.builder.appName(&#34;MyApp&#34;) \&#xA;            .config(&#34;spark.jars.packages&#34;, &#34;com.microsoft.azure:synapseml_2.12:0.9.5&#34;) \&#xA;            .getOrCreate()&#xA;import synapse.ml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark Submit&lt;/h3&gt; &#xA;&lt;p&gt;SynapseML can be conveniently installed on existing Spark clusters via the &lt;code&gt;--packages&lt;/code&gt; option, examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark-shell --packages com.microsoft.azure:synapseml_2.12:0.9.5&#xA;pyspark --packages com.microsoft.azure:synapseml_2.12:0.9.5&#xA;spark-submit --packages com.microsoft.azure:synapseml_2.12:0.9.5 MyApp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;If you are building a Spark application in Scala, add the following lines to your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;com.microsoft.azure&#34; % &#34;synapseml_2.12&#34; % &#34;0.9.5&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Apache Livy and HDInsight&lt;/h3&gt; &#xA;&lt;p&gt;To install SynapseML from within a Jupyter notebook served by Apache Livy the following configure magic can be used. You will need to start a new session after this configure cell is executed.&lt;/p&gt; &#xA;&lt;p&gt;Excluding certain packages from the library may be necessary due to current issues with Livy 0.5.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;    &#34;name&#34;: &#34;synapseml&#34;,&#xA;    &#34;conf&#34;: {&#xA;        &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.9.5&#34;,&#xA;        &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to evaluate SynapseML is via our pre-built Docker container. To do so, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it -p 8888:8888 -e ACCEPT_EULA=yes mcr.microsoft.com/mmlspark/release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;http://localhost:8888/&#34;&gt;http://localhost:8888/&lt;/a&gt; in your web browser to run the sample notebooks. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/docs/docker.md&#34;&gt;documentation&lt;/a&gt; for more on Docker use.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To read the EULA for using the docker image, run \ &lt;code&gt;docker run -it -p 8888:8888 mcr.microsoft.com/mmlspark/release eula&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;R (Beta)&lt;/h3&gt; &#xA;&lt;p&gt;To try out SynapseML using the R autogenerated wrappers &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/website/docs/reference/R-setup.md&#34;&gt;see our instructions&lt;/a&gt;. Note: This feature is still under development and some necessary custom wrappers may be missing.&lt;/p&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;SynapseML has recently transitioned to a new build infrastructure. For detailed developer docs please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/website/docs/reference/developer-readme.md&#34;&gt;Developer Readme&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are an existing synapsemldeveloper, you will need to reconfigure your development setup. We now support platform independent development and better integrate with intellij and SBT. If you encounter issues please reach out to our support email!&lt;/p&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.08044&#34;&gt;Large Scale Intelligent Microservices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.07177&#34;&gt;Conditional Image Retrieval&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.08744&#34;&gt;MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.04031&#34;&gt;Flexible and Scalable Deep Learning with SynapseML&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit our &lt;a href=&#34;https://microsoft.github.io/SynapseML/&#34; title=&#34;aka.ms/spark&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Watch our keynote demos at &lt;a href=&#34;https://youtu.be/T_fs4C0aqD0?t=425&#34;&gt;the Spark+AI Summit 2019&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/N3ozCZXeOeU?t=472&#34;&gt;the Spark+AI European Summit 2018&lt;/a&gt;, and &lt;a href=&#34;https://databricks.com/sparkaisummit/north-america/spark-summit-2018-keynotes#Intelligent-cloud&#34; title=&#34;Developing for the Intelligent Cloud and Intelligent Edge&#34;&gt;the Spark+AI Summit 2018&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See how SynapseML is used to &lt;a href=&#34;https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr3&#34; title=&#34;Identifying snow leopards with AI&#34;&gt;help endangered species&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Explore generative adversarial artwork in &lt;a href=&#34;https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr4&#34; title=&#34;Generative art at the MET&#34;&gt;our collaboration with The MET and MIT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Explore &lt;a href=&#34;https://blogs.technet.microsoft.com/machinelearning/2018/03/05/image-data-support-in-apache-spark/&#34; title=&#34;Image Data Support in Apache Spark&#34;&gt;our collaboration with Apache Spark&lt;/a&gt; on image analysis.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing &amp;amp; feedback&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for contribution guidelines.&lt;/p&gt; &#xA;&lt;p&gt;To give feedback and/or report an issue, open a &lt;a href=&#34;https://help.github.com/articles/creating-an-issue/&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other relevant projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit&#34;&gt;Vowpal Wabbit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/LightGBM&#34;&gt;LightGBM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/DMTK&#34;&gt;DMTK: Microsoft Distributed Machine Learning Toolkit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/Recommenders&#34;&gt;Recommenders&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/alipay/jpmml-sparkml-lightgbm&#34;&gt;JPMML-SparkML plugin for converting SynapseML LightGBM models to PMML&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/CNTK&#34;&gt;Microsoft Cognitive Toolkit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Apache®, Apache Spark, and Spark® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gatling/gatling</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/gatling/gatling</id>
    <link href="https://github.com/gatling/gatling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modern Load Testing as Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gatling &lt;a href=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;What is Gatling ?&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is a load test tool. It officially supports HTTP, WebSocket, Server-Sent-Events and JMS.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finding fancy GUIs not that convenient for describing load tests, what you want is a friendly expressive DSL?&lt;/li&gt; &#xA; &lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt; &#xA; &lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Gatling is for you!&lt;/p&gt; &#xA;&lt;h2&gt;Underlying technologies&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is developed in Scala and built upon :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netty.io&#34;&gt;Netty&lt;/a&gt; for non blocking HTTP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://akka.io&#34;&gt;Akka&lt;/a&gt; for virtual users orchestration ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions, help?&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://gatling.io/docs/current/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://groups.google.com/forum/#!forum/gatling&#34;&gt;Gatling User Group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Found a real bug? Raise an &lt;a href=&#34;https://github.com/gatling/gatling/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Partners&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Takima&#34; src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/logo-takima-1-nom-bas.png&#34; width=&#34;80&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/highsoft_logo.png&#34; alt=&#34;Highsoft AS&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lichess-org/lila</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/lichess-org/lila</id>
    <link href="https://github.com/lichess-org/lila" rel="alternate"></link>
    <summary type="html">&lt;p&gt;♞ lichess.org: the forever free, adless and open source chess server ♞&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://lichess.org&#34;&gt;lichess.org&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lichess-org/lila/actions?query=workflow%3A%22Build+server%22&#34;&gt;&lt;img src=&#34;https://github.com/lichess-org/lila/workflows/Build%20server/badge.svg?sanitize=true&#34; alt=&#34;Build server&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lichess-org/lila/actions?query=workflow%3A%22Build+assets%22&#34;&gt;&lt;img src=&#34;https://github.com/lichess-org/lila/workflows/Build%20assets/badge.svg?sanitize=true&#34; alt=&#34;Build assets&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crowdin.com/project/lichess&#34;&gt;&lt;img src=&#34;https://d322cqt584bo4o.cloudfront.net/lichess/localized.svg?sanitize=true&#34; alt=&#34;Crowdin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/lichess&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-%40lichess-blue.svg?sanitize=true&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lichess-org/lila/master/public/images/home-bicolor.png&#34; alt=&#34;Lichess homepage&#34; title=&#34;Lichess comes with light and dark theme, this screenshot shows both.&#34;&gt; &#xA;&lt;p&gt;Lila (li[chess in sca]la) is a free online chess game server focused on &lt;a href=&#34;https://lichess.org/games&#34;&gt;realtime&lt;/a&gt; gameplay and ease of use.&lt;/p&gt; &#xA;&lt;p&gt;It features a &lt;a href=&#34;https://lichess.org/games/search&#34;&gt;search engine&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/ief49lif&#34;&gt;computer analysis&lt;/a&gt; distributed with &lt;a href=&#34;https://github.com/lichess-org/fishnet&#34;&gt;fishnet&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/tournament&#34;&gt;tournaments&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/simul&#34;&gt;simuls&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/forum&#34;&gt;forums&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/team&#34;&gt;teams&lt;/a&gt;, &lt;a href=&#34;https://lichess.org/training&#34;&gt;tactic trainer&lt;/a&gt;, a &lt;a href=&#34;https://lichess.org/mobile&#34;&gt;mobile app&lt;/a&gt;, and a &lt;a href=&#34;https://lichess.org/study&#34;&gt;shared analysis board&lt;/a&gt;. The UI is available in more than &lt;a href=&#34;https://crowdin.com/project/lichess&#34;&gt;130 languages&lt;/a&gt; thanks to the community.&lt;/p&gt; &#xA;&lt;p&gt;Lichess is written in &lt;a href=&#34;https://www.scala-lang.org/&#34;&gt;Scala 2.13&lt;/a&gt;, and relies on the &lt;a href=&#34;https://www.playframework.com/&#34;&gt;Play 2.8&lt;/a&gt; framework. &lt;a href=&#34;https://com-lihaoyi.github.io/scalatags/&#34;&gt;scalatags&lt;/a&gt; is used for templating. Pure chess logic is contained in the &lt;a href=&#34;https://github.com/lichess-org/scalachess&#34;&gt;scalachess&lt;/a&gt; submodule. The server is fully asynchronous, making heavy use of Scala Futures and &lt;a href=&#34;https://akka.io&#34;&gt;Akka streams&lt;/a&gt;. WebSocket connections are handled by a &lt;a href=&#34;https://github.com/lichess-org/lila-ws&#34;&gt;separate server&lt;/a&gt; that communicates using &lt;a href=&#34;https://redis.io/&#34;&gt;redis&lt;/a&gt;. Lichess talks to &lt;a href=&#34;https://stockfishchess.org/&#34;&gt;Stockfish&lt;/a&gt; deployed in an &lt;a href=&#34;https://github.com/lichess-org/fishnet&#34;&gt;AI cluster&lt;/a&gt; of donated servers. It uses &lt;a href=&#34;https://www.mongodb.com&#34;&gt;MongoDB&lt;/a&gt; to store more than 1.7 billion games, which are indexed by &lt;a href=&#34;https://github.com/elastic/elasticsearch&#34;&gt;elasticsearch&lt;/a&gt;. HTTP requests and WebSocket connections can be proxied by &lt;a href=&#34;https://nginx.org&#34;&gt;nginx&lt;/a&gt;. The web client is written in &lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;TypeScript&lt;/a&gt; and &lt;a href=&#34;https://github.com/snabbdom/snabbdom&#34;&gt;snabbdom&lt;/a&gt;, using &lt;a href=&#34;https://sass-lang.com/&#34;&gt;Sass&lt;/a&gt; to generate CSS. The &lt;a href=&#34;https://lichess.org/blog&#34;&gt;blog&lt;/a&gt; uses a free open content plan from &lt;a href=&#34;https://prismic.io&#34;&gt;prismic.io&lt;/a&gt;. All rated games are published in a &lt;a href=&#34;https://database.lichess.org&#34;&gt;free PGN database&lt;/a&gt;. Browser testing done with &lt;a href=&#34;https://www.browserstack.com&#34;&gt;Browserstack&lt;/a&gt;. Proxy detection done with &lt;a href=&#34;https://www.ip2location.com/database/ip2proxy&#34;&gt;IP2Proxy database&lt;/a&gt;. Please help us &lt;a href=&#34;https://crowdin.com/project/lichess&#34;&gt;translate Lichess with Crowdin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://lichess.org/source&#34;&gt;lichess.org/source&lt;/a&gt; for a list of repositories.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/lichess&#34;&gt;Join us on Discord&lt;/a&gt; for more info. Use &lt;a href=&#34;https://github.com/lichess-org/lila/issues&#34;&gt;GitHub issues&lt;/a&gt; for bug reports and feature requests.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;./lila # thin wrapper around sbt&#xA;run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Wiki describes &lt;a href=&#34;https://github.com/lichess-org/lila/wiki/Lichess-Development-Onboarding&#34;&gt;how to setup a development environment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;HTTP API&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to use the &lt;a href=&#34;https://lichess.org/api&#34;&gt;Lichess API&lt;/a&gt; in your applications and websites.&lt;/p&gt; &#xA;&lt;h2&gt;Supported browsers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chromium / Chrome&lt;/td&gt; &#xA;   &lt;td&gt;last 10&lt;/td&gt; &#xA;   &lt;td&gt;Full support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Firefox&lt;/td&gt; &#xA;   &lt;td&gt;61+&lt;/td&gt; &#xA;   &lt;td&gt;Full support (fastest local analysis since FF 79)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Edge&lt;/td&gt; &#xA;   &lt;td&gt;91+&lt;/td&gt; &#xA;   &lt;td&gt;Full support (reasonable support for 17+)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Opera&lt;/td&gt; &#xA;   &lt;td&gt;55+&lt;/td&gt; &#xA;   &lt;td&gt;Reasonable support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Safari&lt;/td&gt; &#xA;   &lt;td&gt;11.1+&lt;/td&gt; &#xA;   &lt;td&gt;Reasonable support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Older browsers (including any version of Internet Explorer) will not work. For your own sake, please upgrade. Security and performance, think about it!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Lila is licensed under the GNU Affero General Public License 3 or any later version at your choice with an exception for Highcharts. See &lt;a href=&#34;https://github.com/lichess-org/lila/raw/master/COPYING.md&#34;&gt;copying&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://lichess.org/thanks&#34;&gt;lichess.org/thanks&lt;/a&gt; and the contributors here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lichess-org/lila/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=lichess-org/lila&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Competence development program&lt;/h2&gt; &#xA;&lt;p&gt;Lichess would like to support its contributors in their competence development by covering costs of relevant training materials and activities. This is a small way to further empower contributors who have given their time to Lichess and to enable or improve additional contributions to Lichess in the future. For more information, including how to apply, check &lt;a href=&#34;https://lichess.org/page/competence-development&#34;&gt;Competence Development for Lichess contributors&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zio/zio</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/zio/zio</id>
    <link href="https://github.com/zio/zio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZIO — A type-safe, composable library for async and concurrent programming in Scala&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/ZIO.png&#34; alt=&#34;ZIO Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Production%20Ready-brightgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Snapshot Artifacts&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://isitmaintained.com/project/zio/zio&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/zio/zio.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scaladoc&lt;/th&gt; &#xA;   &lt;th&gt;Scaladex&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;   &lt;th&gt;Twitter&lt;/th&gt; &#xA;   &lt;th&gt;Gitpod&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://javadoc.io/doc/dev.zio/zio_2.12/latest/zio/index.html&#34;&gt;Scaladoc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://index.scala-lang.org/zio/zio/zio&#34; title=&#34;Scaladex&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/zio/zio/zio/latest.svg?sanitize=true&#34; alt=&#34;Badge-Scaladex-page&#34; title=&#34;Scaladex&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/zioscala&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/zioscala.svg?style=plastic&amp;amp;label=follow&amp;amp;logo=twitter&#34; alt=&#34;Badge-Twitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/zio/zio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod ready-to-code&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Welcome to ZIO&lt;/h1&gt; &#xA;&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt; &#xA;&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala&#39;s &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it&#39;s asynchronous or synchronous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/contributing.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/pulls&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Adopters&lt;/h1&gt; &#xA;&lt;p&gt;Following is a partial list of companies happily using ZIO in production to craft concurrent applications.&lt;/p&gt; &#xA;&lt;p&gt;Want to see your company here? &lt;a href=&#34;https://github.com/zio/zio/edit/master/README.md&#34;&gt;Submit a PR&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adgear.com/en/&#34;&gt;AdGear / Samsung Ads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adidas.com/&#34;&gt;Adidas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adpulse.io/&#34;&gt;adpulse.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adsquare.com/&#34;&gt;adsquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ayolab.com/&#34;&gt;Ayolab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://asana.com/&#34;&gt;Asana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aurinko.io/&#34;&gt;Aurinko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://auto.ru&#34;&gt;auto.ru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.de&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.avast.com&#34;&gt;Avast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bofa.com&#34;&gt;Bank of America&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bpp.it/&#34;&gt;Bpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://broad.app&#34;&gt;Broad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.caesars.com/sportsbook-and-casino&#34;&gt;Caesars Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.calcbank.com.br&#34;&gt;CalcBank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.callhandling.co.uk/&#34;&gt;Call Handling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;Carvana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cellular.de&#34;&gt;Cellular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudfarms.com&#34;&gt;Cloudfarms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codecomprehension.com&#34;&gt;CodeComprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codept.de/&#34;&gt;Codept&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com/en&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collibra.com/&#34;&gt;Collibra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.compellon.com/&#34;&gt;Compellon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complicatedrobot.com/&#34;&gt;Complicated Robot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conduktor.io&#34;&gt;Conduktor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.contramap.dev&#34;&gt;Contramap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coralogix.com&#34;&gt;Coralogix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creditkarma.com&#34;&gt;Credit Karma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.currencycloud.com/&#34;&gt;CurrencyCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://de-solution.com/&#34;&gt;D.E.Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datachef.co&#34;&gt;DataChef&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.demandbase.com&#34;&gt;Demandbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demyst.com&#34;&gt;Demyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devsisters.com/&#34;&gt;Devsisters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijdhl.nl/it&#34;&gt;DHL Parcel The Netherlands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+ Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doomoolmori.com/&#34;&gt;Doomoolmori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dowjones.com&#34;&gt;Dow Jones&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dpgrecruitment.nl&#34;&gt;DPG recruitment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.com&#34;&gt;Dream11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iot.telekom.com/en&#34;&gt;Deutsche Telekom IoT GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebay.com&#34;&gt;eBay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eaglescience.nl&#34;&gt;Eaglescience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edf.fr/&#34;&gt;Electricité de France (EDF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.enelx.com&#34;&gt;EnelX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evolution.engineering&#34;&gt;Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evo.company&#34;&gt;Evo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flipp.com/&#34;&gt;Flipp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fugo.ai&#34;&gt;Fugo.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.garnercorp.com/&#34;&gt;Garner Distributed Workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gleancompany.com&#34;&gt;Glean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://grandparade.co.uk&#34;&gt;GrandParade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://greyflower.media&#34;&gt;greyflower.media GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hunters.ai&#34;&gt;Hunters.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypefactors.com/&#34;&gt;Hypefactors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.iheart.com/&#34;&gt;iHeartRadio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ihsmarkit.com/&#34;&gt;IHS Markit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://investsuite.com/&#34;&gt;Investsuite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaizen-solutions.net/&#34;&gt;Kaizen Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kamon.io/&#34;&gt;Kamon APM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kodmagi.se&#34;&gt;Kodmagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kensu.io&#34;&gt;Kensu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lambdaworks.io/&#34;&gt;LambdaWorks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leadiq.com&#34;&gt;LeadIQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lernkunst.com/&#34;&gt;Lernkunst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liveintent.com&#34;&gt;LiveIntent Inc.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lottoland.com&#34;&gt;Lottoland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matechs.com&#34;&gt;MATECHS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://megogo.net&#34;&gt;Megogo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mylivn.com/&#34;&gt;Mylivn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newmotion.com&#34;&gt;NewMotion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nexxchange.com&#34;&gt;Nexxchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nike.com&#34;&gt;Nike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nslookup.io&#34;&gt;NsLookup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://olyro.de&#34;&gt;Olyro GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optrak.com&#34;&gt;Optrak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.performance-immo.com/&#34;&gt;Performance Immo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playtika.com&#34;&gt;Playtika&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ppcsamurai.com/&#34;&gt;PPC Samurai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.radix.bio/&#34;&gt;Radix Labs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railroad19.com&#34;&gt;Railroad19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijrandstad.nl&#34;&gt;Randstad Groep Nederland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rapidor.co&#34;&gt;Rapidor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pimsolutions.ru/&#34;&gt;PIM Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rewe-digital.com/&#34;&gt;REWE Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://riskident.com/&#34;&gt;Risk Ident&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocker.com/&#34;&gt;Rocker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rudder.io/&#34;&gt;Rudder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sanjagh.pro/&#34;&gt;Sanjagh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scalac.io/&#34;&gt;Scalac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityscorecard.io/&#34;&gt;SecurityScorecard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sentinelone.com/&#34;&gt;SentinelOne&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.signicat.com/&#34;&gt;Signicat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.sgmarkets.com/en/&#34;&gt;Société Générale Corporate and Investment Banking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaremill.com/&#34;&gt;SoftwareMill&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.streamweaver.com/&#34;&gt;StreamWeaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stuart.com/&#34;&gt;Stuart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teads.com&#34;&gt;Teads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pokemon.com/us/about-pokemon/&#34;&gt;The Pokemon Company International&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tomtom.com&#34;&gt;TomTom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tinka.com/&#34;&gt;Tinka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trackabus.com&#34;&gt;Trackabus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trainor.no&#34;&gt;Trainor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://treutech.io&#34;&gt;TreuTech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tweddle.com&#34;&gt;Tweddle Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.undo.app&#34;&gt;Undo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unit.co&#34;&gt;Unit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univalence.io&#34;&gt;Univalence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.unzer.com&#34;&gt;Unzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vakantiediscounter.nl&#34;&gt;Vakantiediscounter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.verbund.com&#34;&gt;Verbund AG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.waylay.io/&#34;&gt;Waylay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wehkamp.nl&#34;&gt;Wehkamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wolt.com/&#34;&gt;Wolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://o.yandex.ru&#34;&gt;Yandex.Classifieds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://audela.ca&#34;&gt;Audela&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valamis.com&#34;&gt;Valamis Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valsea.com&#34;&gt;Valsea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtuslab.com/&#34;&gt;VirtusLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://getvish.com&#34;&gt;Vish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vivid.money&#34;&gt;Vivid Money&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.com/&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zooz.com/&#34;&gt;Zooz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/ziverge.png&#34; alt=&#34;Ziverge&#34; title=&#34;Ziverge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;Ziverge&lt;/a&gt; is a leading contributor to ZIO.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/scalac.svg?sanitize=true&#34; alt=&#34;Scalac&#34; title=&#34;Scalac&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;Scalac&lt;/a&gt; sponsors ZIO Hackathons and contributes work to multiple projects in ZIO ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/septimal_mind.svg?sanitize=true&#34; alt=&#34;Septimal Mind&#34; title=&#34;Septimal Mind&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/softwaremill.svg?sanitize=true&#34; alt=&#34;SoftwareMill&#34; title=&#34;SoftwareMill&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/yourkit.png&#34; alt=&#34;YourKit&#34; title=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;YourKit&lt;/a&gt; generously provides use of their monitoring and profiling tools to maximize the performance of ZIO applications.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/code_of_conduct.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Come chat with us on &lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Legal&lt;/h3&gt; &#xA;&lt;p&gt;Copyright 2017 - 2020 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/berkeley-hardfloat</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/ucb-bar/berkeley-hardfloat</id>
    <link href="https://github.com/ucb-bar/berkeley-hardfloat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Berkeley Hardware Floating-Point Units&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains hardware floating-point units written in Chisel. This library contains parameterized floating-point units for fused multiply-add operations, conversions between integer and floating-point numbers, and conversions between floating-point conversions with different precision.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: These units are works in progress. They may not be yet completely free of bugs, nor are they fully optimized.&lt;/p&gt; &#xA;&lt;h2&gt;Recoded Format&lt;/h2&gt; &#xA;&lt;p&gt;The floating-point units in this repository work on an internal recoded format (exponent has an additional bit) to handle subnormal numbers more efficiently in a microprocessor. A more detailed explanation will come soon, but in the mean time here are some example mappings for single-precision numbers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;IEEE format                           Recoded format&#xA;----------------------------------    -----------------------------------&#xA;s 00000000 00000000000000000000000    s 000------ 00000000000000000000000&#xA;s 00000000 00000000000000000000001    s 001101011 00000000000000000000000&#xA;s 00000000 0000000000000000000001f    s 001101100 f0000000000000000000000&#xA;s 00000000 000000000000000000001ff    s 001101101 ff000000000000000000000&#xA;    ...              ...                   ...              ... &#xA;s 00000000 001ffffffffffffffffffff    s 001111111 ffffffffffffffffffff000&#xA;s 00000000 01fffffffffffffffffffff    s 010000000 fffffffffffffffffffff00&#xA;s 00000000 1ffffffffffffffffffffff    s 010000001 ffffffffffffffffffffff0&#xA;s 00000001 fffffffffffffffffffffff    s 010000010 fffffffffffffffffffffff&#xA;s 00000010 fffffffffffffffffffffff    s 010000011 fffffffffffffffffffffff&#xA;    ...              ...                   ...              ... &#xA;s 11111101 fffffffffffffffffffffff    s 101111110 fffffffffffffffffffffff&#xA;s 11111110 fffffffffffffffffffffff    s 101111111 fffffffffffffffffffffff&#xA;s 11111111 00000000000000000000000    s 110------ -----------------------&#xA;s 11111111 fffffffffffffffffffffff    s 111------ fffffffffffffffffffffff&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Unit-Testing&lt;/h2&gt; &#xA;&lt;p&gt;To unit-test these floating-point units, you need the berkeley-testfloat-3 package.&lt;/p&gt; &#xA;&lt;p&gt;To test floating-point units with the C simulator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-livy</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/apache/incubator-livy</id>
    <link href="https://github.com/apache/incubator-livy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mirror of Apache livy (Incubating)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Livy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/apache/incubator-livy&#34;&gt;&lt;img src=&#34;https://travis-ci.org/apache/incubator-livy.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apache Livy is an open source REST interface for interacting with &lt;a href=&#34;http://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt; from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in &lt;a href=&#34;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&#34;&gt;Apache Hadoop YARN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interactive Scala, Python and R shells&lt;/li&gt; &#xA; &lt;li&gt;Batch submissions in Scala, Java, Python&lt;/li&gt; &#xA; &lt;li&gt;Multiple users can share the same server (impersonation support)&lt;/li&gt; &#xA; &lt;li&gt;Can be used for submitting jobs from anywhere with REST&lt;/li&gt; &#xA; &lt;li&gt;Does not require any code change to your programs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/incubator-livy/pulls&#34;&gt;Pull requests&lt;/a&gt; are welcomed! But before you begin, please check out the &lt;a href=&#34;http://livy.incubator.apache.org/community#Contributing&#34;&gt;Contributing&lt;/a&gt; section on the &lt;a href=&#34;http://livy.incubator.apache.org/community&#34;&gt;Community&lt;/a&gt; page of our website.&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Guides and documentation on getting started using Livy, example code snippets, and Livy API documentation can be found at &lt;a href=&#34;http://livy.incubator.apache.org&#34;&gt;livy.incubator.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Before Building Livy&lt;/h2&gt; &#xA;&lt;p&gt;To build Livy, you will need:&lt;/p&gt; &#xA;&lt;p&gt;Debian/Ubuntu:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mvn (from &lt;code&gt;maven&lt;/code&gt; package or maven3 tarball)&lt;/li&gt; &#xA; &lt;li&gt;openjdk-8-jdk (or Oracle JDK 8)&lt;/li&gt; &#xA; &lt;li&gt;Python 2.7+&lt;/li&gt; &#xA; &lt;li&gt;R 3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Redhat/CentOS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mvn (from &lt;code&gt;maven&lt;/code&gt; package or maven3 tarball)&lt;/li&gt; &#xA; &lt;li&gt;java-1.8.0-openjdk (or Oracle JDK 8)&lt;/li&gt; &#xA; &lt;li&gt;Python 2.7+&lt;/li&gt; &#xA; &lt;li&gt;R 3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MacOS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Xcode command line tools&lt;/li&gt; &#xA; &lt;li&gt;Oracle&#39;s JDK 1.8&lt;/li&gt; &#xA; &lt;li&gt;Maven (Homebrew)&lt;/li&gt; &#xA; &lt;li&gt;Python 2.7+&lt;/li&gt; &#xA; &lt;li&gt;R 3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Required python packages for building Livy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cloudpickle&lt;/li&gt; &#xA; &lt;li&gt;requests&lt;/li&gt; &#xA; &lt;li&gt;requests-kerberos&lt;/li&gt; &#xA; &lt;li&gt;flake8&lt;/li&gt; &#xA; &lt;li&gt;flaky&lt;/li&gt; &#xA; &lt;li&gt;pytest&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run Livy, you will also need a Spark installation. You can get Spark releases at &lt;a href=&#34;https://spark.apache.org/downloads.html&#34;&gt;https://spark.apache.org/downloads.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Livy requires Spark 2.4+. You can switch to a different version of Spark by setting the &lt;code&gt;SPARK_HOME&lt;/code&gt; environment variable in the Livy server process, without needing to rebuild Livy.&lt;/p&gt; &#xA;&lt;h2&gt;Building Livy&lt;/h2&gt; &#xA;&lt;p&gt;Livy is built using &lt;a href=&#34;http://maven.apache.org&#34;&gt;Apache Maven&lt;/a&gt;. To check out and build Livy, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/incubator-livy.git&#xA;cd incubator-livy&#xA;mvn package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default Livy is built against Apache Spark 2.4.5, but the version of Spark used when running Livy does not need to match the version used to build Livy. Livy internally handles the differences between different Spark versions.&lt;/p&gt; &#xA;&lt;p&gt;The Livy package itself does not contain a Spark distribution. It will work with any supported version of Spark without needing to rebuild.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>akka/akka</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/akka/akka</id>
    <link href="https://github.com/akka/akka" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build highly concurrent, distributed, and resilient message-driven applications on the JVM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Akka &lt;a href=&#34;https://index.scala-lang.org/akka/akka/akka-actor&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/akka/akka/akka-actor/latest.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://travis-ci.com/github/akka/akka&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/akka/akka.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;We believe that writing correct concurrent &amp;amp; distributed, resilient and elastic applications is too hard. Most of the time it&#39;s because we are using the wrong tools and the wrong level of abstraction.&lt;/p&gt; &#xA;&lt;p&gt;Akka is here to change that.&lt;/p&gt; &#xA;&lt;p&gt;Using the Actor Model we raise the abstraction level and provide a better platform to build correct concurrent and scalable applications. This model is a perfect match for the principles laid out in the &lt;a href=&#34;https://www.reactivemanifesto.org/&#34;&gt;Reactive Manifesto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For resilience, we adopt the &#34;Let it crash&#34; model which the telecom industry has used with great success to build applications that self-heal and systems that never stop.&lt;/p&gt; &#xA;&lt;p&gt;Actors also provide the abstraction for transparent distribution and the basis for truly scalable and fault-tolerant applications.&lt;/p&gt; &#xA;&lt;p&gt;Learn more at &lt;a href=&#34;https://akka.io/&#34;&gt;akka.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reference Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The reference documentation is available at &lt;a href=&#34;https://doc.akka.io&#34;&gt;doc.akka.io&lt;/a&gt;, for &lt;a href=&#34;https://doc.akka.io/docs/akka/current/scala.html&#34;&gt;Scala&lt;/a&gt; and &lt;a href=&#34;https://doc.akka.io/docs/akka/current/java.html&#34;&gt;Java&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;You can join these groups and chats to discuss and ask Akka related questions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: &lt;a href=&#34;https://discuss.akka.io&#34;&gt;discuss.akka.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat room about &lt;em&gt;using&lt;/em&gt; Akka: &lt;a href=&#34;https://gitter.im/akka/akka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fakka-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Issue tracker: &lt;a href=&#34;https://github.com/akka/akka/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/github%3A-issues-blue.svg?style=flat-square&#34; alt=&#34;github: akka/akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to that, you may enjoy following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog/news-archive.html&#34;&gt;news&lt;/a&gt; section of the page, which is updated whenever a new version is released&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog/article-archive.html&#34;&gt;Akka Team Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/akkateam&#34;&gt;@akkateam&lt;/a&gt; on Twitter&lt;/li&gt; &#xA; &lt;li&gt;Questions tagged &lt;a href=&#34;https://stackoverflow.com/questions/tagged/akka&#34;&gt;#akka on StackOverflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Projects built with Akka: &lt;a href=&#34;https://index.scala-lang.org/search?q=dependencies:akka/*&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/count.svg?q=dependencies:akka/*&amp;amp;subject=scaladex:&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;akka-dependency-badge&#34; title=&#34;Built with Akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributions are &lt;em&gt;very&lt;/em&gt; welcome!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you see an issue that you&#39;d like to see fixed, or want to shape out some ideas, the best way to make it happen is to help out by submitting a pull request implementing it. We welcome contributions from all, even you are not yet familiar with this project, We are happy to get you started, and will guide you through the process once you&#39;ve submitted your PR.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://index.scala-lang.org/search?q=dependencies:akka/*&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/count.svg?q=dependencies:akka/*&amp;amp;subject=scaladex:&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;akka-dependency-badge&#34; title=&#34;Built with Akka&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://github.com/akka/akka/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file for more details about the workflow, and general hints on how to prepare your pull request. You can also ask for clarifications or guidance in GitHub issues directly, or in the akka/dev chat if a more real time communication would be of benefit.&lt;/p&gt; &#xA;&lt;p&gt;A chat room is available for all questions related to &lt;em&gt;developing and contributing&lt;/em&gt; to Akka: &lt;a href=&#34;https://gitter.im/akka/dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fdev-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/dev&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Akka is Open Source and available under the Apache 2 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mspnp/spark-monitoring</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/mspnp/spark-monitoring</id>
    <link href="https://github.com/mspnp/spark-monitoring" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Monitoring Azure Databricks jobs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Monitoring Azure Databricks in an Azure Log Analytics Workspace&lt;/h1&gt; &#xA;&lt;p&gt;This repository extends the core monitoring functionality of Azure Databricks to send streaming query event information to Azure Monitor. For more information about using this library to monitor Azure Databricks, see &lt;a href=&#34;https://docs.microsoft.com/azure/architecture/databricks-monitoring&#34;&gt;Monitoring Azure Databricks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The project has the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/src&#xA;    /spark-listeners-loganalytics&#xA;    /spark-listeners&#xA;    /pom.xml&#xA;/sample&#xA;    /spark-sample-job&#xA;/perftools&#xA;     /spark-sample-job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;strong&gt;spark-listeners-loganalytics&lt;/strong&gt; and &lt;strong&gt;spark-listeners&lt;/strong&gt; directories contain the code for building the two JAR files that are deployed to the Databricks cluster. The &lt;strong&gt;spark-listeners&lt;/strong&gt; directory includes a &lt;strong&gt;scripts&lt;/strong&gt; directory that contains a cluster node initialization script to copy the JAR files from a staging directory in the Azure Databricks file system to execution nodes. The &lt;strong&gt;pom.xml&lt;/strong&gt; file is the main Maven project object model build file for the entire project.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;spark-sample-job&lt;/strong&gt; directory is a sample Spark application demonstrating how to implement a Spark application metric counter.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;perftools&lt;/strong&gt; directory contains details on how to use Azure Monitor with Grafana to monitor Spark performance.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before you begin, ensure you have the following prerequisites in place:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone or download this GitHub repository.&lt;/li&gt; &#xA; &lt;li&gt;An active Azure Databricks workspace. For instructions on how to deploy an Azure Databricks workspace, see &lt;a href=&#34;https://docs.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal&#34;&gt;get started with Azure Databricks.&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install the &lt;a href=&#34;https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html#install-the-cli&#34;&gt;Azure Databricks CLI&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An Azure Databricks personal access token is required to use the CLI. For instructions, see &lt;a href=&#34;https://docs.azuredatabricks.net/api/latest/authentication.html#token-management&#34;&gt;token management&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can also use the Azure Databricks CLI from the Azure Cloud Shell.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;A Java IDE, with the following resources: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.oracle.com/technetwork/java/javase/downloads/index.html&#34;&gt;Java Devlopment Kit (JDK) version 1.8&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.scala-lang.org/download/&#34;&gt;Scala language SDK 2.11 and/or 2.12&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://maven.apache.org/download.html&#34;&gt;Apache Maven 3.6.3&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported configurations&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Databricks Runtime(s)&lt;/th&gt; &#xA;   &lt;th&gt;Maven Profile&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;6.4 Extended Support&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;scala-2.11_spark-2.4.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7.3 LTS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;scala-2.12_spark-3.0.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;9.1 LTS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;scala-2.12_spark-3.1.2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;10.1&lt;/code&gt; - &lt;code&gt;10.2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;scala-2.12_spark-3.2.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;10.3&lt;/code&gt; - &lt;code&gt;10.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;scala-2.12_spark-3.2.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Logging Event Size Limit&lt;/h2&gt; &#xA;&lt;p&gt;This library currently has a size limit per event of 25MB, based on the &lt;a href=&#34;https://docs.microsoft.com/rest/api/loganalytics/create-request#data-limits&#34;&gt;Log Analytics limit of 30MB per API Call&lt;/a&gt; with additional overhead for formatting. The default behavior when hitting this limit is to throw an exception. This can be changed by modifying the value of &lt;code&gt;EXCEPTION_ON_FAILED_SEND&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/src/spark-listeners/src/main/java/com/microsoft/pnp/client/GenericSendBuffer.java&#34;&gt;GenericSendBuffer.java&lt;/a&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You will see an error like: &lt;code&gt;java.lang.RuntimeException: Failed to schedule batch because first message size nnn exceeds batch size limit 26214400 (bytes).&lt;/code&gt; in the Spark logs if your workload is generating logging messages of greater than 25MB, and your workload may not proceed. You can query Log Analytics for this error condition with:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-kusto&#34;&gt;SparkLoggingEvent_CL&#xA;| where TimeGenerated &amp;gt; ago(24h)&#xA;| where Message contains &#34;java.lang.RuntimeException: Failed to schedule batch because first message size&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Build the Azure Databricks monitoring library&lt;/h2&gt; &#xA;&lt;p&gt;You can build the library using either Docker or Maven. All commands are intended to be run from the base directory of the repository.&lt;/p&gt; &#xA;&lt;p&gt;The jar files that will be produced are:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;spark-listeners_&amp;lt;Spark Version&amp;gt;_&amp;lt;Scala Version&amp;gt;-&amp;lt;Version&amp;gt;.jar&lt;/code&gt;&lt;/strong&gt; - This is the generic implementation of the Spark Listener framework that provides capability for collecting data from the running cluster for forwarding to another logging system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;spark-listeners-loganalytics_&amp;lt;Spark Version&amp;gt;_&amp;lt;Scala Version&amp;gt;-&amp;lt;Version&amp;gt;.jar&lt;/code&gt;&lt;/strong&gt; - This is the specific implementation that extends &lt;strong&gt;spark-listeners&lt;/strong&gt;. This project provides the implementation for connecting to Log Analytics and formatting and passing data via the Log Analytics API.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Docker&lt;/h3&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To build all profiles:&#xA;docker run -it --rm -v `pwd`:/spark-monitoring -v &#34;$HOME/.m2&#34;:/root/.m2 mcr.microsoft.com/java/maven:8-zulu-debian10 /spark-monitoring/build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To build a single profile (example for latest long term support version 10.4 LTS):&#xA;docker run -it --rm -v `pwd`:/spark-monitoring -v &#34;$HOME/.m2&#34;:/root/.m2 -w /spark-monitoring/src mcr.microsoft.com/java/maven:8-zulu-debian10 mvn install -P &#34;scala-2.12_spark-3.2.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To build all profiles:&#xA;docker run -it --rm -v %cd%:/spark-monitoring -v &#34;%USERPROFILE%/.m2&#34;:/root/.m2 mcr.microsoft.com/java/maven:8-zulu-debian10 /spark-monitoring/build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To build a single profile (example for latest long term support version 10.4 LTS):&#xA;docker run -it --rm -v %cd%:/spark-monitoring -v &#34;%USERPROFILE%/.m2&#34;:/root/.m2 -w /spark-monitoring/src mcr.microsoft.com/java/maven:8-zulu-debian10 mvn install -P &#34;scala-2.12_spark-3.2.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Option 2: Maven&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Import the Maven project project object model file, &lt;em&gt;pom.xml&lt;/em&gt;, located in the &lt;strong&gt;/src&lt;/strong&gt; folder into your project. This will import two projects:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;spark-listeners&lt;/li&gt; &#xA;   &lt;li&gt;spark-listeners-loganalytics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Activate a &lt;strong&gt;single&lt;/strong&gt; Maven profile that corresponds to the versions of the Scala/Spark combination that is being used. By default, the Scala 2.12 and Spark 3.0.1 profile is active.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the Maven &lt;strong&gt;package&lt;/strong&gt; phase in your Java IDE to build the JAR files for each of the these projects:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Project&lt;/th&gt; &#xA;     &lt;th&gt;JAR file&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;spark-listeners&lt;/td&gt; &#xA;     &lt;td&gt;&lt;code&gt;spark-listeners_&amp;lt;Spark Version&amp;gt;_&amp;lt;Scala Version&amp;gt;-&amp;lt;Version&amp;gt;.jar&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;spark-listeners-loganalytics&lt;/td&gt; &#xA;     &lt;td&gt;&lt;code&gt;spark-listeners-loganalytics_&amp;lt;Spark Version&amp;gt;_&amp;lt;Scala Version&amp;gt;-&amp;lt;Version&amp;gt;.jar&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configure the Databricks workspace&lt;/h2&gt; &#xA;&lt;p&gt;Copy the JAR files and init scripts to Databricks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the Azure Databricks CLI to create a directory named &lt;strong&gt;dbfs:/databricks/spark-monitoring&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dbfs mkdirs dbfs:/databricks/spark-monitoring&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the &lt;strong&gt;/src/spark-listeners/scripts/spark-monitoring.sh&lt;/strong&gt; script file and add your &lt;a href=&#34;http://docs.microsoft.com/azure/azure-monitor/platform/agent-windows#obtain-workspace-id-and-key&#34;&gt;Log Analytics Workspace ID and Key&lt;/a&gt; to the lines below:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LOG_ANALYTICS_WORKSPACE_ID=&#xA;export LOG_ANALYTICS_WORKSPACE_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you do not want to add your Log Analytics workspace id and key into the init script in plaintext, you can also &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/keyvault-backed-secrets.md&#34;&gt;create an Azure Key Vault backed secret scope&lt;/a&gt; and reference those secrets through your cluster&#39;s environment variables.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In order to add &lt;code&gt;x-ms-AzureResourceId&lt;/code&gt; &lt;a href=&#34;https://docs.microsoft.com/azure/azure-monitor/platform/data-collector-api#request-headers&#34;&gt;header&lt;/a&gt; as part of the http request, modify the following environment variables on &lt;strong&gt;/src/spark-listeners/scripts/spark-monitoring.sh&lt;/strong&gt;. For instance:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export AZ_SUBSCRIPTION_ID=11111111-5c17-4032-ae54-fc33d56047c2&#xA;export AZ_RSRC_GRP_NAME=myAzResourceGroup&#xA;export AZ_RSRC_PROV_NAMESPACE=Microsoft.Databricks&#xA;export AZ_RSRC_TYPE=workspaces&#xA;export AZ_RSRC_NAME=myDatabricks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now the _ResourceId &lt;strong&gt;/subscriptions/11111111-5c17-4032-ae54-fc33d56047c2/resourceGroups/myAzResourceGroup/providers/Microsoft.Databricks/workspaces/myDatabricks&lt;/strong&gt; will be part of the header. (&lt;em&gt;Note: If at least one of them is not set the header won&#39;t be included.&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the Azure Databricks CLI to copy &lt;strong&gt;src/spark-listeners/scripts/spark-monitoring.sh&lt;/strong&gt; to the directory created in step 3:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dbfs cp src/spark-listeners/scripts/spark-monitoring.sh dbfs:/databricks/spark-monitoring/spark-monitoring.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the Azure Databricks CLI to copy all of the jar files from the &lt;strong&gt;src/target&lt;/strong&gt; folder to the directory created in step 3:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dbfs cp --overwrite --recursive src/target/ dbfs:/databricks/spark-monitoring/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Create and configure the Azure Databricks cluster&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to your Azure Databricks workspace in the Azure Portal.&lt;/li&gt; &#xA; &lt;li&gt;Under &#34;Compute&#34;, click &#34;Create Cluster&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Choose a name for your cluster and enter it in &#34;Cluster name&#34; text box.&lt;/li&gt; &#xA; &lt;li&gt;In the &#34;Databricks Runtime Version&#34; dropdown, select &lt;strong&gt;Runtime: 10.4 LTS (Scala 2.12, Spark 3.2.1)&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &#34;Advanced Options&#34;, click on the &#34;Init Scripts&#34; tab. Go to the last line under the &#34;Init Scripts section&#34; Under the &#34;destination&#34; dropdown, select &#34;DBFS&#34;. Enter &#34;dbfs:/databricks/spark-monitoring/spark-monitoring.sh&#34; in the text box. Click the &#34;add&#34; button.&lt;/li&gt; &#xA; &lt;li&gt;Click the &#34;Create Cluster&#34; button to create the cluster. Next, click on the &#34;start&#34; button to start the cluster.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Run the sample job (optional)&lt;/h2&gt; &#xA;&lt;p&gt;The repository includes a sample application that shows how to send application metrics and application logs to Azure Monitor.&lt;/p&gt; &#xA;&lt;p&gt;When building the sample job, specify a maven profile compatible with your databricks runtime from the &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/#supported-configurations&#34;&gt;supported configurations section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use Maven to build the POM located at &lt;code&gt;sample/spark-sample-job/pom.xml&lt;/code&gt; or run the following Docker command:&lt;/p&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -v `pwd`/sample/spark-sample-job:/spark-sample-job -v &#34;$HOME/.m2&#34;:/root/.m2 -w /spark-sample-job mcr.microsoft.com/java/maven:8-zulu-debian10 mvn install -P &amp;lt;maven-profile&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Windows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -v %cd%/sample/spark-sample-job:/spark-sample-job -v &#34;%USERPROFILE%/.m2&#34;:/root/.m2 -w /spark-sample-job mcr.microsoft.com/java/maven:8-zulu-debian10 mvn install -P &amp;lt;maven-profile&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to your Databricks workspace and create a new job, as described &lt;a href=&#34;https://docs.azuredatabricks.net/user-guide/jobs.html#create-a-job&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the job detail page, set &lt;strong&gt;Type&lt;/strong&gt; to &lt;code&gt;JAR&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For &lt;strong&gt;Main class&lt;/strong&gt;, enter &lt;code&gt;com.microsoft.pnp.samplejob.StreamingQueryListenerSampleJob&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Upload the JAR file from &lt;code&gt;/src/spark-jobs/target/spark-jobs-1.0-SNAPSHOT.jar&lt;/code&gt; in the &lt;strong&gt;Dependent Libraries&lt;/strong&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select the cluster you created previously in the &lt;strong&gt;Cluster&lt;/strong&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Run Now&lt;/strong&gt; button to launch the job.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When the job runs, you can view the application logs and metrics in your Log Analytics workspace. After you verify the metrics appear, stop the sample application job.&lt;/p&gt; &#xA;&lt;h3&gt;Viewing the Sample Job&#39;s Logs in Log Analytics&lt;/h3&gt; &#xA;&lt;p&gt;After your sample job has run for a few minutes, you should be able to query for these event types in Log Analytics:&lt;/p&gt; &#xA;&lt;h4&gt;SparkListenerEvent_CL&lt;/h4&gt; &#xA;&lt;p&gt;This custom log will contain Spark events that are serialized to JSON. You can limit the volume of events in this log with &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/filtering.md#limiting-events-in-sparklistenerevent_cl&#34;&gt;filtering&lt;/a&gt;. If filtering is not employed, this can be a large volume of data.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: There is a known issue when the Spark framework or workload generates events that have more than 500 fields, or where data for an individual field is larger than 32kb. Log Analytics will generate an error indicating that data has been dropped. This is an incompatibility between the data being generated by Spark, and the current limitations of the Log Analytics API.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Example for querying &lt;strong&gt;SparkListenerEvent_CL&lt;/strong&gt; for job throughput over the last 7 days:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kusto&#34;&gt;let results=SparkListenerEvent_CL&#xA;| where TimeGenerated &amp;gt; ago(7d)&#xA;| where  Event_s == &#34;SparkListenerJobStart&#34;&#xA;| extend metricsns=column_ifexists(&#34;Properties_spark_metrics_namespace_s&#34;,Properties_spark_app_id_s)&#xA;| extend apptag=iif(isnotempty(metricsns),metricsns,Properties_spark_app_id_s)&#xA;| project Job_ID_d,apptag,Properties_spark_databricks_clusterUsageTags_clusterName_s,TimeGenerated&#xA;| order by TimeGenerated asc nulls last&#xA;| join kind= inner (&#xA;    SparkListenerEvent_CL&#xA;    | where Event_s == &#34;SparkListenerJobEnd&#34;&#xA;    | where Job_Result_Result_s == &#34;JobSucceeded&#34;&#xA;    | project Event_s,Job_ID_d,TimeGenerated&#xA;) on Job_ID_d;&#xA;results&#xA;| extend slice=strcat(&#34;#JobsCompleted &#34;,Properties_spark_databricks_clusterUsageTags_clusterName_s,&#34;-&#34;,apptag)&#xA;| summarize count() by bin(TimeGenerated, 1h),slice&#xA;| order by TimeGenerated asc nulls last&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SparkLoggingEvent_CL&lt;/h4&gt; &#xA;&lt;p&gt;This custom log will contain data forwarded from Log4j (the standard logging system in Spark). The volume of logging can be controlled by &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/filtering.md#limiting-logs-in-sparkloggingevent_cl-basic&#34;&gt;altering the level of logging&lt;/a&gt; to forward or with &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/filtering.md#limiting-logs-in-sparkloggingevent_cl-advanced&#34;&gt;filtering&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example for querying &lt;strong&gt;SparkLoggingEvent_CL&lt;/strong&gt; for logged errors over the last day:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kusto&#34;&gt;SparkLoggingEvent_CL&#xA;| where TimeGenerated &amp;gt; ago(1d)&#xA;| where Level == &#34;ERROR&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SparkMetric_CL&lt;/h4&gt; &#xA;&lt;p&gt;This custom log will contain metrics events as generated by the Spark framework or workload. You can adjust the time period or sources included by modifying &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/src/spark-listeners/scripts/spark-monitoring.sh#L63-L76&#34;&gt;the &lt;code&gt;METRICS_PROPERTIES&lt;/code&gt; section of the spark-monitoring.sh&lt;/a&gt; script or by &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/filtering.md#limiting-metrics-in-sparkmetric_cl&#34;&gt;enabling filtering&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example of querying &lt;strong&gt;SparkMetric_CL&lt;/strong&gt; for the number of active executors per application over the last 7 days summarized every 15 minutes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kusto&#34;&gt;SparkMetric_CL&#xA;| where TimeGenerated &amp;gt; ago(7d)&#xA;| extend sname=split(name_s, &#34;.&#34;)&#xA;| where sname[2] == &#34;executor&#34;&#xA;| extend executor=strcat(sname[1]) &#xA;| extend app=strcat(sname[0])&#xA;| summarize NumExecutors=dcount(executor) by bin(TimeGenerated,  15m),app&#xA;| order by TimeGenerated asc nulls last&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: For more details on how to use the saved search queries in &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/perftools/deployment/loganalytics/logAnalyticsDeploy.json&#34;&gt;logAnalyticsDeploy.json&lt;/a&gt; to understand and troubleshoot performance, see &lt;a href=&#34;https://docs.microsoft.com/azure/architecture/databricks-monitoring/databricks-observability&#34;&gt;Observability patterns and metrics for performance tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Filtering&lt;/h2&gt; &#xA;&lt;p&gt;The library is configurable to limit the volume of logs that are sent to each of the different Azure Monitor log types. See &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/filtering.md&#34;&gt;filtering&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Debugging&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter any issues with the init script, you can refer to the docs on &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/docs/debugging.md&#34;&gt;debugging&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://raw.githubusercontent.com/mspnp/spark-monitoring/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>firesim/firesim</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/firesim/firesim</id>
    <link href="https://github.com/firesim/firesim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FireSim: Easy-to-use, Scalable, FPGA-accelerated Cycle-accurate Hardware Simulation in the Cloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FireSim: Easy-to-use, Scalable, FPGA-accelerated Cycle-accurate Hardware Simulation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://readthedocs.org/projects/firesim/badge/&#34; alt=&#34;FireSim Documentation Status&#34;&gt; &lt;img src=&#34;https://github.com/firesim/firesim/actions/workflows/firesim-run-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Github Actions Status&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#using-firesim&#34;&gt;Using FireSim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#what-is-firesim&#34;&gt;What is FireSim?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#what-can-i-simulate-with-firesim&#34;&gt;What can I simulate with FireSim?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#need-help&#34;&gt;Need help?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/#publications&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Using FireSim&lt;/h2&gt; &#xA;&lt;p&gt;To get started with using FireSim, see the tutorials on the FireSim documentation site: &lt;a href=&#34;https://docs.fires.im/&#34;&gt;https://docs.fires.im/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Another good overview (in video format) is our tutorial from the Chisel Community Conference on &lt;a href=&#34;https://www.youtube.com/watch?v=S3OriQnJXYQ&#34;&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is FireSim?&lt;/h2&gt; &#xA;&lt;p&gt;FireSim is an &lt;a href=&#34;https://github.com/firesim/firesim&#34;&gt;open-source&lt;/a&gt; cycle-accurate FPGA-accelerated full-system hardware simulation platform that runs on cloud FPGAs (Amazon EC2 F1). FireSim is actively developed in the &lt;a href=&#34;http://bar.eecs.berkeley.edu&#34;&gt;Berkeley Architecture Research Group&lt;/a&gt; in the &lt;a href=&#34;https://eecs.berkeley.edu&#34;&gt;Electrical Engineering and Computer Sciences Department&lt;/a&gt; at the &lt;a href=&#34;https://berkeley.edu&#34;&gt;University of California, Berkeley&lt;/a&gt;. You can learn more about FireSim in the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;FireSim website&lt;/strong&gt;: &lt;a href=&#34;https://fires.im&#34;&gt;https://fires.im&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FireSim ISCA 2018 Paper&lt;/strong&gt;: &lt;a href=&#34;https://sagark.org/assets/pubs/firesim-isca2018.pdf&#34;&gt;Paper PDF&lt;/a&gt; | &lt;a href=&#34;https://ieeexplore.ieee.org/document/8416816&#34;&gt;IEEE Xplore&lt;/a&gt; | &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3276543&#34;&gt;ACM DL&lt;/a&gt; | &lt;a href=&#34;https://sagark.org/assets/pubs/firesim-isca2018.bib.txt&#34;&gt;BibTeX&lt;/a&gt; | Selected as one of IEEE Micro’s “Top Picks from Computer Architecture Conferences, 2018”.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FireSim documentation&lt;/strong&gt;: &lt;a href=&#34;https://docs.fires.im&#34;&gt;https://docs.fires.im&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scala API Documentation&lt;/strong&gt;: &lt;a href=&#34;https://fires.im/firesim/latest/api/&#34;&gt;https://fires.im/firesim/latest/api/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Two-minute lightning talk from ISCA 2018&lt;/strong&gt; (FireSim simulating a datacenter): &lt;a href=&#34;https://www.youtube.com/watch?v=4XwoSe5c8lY&#34;&gt;YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chisel Community Conference Tutorial&lt;/strong&gt;: &lt;a href=&#34;https://www.youtube.com/watch?v=S3OriQnJXYQ&#34;&gt;YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Updates/News&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt; | &lt;a href=&#34;https://fires.im/blog/&#34;&gt;FireSim Blog&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/firesimproject&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What can I simulate with FireSim?&lt;/h2&gt; &#xA;&lt;p&gt;FireSim can simulate arbitrary hardware designs written in &lt;a href=&#34;https://chisel.eecs.berkeley.edu&#34;&gt;Chisel&lt;/a&gt;. With FireSim, you can write your own RTL (processors, accelerators, etc.) and run it at near-FPGA-prototype speeds on cloud FPGAs, while obtaining cycle-accurate performance results (i.e. matching what you would find if you taped-out a chip). Depending on the hardware design and the simulation scale, FireSim simulations run at &lt;strong&gt;10s to 100s of MHz&lt;/strong&gt;. You can also integrate custom software models for components that you don&#39;t want/need to write as RTL.&lt;/p&gt; &#xA;&lt;p&gt;FireSim was originally developed to simulate datacenters by combining open RTL for RISC-V processors with a custom cycle-accurate network simulation. By default, FireSim provides all the RTL and models necessary to &lt;strong&gt;cycle-exactly&lt;/strong&gt; simulate from &lt;strong&gt;one to thousands of multi-core compute nodes&lt;/strong&gt;, derived directly from &lt;strong&gt;silicon-proven&lt;/strong&gt; and &lt;strong&gt;open&lt;/strong&gt; target-RTL (&lt;a href=&#34;https://riscv.org/&#34;&gt;RISC-V&lt;/a&gt; &lt;a href=&#34;https://github.com/freechipsproject/rocket-chip&#34;&gt;Rocket Chip&lt;/a&gt; and &lt;a href=&#34;https://github.com/ucb-bar/riscv-boom&#34;&gt;BOOM&lt;/a&gt;), with an optional &lt;strong&gt;cycle-accurate network simulation&lt;/strong&gt; tying them together. FireSim also provides a &lt;a href=&#34;https://github.com/firesim/firesim-software&#34;&gt;Linux distribution&lt;/a&gt; that is compatible with the RISC-V systems it simulates and &lt;a href=&#34;https://docs.fires.im/en/latest/Advanced-Usage/Workloads/Defining-Custom-Workloads.html&#34;&gt;automates&lt;/a&gt; the process of including new workloads into this Linux distribution. These simulations run fast enough to interact with Linux on the simulated system at the command line, &lt;a href=&#34;https://twitter.com/firesimproject/status/1031267637303508993&#34;&gt;like a real computer&lt;/a&gt;. Users can even &lt;a href=&#34;http://docs.fires.im/en/latest/Advanced-Usage/Miscellaneous-Tips.html#experimental-support-for-sshing-into-simulated-nodes-and-accessing-the-internet-from-within-simulations&#34;&gt;SSH into simulated systems in FireSim&lt;/a&gt; and access the Internet from within them.&lt;/p&gt; &#xA;&lt;p&gt;Head to the &lt;a href=&#34;https://fires.im&#34;&gt;FireSim Website&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h2&gt;Need help?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join the FireSim Mailing list: &lt;a href=&#34;https://groups.google.com/forum/#!forum/firesim&#34;&gt;https://groups.google.com/forum/#!forum/firesim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Post an issue on this repo&lt;/li&gt; &#xA; &lt;li&gt;Follow on Twitter for project updates: &lt;a href=&#34;https://twitter.com/firesimproject&#34;&gt;@firesimproject&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/firesim/firesim/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ISCA 2018&lt;/strong&gt;: FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud&lt;/h3&gt; &#xA;&lt;p&gt;You can learn more about FireSim in our ISCA 2018 paper, which covers the overall FireSim infrastructure and large distributed simulations of networked clusters. This paper was &lt;strong&gt;selected as one of IEEE Micro’s “Top Picks from Computer Architecture Conferences, 2018”.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Sagar Karandikar, Howard Mao, Donggyu Kim, David Biancolin, Alon Amid, Dayeol Lee, Nathan Pemberton, Emmanuel Amaro, Colin Schmidt, Aditya Chopra, Qijing Huang, Kyle Kovacs, Borivoje Nikolic, Randy Katz, Jonathan Bachrach, and Krste Asanović. &lt;strong&gt;FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud&lt;/strong&gt;. &lt;em&gt;In proceedings of the 45th International Symposium on Computer Architecture (ISCA’18)&lt;/em&gt;, Los Angeles, CA, June 2018.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sagark.org/assets/pubs/firesim-isca2018.pdf&#34;&gt;Paper PDF&lt;/a&gt; | &lt;a href=&#34;https://ieeexplore.ieee.org/document/8416816&#34;&gt;IEEE Xplore&lt;/a&gt; | &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3276543&#34;&gt;ACM DL&lt;/a&gt; | &lt;a href=&#34;https://sagark.org/assets/pubs/firesim-isca2018.bib.txt&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;FPGA 2019&lt;/strong&gt;: FASED: FPGA-Accelerated Simulation and Evaluation of DRAM&lt;/h3&gt; &#xA;&lt;p&gt;Our paper from FPGA 2019 details the DRAM model used in FireSim:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;David Biancolin, Sagar Karandikar, Donggyu Kim, Jack Koenig, Andrew Waterman, Jonathan Bachrach, Krste Asanović, &lt;strong&gt;FASED: FPGA-Accelerated Simulation and Evaluation of DRAM&lt;/strong&gt;, &lt;em&gt;In proceedings of the 27th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays&lt;/em&gt;, Seaside, CA, February 2018.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~biancolin/papers/fased-fpga19.pdf&#34;&gt;Paper PDF&lt;/a&gt; | &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3293894&#34;&gt;ACM DL&lt;/a&gt; | &lt;a href=&#34;https://people.eecs.berkeley.edu/~biancolin/bib/fased-fpga19.bib&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;IEEE Micro Top Picks of 2018&lt;/strong&gt;: FireSim: FPGA-Accelerated, Cycle-Accurate Scale-Out System Simulation in the Public Cloud&lt;/h3&gt; &#xA;&lt;p&gt;This article discusses several updates since the FireSim ISCA 2018 paper:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Sagar Karandikar, Howard Mao, Donggyu Kim, David Biancolin, Alon Amid, Dayeol Lee, Nathan Pemberton, Emmanuel Amaro, Colin Schmidt, Aditya Chopra, Qijing Huang, Kyle Kovacs, Borivoje Nikolic, Randy Katz, Jonathan Bachrach, and Krste Asanović. &lt;strong&gt;FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud&lt;/strong&gt;. &lt;em&gt;IEEE Micro, vol. 39, no. 3, pp. 56-65, (Micro Top Picks 2018 Issue)&lt;/em&gt;. May-June 2019.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sagark.org/assets/pubs/firesim-micro-top-picks2018.pdf&#34;&gt;Article PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ICCAD 2019&lt;/strong&gt;: Golden Gate: Bridging The Resource-Efficiency Gap Between ASICs and FPGA Prototypes&lt;/h3&gt; &#xA;&lt;p&gt;Our paper describing FireSim&#39;s Compiler, &lt;em&gt;Golden Gate&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Albert Magyar, David T. Biancolin, Jack Koenig, Sanjit Seshia, Jonathan Bachrach, Krste Asanović, &lt;strong&gt;Golden Gate: Bridging The Resource-Efficiency Gap Between ASICs and FPGA Prototypes&lt;/strong&gt;, &lt;em&gt;In proceedings of the 39th International Conference on Computer-Aided Design (ICCAD &#39;19)&lt;/em&gt;, Westminster, CO, November 2019.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://davidbiancolin.github.io/papers/goldengate-iccad19.pdf&#34;&gt;Paper PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ASPLOS 2020&lt;/strong&gt;: FirePerf: FPGA-Accelerated Full-System Hardware/Software Performance Profiling and Co-Design&lt;/h3&gt; &#xA;&lt;p&gt;Our paper to appear in ASPLOS 2020 discusses system-level profiling features in FireSim:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Sagar Karandikar, Albert Ou, Alon Amid, Howard Mao, Randy Katz, Borivoje Nikolić, and Krste Asanović, &lt;strong&gt;FirePerf: FPGA-Accelerated Full-System Hardware/Software Performance Profiling and Co-Design&lt;/strong&gt;, &lt;em&gt;In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2020)&lt;/em&gt;, Lausanne, Switzerland, March 2020.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sagark.org/assets/pubs/fireperf-asplos2020.pdf&#34;&gt;Paper PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;IEEE MICRO 2021&lt;/strong&gt;: Accessible, FPGA Resource-Optimized Simulation of Multi-Clock Systems in FireSim&lt;/h3&gt; &#xA;&lt;p&gt;In this special issue, we describe the automated instance-multithreading optimization and support for multiple clock domains in the simulated target.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;David Biancolin, Albert Magyar, Sagar Karandikar, Alon Amid, Borivoje Nikolić, Jonathan Bachrach, Krste Asanović. &lt;strong&gt;Accessible, FPGA Resource-Optimized Simulation of Multi-Clock Systems in FireSim&lt;/strong&gt;. &lt;em&gt;In IEEE Micro Volume: 41, Issue: 4, July-Aug. 1 2021&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://davidbiancolin.github.io/papers/firesim-micro21.pdf&#34;&gt;Article PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find other publications, including publications that &lt;em&gt;use&lt;/em&gt; FireSim on the &lt;a href=&#34;https://fires.im/publications/&#34;&gt;FireSim Website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>broadinstitute/sam</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/broadinstitute/sam</id>
    <link href="https://github.com/broadinstitute/sam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;workbench identity and access management&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sam - Identity and Access Management (IAM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://static1.squarespace.com/static/52f51a96e4b0ec7646cd474a/5328b57de4b067106916ef7f/56b3b2167da24f50175975bc/1504623030943/geh502.jpg?format=500w&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;In a nutshell&lt;/h2&gt; &#xA;&lt;p&gt;The crux of IAM in Sam is a policy. A policy says &lt;strong&gt;who&lt;/strong&gt; can &lt;strong&gt;do what&lt;/strong&gt; to a &lt;strong&gt;thing&lt;/strong&gt;. More technically the who is called a &lt;strong&gt;subject&lt;/strong&gt; and can be a user or a group of users, the do what is called an &lt;strong&gt;action&lt;/strong&gt; such as read or update, and the thing is called a &lt;strong&gt;resource&lt;/strong&gt; such as a workspace or project. Resources have types which specify what actions are available for its resources, roles (which are collections of actions) and which role is the &#34;owner&#34; role. The &#34;owner&#34; role should have the appropriate actions to administer a resource. When a resource is created a policy with the owner role is automatically created and the creator is added.&lt;/p&gt; &#xA;&lt;h2&gt;Terms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subject - an authenticated user or group&lt;/li&gt; &#xA; &lt;li&gt;Resource - something to which access is controlled&lt;/li&gt; &#xA; &lt;li&gt;Action - may be performed on a resource - meant to be as granular as possible&lt;/li&gt; &#xA; &lt;li&gt;Policy - represents the actions a subject may perform on a resource&lt;/li&gt; &#xA; &lt;li&gt;Role - a collection of actions - meant to aggregate actions into a more meaningful, higher level concept&lt;/li&gt; &#xA; &lt;li&gt;Group - a group of subjects (this can include groups)&lt;/li&gt; &#xA; &lt;li&gt;Resource type - defines a class of resources. Each resource has a type which defines &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Available actions&lt;/li&gt; &#xA;   &lt;li&gt;Available roles and actions for each role&lt;/li&gt; &#xA;   &lt;li&gt;Of the available roles which is the “owner” role - this is used when creating a resource to give the creator ownership access&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Guiding Principles&lt;/h3&gt; &#xA;&lt;p&gt;There are no special/super users in this system. All api calls authenticate as subjects with access rights determined by policies in the same way. In other words, this system should use its own policy mechanisms internally for any authorization needs. (Note that this does leave the problem of bootstrapping, i.e. how is the first user created, which can be achieved by scripts outside the system with direct data store level access.) This system can be publicly facing. This does not mean that it will be in all cases but it should be designed with this in mind. Authentication is handled at a higher level than this application, e.g. via OAuth and an OIDC proxy.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Evaluation is the act of determining what a user may access.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Given a user, resource and action emit a yes or no response, i.e. can the user perform the action on the resource?&lt;/li&gt; &#xA; &lt;li&gt;Given a user and a resource type, list all resources and associated roles the user has (directly or indirectly).&lt;/li&gt; &#xA; &lt;li&gt;Given a user and resource, list all the actions the user may perform on that resource&lt;/li&gt; &#xA; &lt;li&gt;Given a user and resource, list all the user’s roles on that resource&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Of these 1 and 2 are the most important from a performance standpoint. Expect 1 to be called for almost every api call in a system. Expect 2 to be called from UI list pages where users generally want a snappy response.&lt;/p&gt; &#xA;&lt;h3&gt;Resource and Policy Management&lt;/h3&gt; &#xA;&lt;p&gt;A resource may be part of a hierarchy of resources. A parent may be set on a resource. To do so, users must have the set_parent action on the resource and the add_child action on the would be parent. Ancestor resources in the hierarchy control permissions on all descendants.&lt;/p&gt; &#xA;&lt;p&gt;A policy is specific to a resource and a resource may have multiple policies. Each policy consists of&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A set of subjects&lt;/li&gt; &#xA; &lt;li&gt;A set of actions directly applicable to the resource&lt;/li&gt; &#xA; &lt;li&gt;A set of roles directly applicable to the resource&lt;/li&gt; &#xA; &lt;li&gt;A set of descendant permissions - roles and actions applicable to descendant resources All of the subjects may perform all of the actions/roles in the policy. A policy may also be marked as public effectively meaning all users are members. Each policy has a name that is unique within a resource. Access to actions through policies is additive (i.e. the actions available to a user on a resource is an accumulation of all policies the user is a member of for that resource).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There must be functions to create, delete and manage policies for resources. There must be access control around deleting resources and managing policies. There must be some built-in actions to do so (delete, read-policies, alter-policies).&lt;/p&gt; &#xA;&lt;p&gt;There must be functions to create and delete resources. When a resource is created the caller should be the “owner.” The “owner” role generally will include delete action and actions to control sharing but need not always (e.g. if a resource may never be deleted then an owner would not have delete permissions). The actions that make up the “owner” role are defined by the resource type.&lt;/p&gt; &#xA;&lt;p&gt;Resource types define the set of available actions for all resources of that type. It also defines a set of roles and their associated actions. Roles are useful because it can be cumbersome to deal with granular actions and as a point of extensibility (when new actions are added to resource types, they can be added to roles as well, effectively adding the action to all resources with that role). It is not yet necessary to provide apis to create and maintain resource types, this can be achieved through configuration.&lt;/p&gt; &#xA;&lt;h3&gt;Public Policies&lt;/h3&gt; &#xA;&lt;p&gt;There are some cases where it is desirable to grant actions or roles to all authenticated users. For example, granting read-only access to public workspaces. In this case a policy can be created that has the appropriate actions or roles and set to public. Resources with public policies show up when listing resources for a user. For this reason it is not always desirable to allow everyone to make public policies. Again, the example is public workspaces. Public workspaces show up for everyone and should be curated.&lt;/p&gt; &#xA;&lt;p&gt;To change a policy&#39;s public status the caller must be able to share the policy (either via &lt;code&gt;alter_policies&lt;/code&gt; and &lt;code&gt;share_policy::{policy_name}&lt;/code&gt; actions) &lt;em&gt;and&lt;/em&gt; must have the &lt;code&gt;set_public&lt;/code&gt; action on the resource &lt;code&gt;resource_type_admin/{resource type name}&lt;/code&gt;. &lt;code&gt;resource_type_admin&lt;/code&gt; is an internally created resource type. &lt;code&gt;{resource type name}&lt;/code&gt; is for the resource containing the policy. Note that every resource type in sam has a resource of the same name of type &lt;code&gt;resource_type_admin&lt;/code&gt; which is automatically created. When these resources are created they do not have owners, permissions must be granted via direct postgres changes.&lt;/p&gt; &#xA;&lt;h3&gt;User and Group Management&lt;/h3&gt; &#xA;&lt;p&gt;User - Create, enable, disable, get status. Disabled users should be rejected from any api calls. Enabling a user should reinstate any prior access.&lt;/p&gt; &#xA;&lt;p&gt;Group - Create, delete, read, list, add/remove users and groups. Nested groups are supported. Groups are implemented as a resource type with admin and member roles and policies. There is an additional &lt;code&gt;admin_notifier&lt;/code&gt; role and policy that is public by default which allows any authenticate user to request access to a group. Group admins can set public to false if desired.&lt;/p&gt; &#xA;&lt;h3&gt;Built In Actions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;read_policies - may read all policies of a resource&lt;/li&gt; &#xA; &lt;li&gt;alter_policies - may change any policy of a resource&lt;/li&gt; &#xA; &lt;li&gt;delete - may delete a resource&lt;/li&gt; &#xA; &lt;li&gt;share_policy::{policy name} - may add/remove members to/from specified policy of a resource&lt;/li&gt; &#xA; &lt;li&gt;read_policy::{policy name} - may read specified policy of a resource&lt;/li&gt; &#xA; &lt;li&gt;get_parent - may get a resource&#39;s parent&lt;/li&gt; &#xA; &lt;li&gt;set_parent - may set a resource&#39;s parent&lt;/li&gt; &#xA; &lt;li&gt;add_child - may add a child to a resource&lt;/li&gt; &#xA; &lt;li&gt;remove_child - may remove a child from a resource&lt;/li&gt; &#xA; &lt;li&gt;list_children - may list all of a resource&#39;s children&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;UML Model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/model.png&#34; alt=&#34;Sam Model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/uml-class-diagram-tutorial/#uml-class-diagram-relationship&#34;&gt;UML Key&lt;/a&gt; for reference. Note that in this model Group is a Subject. This allows it to be used interchangeably with Users within policies.&lt;/p&gt; &#xA;&lt;h3&gt;ERD&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/sam_erd.png&#34; alt=&#34;Sam ERD&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Sam schema has 3 main sections: users and groups in yellow, resources and policies in green, resource type configuration in purple. Both groups and resources have a hierarchical model (groups can contain groups and resources can have parents). To solve read-query performance issues these hierarchies are also stored in a flattened representation: &lt;a href=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/src/main/scala/org/broadinstitute/dsde/workbench/sam/dataAccess/PostgresGroupDAO.scala&#34;&gt;sam_group_member_flat&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/src/main/scala/org/broadinstitute/dsde/workbench/sam/dataAccess/EffectivePolicyMutationStatements.scala&#34;&gt;sam_effective_*&lt;/a&gt; tables.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://petstore.swagger.io/?url=https://raw.githubusercontent.com/broadinstitute/sam/develop/src/main/resources/swagger/api-docs.yaml&#34;&gt;Sam APIs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Java Client Library&lt;/h4&gt; &#xA;&lt;p&gt;for sbt:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;libraryDependencies += &#34;org.broadinstitute.dsde.workbench&#34; %% &#34;sam-client&#34; % &#34;0.1-&amp;lt;git hash&amp;gt;&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;git hash&amp;gt;&lt;/code&gt; is the first 7 characters of the commit hash of the HEAD of develop&lt;/p&gt; &#xA;&lt;p&gt;Example Scala Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import org.broadinstitute.dsde.workbench.client.sam.api.ResourcesApi&#xA;import org.broadinstitute.dsde.workbench.client.sam.ApiClient&#xA;&#xA;class SamClient(samBasePath: String) {&#xA;  private def samResourcesApi(accessToken: String): ResourcesApi = {&#xA;    val apiClient = new ApiClient()&#xA;    apiClient.setAccessToken(accessToken)&#xA;    apiClient.setBasePath(samBasePath)&#xA;    new ResourcesApi(apiClient)&#xA;  }&#xA;&#xA;  def checkResourceAction(token: String, samResourceType: String, samResource: String, action: String): Boolean = {&#xA;    val samResourceApi = samResourcesApi(token)&#xA;    samResourceApi.resourceAction(samResourceType, samResource, action)&#xA;  }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cloud Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Google&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Groups can be mirrored to google groups.&lt;/li&gt; &#xA; &lt;li&gt;Proxy groups - each user with access to google resources should have a google group known as a proxy. The proxy is 1-to-1 with the user and the user is member of the proxy. The proxy group should be used in place of the user in Google IAM policies and Google groups. Users should not be added directly. This allows easy enable and disable functionality by adding/removing users to their proxy groups. It also allows creation of service accounts that can act as the user (see pet service accounts below).&lt;/li&gt; &#xA; &lt;li&gt;Pet service accounts - Google Compute Engine requires a service account to run compute. Service account credentials are the default credentials on any GCE instance. This is the best way at this time to provide credentials to any processes running on a GCE instance. Pet service accounts correspond with 1 and only 1 user, are added to the user’s proxy group and can call system apis as the user. In this way a pet service account can act as the user in all respects that can be controlled by the system (resources outside control of the system need to be manually shared by the user with the proxy group).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Proposed model for accessing external google resources&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/data_access.png&#34; alt=&#34;Data Access&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that Sam does not actually launch workflows create VMs but appears to in this diagram in order to simplify interactions. The key concept is the user of service accounts.&lt;/p&gt; &#xA;&lt;h4&gt;Google integration requires&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a GSuite domain&lt;/li&gt; &#xA; &lt;li&gt;a project with a service account for the sam application&lt;/li&gt; &#xA; &lt;li&gt;service account with access to &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create PubSub topics and subscriptions&lt;/li&gt; &#xA;   &lt;li&gt;admin google groups in GSuite domain&lt;/li&gt; &#xA;   &lt;li&gt;create service accounts and keys in desired projects (usually easiest to grant this at the org level)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Required Tooling:&lt;/h3&gt; &#xA;&lt;h4&gt;Java:&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have Java JDK 11 installed. Instructions for our recommended package can be found &lt;a href=&#34;https://adoptopenjdk.net/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Scala:&lt;/h4&gt; &#xA;&lt;p&gt;Mac:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-$xslt&#34;&gt;brew install scala&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;To build&lt;/h3&gt; &#xA;&lt;p&gt;Make sure git secrets is installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-$xslt&#34;&gt;brew install git-secrets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run minnie-kenny.sh with -f first time after git-clone to ensure git-secrets is run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./minnie-kenny.sh -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build jar:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./docker/build.sh jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build jar and docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./docker/build.sh jar -d build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;To run unit tests&lt;/h3&gt; &#xA;&lt;h4&gt;Set up your environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;#Spin up a local OpenDJ:&#xA;sh docker/run-opendj.sh start&#xA;#Spin up a local postgres:&#xA;sh docker/run-postgres.sh start&#xA;#Make sure your `SBT_OPTS` are set:&#xA;export SBT_OPTS=&#34;-Dpostgres.host=localhost -Dpostgres.port=5432 -Ddirectory.url=ldap://localhost:3389 -Ddirectory.password=testtesttest&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: if you run Postgres in another way (i.e. you&#39;re running the Postgres Mac app), the unit tests will fail because they will look at that installation instead of the Docker container you&#39;ve spun up. You can either specify a port when starting the Postgres Docker container or quit your Postgres client.&lt;/p&gt; &#xA;&lt;h4&gt;Run tests in sbt&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run tests in intellij&lt;/h4&gt; &#xA;&lt;p&gt;Set up ScalaTest Template:&lt;/p&gt; &#xA;&lt;p&gt;You need to set some default VM parameters for ScalaTest run configurations. In IntelliJ, go to &lt;code&gt;Run&lt;/code&gt; &amp;gt; &lt;code&gt;Edit Configurations...&lt;/code&gt;, select &lt;code&gt;ScalaTest&lt;/code&gt; under &lt;code&gt;🔧Templates&lt;/code&gt;, and add these VM parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-Dpostgres.host=localhost -Dpostgres.port=5432 -Ddirectory.url=ldap://localhost:3389 -Ddirectory.password=testtesttest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run unit tests within IntelliJ by clicking the green play button on a unit test.&lt;/p&gt; &#xA;&lt;h5&gt;Connecting to your local Postgres&lt;/h5&gt; &#xA;&lt;p&gt;In order to connect to your local postgres, open up the database tab, select the plus button at the top, data source, and click postgres.&lt;/p&gt; &#xA;&lt;p&gt;From there, set the:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Host: &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Port: &lt;code&gt;5432&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;User: Check &lt;code&gt;src/test/resources/reference.conf&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Password: Check &lt;code&gt;src/test/resources/reference.conf&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Cleaning up after tests&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;#Stop your local opendj:&#xA;sh docker/run-opendj.sh stop&#xA;#Stop your local postgres:&#xA;sh docker/run-postgres.sh stop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Debugging ScalikeJDBC&lt;/h4&gt; &#xA;&lt;p&gt;To view the SQL commands generated by ScalikeJDBC, set &lt;code&gt;scalikejdbc.global.loggingSQLAndTime.enabled&lt;/code&gt; in &lt;code&gt;src/test/resources/reference.conf&lt;/code&gt; to true&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/automation/README.md&#34;&gt;To run integration tests&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;To run Sam locally&lt;/h3&gt; &#xA;&lt;h4&gt;Local setup&lt;/h4&gt; &#xA;&lt;p&gt;Set up configs using the &lt;a href=&#34;https://github.com/broadinstitute/firecloud-develop#quick-start---how-do-i-set-up-my-configs&#34;&gt;firecloud-develop quick start guide for configs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you haven&#39;t already, add &lt;code&gt;127.0.0.1 local.broadinstitute.org&lt;/code&gt; to &lt;code&gt;/etc/hosts&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo sh -c &#34;echo &#39;127.0.0.1       local.broadinstitute.org&#39; &amp;gt;&amp;gt; /etc/hosts&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then start Sam against dev DBs or local DBs following the instructions below.&lt;/p&gt; &#xA;&lt;h5&gt;Using dev DBs&lt;/h5&gt; &#xA;&lt;p&gt;You must be connected to the Broad Internal network to connect to the Dev DBs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Start up local Sam&#xA;sh config/docker-rsync-local-sam.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Using local DBs&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Start up local Sam with local opendj and postgres&#xA;LOCAL_OPENDJ=true LOCAL_POSTGRES=true sh config/docker-rsync-local-sam.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: OpenDJ has some heavy memory requirements. If you see the OpenDJ container silently dying when running this command, try opening your Docker Desktop preferenes and increasing the Memory resources, 4GB seems to be sufficient, but more may be needed as well as increasing the Swap space maybe.&lt;/p&gt; &#xA;&lt;h4&gt;Verify that local Sam is running&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://local.broadinstitute.org:50443/status&#34;&gt;Status endpoint: https://local.broadinstitute.org:50443/status&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://local.broadinstitute.org:50443/#/&#34;&gt;Swagger page: https://local.broadinstitute.org:50443/#/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/broadinstitute/sam/develop/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>typelevel/cats-effect</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/typelevel/cats-effect</id>
    <link href="https://github.com/typelevel/cats-effect" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The pure asynchronous runtime for Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cats Effect&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://index.scala-lang.org/typelevel/cats-effect/cats-effect&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/typelevel/cats-effect/cats-effect/latest.svg?color=orange&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/QNnHKHq5Ts&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632277896739946517.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=404244&amp;amp;labelColor=6A7EC2&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img align=&#34;right&#34; width=&#34;256px&#34; height=&#34;256px&#34; src=&#34;https://raw.githubusercontent.com/typelevel/cats-effect/series/3.x/images/cats-effect-logo.png&#34;&gt; &#xA;&lt;p&gt;Cats Effect is a high-performance, asynchronous, composable framework for building real-world applications in a purely functional style within the Typelevel ecosystem. It provides a concrete tool, known as &#34;the &lt;code&gt;IO&lt;/code&gt; monad&#34;, for capturing and controlling actions, often referred to as &#34;effects&#34;, that your program wishes to perform within a resource-safe, typed context with seamless support for concurrency and coordination. These effects may be asynchronous (callback-driven) or synchronous (directly returning values); they may return within microseconds or run infinitely.&lt;/p&gt; &#xA;&lt;p&gt;Even more importantly, Cats Effect defines a set of typeclasses which define what it means to be a purely functional runtime system. These abstractions power a thriving ecosystem consisting of streaming frameworks, JDBC database layers, HTTP servers and clients, asynchronous clients for systems like Redis and MongoDB, and so much more! Additionally, you can leverage these abstractions within your own application to unlock powerful capabilities with little-or-no code changes, for example solving problems such as dependency injection, multiple error channels, shared state across modules, tracing, and more.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wired: &lt;strong&gt;3.3.12&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tired: &lt;strong&gt;2.5.5&lt;/strong&gt; (end of life)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;org.typelevel&#34; %% &#34;cats-effect&#34; % &#34;3.3.12&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above represents the core, stable dependency which brings in the entirety of Cats Effect. This is &lt;em&gt;most likely&lt;/em&gt; what you want. All current Cats Effect releases are published for Scala 2.12, 2.13, 3.0, and Scala.js 1.7.&lt;/p&gt; &#xA;&lt;p&gt;Or, if you prefer a less bare-bones starting point, you can try &lt;a href=&#34;https://github.com/typelevel/ce3.g8&#34;&gt;the Giter8 template&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sbt -Dsbt.version=1.5.5 new typelevel/ce3.g8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Depending on your use-case, you may want to consider one of the several other modules which are made available within the Cats Effect release. If you&#39;re a datatype implementer (like &lt;a href=&#34;https://monix.io&#34;&gt;Monix&lt;/a&gt;), you probably only want to depend on &lt;strong&gt;kernel&lt;/strong&gt; (the typeclasses) in your compile scope and &lt;strong&gt;laws&lt;/strong&gt; in your test scope:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies ++= Seq(&#xA;  &#34;org.typelevel&#34; %% &#34;cats-effect-kernel&#34; % &#34;3.3.12&#34;,&#xA;  &#34;org.typelevel&#34; %% &#34;cats-effect-laws&#34;   % &#34;3.3.12&#34; % Test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re a middleware framework (like &lt;a href=&#34;https://fs2.io/&#34;&gt;Fs2&lt;/a&gt;), you probably want to depend on &lt;strong&gt;std&lt;/strong&gt;, which gives you access to &lt;code&gt;Queue&lt;/code&gt;, &lt;code&gt;Semaphore&lt;/code&gt;, and much more without introducing a hard-dependency on &lt;code&gt;IO&lt;/code&gt; outside of your tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies ++= Seq(&#xA;  &#34;org.typelevel&#34; %% &#34;cats-effect-std&#34; % &#34;3.3.12&#34;,&#xA;  &#34;org.typelevel&#34; %% &#34;cats-effect&#34;     % &#34;3.3.12&#34; % Test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also find some utility in the &lt;strong&gt;testkit&lt;/strong&gt; and &lt;strong&gt;kernel-testkit&lt;/strong&gt; projects, which contain &lt;code&gt;TestContext&lt;/code&gt;, generators for &lt;code&gt;IO&lt;/code&gt;, and a few other things:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;org.typelevel&#34; %% &#34;cats-effect-testkit&#34; % &#34;3.3.12&#34; % Test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cats Effect provides backward binary compatibility within the 2.x and 3.x version lines, and both forward and backward compatibility within any major/minor line. This is analogous to the versioning scheme used by Cats itself, as well as other major projects such as Scala.js. Thus, any project depending upon Cats Effect 2.2.1 can be used with libraries compiled against Cats Effect 2.0.0 or 2.2.3, but &lt;em&gt;not&lt;/em&gt; with libraries compiled against 2.3.0 or higher.&lt;/p&gt; &#xA;&lt;h3&gt;Updating from Cats Effect 1.x / 2.x&lt;/h3&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://typelevel.org/cats-effect/docs/migration-guide&#34;&gt;migration guide&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Hello, World&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import cats.effect._&#xA;&#xA;object Main extends IOApp.Simple {&#xA;  val run = IO.println(&#34;Hello, World!&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, if you need the ability to take arguments and return exit codes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import cats.effect._&#xA;&#xA;object Main extends IOApp {&#xA;  def run(args: List[String]): IO[ExitCode] =&#xA;    if (args.headOption.map(_ == &#34;--do-it&#34;).getOrElse(false))&#xA;      IO.println(&#34;I did it!&#34;).as(ExitCode.Success)&#xA;    else&#xA;      IO.println(&#34;Didn&#39;t do it&#34;).as(ExitCode(-1))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Five Simple Rules&lt;/h2&gt; &#xA;&lt;p&gt;Any program written using Cats Effect provides incredibly strong guarantees and powerful functionality, performance, safety, and composability, provided you follow each of the following rules:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wrap &lt;em&gt;all&lt;/em&gt; side-effects&lt;/strong&gt; in &lt;code&gt;delay&lt;/code&gt;, &lt;code&gt;async&lt;/code&gt;, &lt;code&gt;blocking&lt;/code&gt;, or &lt;code&gt;interruptible&lt;/code&gt;/&lt;code&gt;interruptibleMany&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(pro tip: try to keep the size of your &lt;code&gt;delay&lt;/code&gt; blocks small; two &lt;code&gt;delay&lt;/code&gt;s with a &lt;code&gt;flatMap&lt;/code&gt; is much better than one big &lt;code&gt;delay&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use &lt;code&gt;bracket&lt;/code&gt; or &lt;code&gt;Resource&lt;/code&gt;&lt;/strong&gt; for anything which must be &lt;code&gt;close&lt;/code&gt;d&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Never&lt;/em&gt; hard-block a thread&lt;/strong&gt; outside of &lt;code&gt;blocking&lt;/code&gt; or &lt;code&gt;interruptible&lt;/code&gt;/&lt;code&gt;interruptibleMany&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use &lt;code&gt;IOApp&lt;/code&gt;&lt;/strong&gt; instead of writing your own &lt;code&gt;def main&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Never call anything that has &lt;strong&gt;the word &lt;code&gt;unsafe&lt;/code&gt; in the name&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you follow these rules, and you use libraries and frameworks which also follow these rules, you will get a truly astonishing array of things essentially for free:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extremely high performance, elastic, and scalable applications&lt;/li&gt; &#xA; &lt;li&gt;Proven backpressure mechanisms under extreme load in real deployments&lt;/li&gt; &#xA; &lt;li&gt;Reliable resource safety in all cases&lt;/li&gt; &#xA; &lt;li&gt;Aggressive interruption of unnecessary work (e.g. timeouts), automatically, without any extra implementation effort&lt;/li&gt; &#xA; &lt;li&gt;Composable and modular application architecture (real, &lt;em&gt;practical&lt;/em&gt; functional programming)&lt;/li&gt; &#xA; &lt;li&gt;Simple, safe, and incredibly powerful concurrency mechanisms that get &lt;em&gt;faster&lt;/em&gt; under high contention&lt;/li&gt; &#xA; &lt;li&gt;Highly tuned application runtime with optimized threading and memory management semantics&lt;/li&gt; &#xA; &lt;li&gt;Powerful and orthogonal abstractions which enable architectural decomposition that scales to any problem space&lt;/li&gt; &#xA; &lt;li&gt;Access to an entire ecosystem of uniquely powerful libraries and tooling&lt;/li&gt; &#xA; &lt;li&gt;…and so much more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;img width=&#34;461px&#34; height=&#34;356px&#34; align=&#34;right&#34; alt=&#34;a bar chart showing &#39;Fixed Thread Pool&#39; and &#39;Cats Effect 3&#39;, with the latter being substantially taller than the former&#34; src=&#34;https://raw.githubusercontent.com/typelevel/cats-effect/series/3.x/images/contention.png&#34;&gt; &#xA;&lt;p&gt;Most functional and async frameworks will tout their performance on synthetic microbenchmarks, measuring things like how many &lt;code&gt;flatMap&lt;/code&gt;s they can evaluate per microsecond and so on. However, most programs aren&#39;t just a bunch of &lt;code&gt;flatMap&lt;/code&gt;s, and the true performance bottlenecks are usually in things like contention scaling under high load, memory and other resource management, backpressure, page faults, and such. In these areas, Cats Effect is truly unrivaled on the JVM, and in most cases, applications written in a purely functional style using Cats Effect will &lt;em&gt;exceed&lt;/em&gt; the performance and elasticity of the same applications written in an imperative style.&lt;/p&gt; &#xA;&lt;p&gt;The chart to the right shows the results of a synthetic benchmark simulating an extremely high-contention scheduling scenario. The scenario is typical of something like a microservice handling extremely high requests-per-second, with each request representing some sort of scatter/gather semantic in which many complex asynchronous actions must be taken in parallel to produce a timely response.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/typelevel/cats-effect/raw/220d0106ca0ff6106746a41504b6ab07d8fc9199/benchmarks/src/main/scala/cats/effect/benchmarks/WorkStealingBenchmark.scala&#34;&gt;The benchmark&lt;/a&gt; measures the performance of a typical &#34;disruptor pattern&#34; application written using a fixed thread pool (from &lt;code&gt;java.util.concurrent.Executors&lt;/code&gt;) compared to the same workflow implemented using Cats Effect (specifically version 3.0). The scores are not a typo: Cats Effect is &lt;em&gt;almost 55x faster&lt;/em&gt; than the typical disruptor-style, hand-tuned implementation. Similarly dramatic results are consistently observed when comparing Cats Effect with other popular asynchronous and functional frameworks.&lt;/p&gt; &#xA;&lt;p&gt;As always, benchmarks are one thing, and your application is its own special snowflake with its own performance profile. Always measure and test &lt;em&gt;your application&lt;/em&gt; before assuming that someone else&#39;s performance results apply in your use-case. When in doubt, &lt;a href=&#34;https://discord.gg/QNnHKHq5Ts&#34;&gt;come talk with us&lt;/a&gt; and we&#39;ll give you an honest opinion!&lt;/p&gt; &#xA;&lt;h2&gt;Abstraction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/typelevel/cats-effect/series/3.x/images/hierarchy.svg?sanitize=true&#34; alt=&#34;the cats effect hierarchy of typeclasses as of version 3.0&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cats Effect isn&#39;t just designed to enable high performance applications with out-of-the-box safety and elasticity under load. It was intended first and foremost as a tool for implementing &lt;em&gt;composable&lt;/em&gt; and &lt;em&gt;reasonable&lt;/em&gt; software that is easy to write, easy to test, and easy to evolve as your team and requirements change over time. To achieve this goal, Cats Effect embraces and enables strong, typeful, purely-functional programming styles that are uniquely tailored for the Scala language.&lt;/p&gt; &#xA;&lt;p&gt;The typical Cats Effect system is often built in terms of simple, orthogonal, primitive capabilities which come together to represent all the expressive power necessary to encode a modern asynchronous runtime. Much like how the rules of addition, multiplication, and integers come together to define much of what we understand about basic arithmetic, so too do the rules of &lt;code&gt;Functor&lt;/code&gt;, &lt;code&gt;Monad&lt;/code&gt;, and &lt;code&gt;Concurrent&lt;/code&gt; come together to define the nature of a &lt;em&gt;program&lt;/em&gt; which has all the capabilities you need.&lt;/p&gt; &#xA;&lt;p&gt;By learning and leveraging these capabilities directly, it is possible to write functions and classes which clearly state their requirements and capabilities in a &lt;em&gt;statically typed&lt;/em&gt; and discoverable fashion, improving documentation, readability, and separation of concerns.&lt;/p&gt; &#xA;&lt;p&gt;And, just as with arithmetic, even when you don&#39;t directly leverage the nature of abstract mathematics in your daily life, those laws are still present shaping the world around you and enabling powerful and surprising things like computers and trains and restaurant menus. The laws and abstractions of Cats Effect support a powerful and unique ecosystem of frameworks, giving you access to rich and advanced functionality unparalleled in any language or ecosystem, battle tested in production environments ranging from some of the largest companies in the world to some of the nimblest startups.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/typelevel/cats-effect/series/3.x/CONTRIBUTING.md&#34;&gt;&lt;strong&gt;CONTRIBUTING.md&lt;/strong&gt;&lt;/a&gt; for more details. Lots to do!&lt;/p&gt; &#xA;&lt;h3&gt;Website&lt;/h3&gt; &#xA;&lt;p&gt;To build the documentation site locally, the following dependencies are needed, in addition to &lt;code&gt;sbt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node (14.x ideally)&lt;/li&gt; &#xA; &lt;li&gt;Yarn (any version should work)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: &lt;a href=&#34;https://nixos.org/&#34;&gt;Nix&lt;/a&gt; users can just run &lt;code&gt;nix-shell&lt;/code&gt; at the root directory and follow along the next instructions.&lt;/p&gt; &#xA;&lt;p&gt;Next, check out the documentation branch along with its submodules.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout --track origin/docs&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, build the site.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build.sh host&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything goes well, your browser will open at the end of this.&lt;/p&gt; &#xA;&lt;h2&gt;Tool Sponsorship&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img width=&#34;185px&#34; height=&#34;44px&#34; align=&#34;right&#34; src=&#34;https://www.yourkit.com/images/yklogo.png&#34;&gt;Development of Cats Effect is generously supported in part by &lt;a href=&#34;https://www.yourkit.com&#34;&gt;YourKit&lt;/a&gt; through the use of their excellent Java profiler.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2017-2022 Typelevel&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>twitter/finagle</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/twitter/finagle</id>
    <link href="https://github.com/twitter/finagle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fault tolerant, protocol-agnostic RPC system&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/twitter/finagle/raw/develop/doc/src/sphinx/_static/logo_medium.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Finagle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/finagle/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/finagle/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/finagle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/finagle/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;https://twitter.github.io/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/finagle/&#34;&gt;https://github.com/twitter/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://finagle.github.io/blog/&#34;&gt;https://finagle.github.io/blog/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols.&lt;/p&gt; &#xA;&lt;p&gt;For extensive documentation, please see the &lt;a href=&#34;https://twitter.github.io/finagle/guide/&#34;&gt;user guide&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/docs/com/twitter/finagle&#34;&gt;API documentation&lt;/a&gt; websites. Documentation improvements are always welcome, so please send patches our way.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The following are a few of the companies that are using Finagle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://foursquare.com/&#34;&gt;Foursquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ing.nl&#34;&gt;ING Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinterest.com/&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://soundcloud.com/&#34;&gt;SoundCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tumblr.com/&#34;&gt;Tumblr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a more complete list, please see &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/ADOPTERS.md&#34;&gt;our adopter page&lt;/a&gt;. If your organization is using Finagle, consider adding a link there and sending us a pull request!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2010 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lampepfl/dotty</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/lampepfl/dotty</id>
    <link href="https://github.com/lampepfl/dotty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/lampepfl/dotty/workflows/Dotty/badge.svg?branch=master&#34; alt=&#34;Dotty CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632150470000902164&#34; alt=&#34;Join the chat at https://discord.com/invite/scala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Try it out&lt;/h1&gt; &#xA;&lt;p&gt;To try it in your project see also the &lt;a href=&#34;https://docs.scala-lang.org/scala3/getting-started.html&#34;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt dist/packArchive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Dotty uses the &lt;a href=&#34;https://www.scala-lang.org/conduct.html&#34;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; &#xA;&lt;h1&gt;How to Contribute&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&#34;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Dotty is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snowplow/snowplow</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/snowplow/snowplow</id>
    <link href="https://github.com/snowplow/snowplow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The enterprise-grade behavioral data engine (web, mobile, server-side, webhooks), running cloud-natively on AWS and GCP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Snowplow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/releases/tag/22.01&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Snowplow-22.01%20Western%20Ghats-6638b8&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2-blue.svg?style=flat&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/discourse/posts?server=https%3A%2F%2Fdiscourse.snowplowanalytics.com%2F&#34; alt=&#34;Discourse posts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_logo.png&#34; alt=&#34;Snowplow logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Identifies your users, and tracks the way they engage with your website or application&lt;/li&gt; &#xA; &lt;li&gt;Stores your users&#39; behavioral data in a scalable &#34;event data warehouse&#34; you control: Amazon Redshift, Google BigQuery, Snowflake or Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioral data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;To find out more, please check out the &lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;Snowplow website&lt;/a&gt; and the &lt;a href=&#34;https://docs.snowplowanalytics.com/open-source-docs/&#34;&gt;docs website&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version Compatibility Matrix&lt;/h3&gt; &#xA;&lt;p&gt;For compatibility assurance, the version compatibility matrix offers clarity on our recommended stack. It is strongly recommended when setting up a Snowplow pipeline to use the versions listed in the version compatibility matrix which can be found &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/pipeline-components-and-applications/version-compatibility-matrix/&#34;&gt;within our docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Public Roadmap&lt;/h3&gt; &#xA;&lt;p&gt;This repository also contains the &lt;a href=&#34;https://github.com/snowplow/snowplow/projects&#34;&gt;Snowplow Public Roadmap&lt;/a&gt;. The Public Roadmap lets you stay up to date and find out what&#39;s happening on the Snowplow Platform. Help us prioritize our cards: open the issue and leave a 👍 to vote for your favorites. Want us to build a feature or function? Tell us by heading to our &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse forum&lt;/a&gt; 💬.&lt;/p&gt; &#xA;&lt;h3&gt;Try Snowplow&lt;/h3&gt; &#xA;&lt;p&gt;Setting up a full open-source Snowplow pipeline requires a non-trivial amount of engineering expertise and time investment. You might be interested in finding out what Snowplow can do first, by setting up &lt;a href=&#34;https://try.snowplowanalytics.com/?utm_source=github&amp;amp;utm_medium=post&amp;amp;utm_campaign=try-snowplow&#34;&gt;Try Snowplow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Open Source Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/open-source-quick-start/&#34;&gt;Open Source Quick Start&lt;/a&gt; will help you get up and running with a Snowplow open source pipeline. Snowplow publishes a &lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;set of terraform modules&lt;/a&gt;, which automate the setting up &amp;amp; deployment of the required infrastructure &amp;amp; applications for an operational Snowplow open source pipeline, with just a handful of input variables required on your side.&lt;/p&gt; &#xA;&lt;h3&gt;Join the Snowplow Research Panel and help shape the future of open source&lt;/h3&gt; &#xA;&lt;p&gt;As part of our ongoing efforts to improve the Snowplow Open Source experience, we&#39;re looking for users of our open-source software and members of our community to take part in research studies. &lt;a href=&#34;https://forms.gle/pCtYx8naum7A8vvw5&#34;&gt;Join here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Our Commercial Offering&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to get everything setup and managed for you, you can consider &lt;a href=&#34;https://snowplowanalytics.com/products/snowplow-bdp/&#34;&gt;Snowplow BDP&lt;/a&gt;. You can also &lt;a href=&#34;https://go.snowplowanalytics.com/l/571483/2021-05-04/3sv1pg8&#34;&gt;request a demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Snowplow technology 101&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_architecture.png&#34; alt=&#34;Snowplow architecture&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats.&lt;/p&gt; &#xA;&lt;p&gt;To briefly explain these six sub-systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/1-trackers&#34;&gt;Trackers&lt;/a&gt;&lt;/strong&gt; fire Snowplow events. Currently we have 15 trackers, covering web, mobile, desktop, server and IoT&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/2-collectors&#34;&gt;Collector&lt;/a&gt;&lt;/strong&gt; receives Snowplow events from trackers. Currently we have one official collector implementation with different sinks: Amazon Kinesis, Google PubSub, Amazon SQS, Apache Kafka and NSQ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/3-enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/strong&gt; cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have several implementations, built for different environments (GCP, AWS, Apache Kafka) and one core library&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/4-storage&#34;&gt;Storage&lt;/a&gt;&lt;/strong&gt; is where the Snowplow events live. Currently we store the Snowplow events in a flat file structure on S3, and in the Redshift, Postgres, Snowflake and BigQuery databases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/5-data-modeling&#34;&gt;Data modeling&lt;/a&gt;&lt;/strong&gt; is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We officially support data models for Redshift, Snowflake and BigQuery.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/modeling-your-data/analytics-sdk/&#34;&gt;Analytics&lt;/a&gt;&lt;/strong&gt; are performed on the Snowplow events or on the aggregate tables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more information on the current Snowplow architecture, please see the &lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;Technical architecture&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About this repository&lt;/h2&gt; &#xA;&lt;p&gt;This repository is an umbrella repository for all loosely-coupled Snowplow components and is updated on each component release.&lt;/p&gt; &#xA;&lt;p&gt;Since June 2020, all components have been extracted into their dedicated repositories (more info &lt;a href=&#34;https://snowplowanalytics.com/blog/2020/07/16/changing-releasing/&#34;&gt;here&lt;/a&gt;) and this repository serves as an entry point for Snowplow users, the home of our public roadmap and as a historical artifact.&lt;/p&gt; &#xA;&lt;p&gt;Components that have been extracted to their own repository are still here as &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;git submodules&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Trackers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Web&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mobile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Gaming&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TV&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Desktop &amp;amp; Server&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;JavaScript&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-android-tracker&#34;&gt;Android&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-unity-tracker&#34;&gt;Unity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-roku-tracker&#34;&gt;Roku&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-tracking-cli&#34;&gt;Command line&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/google-amp-tracker/&#34;&gt;AMP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-objc-tracker&#34;&gt;iOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-tracker&#34;&gt;.NET&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-react-native-tracker&#34;&gt;React Native&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-tracker&#34;&gt;Go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-flutter-tracker&#34;&gt;Flutter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-java-tracker&#34;&gt;Java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;Node.js&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-php-tracker&#34;&gt;PHP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-tracker&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-ruby-tracker&#34;&gt;Ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-tracker&#34;&gt;Scala&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/stream-collector&#34;&gt;Collector&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Loaders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-bigquery-loader&#34;&gt;BigQuery (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-rdb-loader&#34;&gt;Redshift (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-snowflake-loader&#34;&gt;Snowflake (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-google-cloud-storage-loader&#34;&gt;Google Cloud Storage (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-s3-loader&#34;&gt;Amazon S3 (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-postgres-loader&#34;&gt;Postgres (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-elasticsearch-loader&#34;&gt;Elasticsearch (streaming)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Iglu&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/iglu-server/&#34;&gt;Iglu Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/igluctl/&#34;&gt;igluctl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/iglu-central/&#34;&gt;Iglu Central&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data modeling&lt;/h3&gt; &#xA;&lt;h4&gt;Web&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/web/v1&#34;&gt;Web model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/dbt-snowplow-web&#34;&gt;Web model: dbt version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mobile&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/mobile/v1&#34;&gt;Mobile model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-mini&#34;&gt;Mini&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-micro&#34;&gt;Micro&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parsing enriched event&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-analytics-sdk&#34;&gt;Analytics SDK Scala&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-analytics-sdk&#34;&gt;Analytics SDK Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-analytics-sdk&#34;&gt;Analytics SDK .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-js-analytics-sdk/&#34;&gt;Analytics SDK Javascript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-analytics-sdk&#34;&gt;Analytics SDK Golang&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-badrows&#34;&gt;Bad rows&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;Terraform Modules&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Need help?&lt;/h2&gt; &#xA;&lt;p&gt;We want to make it super-easy for Snowplow users and contributors to talk to us and connect with each other, to share ideas, solve problems and help make Snowplow awesome. Here are the main channels we&#39;re running currently, we&#39;d love to hear from you on one of them:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is for all Snowplow users: engineers setting up Snowplow, data modelers structuring the data and data consumers building insights. You can find guides, recipes, questions and answers from Snowplow users including the Snowplow team.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all questions and contributions!&lt;/p&gt; &#xA;&lt;h3&gt;Twitter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/SnowplowData&#34;&gt;@SnowplowData&lt;/a&gt; for official news or &lt;a href=&#34;https://twitter.com/SnowplowLabs&#34;&gt;@SnowplowLabs&lt;/a&gt; for engineering-heavy conversations and release updates.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;If you spot a bug, then please raise an issue in the GitHub repository of the component in question. Likewise if you have developed a cool new feature or an improvement, please open a pull request, we&#39;ll be glad to integrate it in the codebase!&lt;/p&gt; &#xA;&lt;p&gt;If you want to brainstorm a potential new feature, then &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt; is the best place to start.&lt;/p&gt; &#xA;&lt;h3&gt;Email&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:community@snowplowanalytics.com&#34;&gt;community@snowplowanalytics.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to talk directly to us (e.g. about a commercially sensitive issue), email is the easiest way.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and license&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is copyright 2012-2022 Snowplow Analytics Ltd.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;strong&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the &#34;License&#34;); you may not use this software except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scala/scala</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/scala/scala</id>
    <link href="https://github.com/scala/scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala 2 compiler and standard library. For bugs, see scala/bug&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome!&lt;/h1&gt; &#xA;&lt;p&gt;This is the home of the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala 2&lt;/a&gt; standard library, compiler, and language spec.&lt;/p&gt; &#xA;&lt;h1&gt;How to contribute&lt;/h1&gt; &#xA;&lt;p&gt;Issues and bug reports for Scala 2 are located in &lt;a href=&#34;https://github.com/scala/bug&#34;&gt;scala/bug&lt;/a&gt;. That tracker is also where new contributors may find issues to work on: &lt;a href=&#34;https://github.com/scala/bug/labels/good%20first%20issue&#34;&gt;good first issues&lt;/a&gt;, &lt;a href=&#34;https://github.com/scala/bug/labels/help%20wanted&#34;&gt;help wanted&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For coordinating broader efforts, we also use the &lt;a href=&#34;https://github.com/scala/scala-dev/issues&#34;&gt;scala/scala-dev tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute here, please open a &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/#fork--pull&#34;&gt;pull request&lt;/a&gt; from your fork of this repository.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that we can&#39;t accept additions to the standard library, only modifications to existing code. Binary compatibility forbids adding new public classes or public methods. Additions are made to &lt;a href=&#34;https://github.com/scala/scala-library-next&#34;&gt;scala-library-next&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;We require that you sign the &lt;a href=&#34;https://www.lightbend.com/contribute/cla/scala&#34;&gt;Scala CLA&lt;/a&gt; before we can merge any of your work, to protect Scala&#39;s future as open source software.&lt;/p&gt; &#xA;&lt;p&gt;The general workflow is as follows.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/file an issue in scala/bug (or submit a well-documented PR right away!).&lt;/li&gt; &#xA; &lt;li&gt;Fork the scala/scala repo.&lt;/li&gt; &#xA; &lt;li&gt;Push your changes to a branch in your forked repo. For coding guidelines, go &lt;a href=&#34;https://github.com/scala/scala#coding-guidelines&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request to scala/scala from your forked repo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information on building and developing the core of Scala, read the rest of this README, especially for &lt;a href=&#34;https://github.com/scala/scala#get-ready-to-contribute&#34;&gt;setting up your machine&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Get in touch!&lt;/h1&gt; &#xA;&lt;p&gt;In order to get in touch with other Scala contributors, join the #scala-contributors channel on the &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;Scala Discord&lt;/a&gt; chat, or post on &lt;a href=&#34;https://contributors.scala-lang.org&#34;&gt;contributors.scala-lang.org&lt;/a&gt; (Discourse).&lt;/p&gt; &#xA;&lt;p&gt;If you need some help with your PR at any time, please feel free to @-mention anyone from the list below, and we will do our best to help you out:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;username&lt;/th&gt; &#xA;   &lt;th&gt;talk to me about...&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/lrytz&#34; height=&#34;50px&#34; title=&#34;Lukas Rytz&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lrytz&#34;&gt;&lt;code&gt;@lrytz&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;back end, optimizer, named &amp;amp; default arguments, reporters&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/retronym&#34; height=&#34;50px&#34; title=&#34;Jason Zaugg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/retronym&#34;&gt;&lt;code&gt;@retronym&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.12.x branch, compiler performance, weird compiler bugs, lambdas&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/SethTisue&#34; height=&#34;50px&#34; title=&#34;Seth Tisue&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SethTisue&#34;&gt;&lt;code&gt;@SethTisue&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;getting started, build, CI, community build, Jenkins, docs, library, REPL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/dwijnand&#34; height=&#34;50px&#34; title=&#34;Dale Wijnand&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dwijnand&#34;&gt;&lt;code&gt;@dwijnand&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pattern matcher, MiMa, partest&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/Ichoran&#34; height=&#34;50px&#34; title=&#34;Rex Kerr&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ichoran&#34;&gt;&lt;code&gt;@Ichoran&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;collections library, performance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/viktorklang&#34; height=&#34;50px&#34; title=&#34;Viktor Klang&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/viktorklang&#34;&gt;&lt;code&gt;@viktorklang&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;concurrency, futures&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/sjrd&#34; height=&#34;50px&#34; title=&#34;Sébastien Doeraene&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sjrd&#34;&gt;&lt;code&gt;@sjrd&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;interactions with Scala.js&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/NthPortal&#34; height=&#34;50px&#34; title=&#34;Princess | April&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NthPortal&#34;&gt;&lt;code&gt;@NthPortal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;library, concurrency, &lt;code&gt;scala.math&lt;/code&gt;, &lt;code&gt;LazyList&lt;/code&gt;, &lt;code&gt;Using&lt;/code&gt;, warnings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/bishabosha&#34; height=&#34;50px&#34; title=&#34;Jamie Thompson&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bishabosha&#34;&gt;&lt;code&gt;@bishabosha&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TASTy reader&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/joroKr21&#34; height=&#34;50px&#34; title=&#34;Georgi Krastev&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/joroKr21&#34;&gt;&lt;code&gt;@joroKr21&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;higher-kinded types, implicits, variance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;P.S.: If you have some spare time to help out around here, we would be delighted to add your name to this list!&lt;/p&gt; &#xA;&lt;h1&gt;Branches&lt;/h1&gt; &#xA;&lt;p&gt;Target the oldest branch you would like your changes to end up in. We periodically merge forward from older release branches (e.g., 2.12.x) to new ones (e.g. 2.13.x).&lt;/p&gt; &#xA;&lt;p&gt;If your change is difficult to merge forward, you may be asked to also submit a separate PR targeting the newer branch.&lt;/p&gt; &#xA;&lt;p&gt;If your change is version-specific and shouldn&#39;t be merged forward, put &lt;code&gt;[nomerge]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;p&gt;If your change is a backport from a newer branch and thus doesn&#39;t need to be merged forward, put &lt;code&gt;[backport]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;h2&gt;Choosing a branch&lt;/h2&gt; &#xA;&lt;p&gt;Most changes should target 2.13.x. We are increasingly reluctant to target 2.12.x unless there is a special reason (e.g. if an especially bad bug is found, or if there is commercial sponsorship).&lt;/p&gt; &#xA;&lt;p&gt;The 2.11.x branch is now &lt;a href=&#34;https://github.com/scala/scala-dev/issues/451&#34;&gt;inactive&lt;/a&gt; and no further 2.11.x releases are planned (unless unusual, unforeseeable circumstances arise). You should not target 2.11.x without asking maintainers first.&lt;/p&gt; &#xA;&lt;h1&gt;Repository structure&lt;/h1&gt; &#xA;&lt;p&gt;Most importantly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;+--build.sbt                 The main sbt build definition&#xA;+--project/                  The rest of the sbt build&#xA;+--src/                      All sources&#xA;   +---/library              Scala Standard Library&#xA;   +---/reflect              Scala Reflection&#xA;   +---/compiler             Scala Compiler&#xA;+--test/                     The Scala test suite&#xA;   +---/files                Partest tests&#xA;   +---/junit                JUnit tests&#xA;   +---/scalacheck           ScalaCheck tests&#xA;+--spec/                     The Scala language specification&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;but also:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;   +---/library-aux          Scala Auxiliary Library, for bootstrapping and documentation purposes&#xA;   +---/interactive          Scala Interactive Compiler, for clients such as an IDE (aka Presentation Compiler)&#xA;   +---/intellij             IntelliJ project templates&#xA;   +---/manual               Scala&#39;s runner scripts &#34;man&#34; (manual) pages&#xA;   +---/partest              Scala&#39;s internal parallel testing framework&#xA;   +---/partest-javaagent    Partest&#39;s helper java agent&#xA;   +---/repl                 Scala REPL core&#xA;   +---/repl-frontend        Scala REPL frontend&#xA;   +---/scaladoc             Scala&#39;s documentation tool&#xA;   +---/scalap               Scala&#39;s class file decompiler&#xA;   +---/testkit              Scala&#39;s unit-testing kit&#xA;+--admin/                    Scripts for the CI jobs and releasing&#xA;+--doc/                      Additional licenses and copyrights&#xA;+--scripts/                  Scripts for the CI jobs and releasing&#xA;+--tools/                    Scripts useful for local development&#xA;+--build/                    Build products&#xA;+--dist/                     Build products&#xA;+--target/                   Build products&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Get ready to contribute&lt;/h1&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You need the following tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java SDK. The baseline version is 8 for both 2.12.x and 2.13.x. It is almost always fine to use a later SDK such as 11 or 15 for local development. CI will verify against the baseline version.&lt;/li&gt; &#xA; &lt;li&gt;sbt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MacOS and Linux work. Windows may work if you use Cygwin. Community help with keeping the build working on Windows and documenting any needed setup is appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Tools we use&lt;/h2&gt; &#xA;&lt;p&gt;We are grateful for the following OSS licenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler Java profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.yourkit.com/java/profiler/&#34;&gt;YourKit Java Profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/idea/download/&#34;&gt;IntelliJ IDEA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build setup&lt;/h2&gt; &#xA;&lt;h3&gt;Basics&lt;/h3&gt; &#xA;&lt;p&gt;During ordinary development, a new Scala build is built by the previously released version, known as the &#34;reference compiler&#34; or, slangily, as &#34;STARR&#34; (stable reference release). Building with STARR is sufficient for most kinds of changes.&lt;/p&gt; &#xA;&lt;p&gt;However, a full build of Scala is &lt;em&gt;bootstrapped&lt;/em&gt;. Bootstrapping has two steps: first, build with STARR; then, build again using the freshly built compiler, leaving STARR behind. This guarantees that every Scala version can build itself.&lt;/p&gt; &#xA;&lt;p&gt;If you change the code generation part of the Scala compiler, your changes will only show up in the bytecode of the library and compiler after a bootstrap. Our CI does a bootstrapped build.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bootstrapping locally&lt;/strong&gt;: To perform a bootstrap, run &lt;code&gt;restarrFull&lt;/code&gt; within an sbt session. This will build and publish the Scala distribution to your local artifact repository and then switch sbt to use that version as its new &lt;code&gt;scalaVersion&lt;/code&gt;. You may then revert back with &lt;code&gt;reload&lt;/code&gt;. Note &lt;code&gt;restarrFull&lt;/code&gt; will also write the STARR version to &lt;code&gt;buildcharacter.properties&lt;/code&gt; so you can switch back to it with &lt;code&gt;restarr&lt;/code&gt; without republishing. This will switch the sbt session to use the &lt;code&gt;build-restarr&lt;/code&gt; and &lt;code&gt;target-restarr&lt;/code&gt; directories instead of &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;, which avoids wiping out classfiles and incremental metadata. IntelliJ will continue to be configured to compile and run tests using the starr version in &lt;code&gt;versions.properties&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For history on how the current scheme was arrived at, see &lt;a href=&#34;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&#34;&gt;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Building with fatal warnings&lt;/strong&gt;: To make warnings in the project fatal (i.e. turn them into errors), run &lt;code&gt;set Global / fatalWarnings := true&lt;/code&gt; in sbt (replace &lt;code&gt;Global&lt;/code&gt; with the name of a module—such as &lt;code&gt;reflect&lt;/code&gt;—to only make warnings fatal for that module). To disable fatal warnings again, either &lt;code&gt;reload&lt;/code&gt; sbt, or run &lt;code&gt;set Global / fatalWarnings := false&lt;/code&gt; (again, replace &lt;code&gt;Global&lt;/code&gt; with the name of a module if you only enabled fatal warnings for that module). CI always has fatal warnings enabled.&lt;/p&gt; &#xA;&lt;h3&gt;Using the sbt build&lt;/h3&gt; &#xA;&lt;p&gt;Once you&#39;ve started an &lt;code&gt;sbt&lt;/code&gt; session you can run one of the core commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;compile&lt;/code&gt; compiles all sub-projects (library, reflect, compiler, scaladoc, etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scala&lt;/code&gt; / &lt;code&gt;scalac&lt;/code&gt; run the REPL / compiler directly from sbt (accept options / arguments)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enableOptimizer&lt;/code&gt; reloads the build with the Scala optimizer enabled. Our releases are built this way. Enable this when working on compiler performance improvements. When the optimizer is enabled the build will be slower and incremental builds can be incorrect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setupPublishCore&lt;/code&gt; runs &lt;code&gt;enableOptimizer&lt;/code&gt; and configures a version number based on the current Git SHA. Often used as part of bootstrapping: &lt;code&gt;sbt setupPublishCore publishLocal &amp;amp;&amp;amp; sbt -Dstarr.version=&amp;lt;VERSION&amp;gt; testAll&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkBin&lt;/code&gt; generates runner scripts (&lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt;, etc) in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkPack&lt;/code&gt; creates a build in the Scala distribution format in &lt;code&gt;build/pack&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;junit/test&lt;/code&gt; runs the JUnit tests; &lt;code&gt;junit/testOnly *Foo&lt;/code&gt; runs a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scalacheck/test&lt;/code&gt; runs scalacheck tests, use &lt;code&gt;testOnly&lt;/code&gt; to run a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partest&lt;/code&gt; runs partest tests (accepts options, try &lt;code&gt;partest --help&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;publishLocal&lt;/code&gt; publishes a distribution locally (can be used as &lt;code&gt;scalaVersion&lt;/code&gt; in other sbt projects) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set baseVersionSuffix := &#34;bin-abcd123-SNAPSHOT&#34;&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the git hash of the revision being published. You can also use something custom like &lt;code&gt;&#34;bin-mypatch&#34;&lt;/code&gt;. This changes the version number from &lt;code&gt;2.13.2-SNAPSHOT&lt;/code&gt; to something more stable (&lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Note that the &lt;code&gt;-bin&lt;/code&gt; string marks the version binary compatible. Using it in sbt will cause the &lt;code&gt;scalaBinaryVersion&lt;/code&gt; to be &lt;code&gt;2.13&lt;/code&gt;. If the version is not binary compatible, we recommend using &lt;code&gt;-pre&lt;/code&gt;, e.g., &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set publishArtifact in (Compile, packageDoc) in ThisBuild := false&lt;/code&gt; to skip generating / publishing API docs (speeds up the process).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a command results in an error message like &lt;code&gt;a module is not authorized to depend on itself&lt;/code&gt;, it may be that a global sbt plugin is causing a cyclical dependency. Try disabling global sbt plugins (perhaps by temporarily commenting them out in &lt;code&gt;~/.sbt/1.0/plugins/plugins.sbt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Sandbox&lt;/h4&gt; &#xA;&lt;p&gt;We recommend keeping local test files in the &lt;code&gt;sandbox&lt;/code&gt; directory which is listed in the &lt;code&gt;.gitignore&lt;/code&gt; of the Scala repo.&lt;/p&gt; &#xA;&lt;h4&gt;Incremental compilation&lt;/h4&gt; &#xA;&lt;p&gt;Note that sbt&#39;s incremental compilation is often too coarse for the Scala compiler codebase and re-compiles too many files, resulting in long build times (check &lt;a href=&#34;https://github.com/sbt/sbt/issues/1104&#34;&gt;sbt#1104&lt;/a&gt; for progress on that front). In the meantime you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use IntelliJ IDEA for incremental compiles (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/#ide-setup&#34;&gt;IDE Setup&lt;/a&gt; below) - its incremental compiler is a bit less conservative, but usually correct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;IDE setup&lt;/h3&gt; &#xA;&lt;p&gt;We suggest using IntelliJ IDEA (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/src/intellij/README.md&#34;&gt;src/intellij/README.md&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalameta.org/metals/&#34;&gt;Metals&lt;/a&gt; may also work, but we don&#39;t yet have instructions or sample configuration for that. A pull request in this area would be exceedingly welcome. In the meantime, we are collecting guidance at &lt;a href=&#34;https://github.com/scala/scala-dev/issues/668&#34;&gt;scala/scala-dev#668&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to use IntelliJ&#39;s incremental compiler:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run &lt;code&gt;dist/mkBin&lt;/code&gt; in sbt to get a build and the runner scripts in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &#34;Build&#34; - &#34;Make Project&#34; in IntelliJ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now you can edit and build in IntelliJ and use the scripts (compiler, REPL) to directly test your changes. You can also run the &lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt; and &lt;code&gt;partest&lt;/code&gt; commands in sbt. Enable &#34;Ant mode&#34; (explained above) to prevent sbt&#39;s incremental compiler from re-compiling (too many) files before each &lt;code&gt;partest&lt;/code&gt; invocation.&lt;/p&gt; &#xA;&lt;h1&gt;Coding guidelines&lt;/h1&gt; &#xA;&lt;p&gt;Our guidelines for contributing are explained in &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;. It contains useful information on our coding standards, testing, documentation, how we use git and GitHub and how to get your code reviewed.&lt;/p&gt; &#xA;&lt;p&gt;You may also want to check out the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://scala-lang.org/contribute/hacker-guide.html&#34;&gt;&#34;Scala Hacker Guide&#34;&lt;/a&gt; covers some of the same ground as this README, but in greater detail and in a more tutorial style, using a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org&#34;&gt;Scala documentation site&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Scala CI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/scala/scala&#34;&gt;&lt;img src=&#34;https://travis-ci.com/scala/scala.svg?branch=2.13.x&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once you submit a PR your commits will be automatically tested by the Scala CI.&lt;/p&gt; &#xA;&lt;p&gt;Our CI setup is always evolving. See &lt;a href=&#34;https://github.com/scala/scala-dev/issues/751&#34;&gt;scala/scala-dev#751&lt;/a&gt; for more details on how things currently work and how we expect they might change.&lt;/p&gt; &#xA;&lt;p&gt;If you see a spurious failure on Jenkins, you can post &lt;code&gt;/rebuild&lt;/code&gt; as a PR comment. The &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scabot README&lt;/a&gt; lists all available commands.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to test your patch before having everything polished for review, you can have Travis CI build your branch (make sure you have a fork and have Travis CI enabled for branch builds on it first, and then push your branch). Also feel free to submit a draft PR. In case your draft branch contains a large number of commits (that you didn&#39;t clean up / squash yet for review), consider adding &lt;code&gt;[ci: last-only]&lt;/code&gt; to the PR title. That way only the last commit will be tested, saving some energy and CI-resources. Note that inactive draft PRs will be closed eventually, which does not mean the change is being rejected.&lt;/p&gt; &#xA;&lt;p&gt;CI performs a compiler bootstrap. The first task, &lt;code&gt;validatePublishCore&lt;/code&gt;, publishes a build of your commit to the temporary repository &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&lt;/a&gt;. Note that this build is not yet bootstrapped, its bytecode is built using the current STARR. The version number is &lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the commit hash. For binary incompatible builds, the version number is &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use Scala builds in the validation repository locally by adding a resolver and specifying the corresponding &lt;code&gt;scalaVersion&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set resolvers += &#34;pr&#34; at &#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots/&#34;&#xA;&amp;gt; set scalaVersion := &#34;2.12.2-bin-abcd123-SNAPSHOT&#34;&#xA;&amp;gt; console&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&#34;Nightly&#34; builds&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI builds nightly download releases and publishes them to &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-integration/&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-integration/&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Using a nightly build in sbt is explained in &lt;a href=&#34;https://stackoverflow.com/questions/40622878&#34;&gt;this Stack Overflow answer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Although we casually refer to these as &#34;nightly&#34; builds, they aren&#39;t actually built nightly, but &#34;mergely&#34;. That is to say, a build is published for every merged PR.&lt;/p&gt; &#xA;&lt;h2&gt;Scala CI internals&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI runs as a Jenkins instance on &lt;a href=&#34;https://scala-ci.typesafe.com/&#34;&gt;scala-ci.typesafe.com&lt;/a&gt;, configured by a chef cookbook at &lt;a href=&#34;https://github.com/scala/scala-jenkins-infra&#34;&gt;scala/scala-jenkins-infra&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The build bot that watches PRs, triggers testing builds and applies the &#34;reviewed&#34; label after an LGTM comment is in the &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scala/scabot&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Community build&lt;/h2&gt; &#xA;&lt;p&gt;The Scala community build is an important method for testing Scala releases. A community build can be launched for any Scala commit, even before the commit&#39;s PR has been merged. That commit is then used to build a large number of open-source projects from source and run their test suites.&lt;/p&gt; &#xA;&lt;p&gt;To request a community build run on your PR, just ask in a comment on the PR and a Scala team member (probably @SethTisue) will take care of it. (&lt;a href=&#34;https://github.com/scala/community-builds/wiki#can-i-run-it-against-a-pull-request-in-scalascala&#34;&gt;details&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Community builds run on the Scala Jenkins instance. The jobs are named &lt;code&gt;..-integrate-community-build&lt;/code&gt;. See the &lt;a href=&#34;https://github.com/scala/community-builds&#34;&gt;scala/community-builds&lt;/a&gt; repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dream11/zio-http</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/dream11/zio-http</id>
    <link href="https://github.com/dream11/zio-http" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A scala library to write Http apps.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ZIO Http&lt;/h1&gt; &#xA;&lt;p&gt;ZIO Http is a scala library for building http apps. It is powered by &lt;a href=&#34;https://zio.dev&#34;&gt;ZIO&lt;/a&gt; and &lt;a href=&#34;http://netty.io&#34;&gt;netty&lt;/a&gt; and aims at being the defacto solution for writing, highly scalable and &lt;a href=&#34;https://raw.githubusercontent.com/dream11/zio-http/main/#benchmarks&#34;&gt;performant&lt;/a&gt; web applications using idiomatic scala.&lt;/p&gt; &#xA;&lt;p&gt;Check out the full documentation here: &lt;a href=&#34;https://dream11.github.io/zio-http&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/dream11/zio-http/workflows/Continuous%20Integration/badge.svg?sanitize=true&#34; alt=&#34;Continuous Integration&#34;&gt; &lt;a href=&#34;https://discord.com/channels/629491597070827530/819703129267372113&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530.svg?logo=discord&#34; alt=&#34;Discord Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/io/d11/zhttp_2.13/&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/io.d11/zhttp_2.13?server=https%3A%2F%2Fs01.oss.sonatype.org&#34; alt=&#34;Sonatype Nexus (Releases)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://s01.oss.sonatype.org/content/repositories/snapshots/io/d11/zhttp_2.13/&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/io.d11/zhttp_2.13?server=https%3A%2F%2Fs01.oss.sonatype.org&#34; alt=&#34;Sonatype Nexus (Snapshots)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/dream11/zio-http&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/dream11/zio-http.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://open.vscode.dev/dream11/zio-http&#34;&gt;&lt;img src=&#34;https://open.vscode.dev/badges/open-in-vscode.svg?sanitize=true&#34; alt=&#34;Open in Visual Studio Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream11/zio-http/main/#zio-http&#34;&gt;ZIO Http&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream11/zio-http/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream11/zio-http/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.github.io/zio-http/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;A simple Http server can be built using a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import zio._&#xA;import zhttp.http._&#xA;import zhttp.service.Server&#xA;&#xA;object HelloWorld extends App {&#xA;  val app = Http.collect[Request] {&#xA;    case Method.GET -&amp;gt; !! / &#34;text&#34; =&amp;gt; Response.text(&#34;Hello World!&#34;)&#xA;  }&#xA;&#xA;  override def run(args: List[String]): URIO[zio.ZEnv, ExitCode] =&#xA;    Server.start(8090, app).exitCode&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;p&gt;You can checkout more examples in the &lt;a href=&#34;https://github.com/dream11/zio-http/tree/main/example/src/main/scala/example&#34;&gt;example&lt;/a&gt; project —&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/HelloWorld.scala&#34;&gt;Simple Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/HelloWorldAdvanced.scala&#34;&gt;Advanced Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/WebSocketEcho.scala&#34;&gt;WebSocket Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/StreamingResponse.scala&#34;&gt;Streaming Response&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/SimpleClient.scala&#34;&gt;Simple Client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/FileStreaming.scala&#34;&gt;File Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/example/src/main/scala/example/Authentication.scala&#34;&gt;Authentication&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Steps to run an example&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Edit the &lt;a href=&#34;https://github.com/dream11/zio-http/raw/main/project/BuildHelper.scala#L109&#34;&gt;RunSettings&lt;/a&gt; - modify &lt;code&gt;className&lt;/code&gt; to the example you&#39;d like to run.&lt;/li&gt; &#xA; &lt;li&gt;From sbt shell, run &lt;code&gt;~example/reStart&lt;/code&gt;. You should see &lt;code&gt;Server started on port: 8090&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Send curl request for defined &lt;code&gt;http Routes&lt;/code&gt;, for eg : &lt;code&gt;curl -i &#34;http://localhost:8090/text&#34;&lt;/code&gt; for &lt;code&gt;example.HelloWorld&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Setup via &lt;code&gt;build.sbt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;io.d11&#34; %% &#34;zhttp&#34;      % &#34;[version]&#34;&#xA;libraryDependencies += &#34;io.d11&#34; %% &#34;zhttp-test&#34; % &#34;[version]&#34; % Test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; ZIO Http is compatible with &lt;code&gt;ZIO 1.x&lt;/code&gt; and &lt;code&gt;ZIO 2.x&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Watch Mode&lt;/h1&gt; &#xA;&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/spray/sbt-revolver&#34;&gt;sbt-revolver&lt;/a&gt; plugin to start the server and run it in watch mode using &lt;code&gt;~ reStart&lt;/code&gt; command on the SBT console.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zio/zio-schema</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/zio/zio-schema</id>
    <link href="https://github.com/zio/zio-schema" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Compositional, type-safe schema definitions, which enable auto-derivation of codecs and migrations.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ZIO-SCHEMA&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Development-yellowgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio-schema/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio-schema_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio-schema_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://isitmaintained.com/project/zio/zio-schema&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/zio/zio-schema.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;badge-discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;ZIO Schema&lt;/em&gt; is a &lt;a href=&#34;https://zio.dev&#34;&gt;ZIO&lt;/a&gt;-based library for modeling the schema of data structures as first-class values.&lt;/p&gt; &#xA;&lt;p&gt;With schema descriptions that can be automatically derived for case classes and sealed traits, &lt;em&gt;ZIO Schema&lt;/em&gt; provide powerful features for free:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Codecs for any supported protocol (JSON, protobuf, etc.), so data structures can be serialized and deserialized in a principled way&lt;/li&gt; &#xA; &lt;li&gt;Diffing, patching, merging, and other generic-data-based operations&lt;/li&gt; &#xA; &lt;li&gt;Migration of data structures from one schema to another compatible schema&lt;/li&gt; &#xA; &lt;li&gt;Derivation of arbitrary type classes (&lt;code&gt;Eq&lt;/code&gt;, &lt;code&gt;Show&lt;/code&gt;, &lt;code&gt;Ord&lt;/code&gt;, etc.) from the structure of the data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When your data structures need to be serialized, deserialized, persisted, or transported across the wire, then &lt;em&gt;ZIO Schema&lt;/em&gt; lets you focus on data modeling and automatically tackle all the low-level, messy details for you.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;ZIO Schema&lt;/em&gt; is used by a growing number of ZIO libraries, including &lt;em&gt;ZIO Flow&lt;/em&gt;, &lt;em&gt;ZIO Redis&lt;/em&gt;, &lt;em&gt;ZIO Web&lt;/em&gt;, &lt;em&gt;ZIO SQL&lt;/em&gt; and &lt;em&gt;ZIO DynamoDB&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Add in your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies ++= Seq(&#xA;  &#34;dev.zio&#34; %% &#34;zio-schema&#34; % &#34;&amp;lt;version&amp;gt;&#34;,&#xA;  // Required for automatic generic derivation of schemas&#xA;  &#34;dev.zio&#34; %% &#34;zio-schema-derivation&#34; % &#34;&amp;lt;version&amp;gt;&#34;,&#xA;  &#34;org.scala-lang&#34; % &#34;scala-reflect&#34;  % scalaVersion.value % &#34;provided&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;For the general guidelines, see ZIO &lt;a href=&#34;https://github.com/zio/zio/raw/master/docs/about/contributing.md&#34;&gt;contributor&#39;s guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;TL;DR&lt;/h4&gt; &#xA;&lt;p&gt;Before you submit a PR, make sure your tests are passing, and that the code is properly formatted&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt prepare&#xA;&#xA;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/testchipip</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/ucb-bar/testchipip</id>
    <link href="https://github.com/ucb-bar/testchipip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;testchipip&lt;/h1&gt; &#xA;&lt;p&gt;Useful IP components for chips. BAR projects generally use these components with &lt;a href=&#34;https://github.com/freechipsproject/rocket-chip&#34;&gt;rocket-chip&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Blocks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Block device model&lt;/li&gt; &#xA; &lt;li&gt;Clock utilities for Chisel, e.g. clock mux, clock divider, etc.&lt;/li&gt; &#xA; &lt;li&gt;SERDES &amp;lt;-&amp;gt; TileLink&lt;/li&gt; &#xA; &lt;li&gt;Custom serial interface for debug with simulator interface&lt;/li&gt; &#xA; &lt;li&gt;TileLink splitter, switcher&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Testchipip can be used in your project in one of two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;As an sbt subproject that depends on rocket-chip, as in &lt;a href=&#34;https://github.com/ucb-bar/chipyard/&#34;&gt;chipyard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;As a maven dependency (e.g. write&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;testchipip&#34; % &#34;1.0-020719-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in your build.sbt). Check &lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/edu/berkeley/cs/testchipip_2.12/&#34;&gt;sonatype&lt;/a&gt; to see the latest published version.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/riscv-torture</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/ucb-bar/riscv-torture</id>
    <link href="https://github.com/ucb-bar/riscv-torture" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RISC-V Torture Test&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;=========================================================================== RISC-V Torture Test Generator&lt;/h1&gt; &#xA;&lt;h1&gt;Author: Yunsup Lee and Henry Cook&lt;/h1&gt; &#xA;&lt;h1&gt;Date: January 29th, 2012&lt;/h1&gt; &#xA;&lt;h1&gt;Version: (under version control)&lt;/h1&gt; &#xA;&lt;p&gt;This is the RISC-V torture test generator and framework. This repository contains three sub-projects that build upon one another. The first, [generator], is used to create a single random torture test. The second, [testrun], is used to run a particular test on particular simulators, diffing the resulting signature with the ISA simulator and optionally creating a derivative test subset that pinpoints the divergence. The third, [overnight], wraps testrun, allowing tests to be run repeatedly for a given duration or until a failure count.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Modify &#34;config/default.config&#34; to set the parameters desired for building tests (e.g., setting which instructions to use and in which ratio).&lt;/p&gt; &#xA;&lt;p&gt;Modify &#34;Makefile&#34; as desired to execute the C simulator or RTL simulator of your choice, and to set the other parameters as you require.&lt;/p&gt; &#xA;&lt;p&gt;To build a single test and test it on Spike:&lt;/p&gt; &#xA;&lt;p&gt;$ make igentest&lt;/p&gt; &#xA;&lt;p&gt;To build single test and run it on the C simulator or RTL simulator, use &#34;make cgentest&#34; or &#34;make rgentest&#34;.&lt;/p&gt; &#xA;&lt;p&gt;To run overnight tests, you can use &#34;make cnight&#34; and &#34;make rnight&#34;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Signatures&lt;/h2&gt; &#xA;&lt;p&gt;Torture works by dumping the register state out to memory at the end of the test program execution. This output is then compared against the output from the Spike ISA simulator.&lt;/p&gt; &#xA;&lt;p&gt;The torture program writes the register state to the memory address specified by &#34;xreg_output_data&#34;, which is located in the memory section &#34;.global begin_signature&#34;. The Spike ISA simulator will write out the data found in the &#34;begin_signature&#34; section on exit if provided with the &#34;+signature=&#34; argument:&lt;/p&gt; &#xA;&lt;p&gt;$ spike +signature=my_spike_signature.txt test_binary&lt;/p&gt; &#xA;&lt;p&gt;The Rocket-chip infrastructure uses the &#34;riscv-fesvr&#34; program to control the execution of the C and RTL simulators. The &#34;riscv-fesvr&#34; also accepts the +signature argument too.&lt;/p&gt; &#xA;&lt;p&gt;$ ./csim-rocket-chip +signature=my_rocket_signature.txt test_binary&lt;/p&gt; &#xA;&lt;p&gt;A simple diff between the Spike and chip simulator signatures will tell you if any errors have occurred.&lt;/p&gt; &#xA;&lt;p&gt;$ diff my_spike_signature.txt my_rocket_signature.txt&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PORTING TORTURE TO YOUR OWN RISC-V PROCESSOR:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to use riscv-torture with your own RISC-V processor, you will need to provide a way to dump the &#34;begin_signature&#34; section to a file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Low-level Usage&lt;/h2&gt; &#xA;&lt;p&gt;Some basic use cases are illustrated here (note the Makefile abstracts this for you).&lt;/p&gt; &#xA;&lt;p&gt;Make a single test: % ./sbt generator/run % cd output % make % spike +signature=test.sig test&lt;/p&gt; &#xA;&lt;p&gt;Take an existing test and diff the signatures of ISA and C simulators: % ./sbt &#39;testrun/run -a output/test.S -c /path/to/reference-chip/emulator/emulator&#39;&lt;/p&gt; &#xA;&lt;p&gt;*** Currently, due to the limiation of scala process library, you cannot torture the RTL simulator ***&lt;/p&gt; &#xA;&lt;h1&gt;Generate a random test and diff the signatures of ISA and RTL simulators:&lt;/h1&gt; &#xA;&lt;h1&gt;% ./sbt &#39;testrun/run -r /path/to/reference-chip/vlsi/build/vcs-sim-rtl/simv&#39;&lt;/h1&gt; &#xA;&lt;p&gt;Run tests for 30 minutes, email hcook when done, and save failures to dir: % ./sbt &#39;overnight/run -m 30 -e &lt;a href=&#34;mailto:hcook@eecs.berkeley.edu&#34;&gt;hcook@eecs.berkeley.edu&lt;/a&gt; -p dir&#39;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;% git submodule update --init&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Overnight Overview&lt;/h2&gt; &#xA;&lt;p&gt;This framework utilizes both the test runner and test generator to perform a long terms serach for failing test cases. It takes the following command line arguments:&lt;/p&gt; &#xA;&lt;p&gt;Usage: overnight/run [options] -C &#xA; &lt;file&gt;&#xA;   | --config &#xA;  &lt;file&gt;&#xA;    config file -p &#xA;  &lt;/file&gt;&#xA; &lt;/file&gt;&lt;/p&gt;&#xA;&lt;dir&gt;&#xA;  | --permdir &#xA; &lt;dir&gt;&#xA;   dir to store failing tests -c &#xA;  &lt;file&gt;&#xA;    | --csim &#xA;   &lt;file&gt;&#xA;     C simulator -r &#xA;    &lt;file&gt;&#xA;      | --rtlsim &#xA;     &lt;file&gt;&#xA;       RTL simulator -e &#xA;      &lt;address&gt; | --email &lt;address&gt; email to report to -t &#xA;        &lt;count&gt;&#xA;          | --threshold &#xA;         &lt;count&gt;&#xA;           number of failures to trigger email -m &#xA;          &lt;int&gt;&#xA;            | --minutes &#xA;           &lt;int&gt;&#xA;             number of minutes to run tests&#xA;            &lt;p&gt;&lt;/p&gt; &#xA;            &lt;p&gt;You can only generate tests with one instruction mix at a time (based on the setting in the config file). It doesn&#39;t matter what simulator you use with the -r and -c flags, they just determines the name used to describe whose diff failed.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Testrun Overview&lt;/h2&gt; &#xA;            &lt;p&gt;This utility compares the signatures generated by passing the -testsig flag to the specified simulators. If it encounters a difference, it subdivides the test into many subtests and searches for which exact program segment reveals the failure. It takes the following command line arguments:&lt;/p&gt; &#xA;            &lt;p&gt;Usage: testrun/run [options] -C &#xA;             &lt;file&gt;&#xA;               | --config &#xA;              &lt;file&gt;&#xA;                config file -a &#xA;               &lt;file&gt;&#xA;                 | --asm &#xA;                &lt;file&gt;&#xA;                  input ASM file -c &#xA;                 &lt;file&gt;&#xA;                   | --csim &#xA;                  &lt;file&gt;&#xA;                    C simulator -r &#xA;                   &lt;file&gt;&#xA;                     | --rtlsim &#xA;                    &lt;file&gt;&#xA;                      RTL simulator -s &#xA;                     &lt;boolean&gt;&#xA;                       | --seek &#xA;                      &lt;boolean&gt;&#xA;                        Seek for failing pseg -d &#xA;                       &lt;boolean&gt;&#xA;                         | --dump &#xA;                        &lt;boolean&gt;&#xA;                          Dump mismatched signatures&#xA;                        &lt;/boolean&gt;&#xA;                       &lt;/boolean&gt;&#xA;                      &lt;/boolean&gt;&#xA;                     &lt;/boolean&gt;&#xA;                    &lt;/file&gt;&#xA;                   &lt;/file&gt;&#xA;                  &lt;/file&gt;&#xA;                 &lt;/file&gt;&#xA;                &lt;/file&gt;&#xA;               &lt;/file&gt;&#xA;              &lt;/file&gt;&#xA;             &lt;/file&gt;&lt;/p&gt; &#xA;            &lt;p&gt;If you don&#39;t specify a asm file, a random one will be generated for you. You can only generate tests with one instruction mix at a time (based on the setting in the config file). It doesn&#39;t matter what simulator you use with the -r and -c flags, they just determines the name used to describe whose diff failed. By default, a failed diff will result in the subtest sweep occuring, but this search can be diasbled. Note that the pseg ID reported is actually the pseg following the pseg containing the error. You can optionally dump mistmatched signatures to the dir containing the asm file under test.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Generator Overview&lt;/h2&gt; &#xA;            &lt;p&gt;To generate a random test, the torture test generator randomly generates many test sequences from a set of test sequences that are written by hand, performs a random register allocation for all test sequences, and finally randomly interleaves instructions from these test sequences. To extend the set of tests or coverage, the programmer needs to write new test sequences. It takes the following command line arguments:&lt;/p&gt; &#xA;            &lt;p&gt;Usage: generator/run [options] -o &#xA;             &lt;filename&gt;&#xA;               | --output &#xA;              &lt;filename&gt;&#xA;                output filename -C &#xA;               &lt;file&gt;&#xA;                 | --config &#xA;                &lt;file&gt;&#xA;                  config file&#xA;                &lt;/file&gt;&#xA;               &lt;/file&gt;&#xA;              &lt;/filename&gt;&#xA;             &lt;/filename&gt;&lt;/p&gt; &#xA;            &lt;p&gt;The following sections describe adding new functionality to the generator.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Test sequence example&lt;/h2&gt; &#xA;            &lt;p&gt;Before we talk about how to write a test sequence, let&#39;s look at a very simple example. The following example is a test sequence, which emits an add instruction.&lt;/p&gt; &#xA;            &lt;p&gt;class SeqADD extends Seq { val src1 = reg_read_any() val src2 = reg_read_any() val dest = reg_write(src1, src2) insts += ADD(dest, src1, src2) }&lt;/p&gt; &#xA;            &lt;p&gt;As I hinted in the overview that the test generator will do register allocation you don&#39;t write a string of instructions with architectural registers. You request a virtual registers (i.e., registers that are yet tied down to architectural registers) when you need them, save them in scala values, and use them when you need to (e.g., in an instruction).&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Types of virtual registers&lt;/h2&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt; &lt;p&gt;Hidden (position dependent registers): Registers that will have different values when the code is positioned at a different address. An example is registers that hold addresses. Registers that are hidden should be excluded from the output signature.&lt;/p&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;Visible (position independent registers): Registers that are not hidden, therefore will have the same values when the code is positioned at a different address. These registers should be included as part of the output signature.&lt;/p&gt; &lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;How to write a sequence&lt;/h2&gt; &#xA;            &lt;p&gt;Use the following functions to request a register, and generate a string of instructions (look at Inst.scala to see what instructions are available) that uses these virtual registers, and add them to the insts array.&lt;/p&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt;reg_read_zero(): returns register x0&lt;/li&gt; &#xA;             &lt;li&gt;reg_read_any(): returns any type of register (hidden or visible)&lt;/li&gt; &#xA;             &lt;li&gt;reg_read_visible(): returns a visible register&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_ra(): returns register ra for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_visible(): returns a visible register for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_hidden(): returns a hidden register for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write(regs: Reg*): returns a register that matches the type of regs (if any reg in regs are hidden, the output type is hidden)&lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;            &lt;p&gt;Note that the torture test framework is written in scala, you can use any scala functionality to generate instructions. Look at SeqALU.scala, SeqMem.scala, and SeqBranch.scala to get inspired.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Future TODO&lt;/h2&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt; &lt;p&gt;provide support for loops&lt;/p&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;generate statistics of a test to get a sense of coverage&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;statistics should include instruction count of each type&lt;/li&gt; &#xA;               &lt;li&gt;statistics should include register usage&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;complete floating point tests&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;add floating point memory move tests&lt;/li&gt; &#xA;               &lt;li&gt;improve floating point init randomization&lt;/li&gt; &#xA;               &lt;li&gt;add rounding modes tests&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;complete vector tests&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;better randomization&lt;/li&gt; &#xA;               &lt;li&gt;add SeqVOnly: Tests special vf-only instructions&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;code refactoring&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;consolidate RegPool logic&lt;/li&gt; &#xA;               &lt;li&gt;detect and suppress unallocatable sequences&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;           &lt;/int&gt;&#xA;          &lt;/int&gt;&#xA;         &lt;/count&gt;&#xA;        &lt;/count&gt;&lt;/address&gt;&lt;/address&gt;&#xA;     &lt;/file&gt;&#xA;    &lt;/file&gt;&#xA;   &lt;/file&gt;&#xA;  &lt;/file&gt;&#xA; &lt;/dir&gt;&#xA;&lt;/dir&gt;</summary>
  </entry>
  <entry>
    <title>JohnSnowLabs/spark-nlp</title>
    <updated>2022-05-31T01:52:29Z</updated>
    <id>tag:github.com,2022-05-31:/JohnSnowLabs/spark-nlp</id>
    <link href="https://github.com/JohnSnowLabs/spark-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State of the Art Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark NLP: State of the Art Natural Language Processing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/actions&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/JohnSnowLabs/spark-nlp/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34; alt=&#34;Current Release Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JohnSnowLabs/spark-nlp.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.12&#34; alt=&#34;Maven Central&#34;&gt; &lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.12/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/spark-nlp&#34; alt=&#34;PyPI version&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/spark-nlp.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34; alt=&#34;Anaconda-Cloud&#34;&gt; &lt;img src=&#34;https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/LICENSE&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34; alt=&#34;PyPi downloads&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/spark-nlp?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;performant&lt;/strong&gt; &amp;amp; &lt;strong&gt;accurate&lt;/strong&gt; NLP annotations for machine learning pipelines that &lt;strong&gt;scale&lt;/strong&gt; easily in a distributed environment. Spark NLP comes with &lt;strong&gt;4000+&lt;/strong&gt; pretrained &lt;strong&gt;pipelines&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt; in more than &lt;strong&gt;200+&lt;/strong&gt; languages. It also offers tasks such as &lt;strong&gt;Tokenization&lt;/strong&gt;, &lt;strong&gt;Word Segmentation&lt;/strong&gt;, &lt;strong&gt;Part-of-Speech Tagging&lt;/strong&gt;, Word and Sentence &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Named Entity Recognition&lt;/strong&gt;, &lt;strong&gt;Dependency Parsing&lt;/strong&gt;, &lt;strong&gt;Spell Checking&lt;/strong&gt;, &lt;strong&gt;Text Classification&lt;/strong&gt;, &lt;strong&gt;Sentiment Analysis&lt;/strong&gt;, &lt;strong&gt;Token Classification&lt;/strong&gt;, &lt;strong&gt;Machine Translation&lt;/strong&gt; (+180 languages), &lt;strong&gt;Summarization&lt;/strong&gt; &amp;amp; &lt;strong&gt;Question Answering&lt;/strong&gt;, &lt;strong&gt;Text Generation&lt;/strong&gt;, and many more &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;NLP tasks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark NLP&lt;/strong&gt; is the only open-source NLP library in &lt;strong&gt;production&lt;/strong&gt; that offers state-of-the-art transformers such as &lt;strong&gt;BERT&lt;/strong&gt;, &lt;strong&gt;ALBERT&lt;/strong&gt;, &lt;strong&gt;ELECTRA&lt;/strong&gt;, &lt;strong&gt;XLNet&lt;/strong&gt;, &lt;strong&gt;DistilBERT&lt;/strong&gt;, &lt;strong&gt;RoBERTa&lt;/strong&gt;, &lt;strong&gt;DeBERTa&lt;/strong&gt;, &lt;strong&gt;XLM-RoBERTa&lt;/strong&gt;, &lt;strong&gt;Longformer&lt;/strong&gt;, &lt;strong&gt;ELMO&lt;/strong&gt;, &lt;strong&gt;Universal Sentence Encoder&lt;/strong&gt;, &lt;strong&gt;Google T5&lt;/strong&gt;, &lt;strong&gt;MarianMT&lt;/strong&gt;, and &lt;strong&gt;GPT2&lt;/strong&gt; not only to &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt;, but also to &lt;strong&gt;JVM&lt;/strong&gt; ecosystem (&lt;strong&gt;Java&lt;/strong&gt;, &lt;strong&gt;Scala&lt;/strong&gt;, and &lt;strong&gt;Kotlin&lt;/strong&gt;) at &lt;strong&gt;scale&lt;/strong&gt; by extending &lt;strong&gt;Apache Spark&lt;/strong&gt; natively.&lt;/p&gt; &#xA;&lt;h2&gt;Project&#39;s website&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href=&#34;http://nlp.johnsnowlabs.com/&#34;&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt; &#xA;&lt;h2&gt;Community support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.johnsnowlabs.com/slack-redirect/&#34;&gt;Slack&lt;/a&gt; For live discussion with the Spark NLP community and the team&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp&#34;&gt;GitHub&lt;/a&gt; Bug reports, feature requests, and contributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/discussions&#34;&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/spark-nlp&#34;&gt;Medium&lt;/a&gt; Spark NLP articles&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos&#34;&gt;YouTube&lt;/a&gt; Spark NLP video tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-spark-support&#34;&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala-and-python-support&#34;&gt;Scala &amp;amp; Python Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-support&#34;&gt;Databricks Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-support&#34;&gt;EMR Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#usage&#34;&gt;Using Spark NLP&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-packages&#34;&gt;Spark Packages&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala&#34;&gt;Scala&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#maven&#34;&gt;Maven&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python&#34;&gt;Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipconda&#34;&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-zeppelin&#34;&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#jupyter-notebook-python&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#google-colab-notebook&#34;&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#kaggle-kernel&#34;&gt;Kaggle Kernel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-cluster&#34;&gt;Databricks Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-cluster&#34;&gt;EMR Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#gcp-dataproc&#34;&gt;GCP Dataproc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-nlp-configuration&#34;&gt;Spark NLP Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines-and-models&#34;&gt;Pipelines &amp;amp; Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#offline&#34;&gt;Offline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tokenization&lt;/li&gt; &#xA; &lt;li&gt;Trainable Word Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Stop Words Removal&lt;/li&gt; &#xA; &lt;li&gt;Token Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Document Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Stemmer&lt;/li&gt; &#xA; &lt;li&gt;Lemmatizer&lt;/li&gt; &#xA; &lt;li&gt;NGrams&lt;/li&gt; &#xA; &lt;li&gt;Regex Matching&lt;/li&gt; &#xA; &lt;li&gt;Text Matching&lt;/li&gt; &#xA; &lt;li&gt;Chunking&lt;/li&gt; &#xA; &lt;li&gt;Date Matcher&lt;/li&gt; &#xA; &lt;li&gt;Sentence Detector&lt;/li&gt; &#xA; &lt;li&gt;Deep Sentence Detector (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Dependency parsing (Labeled/unlabeled)&lt;/li&gt; &#xA; &lt;li&gt;Part-of-speech tagging&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Detection (ML models)&lt;/li&gt; &#xA; &lt;li&gt;Spell Checker (ML and DL models)&lt;/li&gt; &#xA; &lt;li&gt;Word Embeddings (GloVe and Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;Doc2Vec (based on Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;BERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;CamemBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa Embeddings (HuggingFace v2 &amp;amp; v3 models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Longformer Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;ALBERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLNet Embeddings&lt;/li&gt; &#xA; &lt;li&gt;ELMO Embeddings (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;Universal Sentence Encoder (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;BERT Sentence Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Sentence Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Chunk Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised keywords extraction&lt;/li&gt; &#xA; &lt;li&gt;Language Detection &amp;amp; Identification (up to 375 languages)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-label Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Text Classification (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;BERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;ALBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLNet for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Longformer for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation (MarianMT)&lt;/li&gt; &#xA; &lt;li&gt;Text-To-Text Transfer Transformer (Google T5)&lt;/li&gt; &#xA; &lt;li&gt;Generative Pre-trained Transformer 2 (OpenAI GPT2)&lt;/li&gt; &#xA; &lt;li&gt;Named entity recognition (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Easy TensorFlow integration&lt;/li&gt; &#xA; &lt;li&gt;GPU Support&lt;/li&gt; &#xA; &lt;li&gt;Full integration with Spark ML functions&lt;/li&gt; &#xA; &lt;li&gt;+3200 pre-trained models in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;+1700 pre-trained pipelines in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To use Spark NLP you need the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8 and 11&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3.2.x, 3.1.x, 3.0.x, 2.4.x, or 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Java 11 is only supported if you are using Spark NLP with Spark/PySpark 3.x and above&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU (optional):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA11 and cuDNN 8.0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ java -version&#xA;# should be Java 8 (Oracle or OpenJDK)&#xA;$ conda create -n sparknlp python=3.7 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python console or Jupyter &lt;code&gt;Python3&lt;/code&gt; kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Spark NLP&#xA;from sparknlp.base import *&#xA;from sparknlp.annotator import *&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;import sparknlp&#xA;&#xA;# Start SparkSession with Spark NLP&#xA;# start() functions has 5 parameters: gpu, spark23, spark24, spark32, and memory&#xA;# sparknlp.start(gpu=True) will start the session with GPU support&#xA;# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed&#xA;# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed&#xA;# sparknlp.start(spark32=True) is when you have Apache Spark 3.2.x installed&#xA;# sparknlp.start(memory=&#34;16G&#34;) to change the default driver memory in SparkSession&#xA;spark = sparknlp.start()&#xA;&#xA;# Download a pre-trained pipeline&#xA;pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;&#xA;# Your testing dataset&#xA;text = &#34;&#34;&#34;&#xA;The Mona Lisa is a 16th century oil painting created by Leonardo.&#xA;It&#39;s held at the Louvre in Paris.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Annotate your testing dataset&#xA;result = pipeline.annotate(text)&#xA;&#xA;# What&#39;s in the pipeline&#xA;list(result.keys())&#xA;Output: [&#39;entities&#39;, &#39;stem&#39;, &#39;checked&#39;, &#39;lemma&#39;, &#39;document&#39;,&#xA;&#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;]&#xA;&#xA;# Check the results&#xA;result[&#39;entities&#39;]&#xA;Output: [&#39;Mona Lisa&#39;, &#39;Leonardo&#39;, &#39;Louvre&#39;, &#39;Paris&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, you can visit our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;repository&lt;/a&gt; to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP &lt;em&gt;3.4.4&lt;/em&gt; has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x, 2.4.x, 3.0.x, 3.1.x, and 3.2.x:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.3.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.4.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.0.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.1.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.2.x&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting 3.0.0 release, the default &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;spark-nlp-gpu&lt;/code&gt; pacakges are based on Scala 2.12 and Apache Spark 3.x by default.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Scala and Python Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.6&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.8&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.9&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Databricks Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following runtimes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5.5 LTS&lt;/li&gt; &#xA; &lt;li&gt;5.5 LTS ML&lt;/li&gt; &#xA; &lt;li&gt;6.4&lt;/li&gt; &#xA; &lt;li&gt;6.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.3&lt;/li&gt; &#xA; &lt;li&gt;7.3 ML&lt;/li&gt; &#xA; &lt;li&gt;7.4&lt;/li&gt; &#xA; &lt;li&gt;7.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.5&lt;/li&gt; &#xA; &lt;li&gt;7.5 ML&lt;/li&gt; &#xA; &lt;li&gt;7.6&lt;/li&gt; &#xA; &lt;li&gt;7.6 ML&lt;/li&gt; &#xA; &lt;li&gt;8.0&lt;/li&gt; &#xA; &lt;li&gt;8.0 ML&lt;/li&gt; &#xA; &lt;li&gt;8.1&lt;/li&gt; &#xA; &lt;li&gt;8.1 ML&lt;/li&gt; &#xA; &lt;li&gt;8.2&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML&lt;/li&gt; &#xA; &lt;li&gt;8.3&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML&lt;/li&gt; &#xA; &lt;li&gt;8.4&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML&lt;/li&gt; &#xA; &lt;li&gt;9.0&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML&lt;/li&gt; &#xA; &lt;li&gt;9.1&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.0&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML&lt;/li&gt; &#xA; &lt;li&gt;10.1&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.2&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML&lt;/li&gt; &#xA; &lt;li&gt;10.3&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML &amp;amp; GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark NLP 3.4.4 is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 8.x and above as listed under GPU.&lt;/p&gt; &#xA;&lt;h2&gt;EMR Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following EMR releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emr-5.20.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.22.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.23.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.25.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.26.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.27.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.28.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.29.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.31.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.32.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.34.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.1.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.2.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.1&lt;/li&gt; &#xA; &lt;li&gt;emr-6.4.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.5.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html&#34;&gt;Amazon EMR 5.x releases&lt;/a&gt; Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html&#34;&gt;Amazon EMR 6.x releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.4.4&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Packages Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;This is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Apache Spark&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on CPU&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on GPU&lt;/th&gt; &#xA;   &lt;th&gt;Start()&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x/3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark32=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark24=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark23=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Spark Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Command line (requires internet connection)&lt;/h3&gt; &#xA;&lt;p&gt;Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, Apache Spark 3.1.x, and Apache Spark 3.2.x. That&#39;s being said, you need to choose the right package name for the right Apache Spark major release:&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.2.x (Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.4.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.3.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x, 3.1.x, and 3.2.x versions. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Maven&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark24_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maven Central: &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects &lt;a href=&#34;https://github.com/maziyarpanahi/spark-nlp-starter&#34;&gt;Spark NLP SBT Starter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Python 3.6.x and above depending on your major PySpark version.&lt;/p&gt; &#xA;&lt;h3&gt;Python without explicit Pyspark installation&lt;/h3&gt; &#xA;&lt;h3&gt;Pip/Conda&lt;/h3&gt; &#xA;&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt; &#xA;&lt;p&gt;Pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34;&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34;&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you&#39;ll have to create a SparkSession either from Spark NLP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;&#xA;spark = sparknlp.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[4]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for comma-delimited jar files. For cluster setups, of course, you&#39;ll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;&#xA;#create or get Spark Session&#xA;&#xA;spark = sparknlp.start()&#xA;&#xA;sparknlp.version()&#xA;spark.version&#xA;&#xA;#download, load and annotate a text by pre-trained pipeline&#xA;&#xA;pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;)&#xA;result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiled JARs&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;h4&gt;spark-nlp&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the jar manually&lt;/h3&gt; &#xA;&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;Maven Central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell --jars spark-nlp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Zeppelin&lt;/h2&gt; &#xA;&lt;p&gt;Use either one of the following options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the following Maven Coordinates to the interpreter&#39;s library list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a path to pre-built jar from &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;here&lt;/a&gt; in the interpreter&#39;s library list making sure the jar is available to driver path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Python in Zeppelin&lt;/h3&gt; &#xA;&lt;p&gt;Apart from the previous step, install the python module through pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt; &#xA;&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier since it includes both scala and python side installation.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook (Python)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recomended:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get this done on Linux and macOS is to simply install &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;pyspark&lt;/code&gt; PyPI packages and launch the Jupyter from the same Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda create -n sparknlp python=3.8 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2 jupyter&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The you can use &lt;code&gt;python3&lt;/code&gt; kernel to run your code with creating SparkSession via &lt;code&gt;spark = sparknlp.start()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_HOME=/path/to/your/spark/folder&#xA;export PYSPARK_PYTHON=python3&#xA;export PYSPARK_DRIVER_PYTHON=jupyter&#xA;export PYSPARK_DRIVER_PYTHON_OPTS=notebook&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If not using pyspark at all, you&#39;ll have to run the instructions pointed &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python-without-explicit-Pyspark-installation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# This is only to setup PySpark and Spark NLP on Colab&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script comes with the two options to define &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-nlp&lt;/code&gt; versions via options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# -p is for pyspark&#xA;# -s is for spark-nlp&#xA;# by default they are set to the latest&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.1.2 -s 3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb&#34;&gt;Spark NLP quick start on Google Colab&lt;/a&gt; is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Kaggle Kernel&lt;/h2&gt; &#xA;&lt;p&gt;Run the following code in Kaggle Kernel and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Let&#39;s setup Kaggle for Spark NLP and PySpark&#xA;!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mozzie/spark-nlp-named-entity-recognition&#34;&gt;Spark NLP quick start on Kaggle Kernel&lt;/a&gt; is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Databricks Cluster&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a cluster if you don&#39;t have one already&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;Libraries&lt;/code&gt; tab inside your cluster you need to follow these steps:&lt;/p&gt; &lt;p&gt;3.1. Install New -&amp;gt; PyPI -&amp;gt; &lt;code&gt;spark-nlp==3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;p&gt;3.2. Install New -&amp;gt; Maven -&amp;gt; Coordinates -&amp;gt; &lt;code&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can attach your notebook to the cluster and use Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: Databrick&#39;s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name for your runtime from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EMR Cluster&lt;/h2&gt; &#xA;&lt;p&gt;To launch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.&lt;/p&gt; &#xA;&lt;p&gt;A sample of your bootstrap script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;#!/bin/bash&#xA;set -x -e&#xA;&#xA;echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3&#xA;export HADOOP_CONF_DIR=/etc/hadoop/conf&#xA;export SPARK_JARS_DIR=/usr/lib/spark/jars&#xA;export SPARK_HOME=/usr/lib/spark&#39; &amp;gt;&amp;gt; $HOME/.bashrc &amp;amp;&amp;amp; source $HOME/.bashrc&#xA;&#xA;sudo python3 -m pip install awscli boto spark-nlp&#xA;&#xA;set +x&#xA;exit 0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of your software configuration in JSON on S3 (must be public access):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.json&#34;&gt;[{&#xA;  &#34;Classification&#34;: &#34;spark-env&#34;,&#xA;  &#34;Configurations&#34;: [{&#xA;    &#34;Classification&#34;: &#34;export&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;PYSPARK_PYTHON&#34;: &#34;/usr/bin/python3&#34;&#xA;    }&#xA;  }]&#xA;},&#xA;{&#xA;  &#34;Classification&#34;: &#34;spark-defaults&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;spark.yarn.stagingDir&#34;: &#34;hdfs:///tmp&#34;,&#xA;      &#34;spark.yarn.preserve.staging.files&#34;: &#34;true&#34;,&#xA;      &#34;spark.kryoserializer.buffer.max&#34;: &#34;2000M&#34;,&#xA;      &#34;spark.serializer&#34;: &#34;org.apache.spark.serializer.KryoSerializer&#34;,&#xA;      &#34;spark.driver.maxResultSize&#34;: &#34;0&#34;,&#xA;      &#34;spark.jars.packages&#34;: &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of AWS CLI to launch EMR cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;aws emr create-cluster \&#xA;--name &#34;Spark NLP 3.4.4&#34; \&#xA;--release-label emr-6.2.0 \&#xA;--applications Name=Hadoop Name=Spark Name=Hive \&#xA;--instance-type m4.4xlarge \&#xA;--instance-count 3 \&#xA;--use-default-roles \&#xA;--log-uri &#34;s3://&amp;lt;S3_BUCKET&amp;gt;/&#34; \&#xA;--bootstrap-actions Path=s3://&amp;lt;S3_BUCKET&amp;gt;/emr-bootstrap.sh,Name=custome \&#xA;--configurations &#34;https://&amp;lt;public_access&amp;gt;/sparknlp-config.json&#34; \&#xA;--ec2-attributes KeyName=&amp;lt;your_ssh_key&amp;gt;,EmrManagedMasterSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt;,EmrManagedSlaveSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt; \&#xA;--profile &amp;lt;aws_profile_credentials&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP Dataproc&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a cluster if you don&#39;t have one already as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At gcloud shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud services enable dataproc.googleapis.com \&#xA;  compute.googleapis.com \&#xA;  storage-component.googleapis.com \&#xA;  bigquery.googleapis.com \&#xA;  bigquerystorage.googleapis.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;ZONE=&amp;lt;zone&amp;gt;&#xA;CLUSTER_NAME=&amp;lt;cluster_name&amp;gt;&#xA;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don&#39;t forget to set the maven coordinates for the jar in properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud dataproc clusters create ${CLUSTER_NAME} \&#xA;  --region=${REGION} \&#xA;  --zone=${ZONE} \&#xA;  --image-version=2.0 \&#xA;  --master-machine-type=n1-standard-4 \&#xA;  --worker-machine-type=n1-standard-2 \&#xA;  --master-boot-disk-size=128GB \&#xA;  --worker-boot-disk-size=128GB \&#xA;  --num-workers=2 \&#xA;  --bucket=${BUCKET_NAME} \&#xA;  --optional-components=JUPYTER \&#xA;  --enable-component-gateway \&#xA;  --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; \&#xA;  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh \&#xA;  --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can attach your notebook to the cluster and use the Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Spark NLP Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can change the following Spark NLP configurations via Spark Configuration:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Property Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.pretrained.cache_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/cache_pretrained&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to download and exctract pretrained &lt;code&gt;Models&lt;/code&gt; and &lt;code&gt;Pipelines&lt;/code&gt;. By default, it will be in User&#39;s Home directory under &lt;code&gt;cache_pretrained&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.storage.cluster_tmp_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of &lt;code&gt;hadoop.tmp.dir&lt;/code&gt; set via Hadoop configuration for Apache Spark. NOTE: &lt;code&gt;S3&lt;/code&gt; is not supported and it must be local, HDFS, or DBFS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.annotator.log_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/annotator_logs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to save logs from annotators during training such as &lt;code&gt;NerDLApproach&lt;/code&gt;, &lt;code&gt;ClassifierDLApproach&lt;/code&gt;, &lt;code&gt;SentimentDLApproach&lt;/code&gt;, &lt;code&gt;MultiClassifierDLApproach&lt;/code&gt;, etc. By default, it will be in User&#39;s Home directory under &lt;code&gt;annotator_logs&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.access_key_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.secret_access_key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.session_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.s3_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.region&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How to set Spark NLP Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparkSession:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.config()&lt;/code&gt; during SparkSession creation to set Spark NLP configurations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;        .master(&#34;local[*]&#34;) \&#xA;        .config(&#34;spark.driver.memory&#34;, &#34;16G&#34;) \&#xA;        .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;        .config(&#34;spark.serializer&#34;, &#34;org.apache.spark.serializer.KryoSerializer&#34;) \&#xA;        .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000m&#34;) \&#xA;        .config(&#34;spark.jsl.settings.pretrained.cache_folder&#34;, &#34;sample_data/pretrained&#34;) \&#xA;        .config(&#34;spark.jsl.settings.storage.cluster_tmp_dir&#34;, &#34;sample_data/storage&#34;) \&#xA;        .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;) \&#xA;        .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-shell:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyspark:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pyspark \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE&#xA;spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE&#xA;spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it.&lt;/p&gt; &#xA;&lt;h3&gt;S3 Integration&lt;/h3&gt; &#xA;&lt;p&gt;In Spark NLP we can define S3 locations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Export log files of training models&lt;/li&gt; &#xA; &lt;li&gt;Store tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.annotator.log_folder&#34;, &#34;s3://my/s3/path/logs&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.s3_bucket&#34;, &#34;my.bucket&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can check the log on your S3 path defined in &lt;em&gt;spark.jsl.settings.annotator.log_folder&lt;/em&gt; property. Make sure to use the prefix &lt;em&gt;s3://&lt;/em&gt;, otherwise it will use the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensorflow Graphs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reference S3 location for downloading graphs. We need to set up AWS credentials&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;MFA Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.session_token&#34;, &#34;MY_TOKEN&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of a bash script that gets temporal AWS credentials can be found &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/scripts/aws_tmp_credentials.sh&#34;&gt;here&lt;/a&gt; This script requires three arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./aws_tmp_credentials.sh iam_user duration serial_number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipelines and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;&#xA;SparkNLP.version()&#xA;&#xA;val testData = spark.createDataFrame(Seq(&#xA;(1, &#34;Google has announced the release of a beta version of the popular TensorFlow machine learning library&#34;),&#xA;(2, &#34;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&#34;)&#xA;)).toDF(&#34;id&#34;, &#34;text&#34;)&#xA;&#xA;val pipeline = PretrainedPipeline(&#34;explain_document_dl&#34;, lang=&#34;en&#34;)&#xA;&#xA;val annotation = pipeline.transform(testData)&#xA;&#xA;annotation.show()&#xA;/*&#xA;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;2.5.0&#xA;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&#xA;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&#xA;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&#xA;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;*/&#xA;&#xA;annotation.select(&#34;entities.result&#34;).show(false)&#xA;&#xA;/*&#xA;+----------------------------------+&#xA;|result                            |&#xA;+----------------------------------+&#xA;|[Google, TensorFlow]              |&#xA;|[Donald John Trump, United States]|&#xA;+----------------------------------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all of the available Pipelines of a particular language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;)&#xA;/*&#xA;+--------------------------------------------+------+---------+&#xA;| Pipeline                                   | lang | version |&#xA;+--------------------------------------------+------+---------+&#xA;| dependency_parse                           |  en  | 2.0.2   |&#xA;| analyze_sentiment_ml                       |  en  | 2.0.2   |&#xA;| check_spelling                             |  en  | 2.1.0   |&#xA;| match_datetime                             |  en  | 2.1.0   |&#xA;                               ...&#xA;| explain_document_ml                        |  en  | 3.1.3   |&#xA;+--------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+---------------------------------------+------+---------+&#xA;| Pipeline                              | lang | version |&#xA;+---------------------------------------+------+---------+&#xA;| dependency_parse                      |  en  | 2.0.2   |&#xA;                               ...&#xA;| clean_slang                           |  en  | 3.0.0   |&#xA;| clean_pattern                         |  en  | 3.0.0   |&#xA;| check_spelling                        |  en  | 3.0.0   |&#xA;| dependency_parse                      |  en  | 3.0.0   |&#xA;+---------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained pipelines&lt;/a&gt; with examples, demos, benchmarks, and more&lt;/h4&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some selected languages:&lt;/strong&gt; &lt;code&gt;Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load NER model trained by deep learning approach and GloVe word embeddings&#xA;ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;)&#xA;# load NER model trained by deep learning approach and BERT word embeddings&#xA;ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// load French POS tagger model trained by Universal Dependencies&#xA;val french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;// load Italain LemmatizerModel&#xA;val italian_lemma = LemmatizerModel.pretrained(&#34;lemma_dxc&#34;, lang=&#34;it&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Models&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;)&#xA;/*&#xA;+---------------------------------------------+------+---------+&#xA;| Model                                       | lang | version |&#xA;+---------------------------------------------+------+---------+&#xA;| onto_100                                    |  en  | 2.1.0   |&#xA;| onto_300                                    |  en  | 2.1.0   |&#xA;| ner_dl_bert                                 |  en  | 2.2.0   |&#xA;| onto_100                                    |  en  | 2.4.0   |&#xA;| ner_conll_elmo                              |  en  | 3.2.2   |&#xA;+---------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+----------------------------+------+---------+&#xA;| Model                      | lang | version |&#xA;+----------------------------+------+---------+&#xA;| onto_100                   |  en  | 2.1.0   |&#xA;| ner_aspect_based_sentiment |  en  | 2.6.2   |&#xA;| ner_weibo_glove_840B_300d  |  en  | 2.6.2   |&#xA;| nerdl_atis_840b_300d       |  en  | 2.7.1   |&#xA;| nerdl_snips_100d           |  en  | 2.7.3   |&#xA;+----------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to see a list of available annotators, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showAvailableAnnotators()&#xA;/*&#xA;AlbertEmbeddings&#xA;AlbertForTokenClassification&#xA;AssertionDLModel&#xA;...&#xA;XlmRoBertaSentenceEmbeddings&#xA;XlnetEmbeddings&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained models&lt;/a&gt; with examples, demo, benchmark, and more&lt;/h4&gt; &#xA;&lt;h2&gt;Offline&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of using the Maven package, you need to load our Fat JAR&lt;/li&gt; &#xA; &lt;li&gt;Instead of using PretrainedPipeline for pretrained pipelines or the &lt;code&gt;.pretrained()&lt;/code&gt; function to download pretrained models, you will need to manually download your pipeline/model from &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;Models Hub&lt;/a&gt;, extract it, and load it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of &lt;code&gt;SparkSession&lt;/code&gt; with Fat JAR to have Spark NLP offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[*]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars&#34;, &#34;/tmp/spark-nlp-assembly-3.4.4.jar&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download provided Fat JARs from each &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/spark-nlp-assembly-3.4.4.jar&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of using pretrained Models and Pipelines in offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# instead of using pretrained() for online:&#xA;# french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;# you download this model, extract it, and use .load&#xA;french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)\&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)\&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&#xA;# example for pipelines&#xA;# instead of using PretrainedPipeline&#xA;# pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;# you download this pipeline, extract it, and use PipelineModel&#xA;PipelineModel.load(&#34;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Need more &lt;strong&gt;examples&lt;/strong&gt;? Check out our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;Spark NLP Showcase&lt;/a&gt; repository to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;p&gt;Also, don&#39;t forget to check &lt;a href=&#34;https://nlp.johnsnowlabs.com/demo&#34;&gt;Spark NLP in Action&lt;/a&gt; built by Streamlit.&lt;/p&gt; &#xA;&lt;h3&gt;All examples: &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;spark-nlp-workshop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nlp.johnsnowlabs.com/learn&#34;&gt;Check our Articles and Videos page here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;We have published a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2665963821000063&#34;&gt;paper&lt;/a&gt; that you can cite for the Spark NLP library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{KOCAMAN2021100058,&#xA;    title = {Spark NLP: Natural language understanding at scale},&#xA;    journal = {Software Impacts},&#xA;    pages = {100058},&#xA;    year = {2021},&#xA;    issn = {2665-9638},&#xA;    doi = {https://doi.org/10.1016/j.simpa.2021.100058},&#xA;    url = {https://www.sciencedirect.com/science/article/pii/S2665963.2.100063},&#xA;    author = {Veysel Kocaman and David Talby},&#xA;    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},&#xA;    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world’s most widely used NLP library in the enterprise.}&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ideas&lt;/li&gt; &#xA; &lt;li&gt;feedback&lt;/li&gt; &#xA; &lt;li&gt;documentation&lt;/li&gt; &#xA; &lt;li&gt;bug reports&lt;/li&gt; &#xA; &lt;li&gt;NLP training and testing corpora&lt;/li&gt; &#xA; &lt;li&gt;Development and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;John Snow Labs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://johnsnowlabs.com&#34;&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>