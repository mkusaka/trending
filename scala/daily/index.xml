<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-05T01:33:50Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cric96/advanced-reinforcement-learning-asmd-code</title>
    <updated>2024-05-05T01:33:50Z</updated>
    <id>tag:github.com,2024-05-05:/cric96/advanced-reinforcement-learning-asmd-code</id>
    <link href="https://github.com/cric96/advanced-reinforcement-learning-asmd-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction to Multi-Agent Reinforcement learning (MARL)&lt;/h1&gt; &#xA;&lt;h2&gt;Code for ASMD seminar&lt;/h2&gt; &#xA;&lt;p&gt;This repository shows some basic examples of multi-agent reinforcement learning. It contains: a) facades for the multi-agent environment and environment dynamics definition b) interfaces for agents definition c) implementation of Q learning and Deep Q learning The MARL topic is really broad and consists of several algorithms and environment dynamics. For the interested ones, I suggest reading the papers listed in this repository: &lt;a href=&#34;https://github.com/LantaoYu/MARL-Papers&#34;&gt;https://github.com/LantaoYu/MARL-Papers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also, consider to read the following book: &lt;a href=&#34;https://www.marl-book.com/&#34;&gt;https://www.marl-book.com/&lt;/a&gt; If you have any questions, please feel free to contact me at gianluca[dot]aguzzi[at]unibo[dot]it&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the project, you need to have sbt installed on your machine and python &amp;gt;= 3.8 Firs of all, after cloning the repository, you need to create a virtual environment and install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv env&#xA;source env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, you can install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the project, you need to compile the scala code and then run the python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt compile&#xA;sbt run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything is ok, typying 13 you should see the GYM example shown in class.&lt;/p&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram&#xA;&#xA;class Distribution~E~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def sample(): E&#xA;}&#xA;class StochasticGame~State,Action~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    + def agents: Int&#xA;    + def initialState: Distribution~State~&#xA;    + def transitionFunction: Function2~State, Seq~Action~, Action~&#xA;    + def rewardFunction: Function3~State, Seq~Action~, State~, Seq~Double~~&#xA;}&#xA;&#xA;class MultiAgentEnvironment~State,Action~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def act(actions: Seq~Action~): State&#xA;    def state(): State&#xA;    def reset(): Unit&#xA;}&#xA;&#xA;&#xA;class Agent~State, Action~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def act(state: State): Action&#xA;    def record(state: State, action: Action, reward: Double, nextState: State): Unit&#xA;    def reset(): Unit&#xA;}&#xA;&#xA;class Scheduler {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def episode: Int&#xA;    def step: Int&#xA;}&#xA;&#xA;class SchedulerListener {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def onEpisodeChange(episode: Int): Unit&#xA;    def onStepChange(episode: Int): Unit&#xA;}&#xA;&#xA;class Learner~State, Action~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    + def mode: Training | Test&#xA;    + def trainingMode(): Unit&#xA;    + def testMode(): Unit&#xA;    # def improve(state: State, action: Action, reward: Double, nextState: State): Unit&#xA;}&#xA;&#xA;class Simulation~State, Action~ {&#xA;    def simulate(episode: Int, episodeLength: Int, agents: Seq~Agent~State,Action~~): Unit&#xA;}&#xA;&#xA;class Render~State~ {&#xA;    &amp;lt;&amp;lt;trait&amp;gt;&amp;gt;&#xA;    def render(/state: State): Unit&#xA;}&#xA;&#xA;class DecayReference~V~ {&#xA;    # def update(): Unit&#xA;    def value: V&#xA;}&#xA;&#xA;&#xA;StochasticGame -- Distribution: &amp;lt;&amp;lt; uses &amp;gt;&amp;gt;&#xA;StochasticGame --&amp;gt; MultiAgentEnvironment: &amp;lt;&amp;lt; creates &amp;gt;&amp;gt;&#xA;Agent -- MultiAgentEnvironment: lives&#xA;Agent &amp;lt;|-- Learner&#xA;Learner &amp;lt;|-- QLearner&#xA;Learner &amp;lt;|-- DeepQLearner&#xA;Simulation o-- MultiAgentEnvironment&#xA;Simulation o-- Render&#xA;Simulation o-- Scheduler&#xA;Scheduler o-- SchedulerListener: listen&#xA;SchedulerListener -- DecayReference: &amp;lt;&amp;lt; uses &amp;gt;&amp;gt;&#xA;QLearner -- DecayReference: &amp;lt;&amp;lt; uses &amp;gt;&amp;gt;&#xA;DeepQLearner -- DecayReference: &amp;lt;&amp;lt; uses &amp;gt;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A &lt;code&gt;StochasticGame&lt;/code&gt; extends an MDP (Markov Decision Process) to multi-agent systems, offering a framework for understanding the dynamics of a &lt;code&gt;MultiAgentEnvironment&lt;/code&gt;. More details on Stochastic Games can be found &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_game&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;code&gt;MultiAgentEnvironment&lt;/code&gt; involves multiple &lt;code&gt;Agents&lt;/code&gt; interacting within an environment and perceiving its &lt;code&gt;State&lt;/code&gt;. If an &lt;code&gt;Agent&lt;/code&gt; can improve itself through experience, it qualifies as a &lt;code&gt;Learner&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To simulate a &lt;code&gt;MultiAgentEnvironment&lt;/code&gt;, a &lt;code&gt;Simulation&lt;/code&gt; is utilized. It conducts multiple runs, each defined by an &lt;code&gt;episodeLength&lt;/code&gt;, using the specified agents.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;In this repository, I demonstrate two examples using the general structure outlined above:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Competitive Task, Rock Paper Scissors&lt;/strong&gt;: Two agents engage in the game of rock paper scissors. More details on this scenario are available &lt;a href=&#34;https://direct.mit.edu/isal/proceedings/alife2018/30/404/99610&#34;&gt;here&lt;/a&gt;. I explore outcomes when both agents employ the same learning algorithm and scenarios where one agent has only partial observation of the environment. While a deeper discussion on equilibria and multi-agent dynamics would be beneficial, these topics are beyond the scope of this lesson. For further reading, consider the PhD thesis: &lt;a href=&#34;https://discovery.ucl.ac.uk/id/eprint/10124273/&#34;&gt;Many-agent Reinforcement Learning&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cooperative Task, Agent Alignment&lt;/strong&gt;: Here, &lt;code&gt;N&lt;/code&gt; agents aim to align themselves either in a row or a column. This setup is inherently cooperative, as agents must work together to achieve the correct positioning. I discuss several common frameworks in cooperative tasks, including independent learners, team learning with a shared Q-table, and centralized learning. This serves as an introduction to &#34;infinite&#34; agent learning.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Exercises&lt;/h3&gt; &#xA;&lt;p&gt;After running the examples, try to extend the codebase with one of the following exercise:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;try to incorporate &lt;a href=&#34;https://pettingzoo.farama.org/index.html&#34;&gt;Petting Zoo&lt;/a&gt; environments into the simulation and verify the results using Deep Q learning (shared, independent, centralised)&lt;/li&gt; &#xA; &lt;li&gt;try to create another environment that simulate a flock of drones that need to stay connected. Try to: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;understand how can you create the right reward function to create a flock (cohesion + separation)&lt;/li&gt; &#xA;   &lt;li&gt;discretize the action space (i.e., 8 directions)&lt;/li&gt; &#xA;   &lt;li&gt;understand the right observation space (i.e., the position of the drones? The position of the N nearest drones?)&lt;/li&gt; &#xA;   &lt;li&gt;implement the environment and the agents&lt;/li&gt; &#xA;   &lt;li&gt;verify if the agents are able to learn the right policy, i.e., stay connected and create a flock&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>