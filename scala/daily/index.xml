<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-19T01:45:07Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>databricks/sjsonnet</title>
    <updated>2023-05-19T01:45:07Z</updated>
    <id>tag:github.com,2023-05-19:/databricks/sjsonnet</id>
    <link href="https://github.com/databricks/sjsonnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sjsonnet&lt;/h1&gt; &#xA;&lt;p&gt;A JVM implementation of the &lt;a href=&#34;https://jsonnet.org/&#34;&gt;Jsonnet&lt;/a&gt; configuration language.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet can be used from Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.databricks&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sjsonnet_2.13&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    new String[]{&#34;foo.jsonnet&#34;},&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.package$.MODULE$.pwd(),&#xA;    scala.None$.empty()&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#34;com.databricks&#34; %% &#34;sjsonnet&#34; % &#34;0.4.2&#34; // SBT&#xA;ivy&#34;com.databricks::sjsonnet:0.4.2&#34; // Mill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    Array(&#34;foo.jsonnet&#34;),&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.pwd, // working directory&#xA;    None&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a standalone executable assembly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&#34;&gt;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar &amp;gt; sjsonnet.jar&#xA;&#xA;$ chmod +x sjsonnet.jar&#xA;&#xA;$ ./sjsonnet.jar&#xA;error: Need to pass in a jsonnet file to evaluate&#xA;usage: sjsonnet [sjsonnet-options] script-file&#xA;&#xA;  -i, --interactive  Run Mill in interactive mode, suitable for opening REPLs and taking user input&#xA;  -n, --indent       How much to indent your output JSON&#xA;  -J, --jpath        Specify an additional library search dir (right-most wins)&#xA;  -o, --output-file  Write to the output file rather than stdout&#xA;  ...&#xA;&#xA;$ ./sjsonnet.jar foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or from Javascript:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.js &amp;gt; sjsonnet.js&#xA;&#xA;$ node&#xA;&#xA;&amp;gt; require(&#34;./sjsonnet.js&#34;)&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#34;local f = function(x) x * x; f(11)&#34;, {}, {}, &#34;&#34;, (wd, imported) =&amp;gt; null)&#xA;121&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#xA;    &#34;local f = import &#39;foo&#39;; f + &#39;bar&#39;&#34;, // code&#xA;    {}, // extVars&#xA;    {}, // tlaVars&#xA;    &#34;&#34;, // initial working directory&#xA;&#xA;    // import callback: receives a base directory and the imported path string,&#xA;    // returns a tuple of the resolved file path and file contents or file contents resolve method&#xA;    (wd, imported) =&amp;gt; [wd + &#34;/&#34; + imported, &#34;local bar = 123; bar + bar&#34;],&#xA;    // loader callback: receives the tuple from the import callback and returns the file contents&#xA;    ([path, content]) =&amp;gt; content&#xA;    )&#xA;&#39;246bar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that since Javascript does not necessarily have access to the filesystem, you have to provide an explicit import callback that you can use to resolve imports yourself (whether through Node&#39;s &lt;code&gt;fs&lt;/code&gt; module, or by emulating a filesystem in-memory)&lt;/p&gt; &#xA;&lt;h3&gt;Running deeply recursive Jsonnet programs&lt;/h3&gt; &#xA;&lt;p&gt;The depth of recursion is limited by JVM stack size. You can run Sjsonnet with increased stack size as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -Xss100m -cp sjsonnet.jar sjsonnet.SjsonnetMain foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -Xss option above is responsible for JVM stack size. Please try this if you ever run into &lt;code&gt;sjsonnet.Error: Internal Error ... Caused by: java.lang.StackOverflowError ...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is no analog of &lt;code&gt;--max-stack&lt;/code&gt;/&lt;code&gt;-s&lt;/code&gt; option of &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;. The only stack size limit is the one of the JVM.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet is implementated as an optimizing interpreter. There are roughly 4 phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Parser&lt;/code&gt;: parses an input &lt;code&gt;String&lt;/code&gt; into a &lt;code&gt;sjsonnet.Expr&lt;/code&gt;, which is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;Syntax Tree&lt;/a&gt; representing the Jsonnet document syntax, using the &lt;a href=&#34;https://github.com/lihaoyi/fastparse&#34;&gt;Fastparse&lt;/a&gt; parsing library&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.StaticOptimizer&lt;/code&gt; is a single AST transform that performs static checking, essential rewriting (e.g. assigning indices in the symbol table for variables) and optimizations. The result is another &lt;code&gt;sjsonnet.Expr&lt;/code&gt; per input file that can be stored in the parse cache and reused.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Evaluator&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Expr&lt;/code&gt; produced by the optimizer and converts it into a &lt;code&gt;sjsonnet.Val&lt;/code&gt;, a data structure representing the Jsonnet runtime values (basically lazy JSON which can contain function values).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Materializer&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Val&lt;/code&gt; and converts it into an output &lt;code&gt;ujson.Expr&lt;/code&gt;: a non-lazy JSON structure without any remaining un-evaluated function values. This can be serialized to a string formatted in a variety of ways&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These three phases are encapsulated in the &lt;code&gt;sjsonnet.Interpreter&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some notes on the values used in parts of the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Expr&lt;/code&gt;: this represents &lt;code&gt;{...}&lt;/code&gt; object literal nodes, &lt;code&gt;a + b&lt;/code&gt; binary operation nodes, &lt;code&gt;function(a) {...}&lt;/code&gt; definitions and &lt;code&gt;f(a)&lt;/code&gt; invocations, etc.. Also keeps track of source-offset information so failures can be correlated with line numbers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Val&lt;/code&gt;: essentially the JSON structure (objects, arrays, primitives) but with two modifications. The first is that functions like &lt;code&gt;function(a){...}&lt;/code&gt; can still be present in the structure: in Jsonnet you can pass around functions as values and call then later on. The second is that object values &amp;amp; array entries are &lt;em&gt;lazy&lt;/em&gt;: e.g. &lt;code&gt;[error 123, 456][1]&lt;/code&gt; does not raise an error because the first (erroneous) entry of the array is un-used and thus not evaluated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classes representing literals extend &lt;code&gt;sjsonnet.Val.Literal&lt;/code&gt; which in turn extends &lt;em&gt;both&lt;/em&gt;, &lt;code&gt;Expr&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt;. This allows the evaluator to skip over them instead of having to convert them from one representation to the other.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Due to pervasive caching, sjsonnet is much faster than google/jsonnet. See this blog post for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2018/10/12/writing-a-faster-jsonnet-compiler.html&#34;&gt;Writing a Faster Jsonnet Compiler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the latest set of benchmarks I&#39;ve run (as of 18 May 2023) comparing Sjsonnet against google/go-jsonnet and google/jsonnet, measuring the time taken to evaluate an arbitrary config file in the Databricks codebase:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.4.3&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/go-jsonnet 0.20.0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/jsonnet 0.20.0&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;staging/runbot-app.jsonnet (~6.6mb output JSON)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~0.10s&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~6.5s&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~67s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Sjsonnet was run as a long-lived daemon to keep the JVM warm, while go-jsonnet and google/jsonnet were run as subprocesses, following typical usage patterns. The Sjsonnet command line which is run by all of these is defined in &lt;code&gt;MainBenchmark.mainArgs&lt;/code&gt;. You need to change it to point to a suitable input before running a benchmark or the profiler.&lt;/p&gt; &#xA;&lt;p&gt;Benchmark example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/jmh:run -jvmArgs &#34;-XX:+UseStringDeduplication&#34; sjsonnet.MainBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profiler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Laziness&lt;/h2&gt; &#xA;&lt;p&gt;The Jsonnet language is &lt;em&gt;lazy&lt;/em&gt;: expressions don&#39;t get evaluated unless their value is needed, and thus even erroneous expressions do not cause a failure if un-used. This is represented in the Sjsonnet codebase by &lt;code&gt;sjsonnet.Lazy&lt;/code&gt;: a wrapper type that encapsulates an arbitrary computation that returns a &lt;code&gt;sjsonnet.Val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sjsonnet.Lazy&lt;/code&gt; is used in several places, representing where laziness is present in the language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Scope&lt;/code&gt;, representing local variable name bindings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Arr&lt;/code&gt;, representing the contents of array cells&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Obj&lt;/code&gt;, representing the contents of object values&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Val&lt;/code&gt; extends &lt;code&gt;Lazy&lt;/code&gt; so that an already computed value can be treated as lazy without having to wrap it.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet caches the results of lazy computations the first time they are evaluated, avoiding wasteful re-computation when a value is used more than once.&lt;/p&gt; &#xA;&lt;h2&gt;Standard Library&lt;/h2&gt; &#xA;&lt;p&gt;Different from &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet does not implement the Jsonnet standard library &lt;code&gt;std&lt;/code&gt; in Jsonnet code. Rather, those functions are implemented as intrinsics directly in the host language (in &lt;code&gt;Std.scala&lt;/code&gt;). This allows both better error messages when the input types are wrong, as well as better performance for the more computationally-intense builtin functions.&lt;/p&gt; &#xA;&lt;h2&gt;Client-Server&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet comes with a built in thin-client and background server, to help mitigate the unfortunate JVM warmup overhead that adds ~1s to every invocation down to 0.2-0.3s. For the simple non-client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the executable. For the client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].server.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the Sjsonnet background server lives in &lt;code&gt;~/.sjsonnet&lt;/code&gt;, and lasts 5 minutes before shutting itself when inactive.&lt;/p&gt; &#xA;&lt;p&gt;Since the Sjsonnet client still has 0.2-0.3s of overhead, if using Sjsonnet heavily it is still better to include it in your JVM classpath and invoke it programmatically via &lt;code&gt;new Interpreter(...).interpret(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing&lt;/h2&gt; &#xA;&lt;p&gt;To publish, make sure the version number in &lt;code&gt;build.sc&lt;/code&gt; is correct, then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill -i mill.scalalib.PublishModule/publishAll --sonatypeCreds lihaoyi:$SONATYPE_PASSWORD --publishArtifacts __.publishArtifacts --release true&#xA;&#xA;./mill -i show sjsonnet[2.13.4].js.fullOpt&#xA;./mill -i show sjsonnet[2.13.4].jvm.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;0.4.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update Mill to 0.10.12&lt;/li&gt; &#xA; &lt;li&gt;Fix parsing of k/v cli arguments with an &#34;=&#34; in the value&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make lazy initialization of static Val.Obj thread-safe &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/136&#34;&gt;#136&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deduplicate strings in the parser &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/137&#34;&gt;#137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update the JS example &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/141&#34;&gt;#141&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional significant performance improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/119&#34;&gt;#119&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Error handling fixes and improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/125&#34;&gt;#125&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements with lots of internal changes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/117&#34;&gt;#117&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid catching fatal exceptions during evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--yaml-debug&lt;/code&gt; flag to add source-line comments showing where each line of YAML came from &lt;a href=&#34;&#34;&gt;#105&lt;/a&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/105&#34;&gt;https://github.com/databricks/sjsonnet/pull/105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;objectValues&lt;/code&gt; and &lt;code&gt;objectVlauesAll&lt;/code&gt; to stdlib &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/104&#34;&gt;#104&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow direct YAML output generation via &lt;code&gt;--yaml-out&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Do not allow duplicate field in object when evaluating list list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/100&#34;&gt;#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix compiler crash when &#39;+&#39; signal is true in a field declaration inside a list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/98&#34;&gt;#98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix error message for too many arguments with at least one named arg &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/97&#34;&gt;#97&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming JSON output to disk for lower memory usage &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/85&#34;&gt;#85&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Static detection of duplicate fields &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/86&#34;&gt;#86&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Strict mode to disallow error-prone adjacent object literals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/88&#34;&gt;#88&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.flatMap&lt;/code&gt;, &lt;code&gt;std.repeat&lt;/code&gt;, &lt;code&gt;std.clamp&lt;/code&gt;, &lt;code&gt;std.member&lt;/code&gt;, &lt;code&gt;std.stripChars&lt;/code&gt;, &lt;code&gt;std.rstripChars&lt;/code&gt;, &lt;code&gt;std.lstripChars&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for syntactical key ordering &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/53&#34;&gt;#53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bump dependency versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump verion of Scalatags, uPickle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.9&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version of FastParse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump versions of OS-Lib, uJson, Scalatags&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support std lib methods that take a key lambda &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/40&#34;&gt;#40&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Handle hex in unicode escaoes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/41&#34;&gt;#41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add encodeUTF8, decodeUTF8 std lib methdos &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/42&#34;&gt;#42&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Properly fail on non-boolean conditionals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/44&#34;&gt;#44&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support YAML-steam output &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/45&#34;&gt;#45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~2x performance increase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javascript support, allowing Sjsonnet to be used in the browser or on Node.js&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala 2.13 support&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.mod&lt;/code&gt;, &lt;code&gt;std.min&lt;/code&gt; and &lt;code&gt;std.max&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improvements to error reporting when types do not match&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements to the parser via upgrading to Fastparse 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First release&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>svtk/lecture4-scala</title>
    <updated>2023-05-19T01:45:07Z</updated>
    <id>tag:github.com,2023-05-19:/svtk/lecture4-scala</id>
    <link href="https://github.com/svtk/lecture4-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The code used in a lecture # 4 (28.09)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lecture#4&lt;/h1&gt; &#xA;&lt;p&gt;The code used in a lecture # 4 (28.09)&lt;/p&gt; &#xA;&lt;h1&gt;Task&lt;/h1&gt; &#xA;&lt;p&gt;Implement normalization of a lambda term. Lambda term should be specified like this:&lt;/p&gt; &#xA;&lt;p&gt;Appl(Abst(Var(&#34;x&#34;), Appl(Var(&#34;x&#34;),Var(&#34;y&#34;))),Abst(Var(&#34;x&#34;),Var(&#34;x&#34;)))&lt;/p&gt; &#xA;&lt;p&gt;Use case classes and pattern matching!&lt;/p&gt; &#xA;&lt;p&gt;Send it by e-mail: &lt;a href=&#34;mailto:course.scala@gmail.com&#34;&gt;course.scala@gmail.com&lt;/a&gt; Deadline is 4 October 2012, 21:00&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-csv</title>
    <updated>2023-05-19T01:45:07Z</updated>
    <id>tag:github.com,2023-05-19:/databricks/spark-csv</id>
    <link href="https://github.com/databricks/spark-csv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CSV Data Source for Apache Spark 1.x&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CSV Data Source for Apache Spark 1.x&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: This functionality has been inlined in Apache Spark 2.x. This package is in maintenance mode and we only accept critical bug fixes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-csv&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-csv.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/databricks/spark-csv?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/databricks/spark-csv/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 1.3+&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.10&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.11&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading CSV files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/1.6.0/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;. When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true the first line of files will be used to name columns and will not be included in data. All types will be assumed string. Default value is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. Delimiters inside quotes are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;parserLib&lt;/code&gt;: by default it is &#34;commons&#34; can be set to &#34;univocity&#34; to use that library for CSV parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: determines the parsing mode. By default it is PERMISSIVE. Possible values are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt;: tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt;: drops lines which have fewer or more tokens than expected or tokens which do not match the schema&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt;: aborts with a RuntimeException if encounters any malformed line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: automatically infers column types. It requires one extra pass over the data and is false by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;comment&lt;/code&gt;: skip lines beginning with this character. Default is &lt;code&gt;&#34;#&#34;&lt;/code&gt;. Disable comments by setting this to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. By default, it is &lt;code&gt;null&lt;/code&gt; which means trying to parse times and date by &lt;code&gt;java.sql.Timestamp.valueOf()&lt;/code&gt; and &lt;code&gt;java.sql.Date.valueOf()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The package also supports saving simple (non-nested) DataFrame. When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true, the header (from the schema in the DataFrame) will be written at the first line.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. This is written according to &lt;code&gt;quoteMode&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are written.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, nulls in the DataFrame will be written as this string.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use writing dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. If no dateFormat is specified, then &#34;yyyy-MM-dd HH:mm:ss.S&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quoteMode&lt;/code&gt;: when to quote fields (&lt;code&gt;ALL&lt;/code&gt;, &lt;code&gt;MINIMAL&lt;/code&gt; (default), &lt;code&gt;NON_NUMERIC&lt;/code&gt;, &lt;code&gt;NONE&lt;/code&gt;), see &lt;a href=&#34;https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/QuoteMode.html&#34;&gt;Quote Modes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These examples use a CSV file available for download &lt;a href=&#34;https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;CSV data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;, inferSchema &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars (yearMade double, carMake string, carModel string, comments string, blank string)&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .schema(customSchema)&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv.gz&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;, &#34;inferSchema&#34; -&amp;gt; &#34;true&#34;))&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    schema = customSchema,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;))&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .schema(customSchema)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, customSchema, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.SaveMode;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; saveOptions = new HashMap&amp;lt;String, String&amp;gt;();&#xA;saveOptions.put(&#34;header&#34;, &#34;true&#34;);&#xA;saveOptions.put(&#34;path&#34;, &#34;newcars.csv&#34;);&#xA;saveOptions.put(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;com.databricks.spark.csv&#34;, SaveMode.Overwrite,&#xA;                                saveOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.read \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .options(header=&#39;true&#39;) \&#xA;    .load(&#39;cars.csv&#39;, schema = customSchema)&#xA;&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).options(codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, schema = customSchema, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;customSchema &amp;lt;- structType(&#xA;    structField(&#34;year&#34;, &#34;integer&#34;),&#xA;    structField(&#34;make&#34;, &#34;string&#34;),&#xA;    structField(&#34;model&#34;, &#34;string&#34;),&#xA;    structField(&#34;comment&#34;, &#34;string&#34;),&#xA;    structField(&#34;blank&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, schema = customSchema)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;, which is automatically downloaded by the included shell script. To build a JAR file simply run &lt;code&gt;sbt/sbt package&lt;/code&gt; from the project root. The build configuration includes support for both Scala 2.10 and 2.11.&lt;/p&gt;</summary>
  </entry>
</feed>