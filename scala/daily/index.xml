<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-18T01:51:31Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fpinscala/fpinscala</title>
    <updated>2022-06-18T01:51:31Z</updated>
    <id>tag:github.com,2022-06-18:/fpinscala/fpinscala</id>
    <link href="https://github.com/fpinscala/fpinscala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code, exercises, answers, and hints to go along with the book &#34;Functional Programming in Scala&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://gitter.im/fpinscala/fpinscala?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/fpinscala/fpinscala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains exercises, hints, and answers for the book &lt;a href=&#34;http://manning.com/bjarnason/&#34;&gt;Functional Programming in Scala&lt;/a&gt;. Along with the book itself, it&#39;s the closest you&#39;ll get to having your own private functional programming tutor without actually having one.&lt;/p&gt; &#xA;&lt;p&gt;There are two main branches in this repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fpinscala/fpinscala/tree/first-edition&#34;&gt;first-edition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fpinscala/fpinscala/tree/second-edition&#34;&gt;second-edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Be sure to select the branch which matches the edition of the book you are reading!&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how to use this repository:&lt;/p&gt; &#xA;&lt;p&gt;Each chapter in the book develops a fully working library of functions and data types, built up through a series of exercises and example code given in the book text. The shell of this working library and exercise stubs live in &lt;code&gt;src/main/scala/fpinscala/exercises/&amp;lt;chapter-description&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;chapter-description&amp;gt;&lt;/code&gt; is a package name that corresponds to the chapter title (see below). When you begin working on a chapter, we recommend you open the exercise file(s) for that chapter, and when you encounter exercises, implement them in the exercises file and make sure they work.&lt;/p&gt; &#xA;&lt;p&gt;If you get stuck on an exercise, let&#39;s say exercise 4 in the chapter, you can find hints in &lt;code&gt;answerkey/&amp;lt;chapter-description&amp;gt;/04.hint.md&lt;/code&gt; (if no hints are available for a problem, the file will just have a single &#39;-&#39; as its contents) and the answer along with an explanation of the answer and any variations in &lt;code&gt;answerkey/&amp;lt;chapter-description&amp;gt;/04.answer.md&lt;/code&gt;. The finished Scala modules, with all answers for each chapter live in &lt;code&gt;src/main/scala/fpinscala/answers/&amp;lt;chapter-description&amp;gt;&lt;/code&gt;. Please feel free to submit pull requests for alternate answers, improved hints, and so on, so we can make this repo the very best resource for people working through the book.&lt;/p&gt; &#xA;&lt;p&gt;Chapter descriptions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chapter 2: gettingstarted&lt;/li&gt; &#xA; &lt;li&gt;Chapter 3: datastructures&lt;/li&gt; &#xA; &lt;li&gt;Chapter 4: errorhandling&lt;/li&gt; &#xA; &lt;li&gt;Chapter 5: laziness&lt;/li&gt; &#xA; &lt;li&gt;Chapter 6: state&lt;/li&gt; &#xA; &lt;li&gt;Chapter 7: parallelism&lt;/li&gt; &#xA; &lt;li&gt;Chapter 8: testing&lt;/li&gt; &#xA; &lt;li&gt;Chapter 9: parsing&lt;/li&gt; &#xA; &lt;li&gt;Chapter 10: monoids&lt;/li&gt; &#xA; &lt;li&gt;Chapter 11: monads&lt;/li&gt; &#xA; &lt;li&gt;Chapter 12: applicative&lt;/li&gt; &#xA; &lt;li&gt;Chapter 13: iomonad&lt;/li&gt; &#xA; &lt;li&gt;Chapter 14: localeffects&lt;/li&gt; &#xA; &lt;li&gt;Chapter 15: streamingio&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Setup build environment&lt;/h3&gt; &#xA;&lt;p&gt;The project is setup to use &lt;a href=&#34;https://scala-cli.virtuslab.org&#34;&gt;Scala CLI&lt;/a&gt;. First install Scala CLI by following the &lt;a href=&#34;https://scala-cli.virtuslab.org/install&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll also likely want an editor that&#39;s aware of Scala syntax. &lt;a href=&#34;https://code.visualstudio.com&#34;&gt;VSCode&lt;/a&gt; with the &lt;a href=&#34;https://scalameta.org/metals/docs/editors/vscode.html&#34;&gt;Metals&lt;/a&gt; extension works great.&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;To build the code for the first time, from the root directory of the project (i.e., the directory where this README.md is located):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli compile .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This compiles all exercises and answers. You can also do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli console .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get a Scala REPL (prompt &lt;code&gt;scala&amp;gt;&lt;/code&gt;) with access to exercises and answers, and then for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; import fpinscala.exercises.datastructures.List.*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to import the List package.&lt;/p&gt; &#xA;&lt;p&gt;To run the sample programs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli run .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;gives a list of possible main methods to execute. To run one of them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli run . --main-class fpinscala.answers.gettingstarted.printAbs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run unit-tests for a file you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli test . -- fpinscala.exercises.gettingstarted.GettingStartedSuite.*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run all unit-tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ scala-cli test .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, running all tests will result in failures. As you solve exercises, the tests will start to pass.&lt;/p&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;Note: an &lt;a href=&#34;https://www.scala-sbt.org&#34;&gt;SBT&lt;/a&gt; build is also provided.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;All code in this repository is &lt;a href=&#34;http://opensource.org/licenses/mit-license.php&#34;&gt;MIT-licensed&lt;/a&gt;. See the LICENSE file for details.&lt;/p&gt; &#xA;&lt;p&gt;Have fun, and good luck! Also be sure to check out &lt;a href=&#34;https://github.com/fpinscala/fpinscala/wiki&#34;&gt;the community wiki&lt;/a&gt; for the &lt;strong&gt;chapter notes&lt;/strong&gt;, links to more reading, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Paul, RÃºnar, and Michael&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>luismfonseca/cp-in-scala</title>
    <updated>2022-06-18T01:51:31Z</updated>
    <id>tag:github.com,2022-06-18:/luismfonseca/cp-in-scala</id>
    <link href="https://github.com/luismfonseca/cp-in-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CP in Scala&lt;/h1&gt; &#xA;&lt;h2&gt;The Point&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m aiming to do a complete Android app programmed in Scala for the portuguese train company Comboios de portugal - CP, while learning a bit more Scala in the process.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>seglo/kafka-lag-exporter</title>
    <updated>2022-06-18T01:51:31Z</updated>
    <id>tag:github.com,2022-06-18:/seglo/kafka-lag-exporter</id>
    <link href="https://github.com/seglo/kafka-lag-exporter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Monitor Kafka Consumer Group Latency with Kafka Lag Exporter&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kafka Lag Exporter &lt;a href=&#34;https://github.com/seglo/kafka-lag-exporter/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/seglo/kafka-lag-exporter?include_prereleases&#34; alt=&#34;gh-release-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/seglo/kafka-lag-exporter/actions&#34;&gt;&lt;img src=&#34;https://github.com/seglo/kafka-lag-exporter/workflows/CI/badge.svg?branch=master&#34; alt=&#34;gh-actions-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/seglo/kafka-lag-exporter/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/seglo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/patreon-sponsor-ff69b4.svg?sanitize=true&#34; alt=&#34;patreon-badge&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Monitor Kafka Consumer Group Latency with Kafka Lag Exporter&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Kafka Lag Exporter makes it easy to view the offset lag and calculate an estimate of latency (residence time) of your &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt; consumer groups. It can run anywhere, but it provides features to run easily on &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; clusters against &lt;a href=&#34;https://strimzi.io/&#34;&gt;Strimzi&lt;/a&gt; Kafka clusters using the &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; and &lt;a href=&#34;https://grafana.com/&#34;&gt;Grafana&lt;/a&gt; monitoring stack. Kafka Lag Exporter is an &lt;a href=&#34;https://doc.akka.io/docs/akka/current/typed/index.html&#34;&gt;Akka Typed&lt;/a&gt; application written in &lt;a href=&#34;https://www.scala-lang.org/&#34;&gt;Scala&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Kafka Lag Exporter is maintained by &lt;a href=&#34;https://seanglover.com&#34;&gt;Sean Glover&lt;/a&gt; (&lt;a href=&#34;https://github.com/seglo&#34;&gt;@seglo&lt;/a&gt;) and a community of contributors. If you like using this project and would like to support its development, please consider a donation using &lt;a href=&#34;https://www.patreon.com/seglo&#34;&gt;Patreon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Kafka Lag Exporter interpolates latency based on observed latest committed offset measurements of consumer groups.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/docs/interpolation-sm.png&#34; alt=&#34;Interpolation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more information about Kafka Lag Exporter&#39;s features see Lightbend&#39;s blog post: &lt;a href=&#34;https://www.lightbend.com/blog/monitor-kafka-consumer-group-latency-with-kafka-lag-exporter&#34;&gt;Monitor Kafka Consumer Group Latency with Kafka Lag Exporter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#metrics&#34;&gt;Metrics&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#labels&#34;&gt;Labels&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#run-on-kubernetes&#34;&gt;Run on Kubernetes&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#install-with-helm&#34;&gt;Install with Helm&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#view-the-health-endpoint&#34;&gt;View the health endpoint&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#view-exporter-logs&#34;&gt;View exporter logs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#run-standalone&#34;&gt;Run Standalone&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#reporters&#34;&gt;Reporters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#configuration-1&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#run-as-java-app&#34;&gt;Run as Java App&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#run-as-docker-image&#34;&gt;Run as Docker Image&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#required-permissions-for-kafka-acl&#34;&gt;Required Permissions for Kafka ACL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#strimzi-kafka-cluster-watcher&#34;&gt;Strimzi Kafka Cluster Watcher&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#monitoring-with-grafana&#34;&gt;Monitoring with Grafana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#filtering-metrics-without-prometheus-server&#34;&gt;Filtering Metrics without Prometheus Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#health-check&#34;&gt;Health Check&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#development&#34;&gt;Development&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#tests&#34;&gt;Tests&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#testing-with-local-docker-composeyaml&#34;&gt;Testing with local &lt;code&gt;docker-compose.yaml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#building-your-own-helm-chart&#34;&gt;Building your own Helm Chart&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#release&#34;&gt;Release&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#release-steps&#34;&gt;Release steps&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#change-log&#34;&gt;Change log&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;h2&gt;Metrics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; is a standard way to represent metrics in a modern cross-platform manner. Kafka Lag Exporter exposes several metrics as an HTTP endpoint that can be readily scraped by Prometheus. When installed using Helm and when enabling the Kubernetes pod self-discovery features within Prometheus server, Prometheus server will automatically detect the HTTP endpoint and scrape its data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_offset&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, topic, partition, member_host, consumer_id, client_id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The offset of the last consumed offset for this partition in this topic partition for this group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_lag&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, topic, partition, member_host, consumer_id, client_id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The difference between the last produced offset and the last consumed offset for this partition in this topic partition for this group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_lag_seconds&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, topic, partition, member_host, consumer_id, client_id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The estimated lag in seconds. This metric correlates with lag in offsets. For more information on how this is calculated read the Estimate consumer group lag in time section below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_max_lag&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, is_simple_consumer&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The highest (maximum) lag in offsets for a given consumer group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_max_lag_seconds&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, is_simple_consumer&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The highest (maximum) lag in time for a given consumer group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_sum_lag&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The sum of the difference between the last produced offset and the last consumed offset of all partitions for this group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_group_topic_sum_lag&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, group, topic&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The sum of the difference between the last produced offset and the last consumed offset of all partitions in this topic for this group.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_partition_latest_offset&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, topic, partition&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The latest offset available for topic partition. Kafka Lag Exporter will calculate a set of partitions for all consumer groups available and then poll for the last produced offset. The last produced offset is used in the calculation of other metrics provided, so it is exported for informational purposes. For example, the accompanying Grafana dashboard makes use of it to visualize the last produced offset and the last consumed offset in certain panels.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_partition_earliest_offset&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name, topic, partition&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The earliest offset available for topic partition. Kafka Lag Exporter will calculate a set of partitions for all consumer groups available and then poll for the earliest available offset. The earliest available offset is used in the calculation of other metrics provided, so it is exported for informational purposes. For example, the accompanying Grafana dashboard makes use of it to visualize the offset-based volume of a partition in certain panels.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;kafka_consumergroup_poll_time_ms&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Labels: &lt;code&gt;cluster_name&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The time taken to poll (milli seconds) all the information from all consumer groups for every cluster.&lt;/p&gt; &#xA;&lt;h3&gt;Labels&lt;/h3&gt; &#xA;&lt;p&gt;Each metric may include the following labels when reported. If you define the &lt;code&gt;labels&lt;/code&gt; property for configuration of a cluster then those labels will also be included. The superset of all &lt;code&gt;labels&lt;/code&gt; defined for all cluster configurations are used for each metric. This is due to a restriction in the Java Prometheus client library that only allows us to define one set of labels per metric. Therefore, if the label names across cluster configurations are not consistent then the missing labels for each cluster will appear as blank values (&lt;code&gt;&#34;&#34;&lt;/code&gt;) in the reported metric. An alternative to defining labels in Kafka Lag Exporter is to define &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config&#34;&gt;relabeling rules&lt;/a&gt; in your Prometheus server configuration.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cluster_name&lt;/code&gt; - Either the statically defined Kafka cluster name, or the metadata.name of the Strimzi Kafka cluster that was discovered with the Strimzi auto discovery feature.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; - The Kafka topic.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partition&lt;/code&gt; - The Kafka partition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;group&lt;/code&gt; - The Kafka consumer group.id.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The rest of the labels are passed along from the consumer group metadata requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;member_host&lt;/code&gt; - The hostname or IP of the machine or container running the consumer group member that is assigned this partition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;client_id&lt;/code&gt; - The id of the consumer group member. This is usually generated automatically by the group coordinator.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;consumer_id&lt;/code&gt; - The globally unique id of the consumer group member. This is usually a combination of the client_id and a GUID generated by the group coordinator.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prometheus server may add additional labels based on your configuration. For example, Kubernetes pod information about the Kafka Lag Exporter pod where the metrics were scraped from.&lt;/p&gt; &#xA;&lt;h2&gt;Run on Kubernetes&lt;/h2&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Details for configuration for the Helm Chart can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/charts/kafka-lag-exporter/values.yaml&#34;&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/a&gt; file of the accompanying Helm Chart.&lt;/p&gt; &#xA;&lt;h3&gt;Install with Helm&lt;/h3&gt; &#xA;&lt;p&gt;You can install the chart from the chart repository at the following location&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://seglo.github.io/kafka-lag-exporter/repo/index.yaml&#34;&gt;https://seglo.github.io/kafka-lag-exporter/repo/index.yaml&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm repo add kafka-lag-exporter https://seglo.github.io/kafka-lag-exporter/repo/&#xA;helm repo update&#xA;&#xA;helm install kafka-lag-exporter/kafka-lag-exporter &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;p&gt;Install with the &lt;a href=&#34;https://strimzi.io/&#34;&gt;Strimzi&lt;/a&gt; Kafka discovery feature. See &lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/#strimzi-kafka-cluster-watcher&#34;&gt;Strimzi Kafka Cluster Watcher&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm install kafka-lag-exporter/kafka-lag-exporter \&#xA;  --name kafka-lag-exporter \&#xA;  --namespace kafka-lag-exporter \&#xA;  --set watchers.strimzi=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install with statically defined cluster at the CLI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm install kafka-lag-exporter/kafka-lag-exporter \&#xA;  --name kafka-lag-exporter \&#xA;  --namespace myproject \&#xA;  --set clusters\[0\].name=my-cluster \&#xA;  --set clusters\[0\].bootstrapBrokers=my-cluster-kafka-bootstrap:9092&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install with statically defined cluster at the CLI, but with a non-default service account assigned to the deployment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm install kafka-lag-exporter/kafka-lag-exporter \&#xA;  --name kafka-lag-exporter \&#xA;  --namespace myproject \&#xA;  --set clusters\[0\].name=my-cluster \&#xA;  --set clusters\[0\].bootstrapBrokers=my-cluster-kafka-bootstrap:9092 \&#xA;  --set serviceAccount.create=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run a debug install (&lt;code&gt;DEBUG&lt;/code&gt; logging, debug helm chart install, force docker pull policy to &lt;code&gt;Always&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm repo update  # force refresh chart version&#xA;helm install kafka-lag-exporter/kafka-lag-exporter \&#xA;  --name kafka-lag-exporter \&#xA;  --namespace myproject \&#xA;  --set image.pullPolicy=Always \&#xA;  --set logLevel=DEBUG \&#xA;  --set clusters\[0\].name=my-cluster \&#xA;  --set clusters\[0\].bootstrapBrokers=my-cluster-kafka-bootstrap.myproject:9092 \&#xA;  --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;View the health endpoint&lt;/h3&gt; &#xA;&lt;p&gt;To view the Prometheus health endpoint from outside your Kubernetes cluster, use &lt;code&gt;kubectl port-forward&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl port-forward service/kafka-lag-exporter-service 8080:8000 --namespace myproject&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;View exporter logs&lt;/h4&gt; &#xA;&lt;p&gt;To view the logs of the exporter, identify the pod name of the exporter and use the &lt;code&gt;kubectl logs&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl logs {POD_ID} --namespace myproject -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run Standalone&lt;/h2&gt; &#xA;&lt;p&gt;To run the project in standalone mode you must first define a configuration &lt;code&gt;application.conf&lt;/code&gt;. This configuration must contain at least connection info to your Kafka cluster (&lt;code&gt;kafka-lag-exporter.clusters&lt;/code&gt;). All other configuration has defaults defined in the project itself. See &lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/src/main/resources/reference.conf&#34;&gt;&lt;code&gt;reference.conf&lt;/code&gt;&lt;/a&gt; for defaults.&lt;/p&gt; &#xA;&lt;h3&gt;Reporters&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to report (either one, multiple or all):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;to influxdb via the config &lt;code&gt;kafka-lag-exporter.reporters.influxdb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;to graphite via the config &lt;code&gt;kafka-lag-exporter.reporters.graphite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;as prometheus via the config &lt;code&gt;kafka-lag-exporter.reporters.prometheus&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You must also specify the active reporters in the &lt;code&gt;kafka-lag-exporter.sinks&lt;/code&gt; config.&lt;/p&gt; &#xA;&lt;p&gt;See section below for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;General Configuration (&lt;code&gt;kafka-lag-exporter{}&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Key&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.prometheus.port&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The port to run the Prometheus endpoint on&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.graphite.host&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The graphite host to send metrics to (if not set, will not output to graphite)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.graphite.port&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The graphite port to send metrics to (if not set, will not output to graphite)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.graphite.prefix&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The graphite metric prefix (if not set, prefix will be empty)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.endpoint&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The influxdb host to send metrics to (if not set, will not output to influxdb)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.port&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The influxdb port to send metrics to (if not set, will not output to influxdb)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.database&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;kafka_lag_exporter&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The influxdb database to send metrics to&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.username&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The influxdb username to connect (if not set, username will be empty)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.password&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;The influxdb password to connect (if not set, password will be empty)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reporters.influxdb.async&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Flag to enable influxdb async &lt;strong&gt;non-blocking&lt;/strong&gt; write mode to send metrics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sinks&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;PrometheusEndpointSink&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Specify which reporters must be used to send metrics. Possible values are: &lt;code&gt;PrometheusEndpointSink&lt;/code&gt;, &lt;code&gt;InfluxDBPusherSink&lt;/code&gt;, &lt;code&gt;GraphiteEndpointSink&lt;/code&gt;. (if not set, only Prometheus is activated)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;poll-interval&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30 seconds&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How often to poll Kafka for latest and group offsets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lookup-table-size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The maximum window size of the look up table &lt;strong&gt;per partition&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;client-group-id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;kafkalagexporter&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Consumer group id of kafka-lag-exporter&#39;s client connections&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;kafka-client-timeout&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10 seconds&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Connection timeout when making API calls to Kafka&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clusters&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A statically defined list of Kafka connection details. This list is optional if you choose to use the Strimzi auto-discovery feature&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;watchers&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Settings for Kafka cluster &#34;watchers&#34; used for auto-discovery.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;metric-whitelist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;.*&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Regex of metrics to be exposed via Prometheus endpoint. Eg. &lt;code&gt;[&#34;.*_max_lag.*&#34;, &#34;kafka_partition_latest_offset&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Kafka Cluster Connection Details (&lt;code&gt;kafka-lag-exporter.clusters[]&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Key&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;A unique cluster name to for this Kafka connection detail object&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;bootstrap-brokers&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Kafka bootstrap brokers. Comma delimited list of broker hostnames&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;group-whitelist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;.*&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A list of Regex of consumer groups monitored. For example, if you only wish to expose certain groups with &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; prefixes, use &lt;code&gt;[&#34;^input-.+&#34;, &#34;^output-.+&#34;]&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;group-blacklist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A list of Regex of consumer groups &lt;strong&gt;not&lt;/strong&gt; monitored. For example, if you wish to &lt;strong&gt;not&lt;/strong&gt; expose certain groups, use either &lt;code&gt;[&#34;^unmonitored-group.+&#34;]&lt;/code&gt; or &lt;code&gt;[&#34;unmonitored-group1&#34;, &#34;unmonitored-group2&#34;]&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;topic-whitelist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;.*&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A list of Regex of topics monitored. For example, if you only wish to expose certain topics, use either &lt;code&gt;[&#34;^topic.+&#34;]&lt;/code&gt; or &lt;code&gt;[&#34;topic1&#34;, &#34;topic2&#34;]&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;topic-blacklist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A list of Regex of topics &lt;strong&gt;not&lt;/strong&gt; monitored. For example, if you wish to &lt;strong&gt;not&lt;/strong&gt; expose certain topics, use either &lt;code&gt;[&#34;^unmonitored-topic.+&#34;]&lt;/code&gt; or &lt;code&gt;[&#34;unmonitored-topic1&#34;, &#34;unmonitored-topic2&#34;]&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;consumer-properties&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A map of key value pairs used to configure the &lt;code&gt;KafkaConsumer&lt;/code&gt;. See the &lt;a href=&#34;https://kafka.apache.org/documentation/#consumerconfigs&#34;&gt;Consumer Config&lt;/a&gt; section of the Kafka documentation for options.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;admin-client-properties&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A map of key value pairs used to configure the &lt;code&gt;AdminClient&lt;/code&gt;. See the &lt;a href=&#34;https://kafka.apache.org/documentation/#adminclientconfigs&#34;&gt;Admin Config&lt;/a&gt; section of the Kafka documentation for options.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;labels&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;A map of key value pairs will be set as additional custom labels per cluster for all the metrics in prometheus.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Watchers (&lt;code&gt;kafka-lag-exporters.watchers{}&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Key&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;strimzi&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Toggle for using Strimzi auto-discovery.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Ex) Expose metrics on port &lt;code&gt;9999&lt;/code&gt;, double the default lookup table size, and define &lt;code&gt;client.id&lt;/code&gt;&#39;s for the &lt;code&gt;KafkaConsumer&lt;/code&gt; and &lt;code&gt;AdminClient&lt;/code&gt; used by the project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kafka-lag-exporter {&#xA;  reporters {&#xA;    prometheus {&#xA;      port = 9999&#xA;    }&#xA;  }&#xA;  lookup-table-size = 120&#xA;  clusters = [&#xA;    {&#xA;      name = &#34;a-cluster&#34;&#xA;      bootstrap-brokers = &#34;a-1.cluster-a.xyzcorp.com:9092,a-2.cluster-a.xyzcorp.com:9092,a-3.cluster-a.xyzcorp.com:9092&#34;&#xA;      topic-whitelist = [&#xA;        &#34;widgets-.+&#34;&#xA;      ]&#xA;      consumer-properties = {&#xA;        client.id = &#34;consumer-client-id&#34;&#xA;      }&#xA;      admin-client-properties = {&#xA;        client.id = &#34;admin-client-id&#34;&#xA;      }&#xA;      labels = {&#xA;        location = &#34;ny&#34;&#xA;        zone = &#34;us-east&#34;&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run as Java App&lt;/h3&gt; &#xA;&lt;p&gt;Download the release &lt;strong&gt;zip&lt;/strong&gt; file (&lt;code&gt;kafka-lag-exporter-{VERSION}.zip&lt;/code&gt;) from the &lt;a href=&#34;https://github.com/seglo/kafka-lag-exporter/releases&#34;&gt;GitHub release&lt;/a&gt; page. Extract its contents and run the &lt;code&gt;./bin/kafka-lag-exporter&lt;/code&gt; shell script.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bin/kafka-lag-exporter \&#xA;    -Dconfig.file=/opt/docker/conf/application.conf \ &#xA;    -Dlogback.configurationFile=/opt/docker/conf/logback.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run as Docker Image&lt;/h3&gt; &#xA;&lt;p&gt;Define an &lt;code&gt;application.conf&lt;/code&gt; and optionally a &lt;code&gt;logback.xml&lt;/code&gt; with your configuration.&lt;/p&gt; &#xA;&lt;p&gt;Run the Docker image. Expose metrics endpoint on the host port &lt;code&gt;8000&lt;/code&gt;. Mount a config dir with your &lt;code&gt;application.conf&lt;/code&gt; and &lt;code&gt;logback.xml&lt;/code&gt; into the container.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 8000:8000 \&#xA;    -v $(pwd):/opt/docker/conf/ \&#xA;    seglo/kafka-lag-exporter:0.7.0 \&#xA;    /opt/docker/bin/kafka-lag-exporter \&#xA;    -Dconfig.file=/opt/docker/conf/application.conf \&#xA;    -Dlogback.configurationFile=/opt/docker/conf/logback.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See full example in &lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/examples/standalone&#34;&gt;&lt;code&gt;./examples/standalone&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you observe Kafka Lag Exporter reporting odd or inconsistent metric data then before creating an issue please enable &lt;code&gt;DEBUG&lt;/code&gt; logging to get raw data consumed from Kafka used to calculate metrics that are exported. If this logging does not help you resolve the problem then include logs, and your application configuration in a new GitHub issue.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2020-08-31 16:14:06,478 DEBUG [default-dispatcher-3] [c.l.k.ConsumerGroupCollector$       ]  Received Offsets Snapshot:&#xA;&#xA;Timestamp: 1598904846431&#xA;Groups: group-1-1&#xA;Earliest Offsets:&#xA;  Topic                                                           Partition  Earliest&#xA;  topic-1-2                                                       0          0&#xA;Latest Offsets:&#xA;  Topic                                                           Partition  Offset&#xA;  topic-1-2                                                       0          11&#xA;Last Group Offsets:&#xA;  Group                                                           Topic                                                           Partition  Offset&#xA;  group-1-1                                                       topic-1-2                                                       0          5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If installing with Helm then you can enable &lt;code&gt;DEBUG&lt;/code&gt; logging with the &lt;code&gt;kafkaLogLevel&lt;/code&gt; configuration in the chart&#39;s &lt;code&gt;[values.yaml](https://github.com/seglo/kafka-lag-exporter/blob/master/charts/kafka-lag-exporter/values.yaml)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When running in standalone mode you can either define assign the &lt;code&gt;KAFKA_LAG_EXPORTER_KAFKA_LOG_LEVEL&lt;/code&gt; environment variable to &lt;code&gt;DEBUG&lt;/code&gt;, or override the log level of &lt;code&gt;com.lightbend.kafkalagexporter&lt;/code&gt; directly in the &lt;code&gt;logback.xml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Required Permissions for Kafka ACL&lt;/h2&gt; &#xA;&lt;p&gt;Kafka Lag Exporter (&lt;code&gt;kafka-lag-exporter&lt;/code&gt;) requires the &lt;code&gt;DESCRIBE&lt;/code&gt; operation permission for consumer groups and topics at the cluster level.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ACLs for principal `User:kafka-lag-exporter`&#xA;Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: &#xA; &#x9;User:kafka-lag-exporter has Allow permission for operations: Describe from hosts: * &#xA;&#xA;Current ACLs for resource `Group:LITERAL:*`: &#xA; &#x9;User:kafka-lag-exporter has Allow permission for operations: Describe from hosts: * &#xA;&#xA;Current ACLs for resource `Topic:LITERAL:*`: &#xA; &#x9;User:kafka-lag-exporter has Allow permission for operations: Describe from hosts: * &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can be added using the following command (&lt;code&gt;authorizer-properties&lt;/code&gt; depends on the Kafka installation):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kafka-acls --authorizer-properties &#34;zookeeper.connect=localhost:2181&#34; --add --allow-principal &#34;User:kafka-lag-exporter&#34; --operation DESCRIBE --group &#39;*&#39; --topic &#39;*&#39; --cluster&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Strimzi Kafka Cluster Watcher&lt;/h2&gt; &#xA;&lt;p&gt;When you install the chart with &lt;code&gt;--set watchers.strimzi=true&lt;/code&gt; then the exporter will create a new &lt;code&gt;ClusterRole&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; to allow for the automatic discovery of &lt;a href=&#34;https://strimzi.io/&#34;&gt;Strimzi&lt;/a&gt; Kafka clusters. The exporter will watch for &lt;code&gt;Kafka&lt;/code&gt; resources to be created or destroyed. If the cluster already exists, or was created while the exporter was online then it will automatically begin to collect consumer group metadata and export it. If a &lt;code&gt;Kafka&lt;/code&gt; resource is destroyed then it will stop collecting consumer group metadata for that cluster.&lt;/p&gt; &#xA;&lt;p&gt;The exporter will name the cluster the same as &lt;code&gt;Kafka&lt;/code&gt; resources &lt;code&gt;metadata.name&lt;/code&gt; field.&lt;/p&gt; &#xA;&lt;h2&gt;Monitoring with Grafana&lt;/h2&gt; &#xA;&lt;p&gt;A sample Grafana dashboard is provided in &lt;code&gt;./grafana/&lt;/code&gt;. It can be imported into a Grafana server that is configured with a Prometheus datasource that is reading the Kafka Lag Exporter&#39;s Prometheus health endpoint.&lt;/p&gt; &#xA;&lt;p&gt;The dashboard contains several high level user-configurable variables.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Namespace&lt;/strong&gt; - The namespace of the Kafka Lag Exporter. Only 1 namespace can be selected at a time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cluster Name&lt;/strong&gt; - The name of the Kafka cluster. Only 1 cluster name can be selected at a time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consumer Group&lt;/strong&gt; - The name of the Consumer Group. This is a multi-select list which allows you to view the dashboard for 1 to All consumer groups.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This dashboard has 4 rows that are described below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;All Consumer Group Lag&lt;/strong&gt; - A high level set of 4 panels.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consumer Group Max Time Lag&lt;/li&gt; &#xA; &lt;li&gt;Consumer Group Time Lag Top Partitions&lt;/li&gt; &#xA; &lt;li&gt;Consumer Group Max Offset Lag&lt;/li&gt; &#xA; &lt;li&gt;Consumer Group Offset Lag Top Partitions &lt;img src=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/grafana/consumer_group_max_time_lag.png&#34; alt=&#34;Consumer Group Max Time Lag&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Consumer Group Time Lag Over Offset Lag&lt;/strong&gt; - One panel for each consumer group that shows the max lag in time on the left Y axis and max lag in offsets on the right Y axis. Ex) &lt;img src=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/grafana/max_consumer_group_time_lag_over_offset_lag.png&#34; alt=&#34;Max Consumer Group Time Lag Over Offset Lag Example&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Consumer Group Time Lag Over Summed Offsets&lt;/strong&gt; - One panel for each consumer group that shows the max lag in time on the left Y axis. The right Y axis has the sum of latest and last consumed offsets for all group partitions. Ex) &lt;img src=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/grafana/max_consumer_group_time_lag_over_summed_offsets.png&#34; alt=&#34;Max Consumer Group Time Lag Over Summed Offsets&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka Lag Exporter JVM Metrics&lt;/strong&gt; - JVM metrics for the Kafka Lag Exporter itself.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Filtering Metrics without Prometheus Server&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s possible to filter specific metric names using HTTP query parameters to the metrics health endpoint.&lt;/p&gt; &#xA;&lt;p&gt;To filter 1 or more metrics use the query parameter pattern of &lt;code&gt;name[]=prometheus_metric_name&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ex)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ curl -X GET -g http://localhost:8080?name[]=kafka_consumergroup_group_max_lag&#xA;# HELP kafka_consumergroup_group_max_lag Max group offset lag&#xA;# TYPE kafka_consumergroup_group_max_lag gauge&#xA;kafka_consumergroup_group_max_lag{cluster_name=&#34;pipelines-strimzi&#34;,group=&#34;variable-throughput-runtime.f3-merge.in01&#34;,} 52.0&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is an undocumented feature of the Prometheus HTTP server. For reference consult the &lt;a href=&#34;https://github.com/prometheus/client_java/raw/4e0e7527b048f1ffd0382dcb74c0b9dab23b4d9f/simpleclient_httpserver/src/main/java/io/prometheus/client/exporter/HTTPServer.java#L101&#34;&gt;&lt;code&gt;parseQuery&lt;/code&gt; method&lt;/a&gt; for the HTTP server in the &lt;a href=&#34;https://github.com/prometheus/client_java/&#34;&gt;&lt;code&gt;prometheus/client_java&lt;/code&gt;&lt;/a&gt; GitHub repository.&lt;/p&gt; &#xA;&lt;h2&gt;Health Check&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;kafka_consumergroup_poll_time_ms&lt;/code&gt; metric exposes the time taken the poll all the consumer group information for every cluster. This can be used as health check endpoint and optionally fail the health check if it&#39;s greater than some value (longer than the poll interval) Ex: &lt;code&gt;$ curl -X GET -g http://localhost:8000/metrics?name[]=kafka_consumergroup_poll_time_ms&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Tests&lt;/h3&gt; &#xA;&lt;p&gt;Kafka Lag Exporter has unit and integration tests. The integration tests use &lt;a href=&#34;https://doc.akka.io/docs/akka-stream-kafka/current/testing.html#testing-with-an-embedded-kafka-server&#34;&gt;Alpakka Kafka Testkit&lt;/a&gt; to provide an embedded Kafka instance and simulate consumer group lag.&lt;/p&gt; &#xA;&lt;p&gt;Run all tests with SBT.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing with local &lt;code&gt;docker-compose.yaml&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A Docker Compose cluster with producers and multiple consumer groups is defined in &lt;code&gt;./docker/docker-compose.yaml&lt;/code&gt;. This is useful to manually test the project locally, without K8s infrastructure. These images are based on the popular &lt;a href=&#34;https://hub.docker.com/r/wurstmeister/kafka/&#34;&gt;&lt;code&gt;wurstmeister&lt;/code&gt;&lt;/a&gt; Apache Kafka Docker images. Confirm you match up the version of these images with the correct version of Kafka you wish to test.&lt;/p&gt; &#xA;&lt;p&gt;To configure cluster connection info either create an &lt;code&gt;application.conf&lt;/code&gt; or pass environment variables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;KAFKA_LAG_EXPORTER_CLUSTERS.0.name=default&#xA;KAFKA_LAG_EXPORTER_CLUSTERS.0.bootstrap-brokers=localhost:9094&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remove any previous volume state.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose rm -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start up the cluster in the foreground.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building your own Helm Chart&lt;/h3&gt; &#xA;&lt;p&gt;If you want to build your own Helm Chart and accompanying docker images you can override the Docker repository and username with environment variables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;DOCKER_REPOSITORY&lt;/code&gt; - A custom Docker repository, such as a private company&#39;s docker repository (defaults to DockerHub) &lt;code&gt;DOCKER_USERNAME&lt;/code&gt; - A custom Docker username (defaults to &lt;code&gt;seglo&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;updateHelmChart&lt;/code&gt; sbt task to update the Helm Chart with the appropriate Docker repository and username.&lt;/p&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;docker:publishLocal&lt;/code&gt; sbt task to publish a local Docker image.&lt;/p&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;docker:publish&lt;/code&gt; sbt task to publish the Docker image to the specified Docker repository.&lt;/p&gt; &#xA;&lt;p&gt;For example, to update the Helm Chart to use a custom docker registry and username and to publish the chart locally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export DOCKER_REPOSITORY=&#34;docker.xyzcorp.com&#34;&#xA;$ export DOCKER_USERNAME=&#34;foobar&#34;&#xA;$ sbt updateHelmChart docker:publishLocal&#xA;[info] Loading settings for project global-plugins from idea.sbt ...&#xA;[info] Loading global plugins from /home/seglo/.sbt/1.0/plugins&#xA;[info] Loading settings for project kafka-lag-exporter-build from plugins.sbt ...&#xA;[info] Loading project definition from /home/seglo/source/kafka-lag-exporter/project&#xA;[info] Loading settings for project kafka-lag-exporter from version.sbt,build.sbt ...&#xA;[info] Set current project to kafka-lag-exporter (in build file:/home/seglo/source/kafka-lag-exporter/)&#xA;Update Chart.yaml appVersion to 0.4.0-SNAPSHOT and version to 0.4.0&#xA;Update values.yaml docker image tag to 0.4.0-SNAPSHOT&#xA;Update values.yaml docker repository to docker.xyzcorp.com/foobar/kafka-lag-exporter&#xA;...&#xA;[info] Successfully built f392402958b7&#xA;[info] Successfully tagged docker.xyzcorp.com/foobar/kafka-lag-exporter:0.4.0-SNAPSHOT&#xA;[info] Built image docker.xyzcorp.com/foobar/kafka-lag-exporter with tags [0.4.0-SNAPSHOT]&#xA;[success] Total time: 17 s, completed 1-May-2019 2:37:28 PM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deploy the local chart to K8s:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;helm install ./charts/kafka-lag-exporter \&#xA;  --name kafka-lag-exporter \&#xA;  --namespace kafka-lag-exporter \&#xA;  --set watchers.strimzi=true \&#xA;  --set kafkaLagExporterLogLevel=DEBUG \&#xA;  --set image.pullPolicy=Always&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;The release process is run when a new tag is pushed to the repository. Release steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run &lt;code&gt;doctoc README.md&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update change log &lt;code&gt;docker run -it --rm -v &#34;$(pwd)&#34;:/usr/local/src/your-app githubchangeloggenerator/github-changelog-generator -u seglo -p kafka-lag-exporter -t $(cat ~/.ghtoken-personal) --no-unreleased --no-issues --since-tag v0.6.7&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Push a new tag &lt;code&gt;git tag -a v0.7.0 -m &#34;v0.7.0&#34; &amp;amp;&amp;amp; git push origin --tags&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Change log&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/seglo/kafka-lag-exporter/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>