<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-29T01:39:09Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>supermariolabs/spooq</title>
    <updated>2022-09-29T01:39:09Z</updated>
    <id>tag:github.com,2022-09-29:/supermariolabs/spooq</id>
    <link href="https://github.com/supermariolabs/spooq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/banner.png&#34; alt=&#34;SPOOQ Logo&#34; title=&#34;Title&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#gettingstarted&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#docker&#34;&gt;Play with the tool using Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#howdoesitwork&#34;&gt;How does it work?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#streamprocessing&#34;&gt;Stream Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#interactivemode&#34;&gt;Interactive Mode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#thriftserver&#34;&gt;Thrift Server (Experimental)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#referencedoc&#34;&gt;Reference Documentation&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#referencedoc1&#34;&gt;Configuration Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#referencedoc2&#34;&gt;Steps Kind&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#referencedoc3&#34;&gt;Launch Parameters&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#download&#34;&gt;Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/howtocompile&#34;&gt;How to compile the code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/supermariolabs/spooq/main/#cookbook&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Overview &lt;a id=&#34;overview&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Spooq is an ETL Big Data tool based on the Apache Spark framework that simplifies its use through the ability to implement data pipelines using a declarative approach based on simple configuration files and expressing transformations primarily through SQL.&lt;/p&gt; &#xA;&lt;p&gt;The name is clearly inspired by the Apache Sqoop project, of which the tool is proposed as a replacement capable of exploiting the potential of Spark instead of the original tool&#39;s engine, which was instead based on Hadoop&#39;s MapReduce.&lt;/p&gt; &#xA;&lt;p&gt;Spooq unlike its &#34;predecessor&#34; is capable, by leveraging the capabilities of Apache Spark, of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supporting a large number of data sources in batch and streaming mode (e.g.: datasets stored on HDFS, S3, ADLS Gen2 and GCS in CSV, Parquet, Avro and other formats as well as RDBMS via JDBC or all popular NoSQL, Apache Kafka and more);&lt;/li&gt; &#xA; &lt;li&gt;Allowing to implement complex data pipelines by leveraging Spark SQL&#39;s built-in language and functions and using UDFs and simple custom code blocks where necessary;&lt;/li&gt; &#xA; &lt;li&gt;Use an interactive mode to debug flows and explore data easily and intuitively;&lt;/li&gt; &#xA; &lt;li&gt;Other features that will be better explained in the documentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started &lt;a id=&#34;gettingstarted&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;For the impatient ones, here is a quick guide to get started with the tool quickly. In keeping with tradition we start with the classic &#34;hello world&#34; example (don&#39;t worry if it doesn&#39;t make much sense from a functional point of view, it serves to understand the basic philosophy of the tool).&lt;/p&gt; &#xA;&lt;p&gt;The tool takes as input a configuration file (HOCON, JSON or YAML format can be used) with some identifying information about the flow and a sequence of processing &lt;code&gt;steps&lt;/code&gt;. For example, using the following configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;helloWorld&#34;&#xA;desc = &#34;sample &#39;hello world&#39; job&#34;&#xA;&#xA;steps = [&#xA;{&#xA;    id = hello&#xA;    shortDesc = &#34;execute &#39;hello world&#39; sql query&#34;&#xA;    kind = sql&#xA;    sql = &#34;select &#39;hello world!&#39; as message&#34;&#xA;    show = true&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launching the application we will get the following output: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq1.gif&#34; alt=&#34;Asciinema&#34; title=&#34;Spooq: &#39;hello world&#39;!&#39;&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/522289&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Okay, now let&#39;s try to do something more useful. For example, a CSV format import of a Postgres table by connecting via JDBC:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;sample job&#34;&#xA;desc = &#34;sample spooq job that ingest data from database table through jdbc connection&#34;&#xA;&#xA;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load from jdbc&#34;&#xA;    desc = &#34;load &#39;customer&#39; table from postgres database&#34;&#xA;    kind = input&#xA;    format = jdbc&#xA;    options = {&#xA;        url = &#34;jdbc:postgresql://kandula.db.elephantsql.com:5432/wllbjgnv&#34;&#xA;        driver = &#34;org.postgresql.Driver&#34;&#xA;&#x9;dbtable = &#34;public.customer&#34;&#xA;        user = &#34;wllbjgnv&#34;&#xA;        password = &#34;**********&#34;&#xA;        numPartitions = &#34;1&#34;&#xA;    }&#xA;    cache = true&#xA;    show = true&#xA;},&#xA;{&#xA;      id = out&#xA;      shortDesc = &#34;write to fs&#34;&#xA;      dependsOn = [&#34;customers&#34;]&#xA;      desc = &#34;write &#39;customer&#39; table to fs using csv format&#34;&#xA;      kind = output&#xA;      source = customers&#xA;      format = csv&#xA;      options = {&#xA;        header = &#34;true&#34;&#xA;      }&#xA;      mode = overwrite&#xA;      path = &#34;/tmp/customer.csv&#34;&#xA;},&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Please note:&lt;/em&gt; in order to connect via jdbc to Postgres we must have the connector available in the classpath. We will explain this step in more detail later.&lt;/p&gt; &#xA;&lt;p&gt;Let us now run the tool with the above configuration file: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq2.gif&#34; alt=&#34;Asciinema&#34; title=&#34;Spooq: jdbc import&#39;&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/522300&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s try adding a data preparation step (we can use as many as we need):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;sample job&#34;&#xA;desc = &#34;sample spooq job that ingest data from database table through jdbc connection&#34;&#xA;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load from jdbc&#34;&#xA;    desc = &#34;load &#39;customer&#39; table from postgres database&#34;&#xA;    kind = input&#xA;    format = jdbc&#xA;    options = {&#xA;        url = &#34;jdbc:postgresql://kandula.db.elephantsql.com:5432/wllbjgnv&#34;&#xA;        driver = &#34;org.postgresql.Driver&#34;&#xA;&#x9;dbtable = &#34;public.customer&#34;&#xA;        user = &#34;wllbjgnv&#34;&#xA;        password = &#34;**********&#34;&#xA;        numPartitions = &#34;1&#34;&#xA;    }&#xA;    cache = true&#xA;},&#xA;{&#xA;    id = prepared&#xA;    shortDesc = &#34;filter and prepare output&#34;&#xA;    kind = sql&#xA;    sql = &#34;select customer_id,first_name,last_name,email from customers&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;      id = out&#xA;      shortDesc = &#34;write to fs&#34;&#xA;      desc = &#34;write &#39;customer&#39; table to fs using csv format&#34;&#xA;      kind = output&#xA;      source = prepared&#xA;      format = parquet&#xA;      mode = overwrite&#xA;      path = &#34;/tmp/customer.parquet&#34;&#xA;},&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s launch the tool again using the new configuration: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq3.gif&#34; alt=&#34;Asciinema&#34; title=&#34;Spooq: jdbc import 2&#39;&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/522363&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Play with the tool using Docker &lt;a id=&#34;docker&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The best way to start testing the tool is to use the prepackaged Docker image by following these simple steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In a folder of your choice create two subfolders named: &lt;code&gt;conf&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Inside the conf folder create a hello.conf file with the following content&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;helloWorld&#34;&#xA;desc = &#34;sample &#39;hello world&#39; job&#34;&#xA;&#xA;steps = [&#xA;{&#xA;    id = hello&#xA;    shortDesc = &#34;execute &#39;hello world&#39; sql query&#34;&#xA;    kind = sql&#xA;    sql = &#34;select &#39;hello world!&#39; as message&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;    id = out&#xA;    shortDesc = &#34;sample output&#34;&#xA;    kind = output&#xA;    source = hello&#xA;    format = json&#xA;    mode = overwrite&#xA;    path = /opt/spooq/data/hello.json&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Launch the docker image with the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v $(pwd)/conf:/opt/spooq/conf -v $(pwd)/data:/opt/spooq/data -it mcartia/spooq -c conf/hello.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;If everything went smoothly you should find the job output in the &lt;code&gt;data/hello.json/&lt;/code&gt; directory. Congratulations!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: You can pass any arguments supported by spooq. The application will run locally on a standalone Spark installation embedded in the docker image.&lt;/p&gt; &#xA;&lt;p&gt;It is possible to load additional dependencies by setting the SPOOQ_PACKAGES environment variable. For example, if we wanted to launch the application by loading the jdbc postgres connector it would be enough to use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v $(pwd)/conf:/opt/spooq/conf -v $(pwd)/data:/opt/spooq/data -e SPOOQ_PACKAGES=org.postgresql:postgresql:42.4.0 -it mcartia/spooq -c conf/your.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(to load multiple dependencies just separate them with a comma &lt;code&gt;,&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;How does it work? &lt;a id=&#34;howdoesitwork&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The tool is nothing more than a generic Spark application that can be launched in standalone mode (&lt;code&gt;master=local[*]&lt;/code&gt; via a fat-jar containing within it the Apache Spark framework and other libraries) or on a cluster via &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to use the tool as a library within other spark applications or from spark-shell: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq4.gif&#34; alt=&#34;Asciinema&#34; title=&#34;Spooq: spark-shell&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/mjoaU6GgjhMb1b3dR0flJNP57&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this mode we will receive as output an object with all the dataframes and variables created during the execution of the job defined in the configuration used so we can continue to process it easily interactively in the REPL.&lt;/p&gt; &#xA;&lt;p&gt;The downloadable distribution also contains sample launch scripts in the various modes. This is the ones that make use of &lt;code&gt;spark-submit&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;source $SPOOQ_HOME/bin/loadenv&#xA;&#xA;JAR=$SPOOQ_HOME/lib/spooq-spark3.jar&#xA;MAIN=com.github.supermariolabs.spooq.Application&#xA;&#xA;# Example&#xA;# SPOOQ_PACKAGES=org.postgresql:postgresql:42.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0&#xA;&#xA;if [ -n &#34;$SPOOQ_PACKAGES&#34; ]&#xA;then&#xA;    echo &#34;PACKAGES_CMD=--packages $SPOOQ_PACKAGES&#34;&#xA;    PACKAGES_CMD=&#34;--packages $SPOOQ_PACKAGES&#34;&#xA;else&#xA;    PACKAGES_CMD=&#xA;fi&#xA;&#xA;ARGS=&#34;$@&#34;&#xA;&#xA;if [ -z &#34;$JAVA_HOME&#34; ]&#xA;then&#xA;    echo &#34;JAVA_HOME not defined!&#34;&#xA;else&#xA;    echo &#34;Using JAVA_HOME=$JAVA_HOME&#34;&#xA;fi&#xA;&#xA;if [ -z &#34;$SPARK_HOME&#34; ]&#xA;then&#xA;    echo &#34;SPARK_HOME not defined!&#34;&#xA;else&#xA;    echo &#34;Using SPARK_HOME=$SPARK_HOME&#34;&#xA;    $SPARK_HOME/bin/spark-submit \&#xA;&#x9;--class $MAIN \&#xA;&#x9;--master local[*] \&#xA;&#x9;--conf spark.executor.extraJavaOptions=-Dlog4j.configurationFile=$SPOOQ_HOME/conf/log4j2.properties \&#xA;&#x9;--conf spark.driver.extraJavaOptions=-Dlog4j.configurationFile=$SPOOQ_HOME/conf/log4j2.properties \&#xA;&#x9;$PACKAGES_CMD \&#xA;&#x9;$JAR \&#xA;&#x9;$ARGS&#xA;fi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you can see, you can use the system to load dependencies from maven-compatible repositories using the &lt;code&gt;--packages&lt;/code&gt; (and &lt;code&gt;--repositories&lt;/code&gt;) option of the &lt;code&gt;spark-submit&lt;/code&gt; command!&lt;/p&gt; &#xA;&lt;h2&gt;Stream Processing &lt;a id=&#34;streamprocessing&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The framework also supports the Structured Streaming API via step blocks with &lt;code&gt;input-stream&lt;/code&gt; and &lt;code&gt;output-stream&lt;/code&gt; kind. Let us look at an example and also introduce the use of UDFs to enrich the already large collection of built-in SQL functions provided by Spark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;sample streaming job&#34;&#xA;&#xA;steps = [&#xA;{&#xA;    id = stream&#xA;    shortDesc = &#34;generate fake stream&#34;&#xA;    kind = input-stream&#xA;    format = rate&#xA;    options = {&#xA;        rowsPerSecond = &#34;2&#34;&#xA;    }&#xA;},&#xA;{&#xA;    id = randomCustomer&#xA;    shortDesc = &#34;load sample udf&#34;&#xA;    kind = udf&#xA;    claz = com.github.supermariolabs.spooq.udf.example.FakeCustomerUDF&#xA;},&#xA;{&#xA;    id = enriched&#xA;    shortDesc = &#34;enrich stream using sql and udf&#34;&#xA;    kind = sql&#xA;    sql = &#34;select customer.* from (select randomCustomer(value) customer from stream)&#34;&#xA;},&#xA;{&#xA;    id = outStream&#xA;    source = enriched&#xA;    shortDesc = &#34;stream dump&#34;&#xA;    kind = output-stream&#xA;    format = console&#xA;    outputMode = &#34;append&#34;&#xA;    trigger = {&#xA;                policy = &#34;processingTime&#34;&#xA;                value = &#34;10 seconds&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which once launched will produce: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq5.gif&#34; alt=&#34;Asciinema&#34; title=&#34;Spooq: streaming 1&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/522379&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Mode &lt;a id=&#34;interactivemode&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Interactive mode allows you to be able to execute SQL queries once the pipeline execution is finished. This mode is very useful during pipeline development and debugging as well as for being able to perform interactive analysis of data from multiple sources using Spark&#39;s distributed SQL engine.&lt;/p&gt; &#xA;&lt;p&gt;This mode is triggered by the use of the &lt;code&gt;--interactive&lt;/code&gt; (or &lt;code&gt;-i&lt;/code&gt;) switch when launching the application. Let&#39;s use this simple configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = &#34;sample job&#34;&#xA;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load customer.csv file&#34;&#xA;    kind = input&#xA;    format = csv&#xA;    schema = &#34;customer_id int ,store_id int ,first_name string,last_name string,email string,address_id string,activebool string,create_date string,last_update string,active string&#34;&#xA;    options = {&#xA;        header = &#34;true&#34;&#xA;    }&#xA;    path = &#34;/tmp/customer.csv&#34;&#xA;    cache = true&#xA;}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;we will get: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq6.gif&#34; alt=&#34;DBeaver Community&#34; title=&#34;DBeaver Community&#34;&gt; &lt;a href=&#34;https://asciinema.org/a/522550&#34;&gt;Watch in Asciinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Thrift Server (Experimental) &lt;a id=&#34;thriftserver&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;An experimental feature allows a built-in thrift server to be started (on a configurable port, default: 10001) and use a client via the standard Hive JDBC driver to be able to make SQL queries on the views (temporary tables) created during processing.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can use the opensource &lt;a href=&#34;https://dbeaver.io&#34;&gt;DBeaver Community&lt;/a&gt; client: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq7.png&#34; alt=&#34;DBeaver&#34; title=&#34;DBeaver&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reference Documentation &lt;a id=&#34;referencedoc&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Configuration Overview &lt;a id=&#34;referencedoc1&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Configuration files can be written in &lt;a href=&#34;https://en.wikipedia.org/wiki/HOCON&#34;&gt;HOCON&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/JSON&#34;&gt;JSON&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/YAML&#34;&gt;YAML&lt;/a&gt; format. Decoding is implemented through the use of the &lt;a href=&#34;https://circe.github.io/circe/&#34;&gt;Circe&lt;/a&gt; library. When the format is not specified as a launch parameter, the application tries to infer it from the file extension.&lt;/p&gt; &#xA;&lt;p&gt;All configuration blocks (including the root) have an identifier (&lt;code&gt;id&lt;/code&gt;, mandatory) a short description (&lt;code&gt;shortDesc&lt;/code&gt;, optional) and an extended description (&lt;code&gt;desc&lt;/code&gt;, optional) and a list of &lt;code&gt;steps&lt;/code&gt; (mandatory but can be empty). The following is an example of a valid minimal configuration in HOCON format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;id = minimal&#xA;shortDesc = &#34;Minimal configuration example&#34;&#xA;desc = &#34;This example configuration does nothing but is formally valid&#34;&#xA;steps = []&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which will produce: &lt;img src=&#34;https://supermariolabs.github.io/spooq/docs/assets/images/spooq8.png&#34; alt=&#34;Minimal configuration&#34; title=&#34;minimal configuration&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Steps Kind &lt;a id=&#34;referencedoc2&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;input&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;input&lt;/code&gt; loads a &lt;code&gt;DataFrame&lt;/code&gt; starting from a &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources.html&#34;&gt;data source natively supported by Spark&lt;/a&gt; or using a third-party connector.&lt;/p&gt; &#xA;&lt;p&gt;For each &lt;code&gt;DataFrame&lt;/code&gt; a corresponding &lt;em&gt;temporary view&lt;/em&gt; is also created whose name is the &lt;code&gt;id&lt;/code&gt; of the corresponding &lt;code&gt;step&lt;/code&gt; block. In this way it will be possible to reference the same in subsequent blocks within SQL queries.&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;format&lt;/code&gt; data source format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;options&lt;/code&gt; data source options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;schema&lt;/code&gt; schema in Spark SQL DDL format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt; data source path (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cache&lt;/code&gt; whether to apply dataframe caching (N.B. lazy as default on Spark)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;show&lt;/code&gt; whether to display a diagnostic dataframe data sample&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;   {&#xA;      id = customers&#xA;      shortDesc = &#34;load customer.csv file&#34;&#xA;      kind = input&#xA;      format = csv&#xA;      schema = &#34;customer_id int ,store_id int ,first_name string,last_name string,email string,address_id string,activebool string,create_date string,last_update string,active string&#34;&#xA;      options = {&#xA;         header = &#34;true&#34;&#xA;      }&#xA;      path = &#34;/tmp/customer.csv&#34;&#xA;      cache = true&#xA;      show = true&#xA;   },&#xA;   #...&#xA;   {&#xA;      id = jdbc&#xA;      shortDesc = &#34;load from jdbc&#34;&#xA;      desc = &#34;load &#39;customer&#39; table from postgres database&#34;&#xA;      kind = input&#xA;      format = jdbc&#xA;      options = {&#xA;         url = &#34;jdbc:postgresql://kandula.db.elephantsql.com:5432/wllbjgnv&#34;&#xA;         driver = &#34;org.postgresql.Driver&#34;&#xA;         dbtable = &#34;public.customer&#34;&#xA;         user = &#34;wllbjgnv&#34;&#xA;         password = &#34;F-pOL8v410XRmLrC43PCKlazY_-cT11k&#34;&#xA;         numPartitions = &#34;1&#34;&#xA;      }&#xA;      cache = true&#xA;      show = true&#xA;   }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;input-stream&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;input-stream&lt;/code&gt; loads a (streaming) &lt;code&gt;DataFrame&lt;/code&gt; starting from a data source natively supported by Spark or using a third-party connector. The feature uses &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;Spark&#39;s Structured Streaming&lt;/a&gt; API.&lt;/p&gt; &#xA;&lt;p&gt;For each (streaming) &lt;code&gt;DataFrame&lt;/code&gt; a corresponding &lt;em&gt;temporary view&lt;/em&gt; is also created whose name is the &lt;code&gt;id&lt;/code&gt; of the corresponding &lt;code&gt;step&lt;/code&gt; block. In this way it will be possible to reference the same in subsequent blocks within SQL queries.&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;format&lt;/code&gt; data source format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;options&lt;/code&gt; data source options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;schema&lt;/code&gt; schema in Spark SQL DDL format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;avro&lt;/code&gt; &lt;strong&gt;experimental&lt;/strong&gt; decode the value of a message (from a kafka topic)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;avroSchema&lt;/code&gt; &lt;strong&gt;experimental&lt;/strong&gt; the avro schema that will be used for the above decoding&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt; data source path (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;show&lt;/code&gt; whether to display a diagnostic dataframe for each triggered batch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = stream&#xA;    shortDesc = &#34;generate fake stream&#34;&#xA;    kind = input-stream&#xA;    format = rate&#xA;    options = {&#xA;        rowsPerSecond = &#34;2&#34;&#xA;    }&#xA;},&#xA;#...&#xA;   {&#xA;      id = kafka&#xA;      shortDesc = &#34;kafka topic input&#34;&#xA;      kind = input-stream&#xA;      format = kafka&#xA;      options = {&#xA;         &#34;kafka.bootstrap.servers&#34; = &#34;localhost:9092&#34;&#xA;         subscribe = &#34;spooq&#34;&#xA;         includeHeaders = &#34;true&#34;&#xA;      }&#xA;   }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;sql&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;sql&lt;/code&gt; creates a dataframe (in batch or streaming mode) and the corresponding &lt;em&gt;temporary view&lt;/em&gt; starting from an SQL query that makes use of dataframes previously created in other blocks (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input-stream&lt;/code&gt; or other &lt;code&gt;sql&lt;/code&gt; blocks).&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sql&lt;/code&gt; the transformation query you want to perform&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cache&lt;/code&gt; whether to apply dataframe caching (N.B. lazy as default on Spark)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;show&lt;/code&gt; whether to display a diagnostic dataframe data sample&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load customer.csv file&#34;&#xA;    kind = input&#xA;    format = csv&#xA;    options = {&#xA;        header = &#34;true&#34;&#xA;    }&#xA;    path = &#34;/tmp/customer.csv&#34;&#xA;    cache = true&#xA;    show = true&#xA;},&#xA;{&#xA;    id = filter&#xA;    shortDesc = &#34;filter customers&#34;&#xA;    kind = sql&#xA;    sql = &#34;select * from customers where substr(first_name,0,1)=&#39;B&#39;&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;    id = prepared&#xA;    shortDesc = &#34;prepare output&#34;&#xA;    kind = sql&#xA;    sql = &#34;select first_name, upper(last_name), email from filter&#34;&#xA;    show = true&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;variable&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;variable&lt;/code&gt; creates a variable that is put into the variables map and can be referenced in sql blocks through placeholders.&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sql&lt;/code&gt; the query you want to use to populate the value (it can return multiple values, in this case the variable will be a list)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;  {&#xA;    id = helloVar&#xA;    kind = variable&#xA;    sql = &#34;select &#39;hello world!&#39;&#34;&#xA;    show = true&#xA;  },&#xA;  {&#xA;    id = hello&#xA;    shortDesc = &#34;execute hello world sql query&#34;&#xA;    kind = sql&#xA;    sql = &#34;&#34;&#34;select &#39;#{variables.helloVar}&#39; as message&#34;&#34;&#34;&#xA;    show = true&#xA;  },&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;script (experimental)&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;script&lt;/code&gt; creates a dataframe and the corresponding &lt;em&gt;temporary view&lt;/em&gt; evaluating a script interpreted by a &lt;a href=&#34;https://jcp.org/en/jsr/detail?id=223&#34;&gt;JSR 223&lt;/a&gt; engine.&lt;/p&gt; &#xA;&lt;p&gt;Inside the snippet we will be able to use the variables &lt;code&gt;sc&lt;/code&gt; (SparkContext), &lt;code&gt;spark&lt;/code&gt; (SparkSession), &lt;code&gt;logger&lt;/code&gt; (Logger slf4j), all the &lt;em&gt;dataframes&lt;/em&gt; and &lt;em&gt;variables created&lt;/em&gt; in the previous blocks (referenced using the &lt;code&gt;id&lt;/code&gt; as a name).&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;jsr223Engine&lt;/code&gt; the engine you want to use (default: &lt;code&gt;scala&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt; the code in the chosen language Examples:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = hello&#xA;    shortDesc = &#34;execute &#39;hello world&#39; sql query&#34;&#xA;    kind = sql&#xA;    sql = &#34;select &#39;hello world!&#39; as message&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;    id = scalaTest&#xA;    kind = script&#xA;    jsr223Engine = scala&#xA;    code = &#34;&#34;&#34;&#xA;           //scala code example&#xA;           logger.info(&#34;scala script example...&#34;)&#xA;&#xA;           def helloMsg(): String = {&#xA;                &#34;Hello, SCALA world!&#34;&#xA;                }&#xA;           &#xA;           hello.union(spark.sql(s&#34;select &#39;${helloMsg()}&#39; as hello&#34;))&#xA;           &#34;&#34;&#34;&#xA;},&#xA;{&#xA;    id = jsTest&#xA;    kind = script&#xA;    jsr223Engine = js&#xA;    code = &#34;&#34;&#34;&#xA;           //js code example&#xA;           logger.info(&#34;js script example...&#34;)&#xA;&#xA;           function helloMsg() {&#xA;                 return &#34;Hello, JS world!&#34;;&#xA;           }&#xA;&#xA;           hello.union(spark.sql(&#34;select &#39;&#34;+helloMsg()+&#34;&#39; as hello&#34;))&#xA;           &#34;&#34;&#34;&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;custom&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;custom&lt;/code&gt; creates a dataframe and the corresponding &lt;em&gt;temporary view&lt;/em&gt; executing custom code defined in a class that extends the &lt;code&gt;SimpleStep&lt;/code&gt; trait:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.github.supermariolabs.spooq.etl&#xA;&#xA;import org.apache.spark.sql.DataFrame&#xA;&#xA;trait SimpleStep extends Serializable {&#xA;  def run(dfMap: Map[String, DataFrame], variables: Map[String, Any]): DataFrame&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;claz&lt;/code&gt; class name including package&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;cache&lt;/code&gt; whether to apply dataframe caching (N.B. lazy as default on Spark)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;show&lt;/code&gt; whether to display a diagnostic dataframe data sample&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load customer.csv file&#34;&#xA;    kind = input&#xA;    format = csv&#xA;    schema = &#34;customer_id int ,store_id int ,first_name string,last_name string,email string,address_id string,activebool string,create_date string,last_update string,active string&#34;&#xA;    options = {&#xA;        header = &#34;true&#34;&#xA;    }&#xA;    path = &#34;/tmp/customer.csv&#34;&#xA;    cache = true&#xA;    show = true&#xA;},&#xA;{&#xA;    id = custom&#xA;    shortDesc = &#34;custom step&#34;&#xA;    kind = custom&#xA;    claz = com.github.supermariolabs.spooq.etl.SampleStep&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;udf&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;udf&lt;/code&gt; registers a custom user defined function that can be used inside subsequent &lt;code&gt;sql&lt;/code&gt; blocks.&lt;/p&gt; &#xA;&lt;p&gt;The function must be implemented (and available in the classpath at runtime) by extending the &lt;code&gt;SimpleUDF&lt;/code&gt; trait:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.github.supermariolabs.spooq.udf&#xA;&#xA;import org.apache.spark.sql.expressions.UserDefinedFunction&#xA;&#xA;trait SimpleUDF extends Serializable {&#xA;   val udf: UserDefinedFunction&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;claz&lt;/code&gt; class name including package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = stream&#xA;    shortDesc = &#34;generate fake stream&#34;&#xA;    kind = input-stream&#xA;    format = rate&#xA;    options = {&#xA;        rowsPerSecond = &#34;2&#34;&#xA;    }&#xA;},&#xA;{&#xA;    id = randomCustomer&#xA;    shortDesc = &#34;load sample udf&#34;&#xA;    kind = udf&#xA;    claz = com.github.supermariolabs.spooq.udf.example.FakeCustomerUDF&#xA;},&#xA;{&#xA;    id = enriched&#xA;    shortDesc = &#34;enrich stream using sql and udf&#34;&#xA;    kind = sql&#xA;    sql = &#34;select customer.* from (select randomCustomer(value) customer from stream)&#34;&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;output&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;output&lt;/code&gt; writes a &lt;code&gt;DataFrame&lt;/code&gt; to any &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources.html&#34;&gt;data source natively supported by Spark&lt;/a&gt; or using a third-party connector.&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;format&lt;/code&gt; data source format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;options&lt;/code&gt; data source options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt; data source path (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt; &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes&#34;&gt;save mode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partitionBy&lt;/code&gt; partitioning column(s)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = customers&#xA;    shortDesc = &#34;load customer.csv file&#34;&#xA;    kind = input&#xA;    format = csv&#xA;    schema = &#34;customer_id int ,store_id int ,first_name string,last_name string,email string,address_id string,activebool string,create_date string,last_update string,active string&#34;&#xA;    path = &#34;/tmp/customer.csv&#34;&#xA;    cache = true&#xA;    show = true&#xA;},&#xA;{&#xA;    id = filter&#xA;    shortDesc = &#34;filter customers&#34;&#xA;    kind = sql&#xA;    sql = &#34;select * from customers where substr(first_name,0,1)=&#39;B&#39;&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;    id = prepared&#xA;    shortDesc = &#34;prepare output&#34;&#xA;    kind = sql&#xA;    sql = &#34;select first_name, upper(last_name), email from filter&#34;&#xA;    show = true&#xA;},&#xA;{&#xA;      id = out&#xA;      shortDesc = &#34;write to fs&#34;&#xA;      dependsOn = [&#34;filter&#34;]&#xA;      desc = &#34;write filtered customers table to fs using json format&#34;&#xA;      kind = output&#xA;      source = prepared&#xA;      format = json&#xA;      mode = overwrite&#xA;      path = &#34;/tmp/customer.json&#34;&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;output-stream&lt;/h4&gt; &#xA;&lt;p&gt;A step with kind &lt;code&gt;output-stream&lt;/code&gt; creates a &lt;code&gt;streaming DataFrame&lt;/code&gt; that writes to a data source natively supported by Spark or using a third-party connector. The feature uses &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;Spark&#39;s Structured Streaming&lt;/a&gt; API.&lt;/p&gt; &#xA;&lt;p&gt;The properties supported by this type of block (in addition to the common ones &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;shortDesc&lt;/code&gt; and &lt;code&gt;desc&lt;/code&gt;) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;format&lt;/code&gt; data source format&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;options&lt;/code&gt; data source options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt; data source path (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partitionBy&lt;/code&gt; partitioning column(s)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;outputMode&lt;/code&gt; see &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes&#34;&gt;output modes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trigger&lt;/code&gt; see &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers&#34;&gt;triggers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-hocon&#34;&gt;steps = [&#xA;{&#xA;    id = stream&#xA;    shortDesc = &#34;generate fake stream&#34;&#xA;    kind = input-stream&#xA;    format = rate&#xA;    options = {&#xA;        rowsPerSecond = &#34;2&#34;&#xA;    }&#xA;},&#xA;{&#xA;    id = randomCustomer&#xA;    shortDesc = &#34;load sample udf&#34;&#xA;    kind = udf&#xA;    claz = com.github.supermariolabs.spooq.udf.example.FakeCustomerUDF&#xA;},&#xA;{&#xA;    id = enriched&#xA;    shortDesc = &#34;enrich stream using sql and udf&#34;&#xA;    kind = sql&#xA;    sql = &#34;select customer.* from (select randomCustomer(value) customer from stream)&#34;&#xA;},&#xA;{&#xA;    id = outStream&#xA;    source = enriched&#xA;    shortDesc = &#34;stream dump&#34;&#xA;    kind = output-stream&#xA;    format = console&#xA;    outputMode = &#34;append&#34;&#xA;    trigger = {&#xA;                policy = &#34;processingTime&#34;&#xA;                value = &#34;10 seconds&#34;&#xA;    }&#xA;},&#xA;#...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Launch Parameters &lt;a id=&#34;referencedoc3&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The command parameters supported by the application are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-c&lt;/code&gt; or &lt;code&gt;--configuration-file&lt;/code&gt; configuration file to use (default: spooq.conf)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; or &lt;code&gt;--format&lt;/code&gt; configuration file format (hocon, conf, yaml, yml, json, default is inferred from filename extension)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-i&lt;/code&gt; or &lt;code&gt;--interactive&lt;/code&gt; interactive mode, after performing the steps defined in the configuration file opens a repl shell where you can execute sql commands and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-ts&lt;/code&gt; or &lt;code&gt;--thrift-server&lt;/code&gt; (&lt;strong&gt;experimental&lt;/strong&gt;) starts a thrift server on a port (default: 10001) for connection through third party clients&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-tp&lt;/code&gt; or &lt;code&gt;--thrift-port&lt;/code&gt; (&lt;strong&gt;experimental&lt;/strong&gt;) allows you to bind the thrift-server to a different port&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-v&lt;/code&gt; or &lt;code&gt;--verbose&lt;/code&gt; verbose mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-r&lt;/code&gt; or &lt;code&gt;--rich&lt;/code&gt; whether or not to use the rich ui (ansi) cli&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download &lt;a id=&#34;download&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1y57-YBmEyfLhxI2Rw4Rh3QOBeLAiOcah/view?usp=sharing&#34;&gt;Spooq 0.9.9beta Spark3 (scala 2.12)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1uIAP_6D_tuVU5WrEscVDwE6bj1lewQXp/view?usp=sharing&#34;&gt;Spooq 0.9.9beta Spark3 (scala 2.12) standalone&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/183PFqWqih5O2KR-hL74Kyq925-ocveoe/view?usp=sharing&#34;&gt;Spooq 0.9.9beta Spark2 (scala 2.11)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1pt01Gkx75xGuC53Du5IkyagZ1E9Pp7_C/view?usp=sharing&#34;&gt;Spooq 0.9.9beta Spark2 (scala 2.11) standalone&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to compile the code &lt;a id=&#34;howtocompile&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To compile and create a custom build you can use &lt;a href=&#34;https://www.scala-sbt.org&#34;&gt;sbt&lt;/a&gt;. In particular it is possible to compile the software with &lt;code&gt;Scala 2.11&lt;/code&gt; for &lt;code&gt;Spark 2.x&lt;/code&gt; and with &lt;code&gt;Scala 2.12&lt;/code&gt; for &lt;code&gt;Spark 3.x&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to build the software &lt;em&gt;with&lt;/em&gt; or &lt;em&gt;without&lt;/em&gt; the Spark embedded libraries (depending on whether you want to use it &lt;em&gt;standalone&lt;/em&gt; or on a cluster with &lt;code&gt;spark-submit&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# compile using Scala 2.11/Spark 2.x building with spark provided dependencies&#xA;$ sbt -Dbuild.spark.version=2 configString assembly&#xA;&#xA;# compile using Scala 2.12/Spark 3.x building fat jar (standalone)&#xA;$ sbt -Dbuild.spark.version=3 -Dstandalone=true configString assembly&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cookbook &lt;a id=&#34;cookbook&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt;</summary>
  </entry>
</feed>