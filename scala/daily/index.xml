<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-03T01:52:00Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rtyley/bfg-repo-cleaner</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/rtyley/bfg-repo-cleaner</id>
    <link href="https://github.com/rtyley/bfg-repo-cleaner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Removes large or troublesome blobs like git-filter-branch does, but faster. And written in Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BFG Repo-Cleaner &lt;a href=&#34;https://travis-ci.com/rtyley/bfg-repo-cleaner&#34;&gt;&lt;img src=&#34;https://travis-ci.com/rtyley/bfg-repo-cleaner.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Removes large or troublesome blobs like git-filter-branch does, but faster - and written in Scala&lt;/em&gt; - &lt;a href=&#34;https://j.mp/fund-bfg&#34;&gt;Fund the BFG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bfg --strip-blobs-bigger-than 1M --replace-text banned.txt repo.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The BFG is a simpler, faster (&lt;a href=&#34;https://docs.google.com/spreadsheet/ccc?key=0AsR1d5Zpes8HdER3VGU1a3dOcmVHMmtzT2dsS2xNenc&#34;&gt;10 - 720x&lt;/a&gt; faster) alternative to &lt;code&gt;git-filter-branch&lt;/code&gt; for cleansing bad data out of your Git repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removing &lt;strong&gt;Crazy Big Files&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Removing &lt;strong&gt;Passwords, Credentials&lt;/strong&gt; &amp;amp; other &lt;strong&gt;Private data&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main documentation for The BFG is here : &lt;strong&gt;&lt;a href=&#34;https://rtyley.github.io/bfg-repo-cleaner/&#34;&gt;https://rtyley.github.io/bfg-repo-cleaner/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>geekyouth/SZT-bigdata</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/geekyouth/SZT-bigdata</id>
    <link href="https://github.com/geekyouth/SZT-bigdata" rel="alternate"></link>
    <summary type="html">&lt;p&gt;深圳地铁大数据客流分析系统🚇🚄🌟&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SZT-bigdata 深圳地铁大数据客流分析系统 🚇🚇🚇&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/full-logo.png&#34; alt=&#34;logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/stargazers&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/network/members&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/watchers&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/releases&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/issues&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/raw/master/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://java666.cn&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/博客：-https://java666.cn-red?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code&gt;   ___     ____   _____           _         _      __ _      _             _&#xA;  / __|   |_  /  |_   _|   ___   | |__     (_)    / _` |  __| |   __ _    | |_    __ _&#xA;  \__ \    / /     | |    |___|  | &#39;_ \    | |    \__, | / _` |  / _` |   |  _|  / _` |&#xA;  |___/   /___|   _|_|_   _____  |_.__/   _|_|_   |___/  \__,_|  \__,_|   _\__|  \__,_|&#xA;_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|     |_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|&#xA;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;项目说明🚩：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🎈 该项目主要分析深圳通刷卡数据，通过大数据技术角度来研究深圳地铁客运能力，探索深圳地铁优化服务的方向；&lt;/li&gt; &#xA; &lt;li&gt;✨ 强调学以致用，本项目的原则是尽可能使用较多的常用技术框架，加深对各技术栈的理解和运用，在使用过程中体验各框架的差异和优劣，为以后的项目开发技术选型做基础；&lt;/li&gt; &#xA; &lt;li&gt;👑 解决同一个问题，可能有多种技术实现，实际的企业开发应当遵守最佳实践原则；&lt;/li&gt; &#xA; &lt;li&gt;🎉 学习过程优先选择较新的软件版本，因为新版踩坑一定比老版更多，坑踩的多了，技能也就提高了，遇到新问题可以见招拆招、对症下药；&lt;/li&gt; &#xA; &lt;li&gt;🚀 ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;第一期架构图&lt;/h2&gt; &#xA;&lt;p&gt;原图 &lt;a href=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/SZT-bigdata-2.png&#34;&gt;.file/.doc/SZT-bigdata-2.png&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/SZT-bigdata-2+.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;数字标记不分先后顺序，对应代码：&#xA;1-cn.java666.sztcommon.util.SZTData&#xA;2-cn.java666.etlflink.app.Jsons2Redis&#xA;3-cn.java666.etlspringboot.controller.RedisController#get&#xA;4-cn.java666.etlflink.app.Redis2ES&#xA;5-cn.java666.etlflink.app.Redis2Csv&#xA;6-Hive sql 脚本（开发维护成本最低）&#xA;7-Saprk 程序（开发维护成本最高，但是功能更强）&#xA;8-HUE 方便查询和展示 Hive 数据&#xA;9-cn.java666.etlflink.app.Redis2HBase&#xA;10、14-cn.java666.szthbase.controller.KafkaListen#sink2Hbase&#xA;11-cn.java666.etlflink.app.Redis2HBase&#xA;12-CDH HDFS+HUE+Hbase+Hive 一站式查询&#xA;13-cn.java666.etlflink.app.Redis2Kafka&#xA;15-cn.java666.sztflink.realtime.Kafka2MyCH&#xA;16-cn.java666.sztflink.realtime.sink.MyClickhouseSinkFun&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;下一步，计划开发数据湖中台解决方案&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;核心技术栈 + 版本选择 + 点评 (持续更新)⚡：&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/stack2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java-1.8/Scala-2.11，生态丰富，轮子够多；&lt;/li&gt; &#xA; &lt;li&gt;Flink-1.10，流式业务、ETL 首选。发展势头如日中天，阿里巴巴背书，轻快灵活、健步如飞；就问你信不信马云？？？😚😚😚&lt;/li&gt; &#xA; &lt;li&gt;Redis-3.2，天然去重，自动排序，除了快还是快。廉价版硬盘实现同类产品 SSDB。Win10|CentOS7|Docker Redis-3.2 三选一，CentOS REPL yum 安装默认使用3.2版本；&lt;/li&gt; &#xA; &lt;li&gt;Kafka-2.1，消息队列业务解耦、流量消峰、订阅发布场景首选。最佳 CP：kafka-eagle-1.4.5，集生产、消费、Ksql、大屏、监控、报警于一身，同时监控 zk。其他我用过的 Kafka 监控组件最后都放弃了： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;KafkaOffsetMonitor 问题太多，丑拒；&lt;/li&gt; &#xA;   &lt;li&gt;Kafka Manager，已更名为 CMAK，老外写的软件用起来就觉得很别扭，而且最高只兼容 Kafka 0.11，但是 Kafka 官方已经升级到 2.4 了啊喂；&lt;/li&gt; &#xA;   &lt;li&gt;其他各种开源的 Kafka 监控基本都试过，一个能打的都没有。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Zookeeper-3.4.5，集群基础依赖，选举时 ID 越大越优势，通过会话机制维护各组件在线状态；&lt;/li&gt; &#xA; &lt;li&gt;CDH-6.2，解决了程序员最难搞的软件兼容性问题，全家桶服务一键安装；&lt;/li&gt; &#xA; &lt;li&gt;Docker-19，最快速度部署一款新软件，无侵入、无污染、快速扩容、服务打包。如果当前没有合适的运行环境，那么 docker 一定是首选；&lt;/li&gt; &#xA; &lt;li&gt;SpringBoot-2.13，通用 JAVA 生态，敏捷开发必备；&lt;/li&gt; &#xA; &lt;li&gt;knife4j-2.0，前身为 swagger-bootstrap-ui，REST API 项目调试简直不要太方便，秒杀原版丝袜哥十个数量级；&lt;/li&gt; &#xA; &lt;li&gt;Elasticsearch-7，全文检索领域唯一靠谱的数据库，搜索引擎核心服务，亿级数据毫秒响应，真实时，坑也多🔊🔊🔊；&lt;/li&gt; &#xA; &lt;li&gt;Kibana-7.4，ELK 全家桶成员，前端可视化，小白也不怕；&lt;/li&gt; &#xA; &lt;li&gt;ClickHouse，家喻户晓的 nginx 服务器就是俄罗斯的代表作，接下来大红大紫的 clickhouse 同样身轻如燕，但是性能远超目前市面所有同类数据库，存储容量可达PB级别。目前资料还不多，正在学习中；&lt;/li&gt; &#xA; &lt;li&gt;MongoDB-4.0，文档数据库，对 Json 数据比较友好，主要用于爬虫数据库；&lt;/li&gt; &#xA; &lt;li&gt;Spark-2.3，目前国内大数据框架实时微批处理、离线批处理主流方案。这个组件太吃资源了，曾经在我开发时，把我的笔记本搞到蓝屏，于是我直接远程提交到 spark 集群了。接下来预计 Flink 开始表演了🦘，真的用了更快的框架就爱上了😍😍😍；&lt;/li&gt; &#xA; &lt;li&gt;Hive-2.1，Hadoop 生态数仓必备，大数据离线处理 OLAP 结构化数据库，准确来说是个 HQL 解析器，查询语法接近 Mysql，就是窗口函数比较复杂😭😭😭；&lt;/li&gt; &#xA; &lt;li&gt;Impala-3.2，像羚羊一样轻快矫健，同样的 hive sql 复杂查询，impala 毫秒级返回，hive 却需要80秒左右甚至更多；&lt;/li&gt; &#xA; &lt;li&gt;HBase-2.1 + Phoenix，Hadoop 生态下的非结构化数据库，HBase 的灵魂设计就是 rowkey 和多版本控制，凤凰嫁接 hbase 可以实现更复杂的业务；&lt;/li&gt; &#xA; &lt;li&gt;Kylin-2.5，麒麟多维预分析系统，依赖内存快速计算，但是局限性有点多啊，适用于业务特别稳定，纬度固定少变的场景，渣渣机器就别试了，内存太小带不起；&lt;/li&gt; &#xA; &lt;li&gt;HUE-4.3，CDH 全家桶赠送的，强调用户体验，操作数仓很方便，权限控制、hive + impala 查询、hdfs 文件管理、oozie 任务调度脚本编写全靠他了；&lt;/li&gt; &#xA; &lt;li&gt;阿里巴巴 DataX，异构数据源同步工具，主持大部分主流数据库，甚至可以自己开发插件，马云家的东西，我选你！！！如果你觉得这还满足不了你的特殊业务需求，那么推荐你用 FlinkX，基于 Flink 的分布式数据同步工具。理论上你也可以自己开发插件；&lt;/li&gt; &#xA; &lt;li&gt;Oozie-5.1，本身 UI 奇丑，但是配合 HUE 食用尚可接受，主要用来编写和运行任务调度脚本；&lt;/li&gt; &#xA; &lt;li&gt;Sqoop-1.4，主要用来从 Mysql 导出业务数据到 HDFS 数仓，反过来也行；&lt;/li&gt; &#xA; &lt;li&gt;Mysql-5.7，程序员都要用的吧，如果说全世界程序员都会用的语言，那一定是 SQL。Mysql 8.0 普及率不够高，MariaDB 暂不推荐，复杂的函数不兼容 Mysql，数据库这么基础的依赖组件出了问题你就哭吧；&lt;/li&gt; &#xA; &lt;li&gt;Hadoop3.0（HDFS+Yarn），HDFS 是目前大数据领域最主流的分布式海量数据存储系统，这里的 Yarn 特指 hadoop 生态，主要用来分配集群资源，自带执行引擎 MR；&lt;/li&gt; &#xA; &lt;li&gt;阿里巴巴 DataV 可视化展示；&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;我发现越来越多的国产开源软件用户体验值得肯定。。。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;准备工作🍬：&lt;/h2&gt; &#xA;&lt;p&gt;以下是我的开发环境，仅作参考：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Win10 IDEA 2019.3 旗舰版，JAVA|Scala 开发必备，集万般功能于一身；&lt;/li&gt; &#xA; &lt;li&gt;Win10 DBeaver 企业版 6.3，秒杀全宇宙所有数据库客户端，几乎一切常用数据库都可以连，选好驱动是关键；&lt;/li&gt; &#xA; &lt;li&gt;Win10 Sublime Text3，地表最强轻量级编辑器，光速启动，无限量插件，主要用来编辑零散文件、markdown 实时预览、写前端特别友好（虽然我不擅长🖐🖐🖐），速度快到完全不用担心软件跟不上你的手速；&lt;/li&gt; &#xA; &lt;li&gt;其他一些实用工具参考我的博客：&lt;a href=&#34;https://java666.cn/#/AboutMe&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://java666.cn/#/AboutMe&#34;&gt;https://java666.cn/#/AboutMe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CentOS7 CDH-6.2 集群，包含如下组件，对应的主机角色和配置如图，集群至少需要40 GB 总内存，才可以满足基本使用，不差钱的前提下，RAM 当然是合理范围内越大越好啦，鲁迅都说“天下武功唯快不破”；我们的追求是越快越好；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-view.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-host.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-role.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你选用原版 Apache 组件搭建大数据集群，那么你会有踩不完的坑。我的头发不够掉了，所以我选 CDH！！！⚙🛠😏😏😏&lt;/p&gt; &#xA;&lt;h2&gt;物理机配置💎：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;以上软件分开部署在我的三台电脑上，Win10 笔记本 VMware + Win10 台式机 VMware + 古董笔记本 CentOS7。物理机全都配置 SSD + 千兆以太网卡，HDFS 需要最快的网卡。好马配好鞍，当然你得有个千兆交换机配合千兆网线，木桶原理警告！！！🎈🎈🎈&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;有个机架当然再好不过了，哈哈哈。。。 &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-pcs.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果你想避免网线牵来牵去，可以采用电力猫实现分布式家庭组网方案；&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;数据源🌍：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;深圳市政府数据开放平台，深圳通刷卡数据 133.7 万条【离线数据】貌似已经停止服务😒：&lt;br&gt; &lt;a href=&#34;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&#34;&gt;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;备用数据源(之前上传的一批 jsons 数据有些纰漏，于是重新整理压缩后放到本仓库中，速度慢的同学可以尝试码云 &lt;a href=&#34;https://gitee.com/geekyouth/SZT-bigdata&#34;&gt;https://gitee.com/geekyouth/SZT-bigdata&lt;/a&gt; )：&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/2018record3.zip&#34;&gt;.file/2018record3.zip&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;理论上可以当作实时数据，但是这个接口响应太慢了，如果采用 kafka 队列方式，也可以模拟出实时效果。&lt;/p&gt; &#xA;&lt;p&gt;本项目采用离线 + 实时思路 多种方案处理。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;开发进度🥇：&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;准备好 java、scala、大数据开发常用的环境，比如 IDEA、VMware 虚拟机、CDH等，然后手机静音盖上，跟我一起左手画个龙，右手划一道彩虹，开始表演吧🤪&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1- 获取数据源的 appKey：&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2- 代码开发：&lt;/h3&gt; &#xA;&lt;h4&gt;2.1- 调用 &lt;code&gt;cn.java666.etlspringboot.source.SZTData#saveData&lt;/code&gt; 获取原始数据存盘 &lt;code&gt;/tmp/szt-data/szt-data-page.jsons&lt;/code&gt;，核对数据量 1337，注意这里每条数据包含1000条子数据；&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.2- 调用 &lt;code&gt;cn.java666.etlflink.sink.RedisSinkPageJson#main&lt;/code&gt; 实现 etl 清洗，去除重复数据，redis 天然去重排序，保证数据干净有序，跑完后核对 redis 数据量 1337。&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.3- redis 查询，redis-cli 登录后执行 &lt;code&gt;hget szt:pageJson 1&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;或者 dbeaver 可视化查询：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/redis-szt-pageJson.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.4- &lt;code&gt;cn.java666.etlspringboot.EtlSApp#main&lt;/code&gt; 启动后，也可以用 knife4j 在线调试 REST API：&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/api-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/api-debug.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.5- &lt;code&gt;cn.java666.etlflink.source.MyRedisSourceFun#run&lt;/code&gt; 清洗数据发现 133.7 万数据中，有小部分源数据字段数为9，缺少两个字段：station、car_no；丢弃脏数据。&lt;/h4&gt; &#xA;&lt;p&gt;合格源数据示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;deal_date&#34;: &#34;2018-08-31 21:15:55&#34;,&#xA;&#x9;&#34;close_date&#34;: &#34;2018-09-01 00:00:00&#34;,&#xA;&#x9;&#34;card_no&#34;: &#34;CBHGDEEJB&#34;,&#xA;&#x9;&#34;deal_value&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_type&#34;: &#34;地铁入站&#34;,&#xA;&#x9;&#34;company_name&#34;: &#34;地铁五号线&#34;,&#xA;&#x9;&#34;car_no&#34;: &#34;IGT-104&#34;,&#xA;&#x9;&#34;station&#34;: &#34;布吉&#34;,&#xA;&#x9;&#34;conn_mark&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_money&#34;: &#34;0&#34;,&#xA;&#x9;&#34;equ_no&#34;: &#34;263032104&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;不合格的源数据示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;deal_date&#34;: &#34;2018-09-01 05:24:22&#34;,&#xA;&#x9;&#34;close_date&#34;: &#34;2018-09-01 00:00:00&#34;,&#xA;&#x9;&#34;card_no&#34;: &#34;HHAAABGEH&#34;,&#xA;&#x9;&#34;deal_value&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_type&#34;: &#34;地铁入站&#34;,&#xA;&#x9;&#34;company_name&#34;: &#34;地铁一号线&#34;,&#xA;&#x9;&#34;conn_mark&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_money&#34;: &#34;0&#34;,&#xA;&#x9;&#34;equ_no&#34;: &#34;268005140&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.6- &lt;code&gt;cn.java666.etlflink.app.Redis2Kafka#main&lt;/code&gt; 根据需求推送满足业务要求的源数据到 kafka，&lt;code&gt;topic-flink-szt-all&lt;/code&gt; 保留了所有源数据 1337000 条， &lt;code&gt;topic-flink-szt&lt;/code&gt; 仅包含清洗合格的源数据 1266039 条。&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.7- kafka-eagle 监控查看 topic，基于原版去掉了背景图，漂亮多了：&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka-eagle02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka-eagle01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ksql 命令查询： &lt;code&gt;select * from &#34;topic-flink-szt&#34; where &#34;partition&#34; in (0) limit 1000&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/ksql.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.8- &lt;code&gt;cn.java666.etlflink.app.Redis2Csv#main&lt;/code&gt; 实现了 flink sink csv 格式文件，并且支持按天分块保存。&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/csv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.9- &lt;code&gt;cn.java666.etlflink.app.Redis2ES#main&lt;/code&gt; 实现了 ES 存储源数据。实现实时全文检索，实时跟踪深圳通刷卡数据。&lt;/h4&gt; &#xA;&lt;p&gt;这个模块涉及技术细节比较多，如果没有 ES 使用经验，可以先做下功课，不然的话会很懵。&lt;/p&gt; &#xA;&lt;p&gt;我之前在处理 ES 各种问题踩了不少坑，熬了不少通宵，掉了很多头发。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;遇到问题心态要稳，因为你今天处理了一个问题，明天接触新的版本新的框架大概率又会出现新的问题&lt;/strong&gt;。。🥺🥺🥺&lt;/p&gt; &#xA;&lt;p&gt;所以最佳实践很重要！！！&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;👇👇👇这部分内容有更新：修正了上一个版本时区问题。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;🎬接下来，让我们时光倒流，回到 2018-09-01这一天，调整 kibana 面板时间范围 &lt;code&gt;2018-09-01 00:00:00.000~2018-09-01 23:59:59.999&lt;/code&gt;，看看当天深圳通刷卡记录的统计图曲线走向是否科学，间接验证数据源的完整性。&lt;/p&gt; &#xA;&lt;p&gt;修正时区后统计数量，字段完整的合格源数据 1266039 条，2018-09-01全天 1229180 条。&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/2018-09-01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;图中可以看出 2018-09-01 这一天刷卡记录集中在上午6点~12点之间，早高峰数据比较吻合，虽然这一天是周六，高峰期不是特别明显。我们继续缩放 kibana 时间轴看看更详细的曲线： &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/2018-09-01-am.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;回顾一下本项目 ETL 处理流程：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;1337000 条源数据清洗去除字段不全的脏数据，剩余的合格数据条数 1266039 已经进入 ES 索引 &lt;code&gt;szt-data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;在 1266039 条合格数据中，有 1227234 条数据集中在 2018-09-01 这一天的上午时段；&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;我们暂且相信上午时段的数据是真实的，那么是否说明官方提供的数据并不是全部的当天完整刷卡数据？？？&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果按照上午的刷卡量来估测全天的刷卡量，考虑到是周六，那么深圳通全天的刷卡记录数据应该在 122万 X 2 左右，当然这么武断的判断方式不是程序员的风格，接下来我们用科学的大数据分析方式来研究这些数据背后的意义。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;注意，ES 大坑：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ES 存数据时，带有时间字段的数据如何实时展示到 kibana 的图表面板上？&lt;br&gt; 🤣需要在存入 index 之前设置字段映射。参考格式，不要照抄！！！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;properties&#34;: {&#xA;&#x9;&#34;deal_date&#34;: {&#xA;&#x9;  &#34;format&#34;: &#34;yyyy-MM-dd HH:mm:ss&#34;,&#xA;&#x9;  &#34;type&#34;: &#34;date&#34;&#xA;&#x9;}&#xA;  }&#xA;}  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这里并没有指定时区信息，但是 ES 默认使用 0 时区，这个软件很坑，无法设置全局默认时区。但是很多软件产生的数据都是默认机器所在时区，国内就是东八区。因为我们的源始数据本身也没有包含时区信息，这里我不想改源数据，那就假装自己在 ES 的 0 时区。同时需要修改 kibana 默认时区为 UTC，才可以保证 kibana 索引图表时间轴正确对位。不过这并不是一个科学的解决方案。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果是企业项目，必须要用数据质量监控软件！！！要不然得有多少背锅侠要杀去祭天😂😂😂，数据可以没有但是千万不能错。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ES 存数据时，需要使用 json 格式包装数据，不符合json 语法的纯字符无法保存；&lt;/li&gt; &#xA; &lt;li&gt;ES 序列化复杂的 bean 对象时，如果 fastjson 报错，推荐使用 Gson，很强！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TIPS😙😙😙：&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gson 相比 fastjson：Gson 序列化能力更强，但是 反序列化时，fastjson 速度更快。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.10- 查看 ES 数据库卡号，对比自己的深圳通地铁卡，逐渐发现了一些脱敏规律。&lt;/h4&gt; &#xA;&lt;p&gt;日志当中卡号脱敏字段密文反解猜想：&lt;br&gt; 由脱敏的密文卡号反推真实卡号，因为所有卡号密文当中没有J开头的数据， 但是有A开头的数据，A != 0，而且出现了 BCDEFGHIJ 没有 K，所以猜想卡号映射关系如图！！！&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/parse_card_no.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; 类似摩斯电码解密。。。我现在还不确定这个解密方式是否正确🙄🙄🙄&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.11- &lt;code&gt;cn.java666.sztcommon.util.ParseCardNo#parse&lt;/code&gt; 实现了支持自动识别卡号明文和密文、一键互转功能。 &lt;code&gt;cn.java666.etlspringboot.controller.CardController#get&lt;/code&gt; 实现了卡号明文和密文互转 REST API。&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/parse_no.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3- 搭建数仓：深圳地铁数仓建模&lt;/h3&gt; &#xA;&lt;h4&gt;3.1- 第一步，分析业务&lt;/h4&gt; &#xA;&lt;p&gt;确定业务流程 ---&amp;gt; 声明粒度 ---&amp;gt; 确定维度 ---&amp;gt; 确定事实&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/dim.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2- 第二步，规划数仓结构&lt;/h4&gt; &#xA;&lt;p&gt;参考行业通用的数仓分层模式：ODS、DWD、DWS、ADS，虽然原始数据很简单，但是我们依然使用规范的流程设计数据仓库。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第一层：ODS 原始数据层&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ods/ods_szt_data/day=2018-09-01/   &#xA;# szt_szt_page/day=2018-09-01/  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第二层：DWD 清洗降维层&lt;br&gt; 区分维表 dim_ 和事实表 fact_，为了使粒度更加细化，我们把进站和出站记录分开，巴士数据暂不考虑。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;dwd_fact_szt_in_detail      进站事实详情表&#xA;dwd_fact_szt_out_detail     出站事实详情表&#xA;dwd_fact_szt_in_out_detail  地铁进出站总表&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第三层：DWS 宽表层&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;dws_card_record_day_wide  每卡每日行程记录宽表【单卡单日所有出行记录】&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第四层：ADS 业务指标层【待补充】&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;【体现进站压力】 每站进站人次排行榜      &#xA;&#x9;ads_in_station_day_top&#xA;【体现出站压力】 每站出站人次排行榜      &#xA;&#x9;ads_out_station_day_top&#xA;【体现进出站压力】 每站进出站人次排行榜      &#xA;&#x9;ads_in_out_station_day_top&#xA;【体现通勤车费最多】 每卡日消费排行      &#xA;&#x9;ads_card_deal_day_top  &#xA;【体现线路运输贡献度】 每线路单日运输乘客总次数排行榜，进站算一次，出站并且联程算一次     &#xA;&#x9;ads_line_send_passengers_day_top  &#xA;【体现利用率最高的车站区间】 每日运输乘客最多的车站区间排行榜       &#xA;&#x9;ads_stations_send_passengers_day_top&#xA;【体现线路的平均通勤时间，运输效率】 每条线路单程直达乘客耗时平均值排行榜     &#xA;&#x9;ads_line_single_ride_average_time_day_top&#xA;【体现深圳地铁全市乘客平均通勤时间】 所有乘客从上车到下车间隔时间平均值    &#xA;&#x9;ads_all_passengers_single_ride_spend_time_average&#xA;【体现通勤时间最长的乘客】 单日从上车到下车间隔时间排行榜     &#xA;&#x9;ads_passenger_spend_time_day_top&#xA;【体现车站配置】 每个站点进出站闸机数量排行榜&#xA;&#x9;每个站点入站闸机数量  &#x9;&#x9;ads_station_in_equ_num_top&#xA;&#x9;每个站点出站闸机数量    &#x9;&#x9;ads_station_out_equ_num_top&#xA;【体现各线路综合服务水平】 各线路进出站闸机数排行榜&#xA;&#x9;各线路进站闸机数排行榜 &#x9;&#x9;ads_line_in_equ_num_top.png&#xA;&#x9;各线路出站闸机数排行榜 &#x9;&#x9;ads_line_out_equ_num_top&#xA;【体现收入最多的车站】 出站交易收入排行榜   &#xA;&#x9;ads_station_deal_day_top&#xA;【体现收入最多的线路】 出站交易所在线路收入排行榜   &#xA;&#x9;ads_line_deal_day_top&#xA;【体现换乘比例、乘车体验】 每天每线路换乘出站乘客百分比排行榜  &#xA;&#x9;ads_conn_ratio_day_top&#xA;【体现每条线的深圳通乘车卡普及程度 9.5 折优惠】 出站交易优惠人数百分比排行榜     &#xA;&#x9;ads_line_sale_ratio_top&#xA;【体现换乘的心酸】 换乘耗时最久的乘客排行榜&#x9;&#xA;&#x9;ads_conn_spend_time_top&#xA;【体现线路拥挤程度】 上车以后还没下车，每分钟、小时每条线在线人数   &#xA;&#x9;ads_on_line_min_top&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.3- 第三步：建库建表计算指标&lt;/h4&gt; &#xA;&lt;p&gt;hdfs 关闭权限检查。hive 设置保存目录 /warehouse；&lt;br&gt; hue 创建 hue 用户，赋予超级组。hue 切换到 hue 用户，执行 hive sql 建库 szt；&lt;br&gt; 库下面建目录 ods dwd dws ads；&lt;/p&gt; &#xA;&lt;p&gt;上传原始数据到 /warehouse/szt.db/ods/&lt;br&gt; szt-etl-data.csv szt-etl-data_2018-09-01.csv szt-page.jsons&lt;/p&gt; &#xA;&lt;p&gt;查看： &lt;code&gt;hdfs dfs -ls -h hdfs://cdh231:8020/warehouse/szt.db/ods/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;接下来使用 HUE 按照 &lt;code&gt;sql/hive.sql&lt;/code&gt; 依次执行 HQL 语句.....&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;也可以使用 IDEA Database 工具栏操作，附送idea cdh hive 完美驱动 &lt;a href=&#34;https://github.com/timveil/hive-jdbc-uber-jar/releases&#34;&gt;https://github.com/timveil/hive-jdbc-uber-jar/releases&lt;/a&gt;：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/idea-dev+hive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;也可以使用 DBeaver （我只想说， 上古产品 Sqlyog、navicat、heidisql、workbench 全都是战五渣），因为有时候复杂的查询可以一边执行一边在另一个客户端工具查看结果，这对于复杂的嵌套查询 debug 非常有助于分析和跟踪问题。DBeaver 客户端自带图表，不过没有 HUE 好看：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/dbeaver-dev+hive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;已经完成的指标分析：&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.1 - 深圳地铁进站人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天依次为：五和、布吉、丹竹头，数据说明当天这几个站点进站人数最多。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_station_day_top2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.2 - 深圳地铁出站人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天出站乘客主要去向分别为：深圳北高铁站、罗湖火车站、福田口岸。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_out_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_out_station_day_top2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.3- 深圳地铁进出站总人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天车站吞吐量排行榜：&lt;br&gt; 五和站？？？、布吉站（深圳东火车站）、罗湖站（深圳火车站）、深圳北（深圳北高铁站）。。。&lt;br&gt; 五和站为什么这么秀？？？ 🚀&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_out_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.4- 深圳地铁乘客车费排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天车费最高的乘客花了 48 元人民币&lt;br&gt; 🚄🚄🚄 说明：深圳通地铁卡不记名，未涉及个人隐私！！！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_card_deal_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.5- 深圳地铁各线路单日发送旅客排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天五号线客运量遥遥领先，龙岗线碾压一号线，心疼龙岗人民！😳&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_send_passengers_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.6- 深圳地铁每日运输乘客最多的区间排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01当天前三名分别是：赤尾&amp;gt;华强北，福民&amp;gt;福田口岸，五和&amp;gt;深圳北&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_stations_send.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.7- 深圳地铁每条线路单程直达乘客耗时平均值排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天五号线单程直达乘客平均耗时1500s，约合25分钟，平均值最长的是 11号线，平均耗时 40 分钟&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_single_ride_average_time_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.8- 深圳地铁所有乘客通勤时间平均值：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天所有乘客通勤时间平均值 1791 s，约合 30 分钟&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_all_passengers_single_ride_spend_time_average.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.9- 深圳地铁所有乘客通勤时间排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天所有乘客通勤时间排行榜，站内滞留最久的乘客间隔 17123 秒，约合 4.75 小时，实际情况只需要 20 分钟车程，难道是进站搞事情？？？&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kibana-search-card-1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/baiduMap1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_passenger_spend_time_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.10- 深圳地铁每个站点进出站闸机数量排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天福田站双项第一&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_in_equ_num_top.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_out_equ_num_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.11- 深圳地铁各线路进出站闸机数量排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天深圳地铁一号线长脸了@_@，两个指标都是第一，港铁四号线全部垫底，后妈养的？？？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_in_equ_num_top.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_out_equ_num_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.12- 深圳地铁各站收入排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天上午深圳北站收入 4 万元人民币，排名第一&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_deal_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.12- 深圳地铁各线路收入排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，数据显示一号线依然是深圳地铁最多收入的线路，1号线上午收入 30 万元人民币，其次是五号线紧随其后&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_deal_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.13- 深圳地铁各线路换乘出站乘客百分比排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;换乘后从五号线出来的乘客是占比最高的 15.6%，从九号线出站的乘客，换乘比例最低，仅 9.42%&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_conn_ratio_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.14- 深圳地铁各线路直达乘客优惠人次百分比排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;目前可以确定的是，持有深圳通地铁卡可以享受9.5折优惠乘坐地铁，从统计结果看，2018-09-01当天，七号线使用地铁卡优惠的乘客人次占比最高，达到 90.36%，排名最低的是五号线，占比 84.3%&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_sale_ratio_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.15- 深圳地铁换乘时间最久的乘客排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;统计过程发现难以理解的现象，有几个乘客进站以后，没有刷卡出站就换乘了公交车，于是出现了同一个地铁站进出站，但是标记为联程的记录&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/WTF.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_conn_spend_time_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4- 新增模块：SZT-kafka-hbase&lt;/h3&gt; &#xA;&lt;p&gt;SZT-kafka-hbase project for Spring Boot2&lt;br&gt; 看过开源的 spring-boot-starter-hbase、spring-data-hadoop-hbase，基础依赖过于老旧，长期不更新；引入过程繁琐，而且 API 粒度受限；数据库连接没有复用，导致数据库服务读写成本太高。&lt;/p&gt; &#xA;&lt;p&gt;于是自己实现了 hbase-2.1 + springboot-2.1.13 + kafka-2.0 的集成，一个长会话完成 hbase 连续的增删改查👑👑👑，降低服务器资源的开销。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase666.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;主要特色：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;knife4j 在线调试，点击鼠标即可完成 hbase 写入和查询，再也不用记住繁琐的命令😏😏😏。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;hbase 列族版本历史设置为 10，支持配置文件级别的修改。可以查询某卡号最近 10 次交易记录。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;hbase rowkey 设计为卡号反转，使得字典排序过程消耗的服务器算力在分布式环境更加均衡。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;全自动的建库建表【本项目的 hbase 命名空间为 szt】，实现幂等操作，无需担心 hbase 数据库的污染。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;效果展示：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;准备部署完成的 hbase，适当修改本项目配置文件，运行 SZT-kafka-hbase 项目，效果如下：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;启动：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-run.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;api-debug，随便写点东西进去，狂点发送。能写多快就考验你的手速了😏😏😏：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-api-debug.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hue-hbase 查表：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hue-hbase-szt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hue-hbase 查看历史版本：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hue-hbase-szt-versions-10.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hbase-shell 命令：&lt;br&gt; 全表扫描，返回十个版本格式化为字符串显示，压榨服务器性能的时候到啦！！！😝😝😝&lt;br&gt; &lt;code&gt;scan &#39;szt:data&#39;, {FORMATTER =&amp;gt; &#39;toString&#39;,VERSIONS=&amp;gt;10}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-shell-toString.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;接下来接入 kafka 🎯🎯🎯&lt;br&gt; 启动 &lt;code&gt;cn.java666.etlflink.app.Redis2Kafka&lt;/code&gt;，生产消息，适当调慢生产速度，以免机器崩溃。&lt;br&gt; 不出意外的话，你会看到 SZT-kafka-hbase 项目的控制台打印了日志：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka2hbase.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果 hbase 崩溃了，看看内存够不够，我就直接怼上 2GB X 3 个节点🌟🌟🌟：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-2GB.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5- &lt;code&gt;SZT-flink&lt;/code&gt; 模块新增 &lt;code&gt;cn.java666.etlflink.app.Json2HBase&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;实现了从 redis 或者其他数据源取出 json 串，保存到 hbase 表。本项目中从 redis 获取 json（当然更推荐 kafka），通过 flink 清洗存到 hbase flink:flink2hbase 表中。用于实时保存深圳通刷卡记录，通过卡号查询可以获取卡号最近10次（如果有10次）交易记录。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/flink2hbase.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;简化了上一版 hbase 写入 bean 的方式，JSON 再一次赢得掌声😏😏😏。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val keys = jsonObj.keySet().toList&#xA;val size = keys.size()&#xA;&#xA;for (i &amp;lt;- 0 until size) {&#xA;&#x9;val key = keys.get(i)&#xA;&#x9;val value = jsonObj.getStr(key)&#xA;&#x9;putCell(card_no_re, cf, key, value)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6- 新增实时处理模块 SZT-flink&lt;/h3&gt; &#xA;&lt;p&gt;完成 flink 读取 kafka，存到 clickhouse 功能。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/clickhouse-tabix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/clickhouse-sql.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;...继续开发中🛠🛠🛠...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO🔔🔔🔔:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 解析 redis pageJson，转换数据格式为最小数据单元存到 csv，减少原始数据的冗余字符，方便存取和传输。丰富数据源的格式，兼容更多的实现方案；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 推送 kafka，使用队列传输数据；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 存入 elasticsearch，使用全文检索实现实时搜索，kibana 可视化展示；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 数仓建模：ODS、DWD、DWS、ADS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; hive on spark 数仓建模、分析计算；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; spark on hive，本地开发 spark 程序，操作远程 hive 数据库；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 刷卡记录实时写入 hbase，支持最近交易记录的查询；&lt;/li&gt; &#xA; &lt;li&gt;[-] &lt;del&gt;oozie 调度，数据太少啊 嘤嘤嘤&lt;/del&gt;😮😮😮;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实时思路分析数据：flink 流式实时分析早晚高峰站点压力排行；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 离线思路分析数据：spark 微批处理；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DataV 可视化大屏展示；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;更新日志🌥：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-05-28:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新 fastjson，修复高危漏洞&lt;/li&gt; &#xA;   &lt;li&gt;格式化代码，使用空格替换制表符&lt;/li&gt; &#xA;   &lt;li&gt;添加 “反996”、apache-2.0 开源许可证&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-25：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;flink 实时流处理功能部分上线。完成 flink 读取 kafka，存到 clickhouse 模块；&lt;/li&gt; &#xA;   &lt;li&gt;补充第一期开发计划架构图；&lt;/li&gt; &#xA;   &lt;li&gt;下一步，计划开发数据湖中台解决方案，规模比较大。目前这个项目已经初现雏形，短期内以维护和优化为主【原则就是先上线后迭代】；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-22:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;补充第一期开发计划的架构图，帮助理解整个业务流程；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-14：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;RedisSinkPageJson&lt;/code&gt; 从 &lt;code&gt;package cn.java666.etlflink.sink&lt;/code&gt; 移到 &lt;code&gt;package cn.java666.etlflink.app&lt;/code&gt; 更名为 &lt;code&gt;Jsons2Redis&lt;/code&gt;，方便归类，该模块用于解析原始数据多行json到redis；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-01：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;实现了从 redis 或者其他数据源取出 json 串，保存到 hbase 表；&lt;/li&gt; &#xA;   &lt;li&gt;实现了 hbase-2.1 + springboot-2.1.13 + kafka-2.0 的集成；&lt;/li&gt; &#xA;   &lt;li&gt;实时消费 kafka 消息存到 hbase 数据库，支持实时查询某卡号最近 n 次交易记录；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-30：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;实现了 hbase-2.1 + springboot-2.1.13 的集成，一个长会话完成 hbase 连续的增删改查👑👑👑，降低服务器资源的开销。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-27：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;彻底的解决了静态资源无法热部署的问题；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;&#x9;&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;&#xA;&#x9;&amp;lt;artifactId&amp;gt;spring-boot-devtools&amp;lt;/artifactId&amp;gt;&#xA;&#x9;&amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&#xA;&#x9;&amp;lt;optional&amp;gt;true&amp;lt;/optional&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&#xA;######################### 实时热部署 ###################################&#xA;#&#34;关闭缓存, 即时刷新&#34;&#xA;spring.freemarker.cache=false&#xA;spring.thymeleaf.cache=false&#xA;&#xA;#热部署生效&#xA;spring.devtools.restart.enabled=true&#xA;#是否支持livereload&#xA;spring.devtools.livereload.enabled=true&#xA;#设置重启的目录,添加那个目录的文件需要restart&#xA;spring.devtools.restart.additional-paths=src/main/*&#xA;#设置不需要重启的目录&#xA;#spring.devtools.restart.exclude=static/**,public/**&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;202-04-27: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成所有线路规划+换乘方案的抓取入库，合计 45932 条；&lt;/li&gt; &#xA;   &lt;li&gt;解决了 hive 注释乱码问题；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;&#xA;alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;&#xA;alter table PARTITION_PARAMS  modify column PARAM_VALUE varchar(4000) character set utf8;&#xA;alter table PARTITION_KEYS  modify column PKEY_COMMENT varchar(4000) character set utf8;&#xA;alter table  INDEX_PARAMS  modify column PARAM_VALUE  varchar(4000) character set utf8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-24：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路换乘出站乘客百分比排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路直达乘客优惠人次百分比排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-23：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路单程直达乘客耗时平均值排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁所有乘客通勤时间平均值；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁所有乘客通勤时间排行榜（倒序）；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各站点、线路，进站、出站闸机数排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各站点、线路，收入排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-22：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新文档；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：每日运输乘客最多的区间排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-21:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增模块：SZT-spark-hive，本地开发 spark 程序，操作远程 Hive 数据库；&lt;/li&gt; &#xA;   &lt;li&gt;Debug：spark on hive 本地开发，远程提交 yarn 踩坑，主要是为了缓解开发主机的压力；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-20：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新项目文档；&lt;/li&gt; &#xA;   &lt;li&gt;自制项目 logo；&lt;/li&gt; &#xA;   &lt;li&gt;继续写 SQL 计算新指标，本打算切到 hive 3.1 使用 TEZ 引擎，但是 hive on spark 速度已经很给力了，至少是 MR 引擎的 10 倍速度，先用着；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-19：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;vmware 虚拟机扩容时误删系统文件&lt;code&gt;rm -rf /usr/&lt;/code&gt; 🥵，好在 HDFS、Kafka、ES 自带副本机制，而且大部分业务数据都是挂载到外部磁盘，所以重要数据和组件日志基本没丢。cdh 集群添加了新的节点；&lt;/li&gt; &#xA;   &lt;li&gt;恢复工作环境，从 hive on MR 切换到 hive on spark；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-18：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;规划数仓，搭建数仓环境；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-17&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;修正错别字；&lt;/li&gt; &#xA;   &lt;li&gt;发布v0.12;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-16&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;重构项目；&lt;/li&gt; &#xA;   &lt;li&gt;补充文档&lt;/li&gt; &#xA;   &lt;li&gt;发布v0.1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-15&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;增加 common 模块，拆分解耦；&lt;/li&gt; &#xA;   &lt;li&gt;支持自动识别卡号明文和密文，一键互转，提供 REST API；&lt;/li&gt; &#xA;   &lt;li&gt;修复 ES 时区导致的错误统计数量；&lt;/li&gt; &#xA;   &lt;li&gt;Redis2Csv 实现了按天转换 csv 存盘；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-14&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;重构；&lt;/li&gt; &#xA;   &lt;li&gt;完成 csv 格式文件的抽取；&lt;/li&gt; &#xA;   &lt;li&gt;添加 GPL-3 开源证书，鼓励开源分发；&lt;/li&gt; &#xA;   &lt;li&gt;添加徽标；&lt;/li&gt; &#xA;   &lt;li&gt;完成写入 ES 数据库，添加时间映射,kibana 实时查看刷卡数据统计曲线的变化；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-13&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;项目初始化；&lt;/li&gt; &#xA;   &lt;li&gt;完成数据源清洗去重，存到 redis；&lt;/li&gt; &#xA;   &lt;li&gt;完成 redis 查询 REST API 的开发；&lt;/li&gt; &#xA;   &lt;li&gt;完成 flink 自定义 source redis 的开发，并且更细粒度清洗源数据；&lt;/li&gt; &#xA;   &lt;li&gt;完成 推送源数据到 kafka；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;联系😪：&lt;/h2&gt; &#xA;&lt;p&gt;欢迎交流技术，接头暗号&lt;code&gt;github&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-wexin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;百度和谷歌能找到的问题就不要再问了！很累的😕😕😕&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;补充💌💌💌：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;不开小密圈；&lt;/li&gt; &#xA; &lt;li&gt;不卖课、不卖教程；&lt;/li&gt; &#xA; &lt;li&gt;不求赞，不求粉；&lt;/li&gt; &#xA; &lt;li&gt;不发广告、不骚扰；&lt;/li&gt; &#xA; &lt;li&gt;不割韭菜&lt;/li&gt; &#xA; &lt;li&gt;不恰饭&lt;/li&gt; &#xA; &lt;li&gt;偶尔发点视频教程&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;坚持原则和底线。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;比心🤞🤞🤞&lt;/p&gt; &#xA;&lt;h2&gt;吐个槽🍦🍦🍦：&lt;/h2&gt; &#xA;&lt;p&gt;程序员这辈子一定会遇到的三个问题：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;乱码问题🌚；&lt;/li&gt; &#xA; &lt;li&gt;时区不一致问题🌗；&lt;/li&gt; &#xA; &lt;li&gt;软件版本不兼容问题❄；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;教训：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;大数据程序员千万不能生产错误的数据，容忍程序运行失败、甚至没有输出数据，失败了可以跟踪原因，至少不会有脏数据。&lt;/li&gt; &#xA; &lt;li&gt;一旦数据错误，会影响后面的所有计算流程，甚至导致错误决策。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;统计信息：&lt;/h2&gt; &#xA;&lt;div align=&#34;right&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://starchart.cc/geekyouth/SZT-bigdata.svg?sanitize=true&#34; alt=&#34;关注曲线&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github-readme-stats.vercel.app/api?username=geekyouth&amp;amp;show_icons=true&amp;amp;theme=monokai&#34; alt=&#34;个人概况&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github-readme-stats.vercel.app/api/pin?username=geekyouth&amp;amp;repo=SZT-bigdata&amp;amp;show_icons=true&amp;amp;theme=monokai&amp;amp;show_owner=true&#34; alt=&#34;仓库概况&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.jetbrains.com/?from=https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://www.jetbrains.com/company/brand/img/logo1.svg?sanitize=true&#34; alt=&#34;赞助伙伴&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>akka/akka-http</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/akka/akka-http</id>
    <link href="https://github.com/akka/akka-http" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Streaming-first HTTP server/module of Akka&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Akka HTTP&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://index.scala-lang.org/akka/akka-http/akka-http-core&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/akka/akka-http/akka-http-core/latest-by-scala-version.svg?sanitize=true&#34; alt=&#34;akka-http-core Scala version support&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Akka HTTP modules implement a full server- and client-side HTTP stack on top of akka-actor and akka-stream. It&#39;s not a web-framework but rather a more general toolkit for providing and consuming HTTP-based services. While interaction with a browser is of course also in scope it is not the primary focus of Akka HTTP.&lt;/p&gt; &#xA;&lt;p&gt;Akka HTTP follows a rather open design and many times offers several different API levels for &#34;doing the same thing&#34;. You get to pick the API level of abstraction that is most suitable for your application. This means that, if you have trouble achieving something using a high-level API, there&#39;s a good chance that you can get it done with a low-level API, which offers more flexibility but might require you to write more application code.&lt;/p&gt; &#xA;&lt;p&gt;Learn more at &lt;a href=&#34;https://akka.io/&#34;&gt;akka.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation is available at &lt;a href=&#34;https://doc.akka.io/docs/akka-http/current/&#34;&gt;doc.akka.io&lt;/a&gt;, for &lt;a href=&#34;https://doc.akka.io/docs/akka-http/current/scala/http/&#34;&gt;Scala&lt;/a&gt; and &lt;a href=&#34;https://doc.akka.io/docs/akka-http/current/java/http/&#34;&gt;Java&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;You can join these groups and chats to discuss and ask Akka related questions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: &lt;a href=&#34;https://discuss.akka.io&#34;&gt;discuss.akka.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat room about &lt;em&gt;using&lt;/em&gt; Akka HTTP: &lt;a href=&#34;https://gitter.im/akka/akka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fakka-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Q&amp;amp;A: &lt;a href=&#34;https://stackoverflow.com/questions/tagged/akka-http&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stackoverflow%3A-akka--http-blue.svg?style=flat-square&#34; alt=&#34;stackoverflow: #akka-http&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Issue tracker: &lt;a href=&#34;https://github.com/akka/akka-http/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/github%3A-issues-blue.svg?style=flat-square&#34; alt=&#34;github: akka/akka-http&#34;&gt;&lt;/a&gt; (Please use the issue tracker for bugs and reasonable feature requests. Please ask usage questions on the other channels.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of our forums, chat rooms, and issue trackers are governed by our &lt;a href=&#34;https://www.lightbend.com/conduct&#34;&gt;Code Of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition to that, you may enjoy following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog/news-archive.html&#34;&gt;news&lt;/a&gt; section of the page, which is updated whenever a new version is released&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog&#34;&gt;Akka Team Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/akkateam&#34;&gt;@akkateam&lt;/a&gt; on Twitter&lt;/li&gt; &#xA; &lt;li&gt;Projects built with Akka HTTP: &lt;a href=&#34;https://index.scala-lang.org/search?q=dependencies:akka/akka-http*&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/count.svg?q=dependencies:akka/akka-http*&amp;amp;subject=scaladex:&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;Built with Akka HTTP&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are &lt;em&gt;very&lt;/em&gt; welcome!&lt;/p&gt; &#xA;&lt;p&gt;If you see an issue that you&#39;d like to see fixed, the best way to make it happen is to help out by submitting a pull request. For ideas of where to contribute, &lt;a href=&#34;https://github.com/akka/akka-http/labels/help%20wanted&#34;&gt;tickets marked as &#34;help wanted&#34;&lt;/a&gt; are a good starting point.&lt;/p&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/akka/akka-http/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file for more details about the workflow, and general hints on how to prepare your pull request. You can also ask for clarifications or guidance in GitHub issues directly, or in the &lt;a href=&#34;https://gitter.im/akka/dev&#34;&gt;akka/dev&lt;/a&gt; chat if a more real-time communication would be of benefit.&lt;/p&gt; &#xA;&lt;p&gt;A chat room is available for all questions related to &lt;em&gt;developing and contributing&lt;/em&gt; to Akka: &lt;a href=&#34;https://gitter.im/akka/dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fdev-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/dev&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maintenance&lt;/h2&gt; &#xA;&lt;p&gt;This project is maintained by Lightbend&#39;s core Akka Team as well as the extended Akka HTTP Team, consisting of excellent and experienced developers who have shown their dedication and knowledge about HTTP and the codebase. This team may grow dynamically, and it is possible to propose new members to it.&lt;/p&gt; &#xA;&lt;p&gt;Joining the extended team in such form gives you, in addition to street-cred, of course committer rights to this repository as well as higher impact onto the roadmap of the project. Come and join us!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Akka HTTP is Open Source and available under the Apache 2 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TheHive-Project/TheHive</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/TheHive-Project/TheHive</id>
    <link href="https://github.com/TheHive-Project/TheHive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TheHive: a Scalable, Open Source and Free Security Incident Response Platform&lt;/p&gt;&lt;hr&gt;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/thehive-logo.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://chat.thehive-project.org&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-7289da.svg?sanitize=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href&gt;&lt;img src=&#34;https://drone.strangebee.com/api/badges/TheHive-Project/TheHive/status.svg?ref=refs/heads/master-th4&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/LICENSE&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TheHive-Project/TheHive&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;TheHive&lt;/a&gt; is a scalable 3-in-1 open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. It is the perfect companion to &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt;. You can synchronize it with one or multiple MISP instances to start investigations out of MISP events. You can also export an investigation&#39;s results as a MISP event to help your peers detect and react to attacks you&#39;ve dealt with. Additionally, when TheHive is used in conjunction with &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt;, security analysts and researchers can easily analyze tens if not hundred of observables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Current_cases.png&#34; alt=&#34;Current Cases View&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Collaborate&lt;/h2&gt; &#xA;&lt;p&gt;Collaboration is at the heart of TheHive.&lt;/p&gt; &#xA;&lt;p&gt;Multiple analysts from one organisations can work together on the same case simultaneously. For example, an analyst may deal with malware analysis while another may work on tracking C2 beaconing activity on proxy logs as soon as IOCs have been added by their coworker. Using TheHive&#39;s live stream, everyone can keep an eye on what&#39;s happening on the platform, in real time.&lt;/p&gt; &#xA;&lt;p&gt;Multi-tenancy and fine grained user profiles let organisations and analysts work and collaborate on a same case accross organisations. For example, one case can be created by a first organisation who start investigating and ask for contribution from other teams or escalate to another organisation.&lt;/p&gt; &#xA;&lt;h2&gt;Elaborate&lt;/h2&gt; &#xA;&lt;p&gt;Within TheHive, every investigation corresponds to a case. Cases can be created from scratch or from &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; events, SIEM alerts, email reports and any other noteworthy source of security events.&lt;/p&gt; &#xA;&lt;p&gt;Each case can be broken down into one or more tasks. Instead of adding the same tasks to a given type of case every time one is created, analysts can use TheHive&#39;s template engine to create them once and for all. Case templates can also be used to associate metrics to specific case types in order to drive the team&#39;s activity, identify the type of investigations that take significant time and seek to automate tedious tasks.&lt;/p&gt; &#xA;&lt;p&gt;Each task can be assigned to a given analyst. Team members can also take charge of a task without waiting for someone to assign it to them.&lt;/p&gt; &#xA;&lt;p&gt;Tasks may contain multiple work logs that contributing analysts can use to describe what they are up to, what was the outcome, attach pieces of evidence or noteworthy files and so on. Logs can be written using a rich text editor or Markdown.&lt;/p&gt; &#xA;&lt;h2&gt;Analyze&lt;/h2&gt; &#xA;&lt;p&gt;You can add one or hundreds if not thousands of observables to each case you create. You can also create a case out of a &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; event. TheHive can be very easily linked to one or several MISP instances and MISP events can be previewed to decide whether they warrant an investigation or not. If an investigation is in order, the analyst can then add the event to an existing case or import it as a new case using a customizable template.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://thehive-project.org/#section_thehive4py&#34;&gt;TheHive4py&lt;/a&gt;, TheHive&#39;s Python API client, it is possible to send SIEM alerts, phishing and other suspicious emails and other security events to TheHive. They will appear in its &lt;code&gt;Alerts&lt;/code&gt; panel along with new or updated MISP events, where they can be previewed, imported into cases or ignored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Alerts_Panel.png&#34; alt=&#34;The Alerts Pane&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TheHive has the ability to automatically identify observables that have been already seen in previous cases. Observables can also be associated with a TLP and the source which provided or generated them using tags. The analyst can also easily mark observables as IOCs and isolate those using a search query then export them for searching in a SIEM or other data stores.&lt;/p&gt; &#xA;&lt;p&gt;Analysts can analyze tens or hundreds of observables in a few clicks by leveraging the analyzers of one or several &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; instances depending on your OPSEC needs: DomainTools, VirusTotal, PassiveTotal, Joe Sandbox, geolocation, threat feed lookups and so on.&lt;/p&gt; &#xA;&lt;p&gt;Security analysts with a knack for scripting can easily add their own analyzers to Cortex in order to automate actions that must be performed on observables or IOCs. They can also decide how analyzers behave according to the TLP. For example, a file added as observable can be submitted to VirusTotal if the associated TLP is WHITE or GREEN. If it&#39;s AMBER, its hash is computed and submitted to VT but not the file. If it&#39;s RED, no VT lookup is done.&lt;/p&gt; &#xA;&lt;h1&gt;Try it&lt;/h1&gt; &#xA;&lt;p&gt;To try TheHive, you can use the &lt;a href=&#34;https://www.strangebee.com/tryit&#34;&gt;training VM&lt;/a&gt; or install it by reading the &lt;a href=&#34;https://docs.thehive-project.org/thehive/installation-and-configuration/installation/step-by-step-guide/&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Details&lt;/h1&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;We have made several guides available in the &lt;a href=&#34;https://docs.thehive-project.org/thehive/&#34;&gt;Documentation repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;h3&gt;Multi-tenancy&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a special multi-tenancy support. It allows the following strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use a siloed multi-tenancy: many organisations can be defined without allowing them to share data;&lt;/li&gt; &#xA; &lt;li&gt;Use a collaborative multi-tenancy: a set of organisations can be allowed to collaborate on specific cases/tasks/observables, using custom defined user profiles (RBAC).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RBAC&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a set of permissions and several pre-configured user profiles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;admin&lt;/code&gt;: full administrative permissions on the platform ; can&#39;t manage any Cases or other data related to investigations;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;org-admin&lt;/code&gt;: manage users and all organisation-level configuration, can create and edit Cases, Tasks, Observables and run Analyzers and Responders;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;analyst&lt;/code&gt;: can create and edit &lt;em&gt;Cases&lt;/em&gt;, &lt;em&gt;Tasks&lt;/em&gt;, &lt;em&gt;Observables&lt;/em&gt; and run &lt;em&gt;Analyzers&lt;/em&gt; &amp;amp; &lt;em&gt;Responders&lt;/em&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read-only&lt;/code&gt;: Can only read, Cases, Tasks and Observables details;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;New profiles can be created by administrators of the platform.&lt;/p&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;TheHive 4 supports authentication methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;local accounts&lt;/li&gt; &#xA; &lt;li&gt;Active Directory&lt;/li&gt; &#xA; &lt;li&gt;LDAP&lt;/li&gt; &#xA; &lt;li&gt;Basic Auth&lt;/li&gt; &#xA; &lt;li&gt;API keys&lt;/li&gt; &#xA; &lt;li&gt;OAUTH2&lt;/li&gt; &#xA; &lt;li&gt;Multi Factor Authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Statistics &amp;amp; Dashboards&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a powerful statistics module that allows you to create meaningful dashboards to drive your activity and support your budget requests.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;MISP and Cortex&lt;/h3&gt; &#xA;&lt;p&gt;TheHive can be configured to import events from one or multiple &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; instances. You can also use TheHive to export cases as MISP events to one or several MISP servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; is the perfect companion for TheHive. Use one or several to analyze observables at scale.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Digital Shadows&lt;/h3&gt; &#xA;&lt;p&gt;TheHive Project provides &lt;a href=&#34;https://github.com/TheHive-Project/DigitalShadows2TH&#34;&gt;DigitalShadows2TH&lt;/a&gt;, a free, open source &lt;a href=&#34;https://www.digitalshadows.com/&#34;&gt;Digital Shadows&lt;/a&gt; alert feeder for TheHive. You can use it to import Digital Shadows &lt;em&gt;incidents&lt;/em&gt; and &lt;em&gt;intel-incidents&lt;/em&gt; as alerts in TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Zerofox&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Zerofox2TH&#34;&gt;Zerofox2TH&lt;/a&gt; is a free, open source &lt;a href=&#34;https://www.zerofox.com/&#34;&gt;ZeroFOX&lt;/a&gt; alert feeder for TheHive, written by TheHive Project. You can use it to feed ZeroFOX alerts into TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;And many more&lt;/h3&gt; &#xA;&lt;p&gt;Lots of &lt;strong&gt;awesome&lt;/strong&gt; integrations shared by the community could be listed there. If you&#39;re looking for a specific one, &lt;strong&gt;a dedicated repository&lt;/strong&gt; containing all known details and references about existing integrations is updated frequently, and can be found here: &lt;a href=&#34;https://github.com/TheHive-Project/awesome&#34;&gt;https://github.com/TheHive-Project/awesome&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;TheHive is an open source and free software released under the &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/raw/master/LICENSE&#34;&gt;AGPL&lt;/a&gt; (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.&lt;/p&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;p&gt;Information, news and updates are regularly posted on &lt;a href=&#34;https://twitter.com/thehive_project&#34;&gt;TheHive Project Twitter account&lt;/a&gt; and on &lt;a href=&#34;https://blog.thehive-project.org/&#34;&gt;the blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/code_of_conduct.md&#34;&gt;Code of conduct&lt;/a&gt;. We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests via &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;open an issue on GitHub&lt;/a&gt; if you&#39;d like to report a bug or request a feature. We are also available on &lt;a href=&#34;https://chat.thehive-project.org&#34;&gt;Discord&lt;/a&gt; to help you out.&lt;/p&gt; &#xA;&lt;p&gt;If you need to contact the project team, send an email to &lt;a href=&#34;mailto:support@thehive-project.org&#34;&gt;support@thehive-project.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have problems with &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py&#34;&gt;TheHive4py&lt;/a&gt;, please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py/issues/new&#34;&gt;open an issue on its dedicated repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you encounter an issue with Cortex or would like to request a Cortex-related feature, please &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/issues/new&#34;&gt;open an issue on its dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the &lt;a href=&#34;https://github.com/TheHive-Project/cortex-analyzers/issues/new&#34;&gt;analyzers&#39; dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community Discussions&lt;/h1&gt; &#xA;&lt;p&gt;We have set up a Google forum at &lt;a href=&#34;https://groups.google.com/a/thehive-project.org/d/forum/users&#34;&gt;https://groups.google.com/a/thehive-project.org/d/forum/users&lt;/a&gt;. To request access, you need a Google account. You may create one &lt;a href=&#34;https://accounts.google.com/SignUp?hl=en&#34;&gt;using a Gmail address&lt;/a&gt; or &lt;a href=&#34;https://accounts.google.com/SignUpWithoutGmail?hl=en&#34;&gt;without it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Website&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;https://thehive-project.org/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>softwaremill/tapir</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/softwaremill/tapir</id>
    <link href="https://github.com/softwaremill/tapir" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Declarative, type-safe web endpoints library&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/banner.png&#34; alt=&#34;tapir, or Typed API descRiptions&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/softwaremill/tapir?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/softwaremill/tapir&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/softwaremill/tapir/actions?query=workflow%3A%22CI%22&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.softwaremill.sttp.tapir/tapir-core_2.13&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.softwaremill.sttp.tapir/tapir-core_2.13/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;With tapir, you can describe HTTP API endpoints as immutable Scala values. Each endpoint can contain a number of input and output parameters. An endpoint specification can be interpreted as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a server, given the &#34;business logic&#34;: a function, which computes output parameters based on input parameters. Currently supported: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/akkahttp.html&#34;&gt;Akka HTTP&lt;/a&gt; &lt;code&gt;Route&lt;/code&gt;s/&lt;code&gt;Directive&lt;/code&gt;s&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/http4s.html&#34;&gt;Http4s&lt;/a&gt; &lt;code&gt;HttpRoutes[F]&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/netty.html&#34;&gt;Netty&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/finatra.html&#34;&gt;Finatra&lt;/a&gt; &lt;code&gt;FinatraRoute&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/play.html&#34;&gt;Play&lt;/a&gt; &lt;code&gt;Route&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/vertx.html&#34;&gt;Vert.X&lt;/a&gt; &lt;code&gt;Router =&amp;gt; Route&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/ziohttp.html&#34;&gt;ZIO Http&lt;/a&gt; &lt;code&gt;Http&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/armeria.html&#34;&gt;Armeria&lt;/a&gt; &lt;code&gt;HttpServiceWithRoutes&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/server/aws.html&#34;&gt;aws&lt;/a&gt; through Lambda/SAM/Terraform&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;a client, which is a function from input parameters to output parameters. Currently supported: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/client/sttp.html&#34;&gt;sttp&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/client/play.html&#34;&gt;Play&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/client/http4s.html&#34;&gt;http4s&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;documentation. Currently supported: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/docs/openapi.html&#34;&gt;OpenAPI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://tapir.softwaremill.com/en/latest/docs/asyncapi.html&#34;&gt;AsyncAPI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Depending on how you prefer to explore the library, take a look at one of the &lt;a href=&#34;https://tapir.softwaremill.com/en/latest/examples.html&#34;&gt;examples&lt;/a&gt; or &lt;a href=&#34;https://tapir.softwaremill.com/en/latest/index.html&#34;&gt;head over to the docs&lt;/a&gt; for a more detailed description of how tapir works!&lt;/p&gt; &#xA;&lt;h2&gt;Why tapir?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;type-safety&lt;/strong&gt;: compile-time guarantees, develop-time completions, read-time information&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;declarative&lt;/strong&gt;: separate the shape of the endpoint (the &#34;what&#34;), from the server logic (the &#34;how&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAPI / Swagger integration&lt;/strong&gt;: generate documentation from endpoint descriptions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;observability&lt;/strong&gt;: leverage the metadata to report rich metrics and tracing information&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;abstraction&lt;/strong&gt;: re-use common endpoint definitions, as well as individual inputs/outputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;library, not a framework&lt;/strong&gt;: integrates with your stack&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;Is your company already using tapir? We&#39;re continually expanding the &#34;adopters&#34; section in the documentation; the more the merrier! It would be great to feature your company&#39;s logo, but in order to do that, we&#39;ll need written permission to avoid any legal misunderstandings.&lt;/p&gt; &#xA;&lt;p&gt;Please email us at &lt;a href=&#34;mailto:tapir@softwaremill.com&#34;&gt;tapir@softwaremill.com&lt;/a&gt; from your company&#39;s email with a link to your logo (if we can use it, of course!) or with details who to kindly ask for permission to feature the logo in tapir&#39;s documentation. We&#39;ll handle the rest.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.adobe.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/adobe.png&#34; alt=&#34;Adobe&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.colisweb.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/colisweb.png&#34; alt=&#34;Colisweb&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://swissborg.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/swissborg.png&#34; alt=&#34;Swissborg&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://kaizo.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/kaizo.png&#34; alt=&#34;Kaizo&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.process.st/&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/process_street.png&#34; alt=&#34;Process Street&#34; width=&#34;100&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.tranzzo.com/&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/tranzzo.svg?sanitize=true&#34; alt=&#34;Tranzzo&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.kelkoogroup.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/kelkoogroup.png&#34; alt=&#34;Kelkoo group&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.softwaremill.com/&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/softwaremill.png&#34; alt=&#34;SoftwareMill&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/carvana.svg?sanitize=true&#34; alt=&#34;Carvana&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.moneyfarm.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/moneyfarm.png&#34; alt=&#34;Moneyfarm&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.ocadogroup.com/about-us/ocado-technology&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/ocado.png&#34; alt=&#34;Ocado Technology&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.wegtam.com&#34;&gt;&lt;img src=&#34;https://github.com/softwaremill/tapir/raw/master/doc/adopters/wegtam.svg?sanitize=true&#34; alt=&#34;Wegtam&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Teaser&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import sttp.tapir._&#xA;import sttp.tapir.generic.auto._&#xA;import sttp.tapir.json.circe._&#xA;import io.circe.generic.auto._&#xA;&#xA;type Limit = Int&#xA;type AuthToken = String&#xA;case class BooksQuery(genre: String, year: Int)&#xA;case class Book(title: String)&#xA;&#xA;&#xA;// Define an endpoint&#xA;&#xA;val booksListing: PublicEndpoint[(BooksQuery, Limit, AuthToken), String, List[Book], Any] = &#xA;  endpoint&#xA;    .get&#xA;    .in((&#34;books&#34; / path[String](&#34;genre&#34;) / path[Int](&#34;year&#34;)).mapTo[BooksQuery])&#xA;    .in(query[Limit](&#34;limit&#34;).description(&#34;Maximum number of books to retrieve&#34;))&#xA;    .in(header[AuthToken](&#34;X-Auth-Token&#34;))&#xA;    .errorOut(stringBody)&#xA;    .out(jsonBody[List[Book]])&#xA;&#xA;&#xA;// Generate OpenAPI documentation&#xA;&#xA;import sttp.apispec.openapi.circe.yaml._&#xA;import sttp.tapir.docs.openapi.OpenAPIDocsInterpreter&#xA;&#xA;val docs = OpenAPIDocsInterpreter().toOpenAPI(booksListing, &#34;My Bookshop&#34;, &#34;1.0&#34;)&#xA;println(docs.toYaml)&#xA;&#xA;&#xA;// Convert to akka-http Route&#xA;&#xA;import sttp.tapir.server.akkahttp.AkkaHttpServerInterpreter&#xA;import akka.http.scaladsl.server.Route&#xA;import scala.concurrent.Future&#xA;import scala.concurrent.ExecutionContext.Implicits.global&#xA;&#xA;def bookListingLogic(bfy: BooksQuery,&#xA;                     limit: Limit,&#xA;                     at: AuthToken): Future[Either[String, List[Book]]] =&#xA;  Future.successful(Right(List(Book(&#34;The Sorrows of Young Werther&#34;))))&#xA;  &#xA;val booksListingRoute: Route = AkkaHttpServerInterpreter()&#xA;  .toRoute(booksListing.serverLogic((bookListingLogic _).tupled))&#xA;&#xA;&#xA;// Convert to sttp Request&#xA;&#xA;import sttp.tapir.client.sttp.SttpClientInterpreter&#xA;import sttp.client3._&#xA;&#xA;val booksListingRequest: Request[DecodeResult[Either[String, List[Book]]], Any] = &#xA;  SttpClientInterpreter()&#xA;    .toRequest(booksListing, Some(uri&#34;http://localhost:8080&#34;))&#xA;    .apply((BooksQuery(&#34;SF&#34;, 2016), 20, &#34;xyz-abc-123&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;tapir documentation is available at &lt;a href=&#34;http://tapir.softwaremill.com&#34;&gt;tapir.softwaremill.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart with sbt&lt;/h2&gt; &#xA;&lt;p&gt;Add the following dependency:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;&#34;com.softwaremill.sttp.tapir&#34; %% &#34;tapir-core&#34; % &#34;1.0.0-RC2&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, import:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import sttp.tapir._&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And finally, type &lt;code&gt;endpoint.&lt;/code&gt; and see where auto-complete gets you!&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.12&lt;/h3&gt; &#xA;&lt;p&gt;Partial unification is now enabled by default from Scala 2.13. However, if you&#39;re using Scala 2.12 or older, then you&#39;ll need partial unification enabled in the compiler (alternatively, you&#39;ll need to manually provide type arguments in some cases):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;scalacOptions += &#34;-Ypartial-unification&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sidenote for scala 2.12.4 and higher: if you encounter an issue with compiling your project because of a &lt;code&gt;StackOverflowException&lt;/code&gt; related to &lt;a href=&#34;https://github.com/scala/bug/issues/10604&#34;&gt;this&lt;/a&gt; scala bug, please increase your stack memory. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt -J-Xss4M clean compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other sttp projects&lt;/h2&gt; &#xA;&lt;p&gt;sttp is a family of Scala HTTP-related projects, and currently includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp&#34;&gt;sttp client&lt;/a&gt;: the Scala HTTP client you always wanted!&lt;/li&gt; &#xA; &lt;li&gt;sttp tapir: this project&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp-model&#34;&gt;sttp model&lt;/a&gt;: simple HTTP model classes (used by client &amp;amp; tapir)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp-shared&#34;&gt;sttp shared&lt;/a&gt;: shared web socket, FP abstractions, capabilities and streaming code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp-apispec&#34;&gt;sttp apispec&lt;/a&gt;: OpenAPI, AsyncAPI and JSON Schema models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;All suggestions welcome :)&lt;/p&gt; &#xA;&lt;p&gt;See the list of &lt;a href=&#34;https://github.com/softwaremill/tapir/issues&#34;&gt;issues&lt;/a&gt; and pick one! Or report your own.&lt;/p&gt; &#xA;&lt;p&gt;If you are having doubts on the &lt;em&gt;why&lt;/em&gt; or &lt;em&gt;how&lt;/em&gt; something works, don&#39;t hesitate to ask a question on &lt;a href=&#34;https://gitter.im/softwaremill/tapir&#34;&gt;gitter&lt;/a&gt; or via github. This probably means that the documentation, scaladocs or code is unclear and be improved for the benefit of all.&lt;/p&gt; &#xA;&lt;h3&gt;Testing locally&lt;/h3&gt; &#xA;&lt;p&gt;The JS tests use &lt;a href=&#34;https://github.com/scala-js/scala-js-env-selenium/issues/119&#34;&gt;Gecko instead of Chrome&lt;/a&gt;, although this causes another problem: out of memory when running JS tests for multiple modules. Work-arounds:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run only JVM tests for a specific Scala version using &lt;code&gt;testJVM2_13&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;test single JS projects&lt;/li&gt; &#xA; &lt;li&gt;use CI (GitHub Actions) to test all projects - the &lt;code&gt;.github/workflows/ci.yml&lt;/code&gt; enumerates them one by one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can test only server/client/doc/other projects using &lt;code&gt;testServers&lt;/code&gt;, &lt;code&gt;testClients&lt;/code&gt;, &lt;code&gt;testDocs&lt;/code&gt; and &lt;code&gt;testOther&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To verify that the code snippet in docs compile, run &lt;code&gt;compileDocumentation&lt;/code&gt;. A full mdoc run is done during a release (when the documentation is generated).&lt;/p&gt; &#xA;&lt;h2&gt;Commercial Support&lt;/h2&gt; &#xA;&lt;p&gt;We offer commercial support for tapir and related technologies, as well as development services. &lt;a href=&#34;https://softwaremill.com&#34;&gt;Contact us&lt;/a&gt; to learn more about our offer!&lt;/p&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (C) 2018-2022 SoftwareMill &lt;a href=&#34;https://softwaremill.com&#34;&gt;https://softwaremill.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HariSekhon/Nagios-Plugin-Kafka</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/HariSekhon/Nagios-Plugin-Kafka</id>
    <link href="https://github.com/HariSekhon/Nagios-Plugin-Kafka" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kafka Scala API CLI / Advanced Nagios Plugin, with Kerberos support (uses Kafka 0.9+ native Java API)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kafka Scala API - Advanced Nagios Plugin / CLI Tool with Kerberos support&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/harisekhon/Nagios-Plugin-Kafka?logo=github&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/network&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/harisekhon/Nagios-Plugin-Kafka?logo=github&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/HariSekhon/Nagios-Plugin-Kafka&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/commits/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/HariSekhon/Nagios-Plugin-Kafka?logo=github&#34; alt=&#34;GitHub Last Commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;[![Dependency Status](https://www.versioneye.com/user/projects/57616d340a82b200276f6669/badge.svg)](https://www.versioneye.com/user/projects/57616d340a82b200276f6669)&#xA;--&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.codacy.com/gh/HariSekhon/Nagios-Plugin-Kafka/dashboard&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/2f6cc8cba0ef4007a3f736bf45ae60f8&#34; alt=&#34;Codacy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/harisekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/harisekhon/Nagios-Plugin-Kafka/badge&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=HariSekhon_Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_Nagios-Plugin-Kafka&amp;amp;metric=alert_status&#34; alt=&#34;Quality Gate Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=HariSekhon_Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_Nagios-Plugin-Kafka&amp;amp;metric=sqale_rating&#34; alt=&#34;Maintainability Rating&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=HariSekhon_Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_Nagios-Plugin-Kafka&amp;amp;metric=reliability_rating&#34; alt=&#34;Reliability Rating&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=HariSekhon_Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_Nagios-Plugin-Kafka&amp;amp;metric=security_rating&#34; alt=&#34;Security Rating&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Linux-blue?logo=linux&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Mac-blue?logo=apple&#34; alt=&#34;Mac&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/harisekhon/nagios-plugin-kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/container-Docker-blue?logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Dockerfiles&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/repo-Dockerfiles-blue?logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Dockerfile&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/harisekhon/nagios-plugin-kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/harisekhon/nagios-plugin-kafka?label=DockerHub%20pulls&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;DockerHub Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/harisekhon/nagios-plugin-kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/automated/harisekhon/nagios-plugin-kafka?logo=docker&amp;amp;logoColor=white&#34; alt=&#34;DockerHub Build Automated&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- these badges don&#39;t work any more&#xA;[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/nagios-plugin-kafka?logo=docker&amp;logoColor=white)](https://hub.docker.com/r/harisekhon/nagios-plugin-kafka/builds)&#xA;[![MicroBadger](https://images.microbadger.com/badges/image/harisekhon/nagios-plugin-kafka.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugin-kafka)&#xA;--&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://harisekhon.github.io/CI-CD/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CI%20Builds-Overview%20Page-blue?logo=circleci&#34; alt=&#34;CI Builds Overview&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/Jenkinsfile&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Jenkins-ready-blue?logo=jenkins&amp;amp;logoColor=white&#34; alt=&#34;Jenkins&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/.concourse.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Concourse-ready-blue?logo=concourse&#34; alt=&#34;Concourse&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/.gocd.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GoCD-ready-blue?logo=go&#34; alt=&#34;GoCD&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/TeamCity-CI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TeamCity-ready-blue?logo=teamcity&#34; alt=&#34;TeamCity&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/HariSekhon/Nagios-Plugin-Kafka.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://buildkite.com/hari-sekhon/nagios-plugin-kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/buildkite/835ba032422b5aa6c1df641e6a7989ac93bb8a34fcca735243/master?label=BuildKite&amp;amp;logo=buildkite&#34; alt=&#34;BuildKite&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/HariSekhon/Nagios-Plugin-Kafka/branch/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/build/harisekhon/Nagios-Plugin-Kafka/master?logo=appveyor&amp;amp;label=AppVeyor&#34; alt=&#34;AppVeyor&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.drone.io/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/drone/build/HariSekhon/Nagios-Plugin-Kafka/master?logo=drone&amp;amp;label=Drone&#34; alt=&#34;Drone&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codeship.com/projects/387257&#34;&gt;&lt;img src=&#34;https://app.codeship.com/projects/faff7930-3c5f-0138-8a0b-32bf6ef9714a/status?branch=master&#34; alt=&#34;Codeship Status for HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://g.codefresh.io/pipelines/edit/new/builds?id=5e58e3573953b779e04b7907&amp;amp;pipeline=Nagios%20Plugin%20Kafka&amp;amp;projects=GitHub&amp;amp;projectId=5e52ca8ea284e00f882ea992&amp;amp;context=github&amp;amp;filter=page:1;pageSize:10;timeFrameStart:week&#34;&gt;&lt;img src=&#34;https://g.codefresh.io/api/badges/pipeline/harisekhon/GitHub%2FNagios%20Plugin%20Kafka?branch=master&amp;amp;key=eyJhbGciOiJIUzI1NiJ9.NWU1MmM5OGNiM2FiOWUzM2Y3ZDZmYjM3.O69674cW7vYom3v5JOGKXDbYgCVIJU9EWhXUMHl3zwA&amp;amp;type=cf-1&#34; alt=&#34;Codefresh&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cirrus-ci.com/github/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/cirrus/github/HariSekhon/Nagios-Plugin-Kafka/master?logo=Cirrus%20CI&amp;amp;label=Cirrus%20CI&#34; alt=&#34;Cirrus CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://harisekhon.semaphoreci.com/projects/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://harisekhon.semaphoreci.com/badges/Nagios-Plugin-Kafka.svg?sanitize=true&#34; alt=&#34;Semaphore&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.wercker.com/harisekhon/nagios-plugin-kafka/runs&#34;&gt;&lt;img src=&#34;https://app.wercker.com/status/fe4f87bf98f31e4c22a3041c0966644b/s/master&#34; alt=&#34;Wercker&#34; title=&#34;wercker status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/buddy.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Buddy-ready-1A86FD?logo=buddy&#34; alt=&#34;Buddy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/shippable.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Shippable-legacy-lightgrey?logo=jfrog&amp;amp;label=Shippable&#34; alt=&#34;Shippable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/.travis.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TravisCI-ready-blue?logo=travis&amp;amp;label=Travis%20CI&#34; alt=&#34;Travis CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--[![Wercker](https://img.shields.io/wercker/ci/5e58efdecdec020800455736/master?label=Wercker&amp;logo=oracle)](https://app.wercker.com/harisekhon/nagios-plugin-kafka/runs)--&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/harisekhon/GitHub/_build/latest?definitionId=11&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/harisekhon/GitHub/_apis/build/status/HariSekhon.Nagios-Plugin-Kafka?branchName=master&#34; alt=&#34;Azure DevOps Pipeline&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/HariSekhon/Nagios-Plugin-Kafka/pipelines&#34;&gt;&lt;img src=&#34;https://img.shields.io/gitlab/pipeline/harisekhon/Nagios-Plugin-Kafka?logo=gitlab&amp;amp;label=GitLab%20CI&#34; alt=&#34;GitLab Pipeline&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bitbucket.org/harisekhon/nagios-plugin-kafka/addon/pipelines/home#!/&#34;&gt;&lt;img src=&#34;https://img.shields.io/bitbucket/pipelines/harisekhon/nagios-plugin-kafka/master?logo=bitbucket&amp;amp;label=BitBucket%20CI&#34; alt=&#34;BitBucket Pipeline&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/buildspec.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AWS%20CodeBuild-ready-blue?logo=amazon%20aws&#34; alt=&#34;AWS CodeBuild&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/raw/master/cloudbuild.yaml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GCP%20Cloud%20Build-ready-blue?logo=google%20cloud&amp;amp;logoColor=white&#34; alt=&#34;GCP Cloud Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/harisekhon/GitHub/_git/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/repo-Azure%20DevOps-0078D7?logo=azure%20devops&#34; alt=&#34;Repo on Azure DevOps&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/repo-GitHub-2088FF?logo=github&#34; alt=&#34;Repo on GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/repo-GitLab-FCA121?logo=gitlab&#34; alt=&#34;Repo on GitLab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bitbucket.org/HariSekhon/Nagios-Plugin-Kafka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/repo-BitBucket-0052CC?logo=bitbucket&#34; alt=&#34;Repo on BitBucket&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/validate.yaml&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/validate.yaml/badge.svg?sanitize=true&#34; alt=&#34;Validation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/semgrep.yaml&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/semgrep.yaml/badge.svg?sanitize=true&#34; alt=&#34;Semgrep&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/kics.yaml&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions/workflows/kics.yaml/badge.svg?sanitize=true&#34; alt=&#34;Kics&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22GitHub+Actions+Ubuntu%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/GitHub%20Actions%20Ubuntu/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Ubuntu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Mac%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Mac/badge.svg?sanitize=true&#34; alt=&#34;Mac&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Mac+10.15%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Mac%2010.15/badge.svg?sanitize=true&#34; alt=&#34;Mac 10.15&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Ubuntu%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Ubuntu/badge.svg?sanitize=true&#34; alt=&#34;Ubuntu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Ubuntu+16.04%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Ubuntu%2016.04/badge.svg?sanitize=true&#34; alt=&#34;Ubuntu 16.04&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Ubuntu+18.04%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Ubuntu%2018.04/badge.svg?sanitize=true&#34; alt=&#34;Ubuntu 18.04&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Ubuntu+20.04%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Ubuntu%2020.04/badge.svg?sanitize=true&#34; alt=&#34;Ubuntu 20.04&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Debian%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Debian/badge.svg?sanitize=true&#34; alt=&#34;Debian&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Debian+9%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Debian%209/badge.svg?sanitize=true&#34; alt=&#34;Debian 9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Debian+10%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Debian%2010/badge.svg?sanitize=true&#34; alt=&#34;Debian 10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22CentOS%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/CentOS/badge.svg?sanitize=true&#34; alt=&#34;CentOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22CentOS+7%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/CentOS%207/badge.svg?sanitize=true&#34; alt=&#34;CentOS 7&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22CentOS+8%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/CentOS%208/badge.svg?sanitize=true&#34; alt=&#34;CentOS 8&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Fedora%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Fedora/badge.svg?sanitize=true&#34; alt=&#34;Fedora&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Alpine%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Alpine/badge.svg?sanitize=true&#34; alt=&#34;Alpine&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Alpine+3%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Alpine%203/badge.svg?sanitize=true&#34; alt=&#34;Alpine 3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Maven%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Maven/badge.svg?sanitize=true&#34; alt=&#34;Maven&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22SBT%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/SBT/badge.svg?sanitize=true&#34; alt=&#34;SBT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/actions?query=workflow%3A%22Gradle%22&#34;&gt;&lt;img src=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/workflows/Gradle/badge.svg?sanitize=true&#34; alt=&#34;Gradle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git.io/nagios-plugin-kafka&#34;&gt;git.io/nagios-plugin-kafka&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kafka 0.9+ API CLI Tester &amp;amp; Advanced Nagios Plugin with Kerberos support, written in Scala.&lt;/p&gt; &#xA;&lt;p&gt;Tested on Hortonworks HDP 2.4.0 with Kerberos + Ranger ACLs and Apache Kafka 0.8.x / 0.9.0.1 &lt;a href=&#34;https://hub.docker.com/r/harisekhon/kafka&#34;&gt;docker images&lt;/a&gt; with regular ACLs.&lt;/p&gt; &#xA;&lt;p&gt;You may need to change the Kafka library version in &lt;code&gt;pom.xml&lt;/code&gt; / &lt;code&gt;build.sbt&lt;/code&gt; / &lt;code&gt;build.gradle&lt;/code&gt; before building to match your deployed Kafka server / cluster otherwise it may hang when run due to version / protocol mismatch.&lt;/p&gt; &#xA;&lt;!--&#xA;Testing shows it does take an extra second to negotiate the Kerberos authentication so make sure not to set ```--timeout``` to less than 2 secs if using Kerberos.&#xA;--&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugins#advanced-nagios-plugins-collection&#34;&gt;The Advanced Nagios Plugins Collection&lt;/a&gt; for many more related enterprise monitoring programs.&lt;/p&gt; &#xA;&lt;p&gt;Hari Sekhon&lt;/p&gt; &#xA;&lt;p&gt;Cloud &amp;amp; Big Data Contractor, United Kingdom&lt;/p&gt; &#xA;&lt;p&gt;(ex-Cloudera, former Hortonworks Consultant)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/HariSekhon/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LinkedIn%20Profile-HariSekhon-blue?logo=linkedin&#34; alt=&#34;My LinkedIn&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h6&gt;(you&#39;re welcome to connect with me on LinkedIn)&lt;/h6&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;This project builds a single self-contained Java jar file with all dependencies included and can simply be run on the command line with full switch option support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;java -jar check_kafka.jar --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and there is an optional convenience shell wrapper script at the top level to make commands shorter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./check_kafka --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run against one or more Kafka brokers, comma separated:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./check_kafka --brokers localhost:9092 --topic test&#xA;OK: Kafka broker successfully returned unique message via topic &#39;test&#39; partition &#39;0&#39;, write time = 0.185s, read time = 0.045s, total time = 1.729s | write_time=0.185s read_time=0.045s total_time=1.729s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;--verbose&lt;/code&gt; switch to also show the brokers list that were tested. If you have specified one of the kerberos switches (or edited the consumer/producer properties files to do so) then the output will additionally contain the marker &lt;code&gt;with sasl authentication&lt;/code&gt; to let you know that it was a secure configuration that was tested (originally I called this &lt;code&gt;with kerberos&lt;/code&gt; but technically it may not be in future).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OK: Kafka broker &#39;&amp;lt;hortonworks_host&amp;gt;:6667&#39; successfully returned unique message via topic &#39;topic3&#39; partition &#39;0&#39; with sasl authentication, write time = 0.148s, read time = 0.043s, total time = 0.691s | write_time=0.148s read_time=0.043s total_time=0.691s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Kafka 0.9+ API Caveats&lt;/h5&gt; &#xA;&lt;p&gt;This program only supports Kafka 0.9+ as the API changed (again) and Kerberos security was only added in the 0.9 API. For Kafka versions before 0.9 you can find Python and Perl versions of this program in the &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugins#advanced-nagios-plugins-collection&#34;&gt;Advanced Nagios Plugins Collection&lt;/a&gt; that support 0.8 onwards (they dosn&#39;t support Kafka &amp;lt;= 0.7 as the API changed in 0.8 too and the underlying libraries in those languages don&#39;t support Kafka &amp;lt;= 0.7).&lt;/p&gt; &#xA;&lt;p&gt;It appears that several errors are caught too early in the new Kafka Java API and result in embedded looping retry behaviour on encountering errors (visible in debug level logging of the base library).&lt;/p&gt; &#xA;&lt;p&gt;I haven&#39;t found a great way of handle that behaviour as it&#39;s not exposed to the client code so it ends up being handled via my generic default self timeout mechanism that I apply to all my tools. Hence if you specify an incorrect &lt;code&gt;--brokers &amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt; or the Kafka brokers are down or you fail to negotiate the protocol due to security settings you will only receive a generic &lt;code&gt;UNKNOWN: self timed out after 10 secs&lt;/code&gt; message as the code self terminates.&lt;/p&gt; &#xA;&lt;p&gt;Otherwise the Kafka API would just hang there indefintely as it keeps retrying deeper in the library. I&#39;ve tried various settings to get it to time out but nothing worked and I even posted to the Kafka users mailing list without an answer. If you know of a setting that will make the Kafka Client library time out and return the more specific error then please let me know and I&#39;ll update this code accordingly.&lt;/p&gt; &#xA;&lt;h4&gt;Kerberos Support&lt;/h4&gt; &#xA;&lt;p&gt;See the &lt;code&gt;conf/&lt;/code&gt; directory for JAAS kerberos configurations.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re running the code on a Hortonworks Kafka broker it&#39;ll auto-detect the HDP configuration and use that.&lt;/p&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;h4&gt;Quick Start - Docker&lt;/h4&gt; &#xA;&lt;p&gt;A Dockerized pre-built version is available on &lt;a href=&#34;https://hub.docker.com/r/harisekhon/nagios-plugin-kafka&#34;&gt;DockerHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have docker installed this one command will download and run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run harisekhon/nagios-plugin-kafka check_kafka --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Automated Build from Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L https://git.io/nagios-plugin-kafka-bootstrap | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OR&lt;/p&gt; &#xA;&lt;p&gt;Maven, Gradle and SBT automated builds are all provided.&lt;/p&gt; &#xA;&lt;p&gt;A self-contained jar file with all dependencies will be created and symlinked to &lt;code&gt;check_kafka.jar&lt;/code&gt; at the top level.&lt;/p&gt; &#xA;&lt;p&gt;The Maven and Gradle builds are best as they will auto bootstap and run with no prior installed dependencies other than Java and &lt;code&gt;make&lt;/code&gt; to kick it off.&lt;/p&gt; &#xA;&lt;p&gt;The default &lt;code&gt;make&lt;/code&gt; build will trigger a Gradle bootstrap from scratch with an embedded checksum for security:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can call any one of the 3 major build systems explicitly instead, which will recurse to build the library submodule using the same mechanism:&lt;/p&gt; &#xA;&lt;p&gt;Maven:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make mvn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Gradle:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make gradle&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SBT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make sbt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Custom TLDs&lt;/h5&gt; &#xA;&lt;p&gt;If using bespoke internal domains such as &lt;code&gt;.local&lt;/code&gt;, &lt;code&gt;.intranet&lt;/code&gt;, &lt;code&gt;.vm&lt;/code&gt;, &lt;code&gt;.cloud&lt;/code&gt; etc. that aren&#39;t part of the official IANA TLD list then this is additionally supported via a custom configuration file &lt;a href=&#34;https://github.com/HariSekhon/lib-java/raw/master/src/main/resources/tlds-alpha-by-domain.txt&#34;&gt;lib/resources/custom_tlds.txt&lt;/a&gt; containing one TLD per line, with support for # comment prefixes. Just add your bespoke internal TLD to the file and it will then pass the host/domain/fqdn validations.&lt;/p&gt; &#xA;&lt;h4&gt;Testing&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/HariSekhon/nagios-plugin-kafka&#34;&gt;Continuous Integration&lt;/a&gt; is run on this repo with tests for success and failure scenarios:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;unit tests for the custom supporting &lt;a href=&#34;https://github.com/HariSekhon/lib-java&#34;&gt;java library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;integration tests of the top level programs using the libraries for things like option parsing&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugin-Kafka/tree/master/tests&#34;&gt;functional tests&lt;/a&gt; for the top level programs using &lt;a href=&#34;https://hub.docker.com/u/harisekhon/&#34;&gt;Docker containers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To trigger all tests run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will start with the underlying libraries, then move on to top level integration tests and functional tests using docker containers if docker is available.&lt;/p&gt; &#xA;&lt;h3&gt;Kafka 0.8 support - Alternative Perl &amp;amp; Python Kafka API Nagios Plugins&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugins#advanced-nagios-plugins-collection&#34;&gt;Advanced Nagios Plugins Collection&lt;/a&gt; has both Perl and Python predecessors to this program which work with Kafka 0.8+. The main differenitator with this Scala version is that it uses the new native 0.9+ Java API which has Kerberos support (the dynamic language versions were built on libraries for Kafka 0.8).&lt;/p&gt; &#xA;&lt;h3&gt;See Also&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/DevOps-Bash-tools&#34;&gt;DevOps Bash Tools&lt;/a&gt; - 700+ DevOps Bash Scripts, Advanced &lt;code&gt;.bashrc&lt;/code&gt;, &lt;code&gt;.vimrc&lt;/code&gt;, &lt;code&gt;.screenrc&lt;/code&gt;, &lt;code&gt;.tmux.conf&lt;/code&gt;, &lt;code&gt;.gitconfig&lt;/code&gt;, CI configs &amp;amp; Utility Code Library - AWS, GCP, Kubernetes, Docker, Kafka, Hadoop, SQL, BigQuery, Hive, Impala, PostgreSQL, MySQL, LDAP, DockerHub, Jenkins, Spotify API &amp;amp; MP3 tools, Git tricks, GitHub API, GitLab API, BitBucket API, Code &amp;amp; build linting, package management for Linux / Mac / Python / Perl / Ruby / NodeJS / Golang, and lots more random goodies&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/SQL-scripts&#34;&gt;SQL Scripts&lt;/a&gt; - 100+ SQL Scripts - PostgreSQL, MySQL, AWS Athena, Google BigQuery&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Templates&#34;&gt;Templates&lt;/a&gt; - dozens of Code &amp;amp; Config templates - AWS, GCP, Docker, Jenkins, Terraform, Vagrant, Puppet, Python, Bash, Go, Perl, Java, Scala, Groovy, Maven, SBT, Gradle, Make, GitHub Actions Workflows, CircleCI, Jenkinsfile, Makefile, Dockerfile, docker-compose.yml, M4 etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Kubernetes-configs&#34;&gt;Kubernetes configs&lt;/a&gt; - Kubernetes YAML configs - Best Practices, Tips &amp;amp; Tricks are baked right into the templates for future deployments&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/DevOps-Python-tools&#34;&gt;DevOps Python Tools&lt;/a&gt; - 80+ DevOps CLI tools for AWS, GCP, Hadoop, HBase, Spark, Log Anonymizer, Ambari Blueprints, AWS CloudFormation, Linux, Docker, Spark Data Converters &amp;amp; Validators (Avro / Parquet / JSON / CSV / INI / XML / YAML), Elasticsearch, Solr, Travis CI, Pig, IPython&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Nagios-Plugins&#34;&gt;The Advanced Nagios Plugins Collection&lt;/a&gt; - 450+ programs for Nagios monitoring your Hadoop &amp;amp; NoSQL clusters. Covers every Hadoop vendor&#39;s management API and every major NoSQL technology (HBase, Cassandra, MongoDB, Elasticsearch, Solr, Riak, Redis etc.) as well as message queues (Kafka, RabbitMQ), continuous integration (Jenkins, Travis CI) and traditional infrastructure (SSL, Whois, DNS, Linux)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/harisekhon/perl-tools&#34;&gt;DevOps Perl Tools&lt;/a&gt; - 25+ DevOps CLI tools for Hadoop, HDFS, Hive, Solr/SolrCloud CLI, Log Anonymizer, Nginx stats &amp;amp; HTTP(S) URL watchers for load balanced web farms, Dockerfiles &amp;amp; SQL ReCaser (MySQL, PostgreSQL, AWS Redshift, Snowflake, Apache Drill, Hive, Impala, Cassandra CQL, Microsoft SQL Server, Oracle, Couchbase N1QL, Dockerfiles, Pig Latin, Neo4j, InfluxDB), Ambari FreeIPA Kerberos, Datameer, Linux...&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/HAProxy-configs&#34;&gt;HAProxy Configs&lt;/a&gt; - 80+ HAProxy Configs for Hadoop, Big Data, NoSQL, Docker, Elasticsearch, SolrCloud, HBase, Cloudera, Hortonworks, MapR, MySQL, PostgreSQL, Apache Drill, Hive, Presto, Impala, ZooKeeper, OpenTSDB, InfluxDB, Prometheus, Kibana, Graphite, SSH, RabbitMQ, Redis, Riak, Rancher etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Dockerfiles&#34;&gt;Dockerfiles&lt;/a&gt; - 50+ DockerHub public images for Docker &amp;amp; Kubernetes - Hadoop, Kafka, ZooKeeper, HBase, Cassandra, Solr, SolrCloud, Presto, Apache Drill, Nifi, Spark, Mesos, Consul, Riak, OpenTSDB, Jython, Advanced Nagios Plugins &amp;amp; DevOps Tools repos on Alpine, CentOS, Debian, Fedora, Ubuntu, Superset, H2O, Serf, Alluxio / Tachyon, FakeS3&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git.io/nagios-plugin-kafka&#34;&gt;git.io/nagios-plugin-kafka&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/common</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/allenai/common</id>
    <link href="https://github.com/allenai/common" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of useful utility classes and functions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;common&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/allenai/common/tree/master&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/allenai/common/tree/master.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A collection of useful utility classes and functions. Slowly on the path to deprecation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;testkit&lt;/code&gt; - Unit test classes and utilities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;guice&lt;/code&gt; - Guice-specific libraries.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;core&lt;/code&gt; - Catchall collection of utilities.&lt;/p&gt; &#xA;&lt;h2&gt;Using this project as a library&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;common&lt;/code&gt; is published to &lt;a href=&#34;https://us-west-2.console.aws.amazon.com/codesuite/codeartifact/d/896129387501/org-allenai-s2/r/private?region=us-west-2&#34;&gt;CodeArtifact&lt;/a&gt;. You will need to add a resolver via the &lt;a href=&#34;https://github.com/bbstilson/sbt-codeartifact/&#34;&gt;&lt;code&gt;sbt-codeartifact&lt;/code&gt;&lt;/a&gt; plugin to use these libraries.&lt;/p&gt; &#xA;&lt;h2&gt;Releasing new versions&lt;/h2&gt; &#xA;&lt;p&gt;To make a release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;&amp;gt; release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Guideline for Contributing to &lt;code&gt;common&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;There is no strict process for contributing to &lt;code&gt;common&lt;/code&gt;. However, following are some general guidelines.&lt;/p&gt; &#xA;&lt;h3&gt;Discuss in Pull Request Code Reviews&lt;/h3&gt; &#xA;&lt;p&gt;If you have implemented something in a repository other than &lt;code&gt;common&lt;/code&gt; and that you think could be a candidate to be migrated into &lt;code&gt;common&lt;/code&gt;, ask reviewers for feedback when issuing your pull request.&lt;/p&gt; &#xA;&lt;h3&gt;Create a GitHub Issue&lt;/h3&gt; &#xA;&lt;p&gt;Feel free create a GitHub issue in the &lt;code&gt;common&lt;/code&gt; project to provide traceability and a forum for discussion.&lt;/p&gt; &#xA;&lt;h3&gt;Use TODO Comments&lt;/h3&gt; &#xA;&lt;p&gt;While working on a task, go ahead and implement the functionality that you think would be a good fit for &lt;code&gt;common&lt;/code&gt;, and comment the implementation with a TODO suggesting it belongs in &lt;code&gt;common&lt;/code&gt;. An example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// TODO(mygithubusername): migrate to common&#xA;object ResourceHandling {&#xA;  type Resource = { def close(): Unit }&#xA;  def using[A](resource: =&amp;gt; Resource)(f: Resource =&amp;gt; A) {&#xA;    try {&#xA;      f(resource)&#xA;    finally {&#xA;      resource.close()&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have created a GitHub issue for the &lt;code&gt;common&lt;/code&gt; candidate, it is a good idea for traceability to reference the issue number in your TODO comment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// TODO(mygithubusername): migrate to common. See https://github.com/allenai/common/issues/123&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Have Two Code Reviewers to &lt;code&gt;common&lt;/code&gt; Pull Requests&lt;/h3&gt; &#xA;&lt;p&gt;Try and always have at least two reviewers for a pull request to &lt;code&gt;common&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datastax/spark-cassandra-connector</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/datastax/spark-cassandra-connector</id>
    <link href="https://github.com/datastax/spark-cassandra-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DataStax Spark Cassandra Connector&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Cassandra Connector&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/datastax/spark-cassandra-connector/actions/workflows/main.yml/badge.svg?branch=master&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What&lt;/th&gt; &#xA;   &lt;th&gt;Where&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Community&lt;/td&gt; &#xA;   &lt;td&gt;Chat with us at &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;Datastax and Cassandra Q&amp;amp;A&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala Docs&lt;/td&gt; &#xA;   &lt;td&gt;Most Recent Release (3.2.0): &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;, &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/driver/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector-Driver&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latest Production Release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.2.0/jar&#34;&gt;3.2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra®.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library lets you expose Cassandra tables as Spark RDDs and Datasets/DataFrames, write Spark RDDs and Datasets/DataFrames to Cassandra tables, and execute arbitrary CQL queries in your Spark applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compatible with Apache Cassandra version 2.1 or higher (see table below)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Apache Spark 1.0 through 3.2 (&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/#version-compatibility&#34;&gt;see table below&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Scala 2.11 and 2.12&lt;/li&gt; &#xA; &lt;li&gt;Exposes Cassandra tables as Spark RDDs and Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Maps table rows to CassandraRow objects or tuples&lt;/li&gt; &#xA; &lt;li&gt;Offers customizable object mapper for mapping rows to objects of user-defined classes&lt;/li&gt; &#xA; &lt;li&gt;Saves RDDs back to Cassandra by implicit &lt;code&gt;saveToCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Delete rows and columns from cassandra by implicit &lt;code&gt;deleteFromCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Join with a subset of Cassandra data using &lt;code&gt;joinWithCassandraTable&lt;/code&gt; call for RDDs, and optimizes join with data in Cassandra when using Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Partition RDDs according to Cassandra replication using &lt;code&gt;repartitionByCassandraReplica&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Converts data types between Cassandra and Scala&lt;/li&gt; &#xA; &lt;li&gt;Supports all Cassandra data types including collections&lt;/li&gt; &#xA; &lt;li&gt;Filters rows on the server side via the CQL &lt;code&gt;WHERE&lt;/code&gt; clause&lt;/li&gt; &#xA; &lt;li&gt;Allows for execution of arbitrary CQL statements&lt;/li&gt; &#xA; &lt;li&gt;Plays nice with Cassandra Virtual Nodes&lt;/li&gt; &#xA; &lt;li&gt;Could be used in all languages supporting Datasets/DataFrames API: Python, R, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Version Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The connector project has several branches, each of which map into different supported versions of Spark and Cassandra. For previous releases the branch is named &#34;bX.Y&#34; where X.Y is the major+minor version; for example the &#34;b1.6&#34; branch corresponds to the 1.6 release. The &#34;master&#34; branch will normally contain development for the next connector release in progress.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the following branches are actively supported: 3.2.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/master&#34;&gt;master&lt;/a&gt;), 3.1.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.1&#34;&gt;b3.1&lt;/a&gt;), 3.0.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.0&#34;&gt;b3.0&lt;/a&gt;) and 2.5.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b2.5&#34;&gt;b2.5&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Spark&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra Java Driver&lt;/th&gt; &#xA;   &lt;th&gt;Minimum Java Version&lt;/th&gt; &#xA;   &lt;th&gt;Supported Scala Versions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.13&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.2&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0, 2.1, 2.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;1.5, 1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;   &lt;td&gt;1.1, 1.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0, 0.9&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*&lt;em&gt;Compatible with 2.1.X where X &amp;gt;= 5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hosted API Docs&lt;/h2&gt; &#xA;&lt;p&gt;API documentation for the Scala and Java interfaces are available online:&lt;/p&gt; &#xA;&lt;h3&gt;3.2.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.0.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.5.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/&#34;&gt;Embedded-Cassandra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;This project is available on the Maven Central Repository. For SBT to download the connector binaries, sources and javadoc, put this in your project SBT config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.datastax.spark&#34; %% &#34;spark-cassandra-connector&#34; % &#34;3.2.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default Scala version for Spark 3.0+ is 2.12 please choose the appropriate build. See the &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;FAQ&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/0_quick_start.md&#34;&gt;Quick-start guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/1_connecting.md&#34;&gt;Connecting to Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/2_loading.md&#34;&gt;Loading datasets from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/3_selection.md&#34;&gt;Server-side data selection and filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/4_mapper.md&#34;&gt;Working with user-defined case classes and tuples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/5_saving.md&#34;&gt;Saving and deleting datasets to/from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/6_advanced_mapper.md&#34;&gt;Customizing the object mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/7_java_api.md&#34;&gt;Using Connector in Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/8_streaming.md&#34;&gt;Spark Streaming with Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/10_embedded.md&#34;&gt;The spark-cassandra-connector-embedded Artifact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/11_metrics.md&#34;&gt;Performance monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/13_spark_shell.md&#34;&gt;The Spark Shell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/14_data_frames.md&#34;&gt;DataFrames&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/15_python.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/16_partitioning.md&#34;&gt;Partitioner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/17_submitting.md&#34;&gt;Submitting applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/reference.md&#34;&gt;Configuration Parameter Reference Table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Training&lt;/h2&gt; &#xA;&lt;h3&gt;DataStax Academy&lt;/h3&gt; &#xA;&lt;p&gt;DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In &lt;a href=&#34;https://academy.datastax.com/courses/ds320-analytics-with-apache-spark&#34;&gt;DS320: Analytics with Spark&lt;/a&gt;, you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Reporting Bugs&lt;/h3&gt; &#xA;&lt;p&gt;New issues may be reported using &lt;a href=&#34;https://datastax-oss.atlassian.net/browse/SPARKC/&#34;&gt;JIRA&lt;/a&gt;. Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Questions and requests for help may be submitted to the &lt;a href=&#34;https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user&#34;&gt;user mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Q/A Exchange&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;DataStax Community&lt;/a&gt; provides a free question and answer website for any and all questions relating to any DataStax Related technology. Including the Spark Cassandra Connector. Both DataStax engineers and community members frequent this board and answer questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To protect the community, all contributors are required to sign the &lt;a href=&#34;http://spark-cassandra-connector-cla.datastax.com/&#34;&gt;DataStax Spark Cassandra Connector Contribution License Agreement&lt;/a&gt;. The process is completely electronic and should only take a few minutes.&lt;/p&gt; &#xA;&lt;p&gt;To develop this project, we recommend using IntelliJ IDEA. Make sure you have installed and enabled the Scala Plugin. Open the project with IntelliJ IDEA and it will automatically create the project structure from the provided SBT configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checklist for contributing changes to the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;a href=&#34;https://datastax-oss.atlassian.net/projects/SPARKC/issues&#34;&gt;SPARKC JIRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make sure that all unit tests and integration tests pass&lt;/li&gt; &#xA; &lt;li&gt;Add an appropriate entry at the top of CHANGES.txt&lt;/li&gt; &#xA; &lt;li&gt;If the change has any end-user impacts, also include changes to the ./doc files as needed&lt;/li&gt; &#xA; &lt;li&gt;Prefix the pull request description with the JIRA number, for example: &#34;SPARKC-123: Fix the ...&#34;&lt;/li&gt; &#xA; &lt;li&gt;Open a pull-request on GitHub and await review&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To run unit and integration tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the integration tests require &lt;a href=&#34;https://github.com/riptano/ccm&#34;&gt;CCM&lt;/a&gt; to be installed on your machine. See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;By default, integration tests start up a separate, single Cassandra instance and run Spark in local mode. It is possible to run integration tests with your own Cassandra and/or Spark cluster. First, prepare a jar with testing code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test:package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then copy the generated test jar to your Spark nodes and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export IT_TEST_CASSANDRA_HOST=&amp;lt;IP of one of the Cassandra nodes&amp;gt;&#xA;export IT_TEST_SPARK_MASTER=&amp;lt;Spark Master URL&amp;gt;&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Documents&lt;/h2&gt; &#xA;&lt;p&gt;To generate the Reference Document use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt spark-cassandra-connector-unshaded/run (outputLocation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;outputLocation defaults to doc/reference.md&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2014-2017, DataStax, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/nlpstack</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/allenai/nlpstack</id>
    <link href="https://github.com/allenai/nlpstack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NLP toolkit (tokenizer, POS-tagger, parser, etc.)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NLP Stack&lt;/h1&gt; &#xA;&lt;p&gt;This contains our basic stack of NLP tools. You can play with them &lt;a href=&#34;http://nlpstack.dev.allenai.org:8062/tools.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have general interfaces on each tool so we have a clear definition of the inputs and outputs of each tool and so we can change the underlying implementation of a tool.&lt;/p&gt; &#xA;&lt;p&gt;Each tool also has a serialization format for its output. For example, there is a dependency string format and a chunked sentence string format.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add NLPStack to your dependencies. NLPStack comes as a collection of multiple tools (see below). To declare dependencies, you can use this code in your Build.scala file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;org.allenai.nlpstack&#34; %% &#34;nlpstack-core&#34; % &#34;0.x&#34;&#xA;&#xA;libraryDependencies += &#34;org.allenai.nlpstack&#34; %% &#34;nlpstack-parse&#34; % &#34;0.x&#34;&#xA;&#xA;libraryDependencies += &#34;org.allenai.nlpstack&#34; %% &#34;nlpstack-postag&#34; % &#34;0.x&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As an option, you can define a function for the various nlpstack components, and use them like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def nlpstackModule(id: String) = &#34;org.allenai.nlpstack&#34; %% s&#34;nlpstack-${id}&#34; % &#34;0.x&#34;&#xA;&#xA;libraryDependencies += nlpstackModule(&#34;parse&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start using NLPStack. Here is a quick code snippet that parses a sentence:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.allenai.nlpstack.tokenize.defaultTokenizer&#xA;import org.allenai.nlpstack.postag.defaultPostagger&#xA;import org.allenai.nlpstack.parse.defaultDependencyParser&#xA;&#xA;/* ... */&#xA;&#xA;val tokens = defaultTokenizer.tokenize(&#xA;  &#34;I was wondering why the ball kept getting bigger and bigger, and then it hit me.&#34;)&#xA;val postaggedTokens = defaultPostagger.postagTokenized(tokens)&#xA;val dependencyGraph = defaultDependencyParser.dependencyGraphPostagged(postaggedTokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Folder Layout&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;tools: this project contains the main Nlpstack code.&lt;/li&gt; &#xA; &lt;li&gt;webapp: a web application for running tools and visualizing serialized representations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tools in the Kit&lt;/h2&gt; &#xA;&lt;p&gt;Presently the AI Toolkit includes the following tools.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenizer&lt;/strong&gt;. Break a sentence into &#34;word&#34; tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lemmatizer&lt;/strong&gt;. Associate a base form to a token or a Part-of-Speech (POS) tagged token. The results will be more accurate if POS tags are available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Postagger&lt;/strong&gt;. Associate a POS tag with a token.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chunker&lt;/strong&gt;. Associate chunk ranges with POS-tagged tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dependency Parser&lt;/strong&gt;. Construct dependencies between POS-tagged tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Segmenter&lt;/strong&gt;. Split a body of text into sentences.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Each tool includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An API so it can be called programatically.&lt;/li&gt; &#xA; &lt;li&gt;A CLI application so it can be run in batch.&lt;/li&gt; &#xA; &lt;li&gt;A simple REST server so it can be called remotely.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tool Subprojects&lt;/h2&gt; &#xA;&lt;p&gt;Nlpstack is split up into multiple subprojects to minimize the number of dependencies needed to install components. The source for each of these is in &lt;code&gt;tools/${projectName}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tools-core&lt;/code&gt;: This contains all of the APIs needed for interoperating with Nlpstack, but none of the implementations. &lt;code&gt;tools-segment&lt;/code&gt;: Implementation of the segmenter. Depends on &lt;code&gt;core&lt;/code&gt;. &lt;code&gt;tools-lemmatize&lt;/code&gt;: Implementation of the lemmatizer. Depends on &lt;code&gt;core&lt;/code&gt;. &lt;code&gt;tools-tokenize&lt;/code&gt;: Implementation of the tokenizer. Depends on &lt;code&gt;core&lt;/code&gt;. &lt;code&gt;tools-postag&lt;/code&gt;: Implementation of the POS tagger. Depends on &lt;code&gt;tokenize&lt;/code&gt;. &lt;code&gt;tools-chunk&lt;/code&gt;: Implementation of the sentence chunker. Depends on &lt;code&gt;postag&lt;/code&gt;. &lt;code&gt;tools-parse&lt;/code&gt;: Implementation of the dependency parser. Depends on &lt;code&gt;postag&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These each produce a single artifact, named &lt;code&gt;nlptools-${projectName}&lt;/code&gt;. A client should depend on every implementation they will be using, as well as &lt;code&gt;nlpstack-core&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These all use the group &lt;code&gt;org.allenai.nlpstack&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;So, if you wanted to use the tokenizer, you should have the dependencies (in sbt):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;org.allenai.nlpstack&#34; %% &#34;nlpstack-core&#34; % &#34;2014.6.23-1-SNAPSHOT&#34;&#xA;&#34;org.allenai.nlpstack&#34; %% &#34;nlpstack-tokenize&#34; % &#34;2014.6.23-1-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The current version is in &lt;a href=&#34;https://raw.githubusercontent.com/allenai/nlpstack/master/version.sbt&#34;&gt;version.sbt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Parsing API Details&lt;/h3&gt; &#xA;&lt;p&gt;The example in &#34;Getting Started&#34; shows how to generate a &lt;a href=&#34;https://github.com/allenai/nlpstack/raw/master/tools/core/src/main/scala/org/allenai/nlpstack/core/parse/graph/DependencyGraph.scala&#34;&gt;dependency graph&lt;/a&gt; from a sentence. The graph object itself contains &lt;a href=&#34;https://github.com/allenai/nlpstack/raw/master/tools/core/src/main/scala/org/allenai/nlpstack/core/parse/graph/DependencyNode.scala&#34;&gt;dependency nodes&lt;/a&gt; with integer IDs. These IDs can be used to index the original tokens given to the parser.&lt;/p&gt; &#xA;&lt;p&gt;If you want to have lemmatized token information, you&#39;ll want to run the tokens through a lemmatizer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import org.allenai.nlpstack.lemmatize.MorphaStemmer&#xA;&#xA;val lemmatizer = new MorphaStemmer()&#xA;val lemmatizedTokens = postaggedTokens map { lemmatizer.lemmatizePostaggedToken }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you have lemmatized tokens, you can build a new dependency graph with token information contained in the nodes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;val dependencyGraphWithTokenInfo = dependencyGraph.tokenized(lemmatizedTokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Releasing new versions&lt;/h2&gt; &#xA;&lt;p&gt;This project releases to Maven Central rather than to our own repository. To do this, you need a bit of setup.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You need the signing keys to publish software with. You can find them in the &lt;code&gt;ai2-secure&lt;/code&gt; bucket in S3 under the key &lt;code&gt;Sonatype Key Pair.zip&lt;/code&gt;. Copy that file to &lt;code&gt;~/.sbt/gpg/&lt;/code&gt; and extract it there.&lt;/li&gt; &#xA; &lt;li&gt;You need the passphrase for that key pair. It&#39;s defined as an array, which is a little weird, and goes into another location in &lt;code&gt;~/.sbt&lt;/code&gt;. The line defining it is in &lt;code&gt;passwords.txt&lt;/code&gt; in the &lt;code&gt;ai2-secure&lt;/code&gt; bucket. Copy that line into &lt;code&gt;~/.sbt/0.13/allenai.sbt&lt;/code&gt; (or into some other &lt;code&gt;.sbt&lt;/code&gt; if you like).&lt;/li&gt; &#xA; &lt;li&gt;To use the passphrase, we have to enable the &lt;code&gt;sbt-pgp&lt;/code&gt; plugin. Put the following line into &lt;code&gt;~/.sbt/0.13/plugins/gpg.sbt&lt;/code&gt;: &lt;code&gt;addSbtPlugin(&#34;com.jsuereth&#34; % &#34;sbt-pgp&#34; % &#34;1.0.0&#34;)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;We also need credentials to the sonatype repository. We get those with the following line in &lt;code&gt;~/.sbt/0.13/sonatypt.sbt&lt;/code&gt;: &lt;code&gt;credentials += Credentials(&#34;Sonatype Nexus Repository Manager&#34;, &#34;oss.sonatype.org&#34;, &#34;allenai-role&#34;, &#34;&amp;lt;password&amp;gt;&#34;)&lt;/code&gt;. You find this password in the same &lt;code&gt;password.txt&lt;/code&gt; file from above.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, you need to register your GPG key.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Start SBT in the nlpstack project&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;At the SBT prompt, type:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pgp-cmd send-key [TAB]&#xA;Paul Allen Institute for Artificial Intelligence &amp;lt;account&amp;gt;&#xA;abcdefg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you hit [TAB], SBT should print out the available key and its ID on the second line (in the example above, &lt;code&gt;abcdefg&lt;/code&gt;. Enter the id:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pgp-cmd send-key abcdefg hkp://keyserver.ubuntu.com [ENTER]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With this, you should be ready to run &lt;code&gt;sbt release&lt;/code&gt; on the common project. When you do, it will upload the build artifacts to a staging repository on &lt;a href=&#34;http://oss.sonatype.org&#34;&gt;http://oss.sonatype.org&lt;/a&gt;. When it&#39;s done, you have to go there and first close, and then release, the staging repository. That initiates the upload to Maven Central, which will take about 10 minutes.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;http://oss.sonatype.org&#34;&gt;http://oss.sonatype.org&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Log in with username &lt;code&gt;allenai-role&lt;/code&gt;, and the password from the &lt;code&gt;password.txt&lt;/code&gt; file. This is the same password you used in step 4 above.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;staging repositories&#34; on the left.&lt;/li&gt; &#xA; &lt;li&gt;Use the search bar at the top right to search for &#34;allenai&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Find your staging repository and confirm that it has the contents you expect. Then, select it and click &#34;Close&#34;. Closing takes a few minutes. Then you can see how the closing process went under &#34;Activity&#34;. It sends an email to &lt;code&gt;dev-role@allenai.org&lt;/code&gt; when it&#39;s done.&lt;/li&gt; &#xA; &lt;li&gt;When it is done, select the repository again and hit &#34;Release&#34;.&lt;/li&gt; &#xA; &lt;li&gt;You should see the new version appear under &lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/org/allenai/nlpstack/&#34;&gt;https://oss.sonatype.org/content/repositories/releases/org/allenai/nlpstack/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You are done!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/taggers</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/allenai/taggers</id>
    <link href="https://github.com/allenai/taggers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easily identify and label sentence intervals using various taggers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Taggers&lt;/h1&gt; &#xA;&lt;p&gt;A tagger is a function from a sentence to a list of &lt;code&gt;Type&lt;/code&gt;s. A &lt;code&gt;Type&lt;/code&gt; consists of a name and the token interval it came from in the source sentence.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;For example, you might have a tagger that identifies animals. Following is the string serialized form of a tagger. To the left of &lt;code&gt;:=&lt;/code&gt; is the name of the tagger - when the tagger finds a type it will have this name. To the right of &lt;code&gt;:=&lt;/code&gt; is the tagger class. This is a Java/Scala class; if no package is specified &lt;code&gt;taggers&lt;/code&gt; will look in &lt;code&gt;org.allenai.taggers.tag&lt;/code&gt;. Between the braces &lt;code&gt;{}&lt;/code&gt; are the arguments to the tagger.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Animal := LemmatizedKeywordTagger {&#xA;  cat&#xA;  kitten&#xA;  dog&#xA;  puppy&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this tagger were to run over the following sentence, we would get some types.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Kittens are very cute , but they turn into cats .&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;Type(name=Animal, text=&#34;Kittens&#34;, interval=[0, 1))&#xA;Type(name=Animal, text=&#34;cats&#34;, interval=[10, 11))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;taggers&lt;/code&gt; project is composed of three subprojects: &lt;code&gt;core&lt;/code&gt;, which contains the algorithms, &lt;code&gt;cli&lt;/code&gt; which has a small cli application, and &lt;code&gt;webapp&lt;/code&gt;, which contains the web demo. The project is built with &lt;code&gt;sbt&lt;/code&gt;. For example, to run the web demo, you can execute the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt compile &#39;project webapp&#39; run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run taggers as a cli.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt compile &#39;project cli&#39; &#39;run examples/reverb.taggers&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want an example of how to use the taggers project as a dependency, please look at &lt;code&gt;taggers-webapp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cascades&lt;/h2&gt; &#xA;&lt;p&gt;Taggers can be organized into a cascade with multiple levels. A cascade is defined by a cascade file which contains a list of taggers files (separated by newline) followed by any number of extractor definitions (see the &#34;Extractors&#34; section).&lt;/p&gt; &#xA;&lt;p&gt;For example, we might have &lt;code&gt;hello.cascade&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;first.taggers&#xA;second.taggers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can use the multiple files to organize our tagger definitions. Cascades can also share tagger definition files between them.&lt;/p&gt; &#xA;&lt;h2&gt;Types of Taggers&lt;/h2&gt; &#xA;&lt;h3&gt;OpenRegex&lt;/h3&gt; &#xA;&lt;p&gt;This tagger compiles regular expressions over the tokens in a sentence into an NFA (for more information, see the &lt;a href=&#34;https://github.com/knowitall/openregex&#34;&gt;Open Regex&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/allenai/taggers/master/git@github.com:knowitall/openregex-scala.git&#34;&gt;Open Regex Scala&lt;/a&gt; libraries). A token is describedas a logical formula between angled brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;. There are a number of fields that can be matched upon for each token.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;string&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;lemma&lt;/strong&gt;: the lemmatized form of the token string (see MorphaStemmer in nlpstack)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pos&lt;/strong&gt;: the part-of-speech tag&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;chunk&lt;/strong&gt;: the chunk tag&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;type&lt;/strong&gt;: any type that intersects this token&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;typeStart&lt;/strong&gt;: any type that starts at this token&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;typeEnd&lt;/strong&gt;: any type that ends at this token&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;typeCont&lt;/strong&gt;: any type that intersects at this token but does not start or end there&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A field can be matched in one of three ways.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;With double quotes &lt;code&gt;&#34;&lt;/code&gt;. Strings are interpreted the same was as Java strings (backslash is the escape character).&lt;/li&gt; &#xA; &lt;li&gt;With single quotes &lt;code&gt;&#39;&lt;/code&gt;. The text between two single quotes will be taken verbatim (there is no escape character).&lt;/li&gt; &#xA; &lt;li&gt;With slashes &lt;code&gt;/&lt;/code&gt;. The text between the slashes will be interpreted as a regular expression. Backslash is the escape character so &lt;code&gt;\\&lt;/code&gt; becomes a single backslash and &lt;code&gt;\/&lt;/code&gt; escapes the forward slash.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If a quotation prefixed by &#34;i&#34; then the match will be case-insensitive (i.e. &lt;code&gt;string = i&#34;cat&#34;&lt;/code&gt; will match &#34;cAt&#34;).&lt;/p&gt; &#xA;&lt;p&gt;A pattern tagger makes types with the tagger name, but also &lt;code&gt;LinkedType&lt;/code&gt;s for each matching group. A &lt;code&gt;LinkedType&lt;/code&gt; has an Optional &lt;code&gt;Type&lt;/code&gt; field that points to its parent &lt;code&gt;Type&lt;/code&gt; and a name field with a special syntax. If the tagger is named &lt;code&gt;T&lt;/code&gt; and a matching group is named &lt;code&gt;G1&lt;/code&gt; for example, the tagger will create a &lt;code&gt;LinkedType&lt;/code&gt; with the name &lt;code&gt;T.G1&lt;/code&gt;. If there is an unnamed matching group a &lt;code&gt;LinkedType&lt;/code&gt; will be created with the group number (i.e. &lt;code&gt;T.1&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;There is a lot of redundancy in their expressiveness. For example, OpenRegex supports pattern matching on the fields .This is not necessary but is an optimization and a shorthand. For example, the following two&#39; patterns match the same text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;pos=/NNPS?/&amp;gt;&#xA;&amp;lt;pos=&#34;NNP&#34;&amp;gt; | &amp;lt;pos=&#34;NNPS&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here are some more equivalent examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;pos=&#34;JJ&#34;&amp;gt;* &amp;lt;pos=/NNP./&amp;gt;+&#xA;&amp;lt;pos=&#34;JJ&#34;&amp;gt;* &amp;lt;pos=/NNPS?/&amp;gt;+&#xA;&amp;lt;pos=&#34;JJ&#34;&amp;gt;* &amp;lt;pos=&#34;NNP&#34; | pos=&#34;NNPS&#34;&amp;gt;+&#xA;&amp;lt;pos=&#34;JJ&#34;&amp;gt;* (?:&amp;lt;pos=&#34;NNP&#34;&amp;gt; | &amp;lt;pos=&#34;NNPS&#34;&amp;gt;)+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that (3) and (4) are not preferred for efficiency reasons. Regex OR (in example (4)) should only be used on multi-token sequences.&lt;/p&gt; &#xA;&lt;p&gt;The Regular Expressions support named groups &lt;code&gt;(&amp;lt;name&amp;gt;: ... )&lt;/code&gt;, unnamed groups &lt;code&gt;(?: ... )&lt;/code&gt;, and capturing groups &lt;code&gt;( ... )&lt;/code&gt;. The operators allowed are &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;?&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, and &lt;code&gt;|&lt;/code&gt;. The Logic Expressions (that describe each token) allow grouping &lt;code&gt;( ... )&lt;/code&gt;, not &lt;code&gt;!&lt;/code&gt;, or &lt;code&gt;|&lt;/code&gt;, and and &lt;code&gt;&amp;amp;&lt;/code&gt;. To learn more about the regular expression language, see &lt;a href=&#34;https://github.com/knowitall/openregex&#34;&gt;https://github.com/knowitall/openregex&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Named groups create output subtypes. For example, if we had the following &lt;code&gt;OpenRegex&lt;/code&gt; applied to the example below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DescribedNoun := OpenRegex {&#xA;    (&amp;lt;Description&amp;gt;:&amp;lt;pos=&#39;JJ&#39;&amp;gt;+) (&amp;lt;Noun&amp;gt;:&amp;lt;pos=&#39;NN&#39;&amp;gt;+)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The huge fat cat lingered in the hallway.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We would get the following output types.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DescribedNoun(huge fat cat)&#xA;DescribedNoun.Description(huge fat)&#xA;DescribedNoun.Noun(cat)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TypedOpenRegex&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;TypedOpenRegex&lt;/code&gt; extends the &lt;code&gt;OpenRegex&lt;/code&gt; with added syntax to match types. Since a type can span multiple tokens but the pattern language operates on the token level, matching types can be tedious and error prone. For example, if you want to match the type &lt;code&gt;Animal&lt;/code&gt;, you need the following pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(?:(?:&amp;lt;typeStart=&#39;Animal&#39; &amp;amp; typeEnd=&#39;Animal&#39;&amp;gt;) | (?: &amp;lt;typeStart=&#39;Animal&#39; &amp;amp; !typeEnd=&#39;Animal&#39;&amp;gt; &amp;lt;typeCont=&#39;Animal&#39; &amp;amp; !typeEnd=&#39;Animal&#39;&amp;gt;* &amp;lt;typeEnd=&#39;Animal&#39;&amp;gt;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Matching many types in this manner quickly makes unreadable patterns, so the &lt;code&gt;TypedOpenRegex&lt;/code&gt; adds the syntax &lt;code&gt;@Type&lt;/code&gt; which, if the type is Animal (&lt;code&gt;@Animal&lt;/code&gt;) it would expand into the above expression. With this syntax, it&#39;s easy to match on types. For an implementation of &lt;code&gt;ReVerb&lt;/code&gt;, see &lt;code&gt;examples/reverb.taggers&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Extractors&lt;/h2&gt; &#xA;&lt;p&gt;You can define extractors which build a structured string from a matched type. Extractors look similar to a Scala anonymous function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;x: NestedExtraction =&amp;gt; (${x.Arg1}, ${x.NestedRelation}, (${x.BaseArg1}, ${x.BaseRelation}, ${x.BaseArg2}))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The left part (split it by &lt;code&gt;=&amp;gt;&lt;/code&gt;) says we should apply this pattern to any &lt;code&gt;NestedExtraction&lt;/code&gt; type. The right part tells us what string we should build from that &lt;code&gt;NestedExtraction&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;String substitutions are used to build an interesting string. If the bound variable (look all the way to the left) is &lt;code&gt;x&lt;/code&gt;, A string substitution looks like &lt;code&gt;${x.expr|expr|...}&lt;/code&gt; where &lt;code&gt;expr&lt;/code&gt; is either just a subtype, or a subtype with a matching expression. Expressions can be chained with &lt;code&gt;|&lt;/code&gt; in case one might fail. The last part of a fallback chain may be a string.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${x.Arg1|x.Arg2|&#39;fallback&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the last fallback option fails, an exception is thrown.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes you want to move to another type. In this case, the &lt;code&gt;x.subtype1:matching.subtype2&lt;/code&gt; pattern is used. Here matching is the type that overlaps &lt;code&gt;x.subtype1&lt;/code&gt; and &lt;code&gt;subtype2&lt;/code&gt; is a subtype of `matching. You may chain matching expressions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/deep_qa_experiments</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/allenai/deep_qa_experiments</id>
    <link href="https://github.com/allenai/deep_qa_experiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala framework for running experiments with allenai/deep_qa&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://semaphoreci.com/allenai/deep_qa_experiments&#34;&gt;&lt;img src=&#34;https://semaphoreci.com/api/v1/allenai/deep_qa_experiments/branches/master/shields_badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Deep QA Experiments&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains scala code for setting up and running experiments with &lt;a href=&#34;https://github.com/allenai/deep_qa&#34;&gt;DeepQA&lt;/a&gt;. The main point here is to have reasonable pipeline management for setting up data and running comparison experiments between several models on some dataset.&lt;/p&gt; &#xA;&lt;h1&gt;Usage Guide&lt;/h1&gt; &#xA;&lt;p&gt;To use this code, you set up an experiment in scala, then run it using &lt;code&gt;sbt&lt;/code&gt;. Some documentation on how to do this is found in the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/deep_qa_experiments/master/src/main/scala/org/allenai/deep_qa/experiments/&#34;&gt;README for the &lt;code&gt;org.allenai.deep_qa.experiments&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you use this code and think something could be improved, pull requests are very welcome. Opening an issue is ok, too, but we&#39;re a lot more likely to respond to a PR. The primary maintainer of this code is &lt;a href=&#34;https://matt-gardner.github.io/&#34;&gt;Matt Gardner&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This code is released under the terms of the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;GNU General Public License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/pnp</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/allenai/pnp</id>
    <link href="https://github.com/allenai/pnp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Probabilistic Neural Programming&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Probabilistic Neural Programs&lt;/h1&gt; &#xA;&lt;p&gt;Probabilistic Neural Programming (PNP) is a Scala library for expressing, training and running inference in neural network models that &lt;strong&gt;include discrete choices&lt;/strong&gt;. The enhanced expressivity of PNP is useful for structured prediction, reinforcement learning, and latent variable models.&lt;/p&gt; &#xA;&lt;p&gt;Probabilistic neural programs have several advantages over computation graph libraries for neural networks, such as TensorFlow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Probabilistic inference&lt;/strong&gt; is implemented within the library. For example, running a beam search to (approximately) generate the highest-scoring output sequence of a sequence-to-sequence model takes 1 line of code in PNP.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Additional training algorithms&lt;/strong&gt; that require running inference during training are part of the library. This includes learning-to-search algorithms, such as LaSO, reinforcement learning, and training latent variable models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Computation graphs&lt;/strong&gt; are a subset of probabilistic neural programs. We use &lt;a href=&#34;https://github.com/clab/dynet&#34;&gt;DyNet&lt;/a&gt; to express neural networks, which provides a rich set of operations and efficient training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This library depends on DyNet with the &lt;a href=&#34;https://github.com/clab/dynet/tree/master/contrib/swig&#34;&gt;Scala DyNet bindings&lt;/a&gt;. See the link for build instructions. After building this library, run the following commands from the &lt;code&gt;pnp&lt;/code&gt; root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lib&#xA;ln -s &amp;lt;PATH_TO_DYNET&amp;gt;/build/contrib/swig/dynet_swigJNI_scala.jar .&#xA;ln -s &amp;lt;PATH_TO_DYNET&amp;gt;/build/contrib/swig/dynet_swigJNI_dylib.jar .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! Verify that your installation works by running &lt;code&gt;sbt test&lt;/code&gt; in the root directory.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;This section describes how to use probabilistic neural programs to define and train a model. The typical usage has three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Define a model.&lt;/strong&gt; Models are implemented by writing a function that takes your problem input and outputs &lt;code&gt;Pnp[X]&lt;/code&gt; objects. The probabilistic neural program type &lt;code&gt;Pnp[X]&lt;/code&gt; represents a function from neural network parameters to probability distributions over values of type &lt;code&gt;X&lt;/code&gt;. Each program describes a (possibly infinite) space of executions, each of which returns a value of type &lt;code&gt;X&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train.&lt;/strong&gt; Training is performed by passing a list of examples to a &lt;code&gt;Trainer&lt;/code&gt;, where each example consists of a &lt;code&gt;Pnp[X]&lt;/code&gt; object and a label. Labels are implemented as functions that assign costs to program executions or as conditional distributions over correct executions. Many training algorithms can be used, from loglikelihood to learning-to-search algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run the model.&lt;/strong&gt; A model can be run on a new input by constructing the appropriate &lt;code&gt;Pnp[X]&lt;/code&gt; object, then running inference on this object with trained parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These steps are illustrated in detail for a sequence-to-sequence model in &lt;a href=&#34;https://raw.githubusercontent.com/allenai/pnp/master/src/main/scala/org/allenai/pnp/examples/Seq2Seq.scala&#34;&gt;Seq2Seq2.scala&lt;/a&gt;. For a more complex example, run the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/pnp/master/experiments/geoquery/scripts/example.sh&#34;&gt;GeoQuery semantic parsing experiment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Defining Probabilistic Neural Programs&lt;/h2&gt; &#xA;&lt;p&gt;Probabilistic neural programs are specified by writing the forward computation of a neural network, using the &lt;code&gt;choose&lt;/code&gt; operation to represent discrete choices. Roughly, we can write:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val pnp = for {&#xA;  scores1 &amp;lt;- ... some neural net operations ...&#xA;  // Make a discrete choice&#xA;  x1 &amp;lt;- choose(values, scores1)&#xA;  scores2 &amp;lt;- ... more neural net operations, may depend on x1 ...&#xA;  ...&#xA;  xn &amp;lt;- choose(values, scoresn)&#xA;} yield {&#xA;  xn&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;pnp&lt;/code&gt; then represents a function that takes some neural network parameters and returns a distribution over possible values of &lt;code&gt;xn&lt;/code&gt; (which in turn depends on the values of intermediate choices). We evaluate &lt;code&gt;pnp&lt;/code&gt; by running inference, which simultaneously runs the forward pass of the network and performs probabilistic inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;nnParams = ... &#xA;val dist = pnp.beamSearch(10, nnParams)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Choose&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;choose&lt;/code&gt; operator defines a distribution over a list of values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val flip: Pnp[Boolean] = choose(Array(true, false), Array(0.5, 0.5))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This snippet creates a probability distribution that returns either true or false with 50% probability. &lt;code&gt;flip&lt;/code&gt; has type &lt;code&gt;Pnp[Boolean]&lt;/code&gt;, which represents a function from neural network parameters to probability distributions over values of type &lt;code&gt;Boolean&lt;/code&gt;. (In this case it&#39;s just a probability distribution since we haven&#39;t referenced any parameters.) Note that &lt;code&gt;flip&lt;/code&gt; is not a draw from the distribution, rather, &lt;em&gt;it is the distribution itself&lt;/em&gt;. The probability of each choice can be given to &lt;code&gt;choose&lt;/code&gt; either in an explicit list (as above) or via an &lt;code&gt;Expression&lt;/code&gt; of a neural network.&lt;/p&gt; &#xA;&lt;p&gt;We compose distributions using &lt;code&gt;for {...} yield {...}&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val twoFlips: Pnp[Boolean] = for {&#xA;  x &amp;lt;- flip&#xA;  y &amp;lt;- flip&#xA;} yield {&#xA;  x &amp;amp;&amp;amp; y&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This program returns &lt;code&gt;true&lt;/code&gt; if two independent draws from &lt;code&gt;flip&lt;/code&gt; both return &lt;code&gt;true&lt;/code&gt;. The notation &lt;code&gt;x &amp;lt;- flip&lt;/code&gt; can be thought of as drawing a value from &lt;code&gt;flip&lt;/code&gt; and assigning it to &lt;code&gt;x&lt;/code&gt;. However, we can only use the value within the for/yield block to construct another probability distribution. We can now run inference on this object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val marginals3 = twoFlips.beamSearch(5)&#xA;println(marginals3.marginals().getProbabilityMap)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prints out the expected probabilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{false=0.75, true=0.25}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;Probabilistic neural programs have access to an underlying computation graph that is used to define neural networks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def mlp(x: FloatVector): Pnp[Boolean] = {&#xA;  for {&#xA;    // Get the computation graph&#xA;    cg &amp;lt;- computationGraph()&#xA;&#xA;    // Get the parameters of a multilayer perceptron by name.&#xA;    // The dimensionalities and values of these parameters are &#xA;    // defined in a PnpModel that is passed to inference.&#xA;    weights1 &amp;lt;- param(&#34;layer1Weights&#34;)&#xA;    bias1 &amp;lt;- param(&#34;layer1Bias&#34;)&#xA;    weights2 &amp;lt;- param(&#34;layer2Weights&#34;)&#xA;&#xA;    // Input the feature vector to the computation graph and&#xA;    // run the multilayer perceptron to produce scores.&#xA;    inputExpression = input(cg.cg, Seq(FEATURE_VECTOR_DIM), x)&#xA;    scores = weights2 * tanh((weights1 * inputExpression) + bias1)&#xA;&#xA;     // Choose a label given the scores. Scores is expected to&#xA;     // be a 2-element vector, where the first element is the score&#xA;     // of true, etc.&#xA;     y &amp;lt;- choose(Array(true, false), scores)&#xA;  } yield {&#xA;    y&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can then evaluate the network on an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val model = PnpModel.init(true)&#xA;// Initialize the network parameters. The values are&#xA;// chosen randomly.&#xA;model.addParameter(&#34;layer1Weights&#34;, Seq(HIDDEN_DIM, FEATURE_VECTOR_DIM))&#xA;model.addParameter(&#34;layer1Bias&#34;, Seq(HIDDEN_DIM))&#xA;model.addParameter(&#34;layer2Weights&#34;, Seq(2, HIDDEN_DIM))&#xA;&#xA;// Run the multilayer perceptron on featureVector&#xA;val featureVector = new FloatVector(Seq(1.0f, 2.0f, 3.0f))&#xA;val dist = mlp(featureVector)&#xA;val marginals = dist.beamSearch(2, model)&#xA; &#xA;for (x &amp;lt;- marginals.executions) {&#xA;  println(x)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prints something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[Execution true -0.4261836111545563]&#xA;[Execution false -1.058420181274414]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each execution has a single value that is an output of our program and a score derived from the neural network computation. In this case, the scores are log probabilities, but the scores may have different semantics depending on the way the model is defined and its parameters are trained.&lt;/p&gt; &#xA;&lt;p&gt;Pnp uses Dynet as the underlying neural network library, which provides a rich set of operations (e.g., LSTMs). See the &lt;a href=&#34;http://dynet.readthedocs.io/en/latest/operations.html&#34;&gt;Dynet documentation&lt;/a&gt; for details, along with the documentation for &lt;a href=&#34;https://github.com/allenai/dynet/tree/master/swig&#34;&gt;Dynet Scala bindings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;TODO: document usage of RNNBuilders, which have to be used statelessly.&lt;/p&gt; &#xA;&lt;h3&gt;Defining Richer Models&lt;/h3&gt; &#xA;&lt;p&gt;Probabilistic neural programs can be easily composed to construct richer models using &lt;code&gt;for {...} yield {...}&lt;/code&gt;. For example, we can define a CRF sequence tagger using the multilayer perceptron above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def sequenceTag(xs: Seq[FloatVector]): Pnp[List[Boolean]] = {&#xA;  xs.foldLeft(Pnp.value(List[Boolean]()))((x, y) =&amp;gt; for {&#xA;    cur &amp;lt;- mlp(y)&#xA;    rest &amp;lt;- x&#xA;&#xA;    cg &amp;lt;- computationGraph()&#xA;    _ &amp;lt;- if (rest.length &amp;gt; 0) {&#xA;      // Add a factor to the model that scores adjacent labels&#xA;      // in the sequence. Here, labelNn runs a neural network&#xA;      // whose inputs are cur and the next label, and whose output&#xA;      // is a 1-element vector containing the score.&#xA;      score(labelNn(cur, rest.head, cg.cg))&#xA;    } else {&#xA;      value(())&#xA;    }&#xA;  } yield {&#xA;    cur :: rest&#xA;  })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can now run this model on a sequence of feature vectors in the same way as the multilayer perceptron:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Same model as before, but make it globally normalized &#xA;// and add some more parameters for labelNn&#xA;model.locallyNormalized = false&#xA;model.addLookupParameter(&#34;left&#34;, 2, Seq(LABEL_DIM))&#xA;model.addLookupParameter(&#34;right&#34;, 2, Seq(LABEL_DIM))&#xA;&#xA;val featureVectors = Seq(new FloatVector(...), new FloatVector(...), new FloatVector(...))&#xA;val dist = sequenceTag(featureVectors)&#xA;val marginals = dist.beamSearch(5, model)&#xA;for (x &amp;lt;- marginals.executions) {&#xA;  println(x)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prints something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[Execution List(true, true, true) 5.28779661655426]&#xA;[Execution List(false, true, true) 1.7529568672180176]&#xA;[Execution List(true, true, false) 1.4970757961273193]&#xA;[Execution List(true, false, false) -0.007531404495239258]&#xA;[Execution List(true, false, true) -0.42748916149139404]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>windymelt/FNFIS</title>
    <updated>2022-06-03T01:52:00Z</updated>
    <id>tag:github.com,2022-06-03:/windymelt/FNFIS</id>
    <link href="https://github.com/windymelt/FNFIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;FNFIS - FreeNet File Indexing on Scala Testing.&lt;/p&gt;</summary>
  </entry>
</feed>