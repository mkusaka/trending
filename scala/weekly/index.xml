<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-28T01:50:27Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TheHive-Project/TheHive</title>
    <updated>2022-08-28T01:50:27Z</updated>
    <id>tag:github.com,2022-08-28:/TheHive-Project/TheHive</id>
    <link href="https://github.com/TheHive-Project/TheHive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TheHive: a Scalable, Open Source and Free Security Incident Response Platform&lt;/p&gt;&lt;hr&gt;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/thehive-logo.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://chat.thehive-project.org&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-7289da.svg?sanitize=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href&gt;&lt;img src=&#34;https://drone.strangebee.com/api/badges/TheHive-Project/TheHive/status.svg?ref=refs/heads/master-th4&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/LICENSE&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TheHive-Project/TheHive&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;TheHive&lt;/a&gt; is a scalable 3-in-1 open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. It is the perfect companion to &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt;. You can synchronize it with one or multiple MISP instances to start investigations out of MISP events. You can also export an investigation&#39;s results as a MISP event to help your peers detect and react to attacks you&#39;ve dealt with. Additionally, when TheHive is used in conjunction with &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt;, security analysts and researchers can easily analyze tens if not hundred of observables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Current_cases.png&#34; alt=&#34;Current Cases View&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Collaborate&lt;/h2&gt; &#xA;&lt;p&gt;Collaboration is at the heart of TheHive.&lt;/p&gt; &#xA;&lt;p&gt;Multiple analysts from one organisations can work together on the same case simultaneously. For example, an analyst may deal with malware analysis while another may work on tracking C2 beaconing activity on proxy logs as soon as IOCs have been added by their coworker. Using TheHive&#39;s live stream, everyone can keep an eye on what&#39;s happening on the platform, in real time.&lt;/p&gt; &#xA;&lt;p&gt;Multi-tenancy and fine grained user profiles let organisations and analysts work and collaborate on a same case accross organisations. For example, one case can be created by a first organisation who start investigating and ask for contribution from other teams or escalate to another organisation.&lt;/p&gt; &#xA;&lt;h2&gt;Elaborate&lt;/h2&gt; &#xA;&lt;p&gt;Within TheHive, every investigation corresponds to a case. Cases can be created from scratch or from &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; events, SIEM alerts, email reports and any other noteworthy source of security events.&lt;/p&gt; &#xA;&lt;p&gt;Each case can be broken down into one or more tasks. Instead of adding the same tasks to a given type of case every time one is created, analysts can use TheHive&#39;s template engine to create them once and for all. Case templates can also be used to associate metrics to specific case types in order to drive the team&#39;s activity, identify the type of investigations that take significant time and seek to automate tedious tasks.&lt;/p&gt; &#xA;&lt;p&gt;Each task can be assigned to a given analyst. Team members can also take charge of a task without waiting for someone to assign it to them.&lt;/p&gt; &#xA;&lt;p&gt;Tasks may contain multiple work logs that contributing analysts can use to describe what they are up to, what was the outcome, attach pieces of evidence or noteworthy files and so on. Logs can be written using a rich text editor or Markdown.&lt;/p&gt; &#xA;&lt;h2&gt;Analyze&lt;/h2&gt; &#xA;&lt;p&gt;You can add one or hundreds if not thousands of observables to each case you create. You can also create a case out of a &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; event. TheHive can be very easily linked to one or several MISP instances and MISP events can be previewed to decide whether they warrant an investigation or not. If an investigation is in order, the analyst can then add the event to an existing case or import it as a new case using a customizable template.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://thehive-project.org/#section_thehive4py&#34;&gt;TheHive4py&lt;/a&gt;, TheHive&#39;s Python API client, it is possible to send SIEM alerts, phishing and other suspicious emails and other security events to TheHive. They will appear in its &lt;code&gt;Alerts&lt;/code&gt; panel along with new or updated MISP events, where they can be previewed, imported into cases or ignored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Alerts_Panel.png&#34; alt=&#34;The Alerts Pane&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TheHive has the ability to automatically identify observables that have been already seen in previous cases. Observables can also be associated with a TLP and the source which provided or generated them using tags. The analyst can also easily mark observables as IOCs and isolate those using a search query then export them for searching in a SIEM or other data stores.&lt;/p&gt; &#xA;&lt;p&gt;Analysts can analyze tens or hundreds of observables in a few clicks by leveraging the analyzers of one or several &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; instances depending on your OPSEC needs: DomainTools, VirusTotal, PassiveTotal, Joe Sandbox, geolocation, threat feed lookups and so on.&lt;/p&gt; &#xA;&lt;p&gt;Security analysts with a knack for scripting can easily add their own analyzers to Cortex in order to automate actions that must be performed on observables or IOCs. They can also decide how analyzers behave according to the TLP. For example, a file added as observable can be submitted to VirusTotal if the associated TLP is WHITE or GREEN. If it&#39;s AMBER, its hash is computed and submitted to VT but not the file. If it&#39;s RED, no VT lookup is done.&lt;/p&gt; &#xA;&lt;h1&gt;Try it&lt;/h1&gt; &#xA;&lt;p&gt;To try TheHive, you can use the &lt;a href=&#34;https://www.strangebee.com/tryit&#34;&gt;training VM&lt;/a&gt; or install it by reading the &lt;a href=&#34;https://docs.thehive-project.org/thehive/installation-and-configuration/installation/step-by-step-guide/&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Details&lt;/h1&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;We have made several guides available in the &lt;a href=&#34;https://docs.thehive-project.org/thehive/&#34;&gt;Documentation repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;h3&gt;Multi-tenancy&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a special multi-tenancy support. It allows the following strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use a siloed multi-tenancy: many organisations can be defined without allowing them to share data;&lt;/li&gt; &#xA; &lt;li&gt;Use a collaborative multi-tenancy: a set of organisations can be allowed to collaborate on specific cases/tasks/observables, using custom defined user profiles (RBAC).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RBAC&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a set of permissions and several pre-configured user profiles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;admin&lt;/code&gt;: full administrative permissions on the platform ; can&#39;t manage any Cases or other data related to investigations;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;org-admin&lt;/code&gt;: manage users and all organisation-level configuration, can create and edit Cases, Tasks, Observables and run Analyzers and Responders;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;analyst&lt;/code&gt;: can create and edit &lt;em&gt;Cases&lt;/em&gt;, &lt;em&gt;Tasks&lt;/em&gt;, &lt;em&gt;Observables&lt;/em&gt; and run &lt;em&gt;Analyzers&lt;/em&gt; &amp;amp; &lt;em&gt;Responders&lt;/em&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read-only&lt;/code&gt;: Can only read, Cases, Tasks and Observables details;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;New profiles can be created by administrators of the platform.&lt;/p&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;TheHive 4 supports authentication methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;local accounts&lt;/li&gt; &#xA; &lt;li&gt;Active Directory&lt;/li&gt; &#xA; &lt;li&gt;LDAP&lt;/li&gt; &#xA; &lt;li&gt;Basic Auth&lt;/li&gt; &#xA; &lt;li&gt;API keys&lt;/li&gt; &#xA; &lt;li&gt;OAUTH2&lt;/li&gt; &#xA; &lt;li&gt;Multi Factor Authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Statistics &amp;amp; Dashboards&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a powerful statistics module that allows you to create meaningful dashboards to drive your activity and support your budget requests.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;MISP and Cortex&lt;/h3&gt; &#xA;&lt;p&gt;TheHive can be configured to import events from one or multiple &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; instances. You can also use TheHive to export cases as MISP events to one or several MISP servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; is the perfect companion for TheHive. Use one or several to analyze observables at scale.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Digital Shadows&lt;/h3&gt; &#xA;&lt;p&gt;TheHive Project provides &lt;a href=&#34;https://github.com/TheHive-Project/DigitalShadows2TH&#34;&gt;DigitalShadows2TH&lt;/a&gt;, a free, open source &lt;a href=&#34;https://www.digitalshadows.com/&#34;&gt;Digital Shadows&lt;/a&gt; alert feeder for TheHive. You can use it to import Digital Shadows &lt;em&gt;incidents&lt;/em&gt; and &lt;em&gt;intel-incidents&lt;/em&gt; as alerts in TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Zerofox&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Zerofox2TH&#34;&gt;Zerofox2TH&lt;/a&gt; is a free, open source &lt;a href=&#34;https://www.zerofox.com/&#34;&gt;ZeroFOX&lt;/a&gt; alert feeder for TheHive, written by TheHive Project. You can use it to feed ZeroFOX alerts into TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;And many more&lt;/h3&gt; &#xA;&lt;p&gt;Lots of &lt;strong&gt;awesome&lt;/strong&gt; integrations shared by the community could be listed there. If you&#39;re looking for a specific one, &lt;strong&gt;a dedicated repository&lt;/strong&gt; containing all known details and references about existing integrations is updated frequently, and can be found here: &lt;a href=&#34;https://github.com/TheHive-Project/awesome&#34;&gt;https://github.com/TheHive-Project/awesome&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;TheHive is an open source and free software released under the &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/raw/master/LICENSE&#34;&gt;AGPL&lt;/a&gt; (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.&lt;/p&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;p&gt;Information, news and updates are regularly posted on &lt;a href=&#34;https://twitter.com/thehive_project&#34;&gt;TheHive Project Twitter account&lt;/a&gt; and on &lt;a href=&#34;https://blog.thehive-project.org/&#34;&gt;the blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/code_of_conduct.md&#34;&gt;Code of conduct&lt;/a&gt;. We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests via &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;open an issue on GitHub&lt;/a&gt; if you&#39;d like to report a bug or request a feature. We are also available on &lt;a href=&#34;https://chat.thehive-project.org&#34;&gt;Discord&lt;/a&gt; to help you out.&lt;/p&gt; &#xA;&lt;p&gt;If you need to contact the project team, send an email to &lt;a href=&#34;mailto:support@thehive-project.org&#34;&gt;support@thehive-project.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have problems with &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py&#34;&gt;TheHive4py&lt;/a&gt;, please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py/issues/new&#34;&gt;open an issue on its dedicated repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you encounter an issue with Cortex or would like to request a Cortex-related feature, please &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/issues/new&#34;&gt;open an issue on its dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the &lt;a href=&#34;https://github.com/TheHive-Project/cortex-analyzers/issues/new&#34;&gt;analyzers&#39; dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community Discussions&lt;/h1&gt; &#xA;&lt;p&gt;We have set up a Google forum at &lt;a href=&#34;https://groups.google.com/a/thehive-project.org/d/forum/users&#34;&gt;https://groups.google.com/a/thehive-project.org/d/forum/users&lt;/a&gt;. To request access, you need a Google account. You may create one &lt;a href=&#34;https://accounts.google.com/SignUp?hl=en&#34;&gt;using a Gmail address&lt;/a&gt; or &lt;a href=&#34;https://accounts.google.com/SignUpWithoutGmail?hl=en&#34;&gt;without it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Website&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;https://thehive-project.org/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-redshift</title>
    <updated>2022-08-28T01:50:27Z</updated>
    <id>tag:github.com,2022-08-28:/databricks/spark-redshift</id>
    <link href="https://github.com/databricks/spark-redshift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Redshift data source for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Redshift Data Source for Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-redshift&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-redshift.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/databricks/spark-redshift?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/databricks/spark-redshift/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;p&gt;To ensure the best experience for our customers, we have decided to inline this connector directly in Databricks Runtime. The latest version of Databricks Runtime (3.0+) includes an advanced version of the RedShift connector for Spark that features both performance improvements (full query pushdown) as well as security improvements (automatic encryption). For more information, refer to the &lt;a href=&#34;https://docs.databricks.com/spark/latest/data-sources/aws/amazon-redshift.html&#34;&gt;Databricks documentation&lt;/a&gt;. As a result, we will no longer be making releases separately from Databricks Runtime.&lt;/p&gt; &#xA;&lt;h2&gt;Original Readme&lt;/h2&gt; &#xA;&lt;p&gt;A library to load data into Spark SQL DataFrames from Amazon Redshift, and write them back to Redshift tables. Amazon S3 is used to efficiently transfer data in and out of Redshift, and JDBC is used to automatically trigger the appropriate &lt;code&gt;COPY&lt;/code&gt; and &lt;code&gt;UNLOAD&lt;/code&gt; commands on Redshift.&lt;/p&gt; &#xA;&lt;p&gt;This library is more suited to ETL than interactive queries, since large amounts of data could be extracted to S3 for each query execution. If you plan to perform many queries against the same Redshift tables then we recommend saving the extracted data in a format such as Parquet.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#snapshot-builds&#34;&gt;Snapshot builds&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Usage: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data sources API: &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#scala&#34;&gt;Scala&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#python&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#sql&#34;&gt;SQL&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#r&#34;&gt;R&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#hadoop-inputformat&#34;&gt;Hadoop InputFormat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#configuration&#34;&gt;Configuration&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#authenticating-to-s3-and-redshift&#34;&gt;Authenticating to S3 and Redshift&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#encryption&#34;&gt;Encryption&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#additional-configuration-options&#34;&gt;Additional configuration options&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#configuring-the-maximum-size-of-string-columns&#34;&gt;Configuring the maximum size of string columns&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#setting-a-custom-column-type&#34;&gt;Setting a custom column type&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#configuring-column-encoding&#34;&gt;Configuring column encoding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#setting-descriptions-on-columns&#34;&gt;Setting descriptions on columns&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#transactional-guarantees&#34;&gt;Transactional Guarantees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#common-problems-and-solutions&#34;&gt;Common problems and solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#s3-bucket-and-redshift-cluster-are-in-different-aws-regions&#34;&gt;S3 bucket and Redshift cluster are in different AWS regions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#migration-guide&#34;&gt;Migration Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Apache Spark 2.0+ and Amazon Redshift 1.0.963+.&lt;/p&gt; &#xA;&lt;p&gt;For version that works with Spark 1.x, please check for the &lt;a href=&#34;https://github.com/databricks/spark-redshift/tree/branch-1.x&#34;&gt;1.x branch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may use this library in your applications with the following dependency information:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scala 2.10&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-redshift_2.10&#xA;version: 3.0.0-preview1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scala 2.11&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-redshift_2.11&#xA;version: 3.0.0-preview1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need to provide a JDBC driver that is compatible with Redshift. Amazon recommend that you use &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html&#34;&gt;their driver&lt;/a&gt;, which is distributed as a JAR that is hosted on Amazon&#39;s website. This library has also been successfully tested using the Postgres JDBC driver.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note on Hadoop versions&lt;/strong&gt;: This library depends on &lt;a href=&#34;https://github.com/databricks/spark-avro&#34;&gt;&lt;code&gt;spark-avro&lt;/code&gt;&lt;/a&gt;, which should automatically be downloaded because it is declared as a dependency. However, you may need to provide the corresponding &lt;code&gt;avro-mapred&lt;/code&gt; dependency which matches your Hadoop distribution. In most deployments, however, this dependency will be automatically provided by your cluster&#39;s Spark assemblies and no additional action will be required.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note on Amazon SDK dependency&lt;/strong&gt;: This library declares a &lt;code&gt;provided&lt;/code&gt; dependency on components of the AWS Java SDK. In most cases, these libraries will be provided by your deployment environment. However, if you get ClassNotFoundExceptions for Amazon SDK classes then you will need to add explicit dependencies on &lt;code&gt;com.amazonaws.aws-java-sdk-core&lt;/code&gt; and &lt;code&gt;com.amazonaws.aws-java-sdk-s3&lt;/code&gt; as part of your build / runtime configuration. See the comments in &lt;code&gt;project/SparkRedshiftBuild.scala&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Snapshot builds&lt;/h3&gt; &#xA;&lt;p&gt;Master snapshot builds of this library are built using &lt;a href=&#34;https://jitpack.io/&#34;&gt;jitpack.io&lt;/a&gt;. In order to use these snapshots in your build, you&#39;ll need to add the JitPack repository to your build file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;In Maven&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;repositories&amp;gt;&#xA;   &amp;lt;repository&amp;gt;&#xA;     &amp;lt;id&amp;gt;jitpack.io&amp;lt;/id&amp;gt;&#xA;     &amp;lt;url&amp;gt;https://jitpack.io&amp;lt;/url&amp;gt;&#xA;   &amp;lt;/repository&amp;gt;&#xA;&amp;lt;/repositories&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.github.databricks&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;spark-redshift_2.10&amp;lt;/artifactId&amp;gt;  &amp;lt;!-- For Scala 2.11, use spark-redshift_2.11 instead --&amp;gt;&#xA;  &amp;lt;version&amp;gt;master-SNAPSHOT&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;In SBT&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;resolvers += &#34;jitpack&#34; at &#34;https://jitpack.io&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then&lt;/p&gt; &lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.github.databricks&#34; %% &#34;spark-redshift&#34; % &#34;master-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In Databricks: use the &#34;Advanced Options&#34; toggle in the &#34;Create Library&#34; screen to specify a custom Maven repository:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/50748/20371277/6c34a8d2-ac18-11e6-879f-d07320d56fa4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;p&gt;Use &lt;code&gt;https://jitpack.io&lt;/code&gt; as the repository.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For Scala 2.10: use the coordinate &lt;code&gt;com.github.databricks:spark-redshift_2.10:master-SNAPSHOT&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For Scala 2.11: use the coordinate &lt;code&gt;com.github.databricks:spark-redshift_2.11:master-SNAPSHOT&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Data Sources API&lt;/h3&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#aws-credentials&#34;&gt;configured your AWS credentials&lt;/a&gt;, you can use this library via the Data Sources API in Scala, Python or SQL, as follows:&lt;/p&gt; &#xA;&lt;h4&gt;Scala&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val sc = // existing SparkContext&#xA;val sqlContext = new SQLContext(sc)&#xA;&#xA;// Get some data from a Redshift table&#xA;val df: DataFrame = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.redshift&#34;)&#xA;    .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;)&#xA;    .option(&#34;dbtable&#34;, &#34;my_table&#34;)&#xA;    .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;)&#xA;    .load()&#xA;&#xA;// Can also load data from a Redshift query&#xA;val df: DataFrame = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.redshift&#34;)&#xA;    .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;)&#xA;    .option(&#34;query&#34;, &#34;select x, count(*) my_table group by x&#34;)&#xA;    .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;)&#xA;    .load()&#xA;&#xA;// Apply some transformations to the data as per normal, then you can use the&#xA;// Data Source API to write the data back to another table&#xA;&#xA;df.write&#xA;  .format(&#34;com.databricks.spark.redshift&#34;)&#xA;  .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;)&#xA;  .option(&#34;dbtable&#34;, &#34;my_table_copy&#34;)&#xA;  .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;)&#xA;  .mode(&#34;error&#34;)&#xA;  .save()&#xA;&#xA;// Using IAM Role based authentication&#xA;df.write&#xA;  .format(&#34;com.databricks.spark.redshift&#34;)&#xA;  .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;)&#xA;  .option(&#34;dbtable&#34;, &#34;my_table_copy&#34;)&#xA;  .option(&#34;aws_iam_role&#34;, &#34;arn:aws:iam::123456789000:role/redshift_iam_role&#34;)&#xA;  .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;)&#xA;  .mode(&#34;error&#34;)&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Python&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;&#xA;sc = # existing SparkContext&#xA;sql_context = SQLContext(sc)&#xA;&#xA;# Read data from a table&#xA;df = sql_context.read \&#xA;    .format(&#34;com.databricks.spark.redshift&#34;) \&#xA;    .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;) \&#xA;    .option(&#34;dbtable&#34;, &#34;my_table&#34;) \&#xA;    .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;) \&#xA;    .load()&#xA;&#xA;# Read data from a query&#xA;df = sql_context.read \&#xA;    .format(&#34;com.databricks.spark.redshift&#34;) \&#xA;    .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;) \&#xA;    .option(&#34;query&#34;, &#34;select x, count(*) my_table group by x&#34;) \&#xA;    .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;) \&#xA;    .load()&#xA;&#xA;# Write back to a table&#xA;df.write \&#xA;  .format(&#34;com.databricks.spark.redshift&#34;) \&#xA;  .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;) \&#xA;  .option(&#34;dbtable&#34;, &#34;my_table_copy&#34;) \&#xA;  .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;) \&#xA;  .mode(&#34;error&#34;) \&#xA;  .save()&#xA;&#xA;# Using IAM Role based authentication&#xA;df.write \&#xA;  .format(&#34;com.databricks.spark.redshift&#34;) \&#xA;  .option(&#34;url&#34;, &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;) \&#xA;  .option(&#34;dbtable&#34;, &#34;my_table_copy&#34;) \&#xA;  .option(&#34;tempdir&#34;, &#34;s3n://path/for/temp/data&#34;) \&#xA;  .option(&#34;aws_iam_role&#34;, &#34;arn:aws:iam::123456789000:role/redshift_iam_role&#34;) \&#xA;  .mode(&#34;error&#34;) \&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SQL&lt;/h4&gt; &#xA;&lt;p&gt;Reading data using SQL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE my_table&#xA;USING com.databricks.spark.redshift&#xA;OPTIONS (&#xA;  dbtable &#39;my_table&#39;,&#xA;  tempdir &#39;s3n://path/for/temp/data&#39;,&#xA;  url &#39;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#39;&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Writing data using SQL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- Create a new table, throwing an error if a table with the same name already exists:&#xA;CREATE TABLE my_table&#xA;USING com.databricks.spark.redshift&#xA;OPTIONS (&#xA;  dbtable &#39;my_table&#39;,&#xA;  tempdir &#39;s3n://path/for/temp/data&#39;&#xA;  url &#39;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#39;&#xA;)&#xA;AS SELECT * FROM tabletosave;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the SQL API only supports the creation of new tables and not overwriting or appending; this corresponds to the default save mode of the other language APIs.&lt;/p&gt; &#xA;&lt;h4&gt;R&lt;/h4&gt; &#xA;&lt;p&gt;Reading data using R:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df &amp;lt;- read.df(&#xA;   NULL,&#xA;   &#34;com.databricks.spark.redshift&#34;,&#xA;   tempdir = &#34;s3n://path/for/temp/data&#34;,&#xA;   dbtable = &#34;my_table&#34;,&#xA;   url = &#34;jdbc:redshift://redshifthost:5439/database?user=username&amp;amp;password=pass&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hadoop InputFormat&lt;/h3&gt; &#xA;&lt;p&gt;The library contains a Hadoop input format for Redshift tables unloaded with the ESCAPE option, which you may make direct use of as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.redshift.RedshiftInputFormat&#xA;&#xA;val records = sc.newAPIHadoopFile(&#xA;  path,&#xA;  classOf[RedshiftInputFormat],&#xA;  classOf[java.lang.Long],&#xA;  classOf[Array[String]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Authenticating to S3 and Redshift&lt;/h3&gt; &#xA;&lt;p&gt;The use of this library involves several connections which must be authenticated / secured, all of which are illustrated in the following diagram:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;                            ┌───────┐&#xA;       ┌───────────────────▶│  S3   │◀─────────────────┐&#xA;       │    IAM or keys     └───────┘    IAM or keys   │&#xA;       │                        ▲                      │&#xA;       │                        │ IAM or keys          │&#xA;       ▼                        ▼               ┌──────▼────┐&#xA;┌────────────┐            ┌───────────┐         │┌──────────┴┐&#xA;│  Redshift  │            │   Spark   │         ││   Spark   │&#xA;│            │◀──────────▶│  Driver   │◀────────▶┤ Executors │&#xA;└────────────┘            └───────────┘          └───────────┘&#xA;               JDBC with                  Configured&#xA;               username /                     in&#xA;                password                    Spark&#xA;            (can enable SSL)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This library reads and writes data to S3 when transferring data to/from Redshift. As a result, it requires AWS credentials with read and write access to a S3 bucket (specified using the &lt;code&gt;tempdir&lt;/code&gt; configuration parameter).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;span&gt;⚠&lt;/span&gt; Note&lt;/strong&gt;: This library does not clean up the temporary files that it creates in S3. As a result, we recommend that you use a dedicated temporary S3 bucket with an &lt;a href=&#34;http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html&#34;&gt;object lifecycle configuration&lt;/a&gt; to ensure that temporary files are automatically deleted after a specified expiration period. See the &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#encryption&#34;&gt;&lt;em&gt;Encryption&lt;/em&gt;&lt;/a&gt; section of this document for a discussion of how these files may be encrypted.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The following describes how each connection can be authenticated:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Spark driver to Redshift&lt;/strong&gt;: The Spark driver connects to Redshift via JDBC using a username and password. Redshift does not support the use of IAM roles to authenticate this connection. This connection can be secured using SSL; for more details, see the Encryption section below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Spark to S3&lt;/strong&gt;: S3 acts as a middleman to store bulk data when reading from or writing to Redshift. Spark connects to S3 using both the Hadoop FileSystem interfaces and directly using the Amazon Java SDK&#39;s S3 client.&lt;/p&gt; &lt;p&gt;This connection can be authenticated using either AWS keys or IAM roles (DBFS mountpoints are not currently supported, so Databricks users who do not want to rely on AWS keys should use cluster IAM roles instead).&lt;/p&gt; &lt;p&gt;There are multiple ways of providing these credentials:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Default Credential Provider Chain (best option for most users):&lt;/strong&gt; AWS credentials will automatically be retrieved through the &lt;a href=&#34;http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#id6&#34;&gt;DefaultAWSCredentialsProviderChain&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you use IAM instance roles to authenticate to S3 (e.g. on Databricks, EMR, or EC2), then you should probably use this method.&lt;/p&gt; &lt;p&gt;If another method of providing credentials is used (methods 2 or 3), then that will take precedence over this default.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set keys in Hadoop conf:&lt;/strong&gt; You can specify AWS keys via &lt;a href=&#34;https://github.com/apache/hadoop/raw/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md&#34;&gt;Hadoop configuration properties&lt;/a&gt;. For example, if your &lt;code&gt;tempdir&lt;/code&gt; configuration points to a &lt;code&gt;s3n://&lt;/code&gt; filesystem then you can set the &lt;code&gt;fs.s3n.awsAccessKeyId&lt;/code&gt; and &lt;code&gt;fs.s3n.awsSecretAccessKey&lt;/code&gt; properties in a Hadoop XML configuration file or call &lt;code&gt;sc.hadoopConfiguration.set()&lt;/code&gt; to mutate Spark&#39;s global Hadoop configuration.&lt;/p&gt; &lt;p&gt;For example, if you are using the &lt;code&gt;s3n&lt;/code&gt; filesystem then add&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sc.hadoopConfiguration.set(&#34;fs.s3n.awsAccessKeyId&#34;, &#34;YOUR_KEY_ID&#34;)&#xA;sc.hadoopConfiguration.set(&#34;fs.s3n.awsSecretAccessKey&#34;, &#34;YOUR_SECRET_ACCESS_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and for the &lt;code&gt;s3a&lt;/code&gt; filesystem add&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sc.hadoopConfiguration.set(&#34;fs.s3a.access.key&#34;, &#34;YOUR_KEY_ID&#34;)&#xA;sc.hadoopConfiguration.set(&#34;fs.s3a.secret.key&#34;, &#34;YOUR_SECRET_ACCESS_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Python users will have to use a slightly different method to modify the &lt;code&gt;hadoopConfiguration&lt;/code&gt;, since this field is not exposed in all versions of PySpark. Although the following command relies on some Spark internals, it should work with all PySpark versions and is unlikely to break or change in the future:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sc._jsc.hadoopConfiguration().set(&#34;fs.s3n.awsAccessKeyId&#34;, &#34;YOUR_KEY_ID&#34;)&#xA;sc._jsc.hadoopConfiguration().set(&#34;fs.s3n.awsSecretAccessKey&#34;, &#34;YOUR_SECRET_ACCESS_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Encode keys in &lt;code&gt;tempdir&lt;/code&gt; URI&lt;/strong&gt;: For example, the URI &lt;code&gt;s3n://ACCESSKEY:SECRETKEY@bucket/path/to/temp/dir&lt;/code&gt; encodes the key pair (&lt;code&gt;ACCESSKEY&lt;/code&gt;, &lt;code&gt;SECRETKEY&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;Due to [Hadoop limitations](https://issues.apache.org/jira/browse/HADOOP-3733), this&#xA;approach will not work for secret keys which contain forward slash (`/`) characters, even if&#xA;those characters are urlencoded.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Redshift to S3&lt;/strong&gt;: Redshift also connects to S3 during &lt;code&gt;COPY&lt;/code&gt; and &lt;code&gt;UNLOAD&lt;/code&gt; queries. There are three methods of authenticating this connection:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Have Redshift assume an IAM role (most secure)&lt;/strong&gt;: You can grant Redshift permission to assume an IAM role during &lt;code&gt;COPY&lt;/code&gt; or &lt;code&gt;UNLOAD&lt;/code&gt; operations and then configure this library to instruct Redshift to use that role:&lt;/p&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Create an IAM role granting appropriate S3 permissions to your bucket.&lt;/li&gt; &#xA;     &lt;li&gt;Follow the guide &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html&#34;&gt;&lt;em&gt;Authorizing Amazon Redshift to Access Other AWS Services On Your Behalf&lt;/em&gt;&lt;/a&gt; to configure this role&#39;s trust policy in order to allow Redshift to assume this role.&lt;/li&gt; &#xA;     &lt;li&gt;Follow the steps in the &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html&#34;&gt;&lt;em&gt;Authorizing COPY and UNLOAD Operations Using IAM Roles&lt;/em&gt;&lt;/a&gt; guide to associate that IAM role with your Redshift cluster.&lt;/li&gt; &#xA;     &lt;li&gt;Set this library&#39;s &lt;code&gt;aws_iam_role&lt;/code&gt; option to the role&#39;s ARN.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Forward Spark&#39;s S3 credentials to Redshift&lt;/strong&gt;: if the &lt;code&gt;forward_spark_s3_credentials&lt;/code&gt; option is set to &lt;code&gt;true&lt;/code&gt; then this library will automatically discover the credentials that Spark is using to connect to S3 and will forward those credentials to Redshift over JDBC. If Spark is authenticating to S3 using an IAM instance role then a set of temporary STS credentials will be passed to Redshift; otherwise, AWS keys will be passed. These credentials are sent as part of the JDBC query, so therefore it is &lt;strong&gt;strongly recommended&lt;/strong&gt; to enable SSL encryption of the JDBC connection when using this authentication method.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use Security Token Service (STS) credentials&lt;/strong&gt;: You may configure the &lt;code&gt;temporary_aws_access_key_id&lt;/code&gt;, &lt;code&gt;temporary_aws_secret_access_key&lt;/code&gt;, and &lt;code&gt;temporary_aws_session_token&lt;/code&gt; configuration properties to point to temporary keys created via the AWS &lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html&#34;&gt;Security Token Service&lt;/a&gt;. These credentials are sent as part of the JDBC query, so therefore it is &lt;strong&gt;strongly recommended&lt;/strong&gt; to enable SSL encryption of the JDBC connection when using this authentication method. If you choose this option then please be aware of the risk that the credentials expire before the read / write operation succeeds.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;These three options are mutually-exclusive and you must explicitly choose which one to use.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Encryption&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Securing JDBC&lt;/strong&gt;: The Redshift and Postgres JDBC drivers both support SSL. To enable SSL support, first configure Java to add the required certificates by following the &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/connecting-ssl-support.html#connecting-ssl-support-java&#34;&gt;&lt;em&gt;Using SSL and Server Certificates in Java&lt;/em&gt;&lt;/a&gt; instructions in the Redshift documentation. Then, follow the instructions in &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-options.html&#34;&gt;&lt;em&gt;JDBC Driver Configuration Options&lt;/em&gt;&lt;/a&gt; to add the appropriate SSL options to the JDBC &lt;code&gt;url&lt;/code&gt; used with this library.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Encrypting &lt;code&gt;UNLOAD&lt;/code&gt; data stored in S3 (data stored when reading from Redshift)&lt;/strong&gt;: According to the Redshift documentation on &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/t_Unloading_tables.html&#34;&gt;&lt;em&gt;Unloading Data to S3&lt;/em&gt;&lt;/a&gt;, &#34;UNLOAD automatically encrypts data files using Amazon S3 server-side encryption (SSE-S3).&#34;&lt;/p&gt; &lt;p&gt;Redshift also supports client-side encryption with a custom key (see: &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/t_unloading_encrypted_files.html&#34;&gt;&lt;em&gt;Unloading Encrypted Data Files&lt;/em&gt;&lt;/a&gt;) but this library currently lacks the capability to specify the required symmetric key.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Encrypting &lt;code&gt;COPY&lt;/code&gt; data stored in S3 (data stored when writing to Redshift)&lt;/strong&gt;: According to the Redshift documentation on &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html&#34;&gt;&lt;em&gt;Loading Encrypted Data Files from Amazon S3&lt;/em&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;You can use the COPY command to load data files that were uploaded to Amazon S3 using server-side encryption with AWS-managed encryption keys (SSE-S3 or SSE-KMS), client-side encryption, or both. COPY does not support Amazon S3 server-side encryption with a customer-supplied key (SSE-C)&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;To use this capability, you should configure your Hadoop S3 FileSystem to use encryption by setting the appropriate configuration properties (which will vary depending on whether you are using &lt;code&gt;s3a&lt;/code&gt;, &lt;code&gt;s3n&lt;/code&gt;, EMRFS, etc.). Note that the &lt;code&gt;MANIFEST&lt;/code&gt; file (a list of all files written) will not be encrypted.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parameters&lt;/h3&gt; &#xA;&lt;p&gt;The parameter map or &lt;tt&gt;OPTIONS&lt;/tt&gt; provided in Spark SQL supports the following settings.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;dbtable&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes, unless &lt;tt&gt;query&lt;/tt&gt; is specified&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;The table to create or read from in Redshift. This parameter is required when saving data back to Redshift.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;query&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes, unless &lt;tt&gt;dbtable&lt;/tt&gt; is specified&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;The query to read from in Redshift&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;user&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;The Redshift username. Must be used in tandem with &lt;tt&gt;password&lt;/tt&gt; option. May only be used if the user and password are not passed in the URL, passing both will result in an error.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;password&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;The Redshift password. Must be used in tandem with &lt;tt&gt;user&lt;/tt&gt; option. May only be used if the user and password are not passed in the URL; passing both will result in an error.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;url&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;A JDBC URL, of the format, &lt;tt&gt;jdbc:subprotocol://host:port/database?user=username&amp;amp;password=password&lt;/tt&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;tt&gt;subprotocol&lt;/tt&gt; can be &lt;tt&gt;postgresql&lt;/tt&gt; or &lt;tt&gt;redshift&lt;/tt&gt;, depending on which JDBC driver you have loaded. Note however that one Redshift-compatible driver must be on the classpath and match this URL.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;tt&gt;host&lt;/tt&gt; and &lt;tt&gt;port&lt;/tt&gt; should point to the Redshift master node, so security groups and/or VPC will need to be configured to allow access from your driver application. &lt;/li&gt;&#xA;     &lt;li&gt;&lt;tt&gt;database&lt;/tt&gt; identifies a Redshift database name&lt;/li&gt; &#xA;     &lt;li&gt;&lt;tt&gt;user&lt;/tt&gt; and &lt;tt&gt;password&lt;/tt&gt; are credentials to access the database, which must be embedded in this URL for JDBC, and your user account should have necessary privileges for the table being referenced. &lt;/li&gt; &#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;aws_iam_role&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Only if using IAM roles to authorize Redshift COPY/UNLOAD operations&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;Fully specified ARN of the &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html&#34;&gt;IAM Role&lt;/a&gt; attached to the Redshift cluster, ex: arn:aws:iam::123456789000:role/redshift_iam_role&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;forward_spark_s3_credentials&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt; If &lt;tt&gt;true&lt;/tt&gt; then this library will automatically discover the credentials that Spark is using to connect to S3 and will forward those credentials to Redshift over JDBC. These credentials are sent as part of the JDBC query, so therefore it is strongly recommended to enable SSL encryption of the JDBC connection when using this option. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;temporary_aws_access_key_id&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;AWS access key, must have write permissions to the S3 bucket.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;temporary_aws_secret_access_key&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;AWS secret access key corresponding to provided access key.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;temporary_aws_session_token&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;AWS session token corresponding to provided access key.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;tempdir&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;A writeable location in Amazon S3, to be used for unloaded data when reading and Avro data to be loaded into Redshift when writing. If you&#39;re using Redshift data source for Spark as part of a regular ETL pipeline, it can be useful to set a &lt;a href=&#34;http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html&#34;&gt;Lifecycle Policy&lt;/a&gt; on a bucket and use that as a temp location for this data. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;jdbcdriver&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Determined by the JDBC URL&#39;s subprotocol&lt;/td&gt; &#xA;   &lt;td&gt;The class name of the JDBC driver to use. This class must be on the classpath. In most cases, it should not be necessary to specify this option, as the appropriate driver classname should automatically be determined by the JDBC URL&#39;s subprotocol.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;diststyle&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;tt&gt;EVEN&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The Redshift &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html&#34;&gt;Distribution Style&lt;/a&gt; to be used when creating a table. Can be one of &lt;tt&gt;EVEN&lt;/tt&gt;, &lt;tt&gt;KEY&lt;/tt&gt; or &lt;tt&gt;ALL&lt;/tt&gt; (see Redshift docs). When using &lt;tt&gt;KEY&lt;/tt&gt;, you must also set a distribution key with the &lt;tt&gt;distkey&lt;/tt&gt; option. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;distkey&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No, unless using &lt;tt&gt;DISTSTYLE KEY&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt;The name of a column in the table to use as the distribution key when creating a table.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;sortkeyspec&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;A full Redshift &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html&#34;&gt;Sort Key&lt;/a&gt; definition.&lt;/p&gt; &lt;p&gt;Examples include:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;tt&gt;SORTKEY(my_sort_column)&lt;/tt&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;tt&gt;COMPOUND SORTKEY(sort_col_1, sort_col_2)&lt;/tt&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;tt&gt;INTERLEAVED SORTKEY(sort_col_1, sort_col_2)&lt;/tt&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;del&gt;&lt;tt&gt;usestagingtable&lt;/tt&gt;&lt;/del&gt; (Deprecated)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;tt&gt;true&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt; Setting this deprecated option to &lt;tt&gt;false&lt;/tt&gt; will cause an overwrite operation&#39;s destination table to be dropped immediately at the beginning of the write, making the overwrite operation non-atomic and reducing the availability of the destination table. This may reduce the temporary disk space requirements for overwrites. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;p&amp;gt;Since setting &amp;lt;tt&amp;gt;usestagingtable=false&amp;lt;/tt&amp;gt; operation risks data loss / unavailability, we have chosen to deprecate it in favor of requiring users to manually drop the destination table themselves.&amp;lt;/p&amp;gt;&#xA;&amp;lt;/td&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;description&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;A description for the table. Will be set using the SQL COMMENT command, and should show up in most query tools. See also the &lt;tt&gt;description&lt;/tt&gt; metadata to set descriptions on individual columns. &lt;/p&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;preactions&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;This can be a &lt;tt&gt;;&lt;/tt&gt; separated list of SQL commands to be executed before loading &lt;tt&gt;COPY&lt;/tt&gt; command. It may be useful to have some &lt;tt&gt;DELETE&lt;/tt&gt; commands or similar run here before loading new data. If the command contains &lt;tt&gt;%s&lt;/tt&gt;, the table name will be formatted in before execution (in case you&#39;re using a staging table).&lt;/p&gt; &lt;p&gt;Be warned that if this commands fail, it is treated as an error and you&#39;ll get an exception. If using a staging table, the changes will be reverted and the backup table restored if pre actions fail.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;postactions&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;This can be a &lt;tt&gt;;&lt;/tt&gt; separated list of SQL commands to be executed after a successful &lt;tt&gt;COPY&lt;/tt&gt; when loading data. It may be useful to have some &lt;tt&gt;GRANT&lt;/tt&gt; commands or similar run here when loading new data. If the command contains &lt;tt&gt;%s&lt;/tt&gt;, the table name will be formatted in before execution (in case you&#39;re using a staging table).&lt;/p&gt; &lt;p&gt;Be warned that if this commands fail, it is treated as an error and you&#39;ll get an exception. If using a staging table, the changes will be reverted and the backup table restored if post actions fail.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;extracopyoptions&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No default&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;A list extra options to append to the Redshift &lt;tt&gt;COPY&lt;/tt&gt; command when loading data, e.g. &lt;tt&gt;TRUNCATECOLUMNS&lt;/tt&gt; or &lt;tt&gt;MAXERROR n&lt;/tt&gt; (see the &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax-overview-optional-parameters&#34;&gt;Redshift docs&lt;/a&gt; for other options).&lt;/p&gt; &lt;p&gt;Note that since these options are appended to the end of the &lt;tt&gt;COPY&lt;/tt&gt; command, only options that make sense at the end of the command can be used, but that should cover most possible use cases.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;tempformat&lt;/tt&gt; (Experimental)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;tt&gt;AVRO&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt; The format in which to save temporary files in S3 when writing to Redshift. Defaults to &#34;AVRO&#34;; the other allowed values are &#34;CSV&#34; and &#34;CSV GZIP&#34; for CSV and gzipped CSV, respectively. &lt;/p&gt; &lt;p&gt; Redshift is significantly faster when loading CSV than when loading Avro files, so using that &lt;tt&gt;tempformat&lt;/tt&gt; may provide a large performance boost when writing to Redshift. &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;tt&gt;csvnullstring&lt;/tt&gt; (Experimental)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;tt&gt;@NULL@&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt; The String value to write for nulls when using the CSV &lt;tt&gt;tempformat&lt;/tt&gt;. This should be a value which does not appear in your actual data. &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Additional configuration options&lt;/h2&gt; &#xA;&lt;h3&gt;Configuring the maximum size of string columns&lt;/h3&gt; &#xA;&lt;p&gt;When creating Redshift tables, this library&#39;s default behavior is to create &lt;code&gt;TEXT&lt;/code&gt; columns for string columns. Redshift stores &lt;code&gt;TEXT&lt;/code&gt; columns as &lt;code&gt;VARCHAR(256)&lt;/code&gt;, so these columns have a maximum size of 256 characters (&lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/r_Character_types.html&#34;&gt;source&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;To support larger columns, you can use the &lt;code&gt;maxlength&lt;/code&gt; column metadata field to specify the maximum length of individual string columns. This can also be done as a space-savings performance optimization in order to declare columns with a smaller maximum length than the default.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;span&gt;⚠&lt;/span&gt; Note&lt;/strong&gt;: Due to limitations in Spark, metadata modification is unsupported in the Python, SQL, and R language APIs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Here is an example of updating multiple columns&#39; metadata fields using Spark&#39;s Scala API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.types.MetadataBuilder&#xA;&#xA;// Specify the custom width of each column&#xA;val columnLengthMap = Map(&#xA;  &#34;language_code&#34; -&amp;gt; 2,&#xA;  &#34;country_code&#34; -&amp;gt; 2,&#xA;  &#34;url&#34; -&amp;gt; 2083&#xA;)&#xA;&#xA;var df = ... // the dataframe you&#39;ll want to write to Redshift&#xA;&#xA;// Apply each column metadata customization&#xA;columnLengthMap.foreach { case (colName, length) =&amp;gt;&#xA;  val metadata = new MetadataBuilder().putLong(&#34;maxlength&#34;, length).build()&#xA;  df = df.withColumn(colName, df(colName).as(colName, metadata))&#xA;}&#xA;&#xA;df.write&#xA;  .format(&#34;com.databricks.spark.redshift&#34;)&#xA;  .option(&#34;url&#34;, jdbcURL)&#xA;  .option(&#34;tempdir&#34;, s3TempDirectory)&#xA;  .option(&#34;dbtable&#34;, sessionTable)&#xA;  .save()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setting a custom column type&lt;/h3&gt; &#xA;&lt;p&gt;If you need to manually set a column type, you can use the &lt;code&gt;redshift_type&lt;/code&gt; column metadata. For example, if you desire to override the &lt;code&gt;Spark SQL Schema -&amp;gt; Redshift SQL&lt;/code&gt; type matcher to assign a user-defined column type, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.types.MetadataBuilder&#xA;&#xA;// Specify the custom width of each column&#xA;val columnTypeMap = Map(&#xA;  &#34;language_code&#34; -&amp;gt; &#34;CHAR(2)&#34;,&#xA;  &#34;country_code&#34; -&amp;gt; &#34;CHAR(2)&#34;,&#xA;  &#34;url&#34; -&amp;gt; &#34;BPCHAR(111)&#34;&#xA;)&#xA;&#xA;var df = ... // the dataframe you&#39;ll want to write to Redshift&#xA;&#xA;// Apply each column metadata customization&#xA;columnTypeMap.foreach { case (colName, colType) =&amp;gt;&#xA;  val metadata = new MetadataBuilder().putString(&#34;redshift_type&#34;, colType).build()&#xA;  df = df.withColumn(colName, df(colName).as(colName, metadata))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring column encoding&lt;/h3&gt; &#xA;&lt;p&gt;When creating a table, this library can be configured to use a specific compression encoding on individual columns. You can use the &lt;code&gt;encoding&lt;/code&gt; column metadata field to specify a compression encoding for each column (see &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html&#34;&gt;Amazon docs&lt;/a&gt; for available encodings).&lt;/p&gt; &#xA;&lt;h3&gt;Setting descriptions on columns&lt;/h3&gt; &#xA;&lt;p&gt;Redshift allows columns to have descriptions attached that should show up in most query tools (using the &lt;code&gt;COMMENT&lt;/code&gt; command). You can set the &lt;code&gt;description&lt;/code&gt; column metadata field to specify a description for individual columns.&lt;/p&gt; &#xA;&lt;h2&gt;Transactional Guarantees&lt;/h2&gt; &#xA;&lt;p&gt;This section describes the transactional guarantees of the Redshift data source for Spark&lt;/p&gt; &#xA;&lt;h3&gt;General background on Redshift and S3&#39;s properties&lt;/h3&gt; &#xA;&lt;p&gt;For general information on Redshift&#39;s transactional guarantees, see the &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/c_Concurrent_writes.html&#34;&gt;Managing Concurrent Write Operations&lt;/a&gt; chapter in the Redshift documentation. In a nutshell, Redshift provides &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/c_serial_isolation.html&#34;&gt;serializable isolation&lt;/a&gt; (according to the documentation for Redshift&#39;s &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/r_BEGIN.html&#34;&gt;&lt;code&gt;BEGIN&lt;/code&gt;&lt;/a&gt; command, &#34;[although] you can use any of the four transaction isolation levels, Amazon Redshift processes all isolation levels as serializable&#34;). According to its &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/c_serial_isolation.html&#34;&gt;documentation&lt;/a&gt;, &#34;Amazon Redshift supports a default &lt;em&gt;automatic commit&lt;/em&gt; behavior in which each separately-executed SQL command commits individually.&#34; Thus, individual commands like &lt;code&gt;COPY&lt;/code&gt; and &lt;code&gt;UNLOAD&lt;/code&gt; are atomic and transactional, while explicit &lt;code&gt;BEGIN&lt;/code&gt; and &lt;code&gt;END&lt;/code&gt; should only be necessary to enforce the atomicity of multiple commands / queries.&lt;/p&gt; &#xA;&lt;p&gt;When reading from / writing to Redshift, this library reads and writes data in S3. Both Spark and Redshift produce partitioned output which is stored in multiple files in S3. According to the &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel&#34;&gt;Amazon S3 Data Consistency Model&lt;/a&gt; documentation, S3 bucket listing operations are eventually-consistent, so the files must to go to special lengths to avoid missing / incomplete data due to this source of eventual-consistency.&lt;/p&gt; &#xA;&lt;h3&gt;Guarantees of the Redshift data source for Spark&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Appending to an existing table&lt;/strong&gt;: In the &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html&#34;&gt;&lt;code&gt;COPY&lt;/code&gt;&lt;/a&gt; command, this library uses &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html&#34;&gt;manifests&lt;/a&gt; to guard against certain eventually-consistent S3 operations. As a result, it appends to existing tables have the same atomic and transactional properties as regular Redshift &lt;code&gt;COPY&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Appending to an existing table&lt;/strong&gt;: When inserting rows into Redshift, this library uses the &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html&#34;&gt;&lt;code&gt;COPY&lt;/code&gt;&lt;/a&gt; command and specifies &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html&#34;&gt;manifests&lt;/a&gt; to guard against certain eventually-consistent S3 operations. As a result, &lt;code&gt;spark-redshift&lt;/code&gt; appends to existing tables have the same atomic and transactional properties as regular Redshift &lt;code&gt;COPY&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Creating a new table (&lt;code&gt;SaveMode.CreateIfNotExists&lt;/code&gt;)&lt;/strong&gt;: Creating a new table is a two-step process, consisting of a &lt;code&gt;CREATE TABLE&lt;/code&gt; command followed by a &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html&#34;&gt;&lt;code&gt;COPY&lt;/code&gt;&lt;/a&gt; command to append the initial set of rows. Both of these operations are performed in a single transaction.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overwriting an existing table&lt;/strong&gt;: By default, this library uses transactions to perform overwrites, which are implemented by deleting the destination table, creating a new empty table, and appending rows to it.&lt;/p&gt; &#xA;&lt;p&gt;If the deprecated &lt;code&gt;usestagingtable&lt;/code&gt; setting is set to &lt;code&gt;false&lt;/code&gt; then this library will commit the &lt;code&gt;DELETE TABLE&lt;/code&gt; command before appending rows to the new table, sacrificing the atomicity of the overwrite operation but reducing the amount of staging space that Redshift needs during the overwrite.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Querying Redshift tables&lt;/strong&gt;: Queries use Redshift&#39;s &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html&#34;&gt;&lt;code&gt;UNLOAD&lt;/code&gt;&lt;/a&gt; command to execute a query and save its results to S3 and use &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html&#34;&gt;manifests&lt;/a&gt; to guard against certain eventually-consistent S3 operations. As a result, queries from Redshift data source for Spark should have the same consistency properties as regular Redshift queries.&lt;/p&gt; &#xA;&lt;h2&gt;Common problems and solutions&lt;/h2&gt; &#xA;&lt;h3&gt;S3 bucket and Redshift cluster are in different AWS regions&lt;/h3&gt; &#xA;&lt;p&gt;By default, S3 &amp;lt;-&amp;gt; Redshift copies will not work if the S3 bucket and Redshift cluster are in different AWS regions.&lt;/p&gt; &#xA;&lt;p&gt;If you attempt to perform a read of a Redshift table and the regions are mismatched then you may see a confusing error, such as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;java.sql.SQLException: [Amazon](500310) Invalid operation: S3ServiceException:The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, attempting to write to Redshift using a S3 bucket in a different region may cause the following error:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;error:  Problem reading manifest file - S3ServiceException:The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.,Status 301,Error PermanentRedirect&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For writes:&lt;/strong&gt; Redshift&#39;s &lt;code&gt;COPY&lt;/code&gt; command allows the S3 bucket&#39;s region to be explicitly specified, so you can make writes to Redshift work properly in these cases by adding&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;region &#39;the-region-name&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to the &lt;code&gt;extracopyoptions&lt;/code&gt; setting. For example, with a bucket in the US East (Virginia) region and the Scala API, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.option(&#34;extracopyoptions&#34;, &#34;region &#39;us-east-1&#39;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For reads:&lt;/strong&gt; According to &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html&#34;&gt;its documentation&lt;/a&gt;, the Redshift &lt;code&gt;UNLOAD&lt;/code&gt; command does not support writing to a bucket in a different region:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;The Amazon S3 bucket where Amazon Redshift will write the output files must reside in the same region as your cluster.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;As a result, this use-case is not supported by this library. The only workaround is to use a new bucket in the same region as your Redshift cluster.&lt;/p&gt; &#xA;&lt;h2&gt;Migration Guide&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Version 3.0 now requires &lt;code&gt;forward_spark_s3_credentials&lt;/code&gt; to be explicitly set before Spark S3 credentials will be forwarded to Redshift. Users who use the &lt;code&gt;aws_iam_role&lt;/code&gt; or &lt;code&gt;temporary_aws_*&lt;/code&gt; authentication mechanisms will be unaffected by this change. Users who relied on the old default behavior will now need to explicitly set &lt;code&gt;forward_spark_s3_credentials&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; to continue using their previous Redshift to S3 authentication mechanism. For a discussion of the three authentication mechanisms and their security trade-offs, see the &lt;a href=&#34;https://raw.githubusercontent.com/databricks/spark-redshift/master/#authenticating-to-s3-and-redshift&#34;&gt;&lt;em&gt;Authenticating to S3 and Redshift&lt;/em&gt;&lt;/a&gt; section of this README.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/firrtl</title>
    <updated>2022-08-28T01:50:27Z</updated>
    <id>tag:github.com,2022-08-28:/chipsalliance/firrtl</id>
    <link href="https://github.com/chipsalliance/firrtl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Flexible Intermediate Representation for RTL&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/freechipsproject/firrtl/master/doc/images/firrtl_logo.svg?sanitize=true&#34; alt=&#34;FIRRTL&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/firrtl?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/freechipsproject/firrtl.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/freechipsproject/firrtl&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/chipsalliance/firrtl/workflows/Continuous%20Integration/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;a href=&#34;https://mergify.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://gh.mergify.io/badges/chipsalliance/firrtl&amp;amp;style=flat&#34; alt=&#34;Mergify Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Flexible Internal Representation for RTL&lt;/h4&gt; &#xA;&lt;p&gt;Firrtl is an intermediate representation (IR) for digital circuits designed as a platform for writing circuit-level transformations. This repository consists of a collection of transformations (written in Scala) which simplify, verify, transform, or emit their input circuit.&lt;/p&gt; &#xA;&lt;p&gt;A Firrtl compiler is constructed by chaining together these transformations, then writing the final circuit to a file.&lt;/p&gt; &#xA;&lt;p&gt;For a detailed description of Firrtl&#39;s intermediate representation, see the &lt;a href=&#34;https://github.com/chipsalliance/firrtl-spec/releases/latest/download/spec.pdf&#34;&gt;FIRRTL Language Specification&lt;/a&gt; (&lt;a href=&#34;https://github.com/chipsalliance/firrtl-spec&#34;&gt;source&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Wiki Pages and Tutorials&lt;/h4&gt; &#xA;&lt;p&gt;Useful information is on our wiki, located here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/firrtl/wiki&#34;&gt;https://github.com/freechipsproject/firrtl/wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some important pages to read, before writing your own transform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/firrtl/wiki/Submitting-a-Pull-Request&#34;&gt;Submitting Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/firrtl/wiki/Understanding-Firrtl-Intermediate-Representation&#34;&gt;Understanding Firrtl&#39;s IR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/firrtl/wiki/traversing-a-circuit&#34;&gt;Traversing a Circuit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/firrtl/wiki/Common-Pass-Idioms&#34;&gt;Common Pass Idioms&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To write a Firrtl transform, please start with the tutorial here: &lt;a href=&#34;https://github.com/freechipsproject/firrtl/raw/master/src/main/scala/tutorial&#34;&gt;src/main/scala/tutorial&lt;/a&gt;. To run these examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt assembly&#xA;./utils/bin/firrtl -td regress -i regress/RocketCore.fir --custom-transforms tutorial.lesson1.AnalyzeCircuit&#xA;./utils/bin/firrtl -td regress -i regress/RocketCore.fir --custom-transforms tutorial.lesson2.AnalyzeCircuit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Other Tools&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Firrtl syntax highlighting for Vim users: &lt;a href=&#34;https://github.com/azidar/firrtl-syntax&#34;&gt;https://github.com/azidar/firrtl-syntax&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Firrtl syntax highlighting for Sublime Text 3 users: &lt;a href=&#34;https://github.com/codelec/highlight-firrtl&#34;&gt;https://github.com/codelec/highlight-firrtl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Firrtl syntax highlighting for Atom users: &lt;a href=&#34;https://atom.io/packages/language-firrtl&#34;&gt;https://atom.io/packages/language-firrtl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Firrtl syntax highlighting, structure view, navigate to corresponding Chisel code for IntelliJ platform: &lt;a href=&#34;https://plugins.jetbrains.com/plugin/14183-easysoc-firrtl&#34;&gt;install&lt;/a&gt;, &lt;a href=&#34;https://github.com/easysoc/easysoc-firrtl&#34;&gt;source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Firrtl mode for Emacs users: &lt;a href=&#34;https://github.com/ibm/firrtl-mode&#34;&gt;https://github.com/ibm/firrtl-mode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chisel3, an embedded hardware DSL that generates Firrtl: &lt;a href=&#34;https://github.com/freechipsproject/chisel3&#34;&gt;https://github.com/freechipsproject/chisel3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Treadle, a Firrtl Interpreter: &lt;a href=&#34;https://github.com/freechipsproject/treadle&#34;&gt;https://github.com/freechipsproject/treadle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Yosys Verilog-to-Firrtl Front-end: &lt;a href=&#34;https://github.com/cliffordwolf/yosys&#34;&gt;https://github.com/cliffordwolf/yosys&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Installation Instructions&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer&lt;/em&gt;: The installation instructions should work for OSX/Linux machines. Other environments may not be tested.&lt;/p&gt; &#xA;&lt;h5&gt;Prerequisites&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If not already installed, install &lt;a href=&#34;http://www.veripool.org/projects/verilator/wiki/Installing&#34;&gt;verilator&lt;/a&gt; (Requires at least v3.886)&lt;/li&gt; &#xA; &lt;li&gt;If not already installed, install &lt;a href=&#34;https://github.com/YosysHQ/yosys&#34;&gt;yosys&lt;/a&gt; (Requires at least v0.8)&lt;/li&gt; &#xA; &lt;li&gt;If not already installed, install &lt;a href=&#34;http://www.scala-sbt.org/&#34;&gt;sbt&lt;/a&gt; (Recommend v1.6.2)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Installation&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;code&gt;git clone https://github.com/freechipsproject/firrtl.git &amp;amp;&amp;amp; cd firrtl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Compile firrtl: &lt;code&gt;sbt compile&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run tests: &lt;code&gt;sbt test&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build executable (&lt;code&gt;utils/bin/firrtl&lt;/code&gt;): &lt;code&gt;sbt assembly&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can add &lt;code&gt;utils/bin&lt;/code&gt; to your path to call firrtl from other processes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Publish this version locally in order to satisfy other tool chain library dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Useful sbt Tips&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run a single test suite: &lt;code&gt;sbt &#34;testOnly firrtlTests.UnitTests&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Continually execute a command: &lt;code&gt;sbt ~compile&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Only invoke sbt once:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt&#xA;&amp;gt; compile&#xA;&amp;gt; test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Use scalafix to remove unused import and deprecated procedure syntax&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Remove unused import:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt &#34;firrtl/scalafix RemoveUnused&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Remove deprecated procedure syntax&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt &#34;firrtl/scalafix ProcedureSyntax&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Using Firrtl as a commandline tool&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;utils/bin/firrtl -i regress/rocket.fir -o regress/rocket.v -X verilog // Compiles rocket-chip to Verilog&#xA;utils/bin/firrtl --help // Returns usage string&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Using the JQF Fuzzer&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;build.sbt&lt;/code&gt; defines the &lt;code&gt;fuzzer/jqfFuzz&lt;/code&gt; and &lt;code&gt;fuzzer/jqfRepro&lt;/code&gt; tasks. These can be used to randomly generate and run test cases and reproduce failing test cases respectively. These tasks are Scala implementations of the &lt;a href=&#34;https://github.com/rohanpadhye/JQF/tree/master/maven-plugin/src/main/java/edu/berkeley/cs/jqf/plugin&#34;&gt;FuzzGoal and ReproGoal&lt;/a&gt; of the JQF maven plugin and should be functionally identical.&lt;/p&gt; &#xA;&lt;p&gt;The format for the arguments to jqfFuzz are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt&amp;gt; fuzzer/jqfFuzz &amp;lt;testClassName&amp;gt; &amp;lt;testMethodName&amp;gt; &amp;lt;otherArgs&amp;gt;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The available options are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  --classpath &amp;lt;value&amp;gt;       the classpath to instrument and load the test class from&#xA;  --outputDirectory &amp;lt;value&amp;gt; the directory to output test results&#xA;  --testClassName &amp;lt;value&amp;gt;   the full class path of the test class&#xA;  --testMethod &amp;lt;value&amp;gt;      the method of the test class to run&#xA;  --excludes &amp;lt;value&amp;gt;        comma-separated list of FQN prefixes to exclude from coverage instrumentation&#xA;  --includes &amp;lt;value&amp;gt;        comma-separated list of FQN prefixes to forcibly include, even if they match an exclude&#xA;  --time &amp;lt;value&amp;gt;            the duration of time for which to run fuzzing&#xA;  --blind                   whether to generate inputs blindly without taking into account coverage feedback&#xA;  --engine &amp;lt;value&amp;gt;          the fuzzing engine, valid choices are zest|zeal&#xA;  --disableCoverage         disable code-coverage instrumentation&#xA;  --inputDirectory &amp;lt;value&amp;gt;  the name of the input directory containing seed files&#xA;  --saveAll                 save ALL inputs generated during fuzzing, even the ones that do not have any unique code coverage&#xA;  --libFuzzerCompatOutput   use libFuzzer like output instead of AFL like stats screen&#xA;  --quiet                   avoid printing fuzzing statistics progress in the console&#xA;  --exitOnCrash             stop fuzzing once a crash is found.&#xA;  --runTimeout &amp;lt;value&amp;gt;      the timeout for each individual trial, in milliseconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;fuzzer/jqfFuzz&lt;/code&gt; sbt task is a thin wrapper around the &lt;code&gt;firrtl.jqf.jqfFuzz&lt;/code&gt; main method that provides the &lt;code&gt;--classpath&lt;/code&gt; argument and a default &lt;code&gt;--outputDirectory&lt;/code&gt; and passes the rest of the arguments to the main method verbatim.&lt;/p&gt; &#xA;&lt;p&gt;The results will be put in the &lt;code&gt;fuzzer/target/JQf/$testClassName/$testMethod&lt;/code&gt; directory. Input files in the &lt;code&gt;fuzzer/target/JQf/$testClassName/$testMethod/corpus&lt;/code&gt; and &lt;code&gt;fuzzer/target/JQf/$testClassName/$testMethod/failures&lt;/code&gt; directories can be passed as inputs to the &lt;code&gt;fuzzer/jqfRepro&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;p&gt;The format for the arguments to jqfRepro are the same as &lt;code&gt;jqfFuzz&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt&amp;gt; fuzzer/jqfRepro &amp;lt;testClassName&amp;gt; &amp;lt;testMethodName&amp;gt; &amp;lt;otherArgs&amp;gt;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The available options are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  --classpath &amp;lt;value&amp;gt;      the classpath to instrument and load the test class from&#xA;  --testClassName &amp;lt;value&amp;gt;  the full class path of the test class&#xA;  --testMethod &amp;lt;value&amp;gt;     the method of the test class to run&#xA;  --input &amp;lt;value&amp;gt;          input file or directory to reproduce test case(s)&#xA;  --logCoverage &amp;lt;value&amp;gt;    output file to dump coverage info&#xA;  --excludes &amp;lt;value&amp;gt;       comma-separated list of FQN prefixes to exclude from coverage instrumentation&#xA;  --includes &amp;lt;value&amp;gt;       comma-separated list of FQN prefixes to forcibly include, even if they match an exclude&#xA;  --printArgs              whether to print the args to each test case&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Like &lt;code&gt;fuzzer/jqfFuzz&lt;/code&gt;, the &lt;code&gt;fuzzer/jqfRepro&lt;/code&gt; sbt task is a thin wrapper around the &lt;code&gt;firrtl.jqf.jqfRepro&lt;/code&gt; main method that provides the &lt;code&gt;--classpath&lt;/code&gt; argument and a default &lt;code&gt;--outputDirectory&lt;/code&gt; and passes the rest of the arguments to the main method verbatim.&lt;/p&gt; &#xA;&lt;h5&gt;Citing Firrtl&lt;/h5&gt; &#xA;&lt;p&gt;If you use Firrtl in a paper, please cite the following ICCAD paper and technical report: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8203780&#34;&gt;https://ieeexplore.ieee.org/document/8203780&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@INPROCEEDINGS{8203780, &#xA;author={A. Izraelevitz and J. Koenig and P. Li and R. Lin and A. Wang and A. Magyar and D. Kim and C. Schmidt and C. Markley and J. Lawson and J. Bachrach}, &#xA;booktitle={2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, &#xA;title={Reusability is FIRRTL ground: Hardware construction languages, compiler frameworks, and transformations}, &#xA;year={2017}, &#xA;volume={}, &#xA;number={}, &#xA;pages={209-216}, &#xA;keywords={field programmable gate arrays;hardware description languages;program compilers;software reusability;hardware development practices;hardware libraries;open-source hardware intermediate representation;hardware compiler transformations;Hardware construction languages;retargetable compilers;software development;virtual Cambrian explosion;hardware compiler frameworks;parameterized libraries;FIRRTL;FPGA mappings;Chisel;Flexible Intermediate Representation for RTL;Reusability;Hardware;Libraries;Hardware design languages;Field programmable gate arrays;Tools;Open source software;RTL;Design;FPGA;ASIC;Hardware;Modeling;Reusability;Hardware Design Language;Hardware Construction Language;Intermediate Representation;Compiler;Transformations;Chisel;FIRRTL}, &#xA;doi={10.1109/ICCAD.2017.8203780}, &#xA;ISSN={1558-2434}, &#xA;month={Nov},}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-9.html&#34;&gt;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-9.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@techreport{Li:EECS-2016-9,&#xA;    Author = {Li, Patrick S. and Izraelevitz, Adam M. and Bachrach, Jonathan},&#xA;    Title = {Specification for the FIRRTL Language},&#xA;    Institution = {EECS Department, University of California, Berkeley},&#xA;    Year = {2016},&#xA;    Month = {Feb},&#xA;    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-9.html},&#xA;    Number = {UCB/EECS-2016-9}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>