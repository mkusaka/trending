<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-21T02:02:08Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>metarank/metarank</title>
    <updated>2023-05-21T02:02:08Z</updated>
    <id>tag:github.com,2023-05-21:/metarank/metarank</id>
    <link href="https://github.com/metarank/metarank" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A low code Machine Learning personalized ranking service for articles, listings, search results, recommendations that boosts user engagement. A friendly Learn-to-Rank engine&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a style=&#34;text-decoration: none&#34; href=&#34;https://www.metarank.ai&#34;&gt; &lt;img width=&#34;120&#34; src=&#34;https://raw.githubusercontent.com/metarank/metarank/master/doc/img/logo.svg?sanitize=true&#34;&gt; &lt;p align=&#34;center&#34;&gt;Metarank: real time personalization as a service&lt;/p&gt; &lt;/a&gt; &lt;/h1&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.metarank.ai&#34;&gt;Docs&lt;/a&gt; | &lt;a href=&#34;https://metarank.ai&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://metarank.ai/slack&#34;&gt;Community Slack&lt;/a&gt; | &lt;a href=&#34;https://blog.metarank.ai&#34;&gt;Blog&lt;/a&gt; | &lt;a href=&#34;https://demo.metarank.ai&#34;&gt;Demo&lt;/a&gt; &lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/metarank/metarank/actions&#34;&gt;&lt;img src=&#34;https://github.com/metarank/metarank/workflows/Tests/badge.svg?sanitize=true&#34; alt=&#34;CI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache2-green.svg?sanitize=true&#34; alt=&#34;License: Apache 2&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/metarank/metarank&#34; alt=&#34;Last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/metarank/metarank&#34; alt=&#34;Last release&#34;&gt; &lt;a href=&#34;https://metarank.ai/slack&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join%20the%20community-blue?logo=slack&amp;amp;style=social&#34; alt=&#34;Join our slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is Metarank?&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://metarank.ai&#34;&gt;Metarank&lt;/a&gt; is an open-source ranking service. It can help you to build a personalized semantic/neural search and recommendations.&lt;/p&gt; &#xA;&lt;p&gt;If you just want to get started, try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://docs.metarank.ai/introduction/quickstart&#34;&gt;quickstart&lt;/a&gt; tutorial of implementing Learning-to-Rank on top of your search engine.&lt;/li&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/TODO&#34;&gt;semantic search guide&lt;/a&gt; of building an LLM-based neural search.&lt;/li&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/TODO&#34;&gt;collaborative filtering recommendations guide&lt;/a&gt; to create a &#34;you may also like&#34; widget as seen on many e-commerce stores.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Metarank?&lt;/h2&gt; &#xA;&lt;p&gt;With Metarank, you can make your existing search and recommendations &lt;strong&gt;smarter&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrate customer signals like clicks and purchases into the ranking - and optimize for maximal CTR!&lt;/li&gt; &#xA; &lt;li&gt;Track &lt;a href=&#34;https://docs.metarank.ai/reference/overview/feature-extractors/user-session&#34;&gt;visitor profile&lt;/a&gt; and make search results adapt to user actions with real-time personalization.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://docs.metarank.ai/reference/overview/feature-extractors/text&#34;&gt;LLMs in bi- and cross-encoder mode&lt;/a&gt; to make your search understand the true meaning of search queries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Metarank is &lt;strong&gt;fast&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;optimized for reranking latency, it can handle even large result sets within 10-20ms. See &lt;a href=&#34;https://docs.metarank.ai/introduction/performance&#34;&gt;benchmarks&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;as a stateless cloud-native service (with state managed by Redis), it can scale horizontally and process thousands of RPS. See &lt;a href=&#34;https://docs.metarank.ai/reference/deployment-overview/kubernetes&#34;&gt;Kubernetes deployment guide&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Save your &lt;strong&gt;development time&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Metarank can compute dozens of typical ranking signals out of the box: CTR, referer, User-Agent, time, etc - you don&#39;t need to write custom ad-hoc code for most common ranking factors. See &lt;a href=&#34;https://docs.metarank.ai/reference/overview/feature-extractors&#34;&gt;the full list of supported ranking signals&lt;/a&gt; in our docs.&lt;/li&gt; &#xA; &lt;li&gt;There are integrations with many possible streaming processing systems to ingest visitor signals: See &lt;a href=&#34;https://docs.metarank.ai/reference/overview/data-sources&#34;&gt;data sources&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What can you build with Metarank?&lt;/h2&gt; &#xA;&lt;p&gt;Metarank helps you build advanced ranking systems for search and recommendations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Semantic search: use state-of-the-art LLMs to make your Elasticsearch/OpenSearch understand the meaning of your queries&lt;/li&gt; &#xA; &lt;li&gt;Recommendations: traditional collaborative-filtering and new-age semantic content recommendations.&lt;/li&gt; &#xA; &lt;li&gt;Learning-to-Rank: optimize your existing search&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Content&lt;/h2&gt; &#xA;&lt;p&gt;Blog posts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opensearch.org/blog/ltr-with-opensearch-and-metarank/&#34;&gt;Learn-to-Rank with OpenSearch and Metarank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/metarank/&#34;&gt;Hybrid Search and Learning-to-Rank with Metarank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.metarank.ai/solving-a-search-cold-start-problem-with-aggregated-ctr-b88c14f4d03c&#34;&gt;Solving a search cold-start problem with aggregated CTR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.metarank.ai/personalized-search-with-metarank-and-elasticsearch-a5a098548da7&#34;&gt;Personalized search with Metarank and Elasticsearch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meetups and conference talks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lbbp4CFWZGk&#34;&gt;Building an open-source online Learn-to-rank engine&lt;/a&gt;, Haystack EU 23, &lt;a href=&#34;https://metarank.github.io/haystack-eu22/#/&#34;&gt;slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PqbYdDiwKBY&#34;&gt;Overcoming position and presentation biases in search and recommender systems&lt;/a&gt;, Data Natives Meetup Berlin, &lt;a href=&#34;https://metarank.github.io/bias-talk/#/&#34;&gt;slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oXfFqAKf4Ac&#34;&gt;Learning-to-rank: Deep, fast, precise - choose any two&lt;/a&gt;, DataTalks meetup, &lt;a href=&#34;https://metarank.github.io/datatalks-ltr-talk/#/&#34;&gt;slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Semantic neural search: [TODO]&lt;/li&gt; &#xA; &lt;li&gt;Recommendations: &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/configuration/recommendations/trending.md&#34;&gt;trending&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/configuration/recommendations/similar.md&#34;&gt;similar-items&lt;/a&gt; (MF ALS).&lt;/li&gt; &#xA; &lt;li&gt;Personalization: &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/quickstart/quickstart.md&#34;&gt;secondary reranking&lt;/a&gt; (LambdaMART)&lt;/li&gt; &#xA; &lt;li&gt;AutoML: &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/howto/autofeature.md&#34;&gt;automatic feature generation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/howto/model-retraining.md&#34;&gt;model re-training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A/B testing: &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/configuration/overview.md#models&#34;&gt;multiple model serving&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can play with Metarank demo on &lt;a href=&#34;https://demo.metarank.ai&#34;&gt;demo.metarank.ai&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/metarank/metarank/master/doc/img/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The demo itself and &lt;a href=&#34;https://github.com/metarank/msrd&#34;&gt;the data used&lt;/a&gt; are open-source and you can grab a copy of training events and config file &lt;a href=&#34;https://github.com/metarank/metarank/tree/master/src/test/resources/ranklens&#34;&gt;in the github repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Metarank in One Minute&lt;/h2&gt; &#xA;&lt;p&gt;Let us show how you can start personalizing content with LambdaMART-based reranking in just under a minute:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare the data: we will get the dataset and config file from the &lt;a href=&#34;https://demo.metarank.ai&#34;&gt;demo.metarank.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start Metarank in a standalone mode: it will import the data, train the ML model and start the API.&lt;/li&gt; &#xA; &lt;li&gt;Send a couple of requests to the API.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 1: Prepare data&lt;/h3&gt; &#xA;&lt;p&gt;We will use the &lt;a href=&#34;https://github.com/metarank/ranklens&#34;&gt;ranklens dataset&lt;/a&gt;, which is used in our &lt;a href=&#34;https://demo.metarank.ai&#34;&gt;Demo&lt;/a&gt;, so just download the data file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -O -L https://github.com/metarank/metarank/raw/master/src/test/resources/ranklens/events/events.jsonl.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Prepare configuration file&lt;/h3&gt; &#xA;&lt;p&gt;We will again use the configuration file from our &lt;a href=&#34;https://demo.metarank.ai&#34;&gt;Demo&lt;/a&gt;. It utilizes in-memory store, so no other dependencies are needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -O -L https://raw.githubusercontent.com/metarank/metarank/master/src/test/resources/ranklens/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3: Start Metarank!&lt;/h3&gt; &#xA;&lt;p&gt;With the final step we will use Metarank’s &lt;code&gt;standalone&lt;/code&gt; mode that combines training and running the API into one command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -i -t -p 8080:8080 -v $(pwd):/opt/metarank metarank/metarank:latest standalone --config /opt/metarank/config.yml --data /opt/metarank/events.jsonl.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see some useful output while Metarank is starting and grinding through the data. Once this is done, you can send requests to &lt;code&gt;localhost:8080&lt;/code&gt; to get personalized results.&lt;/p&gt; &#xA;&lt;p&gt;Here we will interact with several movies by clicking on one of them and observing the results.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;First, let&#39;s see the initial output provided by Metarank without before we interact with it&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get initial ranking for some items&#xA;curl http://localhost:8080/rank/xgboost \&#xA;    -d &#39;{&#xA;    &#34;event&#34;: &#34;ranking&#34;,&#xA;    &#34;id&#34;: &#34;id1&#34;,&#xA;    &#34;items&#34;: [&#xA;        {&#34;id&#34;:&#34;72998&#34;}, {&#34;id&#34;:&#34;67197&#34;}, {&#34;id&#34;:&#34;77561&#34;},&#xA;        {&#34;id&#34;:&#34;68358&#34;}, {&#34;id&#34;:&#34;79132&#34;}, {&#34;id&#34;:&#34;103228&#34;}, &#xA;        {&#34;id&#34;:&#34;72378&#34;}, {&#34;id&#34;:&#34;85131&#34;}, {&#34;id&#34;:&#34;94864&#34;}, &#xA;        {&#34;id&#34;:&#34;68791&#34;}, {&#34;id&#34;:&#34;93363&#34;}, {&#34;id&#34;:&#34;112623&#34;}&#xA;    ],&#xA;    &#34;user&#34;: &#34;alice&#34;,&#xA;    &#34;session&#34;: &#34;alice1&#34;,&#xA;    &#34;timestamp&#34;: 1661431886711&#xA;}&#39;&#xA;&#xA;# {&#34;item&#34;:&#34;72998&#34;,&#34;score&#34;:0.9602446652021992},{&#34;item&#34;:&#34;79132&#34;,&#34;score&#34;:0.7819134441404151},{&#34;item&#34;:&#34;68358&#34;,&#34;score&#34;:0.33377910321385645},{&#34;item&#34;:&#34;112623&#34;,&#34;score&#34;:0.32591281190727805},{&#34;item&#34;:&#34;103228&#34;,&#34;score&#34;:0.31640256043322723},{&#34;item&#34;:&#34;77561&#34;,&#34;score&#34;:0.3040782705414116},{&#34;item&#34;:&#34;94864&#34;,&#34;score&#34;:0.17659007036183608},{&#34;item&#34;:&#34;72378&#34;,&#34;score&#34;:0.06164568676567339},{&#34;item&#34;:&#34;93363&#34;,&#34;score&#34;:0.058120639770243385},{&#34;item&#34;:&#34;68791&#34;,&#34;score&#34;:0.026919880032451306},{&#34;item&#34;:&#34;85131&#34;,&#34;score&#34;:-0.35794106000271037},{&#34;item&#34;:&#34;67197&#34;,&#34;score&#34;:-0.48735167237049154}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# tell Metarank which items were presented to the user and in which order from the previous request&#xA;# optionally, we can include the score calculated by Metarank or your internal retrieval system&#xA;curl http://localhost:8080/feedback \&#xA; -d &#39;{&#xA;  &#34;event&#34;: &#34;ranking&#34;,&#xA;  &#34;fields&#34;: [],&#xA;  &#34;id&#34;: &#34;test-ranking&#34;,&#xA;  &#34;items&#34;: [&#xA;    {&#34;id&#34;:&#34;72998&#34;,&#34;score&#34;:0.9602446652021992},{&#34;id&#34;:&#34;79132&#34;,&#34;score&#34;:0.7819134441404151},{&#34;id&#34;:&#34;68358&#34;,&#34;score&#34;:0.33377910321385645},&#xA;    {&#34;id&#34;:&#34;112623&#34;,&#34;score&#34;:0.32591281190727805},{&#34;id&#34;:&#34;103228&#34;,&#34;score&#34;:0.31640256043322723},{&#34;id&#34;:&#34;77561&#34;,&#34;score&#34;:0.3040782705414116},&#xA;    {&#34;id&#34;:&#34;94864&#34;,&#34;score&#34;:0.17659007036183608},{&#34;id&#34;:&#34;72378&#34;,&#34;score&#34;:0.06164568676567339},{&#34;id&#34;:&#34;93363&#34;,&#34;score&#34;:0.058120639770243385},&#xA;    {&#34;id&#34;:&#34;68791&#34;,&#34;score&#34;:0.026919880032451306},{&#34;id&#34;:&#34;85131&#34;,&#34;score&#34;:-0.35794106000271037},{&#34;id&#34;:&#34;67197&#34;,&#34;score&#34;:-0.48735167237049154}&#xA;  ],&#xA;  &#34;user&#34;: &#34;test2&#34;,&#xA;  &#34;session&#34;: &#34;test2&#34;,&#xA;  &#34;timestamp&#34;: 1661431888711&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Now, let&#39;s intereact with the items &lt;code&gt;93363&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# click on the item with id 93363&#xA;curl http://localhost:8080/feedback \&#xA; -d &#39;{&#xA;  &#34;event&#34;: &#34;interaction&#34;,&#xA;  &#34;type&#34;: &#34;click&#34;,&#xA;  &#34;fields&#34;: [],&#xA;  &#34;id&#34;: &#34;test-interaction&#34;,&#xA;  &#34;ranking&#34;: &#34;test-ranking&#34;,&#xA;  &#34;item&#34;: &#34;93363&#34;,&#xA;  &#34;user&#34;: &#34;test&#34;,&#xA;  &#34;session&#34;: &#34;test&#34;,&#xA;  &#34;timestamp&#34;: 1661431890711&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Now, Metarank will personalize the items, the order of the items in the response will be different&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# personalize the same list of items&#xA;# they will be returned in a different order by Metarank&#xA;curl http://localhost:8080/rank/xgboost \&#xA; -d &#39;{&#xA;  &#34;event&#34;: &#34;ranking&#34;,&#xA;  &#34;fields&#34;: [],&#xA;  &#34;id&#34;: &#34;test-personalized&#34;,&#xA;  &#34;items&#34;: [&#xA;    {&#34;id&#34;:&#34;72998&#34;}, {&#34;id&#34;:&#34;67197&#34;}, {&#34;id&#34;:&#34;77561&#34;},&#xA;    {&#34;id&#34;:&#34;68358&#34;}, {&#34;id&#34;:&#34;79132&#34;}, {&#34;id&#34;:&#34;103228&#34;}, &#xA;    {&#34;id&#34;:&#34;72378&#34;}, {&#34;id&#34;:&#34;85131&#34;}, {&#34;id&#34;:&#34;94864&#34;}, &#xA;    {&#34;id&#34;:&#34;68791&#34;}, {&#34;id&#34;:&#34;93363&#34;}, {&#34;id&#34;:&#34;112623&#34;}&#xA;  ],&#xA;  &#34;user&#34;: &#34;test&#34;,&#xA;  &#34;session&#34;: &#34;test&#34;,&#xA;  &#34;timestamp&#34;: 1661431892711&#xA;}&#39;&#xA;&#xA;# {&#34;items&#34;:[{&#34;item&#34;:&#34;93363&#34;,&#34;score&#34;:2.2013986484185124},{&#34;item&#34;:&#34;72998&#34;,&#34;score&#34;:1.1542776301073876},{&#34;item&#34;:&#34;68358&#34;,&#34;score&#34;:0.9828904282341605},{&#34;item&#34;:&#34;112623&#34;,&#34;score&#34;:0.9521647429731446},{&#34;item&#34;:&#34;79132&#34;,&#34;score&#34;:0.9258841742518286},{&#34;item&#34;:&#34;77561&#34;,&#34;score&#34;:0.8990921381835769},{&#34;item&#34;:&#34;103228&#34;,&#34;score&#34;:0.8990921381835769},{&#34;item&#34;:&#34;94864&#34;,&#34;score&#34;:0.7131600718467729},{&#34;item&#34;:&#34;68791&#34;,&#34;score&#34;:0.624462038351694},{&#34;item&#34;:&#34;72378&#34;,&#34;score&#34;:0.5269765094008626},{&#34;item&#34;:&#34;85131&#34;,&#34;score&#34;:0.29198666089255343},{&#34;item&#34;:&#34;67197&#34;,&#34;score&#34;:0.16412780810560743}]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Useful Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.metarank.ai&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/metarank/ranklens&#34;&gt;Ranklens Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/CONTRIBUTING.md&#34;&gt;Contribution guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s next?&lt;/h2&gt; &#xA;&lt;p&gt;Check out a more in-depth &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/doc/quickstart/quickstart.md&#34;&gt;Quickstart&lt;/a&gt; full &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/doc/installation.md&#34;&gt;Reference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions, don&#39;t hesitate to join our &lt;a href=&#34;https://communityinviter.com/apps/metarank/metarank&#34;&gt;Slack&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is released under the Apache 2.0 license, as specified in the &lt;a href=&#34;https://raw.githubusercontent.com/metarank/metarank/master/LICENSE&#34;&gt;License&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>crealytics/spark-excel</title>
    <updated>2023-05-21T02:02:08Z</updated>
    <id>tag:github.com,2023-05-21:/crealytics/spark-excel</id>
    <link href="https://github.com/crealytics/spark-excel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Spark plugin for reading and writing Excel files&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Excel Library&lt;/h1&gt; &#xA;&lt;p&gt;A library for querying Excel files with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crealytics/spark-excel/actions&#34;&gt;&lt;img src=&#34;https://github.com/crealytics/spark-excel/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Co-maintainers wanted&lt;/h2&gt; &#xA;&lt;p&gt;Due to personal and professional constraints, the development of this library has been rather slow. If you find value in this library, please consider stepping up as a co-maintainer by leaving a comment &lt;a href=&#34;https://github.com/crealytics/spark-excel/issues/191&#34;&gt;here&lt;/a&gt;. Help is very welcome e.g. in the following areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional features&lt;/li&gt; &#xA; &lt;li&gt;Code improvements and reviews&lt;/li&gt; &#xA; &lt;li&gt;Bug analysis and fixing&lt;/li&gt; &#xA; &lt;li&gt;Documentation improvements&lt;/li&gt; &#xA; &lt;li&gt;Build / test infrastructure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 2.0+.&lt;/p&gt; &#xA;&lt;p&gt;List of spark versions, those are automatically tested:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark: [&#34;2.4.1&#34;, &#34;2.4.7&#34;, &#34;2.4.8&#34;, &#34;3.0.1&#34;, &#34;3.0.3&#34;, &#34;3.1.1&#34;, &#34;3.1.2&#34;, &#34;3.2.1&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more detail, please refer to project CI: &lt;a href=&#34;https://github.com/crealytics/spark-excel/raw/main/.github/workflows/ci.yml#L10&#34;&gt;ci.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.12&#xA;version: &amp;lt;spark-version&amp;gt;_0.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.11&#xA;version: &amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.12:&amp;lt;spark-version&amp;gt;_0.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.11:&amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This package allows querying Excel spreadsheets as &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;From spark-excel &lt;a href=&#34;https://github.com/crealytics/spark-excel/releases/tag/v0.14.0&#34;&gt;0.14.0&lt;/a&gt; (August 24, 2021), there are two implementation of spark-excel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Original Spark-Excel with Spark data source API 1.0&lt;/li&gt; &#xA;   &lt;li&gt;Spark-Excel V2 with data source API V2.0+, which supports loading from multiple files, corrupted record handling and some improvement on handling data types. See below for further details&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/#excel-api-based-on-datasourcev2&#34;&gt;below&lt;/a&gt; for some details&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/CHANGELOG.md&#34;&gt;changelog&lt;/a&gt; for latest features, fixes etc.&lt;/p&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 2.0+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;) // Optional, default: &#34;A1&#34;&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Required&#xA;    .option(&#34;treatEmptyValuesAsNulls&#34;, &#34;false&#34;) // Optional, default: true&#xA;    .option(&#34;setErrorCellsToFallbackValues&#34;, &#34;true&#34;) // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    .option(&#34;usePlainNumberFormat&#34;, &#34;false&#34;) // Optional, default: false, If true, format the cells without rounding and scientific notations&#xA;    .option(&#34;inferSchema&#34;, &#34;false&#34;) // Optional, default: false&#xA;    .option(&#34;addColorColumns&#34;, &#34;true&#34;) // Optional, default: false&#xA;    .option(&#34;timestampFormat&#34;, &#34;MM-dd-yyyy HH:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    .option(&#34;maxRowsInMemory&#34;, 20) // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    .option(&#34;maxByteArraySize&#34;, 2147483647) // Optional, default None. See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-&#xA;    .option(&#34;tempFileThreshold&#34;, 10000000) // Optional, default None. Number of bytes at which a zip entry is regarded as too large for holding in memory and the data is put in a temp file instead&#xA;    .option(&#34;excerptSize&#34;, 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    .option(&#34;workbookPassword&#34;, &#34;pass&#34;) // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;    .schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA;    .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For convenience, there is an implicit that wraps the &lt;code&gt;DataFrameReader&lt;/code&gt; returned by &lt;code&gt;spark.read&lt;/code&gt; and provides a &lt;code&gt;.excel&lt;/code&gt; method which accepts all possible options and provides default values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import com.crealytics.spark.excel._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read.excel(&#xA;    header = true,  // Required&#xA;    dataAddress = &#34;&#39;My Sheet&#39;!B3:C35&#34;, // Optional, default: &#34;A1&#34;&#xA;    treatEmptyValuesAsNulls = false,  // Optional, default: true&#xA;    setErrorCellsToFallbackValues = false, // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    usePlainNumberFormat = false,  // Optional, default: false. If true, format the cells without rounding and scientific notations&#xA;    inferSchema = false,  // Optional, default: false&#xA;    addColorColumns = true,  // Optional, default: false&#xA;    timestampFormat = &#34;MM-dd-yyyy HH:mm:ss&#34;,  // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    maxRowsInMemory = 20,  // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    maxByteArraySize = 2147483647,  // Optional, default None. See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-&#xA;    tempFileThreshold = 10000000, // Optional, default None. Number of bytes at which a zip entry is regarded as too large for holding in memory and the data is put in a temp file instead&#xA;    excerptSize = 10,  // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    workbookPassword = &#34;pass&#34;  // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;).schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA; .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the sheet name is unavailable, it is possible to pass in an index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = &#34;0!B3:C35&#34;&#xA;).load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to read in the names dynamically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.crealytics.spark.excel.WorkbookReader&#xA;val sheetNames = WorkbookReader( Map(&#34;path&#34; -&amp;gt; &#34;Worktime.xlsx&#34;)&#xA;                               , spark.sparkContext.hadoopConfiguration&#xA;                               ).sheetNames&#xA;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = sheetNames(0)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file using custom schema&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import org.apache.spark.sql.types._&#xA;&#xA;val peopleSchema = StructType(Array(&#xA;    StructField(&#34;Name&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Age&#34;, DoubleType, nullable = false),&#xA;    StructField(&#34;Occupation&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Date of birth&#34;, StringType, nullable = false)))&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;Info&#39;!A1&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .schema(peopleSchema)&#xA;    .load(&#34;People.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Write a DataFrame to an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val df: DataFrame = ???&#xA;df.write&#xA;  .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;  .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;)&#xA;  .option(&#34;header&#34;, &#34;true&#34;)&#xA;  .option(&#34;dateFormat&#34;, &#34;yy-mmm-d&#34;) // Optional, default: yy-m-d h:mm&#xA;  .option(&#34;timestampFormat&#34;, &#34;mm-dd-yyyy hh:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss.000&#xA;  .mode(&#34;append&#34;) // Optional, default: overwrite.&#xA;  .save(&#34;Worktime2.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Data Addresses&lt;/h4&gt; &#xA;&lt;p&gt;As you can see in the examples above, the location of data to read or write can be specified with the &lt;code&gt;dataAddress&lt;/code&gt; option. Currently the following address styles are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;B3&lt;/code&gt;: Start cell of the data. Reading will return all rows below and all columns to the right. Writing will start here and use as many columns and rows as required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;B3:F35&lt;/code&gt;: Cell range of data. Reading will return only rows and columns in the specified range. Writing will start in the first cell (&lt;code&gt;B3&lt;/code&gt; in this example) and use only the specified columns and rows. If there are more rows or columns in the DataFrame to write, they will be truncated. Make sure this is what you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;My Sheet&#39;!B3:F35&lt;/code&gt;: Same as above, but with a specific sheet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MyTable[#All]&lt;/code&gt;: Table of data. Reading will return all rows and columns in this table. Writing will only write within the current range of the table. No growing of the table will be performed. PRs to change this are welcome.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Excel API based on DataSourceV2&lt;/h3&gt; &#xA;&lt;p&gt;The V2 API offers you several improvements when it comes to file and folder handling. and works in a very similar way than data sources like csv and parquet.&lt;/p&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The big difference is the fact that you provide a path to read / write data from/to and not an individual single file only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.read&#xA;        .format(&#34;excel&#34;)&#xA;        // ... insert excel read specific options you need&#xA;        .load(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because folders are supported you can read/write from/to a &#34;partitioned&#34; folder structure, just the same way as csv or parquet. Note that writing partitioned structures is only available for spark &amp;gt;=3.0.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .partitionBy(&#34;col1&#34;)&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Need some more examples? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/src/test/scala/com/crealytics/spark/v2/excel/DataFrameWriterApiComplianceSuite.scala&#34;&gt;test cases&lt;/a&gt; or have a look at our wiki&lt;/p&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;https://github.com/com-lihaoyi/mill&#34;&gt;Mill&lt;/a&gt;. To build a JAR file simply run e.g. &lt;code&gt;mill spark-excel[2.13.10,3.3.1].assembly&lt;/code&gt; from the project root, where &lt;code&gt;2.13.10&lt;/code&gt; is the Scala version and &lt;code&gt;3.3.1&lt;/code&gt; the Spark version. To list all available combinations of Scala and Spark, run &lt;code&gt;mill resolve spark-excel[__]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#crealytics/spark-excel&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=crealytics/spark-excel&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/rocket-chip-fpga-shells</title>
    <updated>2023-05-21T02:02:08Z</updated>
    <id>tag:github.com,2023-05-21:/chipsalliance/rocket-chip-fpga-shells</id>
    <link href="https://github.com/chipsalliance/rocket-chip-fpga-shells" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Wrapper shells enabling designs generated by rocket-chip to map onto certain FPGA boards&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FPGA Shells for the Rocket Chip Generator&lt;/h1&gt; &#xA;&lt;p&gt;An FPGA shell is a Chisel module designed to wrap any Rocket Chip subsystem configuration. The goal of the fpga-shell system is to reduce the number of wrappers so as to have only one for each physical device rather than one for every combination of physical device and core configuration.&lt;/p&gt; &#xA;&lt;p&gt;This repository replaces &lt;a href=&#34;https://github.com/sifive/fpga-shells&#34;&gt;https://github.com/sifive/fpga-shells&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each shell consists of Overlays which use dependency injection to create and connect peripheral device interfaces in an FPGADesign to the toplevel shell module.&lt;/p&gt; &#xA;&lt;p&gt;Most devices already have an overlay defined for them in &lt;code&gt;src/main/scala/shell[/xilinx]&lt;/code&gt;. If you&#39;re using a Xilinx device, you&#39;ll probably want to use the xilinx-specific overlay because it defines a few things that you&#39;d otherwise have to specify yourself.&lt;/p&gt; &#xA;&lt;p&gt;Generally, you&#39;ll want to create a device shell that extends &lt;code&gt;Series7Shell&lt;/code&gt; or &lt;code&gt;UltraScaleShell&lt;/code&gt;. If you need different functionality (or you&#39;re not using a Xilinx device), you can extend &lt;code&gt;Shell&lt;/code&gt; and implement abstract members. Some Microsemi devices are supported by fgpa-shells as well (and can be found in &lt;code&gt;src/main/scala/shell/microsemi&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;class DeviceShell()(implicit p: Parameters) extends UltraScaleShell {&#xA;  // create Overlays&#xA;  val myperipheral = Overlay(PeripheralOverlayKey) (new PeripheralOverlay(_,_,_))&#xA;  // ...&#xA;&#xA;  // assign abstract members&#xA;  val pllReset = InModuleBody { Wire(Bool()) }&#xA;  val topDesign = LazyModule(p(DesignKey)(designParameters))&#xA;&#xA;  // ensure clocks are connected&#xA;  designParameters(ClockInputOverlayKey).foreach { unused =&amp;gt;&#xA;    val source = unused(ClockInputOverlayParams())&#xA;    val sink = ClockSinkNode(Seq(ClockSinkParameters()))&#xA;    sink := source&#xA;  }&#xA;&#xA;  // override module implementation to connect reset&#xA;  override lazy val module = new LazyRawModuleImp(this) {&#xA;    val reset = IO(Input(Bool()))&#xA;    pllReset := reset&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each peripheral device to be added to the shell must define an &lt;code&gt;Overlay&lt;/code&gt;, which creates the device and connects it to the toplevel shell. In addition, in order to access the overlay, the device needs to have a &lt;code&gt;case class OverlayParams&lt;/code&gt; and a &lt;code&gt;case object OverlayKey&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;case class PeripheralOverlayParams()(implicit val p: Parameters)&#xA;case object PeripheralOverlayKey extends Field[Seq[DesignOverlay[PeripheralOverlayParams, PeripheralDesignOutput]]](Nil)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the device is parameterizable, then each parameter for the device creation can be passed to the &lt;code&gt;PeripheralOverlayParams&lt;/code&gt; constructor by adding a field for said parameter. Typically, devices are connected to a TileLink bus for processor control, so &lt;code&gt;PeripheralDesignOutput&lt;/code&gt; can usually be substituted with &lt;code&gt;TLInwardNode&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Overlay&lt;/code&gt; extends &lt;code&gt;IOOverlay&lt;/code&gt; which is paramerized by the device&#39;s &lt;code&gt;IO&lt;/code&gt; (in this case &lt;code&gt;PeripheralDeviceIO&lt;/code&gt; is a subtype of &lt;code&gt;Data&lt;/code&gt; and is a port specification for the peripheral device) and &lt;code&gt;DesignOutput&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;abstract class AbstractPeripheralOverlay(val params: PeripheralOverlayParams)&#xA;  extends IOOverlay[PeripheralDeviceIO, PeripheralDesignOutput]&#xA;{&#xA;  // assign abstract member p (used to access overlays with their key)&#xA;  // e.g. p(PeripheralOverlayKey) will return a Seq[DesignOverlay[PeripheralOverlayParams, PeripheralDesignOutput]]&#xA;  implicit val p = params.p&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Continuing our example with a &lt;code&gt;DeviceShell&lt;/code&gt; shell, the actual overlay is constructed by extending our abstract &lt;code&gt;PeripheralOverlay&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;class ConcretePeripheralOverlay(val shell: DeviceShell, val name: String, params: PeripheralOverlayParams)&#xA;  extends AbstractPeripheralOverlay(params)&#xA;{&#xA;  val device = LazyModule(new PeripheralDevice(PeripheralDeviceParams(???))) // if your peripheral device isn&#39;t parameterizable, then it&#39;ll have an empty constructor&#xA;&#xA;  def ioFactory = new PeripheralDeviceIO // ioFactory defines interface of val io&#xA;  val designOutput = device.node&#xA;&#xA;  // this is where &#34;code-injection&#34; starts&#xA;  val ioSource = BundleBridgeSource(() =&amp;gt; device.module.io.cloneType) // create a bridge between device (source) and shell (sink)&#xA;  val ioSink = shell { ioSource.makeSink() }&#xA;&#xA;  InModuleBody { ioSource.bundle &amp;lt;&amp;gt; device.module.io }&#xA;&#xA;  shell { InModuleBody {&#xA;    val port = ioSink.bundle&#xA;&#xA;    io &amp;lt;&amp;gt; port // io is the bundle of shell-level IO as specified by ioFactory&#xA;  } }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The actual device implementation (where the device&#39;s functionality is defined) will be something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;case class PeripheralDeviceParams(param1: Param1Type, ???) // only necessary if your device is parameterizable&#xA;class PeripheralDevice(c: PeripheralDeviceParams)(implicit p: Parameters) extends LazyModule {&#xA;  &#xA;  val node: PeripheralDesignOutput = ???&#xA;&#xA;  // device implementation&#xA;  lazy val module = new LazyModuleImp(this) {&#xA;    val io = ???&#xA;    ???&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>