<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-21T03:35:25Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>databricks/tableau-connector</title>
    <updated>2024-04-21T03:35:25Z</updated>
    <id>tag:github.com,2024-04-21:/databricks/tableau-connector</id>
    <link href="https://github.com/databricks/tableau-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tableau Databricks Connector&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This directory contains the Tableau Databricks Connector.&lt;/p&gt; &#xA;&lt;p&gt;The connector is built with the Tableau Connector SDK and provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a simplified connection dialog&lt;/li&gt; &#xA; &lt;li&gt;extensions of the Spark SQL dialect that clear all Tableau Datasource Verification Tool tests&lt;/li&gt; &#xA; &lt;li&gt;optimized ODBC connection settings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The connector consists of a set of files that describe the UI elements needed to collect user input for creating a connection to a Databricks cluster and dialect specifications and customizations needed for the connection.&lt;/p&gt; &#xA;&lt;p&gt;The connector includes a connection string builder in which we incorporate optimal ODBC parameter settings and a driver resolver that contains the ODBC driver version.&lt;/p&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://docs.databricks.com/user-guide/bi/tableau.html&#34;&gt;https://docs.databricks.com/user-guide/bi/tableau.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dev Cycle&lt;/h2&gt; &#xA;&lt;p&gt;To test a custom connector, follow the steps below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Remove the embedded connector file from the connector directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS: &lt;code&gt;/Applications/Tableau Desktop &amp;lt;version&amp;gt;.app/Contents/Frameworks/connectors/libdatabricks.dylib&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows: &lt;code&gt;C:\Program Files\Tableau\Tableau 2020.2\bin\connectors\databricks.dll&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Change the name of the connector in the &lt;code&gt;manifest.xml&lt;/code&gt; file. This is not strictly necessary, but the name field uses a string constant which remains unresolved when loaded as a plugin. Therefore, the display name will be something similar to &lt;code&gt;@string/databricks&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Include the vendor information in the &lt;code&gt;manifest.xml&lt;/code&gt;. The fact that the connector plugin doesn’t include vendor information is intentional, so this change shouldn’t be pushed to the repo. Tableau incorporated connectors have a slightly different integration than most custom connectors. However, when loading it using the command line argument you will need to add it to pass the &lt;code&gt;XSD&lt;/code&gt; validation for the manifest file when Tableau starts up.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;connector-plugin class=&#39;databricks&#39; superclass=&#39;spark&#39; plugin-version=&#39;0.1&#39; name=&#39;Databricks&#39; version=&#39;18.1&#39;&amp;gt; &amp;lt;vendor-information&amp;gt; &amp;lt;company name=&#34;Company Name&#34;/&amp;gt; &amp;lt;support-link url = &#34;https://example.com&#34;/&amp;gt; &amp;lt;/vendor-information&amp;gt; ... &amp;lt;/connector-plugin&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Comment the &lt;code&gt;UserAgentEntry&lt;/code&gt; line in &lt;code&gt;connection-builder.js&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start Tableau with the connector plugin path:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS: &lt;code&gt;/Applications/Tableau\ Desktop\&amp;lt;version&amp;gt;.app/Contents/MacOS/Tableau -DConnectPluginsPath=$PATH/tableau-connector/connectors -DLogLevel=Debug&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;C:\Program Files\Tableau\Tableau &amp;lt;version&amp;gt;\bin\tableau.exe&#34; &#34;-DConnectPluginsPath=$PATH\tableau-connector\connectors&lt;/code&gt; -DLogLevel=Debug`&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing the Connector&lt;/h2&gt; &#xA;&lt;h3&gt;Tableau Desktop&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Test connecting to the correct database with a valid user but without the drivers installed, verify that the error message is good&lt;/li&gt; &#xA; &lt;li&gt;Clicking Connect (and canceling)&lt;/li&gt; &#xA; &lt;li&gt;Enter valid items (name, user, password, db, port, client, language etc) in each field and verify it connects&lt;/li&gt; &#xA; &lt;li&gt;Verify that the default Connect name is correct and you can change it&lt;/li&gt; &#xA; &lt;li&gt;Can connect successfully using all supported methods of authentication&lt;/li&gt; &#xA; &lt;li&gt;Duplicating the data connection&lt;/li&gt; &#xA; &lt;li&gt;Closing the data connection&lt;/li&gt; &#xA; &lt;li&gt;Properties of data connection are correct. Enter a bad password and verify that Tableau detects the error correctly.&lt;/li&gt; &#xA; &lt;li&gt;Making Extracts&lt;/li&gt; &#xA; &lt;li&gt;Refreshing Extracts&lt;/li&gt; &#xA; &lt;li&gt;Edit your connection (right-click -&amp;gt; &lt;code&gt;Edit Connection&lt;/code&gt;) and change all possible items (server name, auth, user, password, database, tables, custom SQL and any other available items), verify changes are applied.&lt;/li&gt; &#xA; &lt;li&gt;Simple localization test&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Tableau Server&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Publish workbook to and open from data server without extract&lt;/li&gt; &#xA; &lt;li&gt;Publish workbook to and open from data server with extract&lt;/li&gt; &#xA; &lt;li&gt;Publish workbooks refresh extract on server&lt;/li&gt; &#xA; &lt;li&gt;Verify the views ask you to login if you did not embed credentials in the preview panel&lt;/li&gt; &#xA; &lt;li&gt;Open workbooks&lt;/li&gt; &#xA; &lt;li&gt;Publish datasource without extract&lt;/li&gt; &#xA; &lt;li&gt;Publish datasource with extract&lt;/li&gt; &#xA; &lt;li&gt;Extract or refresh the extracts of the data server data sources&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>airbnb/chronon</title>
    <updated>2024-04-21T03:35:25Z</updated>
    <id>tag:github.com,2024-04-21:/airbnb/chronon</id>
    <link href="https://github.com/airbnb/chronon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chronon is a data platform for serving for AI/ML applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chronon: A Data Platform for AI/ML&lt;/h1&gt; &#xA;&lt;p&gt;Chronon is a platform that abstracts away the complexity of data computation and serving for AI/ML applications. Users define features as transformation of raw data, then Chronon can perform batch and streaming computation, scalable backfills, low-latency serving, guaranteed correctness and consistency, as well as a host of observability and monitoring tools.&lt;/p&gt; &#xA;&lt;p&gt;It allows you to utilize all of the data within your organization, from batch tables, event streams or services to power your AI/ML projects, without needing to worry about all the complex orchestration that this would usually entail.&lt;/p&gt; &#xA;&lt;p&gt;More information about Chronon can be found at &lt;a href=&#34;https://chronon.ai/&#34;&gt;chronon.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://chronon.ai/_images/intro.png&#34; alt=&#34;High Level&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Platform Features&lt;/h2&gt; &#xA;&lt;h3&gt;Online Serving&lt;/h3&gt; &#xA;&lt;p&gt;Chronon offers an API for realtime fetching which returns up-to-date values for your features. It supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Managed pipelines for batch and realtime feature computation and updates to the serving backend&lt;/li&gt; &#xA; &lt;li&gt;Low latency serving of computed features&lt;/li&gt; &#xA; &lt;li&gt;Scalable for high fanout feature sets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Backfills&lt;/h3&gt; &#xA;&lt;p&gt;ML practitioners often need historical views of feature values for model training and evaluation. Chronon&#39;s backfills are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalable for large time windows&lt;/li&gt; &#xA; &lt;li&gt;Resilient to highly skewed data&lt;/li&gt; &#xA; &lt;li&gt;Point-in-time accurate such that consistency with online serving is guaranteed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Observability, monitoring and data quality&lt;/h3&gt; &#xA;&lt;p&gt;Chronon offers visibility into:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data freshness - ensure that online values are being updated in realtime&lt;/li&gt; &#xA; &lt;li&gt;Online/Offline consistency - ensure that backfill data for model training and evaluation is consistent with what is being observed in online serving&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Complex transformations and windowed aggregations&lt;/h3&gt; &#xA;&lt;p&gt;Chronon supports a range of aggregation types. For a full list see the documentation &lt;a href=&#34;https://chronon.ai/Aggregations.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These aggregations can all be configured to be computed over arbitrary window sizes.&lt;/p&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;This section walks you through the steps to create a training dataset with Chronon, using a fabricated underlying raw dataset.&lt;/p&gt; &#xA;&lt;p&gt;Includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example implementation of the main API components for defining features - &lt;code&gt;GroupBy&lt;/code&gt; and &lt;code&gt;Join&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The workflow for authoring these entities.&lt;/li&gt; &#xA; &lt;li&gt;The workflow for backfilling training data.&lt;/li&gt; &#xA; &lt;li&gt;The workflows for uploading and serving this data.&lt;/li&gt; &#xA; &lt;li&gt;The workflow for measuring consistency between backfilled training data and online inference data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Does not include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A deep dive on the various concepts and terminologies in Chronon. For that, please see the &lt;a href=&#34;https://chronon.ai/authoring_features/GroupBy.html&#34;&gt;Introductory&lt;/a&gt; documentation.&lt;/li&gt; &#xA; &lt;li&gt;Running streaming jobs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To get started with the Chronon, all you need to do is download the &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; file and run it locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -o docker-compose.yml https://chronon.ai/docker-compose.yml&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you see some data printed with a &lt;code&gt;only showing top 20 rows&lt;/code&gt; notice, you&#39;re ready to proceed with the tutorial.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In this example, let&#39;s assume that we&#39;re a large online retailer, and we&#39;ve detected a fraud vector based on users making purchases and later returning items. We want to train a model that will be called when the &lt;strong&gt;checkout&lt;/strong&gt; flow commences and predicts whether this transaction is likely to result in a fraudulent return.&lt;/p&gt; &#xA;&lt;h2&gt;Raw data sources&lt;/h2&gt; &#xA;&lt;p&gt;Fabricated raw data is included in the &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/data&#34;&gt;data&lt;/a&gt; directory. It includes four tables:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Users - includes basic information about users such as account created date; modeled as a batch data source that updates daily&lt;/li&gt; &#xA; &lt;li&gt;Purchases - a log of all purchases by users; modeled as a log table with a streaming (i.e. Kafka) event-bus counterpart&lt;/li&gt; &#xA; &lt;li&gt;Returns - a log of all returns made by users; modeled as a log table with a streaming (i.e. Kafka) event-bus counterpart&lt;/li&gt; &#xA; &lt;li&gt;Checkouts - a log of all checkout events; &lt;strong&gt;this is the event that drives our model predictions&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Start a shell session in the Docker container&lt;/h3&gt; &#xA;&lt;p&gt;In a new terminal window, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker-compose exec main bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will open a shell within the chronon docker container.&lt;/p&gt; &#xA;&lt;h2&gt;Chronon Development&lt;/h2&gt; &#xA;&lt;p&gt;Now that the setup steps are complete, we can start creating and testing various Chronon objects to define transformation and aggregations, and generate data.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1 - Define some features&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s start with three feature sets, built on top of our raw input sources.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: These python definitions are already in your &lt;code&gt;chronon&lt;/code&gt; image. There&#39;s nothing for you to run until &lt;a href=&#34;https://raw.githubusercontent.com/airbnb/chronon/main/#step-3---backfilling-data&#34;&gt;Step 3 - Backfilling Data&lt;/a&gt; when you&#39;ll run computation for these definitions.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature set 1: Purchases data features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can aggregate the purchases log data to the user level, to give us a view into this user&#39;s previous activity on our platform. Specifically, we can compute &lt;code&gt;SUM&lt;/code&gt;s &lt;code&gt;COUNT&lt;/code&gt;s and &lt;code&gt;AVERAGE&lt;/code&gt;s of their previous purchase amounts over various windows.&lt;/p&gt; &#xA;&lt;p&gt;Because this feature is built upon a source that includes both a table and a topic, its features can be computed in both batch and streaming.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;source = Source(&#xA;    events=EventSource(&#xA;        table=&#34;data.purchases&#34;, # This points to the log table with historical purchase events&#xA;        topic=None, # Streaming is not currently part of quickstart, but this would be where you define the topic for realtime events&#xA;        query=Query(&#xA;            selects=select(&#34;user_id&#34;,&#34;purchase_price&#34;), # Select the fields we care about&#xA;            time_column=&#34;ts&#34;) # The event time&#xA;    ))&#xA;&#xA;window_sizes = [Window(length=day, timeUnit=TimeUnit.DAYS) for day in [3, 14, 30]] # Define some window sizes to use below&#xA;&#xA;v1 = GroupBy(&#xA;    sources=[source],&#xA;    keys=[&#34;user_id&#34;], # We are aggregating by user&#xA;    aggregations=[Aggregation(&#xA;            input_column=&#34;purchase_price&#34;,&#xA;            operation=Operation.SUM,&#xA;            windows=window_sizes&#xA;        ), # The sum of purchases prices in various windows&#xA;        Aggregation(&#xA;            input_column=&#34;purchase_price&#34;,&#xA;            operation=Operation.COUNT,&#xA;            windows=window_sizes&#xA;        ), # The count of purchases in various windows&#xA;        Aggregation(&#xA;            input_column=&#34;purchase_price&#34;,&#xA;            operation=Operation.AVERAGE,&#xA;            windows=window_sizes&#xA;        ) # The average purchases by user in various windows&#xA;    ],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the whole code file here: &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/purchases.py&#34;&gt;purchases GroupBy&lt;/a&gt;. This is also in your docker image. We&#39;ll be running computation for it and the other GroupBys in &lt;a href=&#34;https://raw.githubusercontent.com/airbnb/chronon/main/#step-3---backfilling-data&#34;&gt;Step 3 - Backfilling Data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature set 2: Returns data features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We perform a similar set of aggregations on returns data in the &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/returns.py&#34;&gt;returns GroupBy&lt;/a&gt;. The code is not included here because it looks similar to the above example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature set 3: User data features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Turning User data into features is a littler simpler, primarily because there are no aggregations to include. In this case, the primary key of the source data is the same as the primary key of the feature, so we&#39;re simply extracting column values rather than performing aggregations over rows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;source = Source(&#xA;    entities=EntitySource(&#xA;        snapshotTable=&#34;data.users&#34;, # This points to a table that contains daily snapshots of the entire product catalog&#xA;        query=Query(&#xA;            selects=select(&#34;user_id&#34;,&#34;account_created_ds&#34;,&#34;email_verified&#34;), # Select the fields we care about&#xA;        )&#xA;    ))&#xA;&#xA;v1 = GroupBy(&#xA;    sources=[source],&#xA;    keys=[&#34;user_id&#34;], # Primary key is the same as the primary key for the source table&#xA;    aggregations=None # In this case, there are no aggregations or windows to define&#xA;) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Taken from the &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/users.py&#34;&gt;users GroupBy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2 - Join the features together&lt;/h3&gt; &#xA;&lt;p&gt;Next, we need the features that we previously defined backfilled in a single table for model training. This can be achieved using the &lt;code&gt;Join&lt;/code&gt; API.&lt;/p&gt; &#xA;&lt;p&gt;For our use case, it&#39;s very important that features are computed as of the correct timestamp. Because our model runs when the checkout flow begins, we&#39;ll want to be sure to use the corresponding timestamp in our backfill, such that features values for model training logically match what the model will see in online inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Join&lt;/code&gt; is the API that drives feature backfills for training data. It primarilly performs the following functions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Combines many features together into a wide view (hence the name &lt;code&gt;Join&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Defines the primary keys and timestamps for which feature backfills should be performed. Chronon can then guarantee that feature values are correct as of this timestamp.&lt;/li&gt; &#xA; &lt;li&gt;Performs scalable backfills.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here is what our join looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;source = Source(&#xA;    events=EventSource(&#xA;        table=&#34;data.checkouts&#34;, &#xA;        query=Query(&#xA;            selects=select(&#34;user_id&#34;), # The primary key used to join various GroupBys together&#xA;            time_column=&#34;ts&#34;,&#xA;            ) # The event time used to compute feature values as-of&#xA;    ))&#xA;&#xA;v1 = Join(  &#xA;    left=source,&#xA;    right_parts=[JoinPart(group_by=group_by) for group_by in [purchases_v1, refunds_v1, users]] # Include the three GroupBys&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Taken from the &lt;a href=&#34;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/joins/quickstart/training_set.py&#34;&gt;training_set Join&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;left&lt;/code&gt; side of the join is what defines the timestamps and primary keys for the backfill (notice that it is built on top of the &lt;code&gt;checkout&lt;/code&gt; event, as dictated by our use case).&lt;/p&gt; &#xA;&lt;p&gt;Note that this &lt;code&gt;Join&lt;/code&gt; combines the above three &lt;code&gt;GroupBy&lt;/code&gt;s into one data definition. In the next step, we&#39;ll run the command to execute computation for this whole pipeline.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3 - Backfilling Data&lt;/h3&gt; &#xA;&lt;p&gt;Once the join is defined, we compile it using this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;compile.py --conf=joins/quickstart/training_set.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This converts it into a thrift definition that we can submit to spark with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;run.py --conf production/joins/quickstart/training_set.v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output of the backfill would contain the user_id and ts columns from the left source, as well as the 11 feature columns from the three GroupBys that we created.&lt;/p&gt; &#xA;&lt;p&gt;Feature values would be computed for each user_id and ts on the left side, with guaranteed temporal accuracy. So, for example, if one of the rows on the left was for &lt;code&gt;user_id = 123&lt;/code&gt; and &lt;code&gt;ts = 2023-10-01 10:11:23.195&lt;/code&gt;, then the &lt;code&gt;purchase_price_avg_30d&lt;/code&gt; feature would be computed for that user with a precise 30 day window ending on that timestamp.&lt;/p&gt; &#xA;&lt;p&gt;You can now query the backfilled data using the spark sql shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;spark-sql&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;spark-sql&amp;gt; SELECT user_id, quickstart_returns_v1_refund_amt_sum_30d, quickstart_purchases_v1_purchase_price_sum_14d, quickstart_users_v1_email_verified from default.quickstart_training_set_v1 limit 100;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this only selects a few columns. You can also run a &lt;code&gt;select * from default.quickstart_training_set_v1 limit 100&lt;/code&gt; to see all columns, however, note that the table is quite wide and the results might not be very readable on your screen.&lt;/p&gt; &#xA;&lt;p&gt;To exit the sql shell you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;spark-sql&amp;gt; quit;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Online Flows&lt;/h2&gt; &#xA;&lt;p&gt;Now that we&#39;ve created a join and backfilled data, the next step would be to train a model. That is not part of this tutorial, but assuming it was complete, the next step after that would be to productionize the model online. To do this, we need to be able to fetch feature vectors for model inference. That&#39;s what this next section covers.&lt;/p&gt; &#xA;&lt;h3&gt;Uploading data&lt;/h3&gt; &#xA;&lt;p&gt;In order to serve online flows, we first need the data uploaded to the online KV store. This is different than the backfill that we ran in the previous step in two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The data is not a historic backfill, but rather the most up-to-date feature values for each primary key.&lt;/li&gt; &#xA; &lt;li&gt;The datastore is a transactional KV store suitable for point lookups. We use MongoDB in the docker image, however you are free to integrate with a database of your choice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Upload the purchases GroupBy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;run.py --mode upload --conf production/group_bys/quickstart/purchases.v1 --ds  2023-12-01&#xA;&#xA;spark-submit --class ai.chronon.quickstart.online.Spark2MongoLoader --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.quickstart_purchases_v1_upload mongodb://admin:admin@mongodb:27017/?authSource=admin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upload the returns GroupBy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;run.py --mode upload --conf production/group_bys/quickstart/returns.v1 --ds  2023-12-01&#xA;&#xA;spark-submit --class ai.chronon.quickstart.online.Spark2MongoLoader --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.quickstart_returns_v1_upload mongodb://admin:admin@mongodb:27017/?authSource=admin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upload Join Metadata&lt;/h3&gt; &#xA;&lt;p&gt;If we want to use the &lt;code&gt;FetchJoin&lt;/code&gt; api rather than &lt;code&gt;FetchGroupby&lt;/code&gt;, then we also need to upload the join metadata:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run.py --mode metadata-upload --conf production/joins/quickstart/training_set.v2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This makes it so that the online fetcher knows how to take a request for this join and break it up into individual GroupBy requests, returning the unified vector, similar to how the Join backfill produces the wide view table with all features.&lt;/p&gt; &#xA;&lt;h3&gt;Fetching Data&lt;/h3&gt; &#xA;&lt;p&gt;With the above entities defined, you can now easily fetch feature vectors with a simple API call.&lt;/p&gt; &#xA;&lt;p&gt;Fetching a join:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run.py --mode fetch --type join --name quickstart/training_set.v2 -k &#39;{&#34;user_id&#34;:&#34;5&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also fetch a single GroupBy (this would not require the Join metadata upload step performed earlier):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run.py --mode fetch --type group-by --name quickstart/purchases.v1 -k &#39;{&#34;user_id&#34;:&#34;5&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For production, the Java client is usually embedded directly into services.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;Map&amp;lt;String, String&amp;gt; keyMap = new HashMap&amp;lt;&amp;gt;();&#xA;keyMap.put(&#34;user_id&#34;, &#34;123&#34;);&#xA;Fetcher.fetch_join(new Request(&#34;quickstart/training_set_v1&#34;, keyMap))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;sample response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; &#39;{&#34;purchase_price_avg_3d&#34;:14.3241, &#34;purchase_price_avg_14d&#34;:11.89352, ...}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This java code is not runnable in the docker env, it is just an illustrative example.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Log fetches and measure online/offline consistency&lt;/h2&gt; &#xA;&lt;p&gt;As discussed in the introductory sections of this &lt;a href=&#34;https://github.com/airbnb/chronon?tab=readme-ov-file#platform-features&#34;&gt;README&lt;/a&gt;, one of Chronon&#39;s core guarantees is online/offline consistency. This means that the data that you use to train your model (offline) matches the data that the model sees for production inference (online).&lt;/p&gt; &#xA;&lt;p&gt;A key element of this is temporal accuracy. This can be phrased as: &lt;strong&gt;when backfilling features, the value that is produced for any given &lt;code&gt;timestamp&lt;/code&gt; provided by the left side of the join should be the same as what would have been returned online if that feature was fetched at that particular &lt;code&gt;timestamp&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Chronon not only guarantees this temporal accuracy, but also offers a way to measure it.&lt;/p&gt; &#xA;&lt;p&gt;The measurement pipeline starts with the logs of the online fetch requests. These logs include the primary keys and timestamp of the request, along with the fetched feature values. Chronon then passes the keys and timestamps to a Join backfill as the left side, asking the compute engine to backfill the feature values. It then compares the backfilled values to actual fetched values to measure consistency.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: log fetches&lt;/p&gt; &#xA;&lt;p&gt;First, make sure you&#39;ve ran a few fetch requests. Run:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;run.py --mode fetch --type join --name quickstart/training_set.v2 -k &#39;{&#34;user_id&#34;:&#34;5&#34;}&#39;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;A few times to generate some fetches.&lt;/p&gt; &#xA;&lt;p&gt;With that complete, you can run this to create a usable log table (these commands produce a logging hive table with the correct schema):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark-submit --class ai.chronon.quickstart.online.MongoLoggingDumper --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.chronon_log_table mongodb://admin:admin@mongodb:27017/?authSource=admin&#xA;compile.py --conf group_bys/quickstart/schema.py&#xA;run.py --mode backfill --conf production/group_bys/quickstart/schema.v1&#xA;run.py --mode log-flattener --conf production/joins/quickstart/training_set.v2 --log-table default.chronon_log_table --schema-table default.quickstart_schema_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This creates a &lt;code&gt;default.quickstart_training_set_v2_logged&lt;/code&gt; table that contains the results of each of the fetch requests that you previously made, along with the timestamp at which you made them and the &lt;code&gt;user&lt;/code&gt; that you requested.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Once you run the above command, it will create and &#34;close&#34; the log partitions, meaning that if you make additional fetches on the same day (UTC time) it will not append. If you want to go back and generate more requests for online/offline consistency, you can drop the table (run &lt;code&gt;DROP TABLE default.quickstart_training_set_v2_logged&lt;/code&gt; in a &lt;code&gt;spark-sql&lt;/code&gt; shell) before rerunning the above command.&lt;/p&gt; &#xA;&lt;p&gt;Now you can compute consistency metrics with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run.py --mode consistency-metrics-compute --conf production/joins/quickstart/training_set.v2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This job takes will take the primary key(s) and timestamps from the log table (&lt;code&gt;default.quickstart_training_set_v2_logged&lt;/code&gt; in this case), and uses those to create and run a join backfill. It then compares the backfilled results to the actual logged values that were fetched online&lt;/p&gt; &#xA;&lt;p&gt;It produces two output tables:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;default.quickstart_training_set_v2_consistency&lt;/code&gt;: A human readable table that you can query to see the results of the consistency checks. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;You can enter a sql shell by running &lt;code&gt;spark-sql&lt;/code&gt; from your docker bash sesion, then query the table.&lt;/li&gt; &#xA;   &lt;li&gt;Note that it has many columns (multiple metrics per feature), so you might want to run a &lt;code&gt;DESC default.quickstart_training_set_v2_consistency&lt;/code&gt; first, then select a few columns that you care about to query.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;default.quickstart_training_set_v2_consistency_upload&lt;/code&gt;: A list of KV bytes that is uploaded to the online KV store, that can be used to power online data quality monitoring flows. Not meant to be human readable.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Conclusion&lt;/h2&gt; &#xA;&lt;p&gt;Using chronon for your feature engineering work simplifies and improves your ML Workflow in a number of ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can define features in one place, and use those definitions both for training data backfills and for online serving.&lt;/li&gt; &#xA; &lt;li&gt;Backfills are automatically point-in-time correct, which avoids label leakage and inconsistencies between training data and online inference.&lt;/li&gt; &#xA; &lt;li&gt;Orchestration for batch and streaming pipelines to keep features up to date is made simple.&lt;/li&gt; &#xA; &lt;li&gt;Chronon exposes easy endpoints for feature fetching.&lt;/li&gt; &#xA; &lt;li&gt;Consistency is guaranteed and measurable.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For a more detailed view into the benefits of using Chronon, see &lt;a href=&#34;https://github.com/airbnb/chronon/tree/main?tab=readme-ov-file#benefits-of-chronon-over-other-approaches&#34;&gt;Benefits of Chronon documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Benefits of Chronon over other approaches&lt;/h1&gt; &#xA;&lt;p&gt;Chronon offers the most value to AI/ML practitioners who are trying to build &#34;online&#34; models that are serving requests in real-time as opposed to batch workflows.&lt;/p&gt; &#xA;&lt;p&gt;Without Chronon, engineers working on these projects need to figure out how to get data to their models for training/eval as well as production inference. As the complexity of data going into these models increases (multiple sources, complex transformation such as windowed aggregations, etc), so does the infrastructure challenge of supporting this data plumbing.&lt;/p&gt; &#xA;&lt;p&gt;Generally, we observed ML practitioners taking one of two approaches:&lt;/p&gt; &#xA;&lt;h2&gt;The log-and-wait approach&lt;/h2&gt; &#xA;&lt;p&gt;With this approach, users start with the data that is available in the online serving environment from which the model inference will run. Log relevant features to the data warehouse. Once enough data has accumulated, train the model on the logs, and serve with the same data.&lt;/p&gt; &#xA;&lt;p&gt;Pros:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Features used to train the model are guaranteed to be available at serving time&lt;/li&gt; &#xA; &lt;li&gt;The model can access service call features&lt;/li&gt; &#xA; &lt;li&gt;The model can access data from the the request context&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It might take a long to accumulate enough data to train the model&lt;/li&gt; &#xA; &lt;li&gt;Performing windowed aggregations is not always possible (running large range queries against production databases doesn&#39;t scale, same for event streams)&lt;/li&gt; &#xA; &lt;li&gt;Cannot utilize the wealth of data already in the data warehouse&lt;/li&gt; &#xA; &lt;li&gt;Maintaining data transformation logic in the application layer is messy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The replicate offline-online approach&lt;/h2&gt; &#xA;&lt;p&gt;With this approach, users train the model with data from the data warehouse, then figure out ways to replicate those features in the online environment.&lt;/p&gt; &#xA;&lt;p&gt;Pros:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can use a broad set of data for training&lt;/li&gt; &#xA; &lt;li&gt;The data warehouse is well suited for large aggregations and other computationally intensive transformation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Often very error prone, resulting in inconsistent data between training and serving&lt;/li&gt; &#xA; &lt;li&gt;Requires maintaining a lot of complicated infrastructure to even get started with this approach,&lt;/li&gt; &#xA; &lt;li&gt;Serving features with realtime updates gets even more complicated, especially with large windowed aggregations&lt;/li&gt; &#xA; &lt;li&gt;Unlikely to scale well to many models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Chronon approach&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;With Chronon you can use any data available in your organization, including everything in the data warehouse, any streaming source, service calls, etc, with guaranteed consistency between online and offline environments. It abstracts away the infrastructure complexity of orchestrating and maintining this data plumbing, so that users can simply define features in a simple API, and trust Chronon to handle the rest.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome contributions to the Chronon project! Please read &lt;a href=&#34;https://raw.githubusercontent.com/airbnb/chronon/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Use the GitHub issue tracker for reporting bugs or feature requests. Join our &lt;a href=&#34;https://discord.gg/GbmGATNqqP&#34;&gt;community Discord channel&lt;/a&gt; for discussions, tips, and support.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>raptros/gmm-project-2012</title>
    <updated>2024-04-21T03:35:25Z</updated>
    <id>tag:github.com,2024-04-21:/raptros/gmm-project-2012</id>
    <link href="https://github.com/raptros/gmm-project-2012" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Everything related to a project for the Grounded Models Meaning seminar.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gmm-project-2012&lt;/h1&gt; &#xA;&lt;p&gt;Repo for everything related to this project for &lt;a href=&#34;http://gmm-f12.utcompling.com/home&#34; title=&#34;LIN 386M Home&#34;&gt;Learning Grounded Models of Meaning&lt;/a&gt;, a UT Grad seminar, offered in Fall 2012.&lt;/p&gt; &#xA;&lt;h2&gt;Who is doing this?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prateek Maheshwari&lt;/li&gt; &#xA; &lt;li&gt;Aidan Coyne&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What are you doing?&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re thinking about applying label propagation to document geolocation in wikipedia.&lt;/p&gt; &#xA;&lt;h2&gt;LaTeX&lt;/h2&gt; &#xA;&lt;p&gt;All the LaTeX sources for all the various submissions can be found under latex/.&lt;/p&gt; &#xA;&lt;h3&gt;inventory&lt;/h3&gt; &#xA;&lt;p&gt;What&#39;s in there:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;acl2012.bst : styling for bibliographies&lt;/li&gt; &#xA; &lt;li&gt;acl2012.sty : styling for all submissions&lt;/li&gt; &#xA; &lt;li&gt;acl2012.tex : a neat example of a submission, with all formatting stuff. Handy to have easily available.&lt;/li&gt; &#xA; &lt;li&gt;project_ideas.tex : Aidan&#39;s project ideas submission file.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>