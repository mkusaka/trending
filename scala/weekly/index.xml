<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-31T02:21:26Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zio/zio</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/zio/zio</id>
    <link href="https://github.com/zio/zio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZIO — A type-safe, composable library for async and concurrent programming in Scala&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/ZIO.png&#34; alt=&#34;ZIO Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Production%20Ready-brightgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Snapshot Artifacts&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://isitmaintained.com/project/zio/zio&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/zio/zio.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scaladoc&lt;/th&gt; &#xA;   &lt;th&gt;Scaladex&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;   &lt;th&gt;Twitter&lt;/th&gt; &#xA;   &lt;th&gt;Gitpod&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://javadoc.io/doc/dev.zio/zio_2.12/latest/zio/index.html&#34;&gt;Scaladoc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://index.scala-lang.org/zio/zio/zio&#34; title=&#34;Scaladex&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/zio/zio/zio/latest.svg?sanitize=true&#34; alt=&#34;Badge-Scaladex-page&#34; title=&#34;Scaladex&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/zioscala&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/zioscala.svg?style=plastic&amp;amp;label=follow&amp;amp;logo=twitter&#34; alt=&#34;Badge-Twitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/zio/zio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod ready-to-code&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Welcome to ZIO&lt;/h1&gt; &#xA;&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt; &#xA;&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala&#39;s &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it&#39;s asynchronous or synchronous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/contributing.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/pulls&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Adopters&lt;/h1&gt; &#xA;&lt;p&gt;Following is a partial list of companies happily using ZIO in production to craft concurrent applications.&lt;/p&gt; &#xA;&lt;p&gt;Want to see your company here? &lt;a href=&#34;https://github.com/zio/zio/edit/master/README.md&#34;&gt;Submit a PR&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adgear.com/en/&#34;&gt;AdGear / Samsung Ads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adidas.com/&#34;&gt;Adidas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adpulse.io/&#34;&gt;adpulse.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adsquare.com/&#34;&gt;adsquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ayolab.com/&#34;&gt;Ayolab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://asana.com/&#34;&gt;Asana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aurinko.io/&#34;&gt;Aurinko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://auto.ru&#34;&gt;auto.ru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.de&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.avast.com&#34;&gt;Avast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bofa.com&#34;&gt;Bank of America&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bpp.it/&#34;&gt;Bpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://broad.app&#34;&gt;Broad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.caesars.com/sportsbook-and-casino&#34;&gt;Caesars Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.calcbank.com.br&#34;&gt;CalcBank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.callhandling.co.uk/&#34;&gt;Call Handling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;Carvana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cellular.de&#34;&gt;Cellular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudfarms.com&#34;&gt;Cloudfarms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codecomprehension.com&#34;&gt;CodeComprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codept.de/&#34;&gt;Codept&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com/en&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collibra.com/&#34;&gt;Collibra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.compellon.com/&#34;&gt;Compellon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complicatedrobot.com/&#34;&gt;Complicated Robot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conduktor.io&#34;&gt;Conduktor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.contramap.dev&#34;&gt;Contramap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coralogix.com&#34;&gt;Coralogix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creditkarma.com&#34;&gt;Credit Karma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.currencycloud.com/&#34;&gt;CurrencyCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://de-solution.com/&#34;&gt;D.E.Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datachef.co&#34;&gt;DataChef&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.demandbase.com&#34;&gt;Demandbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demyst.com&#34;&gt;Demyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devsisters.com/&#34;&gt;Devsisters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijdhl.nl/it&#34;&gt;DHL Parcel The Netherlands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+ Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doomoolmori.com/&#34;&gt;Doomoolmori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dowjones.com&#34;&gt;Dow Jones&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dpgrecruitment.nl&#34;&gt;DPG recruitment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.com&#34;&gt;Dream11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iot.telekom.com/en&#34;&gt;Deutsche Telekom IoT GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebay.com&#34;&gt;eBay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eaglescience.nl&#34;&gt;Eaglescience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edf.fr/&#34;&gt;Electricité de France (EDF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.enelx.com&#34;&gt;EnelX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evolution.engineering&#34;&gt;Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evo.company&#34;&gt;Evo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flipp.com/&#34;&gt;Flipp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fugo.ai&#34;&gt;Fugo.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.garnercorp.com/&#34;&gt;Garner Distributed Workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gleancompany.com&#34;&gt;Glean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://grandparade.co.uk&#34;&gt;GrandParade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://greyflower.media&#34;&gt;greyflower.media GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hunters.ai&#34;&gt;Hunters.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypefactors.com/&#34;&gt;Hypefactors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.iheart.com/&#34;&gt;iHeartRadio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ihsmarkit.com/&#34;&gt;IHS Markit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://investsuite.com/&#34;&gt;Investsuite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaizen-solutions.net/&#34;&gt;Kaizen Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kamon.io/&#34;&gt;Kamon APM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kodmagi.se&#34;&gt;Kodmagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kensu.io&#34;&gt;Kensu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lambdaworks.io/&#34;&gt;LambdaWorks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leadiq.com&#34;&gt;LeadIQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lernkunst.com/&#34;&gt;Lernkunst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liveintent.com&#34;&gt;LiveIntent Inc.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lottoland.com&#34;&gt;Lottoland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matechs.com&#34;&gt;MATECHS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://megogo.net&#34;&gt;Megogo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mylivn.com/&#34;&gt;Mylivn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newmotion.com&#34;&gt;NewMotion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nexxchange.com&#34;&gt;Nexxchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nike.com&#34;&gt;Nike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nslookup.io&#34;&gt;NsLookup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://olyro.de&#34;&gt;Olyro GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optrak.com&#34;&gt;Optrak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.performance-immo.com/&#34;&gt;Performance Immo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playtika.com&#34;&gt;Playtika&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ppcsamurai.com/&#34;&gt;PPC Samurai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.radix.bio/&#34;&gt;Radix Labs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railroad19.com&#34;&gt;Railroad19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijrandstad.nl&#34;&gt;Randstad Groep Nederland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rapidor.co&#34;&gt;Rapidor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pimsolutions.ru/&#34;&gt;PIM Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rewe-digital.com/&#34;&gt;REWE Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://riskident.com/&#34;&gt;Risk Ident&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocker.com/&#34;&gt;Rocker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rudder.io/&#34;&gt;Rudder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sanjagh.pro/&#34;&gt;Sanjagh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scalac.io/&#34;&gt;Scalac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityscorecard.io/&#34;&gt;SecurityScorecard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sentinelone.com/&#34;&gt;SentinelOne&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.signicat.com/&#34;&gt;Signicat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.sgmarkets.com/en/&#34;&gt;Société Générale Corporate and Investment Banking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaremill.com/&#34;&gt;SoftwareMill&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.streamweaver.com/&#34;&gt;StreamWeaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stuart.com/&#34;&gt;Stuart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teads.com&#34;&gt;Teads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pokemon.com/us/about-pokemon/&#34;&gt;The Pokemon Company International&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tomtom.com&#34;&gt;TomTom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tinka.com/&#34;&gt;Tinka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trackabus.com&#34;&gt;Trackabus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trainor.no&#34;&gt;Trainor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://treutech.io&#34;&gt;TreuTech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tweddle.com&#34;&gt;Tweddle Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.undo.app&#34;&gt;Undo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unit.co&#34;&gt;Unit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univalence.io&#34;&gt;Univalence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.unzer.com&#34;&gt;Unzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vakantiediscounter.nl&#34;&gt;Vakantiediscounter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.verbund.com&#34;&gt;Verbund AG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.waylay.io/&#34;&gt;Waylay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wehkamp.nl&#34;&gt;Wehkamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wolt.com/&#34;&gt;Wolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://o.yandex.ru&#34;&gt;Yandex.Classifieds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://audela.ca&#34;&gt;Audela&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valamis.com&#34;&gt;Valamis Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valsea.com&#34;&gt;Valsea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtuslab.com/&#34;&gt;VirtusLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://getvish.com&#34;&gt;Vish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vivid.money&#34;&gt;Vivid Money&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.com/&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zooz.com/&#34;&gt;Zooz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/ziverge.png&#34; alt=&#34;Ziverge&#34; title=&#34;Ziverge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;Ziverge&lt;/a&gt; is a leading contributor to ZIO.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/scalac.svg?sanitize=true&#34; alt=&#34;Scalac&#34; title=&#34;Scalac&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;Scalac&lt;/a&gt; sponsors ZIO Hackathons and contributes work to multiple projects in ZIO ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/septimal_mind.svg?sanitize=true&#34; alt=&#34;Septimal Mind&#34; title=&#34;Septimal Mind&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/softwaremill.svg?sanitize=true&#34; alt=&#34;SoftwareMill&#34; title=&#34;SoftwareMill&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/yourkit.png&#34; alt=&#34;YourKit&#34; title=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;YourKit&lt;/a&gt; generously provides use of their monitoring and profiling tools to maximize the performance of ZIO applications.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/code_of_conduct.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Come chat with us on &lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Legal&lt;/h3&gt; &#xA;&lt;p&gt;Copyright 2017 - 2020 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>twitter/finagle</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/twitter/finagle</id>
    <link href="https://github.com/twitter/finagle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fault tolerant, protocol-agnostic RPC system&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/twitter/finagle/raw/develop/doc/src/sphinx/_static/logo_medium.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Finagle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/finagle/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/finagle/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/finagle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/finagle/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;https://twitter.github.io/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/finagle/&#34;&gt;https://github.com/twitter/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://finagle.github.io/blog/&#34;&gt;https://finagle.github.io/blog/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols.&lt;/p&gt; &#xA;&lt;p&gt;For extensive documentation, please see the &lt;a href=&#34;https://twitter.github.io/finagle/guide/&#34;&gt;user guide&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/docs/com/twitter/finagle&#34;&gt;API documentation&lt;/a&gt; websites. Documentation improvements are always welcome, so please send patches our way.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The following are a few of the companies that are using Finagle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://foursquare.com/&#34;&gt;Foursquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ing.nl&#34;&gt;ING Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinterest.com/&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://soundcloud.com/&#34;&gt;SoundCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tumblr.com/&#34;&gt;Tumblr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a more complete list, please see &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/ADOPTERS.md&#34;&gt;our adopter page&lt;/a&gt;. If your organization is using Finagle, consider adding a link there and sending us a pull request!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2010 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark™&lt;/a&gt;: This connector allows Apache Spark™ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table’s metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lampepfl/dotty</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/lampepfl/dotty</id>
    <link href="https://github.com/lampepfl/dotty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/lampepfl/dotty/workflows/Dotty/badge.svg?branch=master&#34; alt=&#34;Dotty CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632150470000902164&#34; alt=&#34;Join the chat at https://discord.com/invite/scala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Try it out&lt;/h1&gt; &#xA;&lt;p&gt;To try it in your project see also the &lt;a href=&#34;https://docs.scala-lang.org/scala3/getting-started.html&#34;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt dist/packArchive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Dotty uses the &lt;a href=&#34;https://www.scala-lang.org/conduct.html&#34;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; &#xA;&lt;h1&gt;How to Contribute&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&#34;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Dotty is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>flix/flix</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/flix/flix</id>
    <link href="https://github.com/flix/flix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Flix Programming Language&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/logo.png&#34; height=&#34;91px&#34; alt=&#34;The Flix Programming Language&#34; title=&#34;The Flix Programming Language&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flix&lt;/strong&gt; is a statically typed functional, imperative, and logic programming language.&lt;/p&gt; &#xA;&lt;p&gt;We refer you to the &lt;a href=&#34;https://flix.dev/&#34;&gt;official Flix website (flix.dev)&lt;/a&gt; for more information about Flix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/flix/Lobby&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/example.png&#34; height=&#34;627px&#34; alt=&#34;Example Flix Program&#34; title=&#34;Example Flix Program&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/BUILD.md&#34;&gt;docs/BUILD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Flix is available under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We kindly thank &lt;a href=&#34;https://www.ej-technologies.com/&#34;&gt;EJ Technologies&lt;/a&gt; for providing us with &lt;a href=&#34;http://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler&lt;/a&gt; and &lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt; for providing us with &lt;a href=&#34;https://www.jetbrains.com/idea/&#34;&gt;IntelliJ IDEA&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/openwhisk</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/apache/openwhisk</id>
    <link href="https://github.com/apache/openwhisk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenWhisk is an open source serverless cloud platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenWhisk&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/github/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://travis-ci.com/apache/openwhisk.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openwhisk-team.slack.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-slack-9B69A0.svg?sanitize=true&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/openwhisk/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=openwhisk&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/openwhisk.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. Learn more at &lt;a href=&#34;http://openwhisk.apache.org&#34;&gt;http://openwhisk.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; (Deploy and Use OpenWhisk on your machine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#deploy-to-kubernetes&#34;&gt;Deploy to Kubernetes&lt;/a&gt; (For development and production)&lt;/li&gt; &#xA; &lt;li&gt;For project contributors and Docker deployments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/macos/README.md&#34;&gt;Deploy to Docker for Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/ubuntu-setup/README.md&#34;&gt;Deploy to Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#learn-concepts-and-commands&#34;&gt;Learn Concepts and Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#openwhisk-community-and-support&#34;&gt;OpenWhisk Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#project-repository-structure&#34;&gt;Project Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start using OpenWhisk is to install the &#34;Standalone&#34; OpenWhisk stack. This is a full-featured OpenWhisk stack running as a Java process for convenience. Serverless functions run within Docker containers. You will need &lt;a href=&#34;https://docs.docker.com/install&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://java.com/en/download/help/download_options.xml&#34;&gt;Java&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; available on your machine.&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk.git&#xA;cd openwhisk&#xA;./gradlew core:standalone:bootRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When the OpenWhisk stack is up, it will open your browser to a functions &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/playground-ui.png&#34;&gt;Playground&lt;/a&gt;, typically served from &lt;a href=&#34;http://localhost:3232&#34;&gt;http://localhost:3232&lt;/a&gt;. The Playground allows you create and run functions directly from your browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called &lt;code&gt;wsk&lt;/code&gt; which you can download from &lt;a href=&#34;https://s.apache.org/openwhisk-cli-download&#34;&gt;https://s.apache.org/openwhisk-cli-download&lt;/a&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/cli.md&#34;&gt;CLI configuration&lt;/a&gt; for additional details. Typically you configure the CLI for Standalone OpenWhisk as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;wsk property set \&#xA;  --apihost &#39;http://localhost:3233&#39; \&#xA;  --auth &#39;23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone OpenWhisk can be configured to deploy additional capabilities when that is desirable. Additional resources are available &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/core/standalone/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deploy to Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;OpenWhisk can also be installed on a Kubernetes cluster. You can use a managed Kubernetes cluster provisioned from a public cloud provider (e.g., AKS, EKS, IKS, GKE), or a cluster you manage yourself. Additionally for local development, OpenWhisk is compatible with Minikube, and Kubernetes for Mac using the support built into Docker 18.06 (or higher).&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk-deploy-kube.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the instructions in the &lt;a href=&#34;https://github.com/apache/openwhisk-deploy-kube/raw/master/README.md&#34;&gt;OpenWhisk on Kubernetes README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn Concepts and Commands&lt;/h3&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/&#34;&gt;documentation&lt;/a&gt; to learn more. Here are some topics you may be interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/about.md&#34;&gt;System overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/README.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions.md&#34;&gt;Create and invoke actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/triggers_rules.md&#34;&gt;Create triggers and rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/packages.md&#34;&gt;Use and create packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/catalog.md&#34;&gt;Browse and use the catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/reference.md&#34;&gt;OpenWhisk system details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/feeds.md&#34;&gt;Implementing feeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions-actionloop.md&#34;&gt;Developing a runtime for a new language&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenWhisk Community and Support&lt;/h3&gt; &#xA;&lt;p&gt;Report bugs, ask questions and request features &lt;a href=&#34;https://raw.githubusercontent.com/apache/issues&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join the OpenWhisk Team on Slack &lt;a href=&#34;https://openwhisk-team.slack.com&#34;&gt;https://openwhisk-team.slack.com&lt;/a&gt; and chat with developers. To get access to our public Slack team, request an invite &lt;a href=&#34;https://openwhisk.apache.org/slack.html&#34;&gt;https://openwhisk.apache.org/slack.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Project Repository Structure&lt;/h3&gt; &#xA;&lt;p&gt;The OpenWhisk system is built from a &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/dev/modules.md&#34;&gt;number of components&lt;/a&gt;. The picture below groups the components by their GitHub repos. Please open issues for a component against the appropriate repo (if in doubt just open against the main openwhisk repo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/components_to_repos.png&#34; alt=&#34;component/repo mapping&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-kyuubi</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/apache/incubator-kyuubi</id>
    <link href="https://github.com/apache/incubator-kyuubi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Kyuubi is a distributed multi-tenant JDBC server for large-scale data processing and analytics, built on top of Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Kyuubi (Incubating)&lt;/h1&gt; &#xA;&lt;img src=&#34;https://svn.apache.org/repos/asf/comdev/project-logos/originals/kyuubi-1.svg?sanitize=true&#34; alt=&#34;Kyuubi logo&#34; height=&#34;120px&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/apache/incubator-kyuubi?label=release&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://tokei.rs/b1/github.com/apache/incubator-kyuubi&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/incubator-kyuubi/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/apache/incubator-kyuubi/Kyuubi/master?style=plastic&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;a href=&#34;https://travis-ci.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/incubator-kyuubi.svg?branch=master&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/kyuubi/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/apache/incubator-kyuubi&#34; alt=&#34;GitHub top language&#34;&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/apache/incubator-kyuubi&#34; alt=&#34;Commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Kyuubi?&lt;/h2&gt; &#xA;&lt;p&gt;Kyuubi is a distributed multi-tenant Thrift JDBC/ODBC server for large-scale data management, processing, and analytics, built on top of Apache Spark and designed to support more engines (i.e., Flink). It has been open-sourced by NetEase since 2018. We are aiming to make Kyuubi an &#34;out-of-the-box&#34; tool for data warehouses and data lakes.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi provides a pure SQL gateway through Thrift JDBC/ODBC interface for end-users to manipulate large-scale data with pre-programmed and extensible Spark SQL engines. This &#34;out-of-the-box&#34; model minimizes the barriers and costs for end-users to use Spark at the client side. At the server-side, Kyuubi server and engines&#39; multi-tenant architecture provides the administrators a way to achieve computing resource isolation, data security, high availability, high client concurrency, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_positioning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A HiveServer2-like API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-tenant Spark Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Running Spark in a serverless way&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target Users&lt;/h3&gt; &#xA;&lt;p&gt;Kyuubi&#39;s goal is to make it easy and efficient for &lt;code&gt;anyone&lt;/code&gt; to use Spark(maybe other engines soon) and facilitate users to handle big data like ordinary data. Here, &lt;code&gt;anyone&lt;/code&gt; means that users do not need to have a Spark technical background but a human language, SQL only. Sometimes, SQL skills are unnecessary when integrating Kyuubi with Apache Superset, which supports rich visualizations and dashboards.&lt;/p&gt; &#xA;&lt;p&gt;In typical big data production environments with Kyuubi, there should be system administrators and end-users.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System administrators: A small group consists of Spark experts responsible for Kyuubi deployment, configuration, and tuning.&lt;/li&gt; &#xA; &lt;li&gt;End-users: Focus on business data of their own, not where it stores, how it computes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, the Kyuubi community will continuously optimize the whole system with various features, such as History-Based Optimizer, Auto-tuning, Materialized View, SQL Dialects, Functions, e.t.c.&lt;/p&gt; &#xA;&lt;h3&gt;Usage scenarios&lt;/h3&gt; &#xA;&lt;h4&gt;Port workloads from HiveServer2 to Spark SQL&lt;/h4&gt; &#xA;&lt;p&gt;In typical big data production environments, especially secured ones, all bundled services manage access control lists to restricting access to authorized users. For example, Hadoop YARN divides compute resources into queues. With Queue ACLs, it can identify and control which users/groups can take actions on particular queues. Similarly, HDFS ACLs control access of HDFS files by providing a way to set different permissions for specific users/groups.&lt;/p&gt; &#xA;&lt;p&gt;Apache Spark is a unified analytics engine for large-scale data processing. It provides a Distributed SQL Engine, a.k.a, the Spark Thrift Server(STS), designed to be seamlessly compatible with HiveServer2 and get even better performance.&lt;/p&gt; &#xA;&lt;p&gt;HiveServer2 can identify and authenticate a caller, and then if the caller also has permissions for the YARN queue and HDFS files, it succeeds. Otherwise, it fails. However, on the one hand, STS is a single Spark application. The user and queue to which STS belongs are uniquely determined at startup. Consequently, STS cannot leverage cluster managers such as YARN and Kubernetes for resource isolation and sharing or control the access for callers by the single user inside the whole system. On the other hand, the Thrift Server is coupled in the Spark driver&#39;s JVM process. This coupled architect puts a high risk on server stability and makes it unable to handle high client concurrency or apply high availability such as load balancing as it is stateful.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi extends the use of STS in a multi-tenant model based on a unified interface and relies on the concept of multi-tenancy to interact with cluster managers to finally gain the ability of resources sharing/isolation and data security. The loosely coupled architecture of the Kyuubi server and engine dramatically improves the client concurrency and service stability of the service itself.&lt;/p&gt; &#xA;&lt;h4&gt;DataLake/LakeHouse Support&lt;/h4&gt; &#xA;&lt;p&gt;The vision of Kyuubi is to unify the portal and become an easy-to-use data lake management platform. Different kinds of workloads, such as ETL processing and BI analytics, can be supported by one platform, using one copy of data, with one SQL interface.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logical View support via Kyuubi DataLake Metadata APIs&lt;/li&gt; &#xA; &lt;li&gt;Multiple Catalogs support&lt;/li&gt; &#xA; &lt;li&gt;SQL Standard Authorization support for DataLake(coming)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Cloud Native Support&lt;/h4&gt; &#xA;&lt;p&gt;Kyuubi can deploy its engines on different kinds of Cluster Managers, such as, Hadoop YARN, Kubernetes, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_migrating_yarn_to_k8s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The Kyuubi Ecosystem(present and future)&lt;/h3&gt; &#xA;&lt;p&gt;The figure below shows our vision for the Kyuubi Ecosystem. Some of them have been realized, some in development, and others would not be possible without your help.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_ecosystem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Since Kyuubi 1.3.0-incubating, the Kyuubi online documentation is hosted by &lt;a href=&#34;https://kyuubi.apache.org/&#34;&gt;https://kyuubi.apache.org/&lt;/a&gt;. You can find the latest Kyuubi documentation on &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;this web page&lt;/a&gt;. For 1.2 and earlier versions, please check the &lt;a href=&#34;https://kyuubi.readthedocs.io/en/v1.2.0/&#34;&gt;Readthedocs&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ready? &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/quick_start/quick_start.html&#34;&gt;Getting Started&lt;/a&gt; with Kyuubi.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Contributor over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://api7.ai/contributor-graph?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://contributor-graph-api.apiseven.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34; alt=&#34;Contributor over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Aside&lt;/h2&gt; &#xA;&lt;p&gt;The project took its name from a character of a popular Japanese manga - &lt;code&gt;Naruto&lt;/code&gt;. The character is named &lt;code&gt;Kyuubi Kitsune/Kurama&lt;/code&gt;, which is a nine-tailed fox in mythology. &lt;code&gt;Kyuubi&lt;/code&gt; spread the power and spirit of fire, which is used here to represent the powerful &lt;a href=&#34;http://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;. Its nine tails stand for end-to-end multi-tenancy support of this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JohnSnowLabs/spark-nlp</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/JohnSnowLabs/spark-nlp</id>
    <link href="https://github.com/JohnSnowLabs/spark-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State of the Art Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark NLP: State of the Art Natural Language Processing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/actions&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/JohnSnowLabs/spark-nlp/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34; alt=&#34;Current Release Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JohnSnowLabs/spark-nlp.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.12&#34; alt=&#34;Maven Central&#34;&gt; &lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.12/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/spark-nlp&#34; alt=&#34;PyPI version&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/spark-nlp.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34; alt=&#34;Anaconda-Cloud&#34;&gt; &lt;img src=&#34;https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/LICENSE&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34; alt=&#34;PyPi downloads&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/spark-nlp?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;performant&lt;/strong&gt; &amp;amp; &lt;strong&gt;accurate&lt;/strong&gt; NLP annotations for machine learning pipelines that &lt;strong&gt;scale&lt;/strong&gt; easily in a distributed environment. Spark NLP comes with &lt;strong&gt;4000+&lt;/strong&gt; pretrained &lt;strong&gt;pipelines&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt; in more than &lt;strong&gt;200+&lt;/strong&gt; languages. It also offers tasks such as &lt;strong&gt;Tokenization&lt;/strong&gt;, &lt;strong&gt;Word Segmentation&lt;/strong&gt;, &lt;strong&gt;Part-of-Speech Tagging&lt;/strong&gt;, Word and Sentence &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Named Entity Recognition&lt;/strong&gt;, &lt;strong&gt;Dependency Parsing&lt;/strong&gt;, &lt;strong&gt;Spell Checking&lt;/strong&gt;, &lt;strong&gt;Text Classification&lt;/strong&gt;, &lt;strong&gt;Sentiment Analysis&lt;/strong&gt;, &lt;strong&gt;Token Classification&lt;/strong&gt;, &lt;strong&gt;Machine Translation&lt;/strong&gt; (+180 languages), &lt;strong&gt;Summarization&lt;/strong&gt; &amp;amp; &lt;strong&gt;Question Answering&lt;/strong&gt;, &lt;strong&gt;Text Generation&lt;/strong&gt;, and many more &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;NLP tasks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark NLP&lt;/strong&gt; is the only open-source NLP library in &lt;strong&gt;production&lt;/strong&gt; that offers state-of-the-art transformers such as &lt;strong&gt;BERT&lt;/strong&gt;, &lt;strong&gt;ALBERT&lt;/strong&gt;, &lt;strong&gt;ELECTRA&lt;/strong&gt;, &lt;strong&gt;XLNet&lt;/strong&gt;, &lt;strong&gt;DistilBERT&lt;/strong&gt;, &lt;strong&gt;RoBERTa&lt;/strong&gt;, &lt;strong&gt;DeBERTa&lt;/strong&gt;, &lt;strong&gt;XLM-RoBERTa&lt;/strong&gt;, &lt;strong&gt;Longformer&lt;/strong&gt;, &lt;strong&gt;ELMO&lt;/strong&gt;, &lt;strong&gt;Universal Sentence Encoder&lt;/strong&gt;, &lt;strong&gt;Google T5&lt;/strong&gt;, &lt;strong&gt;MarianMT&lt;/strong&gt;, and &lt;strong&gt;GPT2&lt;/strong&gt; not only to &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt;, but also to &lt;strong&gt;JVM&lt;/strong&gt; ecosystem (&lt;strong&gt;Java&lt;/strong&gt;, &lt;strong&gt;Scala&lt;/strong&gt;, and &lt;strong&gt;Kotlin&lt;/strong&gt;) at &lt;strong&gt;scale&lt;/strong&gt; by extending &lt;strong&gt;Apache Spark&lt;/strong&gt; natively.&lt;/p&gt; &#xA;&lt;h2&gt;Project&#39;s website&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href=&#34;http://nlp.johnsnowlabs.com/&#34;&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt; &#xA;&lt;h2&gt;Community support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.johnsnowlabs.com/slack-redirect/&#34;&gt;Slack&lt;/a&gt; For live discussion with the Spark NLP community and the team&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp&#34;&gt;GitHub&lt;/a&gt; Bug reports, feature requests, and contributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/discussions&#34;&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/spark-nlp&#34;&gt;Medium&lt;/a&gt; Spark NLP articles&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos&#34;&gt;YouTube&lt;/a&gt; Spark NLP video tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-spark-support&#34;&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala-and-python-support&#34;&gt;Scala &amp;amp; Python Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-support&#34;&gt;Databricks Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-support&#34;&gt;EMR Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#usage&#34;&gt;Using Spark NLP&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-packages&#34;&gt;Spark Packages&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala&#34;&gt;Scala&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#maven&#34;&gt;Maven&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python&#34;&gt;Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipconda&#34;&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-zeppelin&#34;&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#jupyter-notebook-python&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#google-colab-notebook&#34;&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#kaggle-kernel&#34;&gt;Kaggle Kernel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-cluster&#34;&gt;Databricks Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-cluster&#34;&gt;EMR Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#gcp-dataproc&#34;&gt;GCP Dataproc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-nlp-configuration&#34;&gt;Spark NLP Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines-and-models&#34;&gt;Pipelines &amp;amp; Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#offline&#34;&gt;Offline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tokenization&lt;/li&gt; &#xA; &lt;li&gt;Trainable Word Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Stop Words Removal&lt;/li&gt; &#xA; &lt;li&gt;Token Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Document Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Stemmer&lt;/li&gt; &#xA; &lt;li&gt;Lemmatizer&lt;/li&gt; &#xA; &lt;li&gt;NGrams&lt;/li&gt; &#xA; &lt;li&gt;Regex Matching&lt;/li&gt; &#xA; &lt;li&gt;Text Matching&lt;/li&gt; &#xA; &lt;li&gt;Chunking&lt;/li&gt; &#xA; &lt;li&gt;Date Matcher&lt;/li&gt; &#xA; &lt;li&gt;Sentence Detector&lt;/li&gt; &#xA; &lt;li&gt;Deep Sentence Detector (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Dependency parsing (Labeled/unlabeled)&lt;/li&gt; &#xA; &lt;li&gt;Part-of-speech tagging&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Detection (ML models)&lt;/li&gt; &#xA; &lt;li&gt;Spell Checker (ML and DL models)&lt;/li&gt; &#xA; &lt;li&gt;Word Embeddings (GloVe and Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;Doc2Vec (based on Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;BERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;CamemBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa Embeddings (HuggingFace v2 &amp;amp; v3 models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Longformer Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;ALBERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLNet Embeddings&lt;/li&gt; &#xA; &lt;li&gt;ELMO Embeddings (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;Universal Sentence Encoder (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;BERT Sentence Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Sentence Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Chunk Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised keywords extraction&lt;/li&gt; &#xA; &lt;li&gt;Language Detection &amp;amp; Identification (up to 375 languages)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-label Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Text Classification (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;BERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;ALBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLNet for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Longformer for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation (MarianMT)&lt;/li&gt; &#xA; &lt;li&gt;Text-To-Text Transfer Transformer (Google T5)&lt;/li&gt; &#xA; &lt;li&gt;Generative Pre-trained Transformer 2 (OpenAI GPT2)&lt;/li&gt; &#xA; &lt;li&gt;Named entity recognition (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Easy TensorFlow integration&lt;/li&gt; &#xA; &lt;li&gt;GPU Support&lt;/li&gt; &#xA; &lt;li&gt;Full integration with Spark ML functions&lt;/li&gt; &#xA; &lt;li&gt;+3200 pre-trained models in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;+1700 pre-trained pipelines in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To use Spark NLP you need the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8 and 11&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3.2.x, 3.1.x, 3.0.x, 2.4.x, or 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Java 11 is only supported if you are using Spark NLP with Spark/PySpark 3.x and above&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU (optional):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA11 and cuDNN 8.0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ java -version&#xA;# should be Java 8 (Oracle or OpenJDK)&#xA;$ conda create -n sparknlp python=3.7 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python console or Jupyter &lt;code&gt;Python3&lt;/code&gt; kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Spark NLP&#xA;from sparknlp.base import *&#xA;from sparknlp.annotator import *&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;import sparknlp&#xA;&#xA;# Start SparkSession with Spark NLP&#xA;# start() functions has 5 parameters: gpu, spark23, spark24, spark32, and memory&#xA;# sparknlp.start(gpu=True) will start the session with GPU support&#xA;# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed&#xA;# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed&#xA;# sparknlp.start(spark32=True) is when you have Apache Spark 3.2.x installed&#xA;# sparknlp.start(memory=&#34;16G&#34;) to change the default driver memory in SparkSession&#xA;spark = sparknlp.start()&#xA;&#xA;# Download a pre-trained pipeline&#xA;pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;&#xA;# Your testing dataset&#xA;text = &#34;&#34;&#34;&#xA;The Mona Lisa is a 16th century oil painting created by Leonardo.&#xA;It&#39;s held at the Louvre in Paris.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Annotate your testing dataset&#xA;result = pipeline.annotate(text)&#xA;&#xA;# What&#39;s in the pipeline&#xA;list(result.keys())&#xA;Output: [&#39;entities&#39;, &#39;stem&#39;, &#39;checked&#39;, &#39;lemma&#39;, &#39;document&#39;,&#xA;&#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;]&#xA;&#xA;# Check the results&#xA;result[&#39;entities&#39;]&#xA;Output: [&#39;Mona Lisa&#39;, &#39;Leonardo&#39;, &#39;Louvre&#39;, &#39;Paris&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, you can visit our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;repository&lt;/a&gt; to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP &lt;em&gt;3.4.4&lt;/em&gt; has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x, 2.4.x, 3.0.x, 3.1.x, and 3.2.x:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.3.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.4.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.0.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.1.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.2.x&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting 3.0.0 release, the default &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;spark-nlp-gpu&lt;/code&gt; pacakges are based on Scala 2.12 and Apache Spark 3.x by default.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Scala and Python Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.6&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.8&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.9&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Databricks Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following runtimes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5.5 LTS&lt;/li&gt; &#xA; &lt;li&gt;5.5 LTS ML&lt;/li&gt; &#xA; &lt;li&gt;6.4&lt;/li&gt; &#xA; &lt;li&gt;6.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.3&lt;/li&gt; &#xA; &lt;li&gt;7.3 ML&lt;/li&gt; &#xA; &lt;li&gt;7.4&lt;/li&gt; &#xA; &lt;li&gt;7.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.5&lt;/li&gt; &#xA; &lt;li&gt;7.5 ML&lt;/li&gt; &#xA; &lt;li&gt;7.6&lt;/li&gt; &#xA; &lt;li&gt;7.6 ML&lt;/li&gt; &#xA; &lt;li&gt;8.0&lt;/li&gt; &#xA; &lt;li&gt;8.0 ML&lt;/li&gt; &#xA; &lt;li&gt;8.1&lt;/li&gt; &#xA; &lt;li&gt;8.1 ML&lt;/li&gt; &#xA; &lt;li&gt;8.2&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML&lt;/li&gt; &#xA; &lt;li&gt;8.3&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML&lt;/li&gt; &#xA; &lt;li&gt;8.4&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML&lt;/li&gt; &#xA; &lt;li&gt;9.0&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML&lt;/li&gt; &#xA; &lt;li&gt;9.1&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.0&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML&lt;/li&gt; &#xA; &lt;li&gt;10.1&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.2&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML&lt;/li&gt; &#xA; &lt;li&gt;10.3&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML &amp;amp; GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark NLP 3.4.4 is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 8.x and above as listed under GPU.&lt;/p&gt; &#xA;&lt;h2&gt;EMR Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following EMR releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emr-5.20.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.22.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.23.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.25.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.26.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.27.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.28.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.29.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.31.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.32.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.34.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.1.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.2.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.1&lt;/li&gt; &#xA; &lt;li&gt;emr-6.4.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.5.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html&#34;&gt;Amazon EMR 5.x releases&lt;/a&gt; Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html&#34;&gt;Amazon EMR 6.x releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.4.4&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Packages Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;This is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Apache Spark&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on CPU&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on GPU&lt;/th&gt; &#xA;   &lt;th&gt;Start()&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x/3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark32=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark24=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark23=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Spark Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Command line (requires internet connection)&lt;/h3&gt; &#xA;&lt;p&gt;Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, Apache Spark 3.1.x, and Apache Spark 3.2.x. That&#39;s being said, you need to choose the right package name for the right Apache Spark major release:&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.2.x (Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.4.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.3.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x, 3.1.x, and 3.2.x versions. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Maven&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark24_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maven Central: &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects &lt;a href=&#34;https://github.com/maziyarpanahi/spark-nlp-starter&#34;&gt;Spark NLP SBT Starter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Python 3.6.x and above depending on your major PySpark version.&lt;/p&gt; &#xA;&lt;h3&gt;Python without explicit Pyspark installation&lt;/h3&gt; &#xA;&lt;h3&gt;Pip/Conda&lt;/h3&gt; &#xA;&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt; &#xA;&lt;p&gt;Pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34;&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34;&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you&#39;ll have to create a SparkSession either from Spark NLP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;&#xA;spark = sparknlp.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[4]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for comma-delimited jar files. For cluster setups, of course, you&#39;ll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;&#xA;#create or get Spark Session&#xA;&#xA;spark = sparknlp.start()&#xA;&#xA;sparknlp.version()&#xA;spark.version&#xA;&#xA;#download, load and annotate a text by pre-trained pipeline&#xA;&#xA;pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;)&#xA;result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiled JARs&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;h4&gt;spark-nlp&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the jar manually&lt;/h3&gt; &#xA;&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;Maven Central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell --jars spark-nlp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Zeppelin&lt;/h2&gt; &#xA;&lt;p&gt;Use either one of the following options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the following Maven Coordinates to the interpreter&#39;s library list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a path to pre-built jar from &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;here&lt;/a&gt; in the interpreter&#39;s library list making sure the jar is available to driver path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Python in Zeppelin&lt;/h3&gt; &#xA;&lt;p&gt;Apart from the previous step, install the python module through pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt; &#xA;&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier since it includes both scala and python side installation.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook (Python)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recomended:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get this done on Linux and macOS is to simply install &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;pyspark&lt;/code&gt; PyPI packages and launch the Jupyter from the same Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda create -n sparknlp python=3.8 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2 jupyter&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The you can use &lt;code&gt;python3&lt;/code&gt; kernel to run your code with creating SparkSession via &lt;code&gt;spark = sparknlp.start()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_HOME=/path/to/your/spark/folder&#xA;export PYSPARK_PYTHON=python3&#xA;export PYSPARK_DRIVER_PYTHON=jupyter&#xA;export PYSPARK_DRIVER_PYTHON_OPTS=notebook&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If not using pyspark at all, you&#39;ll have to run the instructions pointed &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python-without-explicit-Pyspark-installation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# This is only to setup PySpark and Spark NLP on Colab&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script comes with the two options to define &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-nlp&lt;/code&gt; versions via options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# -p is for pyspark&#xA;# -s is for spark-nlp&#xA;# by default they are set to the latest&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.1.2 -s 3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb&#34;&gt;Spark NLP quick start on Google Colab&lt;/a&gt; is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Kaggle Kernel&lt;/h2&gt; &#xA;&lt;p&gt;Run the following code in Kaggle Kernel and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Let&#39;s setup Kaggle for Spark NLP and PySpark&#xA;!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mozzie/spark-nlp-named-entity-recognition&#34;&gt;Spark NLP quick start on Kaggle Kernel&lt;/a&gt; is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Databricks Cluster&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a cluster if you don&#39;t have one already&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;Libraries&lt;/code&gt; tab inside your cluster you need to follow these steps:&lt;/p&gt; &lt;p&gt;3.1. Install New -&amp;gt; PyPI -&amp;gt; &lt;code&gt;spark-nlp==3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;p&gt;3.2. Install New -&amp;gt; Maven -&amp;gt; Coordinates -&amp;gt; &lt;code&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can attach your notebook to the cluster and use Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: Databrick&#39;s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name for your runtime from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EMR Cluster&lt;/h2&gt; &#xA;&lt;p&gt;To launch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.&lt;/p&gt; &#xA;&lt;p&gt;A sample of your bootstrap script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;#!/bin/bash&#xA;set -x -e&#xA;&#xA;echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3&#xA;export HADOOP_CONF_DIR=/etc/hadoop/conf&#xA;export SPARK_JARS_DIR=/usr/lib/spark/jars&#xA;export SPARK_HOME=/usr/lib/spark&#39; &amp;gt;&amp;gt; $HOME/.bashrc &amp;amp;&amp;amp; source $HOME/.bashrc&#xA;&#xA;sudo python3 -m pip install awscli boto spark-nlp&#xA;&#xA;set +x&#xA;exit 0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of your software configuration in JSON on S3 (must be public access):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.json&#34;&gt;[{&#xA;  &#34;Classification&#34;: &#34;spark-env&#34;,&#xA;  &#34;Configurations&#34;: [{&#xA;    &#34;Classification&#34;: &#34;export&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;PYSPARK_PYTHON&#34;: &#34;/usr/bin/python3&#34;&#xA;    }&#xA;  }]&#xA;},&#xA;{&#xA;  &#34;Classification&#34;: &#34;spark-defaults&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;spark.yarn.stagingDir&#34;: &#34;hdfs:///tmp&#34;,&#xA;      &#34;spark.yarn.preserve.staging.files&#34;: &#34;true&#34;,&#xA;      &#34;spark.kryoserializer.buffer.max&#34;: &#34;2000M&#34;,&#xA;      &#34;spark.serializer&#34;: &#34;org.apache.spark.serializer.KryoSerializer&#34;,&#xA;      &#34;spark.driver.maxResultSize&#34;: &#34;0&#34;,&#xA;      &#34;spark.jars.packages&#34;: &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of AWS CLI to launch EMR cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;aws emr create-cluster \&#xA;--name &#34;Spark NLP 3.4.4&#34; \&#xA;--release-label emr-6.2.0 \&#xA;--applications Name=Hadoop Name=Spark Name=Hive \&#xA;--instance-type m4.4xlarge \&#xA;--instance-count 3 \&#xA;--use-default-roles \&#xA;--log-uri &#34;s3://&amp;lt;S3_BUCKET&amp;gt;/&#34; \&#xA;--bootstrap-actions Path=s3://&amp;lt;S3_BUCKET&amp;gt;/emr-bootstrap.sh,Name=custome \&#xA;--configurations &#34;https://&amp;lt;public_access&amp;gt;/sparknlp-config.json&#34; \&#xA;--ec2-attributes KeyName=&amp;lt;your_ssh_key&amp;gt;,EmrManagedMasterSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt;,EmrManagedSlaveSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt; \&#xA;--profile &amp;lt;aws_profile_credentials&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP Dataproc&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a cluster if you don&#39;t have one already as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At gcloud shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud services enable dataproc.googleapis.com \&#xA;  compute.googleapis.com \&#xA;  storage-component.googleapis.com \&#xA;  bigquery.googleapis.com \&#xA;  bigquerystorage.googleapis.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;ZONE=&amp;lt;zone&amp;gt;&#xA;CLUSTER_NAME=&amp;lt;cluster_name&amp;gt;&#xA;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don&#39;t forget to set the maven coordinates for the jar in properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud dataproc clusters create ${CLUSTER_NAME} \&#xA;  --region=${REGION} \&#xA;  --zone=${ZONE} \&#xA;  --image-version=2.0 \&#xA;  --master-machine-type=n1-standard-4 \&#xA;  --worker-machine-type=n1-standard-2 \&#xA;  --master-boot-disk-size=128GB \&#xA;  --worker-boot-disk-size=128GB \&#xA;  --num-workers=2 \&#xA;  --bucket=${BUCKET_NAME} \&#xA;  --optional-components=JUPYTER \&#xA;  --enable-component-gateway \&#xA;  --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; \&#xA;  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh \&#xA;  --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can attach your notebook to the cluster and use the Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Spark NLP Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can change the following Spark NLP configurations via Spark Configuration:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Property Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.pretrained.cache_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/cache_pretrained&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to download and exctract pretrained &lt;code&gt;Models&lt;/code&gt; and &lt;code&gt;Pipelines&lt;/code&gt;. By default, it will be in User&#39;s Home directory under &lt;code&gt;cache_pretrained&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.storage.cluster_tmp_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of &lt;code&gt;hadoop.tmp.dir&lt;/code&gt; set via Hadoop configuration for Apache Spark. NOTE: &lt;code&gt;S3&lt;/code&gt; is not supported and it must be local, HDFS, or DBFS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.annotator.log_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/annotator_logs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to save logs from annotators during training such as &lt;code&gt;NerDLApproach&lt;/code&gt;, &lt;code&gt;ClassifierDLApproach&lt;/code&gt;, &lt;code&gt;SentimentDLApproach&lt;/code&gt;, &lt;code&gt;MultiClassifierDLApproach&lt;/code&gt;, etc. By default, it will be in User&#39;s Home directory under &lt;code&gt;annotator_logs&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.access_key_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.secret_access_key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.session_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.s3_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.region&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How to set Spark NLP Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparkSession:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.config()&lt;/code&gt; during SparkSession creation to set Spark NLP configurations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;        .master(&#34;local[*]&#34;) \&#xA;        .config(&#34;spark.driver.memory&#34;, &#34;16G&#34;) \&#xA;        .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;        .config(&#34;spark.serializer&#34;, &#34;org.apache.spark.serializer.KryoSerializer&#34;) \&#xA;        .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000m&#34;) \&#xA;        .config(&#34;spark.jsl.settings.pretrained.cache_folder&#34;, &#34;sample_data/pretrained&#34;) \&#xA;        .config(&#34;spark.jsl.settings.storage.cluster_tmp_dir&#34;, &#34;sample_data/storage&#34;) \&#xA;        .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;) \&#xA;        .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-shell:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyspark:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pyspark \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE&#xA;spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE&#xA;spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it.&lt;/p&gt; &#xA;&lt;h3&gt;S3 Integration&lt;/h3&gt; &#xA;&lt;p&gt;In Spark NLP we can define S3 locations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Export log files of training models&lt;/li&gt; &#xA; &lt;li&gt;Store tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.annotator.log_folder&#34;, &#34;s3://my/s3/path/logs&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.s3_bucket&#34;, &#34;my.bucket&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can check the log on your S3 path defined in &lt;em&gt;spark.jsl.settings.annotator.log_folder&lt;/em&gt; property. Make sure to use the prefix &lt;em&gt;s3://&lt;/em&gt;, otherwise it will use the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensorflow Graphs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reference S3 location for downloading graphs. We need to set up AWS credentials&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;MFA Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.session_token&#34;, &#34;MY_TOKEN&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of a bash script that gets temporal AWS credentials can be found &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/scripts/aws_tmp_credentials.sh&#34;&gt;here&lt;/a&gt; This script requires three arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./aws_tmp_credentials.sh iam_user duration serial_number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipelines and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;&#xA;SparkNLP.version()&#xA;&#xA;val testData = spark.createDataFrame(Seq(&#xA;(1, &#34;Google has announced the release of a beta version of the popular TensorFlow machine learning library&#34;),&#xA;(2, &#34;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&#34;)&#xA;)).toDF(&#34;id&#34;, &#34;text&#34;)&#xA;&#xA;val pipeline = PretrainedPipeline(&#34;explain_document_dl&#34;, lang=&#34;en&#34;)&#xA;&#xA;val annotation = pipeline.transform(testData)&#xA;&#xA;annotation.show()&#xA;/*&#xA;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;2.5.0&#xA;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&#xA;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&#xA;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&#xA;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;*/&#xA;&#xA;annotation.select(&#34;entities.result&#34;).show(false)&#xA;&#xA;/*&#xA;+----------------------------------+&#xA;|result                            |&#xA;+----------------------------------+&#xA;|[Google, TensorFlow]              |&#xA;|[Donald John Trump, United States]|&#xA;+----------------------------------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all of the available Pipelines of a particular language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;)&#xA;/*&#xA;+--------------------------------------------+------+---------+&#xA;| Pipeline                                   | lang | version |&#xA;+--------------------------------------------+------+---------+&#xA;| dependency_parse                           |  en  | 2.0.2   |&#xA;| analyze_sentiment_ml                       |  en  | 2.0.2   |&#xA;| check_spelling                             |  en  | 2.1.0   |&#xA;| match_datetime                             |  en  | 2.1.0   |&#xA;                               ...&#xA;| explain_document_ml                        |  en  | 3.1.3   |&#xA;+--------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+---------------------------------------+------+---------+&#xA;| Pipeline                              | lang | version |&#xA;+---------------------------------------+------+---------+&#xA;| dependency_parse                      |  en  | 2.0.2   |&#xA;                               ...&#xA;| clean_slang                           |  en  | 3.0.0   |&#xA;| clean_pattern                         |  en  | 3.0.0   |&#xA;| check_spelling                        |  en  | 3.0.0   |&#xA;| dependency_parse                      |  en  | 3.0.0   |&#xA;+---------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained pipelines&lt;/a&gt; with examples, demos, benchmarks, and more&lt;/h4&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some selected languages:&lt;/strong&gt; &lt;code&gt;Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load NER model trained by deep learning approach and GloVe word embeddings&#xA;ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;)&#xA;# load NER model trained by deep learning approach and BERT word embeddings&#xA;ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// load French POS tagger model trained by Universal Dependencies&#xA;val french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;// load Italain LemmatizerModel&#xA;val italian_lemma = LemmatizerModel.pretrained(&#34;lemma_dxc&#34;, lang=&#34;it&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Models&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;)&#xA;/*&#xA;+---------------------------------------------+------+---------+&#xA;| Model                                       | lang | version |&#xA;+---------------------------------------------+------+---------+&#xA;| onto_100                                    |  en  | 2.1.0   |&#xA;| onto_300                                    |  en  | 2.1.0   |&#xA;| ner_dl_bert                                 |  en  | 2.2.0   |&#xA;| onto_100                                    |  en  | 2.4.0   |&#xA;| ner_conll_elmo                              |  en  | 3.2.2   |&#xA;+---------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+----------------------------+------+---------+&#xA;| Model                      | lang | version |&#xA;+----------------------------+------+---------+&#xA;| onto_100                   |  en  | 2.1.0   |&#xA;| ner_aspect_based_sentiment |  en  | 2.6.2   |&#xA;| ner_weibo_glove_840B_300d  |  en  | 2.6.2   |&#xA;| nerdl_atis_840b_300d       |  en  | 2.7.1   |&#xA;| nerdl_snips_100d           |  en  | 2.7.3   |&#xA;+----------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to see a list of available annotators, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showAvailableAnnotators()&#xA;/*&#xA;AlbertEmbeddings&#xA;AlbertForTokenClassification&#xA;AssertionDLModel&#xA;...&#xA;XlmRoBertaSentenceEmbeddings&#xA;XlnetEmbeddings&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained models&lt;/a&gt; with examples, demo, benchmark, and more&lt;/h4&gt; &#xA;&lt;h2&gt;Offline&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of using the Maven package, you need to load our Fat JAR&lt;/li&gt; &#xA; &lt;li&gt;Instead of using PretrainedPipeline for pretrained pipelines or the &lt;code&gt;.pretrained()&lt;/code&gt; function to download pretrained models, you will need to manually download your pipeline/model from &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;Models Hub&lt;/a&gt;, extract it, and load it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of &lt;code&gt;SparkSession&lt;/code&gt; with Fat JAR to have Spark NLP offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[*]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars&#34;, &#34;/tmp/spark-nlp-assembly-3.4.4.jar&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download provided Fat JARs from each &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/spark-nlp-assembly-3.4.4.jar&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of using pretrained Models and Pipelines in offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# instead of using pretrained() for online:&#xA;# french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;# you download this model, extract it, and use .load&#xA;french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)\&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)\&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&#xA;# example for pipelines&#xA;# instead of using PretrainedPipeline&#xA;# pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;# you download this pipeline, extract it, and use PipelineModel&#xA;PipelineModel.load(&#34;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Need more &lt;strong&gt;examples&lt;/strong&gt;? Check out our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;Spark NLP Showcase&lt;/a&gt; repository to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;p&gt;Also, don&#39;t forget to check &lt;a href=&#34;https://nlp.johnsnowlabs.com/demo&#34;&gt;Spark NLP in Action&lt;/a&gt; built by Streamlit.&lt;/p&gt; &#xA;&lt;h3&gt;All examples: &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;spark-nlp-workshop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nlp.johnsnowlabs.com/learn&#34;&gt;Check our Articles and Videos page here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;We have published a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2665963821000063&#34;&gt;paper&lt;/a&gt; that you can cite for the Spark NLP library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{KOCAMAN2021100058,&#xA;    title = {Spark NLP: Natural language understanding at scale},&#xA;    journal = {Software Impacts},&#xA;    pages = {100058},&#xA;    year = {2021},&#xA;    issn = {2665-9638},&#xA;    doi = {https://doi.org/10.1016/j.simpa.2021.100058},&#xA;    url = {https://www.sciencedirect.com/science/article/pii/S2665963.2.100063},&#xA;    author = {Veysel Kocaman and David Talby},&#xA;    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},&#xA;    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world’s most widely used NLP library in the enterprise.}&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ideas&lt;/li&gt; &#xA; &lt;li&gt;feedback&lt;/li&gt; &#xA; &lt;li&gt;documentation&lt;/li&gt; &#xA; &lt;li&gt;bug reports&lt;/li&gt; &#xA; &lt;li&gt;NLP training and testing corpora&lt;/li&gt; &#xA; &lt;li&gt;Development and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;John Snow Labs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://johnsnowlabs.com&#34;&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rockthejvm/spark-essentials</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/rockthejvm/spark-essentials</id>
    <link href="https://github.com/rockthejvm/spark-essentials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code we wrote during &lt;a href=&#34;https://rockthejvm.com/course/spark-essentials&#34;&gt;Rock the JVM&#39;s Spark Essentials with Scala&lt;/a&gt; (Udemy version &lt;a href=&#34;https://udemy.com/spark-essentials&#34;&gt;here&lt;/a&gt;) Unless explicitly mentioned, the code in this repository is exactly what was caught on camera.&lt;/p&gt; &#xA;&lt;h2&gt;How to install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install Docker&lt;/li&gt; &#xA; &lt;li&gt;either clone the repo or download as zip&lt;/li&gt; &#xA; &lt;li&gt;open with IntelliJ as an SBT project&lt;/li&gt; &#xA; &lt;li&gt;in a terminal window, navigate to the folder where you downloaded this repo and run &lt;code&gt;docker-compose up&lt;/code&gt; to build and start the PostgreSQL container - we will interact with it from Spark&lt;/li&gt; &#xA; &lt;li&gt;in another terminal window, navigate to &lt;code&gt;spark-cluster/&lt;/code&gt; and build the Docker-based Spark cluster with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x build-images.sh&#xA;./build-images.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;when prompted to start the Spark cluster, go to the &lt;code&gt;spark-cluster&lt;/code&gt; folder and run &lt;code&gt;docker-compose up --scale spark-worker=3&lt;/code&gt; to spin up the Spark containers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to start&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and checkout the &lt;code&gt;start&lt;/code&gt; tag by running the following in the repo folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to see the final code&lt;/h3&gt; &#xA;&lt;p&gt;Udemy students: checkout the &lt;code&gt;udemy&lt;/code&gt; branch of the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout udemy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Premium students: checkout the master branch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run an intermediate state&lt;/h3&gt; &#xA;&lt;p&gt;The repository was built while recording the lectures. Prior to each lecture, I tagged each commit so you can easily go back to an earlier state of the repo!&lt;/p&gt; &#xA;&lt;p&gt;The tags are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.1-scala-recap&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.1-dataframes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.2-dataframes-basics-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.4-datasources&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.5-datasources-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.6-columns-expressions&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.7-columns-expressions-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.8-aggregations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.9-joins&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.10-joins-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.1-common-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.2-complex-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.3-managing-nulls&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.4-datasets&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.5-datasets-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.1-spark-sql-shell&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.2-spark-sql&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.3-spark-sql-exercises&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.1-rdds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.2-rdds-part-2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And for premium students, in addition:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;6.1-spark-job-anatomy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6.2-deploying-to-cluster&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.1-taxi&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.2-taxi-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.3-taxi-3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.4-taxi-4&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you watch a lecture, you can &lt;code&gt;git checkout&lt;/code&gt; the appropriate tag and the repo will go back to the exact code I had when I started the lecture.&lt;/p&gt; &#xA;&lt;h3&gt;For questions or suggestions&lt;/h3&gt; &#xA;&lt;p&gt;If you have changes to suggest to this repo, either&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;submit a GitHub issue&lt;/li&gt; &#xA; &lt;li&gt;tell me in the course Q/A forum&lt;/li&gt; &#xA; &lt;li&gt;submit a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>gatling/gatling</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/gatling/gatling</id>
    <link href="https://github.com/gatling/gatling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modern Load Testing as Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gatling &lt;a href=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;What is Gatling ?&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is a load test tool. It officially supports HTTP, WebSocket, Server-Sent-Events and JMS.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finding fancy GUIs not that convenient for describing load tests, what you want is a friendly expressive DSL?&lt;/li&gt; &#xA; &lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt; &#xA; &lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Gatling is for you!&lt;/p&gt; &#xA;&lt;h2&gt;Underlying technologies&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is developed in Scala and built upon :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netty.io&#34;&gt;Netty&lt;/a&gt; for non blocking HTTP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://akka.io&#34;&gt;Akka&lt;/a&gt; for virtual users orchestration ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions, help?&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://gatling.io/docs/current/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://groups.google.com/forum/#!forum/gatling&#34;&gt;Gatling User Group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Found a real bug? Raise an &lt;a href=&#34;https://github.com/gatling/gatling/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Partners&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Takima&#34; src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/logo-takima-1-nom-bas.png&#34; width=&#34;80&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/highsoft_logo.png&#34; alt=&#34;Highsoft AS&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spark-examples/spark-scala-examples</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/spark-examples/spark-scala-examples</id>
    <link href="https://github.com/spark-examples/spark-scala-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project provides Apache Spark SQL, RDD, DataFrame and Dataset examples in Scala language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Explanation of all Spark SQL, RDD, DataFrame and Dataset examples present on this project are available at &lt;a href=&#34;https://sparkbyexamples.com/&#34;&gt;https://sparkbyexamples.com/&lt;/a&gt; , All these examples are coded in Scala language and tested in our development environment.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents (Spark Examples in Scala)&lt;/h1&gt; &#xA;&lt;h2&gt;Spark RDD Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/&#34;&gt;Create a Spark RDD using Parallelize&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-read-multiple-text-files-into-a-single-rdd/&#34;&gt;Spark – Read multiple text files into single RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-load-csv-file-into-rdd/&#34;&gt;Spark load CSV file into RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/&#34;&gt;Different ways to create Spark RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-how-to-create-an-empty-rdd/&#34;&gt;Spark – How to create an empty RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/&#34;&gt;Spark RDD Transformations with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/&#34;&gt;Spark RDD Actions with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/&#34;&gt;Spark Pair RDD Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/&#34;&gt;Spark Repartition() vs Coalesce()&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-shuffle-partitions/&#34;&gt;Spark Shuffle Partitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-persistence-storage-levels/&#34;&gt;Spark Persistence Storage Levels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-cache-and-persist-example/&#34;&gt;Spark RDD Cache and Persist with Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-broadcast-variables/&#34;&gt;Spark Broadcast Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-accumulators/&#34;&gt;Spark Accumulators Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/&#34;&gt;Convert Spark RDD to DataFrame | Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/&#34;&gt;Spark Create DataFrame with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/&#34;&gt;Spark DataFrame withColumn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/&#34;&gt;Ways to Rename column on Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/&#34;&gt;Spark – How to Drop a DataFrame/Dataset column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-where-filter/&#34;&gt;Working with Spark DataFrame Where Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/&#34;&gt;Spark SQL “case when” and “when otherwise”&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-collect/&#34;&gt;Collect() – Retrieve data from Spark RDD/DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/&#34;&gt;Spark – How to remove duplicate rows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/&#34;&gt;How to Pivot and Unpivot a Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/&#34;&gt;Spark SQL Data Types with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/&#34;&gt;Spark SQL StructType &amp;amp; StructField with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/&#34;&gt;Spark schema – explained with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-groupby-on-dataframe/&#34;&gt;Spark Groupby Example with DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-how-to-sort-dataframe-column-explained/&#34;&gt;Spark – How to Sort DataFrame column explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-join/&#34;&gt;Spark SQL Join Types with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/&#34;&gt;Spark DataFrame Union and UnionAll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/&#34;&gt;Spark map vs mapPartitions transformation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-foreachpartition-vs-foreach-explained/&#34;&gt;Spark foreachPartition vs foreach | what to use?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/&#34;&gt;Spark DataFrame Cache and Persist Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-udf/&#34;&gt;Spark SQL UDF (User Defined Functions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/&#34;&gt;Spark SQL DataFrame Array (ArrayType) Column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/&#34;&gt;Working with Spark DataFrame Map (MapType) column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/&#34;&gt;Spark SQL – Flatten Nested Struct column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/&#34;&gt;Spark – Flatten nested array to single array column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/&#34;&gt;Spark explode array and map columns to rows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/&#34;&gt;Spark SQL String Functions Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/&#34;&gt;Spark SQL Date and Time Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-array-functions/&#34;&gt;Spark SQL Array functions complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-map-functions/&#34;&gt;Spark SQL Map functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-sort-functions/&#34;&gt;Spark SQL Sort functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/&#34;&gt;Spark SQL Aggregate Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-window-functions/&#34;&gt;Spark Window Functions with Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Data Source API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&#34;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-and-write-json-file/&#34;&gt;Spark Read and Write JSON file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/&#34;&gt;Spark Read and Write Apache Parquet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-xml/&#34;&gt;Spark Read XML file using Databricks API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/&#34;&gt;Read &amp;amp; Write Avro files using Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-3-x/&#34;&gt;Using Avro Data Files From Spark SQL 2.3.x or earlier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/&#34;&gt;Spark Read from &amp;amp; Write to HBase table | Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/create-spark-dataframe-from-hbase-using-hortonworks/&#34;&gt;Create Spark DataFrame from HBase using Hortonworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/&#34;&gt;Spark Read ORC file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/&#34;&gt;Spark 3.0 Read Binary File into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Streaming &amp;amp; Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-outputmode/&#34;&gt;Spark Streaming – Different Output modes explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/&#34;&gt;Spark Streaming files from a directory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-from-tcp-socket/&#34;&gt;Spark Streaming – Reading data from TCP Socket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-with-kafka/&#34;&gt;Spark Streaming with Kafka Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-consume-and-produce-kafka-messages-in-avro-format/&#34;&gt;Spark Streaming – Kafka messages in Avro format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-batch-processing-produce-consume-kafka-topic/&#34;&gt;Spark SQL Batch Processing – Produce and Consume Apache Kafka Topic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azure-event-hubs-spark</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/Azure/azure-event-hubs-spark</id>
    <link href="https://github.com/Azure/azure-event-hubs-spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enabling Continuous Data Processing with Apache Spark and Azure Event Hubs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/event-hubs_spark.png&#34; alt=&#34;Azure Event Hubs + Apache Spark Connector&#34; width=&#34;270&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Azure Event Hubs Connector for Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.png&#34; alt=&#34;chat on gitter&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark.svg?branch=master&#34; alt=&#34;build status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#star-our-repo&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/azure/azure-event-hubs-spark.svg?style=social&amp;amp;label=Stars&#34; alt=&#34;star our repo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the source code of the Azure Event Hubs Connector for Apache Spark.&lt;/p&gt; &#xA;&lt;p&gt;Azure Event Hubs is a highly scalable publish-subscribe service that can ingest millions of events per second and stream them into multiple applications. Spark Streaming and Structured Streaming are scalable and fault-tolerant stream processing engines that allow users to process huge amounts of data using complex algorithms expressed with high-level functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, &lt;code&gt;join&lt;/code&gt;, and &lt;code&gt;window&lt;/code&gt;. This data can then be pushed to filesystems, databases, or even back to Event Hubs.&lt;/p&gt; &#xA;&lt;p&gt;By making Event Hubs and Spark easier to use together, we hope this connector makes building scalable, fault-tolerant applications easier for our users.&lt;/p&gt; &#xA;&lt;h2&gt;Latest Releases&lt;/h2&gt; &#xA;&lt;h4&gt;Spark&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark Version&lt;/th&gt; &#xA;   &lt;th&gt;Package Name&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 3.0&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Databricks&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Databricks Runtime Version&lt;/th&gt; &#xA;   &lt;th&gt;Artifact Id&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 8.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 7.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 6.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Roadmap&lt;/h4&gt; &#xA;&lt;p&gt;There is an open issue for each planned feature/enhancement.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;We maintain an &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt; - reach out to us via &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark/Lobby&#34;&gt;gitter&lt;/a&gt; if you think anything needs to be added or clarified!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Linking&lt;/h3&gt; &#xA;&lt;p&gt;For Scala/Java applications using SBT/Maven project definitions, link your application with the artifact below. &lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#latest-releases&#34;&gt;Latest Releases&lt;/a&gt; to find the correct artifact for your version of Apache Spark (or Databricks)!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.11&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.12&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Documentation for our connector can be found &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/docs/&#34;&gt;here&lt;/a&gt;. The integration guides there contain all the information you need to use this library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;re new to Apache Spark and/or Event Hubs, then we highly recommend reading their documentation first.&lt;/strong&gt; You can read Event Hubs documentation &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-what-is-event-hubs&#34;&gt;here&lt;/a&gt;, documentation for Spark Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;, and, the last but not least, Structured Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Further Assistance&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you need additional assistance, please don&#39;t hesitate to ask!&lt;/strong&gt; General questions and discussion should happen on our &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt;gitter chat&lt;/a&gt;. Please open an issue for bug reports and feature requests! Feedback, feature requests, bug reports, etc are all welcomed!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to help contribute (we&#39;d love to have your help!), then go to our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Build Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;In order to use the connector, you need to have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 1.8 SDK installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maven.apache.org/download.cgi&#34;&gt;Maven 3.x&lt;/a&gt; installed (or &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/index.html&#34;&gt;SBT version 1.x&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details on building from source and running tests can be found in our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build Command&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Builds jar and runs all tests&#xA;mvn clean package&#xA;&#xA;// Builds jar, runs all tests, and installs jar to your local maven repository&#xA;mvn clean install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>databricks/reference-apps</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/databricks/reference-apps</id>
    <link href="https://github.com/databricks/reference-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spark reference applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Databricks Reference Apps&lt;/h1&gt; &#xA;&lt;p&gt;At Databricks, we are developing a set of reference applications that demonstrate how to use Apache Spark. This book/repo contains the reference applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;View the code in the Github Repo here: &lt;a href=&#34;https://github.com/databricks/reference-apps&#34;&gt;https://github.com/databricks/reference-apps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the documentation here: &lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/&#34;&gt;http://databricks.gitbooks.io/databricks-spark-reference-applications/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Submit feedback or issues here: &lt;a href=&#34;https://github.com/databricks/reference-apps/issues&#34;&gt;https://github.com/databricks/reference-apps/issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The reference applications will appeal to those who want to learn Spark and learn better by example. Browse the applications, see what features of the reference applications are similar to the features you want to build, and refashion the code samples for your needs. Additionally, this is meant to be a practical guide for using Spark in your systems, so the applications mention other technologies that are compatible with Spark - such as what file systems to use for storing your massive data sets.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/index.html&#34;&gt;Log Analysis Application&lt;/a&gt; - The log analysis reference application contains a series of tutorials for learning Spark by example as well as a final application that can be used to monitor Apache access logs. The examples use Spark in batch mode, cover Spark SQL, as well as Spark Streaming.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html&#34;&gt;Twitter Streaming Language Classifier&lt;/a&gt; - This application demonstrates how to fetch and train a language classifier for Tweets using Spark MLlib. Then Spark Streaming is used to call the trained classifier and filter out live tweets that match a specified cluster. For directions on how to build and run this app - see &lt;a href=&#34;https://github.com/databricks/reference-apps/raw/master/twitter_classifier/scala/README.md&#34;&gt;twitter_classifier/scala/README.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/timeseries/index.html&#34;&gt;Weather TimeSeries Data Application with Cassandra&lt;/a&gt; - This reference application works with Weather Data which is taken for a given weather station at a given point in time. The app demonstrates several strategies for leveraging Spark Streaming integrated with Apache Cassandra and Apache Kafka for fast, fault-tolerant, streaming computations with time series data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These reference apps are covered by license terms covered &lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datastax/spark-cassandra-connector</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/datastax/spark-cassandra-connector</id>
    <link href="https://github.com/datastax/spark-cassandra-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DataStax Spark Cassandra Connector&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Cassandra Connector&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/datastax/spark-cassandra-connector/actions/workflows/main.yml/badge.svg?branch=master&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What&lt;/th&gt; &#xA;   &lt;th&gt;Where&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Community&lt;/td&gt; &#xA;   &lt;td&gt;Chat with us at &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;Datastax and Cassandra Q&amp;amp;A&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala Docs&lt;/td&gt; &#xA;   &lt;td&gt;Most Recent Release (3.2.0): &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;, &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/driver/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector-Driver&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latest Production Release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.2.0/jar&#34;&gt;3.2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra®.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library lets you expose Cassandra tables as Spark RDDs and Datasets/DataFrames, write Spark RDDs and Datasets/DataFrames to Cassandra tables, and execute arbitrary CQL queries in your Spark applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compatible with Apache Cassandra version 2.1 or higher (see table below)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Apache Spark 1.0 through 3.2 (&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/#version-compatibility&#34;&gt;see table below&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Scala 2.11 and 2.12&lt;/li&gt; &#xA; &lt;li&gt;Exposes Cassandra tables as Spark RDDs and Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Maps table rows to CassandraRow objects or tuples&lt;/li&gt; &#xA; &lt;li&gt;Offers customizable object mapper for mapping rows to objects of user-defined classes&lt;/li&gt; &#xA; &lt;li&gt;Saves RDDs back to Cassandra by implicit &lt;code&gt;saveToCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Delete rows and columns from cassandra by implicit &lt;code&gt;deleteFromCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Join with a subset of Cassandra data using &lt;code&gt;joinWithCassandraTable&lt;/code&gt; call for RDDs, and optimizes join with data in Cassandra when using Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Partition RDDs according to Cassandra replication using &lt;code&gt;repartitionByCassandraReplica&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Converts data types between Cassandra and Scala&lt;/li&gt; &#xA; &lt;li&gt;Supports all Cassandra data types including collections&lt;/li&gt; &#xA; &lt;li&gt;Filters rows on the server side via the CQL &lt;code&gt;WHERE&lt;/code&gt; clause&lt;/li&gt; &#xA; &lt;li&gt;Allows for execution of arbitrary CQL statements&lt;/li&gt; &#xA; &lt;li&gt;Plays nice with Cassandra Virtual Nodes&lt;/li&gt; &#xA; &lt;li&gt;Could be used in all languages supporting Datasets/DataFrames API: Python, R, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Version Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The connector project has several branches, each of which map into different supported versions of Spark and Cassandra. For previous releases the branch is named &#34;bX.Y&#34; where X.Y is the major+minor version; for example the &#34;b1.6&#34; branch corresponds to the 1.6 release. The &#34;master&#34; branch will normally contain development for the next connector release in progress.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the following branches are actively supported: 3.2.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/master&#34;&gt;master&lt;/a&gt;), 3.1.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.1&#34;&gt;b3.1&lt;/a&gt;), 3.0.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.0&#34;&gt;b3.0&lt;/a&gt;) and 2.5.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b2.5&#34;&gt;b2.5&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Spark&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra Java Driver&lt;/th&gt; &#xA;   &lt;th&gt;Minimum Java Version&lt;/th&gt; &#xA;   &lt;th&gt;Supported Scala Versions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.13&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.2&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0, 2.1, 2.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;1.5, 1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;   &lt;td&gt;1.1, 1.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0, 0.9&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*&lt;em&gt;Compatible with 2.1.X where X &amp;gt;= 5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hosted API Docs&lt;/h2&gt; &#xA;&lt;p&gt;API documentation for the Scala and Java interfaces are available online:&lt;/p&gt; &#xA;&lt;h3&gt;3.2.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.0.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.5.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/&#34;&gt;Embedded-Cassandra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;This project is available on the Maven Central Repository. For SBT to download the connector binaries, sources and javadoc, put this in your project SBT config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.datastax.spark&#34; %% &#34;spark-cassandra-connector&#34; % &#34;3.2.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default Scala version for Spark 3.0+ is 2.12 please choose the appropriate build. See the &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;FAQ&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/0_quick_start.md&#34;&gt;Quick-start guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/1_connecting.md&#34;&gt;Connecting to Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/2_loading.md&#34;&gt;Loading datasets from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/3_selection.md&#34;&gt;Server-side data selection and filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/4_mapper.md&#34;&gt;Working with user-defined case classes and tuples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/5_saving.md&#34;&gt;Saving and deleting datasets to/from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/6_advanced_mapper.md&#34;&gt;Customizing the object mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/7_java_api.md&#34;&gt;Using Connector in Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/8_streaming.md&#34;&gt;Spark Streaming with Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/10_embedded.md&#34;&gt;The spark-cassandra-connector-embedded Artifact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/11_metrics.md&#34;&gt;Performance monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/13_spark_shell.md&#34;&gt;The Spark Shell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/14_data_frames.md&#34;&gt;DataFrames&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/15_python.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/16_partitioning.md&#34;&gt;Partitioner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/17_submitting.md&#34;&gt;Submitting applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/reference.md&#34;&gt;Configuration Parameter Reference Table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Training&lt;/h2&gt; &#xA;&lt;h3&gt;DataStax Academy&lt;/h3&gt; &#xA;&lt;p&gt;DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In &lt;a href=&#34;https://academy.datastax.com/courses/ds320-analytics-with-apache-spark&#34;&gt;DS320: Analytics with Spark&lt;/a&gt;, you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Reporting Bugs&lt;/h3&gt; &#xA;&lt;p&gt;New issues may be reported using &lt;a href=&#34;https://datastax-oss.atlassian.net/browse/SPARKC/&#34;&gt;JIRA&lt;/a&gt;. Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Questions and requests for help may be submitted to the &lt;a href=&#34;https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user&#34;&gt;user mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Q/A Exchange&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;DataStax Community&lt;/a&gt; provides a free question and answer website for any and all questions relating to any DataStax Related technology. Including the Spark Cassandra Connector. Both DataStax engineers and community members frequent this board and answer questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To protect the community, all contributors are required to sign the &lt;a href=&#34;http://spark-cassandra-connector-cla.datastax.com/&#34;&gt;DataStax Spark Cassandra Connector Contribution License Agreement&lt;/a&gt;. The process is completely electronic and should only take a few minutes.&lt;/p&gt; &#xA;&lt;p&gt;To develop this project, we recommend using IntelliJ IDEA. Make sure you have installed and enabled the Scala Plugin. Open the project with IntelliJ IDEA and it will automatically create the project structure from the provided SBT configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checklist for contributing changes to the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;a href=&#34;https://datastax-oss.atlassian.net/projects/SPARKC/issues&#34;&gt;SPARKC JIRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make sure that all unit tests and integration tests pass&lt;/li&gt; &#xA; &lt;li&gt;Add an appropriate entry at the top of CHANGES.txt&lt;/li&gt; &#xA; &lt;li&gt;If the change has any end-user impacts, also include changes to the ./doc files as needed&lt;/li&gt; &#xA; &lt;li&gt;Prefix the pull request description with the JIRA number, for example: &#34;SPARKC-123: Fix the ...&#34;&lt;/li&gt; &#xA; &lt;li&gt;Open a pull-request on GitHub and await review&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To run unit and integration tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the integration tests require &lt;a href=&#34;https://github.com/riptano/ccm&#34;&gt;CCM&lt;/a&gt; to be installed on your machine. See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;By default, integration tests start up a separate, single Cassandra instance and run Spark in local mode. It is possible to run integration tests with your own Cassandra and/or Spark cluster. First, prepare a jar with testing code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test:package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then copy the generated test jar to your Spark nodes and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export IT_TEST_CASSANDRA_HOST=&amp;lt;IP of one of the Cassandra nodes&amp;gt;&#xA;export IT_TEST_SPARK_MASTER=&amp;lt;Spark Master URL&amp;gt;&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Documents&lt;/h2&gt; &#xA;&lt;p&gt;To generate the Reference Document use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt spark-cassandra-connector-unshaded/run (outputLocation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;outputLocation defaults to doc/reference.md&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2014-2017, DataStax, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>digital-asset/daml</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/digital-asset/daml</id>
    <link href="https://github.com/digital-asset/daml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Daml smart contract language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalasset.com/developers&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/digital-asset/daml/main/daml-logo.png&#34; alt=&#34;Daml logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.daml.com/getting-started/installation.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/digital-asset/daml.svg?label=Download&#34; alt=&#34;Download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digital-asset/daml/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/digitalasset/daml/_build/latest?definitionId=4&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/digitalasset/daml/_apis/build/status/digital-asset.daml?branchName=main&amp;amp;label=Build&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2022 Digital Asset (Switzerland) GmbH and/or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0&lt;/p&gt; &#xA;&lt;h1&gt;Welcome to the Daml repository!&lt;/h1&gt; &#xA;&lt;p&gt;This repository hosts all code for the &lt;a href=&#34;https://www.digitalasset.com/developers&#34;&gt;Daml smart contract language and SDK&lt;/a&gt;, originally created by &lt;a href=&#34;https://www.digitalasset.com&#34;&gt;Digital Asset&lt;/a&gt;. Daml is an open-source smart contract language for building future-proof distributed applications on a safe, privacy-aware runtime. The SDK is a set of tools to help you develop applications based on Daml.&lt;/p&gt; &#xA;&lt;h2&gt;Using Daml&lt;/h2&gt; &#xA;&lt;p&gt;To download Daml, follow &lt;a href=&#34;https://docs.daml.com/getting-started/installation.html&#34;&gt;the installation instructions&lt;/a&gt;. Once installed, to try it out, follow the &lt;a href=&#34;https://docs.daml.com/getting-started/quickstart.html&#34;&gt;quickstart guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have questions about how to use Daml or how to build Daml-based solutions, please join us on the &lt;a href=&#34;https://discuss.daml.com&#34;&gt;Daml forum&lt;/a&gt;. Alternatively, if you prefer asking on StackOverflow, please use &lt;a href=&#34;https://stackoverflow.com/tags/daml&#34;&gt;the &lt;code&gt;daml&lt;/code&gt; tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Daml&lt;/h2&gt; &#xA;&lt;p&gt;We warmly welcome &lt;a href=&#34;https://raw.githubusercontent.com/digital-asset/daml/main/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;. If you are looking for ideas on how to contribute, please browse our &lt;a href=&#34;https://github.com/digital-asset/daml/issues&#34;&gt;issues&lt;/a&gt;. To build and test Daml:&lt;/p&gt; &#xA;&lt;h3&gt;1. Clone this repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:digital-asset/daml.git&#xA;cd daml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Set up the development dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Our builds require various development dependencies (e.g. Java, Bazel, Python), provided by a tool called &lt;code&gt;dev-env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Linux&lt;/h4&gt; &#xA;&lt;p&gt;On Linux and Mac &lt;code&gt;dev-env&lt;/code&gt; can be installed with:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Nix by running: &lt;code&gt;bash &amp;lt;(curl -sSfL https://nixos.org/nix/install)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter &lt;code&gt;dev-env&lt;/code&gt; by running: &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you don&#39;t want to enter &lt;code&gt;dev-env&lt;/code&gt; manually each time using &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;, you can also install &lt;a href=&#34;https://direnv.net&#34;&gt;direnv&lt;/a&gt;. This repo already provides a &lt;code&gt;.envrc&lt;/code&gt; file, with an option to add more in a &lt;code&gt;.envrc.private&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h4&gt;Mac&lt;/h4&gt; &#xA;&lt;p&gt;On Mac &lt;code&gt;dev-env&lt;/code&gt; can be installed with:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Nix by running: &lt;code&gt;bash &amp;lt;(curl -sSfL https://nixos.org/nix/install)&lt;/code&gt; This is a &lt;em&gt;multi-user installation&lt;/em&gt; (there is no single-user installation option for macOS). Because of this, you need to configure &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt; to use Nix caches. You can add the contents of &lt;code&gt;dev-env/etc/nix.conf&lt;/code&gt; to &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt;, but keep &lt;code&gt;build-users-group = nixbld&lt;/code&gt; instead of leaving this empty as is done in &lt;code&gt;dev-env/etc/nix.conf&lt;/code&gt;. Make sure to restart the &lt;code&gt;nix-daemon&lt;/code&gt; after you have made changes to &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt;, for instance by using &lt;code&gt;sudo launchctl stop org.nixos.nix-daemon&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enter &lt;code&gt;dev-env&lt;/code&gt; by running: &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you don&#39;t want to enter &lt;code&gt;dev-env&lt;/code&gt; manually each time using &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;, you can also install &lt;a href=&#34;https://direnv.net&#34;&gt;direnv&lt;/a&gt;. This repo already provides a &lt;code&gt;.envrc&lt;/code&gt; file, with an option to add more in a &lt;code&gt;.envrc.private&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Note that after a macOS update it can appear as if Nix is not installed. This is because macOS updates can modify shell config files in &lt;code&gt;/etc&lt;/code&gt;, which the multi-user installation of Nix modifies as well. A workaround for this problem is to add the following to your shell config file in your &lt;code&gt;$HOME&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Nix&#xA;if [ -e &#39;/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh&#39; ]; then&#xA;  . &#39;/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh&#39;&#xA;fi&#xA;# End Nix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/NixOS/nix/issues/3616&#34;&gt;https://github.com/NixOS/nix/issues/3616&lt;/a&gt; for more information about this issue.&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;On Windows you need to enable long file paths by running the following command in an admin powershell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Set-ItemProperty -Path &#39;HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem&#39; -Name LongPathsEnabled -Type DWord -Value 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to configure Bazel for Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;build --config windows&#34; &amp;gt; .bazelrc.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, if you are on a Windows ad-hoc or CI machine you can use &lt;code&gt;ci/configure-bazel.sh&lt;/code&gt; instead of performing these steps manually. In that case, you should checkout the &lt;code&gt;daml&lt;/code&gt; repository into the path &lt;code&gt;D:\a\1\s&lt;/code&gt; in order to be able to use remote cache artifacts.&lt;/p&gt; &#xA;&lt;p&gt;Then start &lt;code&gt;dev-env&lt;/code&gt; from PowerShell with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.\dev-env\windows\bin\dadew.ps1 install&#xA;.\dev-env\windows\bin\dadew.ps1 sync&#xA;.\dev-env\windows\bin\dadew.ps1 enable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In all new PowerShell processes started, you need to repeat the &lt;code&gt;enable&lt;/code&gt; step.&lt;/p&gt; &#xA;&lt;h3&gt;3. First build and test&lt;/h3&gt; &#xA;&lt;p&gt;We have a single script to build most targets and run the tests. On Linux and Mac run &lt;code&gt;./build.sh&lt;/code&gt;. On Windows run &lt;code&gt;.\build.ps1&lt;/code&gt;. Note that these scripts may take over an hour the first time.&lt;/p&gt; &#xA;&lt;p&gt;To just build do &lt;code&gt;bazel build //...&lt;/code&gt;, and to just test do &lt;code&gt;bazel test //...&lt;/code&gt;. To read more about Bazel and how to use it, see &lt;a href=&#34;https://bazel.build&#34;&gt;the Bazel site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;On Mac if building is causing trouble complaining about missing nix packages, you can try first running &lt;code&gt;nix-build -A tools -A cached nix&lt;/code&gt; repeatedly until it completes without error.&lt;/p&gt; &#xA;&lt;h3&gt;4. Installing a local copy&lt;/h3&gt; &#xA;&lt;p&gt;On Linux and Mac run &lt;code&gt;daml-sdk-head&lt;/code&gt; which installs a version of the SDK with version number &lt;code&gt;0.0.0&lt;/code&gt;. Set the &lt;code&gt;version:&lt;/code&gt; field in any Daml project to 0.0.0 and it will use the locally installed one.&lt;/p&gt; &#xA;&lt;p&gt;On Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bazel build //release:sdk-release-tarball&#xA;tar -vxf .\bazel-bin\release\sdk-release-tarball-ce.tar.gz&#xA;cd sdk-*&#xA;daml\daml.exe install . --install-assistant=yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That should tell you what to put in the path, something along the lines of &lt;code&gt;C:\Users\admin\AppData\Roaming\daml\bin&lt;/code&gt;. Note that the Windows build is not yet fully functional.&lt;/p&gt; &#xA;&lt;h3&gt;Caching: build speed and disk space considerations&lt;/h3&gt; &#xA;&lt;p&gt;Bazel has a lot of nice properties, but they come at the cost of frequently rebuilding &#34;the world&#34;. To make that bearable, we make extensive use of caching. Most artifacts should be cached in our CDN, which is configured in &lt;code&gt;.bazelrc&lt;/code&gt; in this project.&lt;/p&gt; &#xA;&lt;p&gt;However, even then, you may end up spending a lot of time (and bandwidth!) downloading artifacts from the CDN. To alleviate that, by default, our build will create a subfolder &lt;code&gt;.bazel-cache&lt;/code&gt; in this project and keep an on-disk cache. &lt;strong&gt;This can take about 10GB&lt;/strong&gt; at the time of writing.&lt;/p&gt; &#xA;&lt;p&gt;To disable the disk cache, remove the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build:linux --disk_cache=.bazel-cache&#xA;build:darwin --disk_cache=.bazel-cache&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the &lt;code&gt;.bazelrc&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;If you work with multiple copies of this repository, you can point all of them to the same disk cache by overwriting these configs in either a &lt;code&gt;.bazelrc.local&lt;/code&gt; file in each copy, or a &lt;code&gt;~/.bazelrc&lt;/code&gt; file in your home directory.&lt;/p&gt; &#xA;&lt;h3&gt;Shared memory segment issues&lt;/h3&gt; &#xA;&lt;p&gt;On macOS at least, it looks like our setup does not always properly close the resources PostgreSQL uses. After a number of test runs, you may encounter an error message along the lines of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FATAL:  could not create shared memory segment: No space left on device&#xA;DETAIL:  Failed system call was shmget(key=5432001, size=56, 03600).&#xA;HINT:  This error does *not* mean that you have run out of disk space. It occurs either if all available shared memory IDs have been taken, in which case you need to raise the SHMMNI parameter in your kernel, or because the system&#39;s overall limit for shared memory has been reached.&#xA;        The PostgreSQL documentation contains more information about shared memory configuration.&#xA;child process exited with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this case, this is a memory leak, so increasing &lt;code&gt;SHMNI&lt;/code&gt; (or &lt;code&gt;SHMALL&lt;/code&gt; etc.) as suggested will only delay the issue. You can look at the existing shared memory segments on your system by running &lt;code&gt;ipcs -mcopt&lt;/code&gt;; this will print a line per segment, indicating the process ID of the last process to connect to the segment as well as the last access time and the number of currently connected processes.&lt;/p&gt; &#xA;&lt;p&gt;If you identify segments with no connected processes, and you are confident you can remove them, you can do so with &lt;code&gt;ipcrm $sid&lt;/code&gt;, where &lt;code&gt;$sid&lt;/code&gt; is the process ID displayed (as the second column) by &lt;code&gt;ipcs&lt;/code&gt;. Not many macOS applications use shared memory segments; &lt;strong&gt;if you have verified that all the existing memory segments on your machine need to be deleted&lt;/strong&gt;, e.g. because they have all been created by PostgreSQL instances that are no longer running, here is a Bash invocation you can use to remove all shared memory segments from your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is a dangerous command. Make sure you understand what it does before running it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;for shmid in $(ipcs -m | sed 1,3d | awk &#39;{print $2}&#39; | sed &#39;$d&#39;); do ipcrm -m $shmid; done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Haskell profiling builds&lt;/h3&gt; &#xA;&lt;p&gt;To build Haskell executables with profiling enabled, pass &lt;code&gt;-c dbg&lt;/code&gt; to Bazel, e.g. &lt;code&gt;bazel build -c dbg damlc&lt;/code&gt;. If you want to build the whole SDK with profiling enabled use &lt;code&gt;daml-sdk-head --profiling&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scala/scala</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/scala/scala</id>
    <link href="https://github.com/scala/scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala 2 compiler and standard library. For bugs, see scala/bug&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome!&lt;/h1&gt; &#xA;&lt;p&gt;This is the home of the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala 2&lt;/a&gt; standard library, compiler, and language spec.&lt;/p&gt; &#xA;&lt;h1&gt;How to contribute&lt;/h1&gt; &#xA;&lt;p&gt;Issues and bug reports for Scala 2 are located in &lt;a href=&#34;https://github.com/scala/bug&#34;&gt;scala/bug&lt;/a&gt;. That tracker is also where new contributors may find issues to work on: &lt;a href=&#34;https://github.com/scala/bug/labels/good%20first%20issue&#34;&gt;good first issues&lt;/a&gt;, &lt;a href=&#34;https://github.com/scala/bug/labels/help%20wanted&#34;&gt;help wanted&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For coordinating broader efforts, we also use the &lt;a href=&#34;https://github.com/scala/scala-dev/issues&#34;&gt;scala/scala-dev tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute here, please open a &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/#fork--pull&#34;&gt;pull request&lt;/a&gt; from your fork of this repository.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that we can&#39;t accept additions to the standard library, only modifications to existing code. Binary compatibility forbids adding new public classes or public methods. Additions are made to &lt;a href=&#34;https://github.com/scala/scala-library-next&#34;&gt;scala-library-next&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;We require that you sign the &lt;a href=&#34;https://www.lightbend.com/contribute/cla/scala&#34;&gt;Scala CLA&lt;/a&gt; before we can merge any of your work, to protect Scala&#39;s future as open source software.&lt;/p&gt; &#xA;&lt;p&gt;The general workflow is as follows.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/file an issue in scala/bug (or submit a well-documented PR right away!).&lt;/li&gt; &#xA; &lt;li&gt;Fork the scala/scala repo.&lt;/li&gt; &#xA; &lt;li&gt;Push your changes to a branch in your forked repo. For coding guidelines, go &lt;a href=&#34;https://github.com/scala/scala#coding-guidelines&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request to scala/scala from your forked repo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information on building and developing the core of Scala, read the rest of this README, especially for &lt;a href=&#34;https://github.com/scala/scala#get-ready-to-contribute&#34;&gt;setting up your machine&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Get in touch!&lt;/h1&gt; &#xA;&lt;p&gt;In order to get in touch with other Scala contributors, join the #scala-contributors channel on the &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;Scala Discord&lt;/a&gt; chat, or post on &lt;a href=&#34;https://contributors.scala-lang.org&#34;&gt;contributors.scala-lang.org&lt;/a&gt; (Discourse).&lt;/p&gt; &#xA;&lt;p&gt;If you need some help with your PR at any time, please feel free to @-mention anyone from the list below, and we will do our best to help you out:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;username&lt;/th&gt; &#xA;   &lt;th&gt;talk to me about...&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/lrytz&#34; height=&#34;50px&#34; title=&#34;Lukas Rytz&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lrytz&#34;&gt;&lt;code&gt;@lrytz&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;back end, optimizer, named &amp;amp; default arguments, reporters&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/retronym&#34; height=&#34;50px&#34; title=&#34;Jason Zaugg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/retronym&#34;&gt;&lt;code&gt;@retronym&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.12.x branch, compiler performance, weird compiler bugs, lambdas&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/SethTisue&#34; height=&#34;50px&#34; title=&#34;Seth Tisue&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SethTisue&#34;&gt;&lt;code&gt;@SethTisue&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;getting started, build, CI, community build, Jenkins, docs, library, REPL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/dwijnand&#34; height=&#34;50px&#34; title=&#34;Dale Wijnand&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dwijnand&#34;&gt;&lt;code&gt;@dwijnand&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pattern matcher, MiMa, partest&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/Ichoran&#34; height=&#34;50px&#34; title=&#34;Rex Kerr&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ichoran&#34;&gt;&lt;code&gt;@Ichoran&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;collections library, performance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/viktorklang&#34; height=&#34;50px&#34; title=&#34;Viktor Klang&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/viktorklang&#34;&gt;&lt;code&gt;@viktorklang&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;concurrency, futures&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/sjrd&#34; height=&#34;50px&#34; title=&#34;Sébastien Doeraene&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sjrd&#34;&gt;&lt;code&gt;@sjrd&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;interactions with Scala.js&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/NthPortal&#34; height=&#34;50px&#34; title=&#34;Princess | April&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NthPortal&#34;&gt;&lt;code&gt;@NthPortal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;library, concurrency, &lt;code&gt;scala.math&lt;/code&gt;, &lt;code&gt;LazyList&lt;/code&gt;, &lt;code&gt;Using&lt;/code&gt;, warnings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/bishabosha&#34; height=&#34;50px&#34; title=&#34;Jamie Thompson&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bishabosha&#34;&gt;&lt;code&gt;@bishabosha&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TASTy reader&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/joroKr21&#34; height=&#34;50px&#34; title=&#34;Georgi Krastev&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/joroKr21&#34;&gt;&lt;code&gt;@joroKr21&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;higher-kinded types, implicits, variance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;P.S.: If you have some spare time to help out around here, we would be delighted to add your name to this list!&lt;/p&gt; &#xA;&lt;h1&gt;Branches&lt;/h1&gt; &#xA;&lt;p&gt;Target the oldest branch you would like your changes to end up in. We periodically merge forward from older release branches (e.g., 2.12.x) to new ones (e.g. 2.13.x).&lt;/p&gt; &#xA;&lt;p&gt;If your change is difficult to merge forward, you may be asked to also submit a separate PR targeting the newer branch.&lt;/p&gt; &#xA;&lt;p&gt;If your change is version-specific and shouldn&#39;t be merged forward, put &lt;code&gt;[nomerge]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;p&gt;If your change is a backport from a newer branch and thus doesn&#39;t need to be merged forward, put &lt;code&gt;[backport]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;h2&gt;Choosing a branch&lt;/h2&gt; &#xA;&lt;p&gt;Most changes should target 2.13.x. We are increasingly reluctant to target 2.12.x unless there is a special reason (e.g. if an especially bad bug is found, or if there is commercial sponsorship).&lt;/p&gt; &#xA;&lt;p&gt;The 2.11.x branch is now &lt;a href=&#34;https://github.com/scala/scala-dev/issues/451&#34;&gt;inactive&lt;/a&gt; and no further 2.11.x releases are planned (unless unusual, unforeseeable circumstances arise). You should not target 2.11.x without asking maintainers first.&lt;/p&gt; &#xA;&lt;h1&gt;Repository structure&lt;/h1&gt; &#xA;&lt;p&gt;Most importantly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;+--build.sbt                 The main sbt build definition&#xA;+--project/                  The rest of the sbt build&#xA;+--src/                      All sources&#xA;   +---/library              Scala Standard Library&#xA;   +---/reflect              Scala Reflection&#xA;   +---/compiler             Scala Compiler&#xA;+--test/                     The Scala test suite&#xA;   +---/files                Partest tests&#xA;   +---/junit                JUnit tests&#xA;   +---/scalacheck           ScalaCheck tests&#xA;+--spec/                     The Scala language specification&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;but also:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;   +---/library-aux          Scala Auxiliary Library, for bootstrapping and documentation purposes&#xA;   +---/interactive          Scala Interactive Compiler, for clients such as an IDE (aka Presentation Compiler)&#xA;   +---/intellij             IntelliJ project templates&#xA;   +---/manual               Scala&#39;s runner scripts &#34;man&#34; (manual) pages&#xA;   +---/partest              Scala&#39;s internal parallel testing framework&#xA;   +---/partest-javaagent    Partest&#39;s helper java agent&#xA;   +---/repl                 Scala REPL core&#xA;   +---/repl-frontend        Scala REPL frontend&#xA;   +---/scaladoc             Scala&#39;s documentation tool&#xA;   +---/scalap               Scala&#39;s class file decompiler&#xA;   +---/testkit              Scala&#39;s unit-testing kit&#xA;+--admin/                    Scripts for the CI jobs and releasing&#xA;+--doc/                      Additional licenses and copyrights&#xA;+--scripts/                  Scripts for the CI jobs and releasing&#xA;+--tools/                    Scripts useful for local development&#xA;+--build/                    Build products&#xA;+--dist/                     Build products&#xA;+--target/                   Build products&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Get ready to contribute&lt;/h1&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You need the following tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java SDK. The baseline version is 8 for both 2.12.x and 2.13.x. It is almost always fine to use a later SDK such as 11 or 15 for local development. CI will verify against the baseline version.&lt;/li&gt; &#xA; &lt;li&gt;sbt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MacOS and Linux work. Windows may work if you use Cygwin. Community help with keeping the build working on Windows and documenting any needed setup is appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Tools we use&lt;/h2&gt; &#xA;&lt;p&gt;We are grateful for the following OSS licenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler Java profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.yourkit.com/java/profiler/&#34;&gt;YourKit Java Profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/idea/download/&#34;&gt;IntelliJ IDEA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build setup&lt;/h2&gt; &#xA;&lt;h3&gt;Basics&lt;/h3&gt; &#xA;&lt;p&gt;During ordinary development, a new Scala build is built by the previously released version, known as the &#34;reference compiler&#34; or, slangily, as &#34;STARR&#34; (stable reference release). Building with STARR is sufficient for most kinds of changes.&lt;/p&gt; &#xA;&lt;p&gt;However, a full build of Scala is &lt;em&gt;bootstrapped&lt;/em&gt;. Bootstrapping has two steps: first, build with STARR; then, build again using the freshly built compiler, leaving STARR behind. This guarantees that every Scala version can build itself.&lt;/p&gt; &#xA;&lt;p&gt;If you change the code generation part of the Scala compiler, your changes will only show up in the bytecode of the library and compiler after a bootstrap. Our CI does a bootstrapped build.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bootstrapping locally&lt;/strong&gt;: To perform a bootstrap, run &lt;code&gt;restarrFull&lt;/code&gt; within an sbt session. This will build and publish the Scala distribution to your local artifact repository and then switch sbt to use that version as its new &lt;code&gt;scalaVersion&lt;/code&gt;. You may then revert back with &lt;code&gt;reload&lt;/code&gt;. Note &lt;code&gt;restarrFull&lt;/code&gt; will also write the STARR version to &lt;code&gt;buildcharacter.properties&lt;/code&gt; so you can switch back to it with &lt;code&gt;restarr&lt;/code&gt; without republishing. This will switch the sbt session to use the &lt;code&gt;build-restarr&lt;/code&gt; and &lt;code&gt;target-restarr&lt;/code&gt; directories instead of &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;, which avoids wiping out classfiles and incremental metadata. IntelliJ will continue to be configured to compile and run tests using the starr version in &lt;code&gt;versions.properties&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For history on how the current scheme was arrived at, see &lt;a href=&#34;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&#34;&gt;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Building with fatal warnings&lt;/strong&gt;: To make warnings in the project fatal (i.e. turn them into errors), run &lt;code&gt;set Global / fatalWarnings := true&lt;/code&gt; in sbt (replace &lt;code&gt;Global&lt;/code&gt; with the name of a module—such as &lt;code&gt;reflect&lt;/code&gt;—to only make warnings fatal for that module). To disable fatal warnings again, either &lt;code&gt;reload&lt;/code&gt; sbt, or run &lt;code&gt;set Global / fatalWarnings := false&lt;/code&gt; (again, replace &lt;code&gt;Global&lt;/code&gt; with the name of a module if you only enabled fatal warnings for that module). CI always has fatal warnings enabled.&lt;/p&gt; &#xA;&lt;h3&gt;Using the sbt build&lt;/h3&gt; &#xA;&lt;p&gt;Once you&#39;ve started an &lt;code&gt;sbt&lt;/code&gt; session you can run one of the core commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;compile&lt;/code&gt; compiles all sub-projects (library, reflect, compiler, scaladoc, etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scala&lt;/code&gt; / &lt;code&gt;scalac&lt;/code&gt; run the REPL / compiler directly from sbt (accept options / arguments)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enableOptimizer&lt;/code&gt; reloads the build with the Scala optimizer enabled. Our releases are built this way. Enable this when working on compiler performance improvements. When the optimizer is enabled the build will be slower and incremental builds can be incorrect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setupPublishCore&lt;/code&gt; runs &lt;code&gt;enableOptimizer&lt;/code&gt; and configures a version number based on the current Git SHA. Often used as part of bootstrapping: &lt;code&gt;sbt setupPublishCore publishLocal &amp;amp;&amp;amp; sbt -Dstarr.version=&amp;lt;VERSION&amp;gt; testAll&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkBin&lt;/code&gt; generates runner scripts (&lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt;, etc) in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkPack&lt;/code&gt; creates a build in the Scala distribution format in &lt;code&gt;build/pack&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;junit/test&lt;/code&gt; runs the JUnit tests; &lt;code&gt;junit/testOnly *Foo&lt;/code&gt; runs a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scalacheck/test&lt;/code&gt; runs scalacheck tests, use &lt;code&gt;testOnly&lt;/code&gt; to run a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partest&lt;/code&gt; runs partest tests (accepts options, try &lt;code&gt;partest --help&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;publishLocal&lt;/code&gt; publishes a distribution locally (can be used as &lt;code&gt;scalaVersion&lt;/code&gt; in other sbt projects) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set baseVersionSuffix := &#34;bin-abcd123-SNAPSHOT&#34;&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the git hash of the revision being published. You can also use something custom like &lt;code&gt;&#34;bin-mypatch&#34;&lt;/code&gt;. This changes the version number from &lt;code&gt;2.13.2-SNAPSHOT&lt;/code&gt; to something more stable (&lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Note that the &lt;code&gt;-bin&lt;/code&gt; string marks the version binary compatible. Using it in sbt will cause the &lt;code&gt;scalaBinaryVersion&lt;/code&gt; to be &lt;code&gt;2.13&lt;/code&gt;. If the version is not binary compatible, we recommend using &lt;code&gt;-pre&lt;/code&gt;, e.g., &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set publishArtifact in (Compile, packageDoc) in ThisBuild := false&lt;/code&gt; to skip generating / publishing API docs (speeds up the process).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a command results in an error message like &lt;code&gt;a module is not authorized to depend on itself&lt;/code&gt;, it may be that a global sbt plugin is causing a cyclical dependency. Try disabling global sbt plugins (perhaps by temporarily commenting them out in &lt;code&gt;~/.sbt/1.0/plugins/plugins.sbt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Sandbox&lt;/h4&gt; &#xA;&lt;p&gt;We recommend keeping local test files in the &lt;code&gt;sandbox&lt;/code&gt; directory which is listed in the &lt;code&gt;.gitignore&lt;/code&gt; of the Scala repo.&lt;/p&gt; &#xA;&lt;h4&gt;Incremental compilation&lt;/h4&gt; &#xA;&lt;p&gt;Note that sbt&#39;s incremental compilation is often too coarse for the Scala compiler codebase and re-compiles too many files, resulting in long build times (check &lt;a href=&#34;https://github.com/sbt/sbt/issues/1104&#34;&gt;sbt#1104&lt;/a&gt; for progress on that front). In the meantime you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use IntelliJ IDEA for incremental compiles (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/#ide-setup&#34;&gt;IDE Setup&lt;/a&gt; below) - its incremental compiler is a bit less conservative, but usually correct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;IDE setup&lt;/h3&gt; &#xA;&lt;p&gt;We suggest using IntelliJ IDEA (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/src/intellij/README.md&#34;&gt;src/intellij/README.md&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalameta.org/metals/&#34;&gt;Metals&lt;/a&gt; may also work, but we don&#39;t yet have instructions or sample configuration for that. A pull request in this area would be exceedingly welcome. In the meantime, we are collecting guidance at &lt;a href=&#34;https://github.com/scala/scala-dev/issues/668&#34;&gt;scala/scala-dev#668&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to use IntelliJ&#39;s incremental compiler:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run &lt;code&gt;dist/mkBin&lt;/code&gt; in sbt to get a build and the runner scripts in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &#34;Build&#34; - &#34;Make Project&#34; in IntelliJ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now you can edit and build in IntelliJ and use the scripts (compiler, REPL) to directly test your changes. You can also run the &lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt; and &lt;code&gt;partest&lt;/code&gt; commands in sbt. Enable &#34;Ant mode&#34; (explained above) to prevent sbt&#39;s incremental compiler from re-compiling (too many) files before each &lt;code&gt;partest&lt;/code&gt; invocation.&lt;/p&gt; &#xA;&lt;h1&gt;Coding guidelines&lt;/h1&gt; &#xA;&lt;p&gt;Our guidelines for contributing are explained in &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;. It contains useful information on our coding standards, testing, documentation, how we use git and GitHub and how to get your code reviewed.&lt;/p&gt; &#xA;&lt;p&gt;You may also want to check out the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://scala-lang.org/contribute/hacker-guide.html&#34;&gt;&#34;Scala Hacker Guide&#34;&lt;/a&gt; covers some of the same ground as this README, but in greater detail and in a more tutorial style, using a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org&#34;&gt;Scala documentation site&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Scala CI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/scala/scala&#34;&gt;&lt;img src=&#34;https://travis-ci.com/scala/scala.svg?branch=2.13.x&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once you submit a PR your commits will be automatically tested by the Scala CI.&lt;/p&gt; &#xA;&lt;p&gt;Our CI setup is always evolving. See &lt;a href=&#34;https://github.com/scala/scala-dev/issues/751&#34;&gt;scala/scala-dev#751&lt;/a&gt; for more details on how things currently work and how we expect they might change.&lt;/p&gt; &#xA;&lt;p&gt;If you see a spurious failure on Jenkins, you can post &lt;code&gt;/rebuild&lt;/code&gt; as a PR comment. The &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scabot README&lt;/a&gt; lists all available commands.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to test your patch before having everything polished for review, you can have Travis CI build your branch (make sure you have a fork and have Travis CI enabled for branch builds on it first, and then push your branch). Also feel free to submit a draft PR. In case your draft branch contains a large number of commits (that you didn&#39;t clean up / squash yet for review), consider adding &lt;code&gt;[ci: last-only]&lt;/code&gt; to the PR title. That way only the last commit will be tested, saving some energy and CI-resources. Note that inactive draft PRs will be closed eventually, which does not mean the change is being rejected.&lt;/p&gt; &#xA;&lt;p&gt;CI performs a compiler bootstrap. The first task, &lt;code&gt;validatePublishCore&lt;/code&gt;, publishes a build of your commit to the temporary repository &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&lt;/a&gt;. Note that this build is not yet bootstrapped, its bytecode is built using the current STARR. The version number is &lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the commit hash. For binary incompatible builds, the version number is &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use Scala builds in the validation repository locally by adding a resolver and specifying the corresponding &lt;code&gt;scalaVersion&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set resolvers += &#34;pr&#34; at &#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots/&#34;&#xA;&amp;gt; set scalaVersion := &#34;2.12.2-bin-abcd123-SNAPSHOT&#34;&#xA;&amp;gt; console&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&#34;Nightly&#34; builds&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI builds nightly download releases and publishes them to &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-integration/&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-integration/&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Using a nightly build in sbt is explained in &lt;a href=&#34;https://stackoverflow.com/questions/40622878&#34;&gt;this Stack Overflow answer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Although we casually refer to these as &#34;nightly&#34; builds, they aren&#39;t actually built nightly, but &#34;mergely&#34;. That is to say, a build is published for every merged PR.&lt;/p&gt; &#xA;&lt;h2&gt;Scala CI internals&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI runs as a Jenkins instance on &lt;a href=&#34;https://scala-ci.typesafe.com/&#34;&gt;scala-ci.typesafe.com&lt;/a&gt;, configured by a chef cookbook at &lt;a href=&#34;https://github.com/scala/scala-jenkins-infra&#34;&gt;scala/scala-jenkins-infra&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The build bot that watches PRs, triggers testing builds and applies the &#34;reviewed&#34; label after an LGTM comment is in the &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scala/scabot&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Community build&lt;/h2&gt; &#xA;&lt;p&gt;The Scala community build is an important method for testing Scala releases. A community build can be launched for any Scala commit, even before the commit&#39;s PR has been merged. That commit is then used to build a large number of open-source projects from source and run their test suites.&lt;/p&gt; &#xA;&lt;p&gt;To request a community build run on your PR, just ask in a comment on the PR and a Scala team member (probably @SethTisue) will take care of it. (&lt;a href=&#34;https://github.com/scala/community-builds/wiki#can-i-run-it-against-a-pull-request-in-scalascala&#34;&gt;details&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Community builds run on the Scala Jenkins instance. The jobs are named &lt;code&gt;..-integrate-community-build&lt;/code&gt;. See the &lt;a href=&#34;https://github.com/scala/community-builds&#34;&gt;scala/community-builds&lt;/a&gt; repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>polomarcus/tp</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/polomarcus/tp</id>
    <link href="https://github.com/polomarcus/tp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practices - Data engineering&lt;/h1&gt; &#xA;&lt;h2&gt;Tools you need&lt;/h2&gt; &#xA;&lt;p&gt;Have a stackoverflow account : &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;https://stackoverflow.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a github account : &lt;a href=&#34;https://github.com/&#34;&gt;https://github.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a github repo to push your code.&lt;/p&gt; &#xA;&lt;h3&gt;Fork the repo on your own Github account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/tp/fork&#34;&gt;https://github.com/polomarcus/tp/fork&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker and Compose&lt;/h3&gt; &#xA;&lt;p&gt;Take time to read and install&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/overview/&#34;&gt;https://docs.docker.com/get-started/overview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker --version&#xA;Docker version 20.10.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;https://docs.docker.com/compose/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose --version&#xA;docker-compose version 1.29.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TP1 - &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-docker-kafka-bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP2 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-functional-programming-scala&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP3 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-data-processing-framework&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP 4 - Kafka Streams to read and write to Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34;&gt;https://kafka.apache.org/documentation/streams/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&#34;&gt;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka User Interface (UI) : &lt;a href=&#34;https://www.conduktor.io/download/&#34;&gt;https://www.conduktor.io/download/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sjsonnet</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/databricks/sjsonnet</id>
    <link href="https://github.com/databricks/sjsonnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sjsonnet&lt;/h1&gt; &#xA;&lt;p&gt;A JVM implementation of the &lt;a href=&#34;https://jsonnet.org/&#34;&gt;Jsonnet&lt;/a&gt; configuration language.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet can be used from Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.databricks&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sjsonnet_2.13&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    new String[]{&#34;foo.jsonnet&#34;},&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.package$.MODULE$.pwd(),&#xA;    scala.None$.empty()&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#34;com.databricks&#34; %% &#34;sjsonnet&#34; % &#34;0.4.2&#34; // SBT&#xA;ivy&#34;com.databricks::sjsonnet:0.4.2&#34; // Mill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    Array(&#34;foo.jsonnet&#34;),&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.pwd, // working directory&#xA;    None&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a standalone executable assembly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&#34;&gt;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar &amp;gt; sjsonnet.jar&#xA;&#xA;$ chmod +x sjsonnet.jar&#xA;&#xA;$ ./sjsonnet.jar&#xA;error: Need to pass in a jsonnet file to evaluate&#xA;usage: sjsonnet [sjsonnet-options] script-file&#xA;&#xA;  -i, --interactive  Run Mill in interactive mode, suitable for opening REPLs and taking user input&#xA;  -n, --indent       How much to indent your output JSON&#xA;  -J, --jpath        Specify an additional library search dir (right-most wins)&#xA;  -o, --output-file  Write to the output file rather than stdout&#xA;  ...&#xA;&#xA;$ ./sjsonnet.jar foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or from Javascript:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.js &amp;gt; sjsonnet.js&#xA;&#xA;$ node&#xA;&#xA;&amp;gt; require(&#34;./sjsonnet.js&#34;)&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#34;local f = function(x) x * x; f(11)&#34;, {}, {}, &#34;&#34;, (wd, imported) =&amp;gt; null)&#xA;121&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#xA;    &#34;local f = import &#39;foo&#39;; f + &#39;bar&#39;&#34;, // code&#xA;    {}, // extVars&#xA;    {}, // tlaVars&#xA;    &#34;&#34;, // initial working directory&#xA;&#xA;    // import callback: receives a base directory and the imported path string,&#xA;    // returns a tuple of the resolved file path and file contents or file contents resolve method&#xA;    (wd, imported) =&amp;gt; [wd + &#34;/&#34; + imported, &#34;local bar = 123; bar + bar&#34;],&#xA;    // loader callback: receives the tuple from the import callback and returns the file contents&#xA;    ([path, content]) =&amp;gt; content&#xA;    )&#xA;&#39;246bar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that since Javascript does not necessarily have access to the filesystem, you have to provide an explicit import callback that you can use to resolve imports yourself (whether through Node&#39;s &lt;code&gt;fs&lt;/code&gt; module, or by emulating a filesystem in-memory)&lt;/p&gt; &#xA;&lt;h3&gt;Running deeply recursive Jsonnet programs&lt;/h3&gt; &#xA;&lt;p&gt;The depth of recursion is limited by JVM stack size. You can run Sjsonnet with increased stack size as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -Xss100m -cp sjsonnet.jar sjsonnet.SjsonnetMain foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -Xss option above is responsible for JVM stack size. Please try this if you ever run into &lt;code&gt;sjsonnet.Error: Internal Error ... Caused by: java.lang.StackOverflowError ...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is no analog of &lt;code&gt;--max-stack&lt;/code&gt;/&lt;code&gt;-s&lt;/code&gt; option of &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;. The only stack size limit is the one of the JVM.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet is implementated as an optimizing interpreter. There are roughly 4 phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Parser&lt;/code&gt;: parses an input &lt;code&gt;String&lt;/code&gt; into a &lt;code&gt;sjsonnet.Expr&lt;/code&gt;, which is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;Syntax Tree&lt;/a&gt; representing the Jsonnet document syntax, using the &lt;a href=&#34;https://github.com/lihaoyi/fastparse&#34;&gt;Fastparse&lt;/a&gt; parsing library&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.StaticOptimizer&lt;/code&gt; is a single AST transform that performs static checking, essential rewriting (e.g. assigning indices in the symbol table for variables) and optimizations. The result is another &lt;code&gt;sjsonnet.Expr&lt;/code&gt; per input file that can be stored in the parse cache and reused.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Evaluator&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Expr&lt;/code&gt; produced by the optimizer and converts it into a &lt;code&gt;sjsonnet.Val&lt;/code&gt;, a data structure representing the Jsonnet runtime values (basically lazy JSON which can contain function values).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Materializer&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Val&lt;/code&gt; and converts it into an output &lt;code&gt;ujson.Expr&lt;/code&gt;: a non-lazy JSON structure without any remaining un-evaluated function values. This can be serialized to a string formatted in a variety of ways&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These three phases are encapsulated in the &lt;code&gt;sjsonnet.Interpreter&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some notes on the values used in parts of the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Expr&lt;/code&gt;: this represents &lt;code&gt;{...}&lt;/code&gt; object literal nodes, &lt;code&gt;a + b&lt;/code&gt; binary operation nodes, &lt;code&gt;function(a) {...}&lt;/code&gt; definitions and &lt;code&gt;f(a)&lt;/code&gt; invocations, etc.. Also keeps track of source-offset information so failures can be correlated with line numbers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Val&lt;/code&gt;: essentially the JSON structure (objects, arrays, primitives) but with two modifications. The first is that functions like &lt;code&gt;function(a){...}&lt;/code&gt; can still be present in the structure: in Jsonnet you can pass around functions as values and call then later on. The second is that object values &amp;amp; array entries are &lt;em&gt;lazy&lt;/em&gt;: e.g. &lt;code&gt;[error 123, 456][1]&lt;/code&gt; does not raise an error because the first (erroneous) entry of the array is un-used and thus not evaluated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classes representing literals extend &lt;code&gt;sjsonnet.Val.Literal&lt;/code&gt; which in turn extends &lt;em&gt;both&lt;/em&gt;, &lt;code&gt;Expr&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt;. This allows the evaluator to skip over them instead of having to convert them from one representation to the other.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Due to pervasive caching, sjsonnet is much faster than google/jsonnet. See this blog post for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2018/10/12/writing-a-faster-jsonnet-compiler.html&#34;&gt;Writing a Faster Jsonnet Compiler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the latest set of benchmarks I&#39;ve run comparing Sjsonnet against google/jsonnet and google/go-jsonnet, measuring the time taken to&lt;br&gt; evaluate the &lt;code&gt;test_suite/&lt;/code&gt; folder (smaller is better):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.5&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.26ms ± 0.22&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.59ms ± 0.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.07ms ± 0.30&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.29ms ± 0.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/jsonnet&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/go-jsonnet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1277ms&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~274ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;google/jsonnet was built from source on commit f59758d1904bccda99598990f582dd2e1e9ad263, while google/go-jsonnet was &lt;code&gt;go get&lt;/code&gt;ed at version &lt;code&gt;v0.13.0&lt;/code&gt;. You can see the source code of the benchmark in&lt;br&gt; &lt;a href=&#34;https://github.com/databricks/sjsonnet/raw/master/sjsonnet/test/src-jvm/sjsonnet/SjsonnetTestMain.scala&#34;&gt;SjsonnetTestMain.scala&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sjsonnet 0.4.0 and 0.4.1 further improve the performance significantly on our internal benchmarks. A set of new JMH benchmarks provide detailed performance data of an entire run (&lt;code&gt;MainBenchmark&lt;/code&gt;) and the non-evaluation-related parts (&lt;code&gt;MaterializerBenchmark&lt;/code&gt;, &lt;code&gt;OptimizerBenchmark&lt;/code&gt;, &lt;code&gt;ParserBenchmark&lt;/code&gt;). They can be run from the (JVM / Scala 2.13 only) sbt build. The Sjsonnet profiler is located in the same sbt project:&lt;/p&gt; &#xA;&lt;p&gt;The Sjsonnet command line which is run by all of these is defined in &lt;code&gt;MainBenchmark.mainArgs&lt;/code&gt;. You need to change it to point to a suitable input before running a benchmark or the profiler. (For Databricks employees who want to reproduce our benchmarks, the pre-configured command line is expected to be run against databricks/universe @ 7cbd8d7cb071983077d41fcc34f0766d0d2a247d).&lt;/p&gt; &#xA;&lt;p&gt;Benchmark example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/jmh:run -jvmArgs &#34;-XX:+UseStringDeduplication&#34; sjsonnet.MainBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profiler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Laziness&lt;/h2&gt; &#xA;&lt;p&gt;The Jsonnet language is &lt;em&gt;lazy&lt;/em&gt;: expressions don&#39;t get evaluated unless their value is needed, and thus even erroneous expressions do not cause a failure if un-used. This is represented in the Sjsonnet codebase by &lt;code&gt;sjsonnet.Lazy&lt;/code&gt;: a wrapper type that encapsulates an arbitrary computation that returns a &lt;code&gt;sjsonnet.Val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sjsonnet.Lazy&lt;/code&gt; is used in several places, representing where laziness is present in the language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Scope&lt;/code&gt;, representing local variable name bindings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Arr&lt;/code&gt;, representing the contents of array cells&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Obj&lt;/code&gt;, representing the contents of object values&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Val&lt;/code&gt; extends &lt;code&gt;Lazy&lt;/code&gt; so that an already computed value can be treated as lazy without having to wrap it.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet caches the results of lazy computations the first time they are evaluated, avoiding wasteful re-computation when a value is used more than once.&lt;/p&gt; &#xA;&lt;h2&gt;Standard Library&lt;/h2&gt; &#xA;&lt;p&gt;Different from &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet does not implement the Jsonnet standard library &lt;code&gt;std&lt;/code&gt; in Jsonnet code. Rather, those functions are implemented as intrinsics directly in the host language (in &lt;code&gt;Std.scala&lt;/code&gt;). This allows both better error messages when the input types are wrong, as well as better performance for the more computationally-intense builtin functions.&lt;/p&gt; &#xA;&lt;h2&gt;Client-Server&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet comes with a built in thin-client and background server, to help mitigate the unfortunate JVM warmup overhead that adds ~1s to every invocation down to 0.2-0.3s. For the simple non-client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the executable. For the client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].server.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the Sjsonnet background server lives in &lt;code&gt;~/.sjsonnet&lt;/code&gt;, and lasts 5 minutes before shutting itself when inactive.&lt;/p&gt; &#xA;&lt;p&gt;Since the Sjsonnet client still has 0.2-0.3s of overhead, if using Sjsonnet heavily it is still better to include it in your JVM classpath and invoke it programmatically via &lt;code&gt;new Interpreter(...).interpret(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing&lt;/h2&gt; &#xA;&lt;p&gt;To publish, make sure the version number in &lt;code&gt;build.sc&lt;/code&gt; is correct, then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill -i mill.scalalib.PublishModule/publishAll --sonatypeCreds lihaoyi:$SONATYPE_PASSWORD --publishArtifacts __.publishArtifacts --release true&#xA;&#xA;./mill -i show sjsonnet[2.13.4].js.fullOpt&#xA;./mill -i show sjsonnet[2.13.4].jvm.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;0.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make lazy initialization of static Val.Obj thread-safe &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/136&#34;&gt;#136&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deduplicate strings in the parser &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/137&#34;&gt;#137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update the JS example &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/141&#34;&gt;#141&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional significant performance improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/119&#34;&gt;#119&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Error handling fixes and improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/125&#34;&gt;#125&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements with lots of internal changes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/117&#34;&gt;#117&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid catching fatal exceptions during evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--yaml-debug&lt;/code&gt; flag to add source-line comments showing where each line of YAML came from &lt;a href=&#34;&#34;&gt;#105&lt;/a&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/105&#34;&gt;https://github.com/databricks/sjsonnet/pull/105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;objectValues&lt;/code&gt; and &lt;code&gt;objectVlauesAll&lt;/code&gt; to stdlib &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/104&#34;&gt;#104&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow direct YAML output generation via &lt;code&gt;--yaml-out&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Do not allow duplicate field in object when evaluating list list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/100&#34;&gt;#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix compiler crash when &#39;+&#39; signal is true in a field declaration inside a list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/98&#34;&gt;#98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix error message for too many arguments with at least one named arg &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/97&#34;&gt;#97&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming JSON output to disk for lower memory usage &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/85&#34;&gt;#85&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Static detection of duplicate fields &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/86&#34;&gt;#86&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Strict mode to disallow error-prone adjacent object literals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/88&#34;&gt;#88&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.flatMap&lt;/code&gt;, &lt;code&gt;std.repeat&lt;/code&gt;, &lt;code&gt;std.clamp&lt;/code&gt;, &lt;code&gt;std.member&lt;/code&gt;, &lt;code&gt;std.stripChars&lt;/code&gt;, &lt;code&gt;std.rstripChars&lt;/code&gt;, &lt;code&gt;std.lstripChars&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for syntactical key ordering &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/53&#34;&gt;#53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bump dependency versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump verion of Scalatags, uPickle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.9&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version of FastParse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump versions of OS-Lib, uJson, Scalatags&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support std lib methods that take a key lambda &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/40&#34;&gt;#40&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Handle hex in unicode escaoes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/41&#34;&gt;#41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add encodeUTF8, decodeUTF8 std lib methdos &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/42&#34;&gt;#42&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Properly fail on non-boolean conditionals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/44&#34;&gt;#44&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support YAML-steam output &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/45&#34;&gt;#45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~2x performance increase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javascript support, allowing Sjsonnet to be used in the browser or on Node.js&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala 2.13 support&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.mod&lt;/code&gt;, &lt;code&gt;std.min&lt;/code&gt; and &lt;code&gt;std.max&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improvements to error reporting when types do not match&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements to the parser via upgrading to Fastparse 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First release&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>windymelt/FNFIS</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/windymelt/FNFIS</id>
    <link href="https://github.com/windymelt/FNFIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;FNFIS - FreeNet File Indexing on Scala Testing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-sql-perf</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/databricks/spark-sql-perf</id>
    <link href="https://github.com/databricks/spark-sql-perf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark SQL Performance Tests&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-sql-perf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-sql-perf.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a performance testing framework for &lt;a href=&#34;https://spark.apache.org/sql/&#34;&gt;Spark SQL&lt;/a&gt; in &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; 2.2+.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This README is still under development. Please also check our source code for more information.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Running from command line.&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/run --help&#xA;&#xA;spark-sql-perf 0.2.0&#xA;Usage: spark-sql-perf [options]&#xA;&#xA;  -b &amp;lt;value&amp;gt; | --benchmark &amp;lt;value&amp;gt;&#xA;        the name of the benchmark to run&#xA;  -m &amp;lt;value&amp;gt; | --master &amp;lt;value&#xA;        the master url to use&#xA;  -f &amp;lt;value&amp;gt; | --filter &amp;lt;value&amp;gt;&#xA;        a filter on the name of the queries to run&#xA;  -i &amp;lt;value&amp;gt; | --iterations &amp;lt;value&amp;gt;&#xA;        the number of iterations to run&#xA;  --help&#xA;        prints this usage text&#xA;        &#xA;$ bin/run --benchmark DatasetPerformance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first run of &lt;code&gt;bin/run&lt;/code&gt; will build the library.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;sbt package&lt;/code&gt; or &lt;code&gt;sbt assembly&lt;/code&gt; to build the library jar.&lt;br&gt; Use &lt;code&gt;sbt +package&lt;/code&gt; to build for scala 2.11 and 2.12.&lt;/p&gt; &#xA;&lt;h2&gt;Local performance tests&lt;/h2&gt; &#xA;&lt;p&gt;The framework contains twelve benchmarks that can be executed in local mode. They are organized into three classes and target different components and functions of Spark:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/DatasetPerformance.scala&#34;&gt;DatasetPerformance&lt;/a&gt; compares the performance of the old RDD API with the new Dataframe and Dataset APIs. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark DatasetPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/JoinPerformance.scala&#34;&gt;JoinPerformance&lt;/a&gt; compares the performance of joining different table sizes and shapes with different join types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark JoinPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/AggregationPerformance.scala&#34;&gt;AggregationPerformance&lt;/a&gt; compares the performance of aggregating different table sizes using different aggregation types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark AggregationPerformance&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;MLlib tests&lt;/h1&gt; &#xA;&lt;p&gt;To run MLlib tests, run &lt;code&gt;/bin/run-ml yamlfile&lt;/code&gt;, where &lt;code&gt;yamlfile&lt;/code&gt; is the path to a YAML configuration file describing tests to run and their parameters.&lt;/p&gt; &#xA;&lt;h1&gt;TPC-DS&lt;/h1&gt; &#xA;&lt;h2&gt;Setup a benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Before running any query, a dataset needs to be setup by creating a &lt;code&gt;Benchmark&lt;/code&gt; object. Generating the TPCDS data requires dsdgen built and available on the machines. We have a fork of dsdgen that you will need. The fork includes changes to generate TPCDS data to stdout, so that this library can pipe them directly to Spark, without intermediate files. Therefore, this library will not work with the vanilla TPCDS kit.&lt;/p&gt; &#xA;&lt;p&gt;TPCDS kit needs to be installed on all cluster executor nodes under the same path!&lt;/p&gt; &#xA;&lt;p&gt;It can be found &lt;a href=&#34;https://github.com/databricks/tpcds-kit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Generate the data&#xA;build/sbt &#34;test:runMain com.databricks.spark.sql.perf.tpcds.GenTPCDSData -d &amp;lt;dsdgenDir&amp;gt; -s &amp;lt;scaleFactor&amp;gt; -l &amp;lt;location&amp;gt; -f &amp;lt;format&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Create the specified database&#xA;sql(s&#34;create database $databaseName&#34;)&#xA;// Create metastore tables in a specified database for your data.&#xA;// Once tables are created, the current database will be switched to the specified database.&#xA;tables.createExternalTables(rootDir, &#34;parquet&#34;, databaseName, overwrite = true, discoverPartitions = true)&#xA;// Or, if you want to create temporary tables&#xA;// tables.createTemporaryTables(location, format)&#xA;&#xA;// For CBO only, gather statistics on all columns:&#xA;tables.analyzeTables(databaseName, analyzeColumns = true) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run benchmarking queries&lt;/h2&gt; &#xA;&lt;p&gt;After setup, users can use &lt;code&gt;runExperiment&lt;/code&gt; function to run benchmarking queries and record query execution time. Taking TPC-DS as an example, you can start an experiment by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import com.databricks.spark.sql.perf.tpcds.TPCDS&#xA;&#xA;val tpcds = new TPCDS (sqlContext = sqlContext)&#xA;// Set:&#xA;val databaseName = ... // name of database with TPCDS data.&#xA;val resultLocation = ... // place to write results&#xA;val iterations = 1 // how many iterations of queries to run.&#xA;val queries = tpcds.tpcds2_4Queries // queries to run.&#xA;val timeout = 24*60*60 // timeout, in seconds.&#xA;// Run:&#xA;sql(s&#34;use $databaseName&#34;)&#xA;val experiment = tpcds.runExperiment(&#xA;  queries, &#xA;  iterations = iterations,&#xA;  resultLocation = resultLocation,&#xA;  forkThread = true)&#xA;experiment.waitForFinish(timeout)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, experiment will be started in a background thread. For every experiment run (i.e. every call of &lt;code&gt;runExperiment&lt;/code&gt;), Spark SQL Perf will use the timestamp of the start time to identify this experiment. Performance results will be stored in the sub-dir named by the timestamp in the given &lt;code&gt;spark.sql.perf.results&lt;/code&gt; (for example &lt;code&gt;/tmp/results/timestamp=1429213883272&lt;/code&gt;). The performance results are stored in the JSON format.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieve results&lt;/h2&gt; &#xA;&lt;p&gt;While the experiment is running you can use &lt;code&gt;experiment.html&lt;/code&gt; to get a summary, or &lt;code&gt;experiment.getCurrentResults&lt;/code&gt; to get complete current results. Once the experiment is complete, you can still access &lt;code&gt;experiment.getCurrentResults&lt;/code&gt;, or you can load the results from disk.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Get all experiments results.&#xA;val resultTable = spark.read.json(resultLocation)&#xA;resultTable.createOrReplaceTempView(&#34;sqlPerformance&#34;)&#xA;sqlContext.table(&#34;sqlPerformance&#34;)&#xA;// Get the result of a particular run by specifying the timestamp of that run.&#xA;sqlContext.table(&#34;sqlPerformance&#34;).filter(&#34;timestamp = 1429132621024&#34;)&#xA;// or&#xA;val specificResultTable = spark.read.json(experiment.resultPath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get a basic summary by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experiment.getCurrentResults // or: spark.read.json(resultLocation).filter(&#34;timestamp = 1429132621024&#34;)&#xA;  .withColumn(&#34;Name&#34;, substring(col(&#34;name&#34;), 2, 100))&#xA;  .withColumn(&#34;Runtime&#34;, (col(&#34;parsingTime&#34;) + col(&#34;analysisTime&#34;) + col(&#34;optimizationTime&#34;) + col(&#34;planningTime&#34;) + col(&#34;executionTime&#34;)) / 1000.0)&#xA;  .select(&#39;Name, &#39;Runtime)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;TPC-H&lt;/h1&gt; &#xA;&lt;p&gt;TPC-H can be run similarly to TPC-DS replacing &lt;code&gt;tpcds&lt;/code&gt; for &lt;code&gt;tpch&lt;/code&gt;. Take a look at the data generator and &lt;code&gt;tpch_run&lt;/code&gt; notebook code below.&lt;/p&gt; &#xA;&lt;h2&gt;Running in Databricks workspace (or spark-shell)&lt;/h2&gt; &#xA;&lt;p&gt;There are example notebooks in &lt;code&gt;src/main/notebooks&lt;/code&gt; for running TPCDS and TPCH in the Databricks environment. &lt;em&gt;These scripts can also be run from spark-shell command line with minor modifications using &lt;code&gt;:load file_name.scala&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TPC-multi_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook (or scala script) can be use to generate both TPCDS and TPCH data at selected scale factors. It is a newer version from the &lt;code&gt;tpcds_datagen&lt;/code&gt; notebook below. To use it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit the config variables the top of the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the whole notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpcds_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to install dsdgen on all worker nodes, run data generation, and create the TPCDS database. Note that because of the way dsdgen is installed, it will not work on an autoscaling cluster, and &lt;code&gt;num_workers&lt;/code&gt; has to be updated to the number of worker instances on the cluster. Data generation may also break if any of the workers is killed - the restarted worker container will not have &lt;code&gt;dsdgen&lt;/code&gt; anymore.&lt;/p&gt; &#xA;&lt;h3&gt;tpcds_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCDS queries.&lt;/p&gt; &#xA;&lt;p&gt;For running parallel TPCDS streams:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Cluster and attach the spark-sql-perf library to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a Job using the notebook and attaching to the created cluster as &#34;existing cluster&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Allow concurrent runs of the created job.&lt;/li&gt; &#xA; &lt;li&gt;Launch appriopriate number of Runs of the Job to run in parallel on the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpch_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCH queries. Data needs be generated first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yutax77/KeyValueStore-scala</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/yutax77/KeyValueStore-scala</id>
    <link href="https://github.com/yutax77/KeyValueStore-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KeyValueStore&lt;/h1&gt; &#xA;&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/devbox</title>
    <updated>2022-05-31T02:21:26Z</updated>
    <id>tag:github.com,2022-05-31:/databricks/devbox</id>
    <link href="https://github.com/databricks/devbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Databricks main line of development is now in the monorepo. Please see &lt;code&gt;devtools/devbox&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Devbox syncer&lt;/h1&gt; &#xA;&lt;p&gt;A one-way sync from laptop to an EC2 instance.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;To prepare an assembly jar, ready to be tested and deployed in the universe/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill launcher.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result can be found in &lt;code&gt;out/launcher/assembly/dest/out.jar&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run all tests (takes a long time):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill devbox.test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive console (REPL)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill -i devbox.repl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://github.com/databricks/devbox/actions?query=workflow%3ARelease&#34;&gt;Github Action&lt;/a&gt; to release Devbox.&lt;/p&gt; &#xA;&lt;p&gt;Just run the workflow on the target branch (usually master) with the new version number and check the &lt;a href=&#34;https://github.com/databricks/devbox/releases&#34;&gt;releases&lt;/a&gt; page&lt;/p&gt;</summary>
  </entry>
</feed>