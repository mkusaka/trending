<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-30T02:21:00Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>zio/zio</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/zio/zio</id>
    <link href="https://github.com/zio/zio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZIO — A type-safe, composable library for async and concurrent programming in Scala&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/ZIO.png&#34; alt=&#34;ZIO Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Production%20Ready-brightgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Snapshot Artifacts&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://isitmaintained.com/project/zio/zio&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/zio/zio.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scaladoc&lt;/th&gt; &#xA;   &lt;th&gt;Scaladex&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;   &lt;th&gt;Twitter&lt;/th&gt; &#xA;   &lt;th&gt;Gitpod&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://javadoc.io/doc/dev.zio/zio_2.12/latest/zio/index.html&#34;&gt;Scaladoc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://index.scala-lang.org/zio/zio/zio&#34; title=&#34;Scaladex&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/zio/zio/zio/latest.svg?sanitize=true&#34; alt=&#34;Badge-Scaladex-page&#34; title=&#34;Scaladex&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/zioscala&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/zioscala.svg?style=plastic&amp;amp;label=follow&amp;amp;logo=twitter&#34; alt=&#34;Badge-Twitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/zio/zio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod ready-to-code&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Welcome to ZIO&lt;/h1&gt; &#xA;&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt; &#xA;&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala&#39;s &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it&#39;s asynchronous or synchronous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/contributing.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/pulls&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Adopters&lt;/h1&gt; &#xA;&lt;p&gt;Following is a partial list of companies happily using ZIO in production to craft concurrent applications.&lt;/p&gt; &#xA;&lt;p&gt;Want to see your company here? &lt;a href=&#34;https://github.com/zio/zio/edit/master/README.md&#34;&gt;Submit a PR&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adgear.com/en/&#34;&gt;AdGear / Samsung Ads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adidas.com/&#34;&gt;Adidas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adpulse.io/&#34;&gt;adpulse.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adsquare.com/&#34;&gt;adsquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ayolab.com/&#34;&gt;Ayolab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://asana.com/&#34;&gt;Asana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aurinko.io/&#34;&gt;Aurinko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://auto.ru&#34;&gt;auto.ru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.de&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.avast.com&#34;&gt;Avast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bofa.com&#34;&gt;Bank of America&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bpp.it/&#34;&gt;Bpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://broad.app&#34;&gt;Broad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.caesars.com/sportsbook-and-casino&#34;&gt;Caesars Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.calcbank.com.br&#34;&gt;CalcBank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.callhandling.co.uk/&#34;&gt;Call Handling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;Carvana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cellular.de&#34;&gt;Cellular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudfarms.com&#34;&gt;Cloudfarms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codecomprehension.com&#34;&gt;CodeComprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codept.de/&#34;&gt;Codept&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com/en&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collibra.com/&#34;&gt;Collibra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.compellon.com/&#34;&gt;Compellon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complicatedrobot.com/&#34;&gt;Complicated Robot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conduktor.io&#34;&gt;Conduktor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.contramap.dev&#34;&gt;Contramap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coralogix.com&#34;&gt;Coralogix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creditkarma.com&#34;&gt;Credit Karma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.currencycloud.com/&#34;&gt;CurrencyCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://de-solution.com/&#34;&gt;D.E.Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datachef.co&#34;&gt;DataChef&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.demandbase.com&#34;&gt;Demandbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demyst.com&#34;&gt;Demyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devsisters.com/&#34;&gt;Devsisters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijdhl.nl/it&#34;&gt;DHL Parcel The Netherlands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+ Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doomoolmori.com/&#34;&gt;Doomoolmori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dowjones.com&#34;&gt;Dow Jones&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dpgrecruitment.nl&#34;&gt;DPG recruitment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.com&#34;&gt;Dream11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iot.telekom.com/en&#34;&gt;Deutsche Telekom IoT GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebay.com&#34;&gt;eBay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eaglescience.nl&#34;&gt;Eaglescience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edf.fr/&#34;&gt;Electricité de France (EDF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.enelx.com&#34;&gt;EnelX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evolution.engineering&#34;&gt;Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evo.company&#34;&gt;Evo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flipp.com/&#34;&gt;Flipp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fugo.ai&#34;&gt;Fugo.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.garnercorp.com/&#34;&gt;Garner Distributed Workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gleancompany.com&#34;&gt;Glean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://grandparade.co.uk&#34;&gt;GrandParade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://greyflower.media&#34;&gt;greyflower.media GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hunters.ai&#34;&gt;Hunters.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypefactors.com/&#34;&gt;Hypefactors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.iheart.com/&#34;&gt;iHeartRadio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ihsmarkit.com/&#34;&gt;IHS Markit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://investsuite.com/&#34;&gt;Investsuite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaizen-solutions.net/&#34;&gt;Kaizen Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kamon.io/&#34;&gt;Kamon APM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kodmagi.se&#34;&gt;Kodmagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kensu.io&#34;&gt;Kensu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lambdaworks.io/&#34;&gt;LambdaWorks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leadiq.com&#34;&gt;LeadIQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lernkunst.com/&#34;&gt;Lernkunst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liveintent.com&#34;&gt;LiveIntent Inc.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lottoland.com&#34;&gt;Lottoland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matechs.com&#34;&gt;MATECHS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://megogo.net&#34;&gt;Megogo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mylivn.com/&#34;&gt;Mylivn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newmotion.com&#34;&gt;NewMotion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nexxchange.com&#34;&gt;Nexxchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nike.com&#34;&gt;Nike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nslookup.io&#34;&gt;NsLookup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://olyro.de&#34;&gt;Olyro GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optrak.com&#34;&gt;Optrak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.performance-immo.com/&#34;&gt;Performance Immo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playtika.com&#34;&gt;Playtika&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ppcsamurai.com/&#34;&gt;PPC Samurai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.radix.bio/&#34;&gt;Radix Labs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railroad19.com&#34;&gt;Railroad19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijrandstad.nl&#34;&gt;Randstad Groep Nederland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rapidor.co&#34;&gt;Rapidor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pimsolutions.ru/&#34;&gt;PIM Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rewe-digital.com/&#34;&gt;REWE Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://riskident.com/&#34;&gt;Risk Ident&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocker.com/&#34;&gt;Rocker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rudder.io/&#34;&gt;Rudder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sanjagh.pro/&#34;&gt;Sanjagh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scalac.io/&#34;&gt;Scalac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityscorecard.io/&#34;&gt;SecurityScorecard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sentinelone.com/&#34;&gt;SentinelOne&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.signicat.com/&#34;&gt;Signicat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.sgmarkets.com/en/&#34;&gt;Société Générale Corporate and Investment Banking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaremill.com/&#34;&gt;SoftwareMill&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.streamweaver.com/&#34;&gt;StreamWeaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stuart.com/&#34;&gt;Stuart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teads.com&#34;&gt;Teads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pokemon.com/us/about-pokemon/&#34;&gt;The Pokemon Company International&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tomtom.com&#34;&gt;TomTom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tinka.com/&#34;&gt;Tinka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trackabus.com&#34;&gt;Trackabus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trainor.no&#34;&gt;Trainor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://treutech.io&#34;&gt;TreuTech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tweddle.com&#34;&gt;Tweddle Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.undo.app&#34;&gt;Undo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unit.co&#34;&gt;Unit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univalence.io&#34;&gt;Univalence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.unzer.com&#34;&gt;Unzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vakantiediscounter.nl&#34;&gt;Vakantiediscounter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.verbund.com&#34;&gt;Verbund AG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.waylay.io/&#34;&gt;Waylay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wehkamp.nl&#34;&gt;Wehkamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wolt.com/&#34;&gt;Wolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://o.yandex.ru&#34;&gt;Yandex.Classifieds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://audela.ca&#34;&gt;Audela&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valamis.com&#34;&gt;Valamis Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valsea.com&#34;&gt;Valsea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtuslab.com/&#34;&gt;VirtusLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://getvish.com&#34;&gt;Vish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vivid.money&#34;&gt;Vivid Money&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.com/&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zooz.com/&#34;&gt;Zooz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/ziverge.png&#34; alt=&#34;Ziverge&#34; title=&#34;Ziverge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;Ziverge&lt;/a&gt; is a leading contributor to ZIO.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/scalac.svg?sanitize=true&#34; alt=&#34;Scalac&#34; title=&#34;Scalac&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;Scalac&lt;/a&gt; sponsors ZIO Hackathons and contributes work to multiple projects in ZIO ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/septimal_mind.svg?sanitize=true&#34; alt=&#34;Septimal Mind&#34; title=&#34;Septimal Mind&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/softwaremill.svg?sanitize=true&#34; alt=&#34;SoftwareMill&#34; title=&#34;SoftwareMill&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/yourkit.png&#34; alt=&#34;YourKit&#34; title=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;YourKit&lt;/a&gt; generously provides use of their monitoring and profiling tools to maximize the performance of ZIO applications.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/code_of_conduct.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Come chat with us on &lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Legal&lt;/h3&gt; &#xA;&lt;p&gt;Copyright 2017 - 2020 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-kyuubi</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/apache/incubator-kyuubi</id>
    <link href="https://github.com/apache/incubator-kyuubi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Kyuubi is a distributed multi-tenant JDBC server for large-scale data processing and analytics, built on top of Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Kyuubi (Incubating)&lt;/h1&gt; &#xA;&lt;img src=&#34;https://svn.apache.org/repos/asf/comdev/project-logos/originals/kyuubi-1.svg?sanitize=true&#34; alt=&#34;Kyuubi logo&#34; height=&#34;120px&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/apache/incubator-kyuubi?label=release&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://tokei.rs/b1/github.com/apache/incubator-kyuubi&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/incubator-kyuubi/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/apache/incubator-kyuubi/Kyuubi/master?style=plastic&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;a href=&#34;https://travis-ci.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/incubator-kyuubi.svg?branch=master&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/kyuubi/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/apache/incubator-kyuubi&#34; alt=&#34;GitHub top language&#34;&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/apache/incubator-kyuubi&#34; alt=&#34;Commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Kyuubi?&lt;/h2&gt; &#xA;&lt;p&gt;Kyuubi is a distributed multi-tenant Thrift JDBC/ODBC server for large-scale data management, processing, and analytics, built on top of Apache Spark and designed to support more engines (i.e., Flink). It has been open-sourced by NetEase since 2018. We are aiming to make Kyuubi an &#34;out-of-the-box&#34; tool for data warehouses and data lakes.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi provides a pure SQL gateway through Thrift JDBC/ODBC interface for end-users to manipulate large-scale data with pre-programmed and extensible Spark SQL engines. This &#34;out-of-the-box&#34; model minimizes the barriers and costs for end-users to use Spark at the client side. At the server-side, Kyuubi server and engines&#39; multi-tenant architecture provides the administrators a way to achieve computing resource isolation, data security, high availability, high client concurrency, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_positioning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A HiveServer2-like API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-tenant Spark Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Running Spark in a serverless way&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target Users&lt;/h3&gt; &#xA;&lt;p&gt;Kyuubi&#39;s goal is to make it easy and efficient for &lt;code&gt;anyone&lt;/code&gt; to use Spark(maybe other engines soon) and facilitate users to handle big data like ordinary data. Here, &lt;code&gt;anyone&lt;/code&gt; means that users do not need to have a Spark technical background but a human language, SQL only. Sometimes, SQL skills are unnecessary when integrating Kyuubi with Apache Superset, which supports rich visualizations and dashboards.&lt;/p&gt; &#xA;&lt;p&gt;In typical big data production environments with Kyuubi, there should be system administrators and end-users.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System administrators: A small group consists of Spark experts responsible for Kyuubi deployment, configuration, and tuning.&lt;/li&gt; &#xA; &lt;li&gt;End-users: Focus on business data of their own, not where it stores, how it computes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, the Kyuubi community will continuously optimize the whole system with various features, such as History-Based Optimizer, Auto-tuning, Materialized View, SQL Dialects, Functions, e.t.c.&lt;/p&gt; &#xA;&lt;h3&gt;Usage scenarios&lt;/h3&gt; &#xA;&lt;h4&gt;Port workloads from HiveServer2 to Spark SQL&lt;/h4&gt; &#xA;&lt;p&gt;In typical big data production environments, especially secured ones, all bundled services manage access control lists to restricting access to authorized users. For example, Hadoop YARN divides compute resources into queues. With Queue ACLs, it can identify and control which users/groups can take actions on particular queues. Similarly, HDFS ACLs control access of HDFS files by providing a way to set different permissions for specific users/groups.&lt;/p&gt; &#xA;&lt;p&gt;Apache Spark is a unified analytics engine for large-scale data processing. It provides a Distributed SQL Engine, a.k.a, the Spark Thrift Server(STS), designed to be seamlessly compatible with HiveServer2 and get even better performance.&lt;/p&gt; &#xA;&lt;p&gt;HiveServer2 can identify and authenticate a caller, and then if the caller also has permissions for the YARN queue and HDFS files, it succeeds. Otherwise, it fails. However, on the one hand, STS is a single Spark application. The user and queue to which STS belongs are uniquely determined at startup. Consequently, STS cannot leverage cluster managers such as YARN and Kubernetes for resource isolation and sharing or control the access for callers by the single user inside the whole system. On the other hand, the Thrift Server is coupled in the Spark driver&#39;s JVM process. This coupled architect puts a high risk on server stability and makes it unable to handle high client concurrency or apply high availability such as load balancing as it is stateful.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi extends the use of STS in a multi-tenant model based on a unified interface and relies on the concept of multi-tenancy to interact with cluster managers to finally gain the ability of resources sharing/isolation and data security. The loosely coupled architecture of the Kyuubi server and engine dramatically improves the client concurrency and service stability of the service itself.&lt;/p&gt; &#xA;&lt;h4&gt;DataLake/LakeHouse Support&lt;/h4&gt; &#xA;&lt;p&gt;The vision of Kyuubi is to unify the portal and become an easy-to-use data lake management platform. Different kinds of workloads, such as ETL processing and BI analytics, can be supported by one platform, using one copy of data, with one SQL interface.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logical View support via Kyuubi DataLake Metadata APIs&lt;/li&gt; &#xA; &lt;li&gt;Multiple Catalogs support&lt;/li&gt; &#xA; &lt;li&gt;SQL Standard Authorization support for DataLake(coming)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Cloud Native Support&lt;/h4&gt; &#xA;&lt;p&gt;Kyuubi can deploy its engines on different kinds of Cluster Managers, such as, Hadoop YARN, Kubernetes, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_migrating_yarn_to_k8s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The Kyuubi Ecosystem(present and future)&lt;/h3&gt; &#xA;&lt;p&gt;The figure below shows our vision for the Kyuubi Ecosystem. Some of them have been realized, some in development, and others would not be possible without your help.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_ecosystem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Since Kyuubi 1.3.0-incubating, the Kyuubi online documentation is hosted by &lt;a href=&#34;https://kyuubi.apache.org/&#34;&gt;https://kyuubi.apache.org/&lt;/a&gt;. You can find the latest Kyuubi documentation on &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;this web page&lt;/a&gt;. For 1.2 and earlier versions, please check the &lt;a href=&#34;https://kyuubi.readthedocs.io/en/v1.2.0/&#34;&gt;Readthedocs&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ready? &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/quick_start/quick_start.html&#34;&gt;Getting Started&lt;/a&gt; with Kyuubi.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Contributor over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://api7.ai/contributor-graph?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://contributor-graph-api.apiseven.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34; alt=&#34;Contributor over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Aside&lt;/h2&gt; &#xA;&lt;p&gt;The project took its name from a character of a popular Japanese manga - &lt;code&gt;Naruto&lt;/code&gt;. The character is named &lt;code&gt;Kyuubi Kitsune/Kurama&lt;/code&gt;, which is a nine-tailed fox in mythology. &lt;code&gt;Kyuubi&lt;/code&gt; spread the power and spirit of fire, which is used here to represent the powerful &lt;a href=&#34;http://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;. Its nine tails stand for end-to-end multi-tenancy support of this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>flix/flix</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/flix/flix</id>
    <link href="https://github.com/flix/flix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Flix Programming Language&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/logo.png&#34; height=&#34;91px&#34; alt=&#34;The Flix Programming Language&#34; title=&#34;The Flix Programming Language&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flix&lt;/strong&gt; is a statically typed functional, imperative, and logic programming language.&lt;/p&gt; &#xA;&lt;p&gt;We refer you to the &lt;a href=&#34;https://flix.dev/&#34;&gt;official Flix website (flix.dev)&lt;/a&gt; for more information about Flix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/flix/Lobby&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/example.png&#34; height=&#34;627px&#34; alt=&#34;Example Flix Program&#34; title=&#34;Example Flix Program&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/BUILD.md&#34;&gt;docs/BUILD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Flix is available under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We kindly thank &lt;a href=&#34;https://www.ej-technologies.com/&#34;&gt;EJ Technologies&lt;/a&gt; for providing us with &lt;a href=&#34;http://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler&lt;/a&gt; and &lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt; for providing us with &lt;a href=&#34;https://www.jetbrains.com/idea/&#34;&gt;IntelliJ IDEA&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lampepfl/dotty</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/lampepfl/dotty</id>
    <link href="https://github.com/lampepfl/dotty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/lampepfl/dotty/workflows/Dotty/badge.svg?branch=master&#34; alt=&#34;Dotty CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632150470000902164&#34; alt=&#34;Join the chat at https://discord.com/invite/scala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Try it out&lt;/h1&gt; &#xA;&lt;p&gt;To try it in your project see also the &lt;a href=&#34;https://docs.scala-lang.org/scala3/getting-started.html&#34;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt dist/packArchive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Dotty uses the &lt;a href=&#34;https://www.scala-lang.org/conduct.html&#34;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; &#xA;&lt;h1&gt;How to Contribute&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&#34;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Dotty is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>twitter/finagle</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/twitter/finagle</id>
    <link href="https://github.com/twitter/finagle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fault tolerant, protocol-agnostic RPC system&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/twitter/finagle/raw/develop/doc/src/sphinx/_static/logo_medium.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Finagle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/finagle/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/finagle/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/finagle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/finagle/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;https://twitter.github.io/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/finagle/&#34;&gt;https://github.com/twitter/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://finagle.github.io/blog/&#34;&gt;https://finagle.github.io/blog/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols.&lt;/p&gt; &#xA;&lt;p&gt;For extensive documentation, please see the &lt;a href=&#34;https://twitter.github.io/finagle/guide/&#34;&gt;user guide&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/docs/com/twitter/finagle&#34;&gt;API documentation&lt;/a&gt; websites. Documentation improvements are always welcome, so please send patches our way.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The following are a few of the companies that are using Finagle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://foursquare.com/&#34;&gt;Foursquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ing.nl&#34;&gt;ING Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinterest.com/&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://soundcloud.com/&#34;&gt;SoundCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tumblr.com/&#34;&gt;Tumblr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a more complete list, please see &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/ADOPTERS.md&#34;&gt;our adopter page&lt;/a&gt;. If your organization is using Finagle, consider adding a link there and sending us a pull request!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2010 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azure-event-hubs-spark</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/Azure/azure-event-hubs-spark</id>
    <link href="https://github.com/Azure/azure-event-hubs-spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enabling Continuous Data Processing with Apache Spark and Azure Event Hubs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/event-hubs_spark.png&#34; alt=&#34;Azure Event Hubs + Apache Spark Connector&#34; width=&#34;270&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Azure Event Hubs Connector for Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.png&#34; alt=&#34;chat on gitter&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark.svg?branch=master&#34; alt=&#34;build status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#star-our-repo&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/azure/azure-event-hubs-spark.svg?style=social&amp;amp;label=Stars&#34; alt=&#34;star our repo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the source code of the Azure Event Hubs Connector for Apache Spark.&lt;/p&gt; &#xA;&lt;p&gt;Azure Event Hubs is a highly scalable publish-subscribe service that can ingest millions of events per second and stream them into multiple applications. Spark Streaming and Structured Streaming are scalable and fault-tolerant stream processing engines that allow users to process huge amounts of data using complex algorithms expressed with high-level functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, &lt;code&gt;join&lt;/code&gt;, and &lt;code&gt;window&lt;/code&gt;. This data can then be pushed to filesystems, databases, or even back to Event Hubs.&lt;/p&gt; &#xA;&lt;p&gt;By making Event Hubs and Spark easier to use together, we hope this connector makes building scalable, fault-tolerant applications easier for our users.&lt;/p&gt; &#xA;&lt;h2&gt;Latest Releases&lt;/h2&gt; &#xA;&lt;h4&gt;Spark&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark Version&lt;/th&gt; &#xA;   &lt;th&gt;Package Name&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 3.0&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Databricks&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Databricks Runtime Version&lt;/th&gt; &#xA;   &lt;th&gt;Artifact Id&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 8.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 7.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 6.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Roadmap&lt;/h4&gt; &#xA;&lt;p&gt;There is an open issue for each planned feature/enhancement.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;We maintain an &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt; - reach out to us via &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark/Lobby&#34;&gt;gitter&lt;/a&gt; if you think anything needs to be added or clarified!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Linking&lt;/h3&gt; &#xA;&lt;p&gt;For Scala/Java applications using SBT/Maven project definitions, link your application with the artifact below. &lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#latest-releases&#34;&gt;Latest Releases&lt;/a&gt; to find the correct artifact for your version of Apache Spark (or Databricks)!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.11&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.12&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Documentation for our connector can be found &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/docs/&#34;&gt;here&lt;/a&gt;. The integration guides there contain all the information you need to use this library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;re new to Apache Spark and/or Event Hubs, then we highly recommend reading their documentation first.&lt;/strong&gt; You can read Event Hubs documentation &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-what-is-event-hubs&#34;&gt;here&lt;/a&gt;, documentation for Spark Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;, and, the last but not least, Structured Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Further Assistance&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you need additional assistance, please don&#39;t hesitate to ask!&lt;/strong&gt; General questions and discussion should happen on our &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt;gitter chat&lt;/a&gt;. Please open an issue for bug reports and feature requests! Feedback, feature requests, bug reports, etc are all welcomed!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to help contribute (we&#39;d love to have your help!), then go to our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Build Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;In order to use the connector, you need to have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 1.8 SDK installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maven.apache.org/download.cgi&#34;&gt;Maven 3.x&lt;/a&gt; installed (or &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/index.html&#34;&gt;SBT version 1.x&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details on building from source and running tests can be found in our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build Command&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Builds jar and runs all tests&#xA;mvn clean package&#xA;&#xA;// Builds jar, runs all tests, and installs jar to your local maven repository&#xA;mvn clean install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>polomarcus/tp</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/polomarcus/tp</id>
    <link href="https://github.com/polomarcus/tp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practices - Data engineering&lt;/h1&gt; &#xA;&lt;h2&gt;Tools you need&lt;/h2&gt; &#xA;&lt;p&gt;Have a stackoverflow account : &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;https://stackoverflow.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a github account : &lt;a href=&#34;https://github.com/&#34;&gt;https://github.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a github repo to push your code.&lt;/p&gt; &#xA;&lt;h3&gt;Fork the repo on your own Github account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/tp/fork&#34;&gt;https://github.com/polomarcus/tp/fork&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker and Compose&lt;/h3&gt; &#xA;&lt;p&gt;Take time to read and install&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/overview/&#34;&gt;https://docs.docker.com/get-started/overview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker --version&#xA;Docker version 20.10.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;https://docs.docker.com/compose/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose --version&#xA;docker-compose version 1.29.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TP1 - &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-docker-kafka-bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP2 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-functional-programming-scala&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP3 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-data-processing-framework&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP 4 - Kafka Streams to read and write to Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34;&gt;https://kafka.apache.org/documentation/streams/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&#34;&gt;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka User Interface (UI) : &lt;a href=&#34;https://www.conduktor.io/download/&#34;&gt;https://www.conduktor.io/download/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>apache/openwhisk</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/apache/openwhisk</id>
    <link href="https://github.com/apache/openwhisk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenWhisk is an open source serverless cloud platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenWhisk&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/github/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://travis-ci.com/apache/openwhisk.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openwhisk-team.slack.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-slack-9B69A0.svg?sanitize=true&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/openwhisk/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=openwhisk&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/openwhisk.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. Learn more at &lt;a href=&#34;http://openwhisk.apache.org&#34;&gt;http://openwhisk.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; (Deploy and Use OpenWhisk on your machine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#deploy-to-kubernetes&#34;&gt;Deploy to Kubernetes&lt;/a&gt; (For development and production)&lt;/li&gt; &#xA; &lt;li&gt;For project contributors and Docker deployments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/macos/README.md&#34;&gt;Deploy to Docker for Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/ubuntu-setup/README.md&#34;&gt;Deploy to Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#learn-concepts-and-commands&#34;&gt;Learn Concepts and Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#openwhisk-community-and-support&#34;&gt;OpenWhisk Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#project-repository-structure&#34;&gt;Project Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start using OpenWhisk is to install the &#34;Standalone&#34; OpenWhisk stack. This is a full-featured OpenWhisk stack running as a Java process for convenience. Serverless functions run within Docker containers. You will need &lt;a href=&#34;https://docs.docker.com/install&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://java.com/en/download/help/download_options.xml&#34;&gt;Java&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; available on your machine.&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk.git&#xA;cd openwhisk&#xA;./gradlew core:standalone:bootRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When the OpenWhisk stack is up, it will open your browser to a functions &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/playground-ui.png&#34;&gt;Playground&lt;/a&gt;, typically served from &lt;a href=&#34;http://localhost:3232&#34;&gt;http://localhost:3232&lt;/a&gt;. The Playground allows you create and run functions directly from your browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called &lt;code&gt;wsk&lt;/code&gt; which you can download from &lt;a href=&#34;https://s.apache.org/openwhisk-cli-download&#34;&gt;https://s.apache.org/openwhisk-cli-download&lt;/a&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/cli.md&#34;&gt;CLI configuration&lt;/a&gt; for additional details. Typically you configure the CLI for Standalone OpenWhisk as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;wsk property set \&#xA;  --apihost &#39;http://localhost:3233&#39; \&#xA;  --auth &#39;23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone OpenWhisk can be configured to deploy additional capabilities when that is desirable. Additional resources are available &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/core/standalone/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deploy to Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;OpenWhisk can also be installed on a Kubernetes cluster. You can use a managed Kubernetes cluster provisioned from a public cloud provider (e.g., AKS, EKS, IKS, GKE), or a cluster you manage yourself. Additionally for local development, OpenWhisk is compatible with Minikube, and Kubernetes for Mac using the support built into Docker 18.06 (or higher).&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk-deploy-kube.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the instructions in the &lt;a href=&#34;https://github.com/apache/openwhisk-deploy-kube/raw/master/README.md&#34;&gt;OpenWhisk on Kubernetes README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn Concepts and Commands&lt;/h3&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/&#34;&gt;documentation&lt;/a&gt; to learn more. Here are some topics you may be interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/about.md&#34;&gt;System overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/README.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions.md&#34;&gt;Create and invoke actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/triggers_rules.md&#34;&gt;Create triggers and rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/packages.md&#34;&gt;Use and create packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/catalog.md&#34;&gt;Browse and use the catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/reference.md&#34;&gt;OpenWhisk system details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/feeds.md&#34;&gt;Implementing feeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions-actionloop.md&#34;&gt;Developing a runtime for a new language&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenWhisk Community and Support&lt;/h3&gt; &#xA;&lt;p&gt;Report bugs, ask questions and request features &lt;a href=&#34;https://raw.githubusercontent.com/apache/issues&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join the OpenWhisk Team on Slack &lt;a href=&#34;https://openwhisk-team.slack.com&#34;&gt;https://openwhisk-team.slack.com&lt;/a&gt; and chat with developers. To get access to our public Slack team, request an invite &lt;a href=&#34;https://openwhisk.apache.org/slack.html&#34;&gt;https://openwhisk.apache.org/slack.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Project Repository Structure&lt;/h3&gt; &#xA;&lt;p&gt;The OpenWhisk system is built from a &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/dev/modules.md&#34;&gt;number of components&lt;/a&gt;. The picture below groups the components by their GitHub repos. Please open issues for a component against the appropriate repo (if in doubt just open against the main openwhisk repo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/components_to_repos.png&#34; alt=&#34;component/repo mapping&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spark-examples/spark-scala-examples</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/spark-examples/spark-scala-examples</id>
    <link href="https://github.com/spark-examples/spark-scala-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project provides Apache Spark SQL, RDD, DataFrame and Dataset examples in Scala language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Explanation of all Spark SQL, RDD, DataFrame and Dataset examples present on this project are available at &lt;a href=&#34;https://sparkbyexamples.com/&#34;&gt;https://sparkbyexamples.com/&lt;/a&gt; , All these examples are coded in Scala language and tested in our development environment.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents (Spark Examples in Scala)&lt;/h1&gt; &#xA;&lt;h2&gt;Spark RDD Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/&#34;&gt;Create a Spark RDD using Parallelize&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-read-multiple-text-files-into-a-single-rdd/&#34;&gt;Spark – Read multiple text files into single RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-load-csv-file-into-rdd/&#34;&gt;Spark load CSV file into RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/&#34;&gt;Different ways to create Spark RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-how-to-create-an-empty-rdd/&#34;&gt;Spark – How to create an empty RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/&#34;&gt;Spark RDD Transformations with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/&#34;&gt;Spark RDD Actions with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/&#34;&gt;Spark Pair RDD Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/&#34;&gt;Spark Repartition() vs Coalesce()&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-shuffle-partitions/&#34;&gt;Spark Shuffle Partitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-persistence-storage-levels/&#34;&gt;Spark Persistence Storage Levels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-cache-and-persist-example/&#34;&gt;Spark RDD Cache and Persist with Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-broadcast-variables/&#34;&gt;Spark Broadcast Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-accumulators/&#34;&gt;Spark Accumulators Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/&#34;&gt;Convert Spark RDD to DataFrame | Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/&#34;&gt;Spark Create DataFrame with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/&#34;&gt;Spark DataFrame withColumn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/&#34;&gt;Ways to Rename column on Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/&#34;&gt;Spark – How to Drop a DataFrame/Dataset column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-where-filter/&#34;&gt;Working with Spark DataFrame Where Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/&#34;&gt;Spark SQL “case when” and “when otherwise”&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-collect/&#34;&gt;Collect() – Retrieve data from Spark RDD/DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/&#34;&gt;Spark – How to remove duplicate rows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/&#34;&gt;How to Pivot and Unpivot a Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/&#34;&gt;Spark SQL Data Types with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/&#34;&gt;Spark SQL StructType &amp;amp; StructField with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/&#34;&gt;Spark schema – explained with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-groupby-on-dataframe/&#34;&gt;Spark Groupby Example with DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-how-to-sort-dataframe-column-explained/&#34;&gt;Spark – How to Sort DataFrame column explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-join/&#34;&gt;Spark SQL Join Types with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/&#34;&gt;Spark DataFrame Union and UnionAll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/&#34;&gt;Spark map vs mapPartitions transformation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-foreachpartition-vs-foreach-explained/&#34;&gt;Spark foreachPartition vs foreach | what to use?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/&#34;&gt;Spark DataFrame Cache and Persist Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-udf/&#34;&gt;Spark SQL UDF (User Defined Functions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/&#34;&gt;Spark SQL DataFrame Array (ArrayType) Column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/&#34;&gt;Working with Spark DataFrame Map (MapType) column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/&#34;&gt;Spark SQL – Flatten Nested Struct column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/&#34;&gt;Spark – Flatten nested array to single array column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/&#34;&gt;Spark explode array and map columns to rows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/&#34;&gt;Spark SQL String Functions Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/&#34;&gt;Spark SQL Date and Time Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-array-functions/&#34;&gt;Spark SQL Array functions complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-map-functions/&#34;&gt;Spark SQL Map functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-sort-functions/&#34;&gt;Spark SQL Sort functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/&#34;&gt;Spark SQL Aggregate Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-window-functions/&#34;&gt;Spark Window Functions with Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Data Source API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&#34;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-and-write-json-file/&#34;&gt;Spark Read and Write JSON file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/&#34;&gt;Spark Read and Write Apache Parquet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-xml/&#34;&gt;Spark Read XML file using Databricks API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/&#34;&gt;Read &amp;amp; Write Avro files using Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-3-x/&#34;&gt;Using Avro Data Files From Spark SQL 2.3.x or earlier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/&#34;&gt;Spark Read from &amp;amp; Write to HBase table | Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/create-spark-dataframe-from-hbase-using-hortonworks/&#34;&gt;Create Spark DataFrame from HBase using Hortonworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/&#34;&gt;Spark Read ORC file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/&#34;&gt;Spark 3.0 Read Binary File into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Streaming &amp;amp; Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-outputmode/&#34;&gt;Spark Streaming – Different Output modes explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/&#34;&gt;Spark Streaming files from a directory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-from-tcp-socket/&#34;&gt;Spark Streaming – Reading data from TCP Socket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-with-kafka/&#34;&gt;Spark Streaming with Kafka Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-consume-and-produce-kafka-messages-in-avro-format/&#34;&gt;Spark Streaming – Kafka messages in Avro format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-batch-processing-produce-consume-kafka-topic/&#34;&gt;Spark SQL Batch Processing – Produce and Consume Apache Kafka Topic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>datastax/spark-cassandra-connector</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/datastax/spark-cassandra-connector</id>
    <link href="https://github.com/datastax/spark-cassandra-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DataStax Spark Cassandra Connector&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Cassandra Connector&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/datastax/spark-cassandra-connector/actions/workflows/main.yml/badge.svg?branch=master&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What&lt;/th&gt; &#xA;   &lt;th&gt;Where&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Community&lt;/td&gt; &#xA;   &lt;td&gt;Chat with us at &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;Datastax and Cassandra Q&amp;amp;A&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala Docs&lt;/td&gt; &#xA;   &lt;td&gt;Most Recent Release (3.2.0): &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;, &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/driver/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector-Driver&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latest Production Release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.2.0/jar&#34;&gt;3.2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra®.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library lets you expose Cassandra tables as Spark RDDs and Datasets/DataFrames, write Spark RDDs and Datasets/DataFrames to Cassandra tables, and execute arbitrary CQL queries in your Spark applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compatible with Apache Cassandra version 2.1 or higher (see table below)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Apache Spark 1.0 through 3.2 (&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/#version-compatibility&#34;&gt;see table below&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Scala 2.11 and 2.12&lt;/li&gt; &#xA; &lt;li&gt;Exposes Cassandra tables as Spark RDDs and Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Maps table rows to CassandraRow objects or tuples&lt;/li&gt; &#xA; &lt;li&gt;Offers customizable object mapper for mapping rows to objects of user-defined classes&lt;/li&gt; &#xA; &lt;li&gt;Saves RDDs back to Cassandra by implicit &lt;code&gt;saveToCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Delete rows and columns from cassandra by implicit &lt;code&gt;deleteFromCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Join with a subset of Cassandra data using &lt;code&gt;joinWithCassandraTable&lt;/code&gt; call for RDDs, and optimizes join with data in Cassandra when using Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Partition RDDs according to Cassandra replication using &lt;code&gt;repartitionByCassandraReplica&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Converts data types between Cassandra and Scala&lt;/li&gt; &#xA; &lt;li&gt;Supports all Cassandra data types including collections&lt;/li&gt; &#xA; &lt;li&gt;Filters rows on the server side via the CQL &lt;code&gt;WHERE&lt;/code&gt; clause&lt;/li&gt; &#xA; &lt;li&gt;Allows for execution of arbitrary CQL statements&lt;/li&gt; &#xA; &lt;li&gt;Plays nice with Cassandra Virtual Nodes&lt;/li&gt; &#xA; &lt;li&gt;Could be used in all languages supporting Datasets/DataFrames API: Python, R, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Version Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The connector project has several branches, each of which map into different supported versions of Spark and Cassandra. For previous releases the branch is named &#34;bX.Y&#34; where X.Y is the major+minor version; for example the &#34;b1.6&#34; branch corresponds to the 1.6 release. The &#34;master&#34; branch will normally contain development for the next connector release in progress.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the following branches are actively supported: 3.2.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/master&#34;&gt;master&lt;/a&gt;), 3.1.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.1&#34;&gt;b3.1&lt;/a&gt;), 3.0.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.0&#34;&gt;b3.0&lt;/a&gt;) and 2.5.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b2.5&#34;&gt;b2.5&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Spark&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra Java Driver&lt;/th&gt; &#xA;   &lt;th&gt;Minimum Java Version&lt;/th&gt; &#xA;   &lt;th&gt;Supported Scala Versions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.13&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.2&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0, 2.1, 2.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;1.5, 1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;   &lt;td&gt;1.1, 1.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0, 0.9&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*&lt;em&gt;Compatible with 2.1.X where X &amp;gt;= 5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hosted API Docs&lt;/h2&gt; &#xA;&lt;p&gt;API documentation for the Scala and Java interfaces are available online:&lt;/p&gt; &#xA;&lt;h3&gt;3.2.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.0.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.5.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/&#34;&gt;Embedded-Cassandra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;This project is available on the Maven Central Repository. For SBT to download the connector binaries, sources and javadoc, put this in your project SBT config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.datastax.spark&#34; %% &#34;spark-cassandra-connector&#34; % &#34;3.2.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default Scala version for Spark 3.0+ is 2.12 please choose the appropriate build. See the &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;FAQ&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/0_quick_start.md&#34;&gt;Quick-start guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/1_connecting.md&#34;&gt;Connecting to Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/2_loading.md&#34;&gt;Loading datasets from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/3_selection.md&#34;&gt;Server-side data selection and filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/4_mapper.md&#34;&gt;Working with user-defined case classes and tuples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/5_saving.md&#34;&gt;Saving and deleting datasets to/from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/6_advanced_mapper.md&#34;&gt;Customizing the object mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/7_java_api.md&#34;&gt;Using Connector in Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/8_streaming.md&#34;&gt;Spark Streaming with Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/10_embedded.md&#34;&gt;The spark-cassandra-connector-embedded Artifact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/11_metrics.md&#34;&gt;Performance monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/13_spark_shell.md&#34;&gt;The Spark Shell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/14_data_frames.md&#34;&gt;DataFrames&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/15_python.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/16_partitioning.md&#34;&gt;Partitioner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/17_submitting.md&#34;&gt;Submitting applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/reference.md&#34;&gt;Configuration Parameter Reference Table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Training&lt;/h2&gt; &#xA;&lt;h3&gt;DataStax Academy&lt;/h3&gt; &#xA;&lt;p&gt;DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In &lt;a href=&#34;https://academy.datastax.com/courses/ds320-analytics-with-apache-spark&#34;&gt;DS320: Analytics with Spark&lt;/a&gt;, you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Reporting Bugs&lt;/h3&gt; &#xA;&lt;p&gt;New issues may be reported using &lt;a href=&#34;https://datastax-oss.atlassian.net/browse/SPARKC/&#34;&gt;JIRA&lt;/a&gt;. Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Questions and requests for help may be submitted to the &lt;a href=&#34;https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user&#34;&gt;user mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Q/A Exchange&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;DataStax Community&lt;/a&gt; provides a free question and answer website for any and all questions relating to any DataStax Related technology. Including the Spark Cassandra Connector. Both DataStax engineers and community members frequent this board and answer questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To protect the community, all contributors are required to sign the &lt;a href=&#34;http://spark-cassandra-connector-cla.datastax.com/&#34;&gt;DataStax Spark Cassandra Connector Contribution License Agreement&lt;/a&gt;. The process is completely electronic and should only take a few minutes.&lt;/p&gt; &#xA;&lt;p&gt;To develop this project, we recommend using IntelliJ IDEA. Make sure you have installed and enabled the Scala Plugin. Open the project with IntelliJ IDEA and it will automatically create the project structure from the provided SBT configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checklist for contributing changes to the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;a href=&#34;https://datastax-oss.atlassian.net/projects/SPARKC/issues&#34;&gt;SPARKC JIRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make sure that all unit tests and integration tests pass&lt;/li&gt; &#xA; &lt;li&gt;Add an appropriate entry at the top of CHANGES.txt&lt;/li&gt; &#xA; &lt;li&gt;If the change has any end-user impacts, also include changes to the ./doc files as needed&lt;/li&gt; &#xA; &lt;li&gt;Prefix the pull request description with the JIRA number, for example: &#34;SPARKC-123: Fix the ...&#34;&lt;/li&gt; &#xA; &lt;li&gt;Open a pull-request on GitHub and await review&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To run unit and integration tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the integration tests require &lt;a href=&#34;https://github.com/riptano/ccm&#34;&gt;CCM&lt;/a&gt; to be installed on your machine. See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;By default, integration tests start up a separate, single Cassandra instance and run Spark in local mode. It is possible to run integration tests with your own Cassandra and/or Spark cluster. First, prepare a jar with testing code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test:package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then copy the generated test jar to your Spark nodes and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export IT_TEST_CASSANDRA_HOST=&amp;lt;IP of one of the Cassandra nodes&amp;gt;&#xA;export IT_TEST_SPARK_MASTER=&amp;lt;Spark Master URL&amp;gt;&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Documents&lt;/h2&gt; &#xA;&lt;p&gt;To generate the Reference Document use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt spark-cassandra-connector-unshaded/run (outputLocation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;outputLocation defaults to doc/reference.md&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2014-2017, DataStax, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>digital-asset/daml</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/digital-asset/daml</id>
    <link href="https://github.com/digital-asset/daml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Daml smart contract language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalasset.com/developers&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/digital-asset/daml/main/daml-logo.png&#34; alt=&#34;Daml logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.daml.com/getting-started/installation.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/digital-asset/daml.svg?label=Download&#34; alt=&#34;Download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digital-asset/daml/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/digitalasset/daml/_build/latest?definitionId=4&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/digitalasset/daml/_apis/build/status/digital-asset.daml?branchName=main&amp;amp;label=Build&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2022 Digital Asset (Switzerland) GmbH and/or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0&lt;/p&gt; &#xA;&lt;h1&gt;Welcome to the Daml repository!&lt;/h1&gt; &#xA;&lt;p&gt;This repository hosts all code for the &lt;a href=&#34;https://www.digitalasset.com/developers&#34;&gt;Daml smart contract language and SDK&lt;/a&gt;, originally created by &lt;a href=&#34;https://www.digitalasset.com&#34;&gt;Digital Asset&lt;/a&gt;. Daml is an open-source smart contract language for building future-proof distributed applications on a safe, privacy-aware runtime. The SDK is a set of tools to help you develop applications based on Daml.&lt;/p&gt; &#xA;&lt;h2&gt;Using Daml&lt;/h2&gt; &#xA;&lt;p&gt;To download Daml, follow &lt;a href=&#34;https://docs.daml.com/getting-started/installation.html&#34;&gt;the installation instructions&lt;/a&gt;. Once installed, to try it out, follow the &lt;a href=&#34;https://docs.daml.com/getting-started/quickstart.html&#34;&gt;quickstart guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have questions about how to use Daml or how to build Daml-based solutions, please join us on the &lt;a href=&#34;https://discuss.daml.com&#34;&gt;Daml forum&lt;/a&gt;. Alternatively, if you prefer asking on StackOverflow, please use &lt;a href=&#34;https://stackoverflow.com/tags/daml&#34;&gt;the &lt;code&gt;daml&lt;/code&gt; tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Daml&lt;/h2&gt; &#xA;&lt;p&gt;We warmly welcome &lt;a href=&#34;https://raw.githubusercontent.com/digital-asset/daml/main/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;. If you are looking for ideas on how to contribute, please browse our &lt;a href=&#34;https://github.com/digital-asset/daml/issues&#34;&gt;issues&lt;/a&gt;. To build and test Daml:&lt;/p&gt; &#xA;&lt;h3&gt;1. Clone this repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:digital-asset/daml.git&#xA;cd daml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Set up the development dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Our builds require various development dependencies (e.g. Java, Bazel, Python), provided by a tool called &lt;code&gt;dev-env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Linux&lt;/h4&gt; &#xA;&lt;p&gt;On Linux and Mac &lt;code&gt;dev-env&lt;/code&gt; can be installed with:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Nix by running: &lt;code&gt;bash &amp;lt;(curl -sSfL https://nixos.org/nix/install)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter &lt;code&gt;dev-env&lt;/code&gt; by running: &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you don&#39;t want to enter &lt;code&gt;dev-env&lt;/code&gt; manually each time using &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;, you can also install &lt;a href=&#34;https://direnv.net&#34;&gt;direnv&lt;/a&gt;. This repo already provides a &lt;code&gt;.envrc&lt;/code&gt; file, with an option to add more in a &lt;code&gt;.envrc.private&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h4&gt;Mac&lt;/h4&gt; &#xA;&lt;p&gt;On Mac &lt;code&gt;dev-env&lt;/code&gt; can be installed with:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Nix by running: &lt;code&gt;bash &amp;lt;(curl -sSfL https://nixos.org/nix/install)&lt;/code&gt; This is a &lt;em&gt;multi-user installation&lt;/em&gt; (there is no single-user installation option for macOS). Because of this, you need to configure &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt; to use Nix caches. You can add the contents of &lt;code&gt;dev-env/etc/nix.conf&lt;/code&gt; to &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt;, but keep &lt;code&gt;build-users-group = nixbld&lt;/code&gt; instead of leaving this empty as is done in &lt;code&gt;dev-env/etc/nix.conf&lt;/code&gt;. Make sure to restart the &lt;code&gt;nix-daemon&lt;/code&gt; after you have made changes to &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt;, for instance by using &lt;code&gt;sudo launchctl stop org.nixos.nix-daemon&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enter &lt;code&gt;dev-env&lt;/code&gt; by running: &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you don&#39;t want to enter &lt;code&gt;dev-env&lt;/code&gt; manually each time using &lt;code&gt;eval &#34;$(dev-env/bin/dade assist)&#34;&lt;/code&gt;, you can also install &lt;a href=&#34;https://direnv.net&#34;&gt;direnv&lt;/a&gt;. This repo already provides a &lt;code&gt;.envrc&lt;/code&gt; file, with an option to add more in a &lt;code&gt;.envrc.private&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Note that after a macOS update it can appear as if Nix is not installed. This is because macOS updates can modify shell config files in &lt;code&gt;/etc&lt;/code&gt;, which the multi-user installation of Nix modifies as well. A workaround for this problem is to add the following to your shell config file in your &lt;code&gt;$HOME&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Nix&#xA;if [ -e &#39;/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh&#39; ]; then&#xA;  . &#39;/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh&#39;&#xA;fi&#xA;# End Nix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/NixOS/nix/issues/3616&#34;&gt;https://github.com/NixOS/nix/issues/3616&lt;/a&gt; for more information about this issue.&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;On Windows you need to enable long file paths by running the following command in an admin powershell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Set-ItemProperty -Path &#39;HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem&#39; -Name LongPathsEnabled -Type DWord -Value 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to configure Bazel for Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;build --config windows&#34; &amp;gt; .bazelrc.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, if you are on a Windows ad-hoc or CI machine you can use &lt;code&gt;ci/configure-bazel.sh&lt;/code&gt; instead of performing these steps manually. In that case, you should checkout the &lt;code&gt;daml&lt;/code&gt; repository into the path &lt;code&gt;D:\a\1\s&lt;/code&gt; in order to be able to use remote cache artifacts.&lt;/p&gt; &#xA;&lt;p&gt;Then start &lt;code&gt;dev-env&lt;/code&gt; from PowerShell with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.\dev-env\windows\bin\dadew.ps1 install&#xA;.\dev-env\windows\bin\dadew.ps1 sync&#xA;.\dev-env\windows\bin\dadew.ps1 enable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In all new PowerShell processes started, you need to repeat the &lt;code&gt;enable&lt;/code&gt; step.&lt;/p&gt; &#xA;&lt;h3&gt;3. First build and test&lt;/h3&gt; &#xA;&lt;p&gt;We have a single script to build most targets and run the tests. On Linux and Mac run &lt;code&gt;./build.sh&lt;/code&gt;. On Windows run &lt;code&gt;.\build.ps1&lt;/code&gt;. Note that these scripts may take over an hour the first time.&lt;/p&gt; &#xA;&lt;p&gt;To just build do &lt;code&gt;bazel build //...&lt;/code&gt;, and to just test do &lt;code&gt;bazel test //...&lt;/code&gt;. To read more about Bazel and how to use it, see &lt;a href=&#34;https://bazel.build&#34;&gt;the Bazel site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;On Mac if building is causing trouble complaining about missing nix packages, you can try first running &lt;code&gt;nix-build -A tools -A cached nix&lt;/code&gt; repeatedly until it completes without error.&lt;/p&gt; &#xA;&lt;h3&gt;4. Installing a local copy&lt;/h3&gt; &#xA;&lt;p&gt;On Linux and Mac run &lt;code&gt;daml-sdk-head&lt;/code&gt; which installs a version of the SDK with version number &lt;code&gt;0.0.0&lt;/code&gt;. Set the &lt;code&gt;version:&lt;/code&gt; field in any Daml project to 0.0.0 and it will use the locally installed one.&lt;/p&gt; &#xA;&lt;p&gt;On Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bazel build //release:sdk-release-tarball&#xA;tar -vxf .\bazel-bin\release\sdk-release-tarball-ce.tar.gz&#xA;cd sdk-*&#xA;daml\daml.exe install . --install-assistant=yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That should tell you what to put in the path, something along the lines of &lt;code&gt;C:\Users\admin\AppData\Roaming\daml\bin&lt;/code&gt;. Note that the Windows build is not yet fully functional.&lt;/p&gt; &#xA;&lt;h3&gt;Caching: build speed and disk space considerations&lt;/h3&gt; &#xA;&lt;p&gt;Bazel has a lot of nice properties, but they come at the cost of frequently rebuilding &#34;the world&#34;. To make that bearable, we make extensive use of caching. Most artifacts should be cached in our CDN, which is configured in &lt;code&gt;.bazelrc&lt;/code&gt; in this project.&lt;/p&gt; &#xA;&lt;p&gt;However, even then, you may end up spending a lot of time (and bandwidth!) downloading artifacts from the CDN. To alleviate that, by default, our build will create a subfolder &lt;code&gt;.bazel-cache&lt;/code&gt; in this project and keep an on-disk cache. &lt;strong&gt;This can take about 10GB&lt;/strong&gt; at the time of writing.&lt;/p&gt; &#xA;&lt;p&gt;To disable the disk cache, remove the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build:linux --disk_cache=.bazel-cache&#xA;build:darwin --disk_cache=.bazel-cache&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the &lt;code&gt;.bazelrc&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;If you work with multiple copies of this repository, you can point all of them to the same disk cache by overwriting these configs in either a &lt;code&gt;.bazelrc.local&lt;/code&gt; file in each copy, or a &lt;code&gt;~/.bazelrc&lt;/code&gt; file in your home directory.&lt;/p&gt; &#xA;&lt;h3&gt;Shared memory segment issues&lt;/h3&gt; &#xA;&lt;p&gt;On macOS at least, it looks like our setup does not always properly close the resources PostgreSQL uses. After a number of test runs, you may encounter an error message along the lines of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FATAL:  could not create shared memory segment: No space left on device&#xA;DETAIL:  Failed system call was shmget(key=5432001, size=56, 03600).&#xA;HINT:  This error does *not* mean that you have run out of disk space. It occurs either if all available shared memory IDs have been taken, in which case you need to raise the SHMMNI parameter in your kernel, or because the system&#39;s overall limit for shared memory has been reached.&#xA;        The PostgreSQL documentation contains more information about shared memory configuration.&#xA;child process exited with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this case, this is a memory leak, so increasing &lt;code&gt;SHMNI&lt;/code&gt; (or &lt;code&gt;SHMALL&lt;/code&gt; etc.) as suggested will only delay the issue. You can look at the existing shared memory segments on your system by running &lt;code&gt;ipcs -mcopt&lt;/code&gt;; this will print a line per segment, indicating the process ID of the last process to connect to the segment as well as the last access time and the number of currently connected processes.&lt;/p&gt; &#xA;&lt;p&gt;If you identify segments with no connected processes, and you are confident you can remove them, you can do so with &lt;code&gt;ipcrm $sid&lt;/code&gt;, where &lt;code&gt;$sid&lt;/code&gt; is the process ID displayed (as the second column) by &lt;code&gt;ipcs&lt;/code&gt;. Not many macOS applications use shared memory segments; &lt;strong&gt;if you have verified that all the existing memory segments on your machine need to be deleted&lt;/strong&gt;, e.g. because they have all been created by PostgreSQL instances that are no longer running, here is a Bash invocation you can use to remove all shared memory segments from your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is a dangerous command. Make sure you understand what it does before running it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;for shmid in $(ipcs -m | sed 1,3d | awk &#39;{print $2}&#39; | sed &#39;$d&#39;); do ipcrm -m $shmid; done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Haskell profiling builds&lt;/h3&gt; &#xA;&lt;p&gt;To build Haskell executables with profiling enabled, pass &lt;code&gt;-c dbg&lt;/code&gt; to Bazel, e.g. &lt;code&gt;bazel build -c dbg damlc&lt;/code&gt;. If you want to build the whole SDK with profiling enabled use &lt;code&gt;daml-sdk-head --profiling&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/devbox</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/databricks/devbox</id>
    <link href="https://github.com/databricks/devbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Databricks main line of development is now in the monorepo. Please see &lt;code&gt;devtools/devbox&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Devbox syncer&lt;/h1&gt; &#xA;&lt;p&gt;A one-way sync from laptop to an EC2 instance.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;To prepare an assembly jar, ready to be tested and deployed in the universe/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill launcher.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result can be found in &lt;code&gt;out/launcher/assembly/dest/out.jar&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run all tests (takes a long time):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill devbox.test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive console (REPL)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill -i devbox.repl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://github.com/databricks/devbox/actions?query=workflow%3ARelease&#34;&gt;Github Action&lt;/a&gt; to release Devbox.&lt;/p&gt; &#xA;&lt;p&gt;Just run the workflow on the target branch (usually master) with the new version number and check the &lt;a href=&#34;https://github.com/databricks/devbox/releases&#34;&gt;releases&lt;/a&gt; page&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sjsonnet</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/databricks/sjsonnet</id>
    <link href="https://github.com/databricks/sjsonnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sjsonnet&lt;/h1&gt; &#xA;&lt;p&gt;A JVM implementation of the &lt;a href=&#34;https://jsonnet.org/&#34;&gt;Jsonnet&lt;/a&gt; configuration language.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet can be used from Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.databricks&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sjsonnet_2.13&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    new String[]{&#34;foo.jsonnet&#34;},&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.package$.MODULE$.pwd(),&#xA;    scala.None$.empty()&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#34;com.databricks&#34; %% &#34;sjsonnet&#34; % &#34;0.4.2&#34; // SBT&#xA;ivy&#34;com.databricks::sjsonnet:0.4.2&#34; // Mill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    Array(&#34;foo.jsonnet&#34;),&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.pwd, // working directory&#xA;    None&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a standalone executable assembly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&#34;&gt;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar &amp;gt; sjsonnet.jar&#xA;&#xA;$ chmod +x sjsonnet.jar&#xA;&#xA;$ ./sjsonnet.jar&#xA;error: Need to pass in a jsonnet file to evaluate&#xA;usage: sjsonnet [sjsonnet-options] script-file&#xA;&#xA;  -i, --interactive  Run Mill in interactive mode, suitable for opening REPLs and taking user input&#xA;  -n, --indent       How much to indent your output JSON&#xA;  -J, --jpath        Specify an additional library search dir (right-most wins)&#xA;  -o, --output-file  Write to the output file rather than stdout&#xA;  ...&#xA;&#xA;$ ./sjsonnet.jar foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or from Javascript:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.js &amp;gt; sjsonnet.js&#xA;&#xA;$ node&#xA;&#xA;&amp;gt; require(&#34;./sjsonnet.js&#34;)&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#34;local f = function(x) x * x; f(11)&#34;, {}, {}, &#34;&#34;, (wd, imported) =&amp;gt; null)&#xA;121&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#xA;    &#34;local f = import &#39;foo&#39;; f + &#39;bar&#39;&#34;, // code&#xA;    {}, // extVars&#xA;    {}, // tlaVars&#xA;    &#34;&#34;, // initial working directory&#xA;&#xA;    // import callback: receives a base directory and the imported path string,&#xA;    // returns a tuple of the resolved file path and file contents or file contents resolve method&#xA;    (wd, imported) =&amp;gt; [wd + &#34;/&#34; + imported, &#34;local bar = 123; bar + bar&#34;],&#xA;    // loader callback: receives the tuple from the import callback and returns the file contents&#xA;    ([path, content]) =&amp;gt; content&#xA;    )&#xA;&#39;246bar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that since Javascript does not necessarily have access to the filesystem, you have to provide an explicit import callback that you can use to resolve imports yourself (whether through Node&#39;s &lt;code&gt;fs&lt;/code&gt; module, or by emulating a filesystem in-memory)&lt;/p&gt; &#xA;&lt;h3&gt;Running deeply recursive Jsonnet programs&lt;/h3&gt; &#xA;&lt;p&gt;The depth of recursion is limited by JVM stack size. You can run Sjsonnet with increased stack size as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -Xss100m -cp sjsonnet.jar sjsonnet.SjsonnetMain foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -Xss option above is responsible for JVM stack size. Please try this if you ever run into &lt;code&gt;sjsonnet.Error: Internal Error ... Caused by: java.lang.StackOverflowError ...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is no analog of &lt;code&gt;--max-stack&lt;/code&gt;/&lt;code&gt;-s&lt;/code&gt; option of &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;. The only stack size limit is the one of the JVM.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet is implementated as an optimizing interpreter. There are roughly 4 phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Parser&lt;/code&gt;: parses an input &lt;code&gt;String&lt;/code&gt; into a &lt;code&gt;sjsonnet.Expr&lt;/code&gt;, which is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;Syntax Tree&lt;/a&gt; representing the Jsonnet document syntax, using the &lt;a href=&#34;https://github.com/lihaoyi/fastparse&#34;&gt;Fastparse&lt;/a&gt; parsing library&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.StaticOptimizer&lt;/code&gt; is a single AST transform that performs static checking, essential rewriting (e.g. assigning indices in the symbol table for variables) and optimizations. The result is another &lt;code&gt;sjsonnet.Expr&lt;/code&gt; per input file that can be stored in the parse cache and reused.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Evaluator&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Expr&lt;/code&gt; produced by the optimizer and converts it into a &lt;code&gt;sjsonnet.Val&lt;/code&gt;, a data structure representing the Jsonnet runtime values (basically lazy JSON which can contain function values).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Materializer&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Val&lt;/code&gt; and converts it into an output &lt;code&gt;ujson.Expr&lt;/code&gt;: a non-lazy JSON structure without any remaining un-evaluated function values. This can be serialized to a string formatted in a variety of ways&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These three phases are encapsulated in the &lt;code&gt;sjsonnet.Interpreter&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some notes on the values used in parts of the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Expr&lt;/code&gt;: this represents &lt;code&gt;{...}&lt;/code&gt; object literal nodes, &lt;code&gt;a + b&lt;/code&gt; binary operation nodes, &lt;code&gt;function(a) {...}&lt;/code&gt; definitions and &lt;code&gt;f(a)&lt;/code&gt; invocations, etc.. Also keeps track of source-offset information so failures can be correlated with line numbers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Val&lt;/code&gt;: essentially the JSON structure (objects, arrays, primitives) but with two modifications. The first is that functions like &lt;code&gt;function(a){...}&lt;/code&gt; can still be present in the structure: in Jsonnet you can pass around functions as values and call then later on. The second is that object values &amp;amp; array entries are &lt;em&gt;lazy&lt;/em&gt;: e.g. &lt;code&gt;[error 123, 456][1]&lt;/code&gt; does not raise an error because the first (erroneous) entry of the array is un-used and thus not evaluated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classes representing literals extend &lt;code&gt;sjsonnet.Val.Literal&lt;/code&gt; which in turn extends &lt;em&gt;both&lt;/em&gt;, &lt;code&gt;Expr&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt;. This allows the evaluator to skip over them instead of having to convert them from one representation to the other.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Due to pervasive caching, sjsonnet is much faster than google/jsonnet. See this blog post for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2018/10/12/writing-a-faster-jsonnet-compiler.html&#34;&gt;Writing a Faster Jsonnet Compiler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the latest set of benchmarks I&#39;ve run comparing Sjsonnet against google/jsonnet and google/go-jsonnet, measuring the time taken to&lt;br&gt; evaluate the &lt;code&gt;test_suite/&lt;/code&gt; folder (smaller is better):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.5&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.26ms ± 0.22&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.59ms ± 0.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.07ms ± 0.30&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.29ms ± 0.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/jsonnet&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/go-jsonnet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1277ms&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~274ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;google/jsonnet was built from source on commit f59758d1904bccda99598990f582dd2e1e9ad263, while google/go-jsonnet was &lt;code&gt;go get&lt;/code&gt;ed at version &lt;code&gt;v0.13.0&lt;/code&gt;. You can see the source code of the benchmark in&lt;br&gt; &lt;a href=&#34;https://github.com/databricks/sjsonnet/raw/master/sjsonnet/test/src-jvm/sjsonnet/SjsonnetTestMain.scala&#34;&gt;SjsonnetTestMain.scala&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sjsonnet 0.4.0 and 0.4.1 further improve the performance significantly on our internal benchmarks. A set of new JMH benchmarks provide detailed performance data of an entire run (&lt;code&gt;MainBenchmark&lt;/code&gt;) and the non-evaluation-related parts (&lt;code&gt;MaterializerBenchmark&lt;/code&gt;, &lt;code&gt;OptimizerBenchmark&lt;/code&gt;, &lt;code&gt;ParserBenchmark&lt;/code&gt;). They can be run from the (JVM / Scala 2.13 only) sbt build. The Sjsonnet profiler is located in the same sbt project:&lt;/p&gt; &#xA;&lt;p&gt;The Sjsonnet command line which is run by all of these is defined in &lt;code&gt;MainBenchmark.mainArgs&lt;/code&gt;. You need to change it to point to a suitable input before running a benchmark or the profiler. (For Databricks employees who want to reproduce our benchmarks, the pre-configured command line is expected to be run against databricks/universe @ 7cbd8d7cb071983077d41fcc34f0766d0d2a247d).&lt;/p&gt; &#xA;&lt;p&gt;Benchmark example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/jmh:run -jvmArgs &#34;-XX:+UseStringDeduplication&#34; sjsonnet.MainBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profiler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Laziness&lt;/h2&gt; &#xA;&lt;p&gt;The Jsonnet language is &lt;em&gt;lazy&lt;/em&gt;: expressions don&#39;t get evaluated unless their value is needed, and thus even erroneous expressions do not cause a failure if un-used. This is represented in the Sjsonnet codebase by &lt;code&gt;sjsonnet.Lazy&lt;/code&gt;: a wrapper type that encapsulates an arbitrary computation that returns a &lt;code&gt;sjsonnet.Val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sjsonnet.Lazy&lt;/code&gt; is used in several places, representing where laziness is present in the language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Scope&lt;/code&gt;, representing local variable name bindings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Arr&lt;/code&gt;, representing the contents of array cells&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Obj&lt;/code&gt;, representing the contents of object values&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Val&lt;/code&gt; extends &lt;code&gt;Lazy&lt;/code&gt; so that an already computed value can be treated as lazy without having to wrap it.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet caches the results of lazy computations the first time they are evaluated, avoiding wasteful re-computation when a value is used more than once.&lt;/p&gt; &#xA;&lt;h2&gt;Standard Library&lt;/h2&gt; &#xA;&lt;p&gt;Different from &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet does not implement the Jsonnet standard library &lt;code&gt;std&lt;/code&gt; in Jsonnet code. Rather, those functions are implemented as intrinsics directly in the host language (in &lt;code&gt;Std.scala&lt;/code&gt;). This allows both better error messages when the input types are wrong, as well as better performance for the more computationally-intense builtin functions.&lt;/p&gt; &#xA;&lt;h2&gt;Client-Server&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet comes with a built in thin-client and background server, to help mitigate the unfortunate JVM warmup overhead that adds ~1s to every invocation down to 0.2-0.3s. For the simple non-client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the executable. For the client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].server.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the Sjsonnet background server lives in &lt;code&gt;~/.sjsonnet&lt;/code&gt;, and lasts 5 minutes before shutting itself when inactive.&lt;/p&gt; &#xA;&lt;p&gt;Since the Sjsonnet client still has 0.2-0.3s of overhead, if using Sjsonnet heavily it is still better to include it in your JVM classpath and invoke it programmatically via &lt;code&gt;new Interpreter(...).interpret(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing&lt;/h2&gt; &#xA;&lt;p&gt;To publish, make sure the version number in &lt;code&gt;build.sc&lt;/code&gt; is correct, then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill -i mill.scalalib.PublishModule/publishAll --sonatypeCreds lihaoyi:$SONATYPE_PASSWORD --publishArtifacts __.publishArtifacts --release true&#xA;&#xA;./mill -i show sjsonnet[2.13.4].js.fullOpt&#xA;./mill -i show sjsonnet[2.13.4].jvm.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;0.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make lazy initialization of static Val.Obj thread-safe &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/136&#34;&gt;#136&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deduplicate strings in the parser &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/137&#34;&gt;#137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update the JS example &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/141&#34;&gt;#141&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional significant performance improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/119&#34;&gt;#119&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Error handling fixes and improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/125&#34;&gt;#125&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements with lots of internal changes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/117&#34;&gt;#117&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid catching fatal exceptions during evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--yaml-debug&lt;/code&gt; flag to add source-line comments showing where each line of YAML came from &lt;a href=&#34;&#34;&gt;#105&lt;/a&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/105&#34;&gt;https://github.com/databricks/sjsonnet/pull/105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;objectValues&lt;/code&gt; and &lt;code&gt;objectVlauesAll&lt;/code&gt; to stdlib &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/104&#34;&gt;#104&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow direct YAML output generation via &lt;code&gt;--yaml-out&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Do not allow duplicate field in object when evaluating list list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/100&#34;&gt;#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix compiler crash when &#39;+&#39; signal is true in a field declaration inside a list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/98&#34;&gt;#98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix error message for too many arguments with at least one named arg &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/97&#34;&gt;#97&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming JSON output to disk for lower memory usage &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/85&#34;&gt;#85&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Static detection of duplicate fields &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/86&#34;&gt;#86&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Strict mode to disallow error-prone adjacent object literals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/88&#34;&gt;#88&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.flatMap&lt;/code&gt;, &lt;code&gt;std.repeat&lt;/code&gt;, &lt;code&gt;std.clamp&lt;/code&gt;, &lt;code&gt;std.member&lt;/code&gt;, &lt;code&gt;std.stripChars&lt;/code&gt;, &lt;code&gt;std.rstripChars&lt;/code&gt;, &lt;code&gt;std.lstripChars&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for syntactical key ordering &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/53&#34;&gt;#53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bump dependency versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump verion of Scalatags, uPickle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.9&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version of FastParse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump versions of OS-Lib, uJson, Scalatags&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support std lib methods that take a key lambda &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/40&#34;&gt;#40&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Handle hex in unicode escaoes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/41&#34;&gt;#41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add encodeUTF8, decodeUTF8 std lib methdos &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/42&#34;&gt;#42&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Properly fail on non-boolean conditionals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/44&#34;&gt;#44&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support YAML-steam output &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/45&#34;&gt;#45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~2x performance increase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javascript support, allowing Sjsonnet to be used in the browser or on Node.js&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala 2.13 support&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.mod&lt;/code&gt;, &lt;code&gt;std.min&lt;/code&gt; and &lt;code&gt;std.max&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improvements to error reporting when types do not match&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements to the parser via upgrading to Fastparse 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First release&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-csv</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/databricks/spark-csv</id>
    <link href="https://github.com/databricks/spark-csv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CSV Data Source for Apache Spark 1.x&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CSV Data Source for Apache Spark 1.x&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: This functionality has been inlined in Apache Spark 2.x. This package is in maintenance mode and we only accept critical bug fixes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-csv&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-csv.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/databricks/spark-csv?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/databricks/spark-csv/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 1.3+&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.10&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.11&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading CSV files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/1.6.0/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;. When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true the first line of files will be used to name columns and will not be included in data. All types will be assumed string. Default value is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. Delimiters inside quotes are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;parserLib&lt;/code&gt;: by default it is &#34;commons&#34; can be set to &#34;univocity&#34; to use that library for CSV parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: determines the parsing mode. By default it is PERMISSIVE. Possible values are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt;: tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt;: drops lines which have fewer or more tokens than expected or tokens which do not match the schema&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt;: aborts with a RuntimeException if encounters any malformed line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: automatically infers column types. It requires one extra pass over the data and is false by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;comment&lt;/code&gt;: skip lines beginning with this character. Default is &lt;code&gt;&#34;#&#34;&lt;/code&gt;. Disable comments by setting this to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. By default, it is &lt;code&gt;null&lt;/code&gt; which means trying to parse times and date by &lt;code&gt;java.sql.Timestamp.valueOf()&lt;/code&gt; and &lt;code&gt;java.sql.Date.valueOf()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The package also supports saving simple (non-nested) DataFrame. When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true, the header (from the schema in the DataFrame) will be written at the first line.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. This is written according to &lt;code&gt;quoteMode&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are written.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, nulls in the DataFrame will be written as this string.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use writing dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. If no dateFormat is specified, then &#34;yyyy-MM-dd HH:mm:ss.S&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quoteMode&lt;/code&gt;: when to quote fields (&lt;code&gt;ALL&lt;/code&gt;, &lt;code&gt;MINIMAL&lt;/code&gt; (default), &lt;code&gt;NON_NUMERIC&lt;/code&gt;, &lt;code&gt;NONE&lt;/code&gt;), see &lt;a href=&#34;https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/QuoteMode.html&#34;&gt;Quote Modes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These examples use a CSV file available for download &lt;a href=&#34;https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;CSV data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;, inferSchema &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars (yearMade double, carMake string, carModel string, comments string, blank string)&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .schema(customSchema)&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv.gz&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;, &#34;inferSchema&#34; -&amp;gt; &#34;true&#34;))&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    schema = customSchema,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;))&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .schema(customSchema)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, customSchema, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.SaveMode;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; saveOptions = new HashMap&amp;lt;String, String&amp;gt;();&#xA;saveOptions.put(&#34;header&#34;, &#34;true&#34;);&#xA;saveOptions.put(&#34;path&#34;, &#34;newcars.csv&#34;);&#xA;saveOptions.put(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;com.databricks.spark.csv&#34;, SaveMode.Overwrite,&#xA;                                saveOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.read \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .options(header=&#39;true&#39;) \&#xA;    .load(&#39;cars.csv&#39;, schema = customSchema)&#xA;&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).options(codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, schema = customSchema, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;customSchema &amp;lt;- structType(&#xA;    structField(&#34;year&#34;, &#34;integer&#34;),&#xA;    structField(&#34;make&#34;, &#34;string&#34;),&#xA;    structField(&#34;model&#34;, &#34;string&#34;),&#xA;    structField(&#34;comment&#34;, &#34;string&#34;),&#xA;    structField(&#34;blank&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, schema = customSchema)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;, which is automatically downloaded by the included shell script. To build a JAR file simply run &lt;code&gt;sbt/sbt package&lt;/code&gt; from the project root. The build configuration includes support for both Scala 2.10 and 2.11.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yutax77/KeyValueStore-scala</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/yutax77/KeyValueStore-scala</id>
    <link href="https://github.com/yutax77/KeyValueStore-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KeyValueStore&lt;/h1&gt; &#xA;&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rockthejvm/spark-essentials</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/rockthejvm/spark-essentials</id>
    <link href="https://github.com/rockthejvm/spark-essentials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code we wrote during &lt;a href=&#34;https://rockthejvm.com/course/spark-essentials&#34;&gt;Rock the JVM&#39;s Spark Essentials with Scala&lt;/a&gt; (Udemy version &lt;a href=&#34;https://udemy.com/spark-essentials&#34;&gt;here&lt;/a&gt;) Unless explicitly mentioned, the code in this repository is exactly what was caught on camera.&lt;/p&gt; &#xA;&lt;h2&gt;How to install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install Docker&lt;/li&gt; &#xA; &lt;li&gt;either clone the repo or download as zip&lt;/li&gt; &#xA; &lt;li&gt;open with IntelliJ as an SBT project&lt;/li&gt; &#xA; &lt;li&gt;in a terminal window, navigate to the folder where you downloaded this repo and run &lt;code&gt;docker-compose up&lt;/code&gt; to build and start the PostgreSQL container - we will interact with it from Spark&lt;/li&gt; &#xA; &lt;li&gt;in another terminal window, navigate to &lt;code&gt;spark-cluster/&lt;/code&gt; and build the Docker-based Spark cluster with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x build-images.sh&#xA;./build-images.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;when prompted to start the Spark cluster, go to the &lt;code&gt;spark-cluster&lt;/code&gt; folder and run &lt;code&gt;docker-compose up --scale spark-worker=3&lt;/code&gt; to spin up the Spark containers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to start&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and checkout the &lt;code&gt;start&lt;/code&gt; tag by running the following in the repo folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to see the final code&lt;/h3&gt; &#xA;&lt;p&gt;Udemy students: checkout the &lt;code&gt;udemy&lt;/code&gt; branch of the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout udemy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Premium students: checkout the master branch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run an intermediate state&lt;/h3&gt; &#xA;&lt;p&gt;The repository was built while recording the lectures. Prior to each lecture, I tagged each commit so you can easily go back to an earlier state of the repo!&lt;/p&gt; &#xA;&lt;p&gt;The tags are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.1-scala-recap&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.1-dataframes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.2-dataframes-basics-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.4-datasources&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.5-datasources-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.6-columns-expressions&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.7-columns-expressions-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.8-aggregations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.9-joins&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.10-joins-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.1-common-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.2-complex-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.3-managing-nulls&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.4-datasets&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.5-datasets-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.1-spark-sql-shell&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.2-spark-sql&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.3-spark-sql-exercises&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.1-rdds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.2-rdds-part-2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And for premium students, in addition:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;6.1-spark-job-anatomy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6.2-deploying-to-cluster&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.1-taxi&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.2-taxi-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.3-taxi-3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.4-taxi-4&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you watch a lecture, you can &lt;code&gt;git checkout&lt;/code&gt; the appropriate tag and the repo will go back to the exact code I had when I started the lecture.&lt;/p&gt; &#xA;&lt;h3&gt;For questions or suggestions&lt;/h3&gt; &#xA;&lt;p&gt;If you have changes to suggest to this repo, either&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;submit a GitHub issue&lt;/li&gt; &#xA; &lt;li&gt;tell me in the course Q/A forum&lt;/li&gt; &#xA; &lt;li&gt;submit a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>databricks/reference-apps</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/databricks/reference-apps</id>
    <link href="https://github.com/databricks/reference-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spark reference applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Databricks Reference Apps&lt;/h1&gt; &#xA;&lt;p&gt;At Databricks, we are developing a set of reference applications that demonstrate how to use Apache Spark. This book/repo contains the reference applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;View the code in the Github Repo here: &lt;a href=&#34;https://github.com/databricks/reference-apps&#34;&gt;https://github.com/databricks/reference-apps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the documentation here: &lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/&#34;&gt;http://databricks.gitbooks.io/databricks-spark-reference-applications/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Submit feedback or issues here: &lt;a href=&#34;https://github.com/databricks/reference-apps/issues&#34;&gt;https://github.com/databricks/reference-apps/issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The reference applications will appeal to those who want to learn Spark and learn better by example. Browse the applications, see what features of the reference applications are similar to the features you want to build, and refashion the code samples for your needs. Additionally, this is meant to be a practical guide for using Spark in your systems, so the applications mention other technologies that are compatible with Spark - such as what file systems to use for storing your massive data sets.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/index.html&#34;&gt;Log Analysis Application&lt;/a&gt; - The log analysis reference application contains a series of tutorials for learning Spark by example as well as a final application that can be used to monitor Apache access logs. The examples use Spark in batch mode, cover Spark SQL, as well as Spark Streaming.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html&#34;&gt;Twitter Streaming Language Classifier&lt;/a&gt; - This application demonstrates how to fetch and train a language classifier for Tweets using Spark MLlib. Then Spark Streaming is used to call the trained classifier and filter out live tweets that match a specified cluster. For directions on how to build and run this app - see &lt;a href=&#34;https://github.com/databricks/reference-apps/raw/master/twitter_classifier/scala/README.md&#34;&gt;twitter_classifier/scala/README.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/timeseries/index.html&#34;&gt;Weather TimeSeries Data Application with Cassandra&lt;/a&gt; - This reference application works with Weather Data which is taken for a given weather station at a given point in time. The app demonstrates several strategies for leveraging Spark Streaming integrated with Apache Cassandra and Apache Kafka for fast, fault-tolerant, streaming computations with time series data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These reference apps are covered by license terms covered &lt;a href=&#34;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark™&lt;/a&gt;: This connector allows Apache Spark™ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table’s metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>windymelt/FNFIS</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/windymelt/FNFIS</id>
    <link href="https://github.com/windymelt/FNFIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;FNFIS - FreeNet File Indexing on Scala Testing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linkedin/feathr</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/linkedin/feathr</id>
    <link href="https://github.com/linkedin/feathr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Feathr – An Enterprise-Grade, High Performance Feature Store&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Feathr – An Enterprise-Grade, High Performance Feature Store&lt;/h1&gt; &#xA;&lt;h2&gt;What is Feathr?&lt;/h2&gt; &#xA;&lt;p&gt;Feathr is the feature store that is used in production in LinkedIn for many years and was open sourced in April 2022. Read our announcement on &lt;a href=&#34;https://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m&#34;&gt;Open Sourcing Feathr&lt;/a&gt; and &lt;a href=&#34;https://azure.microsoft.com/en-us/blog/feathr-linkedin-s-feature-store-is-now-available-on-azure/&#34;&gt;Feathr on Azure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Feathr lets you:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Define features&lt;/strong&gt; based on raw data sources (batch and streaming) using pythonic APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Register and get features by names&lt;/strong&gt; during model training and model inferencing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Share features&lt;/strong&gt; across your team and company.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Feathr automatically computes your feature values and joins them to your training data, using point-in-time-correct semantics to avoid data leakage, and supports materializing and deploying your features for use online in production.&lt;/p&gt; &#xA;&lt;h2&gt;Feathr Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable with built-in optimizations.&lt;/strong&gt; For example, based on some internal use case, Feathr can process billions of rows and PB scale data with built-in optimizations such as bloom filters and salted joins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich support for point-in-time joins and aggregations:&lt;/strong&gt; Feathr has high performant built-in operators designed for Feature Store, including time-based aggregation, sliding window joins, look-up features, all with point-in-time correctness.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly customizable user-defined functions (UDFs)&lt;/strong&gt; with native PySpark and Spark SQL support to lower the learning curve for data scientists.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pythonic APIs&lt;/strong&gt; to access everything with low learning curve; Integrated with model building so data scientists can be productive from day one.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich type system&lt;/strong&gt; including support for embeddings for advanced machine learning/deep learning scenarios. One of the common use cases is to build embeddings for customer profiles, and those embeddings can be reused across an organization in all the machine learning applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Native cloud integration&lt;/strong&gt; with simplified and scalable architecture, which is illustrated in the next section.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature sharing and reuse made easy:&lt;/strong&gt; Feathr has built-in feature registry so that features can be easily shared across different teams and boost team productivity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more details on Feathr, read our &lt;a href=&#34;https://linkedin.github.io/feathr/&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For Python API references, read the &lt;a href=&#34;https://feathr.readthedocs.io/&#34;&gt;Python API Reference&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For technical talks on Feathr, see the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/docs/talks/Feathr%20Feature%20Store%20Talk.pdf&#34;&gt;slides here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running Feathr on Azure with 3 Simple Steps&lt;/h2&gt; &#xA;&lt;p&gt;Feathr has native cloud integration. To use Feathr on Azure, you only need three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the &lt;code&gt;Principal ID&lt;/code&gt; of your account by running &lt;code&gt;az ad signed-in-user show --query objectId -o tsv&lt;/code&gt; in the link below (Select &#34;Bash&#34; if asked), and write down that value (something like &lt;code&gt;b65ef2e0-42b8-44a7-9b55-abbccddeefff&lt;/code&gt;). Think this ID as something representing you when accessing Azure, and it will be used to grant permissions in the next step in the UI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://shell.azure.com/bash&#34;&gt;Launch Cloud Shell&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Click the button below to deploy a minimal set of Feathr resources for demo purpose. You will need to fill in the &lt;code&gt;Principal ID&lt;/code&gt; and &lt;code&gt;Resource Prefix&lt;/code&gt;. You will need &#34;Owner&#34; permission of the selected subscription.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Flinkedin%2Ffeathr%2Fmain%2Fdocs%2Fhow-to-guides%2Fazure_resource_provision.json&#34;&gt;&lt;img src=&#34;https://aka.ms/deploytoazurebutton&#34; alt=&#34;Deploy to Azure&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the Feathr Jupyter Notebook by clicking the button below. You only need to change the specified &lt;code&gt;Resource Prefix&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/linkedin/feathr/main?labpath=feathr_project%2Ffeathrcli%2Fdata%2Ffeathr_user_workspace%2Fnyc_driver_demo.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing Feathr Client Locally&lt;/h2&gt; &#xA;&lt;p&gt;If you are not using the above Jupyter Notebook and want to install Feathr client locally, use this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U feathr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use the latest code from GitHub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/linkedin/feathr.git#subdirectory=feathr_project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Feathr Examples&lt;/h2&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://linkedin.github.io/feathr/concepts/feathr-capabilities.html&#34;&gt;Feathr Capabilities&lt;/a&gt; for more examples. Below are a few selected ones:&lt;/p&gt; &#xA;&lt;h3&gt;Rich UDF Support&lt;/h3&gt; &#xA;&lt;p&gt;Feathr has highly customizable UDFs with native PySpark and Spark SQL integration to lower learning curve for data scientists:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def add_new_dropoff_and_fare_amount_column(df: DataFrame):&#xA;    df = df.withColumn(&#34;f_day_of_week&#34;, dayofweek(&#34;lpep_dropoff_datetime&#34;))&#xA;    df = df.withColumn(&#34;fare_amount_cents&#34;, df.fare_amount.cast(&#39;double&#39;) * 100)&#xA;    return df&#xA;&#xA;batch_source = HdfsSource(name=&#34;nycTaxiBatchSource&#34;,&#xA;                        path=&#34;abfss://feathrazuretest3fs@feathrazuretest3storage.dfs.core.windows.net/demo_data/green_tripdata_2020-04.csv&#34;,&#xA;                        preprocessing=add_new_dropoff_and_fare_amount_column,&#xA;                        event_timestamp_column=&#34;new_lpep_dropoff_datetime&#34;,&#xA;                        timestamp_format=&#34;yyyy-MM-dd HH:mm:ss&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Defining Window Aggregation Features&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;agg_features = [Feature(name=&#34;f_location_avg_fare&#34;,&#xA;                        key=location_id,                          # Query/join key of the feature(group)&#xA;                        feature_type=FLOAT,&#xA;                        transform=WindowAggTransformation(        # Window Aggregation transformation&#xA;                            agg_expr=&#34;cast_float(fare_amount)&#34;,&#xA;                            agg_func=&#34;AVG&#34;,                       # Apply average aggregation over the window&#xA;                            window=&#34;90d&#34;)),                       # Over a 90-day window&#xA;                ]&#xA;&#xA;agg_anchor = FeatureAnchor(name=&#34;aggregationFeatures&#34;,&#xA;                           source=batch_source,&#xA;                           features=agg_features)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Define features on top of other features - Derived Features&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute a new feature(a.k.a. derived feature) on top of an existing feature&#xA;derived_feature = DerivedFeature(name=&#34;f_trip_time_distance&#34;,&#xA;                                 feature_type=FLOAT,&#xA;                                 key=trip_key,&#xA;                                 input_features=[f_trip_distance, f_trip_time_duration],&#xA;                                 transform=&#34;f_trip_distance * f_trip_time_duration&#34;)&#xA;&#xA;# Another example to compute embedding similarity&#xA;user_embedding = Feature(name=&#34;user_embedding&#34;, feature_type=DENSE_VECTOR, key=user_key)&#xA;item_embedding = Feature(name=&#34;item_embedding&#34;, feature_type=DENSE_VECTOR, key=item_key)&#xA;&#xA;user_item_similarity = DerivedFeature(name=&#34;user_item_similarity&#34;,&#xA;                                      feature_type=FLOAT,&#xA;                                      key=[user_key, item_key],&#xA;                                      input_features=[user_embedding, item_embedding],&#xA;                                      transform=&#34;cosine_similarity(user_embedding, item_embedding)&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Define Streaming Features&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://linkedin.github.io/feathr/how-to-guides/streaming-source-ingestion.html&#34;&gt;Streaming Source Ingestion Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Point in Time Joins&lt;/h3&gt; &#xA;&lt;p&gt;Read &lt;a href=&#34;https://linkedin.github.io/feathr/concepts/point-in-time-join.html&#34;&gt;Point-in-time Correctness and Point-in-time Join in Feathr&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Running Feathr Examples&lt;/h2&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/feathr_project/feathrcli/data/feathr_user_workspace/nyc_driver_demo.ipynb&#34;&gt;quick start Jupyter Notebook&lt;/a&gt; to try it out. There is also a companion &lt;a href=&#34;https://linkedin.github.io/feathr/quickstart.html&#34;&gt;quick start guide&lt;/a&gt; containing a bit more explanation on the notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Cloud Integrations&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feathr component&lt;/th&gt; &#xA;   &lt;th&gt;Cloud Integrations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Offline store – Object Store&lt;/td&gt; &#xA;   &lt;td&gt;Azure Blob Storage, Azure ADLS Gen2, AWS S3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Offline store – SQL&lt;/td&gt; &#xA;   &lt;td&gt;Azure SQL DB, Azure Synapse Dedicated SQL Pools, Azure SQL in VM, Snowflake&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Streaming Source&lt;/td&gt; &#xA;   &lt;td&gt;Kafka, EventHub&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Online store&lt;/td&gt; &#xA;   &lt;td&gt;Azure Cache for Redis&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Feature Registry&lt;/td&gt; &#xA;   &lt;td&gt;Azure Purview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Compute Engine&lt;/td&gt; &#xA;   &lt;td&gt;Azure Synapse Spark Pools, Databricks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Machine Learning Platform&lt;/td&gt; &#xA;   &lt;td&gt;Azure Machine Learning, Jupyter Notebook&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;File Format&lt;/td&gt; &#xA;   &lt;td&gt;Parquet, ORC, Avro, Delta Lake&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;For a complete roadmap with esitmated dates, please &lt;a href=&#34;https://github.com/linkedin/feathr/milestones?direction=asc&amp;amp;sort=title&amp;amp;state=open&#34;&gt;visit this page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Private Preview release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Public Preview release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Future release &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support streaming&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support common data sources&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support online transformation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature versioning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature monitoring&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature store UI &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Lineage&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Search&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support feature data deletion and retention&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community Guidelines&lt;/h2&gt; &#xA;&lt;p&gt;Build for the community and build by the community. Check out &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/feathr/main/CONTRIBUTING.md&#34;&gt;Community Guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Slack Channel&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://feathrai.slack.com&#34;&gt;Slack channel&lt;/a&gt; for questions and discussions (or click the &lt;a href=&#34;https://join.slack.com/t/feathrai/shared_invite/zt-19dcbquwl-zKiJGYTak6Psw2GbUYtT2g&#34;&gt;invitation link&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scala/scala</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/scala/scala</id>
    <link href="https://github.com/scala/scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala 2 compiler and standard library. For bugs, see scala/bug&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome!&lt;/h1&gt; &#xA;&lt;p&gt;This is the home of the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala 2&lt;/a&gt; standard library, compiler, and language spec.&lt;/p&gt; &#xA;&lt;h1&gt;How to contribute&lt;/h1&gt; &#xA;&lt;p&gt;Issues and bug reports for Scala 2 are located in &lt;a href=&#34;https://github.com/scala/bug&#34;&gt;scala/bug&lt;/a&gt;. That tracker is also where new contributors may find issues to work on: &lt;a href=&#34;https://github.com/scala/bug/labels/good%20first%20issue&#34;&gt;good first issues&lt;/a&gt;, &lt;a href=&#34;https://github.com/scala/bug/labels/help%20wanted&#34;&gt;help wanted&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For coordinating broader efforts, we also use the &lt;a href=&#34;https://github.com/scala/scala-dev/issues&#34;&gt;scala/scala-dev tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute here, please open a &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/#fork--pull&#34;&gt;pull request&lt;/a&gt; from your fork of this repository.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that we can&#39;t accept additions to the standard library, only modifications to existing code. Binary compatibility forbids adding new public classes or public methods. Additions are made to &lt;a href=&#34;https://github.com/scala/scala-library-next&#34;&gt;scala-library-next&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;We require that you sign the &lt;a href=&#34;https://www.lightbend.com/contribute/cla/scala&#34;&gt;Scala CLA&lt;/a&gt; before we can merge any of your work, to protect Scala&#39;s future as open source software.&lt;/p&gt; &#xA;&lt;p&gt;The general workflow is as follows.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/file an issue in scala/bug (or submit a well-documented PR right away!).&lt;/li&gt; &#xA; &lt;li&gt;Fork the scala/scala repo.&lt;/li&gt; &#xA; &lt;li&gt;Push your changes to a branch in your forked repo. For coding guidelines, go &lt;a href=&#34;https://github.com/scala/scala#coding-guidelines&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request to scala/scala from your forked repo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information on building and developing the core of Scala, read the rest of this README, especially for &lt;a href=&#34;https://github.com/scala/scala#get-ready-to-contribute&#34;&gt;setting up your machine&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Get in touch!&lt;/h1&gt; &#xA;&lt;p&gt;In order to get in touch with other Scala contributors, join the #scala-contributors channel on the &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;Scala Discord&lt;/a&gt; chat, or post on &lt;a href=&#34;https://contributors.scala-lang.org&#34;&gt;contributors.scala-lang.org&lt;/a&gt; (Discourse).&lt;/p&gt; &#xA;&lt;p&gt;If you need some help with your PR at any time, please feel free to @-mention anyone from the list below, and we will do our best to help you out:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;username&lt;/th&gt; &#xA;   &lt;th&gt;talk to me about...&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/lrytz&#34; height=&#34;50px&#34; title=&#34;Lukas Rytz&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lrytz&#34;&gt;&lt;code&gt;@lrytz&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;back end, optimizer, named &amp;amp; default arguments, reporters&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/retronym&#34; height=&#34;50px&#34; title=&#34;Jason Zaugg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/retronym&#34;&gt;&lt;code&gt;@retronym&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.12.x branch, compiler performance, weird compiler bugs, lambdas&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/SethTisue&#34; height=&#34;50px&#34; title=&#34;Seth Tisue&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SethTisue&#34;&gt;&lt;code&gt;@SethTisue&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;getting started, build, CI, community build, Jenkins, docs, library, REPL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/dwijnand&#34; height=&#34;50px&#34; title=&#34;Dale Wijnand&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dwijnand&#34;&gt;&lt;code&gt;@dwijnand&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pattern matcher, MiMa, partest&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/Ichoran&#34; height=&#34;50px&#34; title=&#34;Rex Kerr&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ichoran&#34;&gt;&lt;code&gt;@Ichoran&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;collections library, performance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/viktorklang&#34; height=&#34;50px&#34; title=&#34;Viktor Klang&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/viktorklang&#34;&gt;&lt;code&gt;@viktorklang&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;concurrency, futures&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/sjrd&#34; height=&#34;50px&#34; title=&#34;Sébastien Doeraene&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sjrd&#34;&gt;&lt;code&gt;@sjrd&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;interactions with Scala.js&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/NthPortal&#34; height=&#34;50px&#34; title=&#34;Princess | April&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NthPortal&#34;&gt;&lt;code&gt;@NthPortal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;library, concurrency, &lt;code&gt;scala.math&lt;/code&gt;, &lt;code&gt;LazyList&lt;/code&gt;, &lt;code&gt;Using&lt;/code&gt;, warnings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/bishabosha&#34; height=&#34;50px&#34; title=&#34;Jamie Thompson&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bishabosha&#34;&gt;&lt;code&gt;@bishabosha&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TASTy reader&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/joroKr21&#34; height=&#34;50px&#34; title=&#34;Georgi Krastev&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/joroKr21&#34;&gt;&lt;code&gt;@joroKr21&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;higher-kinded types, implicits, variance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;P.S.: If you have some spare time to help out around here, we would be delighted to add your name to this list!&lt;/p&gt; &#xA;&lt;h1&gt;Branches&lt;/h1&gt; &#xA;&lt;p&gt;Target the oldest branch you would like your changes to end up in. We periodically merge forward from older release branches (e.g., 2.12.x) to new ones (e.g. 2.13.x).&lt;/p&gt; &#xA;&lt;p&gt;If your change is difficult to merge forward, you may be asked to also submit a separate PR targeting the newer branch.&lt;/p&gt; &#xA;&lt;p&gt;If your change is version-specific and shouldn&#39;t be merged forward, put &lt;code&gt;[nomerge]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;p&gt;If your change is a backport from a newer branch and thus doesn&#39;t need to be merged forward, put &lt;code&gt;[backport]&lt;/code&gt; in the PR name.&lt;/p&gt; &#xA;&lt;h2&gt;Choosing a branch&lt;/h2&gt; &#xA;&lt;p&gt;Most changes should target 2.13.x. We are increasingly reluctant to target 2.12.x unless there is a special reason (e.g. if an especially bad bug is found, or if there is commercial sponsorship).&lt;/p&gt; &#xA;&lt;p&gt;The 2.11.x branch is now &lt;a href=&#34;https://github.com/scala/scala-dev/issues/451&#34;&gt;inactive&lt;/a&gt; and no further 2.11.x releases are planned (unless unusual, unforeseeable circumstances arise). You should not target 2.11.x without asking maintainers first.&lt;/p&gt; &#xA;&lt;h1&gt;Repository structure&lt;/h1&gt; &#xA;&lt;p&gt;Most importantly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;+--build.sbt                 The main sbt build definition&#xA;+--project/                  The rest of the sbt build&#xA;+--src/                      All sources&#xA;   +---/library              Scala Standard Library&#xA;   +---/reflect              Scala Reflection&#xA;   +---/compiler             Scala Compiler&#xA;+--test/                     The Scala test suite&#xA;   +---/files                Partest tests&#xA;   +---/junit                JUnit tests&#xA;   +---/scalacheck           ScalaCheck tests&#xA;+--spec/                     The Scala language specification&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;but also:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scala/&#xA;   +---/library-aux          Scala Auxiliary Library, for bootstrapping and documentation purposes&#xA;   +---/interactive          Scala Interactive Compiler, for clients such as an IDE (aka Presentation Compiler)&#xA;   +---/intellij             IntelliJ project templates&#xA;   +---/manual               Scala&#39;s runner scripts &#34;man&#34; (manual) pages&#xA;   +---/partest              Scala&#39;s internal parallel testing framework&#xA;   +---/partest-javaagent    Partest&#39;s helper java agent&#xA;   +---/repl                 Scala REPL core&#xA;   +---/repl-frontend        Scala REPL frontend&#xA;   +---/scaladoc             Scala&#39;s documentation tool&#xA;   +---/scalap               Scala&#39;s class file decompiler&#xA;   +---/testkit              Scala&#39;s unit-testing kit&#xA;+--admin/                    Scripts for the CI jobs and releasing&#xA;+--doc/                      Additional licenses and copyrights&#xA;+--scripts/                  Scripts for the CI jobs and releasing&#xA;+--tools/                    Scripts useful for local development&#xA;+--build/                    Build products&#xA;+--dist/                     Build products&#xA;+--target/                   Build products&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Get ready to contribute&lt;/h1&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You need the following tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java SDK. The baseline version is 8 for both 2.12.x and 2.13.x. It is almost always fine to use a later SDK such as 11 or 15 for local development. CI will verify against the baseline version.&lt;/li&gt; &#xA; &lt;li&gt;sbt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MacOS and Linux work. Windows may work if you use Cygwin. Community help with keeping the build working on Windows and documenting any needed setup is appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Tools we use&lt;/h2&gt; &#xA;&lt;p&gt;We are grateful for the following OSS licenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler Java profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.yourkit.com/java/profiler/&#34;&gt;YourKit Java Profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/idea/download/&#34;&gt;IntelliJ IDEA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build setup&lt;/h2&gt; &#xA;&lt;h3&gt;Basics&lt;/h3&gt; &#xA;&lt;p&gt;During ordinary development, a new Scala build is built by the previously released version, known as the &#34;reference compiler&#34; or, slangily, as &#34;STARR&#34; (stable reference release). Building with STARR is sufficient for most kinds of changes.&lt;/p&gt; &#xA;&lt;p&gt;However, a full build of Scala is &lt;em&gt;bootstrapped&lt;/em&gt;. Bootstrapping has two steps: first, build with STARR; then, build again using the freshly built compiler, leaving STARR behind. This guarantees that every Scala version can build itself.&lt;/p&gt; &#xA;&lt;p&gt;If you change the code generation part of the Scala compiler, your changes will only show up in the bytecode of the library and compiler after a bootstrap. Our CI does a bootstrapped build.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bootstrapping locally&lt;/strong&gt;: To perform a bootstrap, run &lt;code&gt;restarrFull&lt;/code&gt; within an sbt session. This will build and publish the Scala distribution to your local artifact repository and then switch sbt to use that version as its new &lt;code&gt;scalaVersion&lt;/code&gt;. You may then revert back with &lt;code&gt;reload&lt;/code&gt;. Note &lt;code&gt;restarrFull&lt;/code&gt; will also write the STARR version to &lt;code&gt;buildcharacter.properties&lt;/code&gt; so you can switch back to it with &lt;code&gt;restarr&lt;/code&gt; without republishing. This will switch the sbt session to use the &lt;code&gt;build-restarr&lt;/code&gt; and &lt;code&gt;target-restarr&lt;/code&gt; directories instead of &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;, which avoids wiping out classfiles and incremental metadata. IntelliJ will continue to be configured to compile and run tests using the starr version in &lt;code&gt;versions.properties&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For history on how the current scheme was arrived at, see &lt;a href=&#34;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&#34;&gt;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Building with fatal warnings&lt;/strong&gt;: To make warnings in the project fatal (i.e. turn them into errors), run &lt;code&gt;set Global / fatalWarnings := true&lt;/code&gt; in sbt (replace &lt;code&gt;Global&lt;/code&gt; with the name of a module—such as &lt;code&gt;reflect&lt;/code&gt;—to only make warnings fatal for that module). To disable fatal warnings again, either &lt;code&gt;reload&lt;/code&gt; sbt, or run &lt;code&gt;set Global / fatalWarnings := false&lt;/code&gt; (again, replace &lt;code&gt;Global&lt;/code&gt; with the name of a module if you only enabled fatal warnings for that module). CI always has fatal warnings enabled.&lt;/p&gt; &#xA;&lt;h3&gt;Using the sbt build&lt;/h3&gt; &#xA;&lt;p&gt;Once you&#39;ve started an &lt;code&gt;sbt&lt;/code&gt; session you can run one of the core commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;compile&lt;/code&gt; compiles all sub-projects (library, reflect, compiler, scaladoc, etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scala&lt;/code&gt; / &lt;code&gt;scalac&lt;/code&gt; run the REPL / compiler directly from sbt (accept options / arguments)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enableOptimizer&lt;/code&gt; reloads the build with the Scala optimizer enabled. Our releases are built this way. Enable this when working on compiler performance improvements. When the optimizer is enabled the build will be slower and incremental builds can be incorrect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setupPublishCore&lt;/code&gt; runs &lt;code&gt;enableOptimizer&lt;/code&gt; and configures a version number based on the current Git SHA. Often used as part of bootstrapping: &lt;code&gt;sbt setupPublishCore publishLocal &amp;amp;&amp;amp; sbt -Dstarr.version=&amp;lt;VERSION&amp;gt; testAll&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkBin&lt;/code&gt; generates runner scripts (&lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt;, etc) in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dist/mkPack&lt;/code&gt; creates a build in the Scala distribution format in &lt;code&gt;build/pack&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;junit/test&lt;/code&gt; runs the JUnit tests; &lt;code&gt;junit/testOnly *Foo&lt;/code&gt; runs a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scalacheck/test&lt;/code&gt; runs scalacheck tests, use &lt;code&gt;testOnly&lt;/code&gt; to run a subset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partest&lt;/code&gt; runs partest tests (accepts options, try &lt;code&gt;partest --help&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;publishLocal&lt;/code&gt; publishes a distribution locally (can be used as &lt;code&gt;scalaVersion&lt;/code&gt; in other sbt projects) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set baseVersionSuffix := &#34;bin-abcd123-SNAPSHOT&#34;&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the git hash of the revision being published. You can also use something custom like &lt;code&gt;&#34;bin-mypatch&#34;&lt;/code&gt;. This changes the version number from &lt;code&gt;2.13.2-SNAPSHOT&lt;/code&gt; to something more stable (&lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Note that the &lt;code&gt;-bin&lt;/code&gt; string marks the version binary compatible. Using it in sbt will cause the &lt;code&gt;scalaBinaryVersion&lt;/code&gt; to be &lt;code&gt;2.13&lt;/code&gt;. If the version is not binary compatible, we recommend using &lt;code&gt;-pre&lt;/code&gt;, e.g., &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Optionally &lt;code&gt;set publishArtifact in (Compile, packageDoc) in ThisBuild := false&lt;/code&gt; to skip generating / publishing API docs (speeds up the process).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a command results in an error message like &lt;code&gt;a module is not authorized to depend on itself&lt;/code&gt;, it may be that a global sbt plugin is causing a cyclical dependency. Try disabling global sbt plugins (perhaps by temporarily commenting them out in &lt;code&gt;~/.sbt/1.0/plugins/plugins.sbt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Sandbox&lt;/h4&gt; &#xA;&lt;p&gt;We recommend keeping local test files in the &lt;code&gt;sandbox&lt;/code&gt; directory which is listed in the &lt;code&gt;.gitignore&lt;/code&gt; of the Scala repo.&lt;/p&gt; &#xA;&lt;h4&gt;Incremental compilation&lt;/h4&gt; &#xA;&lt;p&gt;Note that sbt&#39;s incremental compilation is often too coarse for the Scala compiler codebase and re-compiles too many files, resulting in long build times (check &lt;a href=&#34;https://github.com/sbt/sbt/issues/1104&#34;&gt;sbt#1104&lt;/a&gt; for progress on that front). In the meantime you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use IntelliJ IDEA for incremental compiles (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/#ide-setup&#34;&gt;IDE Setup&lt;/a&gt; below) - its incremental compiler is a bit less conservative, but usually correct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;IDE setup&lt;/h3&gt; &#xA;&lt;p&gt;We suggest using IntelliJ IDEA (see &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/src/intellij/README.md&#34;&gt;src/intellij/README.md&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalameta.org/metals/&#34;&gt;Metals&lt;/a&gt; may also work, but we don&#39;t yet have instructions or sample configuration for that. A pull request in this area would be exceedingly welcome. In the meantime, we are collecting guidance at &lt;a href=&#34;https://github.com/scala/scala-dev/issues/668&#34;&gt;scala/scala-dev#668&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to use IntelliJ&#39;s incremental compiler:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run &lt;code&gt;dist/mkBin&lt;/code&gt; in sbt to get a build and the runner scripts in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &#34;Build&#34; - &#34;Make Project&#34; in IntelliJ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now you can edit and build in IntelliJ and use the scripts (compiler, REPL) to directly test your changes. You can also run the &lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt; and &lt;code&gt;partest&lt;/code&gt; commands in sbt. Enable &#34;Ant mode&#34; (explained above) to prevent sbt&#39;s incremental compiler from re-compiling (too many) files before each &lt;code&gt;partest&lt;/code&gt; invocation.&lt;/p&gt; &#xA;&lt;h1&gt;Coding guidelines&lt;/h1&gt; &#xA;&lt;p&gt;Our guidelines for contributing are explained in &lt;a href=&#34;https://raw.githubusercontent.com/scala/scala/2.13.x/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;. It contains useful information on our coding standards, testing, documentation, how we use git and GitHub and how to get your code reviewed.&lt;/p&gt; &#xA;&lt;p&gt;You may also want to check out the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://scala-lang.org/contribute/hacker-guide.html&#34;&gt;&#34;Scala Hacker Guide&#34;&lt;/a&gt; covers some of the same ground as this README, but in greater detail and in a more tutorial style, using a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org&#34;&gt;Scala documentation site&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Scala CI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/scala/scala&#34;&gt;&lt;img src=&#34;https://travis-ci.com/scala/scala.svg?branch=2.13.x&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once you submit a PR your commits will be automatically tested by the Scala CI.&lt;/p&gt; &#xA;&lt;p&gt;Our CI setup is always evolving. See &lt;a href=&#34;https://github.com/scala/scala-dev/issues/751&#34;&gt;scala/scala-dev#751&lt;/a&gt; for more details on how things currently work and how we expect they might change.&lt;/p&gt; &#xA;&lt;p&gt;If you see a spurious failure on Jenkins, you can post &lt;code&gt;/rebuild&lt;/code&gt; as a PR comment. The &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scabot README&lt;/a&gt; lists all available commands.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to test your patch before having everything polished for review, you can have Travis CI build your branch (make sure you have a fork and have Travis CI enabled for branch builds on it first, and then push your branch). Also feel free to submit a draft PR. In case your draft branch contains a large number of commits (that you didn&#39;t clean up / squash yet for review), consider adding &lt;code&gt;[ci: last-only]&lt;/code&gt; to the PR title. That way only the last commit will be tested, saving some energy and CI-resources. Note that inactive draft PRs will be closed eventually, which does not mean the change is being rejected.&lt;/p&gt; &#xA;&lt;p&gt;CI performs a compiler bootstrap. The first task, &lt;code&gt;validatePublishCore&lt;/code&gt;, publishes a build of your commit to the temporary repository &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&lt;/a&gt;. Note that this build is not yet bootstrapped, its bytecode is built using the current STARR. The version number is &lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the commit hash. For binary incompatible builds, the version number is &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use Scala builds in the validation repository locally by adding a resolver and specifying the corresponding &lt;code&gt;scalaVersion&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set resolvers += &#34;pr&#34; at &#34;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots/&#34;&#xA;&amp;gt; set scalaVersion := &#34;2.12.2-bin-abcd123-SNAPSHOT&#34;&#xA;&amp;gt; console&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&#34;Nightly&#34; builds&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI builds nightly download releases and publishes them to &lt;a href=&#34;https://scala-ci.typesafe.com/artifactory/scala-integration/&#34;&gt;https://scala-ci.typesafe.com/artifactory/scala-integration/&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Using a nightly build in sbt is explained in &lt;a href=&#34;https://stackoverflow.com/questions/40622878&#34;&gt;this Stack Overflow answer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Although we casually refer to these as &#34;nightly&#34; builds, they aren&#39;t actually built nightly, but &#34;mergely&#34;. That is to say, a build is published for every merged PR.&lt;/p&gt; &#xA;&lt;h2&gt;Scala CI internals&lt;/h2&gt; &#xA;&lt;p&gt;The Scala CI runs as a Jenkins instance on &lt;a href=&#34;https://scala-ci.typesafe.com/&#34;&gt;scala-ci.typesafe.com&lt;/a&gt;, configured by a chef cookbook at &lt;a href=&#34;https://github.com/scala/scala-jenkins-infra&#34;&gt;scala/scala-jenkins-infra&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The build bot that watches PRs, triggers testing builds and applies the &#34;reviewed&#34; label after an LGTM comment is in the &lt;a href=&#34;https://github.com/scala/scabot&#34;&gt;scala/scabot&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Community build&lt;/h2&gt; &#xA;&lt;p&gt;The Scala community build is an important method for testing Scala releases. A community build can be launched for any Scala commit, even before the commit&#39;s PR has been merged. That commit is then used to build a large number of open-source projects from source and run their test suites.&lt;/p&gt; &#xA;&lt;p&gt;To request a community build run on your PR, just ask in a comment on the PR and a Scala team member (probably @SethTisue) will take care of it. (&lt;a href=&#34;https://github.com/scala/community-builds/wiki#can-i-run-it-against-a-pull-request-in-scalascala&#34;&gt;details&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Community builds run on the Scala Jenkins instance. The jobs are named &lt;code&gt;..-integrate-community-build&lt;/code&gt;. See the &lt;a href=&#34;https://github.com/scala/community-builds&#34;&gt;scala/community-builds&lt;/a&gt; repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sbt-databricks</title>
    <updated>2022-05-30T02:21:00Z</updated>
    <id>tag:github.com,2022-05-30:/databricks/sbt-databricks</id>
    <link href="https://github.com/databricks/sbt-databricks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An sbt plugin for deploying code to Databricks Cloud&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;sbt-databricks &lt;a href=&#34;http://travis-ci.org/databricks/sbt-databricks&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/sbt-databricks.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;sbt plugin to deploy your projects to Databricks!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://go.databricks.com/register-for-dbc&#34;&gt;http://go.databricks.com/register-for-dbc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;An Account on Databricks: &lt;a href=&#34;https://accounts.cloud.databricks.com/registration.html#signup&#34;&gt;Sign up for a free trial.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Just add the following line to &lt;code&gt;project/plugins.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.18 or greater you must use sbt-databricks version 0.1.5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.8 or greater you must use sbt-databricks version 0.1.3&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Enable sbt-databricks for all your projects&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;sbt-databricks&lt;/code&gt; can be enabled as a &lt;a href=&#34;http://www.scala-sbt.org/0.13/tutorial/Using-Plugins.html#Global+plugins&#34;&gt;global plugin&lt;/a&gt; for use in all of your projects in two easy steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the following line to &lt;code&gt;~/.sbt/0.13/plugins/build.sbt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the settings defined &lt;a href=&#34;https://raw.githubusercontent.com/databricks/sbt-databricks/master/#settings&#34;&gt;here&lt;/a&gt; in &lt;code&gt;~/.sbt/0.13/databricks.sbt&lt;/code&gt;. You&#39;ll have to add the line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import sbtdatabricks.DatabricksPlugin.autoImport._&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to that file in order to import this plugin&#39;s settings into that configuration file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h3&gt;Cluster Controls&lt;/h3&gt; &#xA;&lt;p&gt;There are three primary cluster related actions: Create, Resize and Delete.&lt;/p&gt; &#xA;&lt;p&gt;Creating a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcCreateCluster // Attempts to create a cluster on DBC&#xA;// The following parameters must be set when attempting to create a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcSpotInstance := // Boolean for choosing whether to use Spot or On-Demand instances&#xA;dbcSparkVersion := // String: The Spark version to be used e.g. &#34;1.6.x&#34;&#xA;dbcZoneId := // String: AWS zone e.g. ap-southeast-2&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resizing a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcResizeCluster // Attempts to resize a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deleting a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcDeleteCluster // Attempts to delete a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;There are four major commands that can be used. Please check the next section for mandatory settings before running these commands.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcDeploy&lt;/code&gt;: Uploads your Library to Databricks Cloud, attaches it to specified clusters, and restarts the clusters if a previous version of the library was attached. This method encapsulates the following commands. Only libraries with &lt;code&gt;SNAPSHOT&lt;/code&gt; versions will be deleted and re-uploaded as it is assumed that dependencies will not change very frequently. If you change the version of one of your dependencies, that dependency must be deleted manually in Databricks Cloud to prevent unexpected behavior.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcUpload&lt;/code&gt;: Uploads your Library to Databricks Cloud. Deletes the older version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcAttach&lt;/code&gt;: Attaches your Library to the specified clusters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcRestartClusters&lt;/code&gt;: Restarts the specified clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Command Execution&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;`dbcExecuteCommand` // Runs a command on a specified DBC Cluster&#xA;// The context/command language that will be employed when dbcExecuteCommand is called&#xA;dbcExecutionLanguage := // One of DBCScala, DBCPython, DBCSQL&#xA;// The file containing the code that is to be processed on the DBC cluster&#xA;dbcCommandFile := // Type File&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example, using just an sbt invocation is below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set dbcClusters := Seq(&#34;CLUSTER_NAME&#34;)&#xA;&amp;gt; set dbcCommandFile := new File(&#34;/Path/to/file.py&#34;)&#xA;&amp;gt; set dbcExecutionLanguage := DBCPython&#xA;&amp;gt; dbcExecuteCommand&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;p&gt;Other helpful commands are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcListClusters&lt;/code&gt;: View the states of available clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a name=&#34;settings&#34;&gt;Settings&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;There are a few configuration settings that need to be made in the build file. Please set the following parameters according to your setup:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Your username to login to Databricks Cloud&#xA;dbcUsername := // e.g. &#34;admin&#34;&#xA;&#xA;// Your password (Can be set as an environment variable)&#xA;dbcPassword := // e.g. &#34;admin&#34; or System.getenv(&#34;DBCLOUD_PASSWORD&#34;)&#xA;&#xA;// The URL to the Databricks Cloud DB Api.!&#xA;// Note: this plugin currently does not support the /api/2.0 endpoint, so values using that&#xA;// endpoint will be automatically rewritten to use /api/1.2.&#xA;dbcApiUrl := // https://organization.cloud.databricks.com/api/1.2&#xA;&#xA;// Add any clusters that you would like to deploy your work to. e.g. &#34;My Cluster&#34;&#xA;// or run dbcExecuteCommand&#xA;dbcClusters += // Add &#34;ALL_CLUSTERS&#34; if you want to attach your work to all clusters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using dbcDeploy, if you wish to upload an assembly jar instead of every library by itself, you may override dbcClasspath as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other optional parameters are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// The location to upload your libraries to in the workspace e.g. &#34;/Users/alice&#34;&#xA;dbcLibraryPath := // Default is &#34;/&#34;&#xA;&#xA;// Whether to restart the clusters every time a new version is uploaded to Databricks Cloud&#xA;dbcRestartOnAttach := // Default true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT Tips and Tricks (FAQ)&lt;/h3&gt; &#xA;&lt;p&gt;Here are some SBT tips and tricks to improve your experience with sbt-databricks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;I have a multi-project build. I don&#39;t want to upload the entire project to Databricks Cloud. What should I do?&lt;/p&gt; &lt;p&gt;In a multi-project build, you may run an SBT task (such as dbcDeploy, dbcUpload, etc...) just for that project by &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Tasks.html#Task+Scope&#34;&gt;&lt;em&gt;scoping&lt;/em&gt;&lt;/a&gt; the task. You may &lt;em&gt;scope&lt;/em&gt; the task by using the project id before that task.&lt;/p&gt; &lt;p&gt;For example, assume we have a project with sub-projects &lt;code&gt;core&lt;/code&gt;, &lt;code&gt;ml&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt;. Assume &lt;code&gt;ml&lt;/code&gt; depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;, &lt;code&gt;sql&lt;/code&gt; only depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;core&lt;/code&gt; doesn&#39;t depend on anything. Here is what would happen for the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; dbcUpload          // Uploads core, ml, and sql&#xA;&amp;gt; core/dbcUpload     // Uploads only core&#xA;&amp;gt; sql/dbcUpload      // Uploads core and sql&#xA;&amp;gt; ml/dbcUpload       // Uploads core, ml, and sql&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to pass parameters to &lt;code&gt;dbcDeploy&lt;/code&gt;. For example, in my build file &lt;code&gt;dbcClusters&lt;/code&gt; is set as &lt;code&gt;clusterA&lt;/code&gt; but I want to deploy to &lt;code&gt;clusterB&lt;/code&gt; once in a while. What should I do?&lt;/p&gt; &lt;p&gt;In the SBT console, one way of overriding settings for your session is by using the &lt;code&gt;set&lt;/code&gt; command. Using the example above.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; core/dbcDeploy   // Deploys core to clusterA (clusterA was set inside the build file)&#xA;&amp;gt; set dbcClusters := Seq(&#34;clusterB&#34;)  // change cluster to clusterB&#xA;&amp;gt; ml/dbcDeploy     // Deploys core, sql, and ml to clusterB&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to upload an assembly jar rather than tens of individual jars. How can I do that?&lt;/p&gt; &lt;p&gt;You may override &lt;code&gt;dbcClasspath&lt;/code&gt; such as:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... in your build file, (or using set on the console) in order to upload a single fat jar instead of many individual ones. Beware of dependency conflicts!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Hey, I followed #3, but I&#39;m still uploading &lt;code&gt;core&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt; individually after&lt;code&gt;sql/dbcUpload&lt;/code&gt;. What&#39;s going on!?&lt;/p&gt; &lt;p&gt;Remember scoping tasks? You will need to scope both &lt;code&gt;dbcClasspath&lt;/code&gt; and &lt;code&gt;assembly&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath in sql := Seq((assembly in sql).value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then &lt;code&gt;sql/dbcUpload&lt;/code&gt; should upload an assembly jar of &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Tests&lt;/h1&gt; &#xA;&lt;p&gt;Run tests using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the very last line starts with &lt;code&gt;[success]&lt;/code&gt;, then that means that the tests have passed.&lt;/p&gt; &#xA;&lt;p&gt;Run scalastyle checks using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you encounter bugs or want to contribute, feel free to submit an issue or pull request.&lt;/p&gt;</summary>
  </entry>
</feed>