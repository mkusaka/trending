<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T02:22:17Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apache/openwhisk</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/apache/openwhisk</id>
    <link href="https://github.com/apache/openwhisk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenWhisk is an open source serverless cloud platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenWhisk&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/github/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://travis-ci.com/apache/openwhisk.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openwhisk-team.slack.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-slack-9B69A0.svg?sanitize=true&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/openwhisk/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=openwhisk&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/openwhisk.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. Learn more at &lt;a href=&#34;http://openwhisk.apache.org&#34;&gt;http://openwhisk.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; (Deploy and Use OpenWhisk on your machine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#deploy-to-kubernetes&#34;&gt;Deploy to Kubernetes&lt;/a&gt; (For development and production)&lt;/li&gt; &#xA; &lt;li&gt;For project contributors and Docker deployments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/macos/README.md&#34;&gt;Deploy to Docker for Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/ubuntu-setup/README.md&#34;&gt;Deploy to Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#learn-concepts-and-commands&#34;&gt;Learn Concepts and Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#openwhisk-community-and-support&#34;&gt;OpenWhisk Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#project-repository-structure&#34;&gt;Project Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start using OpenWhisk is to install the &#34;Standalone&#34; OpenWhisk stack. This is a full-featured OpenWhisk stack running as a Java process for convenience. Serverless functions run within Docker containers. You will need &lt;a href=&#34;https://docs.docker.com/install&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://java.com/en/download/help/download_options.xml&#34;&gt;Java&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; available on your machine.&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk.git&#xA;cd openwhisk&#xA;./gradlew core:standalone:bootRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When the OpenWhisk stack is up, it will open your browser to a functions &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/playground-ui.png&#34;&gt;Playground&lt;/a&gt;, typically served from &lt;a href=&#34;http://localhost:3232&#34;&gt;http://localhost:3232&lt;/a&gt;. The Playground allows you create and run functions directly from your browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called &lt;code&gt;wsk&lt;/code&gt; which you can download from &lt;a href=&#34;https://s.apache.org/openwhisk-cli-download&#34;&gt;https://s.apache.org/openwhisk-cli-download&lt;/a&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/cli.md&#34;&gt;CLI configuration&lt;/a&gt; for additional details. Typically you configure the CLI for Standalone OpenWhisk as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;wsk property set \&#xA;  --apihost &#39;http://localhost:3233&#39; \&#xA;  --auth &#39;23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone OpenWhisk can be configured to deploy additional capabilities when that is desirable. Additional resources are available &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/core/standalone/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deploy to Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;OpenWhisk can also be installed on a Kubernetes cluster. You can use a managed Kubernetes cluster provisioned from a public cloud provider (e.g., AKS, EKS, IKS, GKE), or a cluster you manage yourself. Additionally for local development, OpenWhisk is compatible with Minikube, and Kubernetes for Mac using the support built into Docker 18.06 (or higher).&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk-deploy-kube.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the instructions in the &lt;a href=&#34;https://github.com/apache/openwhisk-deploy-kube/raw/master/README.md&#34;&gt;OpenWhisk on Kubernetes README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn Concepts and Commands&lt;/h3&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/&#34;&gt;documentation&lt;/a&gt; to learn more. Here are some topics you may be interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/about.md&#34;&gt;System overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/README.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions.md&#34;&gt;Create and invoke actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/triggers_rules.md&#34;&gt;Create triggers and rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/packages.md&#34;&gt;Use and create packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/catalog.md&#34;&gt;Browse and use the catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/reference.md&#34;&gt;OpenWhisk system details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/feeds.md&#34;&gt;Implementing feeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions-actionloop.md&#34;&gt;Developing a runtime for a new language&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenWhisk Community and Support&lt;/h3&gt; &#xA;&lt;p&gt;Report bugs, ask questions and request features &lt;a href=&#34;https://raw.githubusercontent.com/apache/issues&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join the OpenWhisk Team on Slack &lt;a href=&#34;https://openwhisk-team.slack.com&#34;&gt;https://openwhisk-team.slack.com&lt;/a&gt; and chat with developers. To get access to our public Slack team, request an invite &lt;a href=&#34;https://openwhisk.apache.org/slack.html&#34;&gt;https://openwhisk.apache.org/slack.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Project Repository Structure&lt;/h3&gt; &#xA;&lt;p&gt;The OpenWhisk system is built from a &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/dev/modules.md&#34;&gt;number of components&lt;/a&gt;. The picture below groups the components by their GitHub repos. Please open issues for a component against the appropriate repo (if in doubt just open against the main openwhisk repo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/components_to_repos.png&#34; alt=&#34;component/repo mapping&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/chisel3</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/chipsalliance/chisel3</id>
    <link href="https://github.com/chipsalliance/chisel3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chisel 3: A Modern Hardware Design Language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/chisel_logo.svg?sanitize=true&#34; alt=&#34;Chisel 3&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Upcoming Events&lt;/h2&gt; &#xA;&lt;h3&gt;Chisel Dev Meeting&lt;/h3&gt; &#xA;&lt;p&gt;Chisel/FIRRTL development meetings happen every Monday and Tuesday from 1100--1200 PT.&lt;/p&gt; &#xA;&lt;p&gt;Call-in info and meeting notes are available &lt;a href=&#34;https://docs.google.com/document/d/1BLP2DYt59DqI-FgFCcjw8Ddl4K-WU0nHmQu0sZ_wAGo/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chisel Community Conference 2021, Shanghai, China.&lt;/h3&gt; &#xA;&lt;p&gt;CCC is an annual gathering of Chisel community enthusiasts and technical exchange workshop. This year with the support of the Chisel development community and RISC-V World Conference China 2021 Committee, we have brought together designers and developers with hands-on experience in Chisel from home and abroad to share cutting-edge results and experiences from both the open source community as well as industry.&lt;br&gt; English translated recordings version will be updated soon.&lt;br&gt; Looking forward to CCC 2022! See you then!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/chipsalliance/chisel3.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/chipsalliance/chisel3/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/tag/chipsalliance/chisel3.svg?include_prereleases&amp;amp;sort=semver&#34; alt=&#34;GitHub tag (latest SemVer)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt; is a hardware design language that facilitates &lt;strong&gt;advanced circuit generation and design reuse for both ASIC and FPGA digital logic designs&lt;/strong&gt;. Chisel adds hardware construction primitives to the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala&lt;/a&gt; programming language, providing designers with the power of a modern programming language to write complex, parameterizable circuit generators that produce synthesizable Verilog. This generator methodology enables the creation of re-usable components and libraries, such as the FIFO queue and arbiters in the &lt;a href=&#34;https://www.chisel-lang.org/api/latest/#chisel3.util.package&#34;&gt;Chisel Standard Library&lt;/a&gt;, raising the level of abstraction in design while retaining fine-grained control.&lt;/p&gt; &#xA;&lt;p&gt;For more information on the benefits of Chisel see: &lt;a href=&#34;https://stackoverflow.com/questions/53007782/what-benefits-does-chisel-offer-over-classic-hardware-description-languages&#34;&gt;&#34;What benefits does Chisel offer over classic Hardware Description Languages?&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chisel is powered by &lt;a href=&#34;https://github.com/chipsalliance/firrtl&#34;&gt;FIRRTL (Flexible Intermediate Representation for RTL)&lt;/a&gt;, a hardware compiler framework that performs optimizations of Chisel-generated circuits and supports custom user-defined circuit transformations.&lt;/p&gt; &#xA;&lt;h2&gt;What does Chisel code look like?&lt;/h2&gt; &#xA;&lt;p&gt;Consider an FIR filter that implements a convolution operation, as depicted in this block diagram:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/fir_filter.svg?sanitize=true&#34; width=&#34;512&#34;&gt; &#xA;&lt;p&gt;While Chisel provides similar base primitives as synthesizable Verilog, and &lt;em&gt;could&lt;/em&gt; be used as such:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// 3-point moving sum implemented in the style of a FIR filter&#xA;class MovingSum3(bitWidth: Int) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;&#xA;  val z1 = RegNext(io.in)&#xA;  val z2 = RegNext(z1)&#xA;&#xA;  io.out := (io.in * 1.U) + (z1 * 1.U) + (z2 * 1.U)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the power of Chisel comes from the ability to create generators, such as an FIR filter that is defined by the list of coefficients:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Generalized FIR filter parameterized by the convolution coefficients&#xA;class FirFilter(bitWidth: Int, coeffs: Seq[UInt]) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;  // Create the serial-in, parallel-out shift register&#xA;  val zs = Reg(Vec(coeffs.length, UInt(bitWidth.W)))&#xA;  zs(0) := io.in&#xA;  for (i &amp;lt;- 1 until coeffs.length) {&#xA;    zs(i) := zs(i-1)&#xA;  }&#xA;&#xA;  // Do the multiplies&#xA;  val products = VecInit.tabulate(coeffs.length)(i =&amp;gt; zs(i) * coeffs(i))&#xA;&#xA;  // Sum up the products&#xA;  io.out := products.reduce(_ + _)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use and re-use them across designs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val movingSum3Filter = Module(new FirFilter(8, Seq(1.U, 1.U, 1.U)))  // same 3-point moving sum filter as before&#xA;val delayFilter = Module(new FirFilter(8, Seq(0.U, 1.U)))  // 1-cycle delay as a FIR filter&#xA;val triangleFilter = Module(new FirFilter(8, Seq(1.U, 2.U, 3.U, 2.U, 1.U)))  // 5-point FIR filter with a triangle impulse response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above can be converted to Verilog using &lt;code&gt;ChiselStage&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import chisel3.stage.{ChiselStage, ChiselGeneratorAnnotation}&#xA;&#xA;(new chisel3.stage.ChiselStage).execute(&#xA;  Array(&#34;-X&#34;, &#34;verilog&#34;),&#xA;  Seq(ChiselGeneratorAnnotation(() =&amp;gt; new FirFilter(8, Seq(1.U, 1.U, 1.U)))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you may generate some Verilog directly for inspection:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val verilogString = chisel3.emitVerilog(new FirFilter(8, Seq(0.U, 1.U)))&#xA;println(verilogString)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Bootcamp Interactive Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://mybinder.org/v2/gh/freechipsproject/chisel-bootcamp/master&#34;&gt;&lt;strong&gt;online Chisel Bootcamp&lt;/strong&gt;&lt;/a&gt; is the recommended way to get started with and learn Chisel. &lt;strong&gt;No setup is required&lt;/strong&gt; (it runs in the browser), nor does it assume any prior knowledge of Scala.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/ucb-bar/chisel-tutorial&#34;&gt;&lt;strong&gt;classic Chisel tutorial&lt;/strong&gt;&lt;/a&gt; contains small exercises and runs on your computer.&lt;/p&gt; &#xA;&lt;h3&gt;A Textbook on Chisel&lt;/h3&gt; &#xA;&lt;p&gt;If you like a textbook to learn Chisel and also a bit of digital design in general, you may be interested in reading &lt;a href=&#34;http://www.imm.dtu.dk/~masca/chisel-book.html&#34;&gt;&lt;strong&gt;Digital Design with Chisel&lt;/strong&gt;&lt;/a&gt;. It is available in English, Chinese, Japanese, and Vietnamese.&lt;/p&gt; &#xA;&lt;h3&gt;Build Your Own Chisel Projects&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt; for how to set up your environment to build Chisel locally.&lt;/p&gt; &#xA;&lt;p&gt;When you&#39;re ready to build your own circuits in Chisel, &lt;strong&gt;we recommend starting from the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;Chisel Template&lt;/a&gt; repository&lt;/strong&gt;, which provides a pre-configured project, example design, and testbench. Follow the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template README&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you insist on setting up your own project from scratch, your project needs to depend on both the chisel3-plugin (Scalac plugin) and the chisel3 library. For example, in SBT this could be expressed as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// build.sbt&#xA;scalaVersion := &#34;2.13.7&#34;&#xA;addCompilerPlugin(&#34;edu.berkeley.cs&#34; % &#34;chisel3-plugin&#34; % &#34;3.5.0&#34; cross CrossVersion.full)&#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.5.0&#34;&#xA;// We also recommend using chiseltest for writing unit tests &#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chiseltest&#34; % &#34;0.5.0&#34; % &#34;test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Guide For New Contributors&lt;/h3&gt; &#xA;&lt;p&gt;If you are trying to make a contribution to this project, please read &lt;a href=&#34;https://github.com/Burnleydev1/chisel3/raw/recent_PR/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Design Verification&lt;/h3&gt; &#xA;&lt;p&gt;These simulation-based verification tools are available for Chisel:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-testers&#34;&gt;&lt;strong&gt;iotesters&lt;/strong&gt;&lt;/a&gt;, specifically &lt;a href=&#34;https://github.com/freechipsproject/chisel-testers/wiki/Using%20the%20PeekPokeTester&#34;&gt;PeekPokeTester&lt;/a&gt;, provides constructs (&lt;code&gt;peek&lt;/code&gt;, &lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;expect&lt;/code&gt;) similar to a non-synthesizable Verilog testbench.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/chisel-testers2&#34;&gt;&lt;strong&gt;testers2&lt;/strong&gt;&lt;/a&gt; is an in-development replacement for PeekPokeTester, providing the same base constructs but with a streamlined interface and concurrency support with &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;join&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Useful Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-cheatsheet/releases/latest/download/chisel_cheatsheet.pdf&#34;&gt;&lt;strong&gt;Cheat Sheet&lt;/strong&gt;&lt;/a&gt;, a 2-page reference of the base Chisel syntax and libraries&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org/api/latest/chisel3/index.html&#34;&gt;&lt;strong&gt;ScalaDoc&lt;/strong&gt;&lt;/a&gt;, a listing, description, and examples of the functionality exposed by Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;strong&gt;Gitter&lt;/strong&gt;&lt;/a&gt;, where you can ask questions or discuss anything Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; (&lt;a href=&#34;https://github.com/freechipsproject/www.chisel-lang.org/&#34;&gt;source&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scastie.scala-lang.org/9ga9i2DvQymKlA5JjS1ieA&#34;&gt;&lt;strong&gt;Scastie (3.5.0)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.asic-world.com/verilog/veritut.html&#34;&gt;&lt;strong&gt;asic-world&lt;/strong&gt;&lt;/a&gt; If you aren&#39;t familiar with verilog, this is a good tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are migrating from Chisel2, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/chisel3-vs-chisel2.html&#34;&gt;the migration guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Types Overview&lt;/h3&gt; &#xA;&lt;p&gt;These are the base data types for defining circuit components:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/type_hierarchy.svg?sanitize=true&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributor Documentation&lt;/h2&gt; &#xA;&lt;p&gt;This section describes how to get started contributing to Chisel itself, including how to test your version locally against other projects that pull in Chisel using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Library-Dependencies.html&#34;&gt;sbt&#39;s managed dependencies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Useful Resources for Contributors&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/#useful-resources&#34;&gt;Useful Resources&lt;/a&gt; for users are also helpful for contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/114YihixFBPCfUnv1inqAL8UjsiWfcNWdPHX7SeqlRQc&#34;&gt;&lt;strong&gt;Chisel Breakdown Slides&lt;/strong&gt;&lt;/a&gt;, an introductory talk about Chisel&#39;s internals&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compiling and Testing Chisel&lt;/h3&gt; &#xA;&lt;p&gt;You must first install required dependencies to build Chisel locally, please see &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Clone and build the Chisel library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/chipsalliance/chisel3.git&#xA;cd chisel3&#xA;sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to run the following unit tests, you will need several tools on your &lt;code&gt;PATH&lt;/code&gt;, namely &lt;a href=&#34;https://www.veripool.org/verilator/&#34;&gt;verilator&lt;/a&gt;, &lt;a href=&#34;http://www.clifford.at/yosys/&#34;&gt;yosys&lt;/a&gt;, &lt;a href=&#34;https://github.com/chipsalliance/espresso&#34;&gt;espresso&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Z3Prover/z3&#34;&gt;z3&lt;/a&gt;. Check that each is installed on your &lt;code&gt;PATH&lt;/code&gt; by running &lt;code&gt;which verilator&lt;/code&gt; and so on.&lt;/p&gt; &#xA;&lt;p&gt;If the compilation succeeded and the dependencies noted above are installed, you can then run the included unit tests by invoking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Projects Against Local Chisel&lt;/h3&gt; &#xA;&lt;p&gt;To use the development version of Chisel (&lt;code&gt;master&lt;/code&gt; branch), you will need to build from source and &lt;code&gt;publishLocal&lt;/code&gt;. The repository version can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/build.sbt&#34;&gt;build.sbt&lt;/a&gt; file. As of the time of writing it was:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version := &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To publish your version of Chisel to the local Ivy (sbt&#39;s dependency manager) repository, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The compiled version gets placed in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;. If you need to un-publish your local copy of Chisel, remove the directory generated in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to have your projects use this version of Chisel, you should update the &lt;code&gt;libraryDependencies&lt;/code&gt; setting in your project&#39;s build.sbt file to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building Chisel with FIRRTL in the same SBT Project&lt;/h3&gt; &#xA;&lt;p&gt;While we recommend using the library dependency approach as described above, it is possible to build Chisel and FIRRTL in a single SBT project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This only works for the &#34;main&#34; configuration; you cannot build the Chisel tests this way because &lt;code&gt;treadle&lt;/code&gt; is only supported as a library dependency.&lt;/li&gt; &#xA; &lt;li&gt;Do not &lt;code&gt;publishLocal&lt;/code&gt; when building this way. The published artifact will be missing the FIRRTL dependency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This works by using &lt;a href=&#34;http://eed3si9n.com/hot-source-dependencies-using-sbt-sriracha&#34;&gt;sbt-sriracha&lt;/a&gt;, an SBT plugin for toggling between source and library dependencies. It provides two JVM system properties that, when set, will tell SBT to include FIRRTL as a source project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.sourcemode&lt;/code&gt; - when set to true, SBT will look for FIRRTL in the workspace&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.workspace&lt;/code&gt; - sets the root directory of the workspace&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From root of this repo&#xA;git clone git@github.com:chipsalliance/firrtl.git&#xA;sbt -Dsbt.sourcemode=true -Dsbt.workspace=$PWD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is primarily useful for building projects that themselves want to include Chisel as a source dependency. As an example, see &lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;Rocket Chip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chisel3 Architecture Overview&lt;/h3&gt; &#xA;&lt;p&gt;The Chisel3 compiler consists of these main parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The frontend&lt;/strong&gt;, &lt;code&gt;chisel3.*&lt;/code&gt;, which is the publicly visible &#34;API&#34; of Chisel and what is used in Chisel RTL. These just add data to the...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Builder&lt;/strong&gt;, &lt;code&gt;chisel3.internal.Builder&lt;/code&gt;, which maintains global state (like the currently open Module) and contains commands, generating...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The intermediate data structures&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.*&lt;/code&gt;, which are syntactically very similar to Firrtl. Once the entire circuit has been elaborated, the top-level object (a &lt;code&gt;Circuit&lt;/code&gt;) is then passed to...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Firrtl emitter&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.Emitter&lt;/code&gt;, which turns the intermediate data structures into a string that can be written out into a Firrtl file for further processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also included is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The standard library&lt;/strong&gt; of circuit generators, &lt;code&gt;chisel3.util.*&lt;/code&gt;. These contain commonly used interfaces and constructors (like &lt;code&gt;Decoupled&lt;/code&gt;, which wraps a signal with a ready-valid pair) as well as fully parameterizable circuit generators (like arbiters and multiplexors).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chisel Stage&lt;/strong&gt;, &lt;code&gt;chisel3.stage.*&lt;/code&gt;, which contains compilation and test functions that are invoked in the standard Verilog generation and simulation testing infrastructure. These can also be used as part of custom flows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chisel Sub-Projects&lt;/h3&gt; &#xA;&lt;p&gt;Chisel consists of 4 Scala projects; each is its own separate compilation unit:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/core&#34;&gt;&lt;code&gt;core&lt;/code&gt;&lt;/a&gt; is the bulk of the source code of Chisel, depends on &lt;code&gt;macros&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main&#34;&gt;&lt;code&gt;src/main&lt;/code&gt;&lt;/a&gt; is the &#34;main&#34; that brings it all together and includes a &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main/scala/chisel3/util&#34;&gt;&lt;code&gt;util&lt;/code&gt;&lt;/a&gt; library, which depends on &lt;code&gt;core&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/plugin&#34;&gt;&lt;code&gt;plugin&lt;/code&gt;&lt;/a&gt; is the compiler plugin, no internal dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/macros&#34;&gt;&lt;code&gt;macros&lt;/code&gt;&lt;/a&gt; is most of the macros used in Chisel, no internal dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code that touches lots of APIs that are private to the &lt;code&gt;chisel3&lt;/code&gt; package should belong in &lt;code&gt;core&lt;/code&gt;, while code that is pure Chisel should belong in &lt;code&gt;src/main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Which version should I use?&lt;/h3&gt; &#xA;&lt;p&gt;We encourage Chisel users (as opposed to Chisel developers), to use the latest release version of Chisel. This &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template&lt;/a&gt; repository is kept up-to-date, depending on the most recent version of Chisel. The recommended version is also captured near the top of this README, and in the &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases&#34;&gt;Github releases&lt;/a&gt; section of this repo. If you encounter an issue with a released version of Chisel, please file an issue on GitHub mentioning the Chisel version and provide a simple test case (if possible). Try to reproduce the issue with the associated latest minor release (to verify that the issue hasn&#39;t been addressed).&lt;/p&gt; &#xA;&lt;p&gt;For more information on our versioning policy and what versions of the various Chisel ecosystem projects work together, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&#34;&gt;Chisel Project Versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re developing a Chisel library (or &lt;code&gt;chisel3&lt;/code&gt; itself), you&#39;ll probably want to work closer to the tip of the development trunk. By default, the master branches of the chisel repositories are configured to build and publish their version of the code as &lt;code&gt;Z.Y-SNAPSHOT&lt;/code&gt;. Updated SNAPSHOTs are publised on every push to master. You are encouraged to do your development against the latest SNAPSHOT, but note that neither API nor ABI compatibility is guaranteed so your code may break at any time.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zio/zio</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/zio/zio</id>
    <link href="https://github.com/zio/zio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZIO — A type-safe, composable library for async and concurrent programming in Scala&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/ZIO.png&#34; alt=&#34;ZIO Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Production%20Ready-brightgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Snapshot Artifacts&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://isitmaintained.com/project/zio/zio&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/zio/zio.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scaladoc&lt;/th&gt; &#xA;   &lt;th&gt;Scaladex&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;   &lt;th&gt;Twitter&lt;/th&gt; &#xA;   &lt;th&gt;Gitpod&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://javadoc.io/doc/dev.zio/zio_2.12/latest/zio/index.html&#34;&gt;Scaladoc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://index.scala-lang.org/zio/zio/zio&#34; title=&#34;Scaladex&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/zio/zio/zio/latest.svg?sanitize=true&#34; alt=&#34;Badge-Scaladex-page&#34; title=&#34;Scaladex&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/zioscala&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/zioscala.svg?style=plastic&amp;amp;label=follow&amp;amp;logo=twitter&#34; alt=&#34;Badge-Twitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/zio/zio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod ready-to-code&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Welcome to ZIO&lt;/h1&gt; &#xA;&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt; &#xA;&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala&#39;s &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it&#39;s asynchronous or synchronous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/contributing.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/pulls&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Adopters&lt;/h1&gt; &#xA;&lt;p&gt;Following is a partial list of companies happily using ZIO in production to craft concurrent applications.&lt;/p&gt; &#xA;&lt;p&gt;Want to see your company here? &lt;a href=&#34;https://github.com/zio/zio/edit/master/README.md&#34;&gt;Submit a PR&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adgear.com/en/&#34;&gt;AdGear / Samsung Ads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adidas.com/&#34;&gt;Adidas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adpulse.io/&#34;&gt;adpulse.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adsquare.com/&#34;&gt;adsquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ayolab.com/&#34;&gt;Ayolab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://asana.com/&#34;&gt;Asana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aurinko.io/&#34;&gt;Aurinko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://auto.ru&#34;&gt;auto.ru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.de&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.avast.com&#34;&gt;Avast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bofa.com&#34;&gt;Bank of America&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bpp.it/&#34;&gt;Bpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://broad.app&#34;&gt;Broad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.caesars.com/sportsbook-and-casino&#34;&gt;Caesars Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.calcbank.com.br&#34;&gt;CalcBank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.callhandling.co.uk/&#34;&gt;Call Handling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;Carvana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cellular.de&#34;&gt;Cellular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudfarms.com&#34;&gt;Cloudfarms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codecomprehension.com&#34;&gt;CodeComprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codept.de/&#34;&gt;Codept&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com/en&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collibra.com/&#34;&gt;Collibra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.compellon.com/&#34;&gt;Compellon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complicatedrobot.com/&#34;&gt;Complicated Robot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conduktor.io&#34;&gt;Conduktor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.contramap.dev&#34;&gt;Contramap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coralogix.com&#34;&gt;Coralogix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creditkarma.com&#34;&gt;Credit Karma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.currencycloud.com/&#34;&gt;CurrencyCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://de-solution.com/&#34;&gt;D.E.Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datachef.co&#34;&gt;DataChef&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.demandbase.com&#34;&gt;Demandbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demyst.com&#34;&gt;Demyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devsisters.com/&#34;&gt;Devsisters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijdhl.nl/it&#34;&gt;DHL Parcel The Netherlands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+ Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doomoolmori.com/&#34;&gt;Doomoolmori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dowjones.com&#34;&gt;Dow Jones&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dpgrecruitment.nl&#34;&gt;DPG recruitment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.com&#34;&gt;Dream11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iot.telekom.com/en&#34;&gt;Deutsche Telekom IoT GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebay.com&#34;&gt;eBay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eaglescience.nl&#34;&gt;Eaglescience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edf.fr/&#34;&gt;Electricité de France (EDF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.enelx.com&#34;&gt;EnelX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evolution.engineering&#34;&gt;Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evo.company&#34;&gt;Evo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flipp.com/&#34;&gt;Flipp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fugo.ai&#34;&gt;Fugo.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.garnercorp.com/&#34;&gt;Garner Distributed Workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gleancompany.com&#34;&gt;Glean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://grandparade.co.uk&#34;&gt;GrandParade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://greyflower.media&#34;&gt;greyflower.media GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hunters.ai&#34;&gt;Hunters.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypefactors.com/&#34;&gt;Hypefactors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.iheart.com/&#34;&gt;iHeartRadio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ihsmarkit.com/&#34;&gt;IHS Markit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://investsuite.com/&#34;&gt;Investsuite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaizen-solutions.net/&#34;&gt;Kaizen Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kamon.io/&#34;&gt;Kamon APM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kodmagi.se&#34;&gt;Kodmagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kensu.io&#34;&gt;Kensu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lambdaworks.io/&#34;&gt;LambdaWorks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leadiq.com&#34;&gt;LeadIQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lernkunst.com/&#34;&gt;Lernkunst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liveintent.com&#34;&gt;LiveIntent Inc.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lottoland.com&#34;&gt;Lottoland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matechs.com&#34;&gt;MATECHS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://megogo.net&#34;&gt;Megogo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mylivn.com/&#34;&gt;Mylivn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newmotion.com&#34;&gt;NewMotion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nexxchange.com&#34;&gt;Nexxchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nike.com&#34;&gt;Nike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nslookup.io&#34;&gt;NsLookup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://olyro.de&#34;&gt;Olyro GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optrak.com&#34;&gt;Optrak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.performance-immo.com/&#34;&gt;Performance Immo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playtika.com&#34;&gt;Playtika&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ppcsamurai.com/&#34;&gt;PPC Samurai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.radix.bio/&#34;&gt;Radix Labs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railroad19.com&#34;&gt;Railroad19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijrandstad.nl&#34;&gt;Randstad Groep Nederland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rapidor.co&#34;&gt;Rapidor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pimsolutions.ru/&#34;&gt;PIM Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rewe-digital.com/&#34;&gt;REWE Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://riskident.com/&#34;&gt;Risk Ident&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocker.com/&#34;&gt;Rocker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rudder.io/&#34;&gt;Rudder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sanjagh.pro/&#34;&gt;Sanjagh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scalac.io/&#34;&gt;Scalac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityscorecard.io/&#34;&gt;SecurityScorecard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sentinelone.com/&#34;&gt;SentinelOne&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.signicat.com/&#34;&gt;Signicat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.sgmarkets.com/en/&#34;&gt;Société Générale Corporate and Investment Banking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaremill.com/&#34;&gt;SoftwareMill&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.streamweaver.com/&#34;&gt;StreamWeaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stuart.com/&#34;&gt;Stuart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teads.com&#34;&gt;Teads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pokemon.com/us/about-pokemon/&#34;&gt;The Pokemon Company International&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tomtom.com&#34;&gt;TomTom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tinka.com/&#34;&gt;Tinka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trackabus.com&#34;&gt;Trackabus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trainor.no&#34;&gt;Trainor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://treutech.io&#34;&gt;TreuTech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tweddle.com&#34;&gt;Tweddle Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.undo.app&#34;&gt;Undo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unit.co&#34;&gt;Unit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univalence.io&#34;&gt;Univalence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.unzer.com&#34;&gt;Unzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vakantiediscounter.nl&#34;&gt;Vakantiediscounter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.verbund.com&#34;&gt;Verbund AG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.waylay.io/&#34;&gt;Waylay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wehkamp.nl&#34;&gt;Wehkamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wolt.com/&#34;&gt;Wolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://o.yandex.ru&#34;&gt;Yandex.Classifieds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://audela.ca&#34;&gt;Audela&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valamis.com&#34;&gt;Valamis Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valsea.com&#34;&gt;Valsea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtuslab.com/&#34;&gt;VirtusLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://getvish.com&#34;&gt;Vish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vivid.money&#34;&gt;Vivid Money&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.com/&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zooz.com/&#34;&gt;Zooz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/ziverge.png&#34; alt=&#34;Ziverge&#34; title=&#34;Ziverge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;Ziverge&lt;/a&gt; is a leading contributor to ZIO.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/scalac.svg?sanitize=true&#34; alt=&#34;Scalac&#34; title=&#34;Scalac&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;Scalac&lt;/a&gt; sponsors ZIO Hackathons and contributes work to multiple projects in ZIO ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/septimal_mind.svg?sanitize=true&#34; alt=&#34;Septimal Mind&#34; title=&#34;Septimal Mind&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/softwaremill.svg?sanitize=true&#34; alt=&#34;SoftwareMill&#34; title=&#34;SoftwareMill&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/yourkit.png&#34; alt=&#34;YourKit&#34; title=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;YourKit&lt;/a&gt; generously provides use of their monitoring and profiling tools to maximize the performance of ZIO applications.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/code_of_conduct.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Come chat with us on &lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Legal&lt;/h3&gt; &#xA;&lt;p&gt;Copyright 2017 - 2020 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>twitter/finagle</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/twitter/finagle</id>
    <link href="https://github.com/twitter/finagle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fault tolerant, protocol-agnostic RPC system&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/twitter/finagle/raw/develop/doc/src/sphinx/_static/logo_medium.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Finagle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/finagle/actions?query=workflow%3A%22continuous+integration%22+branch%3Adevelop&#34;&gt;&lt;img src=&#34;https://github.com/twitter/finagle/workflows/continuous%20integration/badge.svg?branch=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/twitter/finagle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/twitter/finagle/branch/develop/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/#status&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/status-active-brightgreen.svg?sanitize=true&#34; alt=&#34;Project status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/twitter/finagle?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/twitter/finagle.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.twitter/finagle_2.12&#34;&gt;Releases&lt;/a&gt; are done on an approximately monthly schedule. While &lt;a href=&#34;https://semver.org/&#34;&gt;semver&lt;/a&gt; is not followed, the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/finagle/develop/CHANGELOG.rst&#34;&gt;changelogs&lt;/a&gt; are detailed and include sections on public API breaks and changes in runtime behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Getting involved&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;https://twitter.github.io/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source: &lt;a href=&#34;https://github.com/twitter/finagle/&#34;&gt;https://github.com/twitter/finagle/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/finaglers&#34;&gt;finaglers@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://gitter.im/twitter/finagle&#34;&gt;https://gitter.im/twitter/finagle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://finagle.github.io/blog/&#34;&gt;https://finagle.github.io/blog/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols.&lt;/p&gt; &#xA;&lt;p&gt;For extensive documentation, please see the &lt;a href=&#34;https://twitter.github.io/finagle/guide/&#34;&gt;user guide&lt;/a&gt; and &lt;a href=&#34;https://twitter.github.io/finagle/docs/com/twitter/finagle&#34;&gt;API documentation&lt;/a&gt; websites. Documentation improvements are always welcome, so please send patches our way.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The following are a few of the companies that are using Finagle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://foursquare.com/&#34;&gt;Foursquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ing.nl&#34;&gt;ING Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinterest.com/&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://soundcloud.com/&#34;&gt;SoundCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tumblr.com/&#34;&gt;Tumblr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a more complete list, please see &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/ADOPTERS.md&#34;&gt;our adopter page&lt;/a&gt;. If your organization is using Finagle, consider adding a link there and sending us a pull request!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/.github/raw/main/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;release&lt;/code&gt; branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the &lt;code&gt;develop&lt;/code&gt; branch. In general pull requests should be submitted against &lt;code&gt;develop&lt;/code&gt;. See &lt;a href=&#34;https://github.com/twitter/finagle/raw/release/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2010 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azure-event-hubs-spark</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/azure-event-hubs-spark</id>
    <link href="https://github.com/Azure/azure-event-hubs-spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enabling Continuous Data Processing with Apache Spark and Azure Event Hubs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/event-hubs_spark.png&#34; alt=&#34;Azure Event Hubs + Apache Spark Connector&#34; width=&#34;270&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Azure Event Hubs Connector for Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.png&#34; alt=&#34;chat on gitter&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark&#34;&gt; &lt;img src=&#34;https://travis-ci.org/Azure/azure-event-hubs-spark.svg?branch=master&#34; alt=&#34;build status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#star-our-repo&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/azure/azure-event-hubs-spark.svg?style=social&amp;amp;label=Stars&#34; alt=&#34;star our repo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the source code of the Azure Event Hubs Connector for Apache Spark.&lt;/p&gt; &#xA;&lt;p&gt;Azure Event Hubs is a highly scalable publish-subscribe service that can ingest millions of events per second and stream them into multiple applications. Spark Streaming and Structured Streaming are scalable and fault-tolerant stream processing engines that allow users to process huge amounts of data using complex algorithms expressed with high-level functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, &lt;code&gt;join&lt;/code&gt;, and &lt;code&gt;window&lt;/code&gt;. This data can then be pushed to filesystems, databases, or even back to Event Hubs.&lt;/p&gt; &#xA;&lt;p&gt;By making Event Hubs and Spark easier to use together, we hope this connector makes building scalable, fault-tolerant applications easier for our users.&lt;/p&gt; &#xA;&lt;h2&gt;Latest Releases&lt;/h2&gt; &#xA;&lt;h4&gt;Spark&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark Version&lt;/th&gt; &#xA;   &lt;th&gt;Package Name&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 3.0&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spark 2.4&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Databricks&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Databricks Runtime Version&lt;/th&gt; &#xA;   &lt;th&gt;Artifact Id&lt;/th&gt; &#xA;   &lt;th&gt;Package Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 8.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 7.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.12%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Databricks Runtime 6.X&lt;/td&gt; &#xA;   &lt;td&gt;azure-eventhubs-spark_2.11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/#artifactdetails%7Ccom.microsoft.azure%7Cazure-eventhubs-spark_2.11%7C2.3.22%7Cjar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/maven%20central-2.3.22-brightgreen.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Roadmap&lt;/h4&gt; &#xA;&lt;p&gt;There is an open issue for each planned feature/enhancement.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;We maintain an &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt; - reach out to us via &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark/Lobby&#34;&gt;gitter&lt;/a&gt; if you think anything needs to be added or clarified!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Linking&lt;/h3&gt; &#xA;&lt;p&gt;For Scala/Java applications using SBT/Maven project definitions, link your application with the artifact below. &lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/#latest-releases&#34;&gt;Latest Releases&lt;/a&gt; to find the correct artifact for your version of Apache Spark (or Databricks)!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.11&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId = com.microsoft.azure&#xA;artifactId = azure-eventhubs-spark_2.12&#xA;version = 2.3.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Documentation for our connector can be found &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/docs/&#34;&gt;here&lt;/a&gt;. The integration guides there contain all the information you need to use this library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;re new to Apache Spark and/or Event Hubs, then we highly recommend reading their documentation first.&lt;/strong&gt; You can read Event Hubs documentation &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-what-is-event-hubs&#34;&gt;here&lt;/a&gt;, documentation for Spark Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;, and, the last but not least, Structured Streaming &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Further Assistance&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you need additional assistance, please don&#39;t hesitate to ask!&lt;/strong&gt; General questions and discussion should happen on our &lt;a href=&#34;https://gitter.im/azure-event-hubs-spark&#34;&gt;gitter chat&lt;/a&gt;. Please open an issue for bug reports and feature requests! Feedback, feature requests, bug reports, etc are all welcomed!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to help contribute (we&#39;d love to have your help!), then go to our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Build Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;In order to use the connector, you need to have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 1.8 SDK installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maven.apache.org/download.cgi&#34;&gt;Maven 3.x&lt;/a&gt; installed (or &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/index.html&#34;&gt;SBT version 1.x&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details on building from source and running tests can be found in our &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-event-hubs-spark/master/.github/CONTRIBUTING.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build Command&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Builds jar and runs all tests&#xA;mvn clean package&#xA;&#xA;// Builds jar, runs all tests, and installs jar to your local maven repository&#xA;mvn clean install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark™&lt;/a&gt;: This connector allows Apache Spark™ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table’s metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-kyuubi</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/apache/incubator-kyuubi</id>
    <link href="https://github.com/apache/incubator-kyuubi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Kyuubi is a distributed multi-tenant JDBC server for large-scale data processing and analytics, built on top of Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Kyuubi (Incubating)&lt;/h1&gt; &#xA;&lt;img src=&#34;https://svn.apache.org/repos/asf/comdev/project-logos/originals/kyuubi-1.svg?sanitize=true&#34; alt=&#34;Kyuubi logo&#34; height=&#34;120px&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/apache/incubator-kyuubi?label=release&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://tokei.rs/b1/github.com/apache/incubator-kyuubi&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/incubator-kyuubi/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/apache/incubator-kyuubi/Kyuubi/master?style=plastic&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;a href=&#34;https://travis-ci.com/apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/incubator-kyuubi.svg?branch=master&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/kyuubi/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/apache/incubator-kyuubi&#34; alt=&#34;GitHub top language&#34;&gt; &lt;a href=&#34;https://github.com/apache/incubator-kyuubi/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/apache/incubator-kyuubi&#34; alt=&#34;Commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/incubator-kyuubi&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/apache/incubator-kyuubi.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Kyuubi?&lt;/h2&gt; &#xA;&lt;p&gt;Kyuubi is a distributed multi-tenant Thrift JDBC/ODBC server for large-scale data management, processing, and analytics, built on top of Apache Spark and designed to support more engines (i.e., Flink). It has been open-sourced by NetEase since 2018. We are aiming to make Kyuubi an &#34;out-of-the-box&#34; tool for data warehouses and data lakes.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi provides a pure SQL gateway through Thrift JDBC/ODBC interface for end-users to manipulate large-scale data with pre-programmed and extensible Spark SQL engines. This &#34;out-of-the-box&#34; model minimizes the barriers and costs for end-users to use Spark at the client side. At the server-side, Kyuubi server and engines&#39; multi-tenant architecture provides the administrators a way to achieve computing resource isolation, data security, high availability, high client concurrency, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_positioning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A HiveServer2-like API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-tenant Spark Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Running Spark in a serverless way&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target Users&lt;/h3&gt; &#xA;&lt;p&gt;Kyuubi&#39;s goal is to make it easy and efficient for &lt;code&gt;anyone&lt;/code&gt; to use Spark(maybe other engines soon) and facilitate users to handle big data like ordinary data. Here, &lt;code&gt;anyone&lt;/code&gt; means that users do not need to have a Spark technical background but a human language, SQL only. Sometimes, SQL skills are unnecessary when integrating Kyuubi with Apache Superset, which supports rich visualizations and dashboards.&lt;/p&gt; &#xA;&lt;p&gt;In typical big data production environments with Kyuubi, there should be system administrators and end-users.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System administrators: A small group consists of Spark experts responsible for Kyuubi deployment, configuration, and tuning.&lt;/li&gt; &#xA; &lt;li&gt;End-users: Focus on business data of their own, not where it stores, how it computes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, the Kyuubi community will continuously optimize the whole system with various features, such as History-Based Optimizer, Auto-tuning, Materialized View, SQL Dialects, Functions, e.t.c.&lt;/p&gt; &#xA;&lt;h3&gt;Usage scenarios&lt;/h3&gt; &#xA;&lt;h4&gt;Port workloads from HiveServer2 to Spark SQL&lt;/h4&gt; &#xA;&lt;p&gt;In typical big data production environments, especially secured ones, all bundled services manage access control lists to restricting access to authorized users. For example, Hadoop YARN divides compute resources into queues. With Queue ACLs, it can identify and control which users/groups can take actions on particular queues. Similarly, HDFS ACLs control access of HDFS files by providing a way to set different permissions for specific users/groups.&lt;/p&gt; &#xA;&lt;p&gt;Apache Spark is a unified analytics engine for large-scale data processing. It provides a Distributed SQL Engine, a.k.a, the Spark Thrift Server(STS), designed to be seamlessly compatible with HiveServer2 and get even better performance.&lt;/p&gt; &#xA;&lt;p&gt;HiveServer2 can identify and authenticate a caller, and then if the caller also has permissions for the YARN queue and HDFS files, it succeeds. Otherwise, it fails. However, on the one hand, STS is a single Spark application. The user and queue to which STS belongs are uniquely determined at startup. Consequently, STS cannot leverage cluster managers such as YARN and Kubernetes for resource isolation and sharing or control the access for callers by the single user inside the whole system. On the other hand, the Thrift Server is coupled in the Spark driver&#39;s JVM process. This coupled architect puts a high risk on server stability and makes it unable to handle high client concurrency or apply high availability such as load balancing as it is stateful.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi extends the use of STS in a multi-tenant model based on a unified interface and relies on the concept of multi-tenancy to interact with cluster managers to finally gain the ability of resources sharing/isolation and data security. The loosely coupled architecture of the Kyuubi server and engine dramatically improves the client concurrency and service stability of the service itself.&lt;/p&gt; &#xA;&lt;h4&gt;DataLake/LakeHouse Support&lt;/h4&gt; &#xA;&lt;p&gt;The vision of Kyuubi is to unify the portal and become an easy-to-use data lake management platform. Different kinds of workloads, such as ETL processing and BI analytics, can be supported by one platform, using one copy of data, with one SQL interface.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logical View support via Kyuubi DataLake Metadata APIs&lt;/li&gt; &#xA; &lt;li&gt;Multiple Catalogs support&lt;/li&gt; &#xA; &lt;li&gt;SQL Standard Authorization support for DataLake(coming)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Cloud Native Support&lt;/h4&gt; &#xA;&lt;p&gt;Kyuubi can deploy its engines on different kinds of Cluster Managers, such as, Hadoop YARN, Kubernetes, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_migrating_yarn_to_k8s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The Kyuubi Ecosystem(present and future)&lt;/h3&gt; &#xA;&lt;p&gt;The figure below shows our vision for the Kyuubi Ecosystem. Some of them have been realized, some in development, and others would not be possible without your help.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/docs/imgs/kyuubi_ecosystem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Since Kyuubi 1.3.0-incubating, the Kyuubi online documentation is hosted by &lt;a href=&#34;https://kyuubi.apache.org/&#34;&gt;https://kyuubi.apache.org/&lt;/a&gt;. You can find the latest Kyuubi documentation on &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/&#34;&gt;this web page&lt;/a&gt;. For 1.2 and earlier versions, please check the &lt;a href=&#34;https://kyuubi.readthedocs.io/en/v1.2.0/&#34;&gt;Readthedocs&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ready? &lt;a href=&#34;https://kyuubi.apache.org/docs/latest/quick_start/quick_start.html&#34;&gt;Getting Started&lt;/a&gt; with Kyuubi.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Contributor over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://api7.ai/contributor-graph?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34;&gt;&lt;img src=&#34;https://contributor-graph-api.apiseven.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=apache/incubator-kyuubi&#34; alt=&#34;Contributor over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Aside&lt;/h2&gt; &#xA;&lt;p&gt;The project took its name from a character of a popular Japanese manga - &lt;code&gt;Naruto&lt;/code&gt;. The character is named &lt;code&gt;Kyuubi Kitsune/Kurama&lt;/code&gt;, which is a nine-tailed fox in mythology. &lt;code&gt;Kyuubi&lt;/code&gt; spread the power and spirit of fire, which is used here to represent the powerful &lt;a href=&#34;http://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;. Its nine tails stand for end-to-end multi-tenancy support of this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-kyuubi/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>polomarcus/tp</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/polomarcus/tp</id>
    <link href="https://github.com/polomarcus/tp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practices - Data engineering&lt;/h1&gt; &#xA;&lt;h2&gt;Tools you need&lt;/h2&gt; &#xA;&lt;p&gt;Have a stackoverflow account : &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;https://stackoverflow.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a github account : &lt;a href=&#34;https://github.com/&#34;&gt;https://github.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a github repo to push your code.&lt;/p&gt; &#xA;&lt;h3&gt;Fork the repo on your own Github account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/tp/fork&#34;&gt;https://github.com/polomarcus/tp/fork&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker and Compose&lt;/h3&gt; &#xA;&lt;p&gt;Take time to read and install&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/overview/&#34;&gt;https://docs.docker.com/get-started/overview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker --version&#xA;Docker version 20.10.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;https://docs.docker.com/compose/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose --version&#xA;docker-compose version 1.29.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TP1 - &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-docker-kafka-bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP2 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-functional-programming-scala&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP3 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-data-processing-framework&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP 4 - Kafka Streams to read and write to Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34;&gt;https://kafka.apache.org/documentation/streams/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&#34;&gt;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka User Interface (UI) : &lt;a href=&#34;https://www.conduktor.io/download/&#34;&gt;https://www.conduktor.io/download/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/aws-sqsd</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/aws-sqsd</id>
    <link href="https://github.com/olxbr/aws-sqsd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances, based on https://github.com/mozart-analytics/sqsd&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AWS SQS Worker Daemon&lt;/h1&gt; &#xA;&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;IMPORTANT:&lt;/em&gt; In order for &lt;code&gt;sqsd&lt;/code&gt; to work, you have to have configured the AWS Authentication Keys on you environment either as ENV VARS or using any of the other methods that AWS provides. For ways to do this, go &lt;a href=&#34;http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Using Environment Variables&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Property&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_DEFAULT_REGION&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;us-east-1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;The region name of the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The access key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The secret key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_QUEUE_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Your queue URL.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_CONCURRENCY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max number of messages process in parallel.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WAIT_TIME_SECONDS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;20&lt;/code&gt; (max: &lt;code&gt;20&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Long polling wait time when querying the queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path where to POST the messages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_REQUEST_CONTENT_TYPE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;application/json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Message MIME Type.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_TIMEOUT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max time that waiting for a worker response in milliseconds.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path for your service health.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_WAIT_TIME&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Time to between health checks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt compile&#xA;sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using an SBT&#39;s docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt compile&#xA;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to build the docker image&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build --tag aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;docker build --tag &amp;lt;some_company&amp;gt;/aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;You should use the pre created GZVR&#39;s image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run vivareal/aws-sqsd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or the image created by yourself&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run some_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue, even better if you can submit a pull request :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datastax/spark-cassandra-connector</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/datastax/spark-cassandra-connector</id>
    <link href="https://github.com/datastax/spark-cassandra-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DataStax Spark Cassandra Connector&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Cassandra Connector&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/datastax/spark-cassandra-connector/actions/workflows/main.yml/badge.svg?branch=master&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What&lt;/th&gt; &#xA;   &lt;th&gt;Where&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Community&lt;/td&gt; &#xA;   &lt;td&gt;Chat with us at &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;Datastax and Cassandra Q&amp;amp;A&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala Docs&lt;/td&gt; &#xA;   &lt;td&gt;Most Recent Release (3.2.0): &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;, &lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/driver/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector-Driver&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latest Production Release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.2.0/jar&#34;&gt;3.2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra®.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library lets you expose Cassandra tables as Spark RDDs and Datasets/DataFrames, write Spark RDDs and Datasets/DataFrames to Cassandra tables, and execute arbitrary CQL queries in your Spark applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compatible with Apache Cassandra version 2.1 or higher (see table below)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Apache Spark 1.0 through 3.2 (&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/#version-compatibility&#34;&gt;see table below&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Compatible with Scala 2.11 and 2.12&lt;/li&gt; &#xA; &lt;li&gt;Exposes Cassandra tables as Spark RDDs and Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Maps table rows to CassandraRow objects or tuples&lt;/li&gt; &#xA; &lt;li&gt;Offers customizable object mapper for mapping rows to objects of user-defined classes&lt;/li&gt; &#xA; &lt;li&gt;Saves RDDs back to Cassandra by implicit &lt;code&gt;saveToCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Delete rows and columns from cassandra by implicit &lt;code&gt;deleteFromCassandra&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Join with a subset of Cassandra data using &lt;code&gt;joinWithCassandraTable&lt;/code&gt; call for RDDs, and optimizes join with data in Cassandra when using Datasets/DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Partition RDDs according to Cassandra replication using &lt;code&gt;repartitionByCassandraReplica&lt;/code&gt; call&lt;/li&gt; &#xA; &lt;li&gt;Converts data types between Cassandra and Scala&lt;/li&gt; &#xA; &lt;li&gt;Supports all Cassandra data types including collections&lt;/li&gt; &#xA; &lt;li&gt;Filters rows on the server side via the CQL &lt;code&gt;WHERE&lt;/code&gt; clause&lt;/li&gt; &#xA; &lt;li&gt;Allows for execution of arbitrary CQL statements&lt;/li&gt; &#xA; &lt;li&gt;Plays nice with Cassandra Virtual Nodes&lt;/li&gt; &#xA; &lt;li&gt;Could be used in all languages supporting Datasets/DataFrames API: Python, R, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Version Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The connector project has several branches, each of which map into different supported versions of Spark and Cassandra. For previous releases the branch is named &#34;bX.Y&#34; where X.Y is the major+minor version; for example the &#34;b1.6&#34; branch corresponds to the 1.6 release. The &#34;master&#34; branch will normally contain development for the next connector release in progress.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the following branches are actively supported: 3.2.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/master&#34;&gt;master&lt;/a&gt;), 3.1.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.1&#34;&gt;b3.1&lt;/a&gt;), 3.0.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b3.0&#34;&gt;b3.0&lt;/a&gt;) and 2.5.x (&lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector/tree/b2.5&#34;&gt;b2.5&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Spark&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra&lt;/th&gt; &#xA;   &lt;th&gt;Cassandra Java Driver&lt;/th&gt; &#xA;   &lt;th&gt;Minimum Java Version&lt;/th&gt; &#xA;   &lt;th&gt;Supported Scala Versions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.13&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;3.1&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x, 4.0&lt;/td&gt; &#xA;   &lt;td&gt;4.12&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.2&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0, 2.1, 2.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.x&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;1.5, 1.6&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*, 2.2, 3.0&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;   &lt;td&gt;2.1.5*&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;   &lt;td&gt;1.1, 1.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1, 2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0, 0.9&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2.10, 2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*&lt;em&gt;Compatible with 2.1.X where X &amp;gt;= 5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hosted API Docs&lt;/h2&gt; &#xA;&lt;p&gt;API documentation for the Scala and Java interfaces are available online:&lt;/p&gt; &#xA;&lt;h3&gt;3.2.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.0.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.5.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/&#34;&gt;Spark-Cassandra-Connector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/&#34;&gt;Embedded-Cassandra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;This project is available on the Maven Central Repository. For SBT to download the connector binaries, sources and javadoc, put this in your project SBT config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.datastax.spark&#34; %% &#34;spark-cassandra-connector&#34; % &#34;3.2.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default Scala version for Spark 3.0+ is 2.12 please choose the appropriate build. See the &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;FAQ&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/0_quick_start.md&#34;&gt;Quick-start guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/1_connecting.md&#34;&gt;Connecting to Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/2_loading.md&#34;&gt;Loading datasets from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/3_selection.md&#34;&gt;Server-side data selection and filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/4_mapper.md&#34;&gt;Working with user-defined case classes and tuples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/5_saving.md&#34;&gt;Saving and deleting datasets to/from Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/6_advanced_mapper.md&#34;&gt;Customizing the object mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/7_java_api.md&#34;&gt;Using Connector in Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/8_streaming.md&#34;&gt;Spark Streaming with Cassandra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/10_embedded.md&#34;&gt;The spark-cassandra-connector-embedded Artifact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/11_metrics.md&#34;&gt;Performance monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/12_building_and_artifacts.md&#34;&gt;Building And Artifacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/13_spark_shell.md&#34;&gt;The Spark Shell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/14_data_frames.md&#34;&gt;DataFrames&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/15_python.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/16_partitioning.md&#34;&gt;Partitioner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/17_submitting.md&#34;&gt;Submitting applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/reference.md&#34;&gt;Configuration Parameter Reference Table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Training&lt;/h2&gt; &#xA;&lt;h3&gt;DataStax Academy&lt;/h3&gt; &#xA;&lt;p&gt;DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In &lt;a href=&#34;https://academy.datastax.com/courses/ds320-analytics-with-apache-spark&#34;&gt;DS320: Analytics with Spark&lt;/a&gt;, you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Reporting Bugs&lt;/h3&gt; &#xA;&lt;p&gt;New issues may be reported using &lt;a href=&#34;https://datastax-oss.atlassian.net/browse/SPARKC/&#34;&gt;JIRA&lt;/a&gt;. Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Questions and requests for help may be submitted to the &lt;a href=&#34;https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user&#34;&gt;user mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Q/A Exchange&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://community.datastax.com/index.html&#34;&gt;DataStax Community&lt;/a&gt; provides a free question and answer website for any and all questions relating to any DataStax Related technology. Including the Spark Cassandra Connector. Both DataStax engineers and community members frequent this board and answer questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To protect the community, all contributors are required to sign the &lt;a href=&#34;http://spark-cassandra-connector-cla.datastax.com/&#34;&gt;DataStax Spark Cassandra Connector Contribution License Agreement&lt;/a&gt;. The process is completely electronic and should only take a few minutes.&lt;/p&gt; &#xA;&lt;p&gt;To develop this project, we recommend using IntelliJ IDEA. Make sure you have installed and enabled the Scala Plugin. Open the project with IntelliJ IDEA and it will automatically create the project structure from the provided SBT configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checklist for contributing changes to the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;a href=&#34;https://datastax-oss.atlassian.net/projects/SPARKC/issues&#34;&gt;SPARKC JIRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make sure that all unit tests and integration tests pass&lt;/li&gt; &#xA; &lt;li&gt;Add an appropriate entry at the top of CHANGES.txt&lt;/li&gt; &#xA; &lt;li&gt;If the change has any end-user impacts, also include changes to the ./doc files as needed&lt;/li&gt; &#xA; &lt;li&gt;Prefix the pull request description with the JIRA number, for example: &#34;SPARKC-123: Fix the ...&#34;&lt;/li&gt; &#xA; &lt;li&gt;Open a pull-request on GitHub and await review&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To run unit and integration tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the integration tests require &lt;a href=&#34;https://github.com/riptano/ccm&#34;&gt;CCM&lt;/a&gt; to be installed on your machine. See &lt;a href=&#34;https://raw.githubusercontent.com/datastax/spark-cassandra-connector/master/doc/developers.md&#34;&gt;Tips for Developing the Spark Cassandra Connector&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;By default, integration tests start up a separate, single Cassandra instance and run Spark in local mode. It is possible to run integration tests with your own Cassandra and/or Spark cluster. First, prepare a jar with testing code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt test:package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then copy the generated test jar to your Spark nodes and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export IT_TEST_CASSANDRA_HOST=&amp;lt;IP of one of the Cassandra nodes&amp;gt;&#xA;export IT_TEST_SPARK_MASTER=&amp;lt;Spark Master URL&amp;gt;&#xA;./sbt/sbt it:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Documents&lt;/h2&gt; &#xA;&lt;p&gt;To generate the Reference Document use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt/sbt spark-cassandra-connector-unshaded/run (outputLocation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;outputLocation defaults to doc/reference.md&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2014-2017, DataStax, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>windymelt/FNFIS</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/windymelt/FNFIS</id>
    <link href="https://github.com/windymelt/FNFIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;FNFIS - FreeNet File Indexing on Scala Testing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yutax77/KeyValueStore-scala</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/yutax77/KeyValueStore-scala</id>
    <link href="https://github.com/yutax77/KeyValueStore-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KeyValueStore&lt;/h1&gt; &#xA;&lt;p&gt;TDDBC Tokyo1.6&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>confluentinc/kafka-deprecated-fork</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/confluentinc/kafka-deprecated-fork</id>
    <link href="https://github.com/confluentinc/kafka-deprecated-fork" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mirror of Apache Kafka&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Kafka&lt;/h1&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;http://kafka.apache.org&#34;&gt;web site&lt;/a&gt; for details on the project.&lt;/p&gt; &#xA;&lt;p&gt;You need to have &lt;a href=&#34;http://www.gradle.org/installation&#34;&gt;gradle&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;h3&gt;First bootstrap and download the wrapper&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd kafka_source_dir&#xA;gradle&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now everything else will work&lt;/p&gt; &#xA;&lt;h3&gt;Building a jar and running it&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew jar  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow instuctions in &lt;a href=&#34;http://kafka.apache.org/documentation.html#quickstart&#34;&gt;http://kafka.apache.org/documentation.html#quickstart&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Building source jar&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew srcJar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building javadocs and scaladocs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew javadoc&#xA;./gradlew javadocJar # builds a jar from the javadocs&#xA;./gradlew scaladoc&#xA;./gradlew scaladocJar # builds a jar from the scaladocs&#xA;./gradlew docsJar # builds both javadoc and scaladoc jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running unit tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Forcing re-running unit tests w/o code change&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew cleanTest test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running a particular unit test&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew -Dtest.single=RequestResponseSerializationTest core:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running a particular test method within a unit test&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew core:test --tests kafka.api.test.ProducerFailureHandlingTest.testCannotSendToInternalTopic&#xA;./gradlew clients:test --tests org.apache.kafka.clients.producer.MetadataTest.testMetadataUpdateWaitTime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running a particular unit test with log4j output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;change the log4j setting in either clients/src/test/resources/log4j.properties or core/src/test/resources/log4j.properties&#xA;./gradlew -i -Dtest.single=RequestResponseSerializationTest core:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building a binary release gzipped tar ball&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew clean&#xA;./gradlew releaseTarGz  &#xA;The above command will fail if you haven&#39;t set up the signing key. To bypass signing the artifact, you can run&#xA;./gradlew releaseTarGz -x signArchives&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The release file can be found inside ./core/build/distributions/.&lt;/p&gt; &#xA;&lt;h3&gt;Cleaning the build&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew clean&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running a task on a particular version of Scala (either 2.9.1, 2.9.2, 2.10.5 or 2.11.6)&lt;/h3&gt; &#xA;&lt;h4&gt;(If building a jar with a version other than 2.10, need to set SCALA_BINARY_VERSION variable or change it in bin/kafka-run-class.sh to run quick start.)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew -PscalaVersion=2.9.1 jar&#xA;./gradlew -PscalaVersion=2.9.1 test&#xA;./gradlew -PscalaVersion=2.9.1 releaseTarGz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running a task for a specific project&lt;/h3&gt; &#xA;&lt;p&gt;This is for &#39;core&#39;, &#39;contrib:hadoop-consumer&#39;, &#39;contrib:hadoop-producer&#39;, &#39;examples&#39; and &#39;clients&#39; ./gradlew core:jar ./gradlew core:test&lt;/p&gt; &#xA;&lt;h3&gt;Listing all gradle tasks&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building IDE project&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew eclipse&#xA;./gradlew idea&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the jar for all scala versions and for all projects&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew jarAll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running unit tests for all scala versions and for all projects&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew testAll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building a binary release gzipped tar ball for all scala versions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew releaseTarGzAll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Publishing the jar for all version of Scala and for all projects to maven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew uploadArchivesAll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note for this to work you should create/update &lt;code&gt;~/.gradle/gradle.properties&lt;/code&gt; and assign the following variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mavenUrl=&#xA;mavenUsername=&#xA;mavenPassword=&#xA;signing.keyId=&#xA;signing.password=&#xA;signing.secretKeyRingFile=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Publishing the jars without signing to a local repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew -Dorg.gradle.project.skipSigning=true -Dorg.gradle.project.mavenUrl=file://path/to/repo uploadArchivesAll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the test jar&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew testJar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Determining how transitive dependencies are added&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew core:dependencies --configuration runtime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running checkstyle on the java code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;./gradlew checkstyleMain checkstyleTest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running in Vagrant&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/confluentinc/kafka-deprecated-fork/trunk/vagrant/README.md&#34;&gt;vagrant/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;Apache Kafka is interested in building the community; we would welcome any thoughts or &lt;a href=&#34;https://issues.apache.org/jira/browse/KAFKA&#34;&gt;patches&lt;/a&gt;. You can reach us &lt;a href=&#34;http://kafka.apache.org/contact.html&#34;&gt;on the Apache mailing lists&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute follow the instructions here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/contributing.html&#34;&gt;http://kafka.apache.org/contributing.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also welcome patches for the website and documentation which can be found here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://svn.apache.org/repos/asf/kafka/site&#34;&gt;https://svn.apache.org/repos/asf/kafka/site&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-sql-perf</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/spark-sql-perf</id>
    <link href="https://github.com/databricks/spark-sql-perf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark SQL Performance Tests&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-sql-perf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-sql-perf.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a performance testing framework for &lt;a href=&#34;https://spark.apache.org/sql/&#34;&gt;Spark SQL&lt;/a&gt; in &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; 2.2+.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This README is still under development. Please also check our source code for more information.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Running from command line.&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/run --help&#xA;&#xA;spark-sql-perf 0.2.0&#xA;Usage: spark-sql-perf [options]&#xA;&#xA;  -b &amp;lt;value&amp;gt; | --benchmark &amp;lt;value&amp;gt;&#xA;        the name of the benchmark to run&#xA;  -m &amp;lt;value&amp;gt; | --master &amp;lt;value&#xA;        the master url to use&#xA;  -f &amp;lt;value&amp;gt; | --filter &amp;lt;value&amp;gt;&#xA;        a filter on the name of the queries to run&#xA;  -i &amp;lt;value&amp;gt; | --iterations &amp;lt;value&amp;gt;&#xA;        the number of iterations to run&#xA;  --help&#xA;        prints this usage text&#xA;        &#xA;$ bin/run --benchmark DatasetPerformance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first run of &lt;code&gt;bin/run&lt;/code&gt; will build the library.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;sbt package&lt;/code&gt; or &lt;code&gt;sbt assembly&lt;/code&gt; to build the library jar.&lt;br&gt; Use &lt;code&gt;sbt +package&lt;/code&gt; to build for scala 2.11 and 2.12.&lt;/p&gt; &#xA;&lt;h2&gt;Local performance tests&lt;/h2&gt; &#xA;&lt;p&gt;The framework contains twelve benchmarks that can be executed in local mode. They are organized into three classes and target different components and functions of Spark:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/DatasetPerformance.scala&#34;&gt;DatasetPerformance&lt;/a&gt; compares the performance of the old RDD API with the new Dataframe and Dataset APIs. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark DatasetPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/JoinPerformance.scala&#34;&gt;JoinPerformance&lt;/a&gt; compares the performance of joining different table sizes and shapes with different join types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark JoinPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/AggregationPerformance.scala&#34;&gt;AggregationPerformance&lt;/a&gt; compares the performance of aggregating different table sizes using different aggregation types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark AggregationPerformance&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;MLlib tests&lt;/h1&gt; &#xA;&lt;p&gt;To run MLlib tests, run &lt;code&gt;/bin/run-ml yamlfile&lt;/code&gt;, where &lt;code&gt;yamlfile&lt;/code&gt; is the path to a YAML configuration file describing tests to run and their parameters.&lt;/p&gt; &#xA;&lt;h1&gt;TPC-DS&lt;/h1&gt; &#xA;&lt;h2&gt;Setup a benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Before running any query, a dataset needs to be setup by creating a &lt;code&gt;Benchmark&lt;/code&gt; object. Generating the TPCDS data requires dsdgen built and available on the machines. We have a fork of dsdgen that you will need. The fork includes changes to generate TPCDS data to stdout, so that this library can pipe them directly to Spark, without intermediate files. Therefore, this library will not work with the vanilla TPCDS kit.&lt;/p&gt; &#xA;&lt;p&gt;TPCDS kit needs to be installed on all cluster executor nodes under the same path!&lt;/p&gt; &#xA;&lt;p&gt;It can be found &lt;a href=&#34;https://github.com/databricks/tpcds-kit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Generate the data&#xA;build/sbt &#34;test:runMain com.databricks.spark.sql.perf.tpcds.GenTPCDSData -d &amp;lt;dsdgenDir&amp;gt; -s &amp;lt;scaleFactor&amp;gt; -l &amp;lt;location&amp;gt; -f &amp;lt;format&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Create the specified database&#xA;sql(s&#34;create database $databaseName&#34;)&#xA;// Create metastore tables in a specified database for your data.&#xA;// Once tables are created, the current database will be switched to the specified database.&#xA;tables.createExternalTables(rootDir, &#34;parquet&#34;, databaseName, overwrite = true, discoverPartitions = true)&#xA;// Or, if you want to create temporary tables&#xA;// tables.createTemporaryTables(location, format)&#xA;&#xA;// For CBO only, gather statistics on all columns:&#xA;tables.analyzeTables(databaseName, analyzeColumns = true) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run benchmarking queries&lt;/h2&gt; &#xA;&lt;p&gt;After setup, users can use &lt;code&gt;runExperiment&lt;/code&gt; function to run benchmarking queries and record query execution time. Taking TPC-DS as an example, you can start an experiment by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import com.databricks.spark.sql.perf.tpcds.TPCDS&#xA;&#xA;val tpcds = new TPCDS (sqlContext = sqlContext)&#xA;// Set:&#xA;val databaseName = ... // name of database with TPCDS data.&#xA;val resultLocation = ... // place to write results&#xA;val iterations = 1 // how many iterations of queries to run.&#xA;val queries = tpcds.tpcds2_4Queries // queries to run.&#xA;val timeout = 24*60*60 // timeout, in seconds.&#xA;// Run:&#xA;sql(s&#34;use $databaseName&#34;)&#xA;val experiment = tpcds.runExperiment(&#xA;  queries, &#xA;  iterations = iterations,&#xA;  resultLocation = resultLocation,&#xA;  forkThread = true)&#xA;experiment.waitForFinish(timeout)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, experiment will be started in a background thread. For every experiment run (i.e. every call of &lt;code&gt;runExperiment&lt;/code&gt;), Spark SQL Perf will use the timestamp of the start time to identify this experiment. Performance results will be stored in the sub-dir named by the timestamp in the given &lt;code&gt;spark.sql.perf.results&lt;/code&gt; (for example &lt;code&gt;/tmp/results/timestamp=1429213883272&lt;/code&gt;). The performance results are stored in the JSON format.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieve results&lt;/h2&gt; &#xA;&lt;p&gt;While the experiment is running you can use &lt;code&gt;experiment.html&lt;/code&gt; to get a summary, or &lt;code&gt;experiment.getCurrentResults&lt;/code&gt; to get complete current results. Once the experiment is complete, you can still access &lt;code&gt;experiment.getCurrentResults&lt;/code&gt;, or you can load the results from disk.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Get all experiments results.&#xA;val resultTable = spark.read.json(resultLocation)&#xA;resultTable.createOrReplaceTempView(&#34;sqlPerformance&#34;)&#xA;sqlContext.table(&#34;sqlPerformance&#34;)&#xA;// Get the result of a particular run by specifying the timestamp of that run.&#xA;sqlContext.table(&#34;sqlPerformance&#34;).filter(&#34;timestamp = 1429132621024&#34;)&#xA;// or&#xA;val specificResultTable = spark.read.json(experiment.resultPath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get a basic summary by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experiment.getCurrentResults // or: spark.read.json(resultLocation).filter(&#34;timestamp = 1429132621024&#34;)&#xA;  .withColumn(&#34;Name&#34;, substring(col(&#34;name&#34;), 2, 100))&#xA;  .withColumn(&#34;Runtime&#34;, (col(&#34;parsingTime&#34;) + col(&#34;analysisTime&#34;) + col(&#34;optimizationTime&#34;) + col(&#34;planningTime&#34;) + col(&#34;executionTime&#34;)) / 1000.0)&#xA;  .select(&#39;Name, &#39;Runtime)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;TPC-H&lt;/h1&gt; &#xA;&lt;p&gt;TPC-H can be run similarly to TPC-DS replacing &lt;code&gt;tpcds&lt;/code&gt; for &lt;code&gt;tpch&lt;/code&gt;. Take a look at the data generator and &lt;code&gt;tpch_run&lt;/code&gt; notebook code below.&lt;/p&gt; &#xA;&lt;h2&gt;Running in Databricks workspace (or spark-shell)&lt;/h2&gt; &#xA;&lt;p&gt;There are example notebooks in &lt;code&gt;src/main/notebooks&lt;/code&gt; for running TPCDS and TPCH in the Databricks environment. &lt;em&gt;These scripts can also be run from spark-shell command line with minor modifications using &lt;code&gt;:load file_name.scala&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TPC-multi_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook (or scala script) can be use to generate both TPCDS and TPCH data at selected scale factors. It is a newer version from the &lt;code&gt;tpcds_datagen&lt;/code&gt; notebook below. To use it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit the config variables the top of the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the whole notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpcds_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to install dsdgen on all worker nodes, run data generation, and create the TPCDS database. Note that because of the way dsdgen is installed, it will not work on an autoscaling cluster, and &lt;code&gt;num_workers&lt;/code&gt; has to be updated to the number of worker instances on the cluster. Data generation may also break if any of the workers is killed - the restarted worker container will not have &lt;code&gt;dsdgen&lt;/code&gt; anymore.&lt;/p&gt; &#xA;&lt;h3&gt;tpcds_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCDS queries.&lt;/p&gt; &#xA;&lt;p&gt;For running parallel TPCDS streams:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Cluster and attach the spark-sql-perf library to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a Job using the notebook and attaching to the created cluster as &#34;existing cluster&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Allow concurrent runs of the created job.&lt;/li&gt; &#xA; &lt;li&gt;Launch appriopriate number of Runs of the Job to run in parallel on the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpch_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCH queries. Data needs be generated first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spark-examples/spark-scala-examples</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/spark-examples/spark-scala-examples</id>
    <link href="https://github.com/spark-examples/spark-scala-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project provides Apache Spark SQL, RDD, DataFrame and Dataset examples in Scala language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Explanation of all Spark SQL, RDD, DataFrame and Dataset examples present on this project are available at &lt;a href=&#34;https://sparkbyexamples.com/&#34;&gt;https://sparkbyexamples.com/&lt;/a&gt; , All these examples are coded in Scala language and tested in our development environment.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents (Spark Examples in Scala)&lt;/h1&gt; &#xA;&lt;h2&gt;Spark RDD Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/&#34;&gt;Create a Spark RDD using Parallelize&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-read-multiple-text-files-into-a-single-rdd/&#34;&gt;Spark – Read multiple text files into single RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-load-csv-file-into-rdd/&#34;&gt;Spark load CSV file into RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/&#34;&gt;Different ways to create Spark RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-how-to-create-an-empty-rdd/&#34;&gt;Spark – How to create an empty RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/&#34;&gt;Spark RDD Transformations with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/&#34;&gt;Spark RDD Actions with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/&#34;&gt;Spark Pair RDD Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/&#34;&gt;Spark Repartition() vs Coalesce()&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-shuffle-partitions/&#34;&gt;Spark Shuffle Partitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-persistence-storage-levels/&#34;&gt;Spark Persistence Storage Levels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-cache-and-persist-example/&#34;&gt;Spark RDD Cache and Persist with Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-broadcast-variables/&#34;&gt;Spark Broadcast Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-accumulators/&#34;&gt;Spark Accumulators Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/&#34;&gt;Convert Spark RDD to DataFrame | Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/&#34;&gt;Spark Create DataFrame with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/&#34;&gt;Spark DataFrame withColumn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/&#34;&gt;Ways to Rename column on Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/&#34;&gt;Spark – How to Drop a DataFrame/Dataset column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-where-filter/&#34;&gt;Working with Spark DataFrame Where Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/&#34;&gt;Spark SQL “case when” and “when otherwise”&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-collect/&#34;&gt;Collect() – Retrieve data from Spark RDD/DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/&#34;&gt;Spark – How to remove duplicate rows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/&#34;&gt;How to Pivot and Unpivot a Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/&#34;&gt;Spark SQL Data Types with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/&#34;&gt;Spark SQL StructType &amp;amp; StructField with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/&#34;&gt;Spark schema – explained with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-groupby-on-dataframe/&#34;&gt;Spark Groupby Example with DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-how-to-sort-dataframe-column-explained/&#34;&gt;Spark – How to Sort DataFrame column explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-join/&#34;&gt;Spark SQL Join Types with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/&#34;&gt;Spark DataFrame Union and UnionAll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/&#34;&gt;Spark map vs mapPartitions transformation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-foreachpartition-vs-foreach-explained/&#34;&gt;Spark foreachPartition vs foreach | what to use?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/&#34;&gt;Spark DataFrame Cache and Persist Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-udf/&#34;&gt;Spark SQL UDF (User Defined Functions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/&#34;&gt;Spark SQL DataFrame Array (ArrayType) Column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/&#34;&gt;Working with Spark DataFrame Map (MapType) column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/&#34;&gt;Spark SQL – Flatten Nested Struct column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/&#34;&gt;Spark – Flatten nested array to single array column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/&#34;&gt;Spark explode array and map columns to rows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/&#34;&gt;Spark SQL String Functions Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/&#34;&gt;Spark SQL Date and Time Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-array-functions/&#34;&gt;Spark SQL Array functions complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-map-functions/&#34;&gt;Spark SQL Map functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-sort-functions/&#34;&gt;Spark SQL Sort functions – complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/&#34;&gt;Spark SQL Aggregate Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-window-functions/&#34;&gt;Spark Window Functions with Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Data Source API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&#34;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-and-write-json-file/&#34;&gt;Spark Read and Write JSON file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/&#34;&gt;Spark Read and Write Apache Parquet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-xml/&#34;&gt;Spark Read XML file using Databricks API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/&#34;&gt;Read &amp;amp; Write Avro files using Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-3-x/&#34;&gt;Using Avro Data Files From Spark SQL 2.3.x or earlier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/&#34;&gt;Spark Read from &amp;amp; Write to HBase table | Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/create-spark-dataframe-from-hbase-using-hortonworks/&#34;&gt;Create Spark DataFrame from HBase using Hortonworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/&#34;&gt;Spark Read ORC file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/&#34;&gt;Spark 3.0 Read Binary File into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Streaming &amp;amp; Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-outputmode/&#34;&gt;Spark Streaming – Different Output modes explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/&#34;&gt;Spark Streaming files from a directory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-from-tcp-socket/&#34;&gt;Spark Streaming – Reading data from TCP Socket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-with-kafka/&#34;&gt;Spark Streaming with Kafka Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-consume-and-produce-kafka-messages-in-avro-format/&#34;&gt;Spark Streaming – Kafka messages in Avro format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-batch-processing-produce-consume-kafka-topic/&#34;&gt;Spark SQL Batch Processing – Produce and Consume Apache Kafka Topic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JetBrains/intellij-scala</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/JetBrains/intellij-scala</id>
    <link href="https://github.com/JetBrains/intellij-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala plugin for IntelliJ IDEA&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub&#34;&gt;&lt;img src=&#34;http://jb.gg/badges/official.svg?sanitize=true&#34; alt=&#34;official JetBrains project&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JetBrains/intellij-scala/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/JetBrains/intellij-scala/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Scala Plugin Build &amp;amp; Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/aUKpZzeHCK&#34;&gt;&lt;img src=&#34;https://badgen.net/badge/icon/discord?icon=discord&amp;amp;label&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/JetBrains/intellij-scala&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Scala Plugin for IntelliJ IDEA&lt;/h1&gt; &#xA;&lt;p&gt;The plugin adds support for the Scala language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Coding assistance (highlighting, completion, formatting, refactorings, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Navigation, search, information about types and implicits&lt;/li&gt; &#xA; &lt;li&gt;Integration with sbt and other build tools&lt;/li&gt; &#xA; &lt;li&gt;Testing frameworks support (ScalaTest, Specs2, uTest)&lt;/li&gt; &#xA; &lt;li&gt;Scala debugger, worksheets and Ammonite scripts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(note that HOCON support was moved to a &lt;a href=&#34;https://plugins.jetbrains.com/plugin/10481-hocon&#34;&gt;separate plugin&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;General information&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To get information about how to install and use this plugin in IDEA, please use &lt;a href=&#34;https://www.jetbrains.com/idea/help/scala.html&#34;&gt;IntelliJ IDEA online help&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have any question about the Scala plugin, we&#39;d be glad to answer it in &lt;a href=&#34;https://discord.gg/aUKpZzeHCK&#34;&gt;our discord channel&lt;/a&gt; or in &lt;a href=&#34;https://devnet.jetbrains.com/community/idea/scala&#34;&gt;our developer community&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you found a bug, please report it on &lt;a href=&#34;https://youtrack.jetbrains.com/issues/SCL#newissue&#34;&gt;our issue tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to contribute, please see our &lt;a href=&#34;https://blog.jetbrains.com/scala/2016/04/21/how-to-contribute-to-intellij-scala-plugin/&#34;&gt;intro to the Scala plugin internals&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setting up the project&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;In order to take part in Scala plugin development, you need:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;IntelliJ IDEA 2021.3 or higher with a compatible version of Scala plugin&lt;/li&gt; &#xA; &lt;li&gt;JDK 11&lt;/li&gt; &#xA; &lt;li&gt;(optional but &lt;strong&gt;recommended&lt;/strong&gt;) &lt;br&gt; Enable &lt;a href=&#34;https://plugins.jetbrains.com/docs/intellij/enabling-internal.html&#34;&gt;internal mode&lt;/a&gt; in IDEA to get access to helpful internal actions and debug information&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository to your computer&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/JetBrains/intellij-scala.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Open IntelliJ IDEA, select &lt;code&gt;File -&amp;gt; New -&amp;gt; Project from existing sources&lt;/code&gt;, point to the directory where the Scala plugin repository is and then import it as sbt project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the next step, select JDK 11 as project JDK (create it from an installed JDK if necessary).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select the &lt;code&gt;scalaCommunity&lt;/code&gt; run configuration and select the &lt;code&gt;Run&lt;/code&gt; or &lt;code&gt;Debug&lt;/code&gt; button to build and start a development version of IDEA with the Scala plugin.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://plugins.jetbrains.com/docs/intellij/welcome.html&#34;&gt;IntelliJ Platform SDK documentation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Browsing IntelliJ Platform sources&lt;/h2&gt; &#xA;&lt;p&gt;When loading Scala Plugin project in sbt, the IntelliJ platform is downloaded to &lt;code&gt;&amp;lt;home&amp;gt;/.ScalaPluginIC/sdk/&amp;lt;sdk version&amp;gt;/&lt;/code&gt;. IntelliJ platform sources should be automatically attached after project has been imported and indices have been built.&lt;/p&gt; &#xA;&lt;p&gt;However, sometimes this doesn&#39;t happen and the sources are not attached. As a result you see decompiled code when opening a Platform API class. To fix this you can invoke &#34;Attach Intellij Sources&#34; action (you need to enable &lt;a href=&#34;https://plugins.jetbrains.com/docs/intellij/enabling-internal.html&#34;&gt;internal mode&lt;/a&gt; to access this action)&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run tests properly, the plugin needs to be packaged. On the sbt shell:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;packageArtifact&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;runFastTests&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The &#34;fast tests&#34; can take over an hour. To get a quick feedback on project health, run only the type inference tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; runTypeInferenceTests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://plugins.jetbrains.com/docs/intellij/testing-plugins.html&#34;&gt;Docs for writing tests for IntelliJ plugins&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;GitHub Actions build&lt;/h2&gt; &#xA;&lt;p&gt;The project is configured to build and run the typeInference tests and fast tests with Github Actions. The full test suite isn&#39;t run to avoid really long build times.&lt;/p&gt; &#xA;&lt;h2&gt;Running the plugin&lt;/h2&gt; &#xA;&lt;h3&gt;Debugging mode&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to try your changes is typically to launch the &lt;code&gt;scalaCommunity&lt;/code&gt; run configuration which is created when you set up the project as described above.&lt;/p&gt; &#xA;&lt;h3&gt;As a standalone plugin&lt;/h3&gt; &#xA;&lt;p&gt;To run and distribute a modified version of the plugin in a regular IntelliJ instance, you need to package it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;on the sbt shell, run &lt;code&gt;packageArtifactZip&lt;/code&gt;. This will output the generated plugin zip location (typically into &lt;code&gt;&amp;lt;project directory&amp;gt;/target/scala-plugin.zip&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, open Preferences, section Plugins, choose &#34;Install plugin from disk...&#34; and navigate to the scala-plugin.zip&lt;/li&gt; &#xA; &lt;li&gt;Restart IntelliJ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Other&lt;/h2&gt; &#xA;&lt;h3&gt;Investigation performance issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YourKit&lt;/li&gt; &#xA; &lt;li&gt;There is a &#34;Scala plugin profiler&#34; tool window to track invocations of methods with &lt;code&gt;@Cached*&lt;/code&gt; or &lt;code&gt;@Measure&lt;/code&gt; annotations (from &lt;code&gt;org.jetbrains.plugins.scala.macroAnnotations&lt;/code&gt; package) in real time. The tool window is available in &lt;a href=&#34;https://plugins.jetbrains.com/docs/intellij/enabling-internal.html&#34;&gt;internal mode&lt;/a&gt; or if &lt;code&gt;-Dinternal.profiler.tracing=true&lt;/code&gt; is passed to IDEA using &lt;a href=&#34;https://www.jetbrains.com/help/idea/tuning-the-ide.html#procedure-jvm-options&#34;&gt;custom VM options&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lampepfl/dotty</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/lampepfl/dotty</id>
    <link href="https://github.com/lampepfl/dotty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/lampepfl/dotty/workflows/Dotty/badge.svg?branch=master&#34; alt=&#34;Dotty CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632150470000902164&#34; alt=&#34;Join the chat at https://discord.com/invite/scala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Try it out&lt;/h1&gt; &#xA;&lt;p&gt;To try it in your project see also the &lt;a href=&#34;https://docs.scala-lang.org/scala3/getting-started.html&#34;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt dist/packArchive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Dotty uses the &lt;a href=&#34;https://www.scala-lang.org/conduct.html&#34;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; &#xA;&lt;h1&gt;How to Contribute&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&#34;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Dotty is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>julienrf/happy-numbers</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/julienrf/happy-numbers</id>
    <link href="https://github.com/julienrf/happy-numbers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;My solution to the Tony Morris “Happy Numbers” exercise&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/load-test</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/load-test</id>
    <link href="https://github.com/olxbr/load-test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;load-test&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;strong&gt;SearchAPI&lt;/strong&gt; uses &lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; to executes load tests.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/resources/conf/application.conf&#34;&gt;application.conf&lt;/a&gt; file that provides all configuration to load tests.&lt;/p&gt; &#xA;&lt;p&gt;You can override each config above using Java property. For example, if you can override &lt;code&gt;gatling.users&lt;/code&gt; property you must use &lt;code&gt;-Dgatling.users=&amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details about how to do override, see &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/#how-to-run&#34;&gt;How To Run&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;How To Build&lt;/h2&gt; &#xA;&lt;p&gt;To build this project, you need to execute these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;vivareal/load-test:load-test&lt;/code&gt; docker image will be created.&lt;/p&gt; &#xA;&lt;h2&gt;How To Run&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; works with a &lt;a href=&#34;http://gatling.io/docs/current/quickstart/#a-word-on-scala&#34;&gt;Simulation&lt;/a&gt; concept and for &lt;strong&gt;SearchAPI&lt;/strong&gt; we creates the &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/scala/com/vivareal/search/simulation/SearchAPIv2Simulation.scala&#34;&gt;SearchAPIv2Simulation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There some steps when you run the load tests:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If not already built, builds and runs &lt;code&gt;load-test&lt;/code&gt; docker image.&lt;/li&gt; &#xA; &lt;li&gt;Executes your simulations.&lt;/li&gt; &#xA; &lt;li&gt;Uploads you simulation report on Amazon S3.&lt;/li&gt; &#xA; &lt;li&gt;Notifies report status on Slack with link to access them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There are two parameters to use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_ENDPOINT&lt;/code&gt;*: load tests target endpoint.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;: gatling extra args&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The environment variables with &lt;code&gt;*&lt;/code&gt; are required. You can override each config using &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;p&gt;To run local, you simple use &lt;code&gt;make run-local&lt;/code&gt; with the target ip to load test, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run-local LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;To run using &lt;a href=&#34;https://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt;, you simple use &lt;code&gt;make run&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Yon can use &lt;code&gt;K8S_RUN_ARGS&lt;/code&gt; to configure &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-overview&#34;&gt;kubectl run&lt;/a&gt; params.&lt;/p&gt; &#xA;&lt;h3&gt;Running with Gradle&lt;/h3&gt; &#xA;&lt;p&gt;To run using Gradle too and you simple use &lt;code&gt;gatlingRun&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The upload and notification process is separated and to do this you must use &lt;code&gt;uploadReport&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun uploadReport&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Test&lt;/h3&gt; &#xA;&lt;p&gt;No tests currently implemented&lt;/p&gt; &#xA;&lt;h2&gt;How To Deploy&lt;/h2&gt; &#xA;&lt;p&gt;load-test is a tool/lib project and the deploy is subjective according to the project that uses.&lt;/p&gt; &#xA;&lt;p&gt;You may use docker to deploy: &lt;code&gt;make push&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;To push successful docker image, make sure you setup docker credentials.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Creating your own Simulation&lt;/h2&gt; &#xA;&lt;p&gt;Just implement the simulation in package &lt;code&gt;com.vivareal.search.simulation&lt;/code&gt; and sent it in &lt;code&gt;gatling.simulations.include&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlingRun -Dscenario.users=1 -Dgatling.rampUp=30 -Dgatling.maxDuration=60 -Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can send it in &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make run LT_EXTRA_ARGS=&#34;-Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Simple Request Simularion&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s a &lt;code&gt;Simulation&lt;/code&gt; that downloads a &lt;code&gt;csv&lt;/code&gt; file from &lt;code&gt;Graylog&lt;/code&gt; API based on a query and executes the resulting &lt;code&gt;URI&lt;/code&gt;s requests in &lt;code&gt;circle&lt;/code&gt; and &lt;code&gt;during&lt;/code&gt; a configured time.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Param&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.query&lt;/td&gt; &#xA;   &lt;td&gt;Graylog query to fetch URIs&lt;/td&gt; &#xA;   &lt;td&gt;application:cloudflare&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.urisFile&lt;/td&gt; &#xA;   &lt;td&gt;file name to save the Graylog API result&lt;/td&gt; &#xA;   &lt;td&gt;uris.csv&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.uriField&lt;/td&gt; &#xA;   &lt;td&gt;the field name of the csv file generated by Graylog API&lt;/td&gt; &#xA;   &lt;td&gt;ClientRequestURI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.authorization&lt;/td&gt; &#xA;   &lt;td&gt;Graylog API encoded basic auth header&lt;/td&gt; &#xA;   &lt;td&gt;Basic LALALALA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.range&lt;/td&gt; &#xA;   &lt;td&gt;Time range in seconds for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.limit&lt;/td&gt; &#xA;   &lt;td&gt;Limit of the results for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;5000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All above params can be overriden using System Properties.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rockthejvm/spark-essentials</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/rockthejvm/spark-essentials</id>
    <link href="https://github.com/rockthejvm/spark-essentials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The official repository for the Rock the JVM Spark Essentials with Scala course&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code we wrote during &lt;a href=&#34;https://rockthejvm.com/course/spark-essentials&#34;&gt;Rock the JVM&#39;s Spark Essentials with Scala&lt;/a&gt; (Udemy version &lt;a href=&#34;https://udemy.com/spark-essentials&#34;&gt;here&lt;/a&gt;) Unless explicitly mentioned, the code in this repository is exactly what was caught on camera.&lt;/p&gt; &#xA;&lt;h2&gt;How to install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install Docker&lt;/li&gt; &#xA; &lt;li&gt;either clone the repo or download as zip&lt;/li&gt; &#xA; &lt;li&gt;open with IntelliJ as an SBT project&lt;/li&gt; &#xA; &lt;li&gt;in a terminal window, navigate to the folder where you downloaded this repo and run &lt;code&gt;docker-compose up&lt;/code&gt; to build and start the PostgreSQL container - we will interact with it from Spark&lt;/li&gt; &#xA; &lt;li&gt;in another terminal window, navigate to &lt;code&gt;spark-cluster/&lt;/code&gt; and build the Docker-based Spark cluster with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x build-images.sh&#xA;./build-images.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;when prompted to start the Spark cluster, go to the &lt;code&gt;spark-cluster&lt;/code&gt; folder and run &lt;code&gt;docker-compose up --scale spark-worker=3&lt;/code&gt; to spin up the Spark containers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to start&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and checkout the &lt;code&gt;start&lt;/code&gt; tag by running the following in the repo folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to see the final code&lt;/h3&gt; &#xA;&lt;p&gt;Udemy students: checkout the &lt;code&gt;udemy&lt;/code&gt; branch of the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout udemy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Premium students: checkout the master branch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run an intermediate state&lt;/h3&gt; &#xA;&lt;p&gt;The repository was built while recording the lectures. Prior to each lecture, I tagged each commit so you can easily go back to an earlier state of the repo!&lt;/p&gt; &#xA;&lt;p&gt;The tags are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.1-scala-recap&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.1-dataframes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.2-dataframes-basics-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.4-datasources&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.5-datasources-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.6-columns-expressions&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.7-columns-expressions-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.8-aggregations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.9-joins&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2.10-joins-exercise&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.1-common-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.2-complex-types&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.3-managing-nulls&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.4-datasets&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3.5-datasets-part-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.1-spark-sql-shell&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.2-spark-sql&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4.3-spark-sql-exercises&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.1-rdds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5.2-rdds-part-2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And for premium students, in addition:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;6.1-spark-job-anatomy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6.2-deploying-to-cluster&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.1-taxi&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.2-taxi-2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.3-taxi-3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;7.4-taxi-4&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you watch a lecture, you can &lt;code&gt;git checkout&lt;/code&gt; the appropriate tag and the repo will go back to the exact code I had when I started the lecture.&lt;/p&gt; &#xA;&lt;h3&gt;For questions or suggestions&lt;/h3&gt; &#xA;&lt;p&gt;If you have changes to suggest to this repo, either&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;submit a GitHub issue&lt;/li&gt; &#xA; &lt;li&gt;tell me in the course Q/A forum&lt;/li&gt; &#xA; &lt;li&gt;submit a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/scala-utils</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/scala-utils</id>
    <link href="https://github.com/olxbr/scala-utils" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Utility code for Scala: logging, testing, configuration and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;scala-utils&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/grupozap/scala-utils&#34;&gt;&lt;img src=&#34;https://travis-ci.org/grupozap/scala-utils.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;scala-utils&lt;/code&gt; is an utility library that attempts to add useful code rapidly in your development pipeline, so that you can focus on what is really needed. It does not replace any existing library, instead it allows you to add production-ready features.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Add features such as a configurable GELF log formatter without the need of a full Graylog connector library (&lt;a href=&#34;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&#34;&gt;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use it in your project&lt;/h2&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;br.com.gzvr&#34; %% &#34;scala-utils&#34; % &#34;1.1.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to add our JFrog repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;resolvers += &#34;Artifactory&#34; at &#34;https://squadzapquality.jfrog.io/artifactory/olxbr-sbt-release/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supported Scala versions: &lt;code&gt;2.11&lt;/code&gt; and &lt;code&gt;2.12&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Renato Silva (&lt;a href=&#34;https://github.com/resilva87&#34;&gt;https://github.com/resilva87&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA; &lt;li&gt;Thiago Pereira (&lt;a href=&#34;https://github.com/thiagoandrade6&#34;&gt;https://github.com/thiagoandrade6&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue; even better if you can submit a pull request :)&lt;/p&gt; &#xA;&lt;h3&gt;Publish&lt;/h3&gt; &#xA;&lt;p&gt;Once you merge your code to the master branch, &lt;a href=&#34;https://github.com/olxbr/scala-utils/actions&#34;&gt;GitHub Actions&lt;/a&gt; should automatically publish it.&lt;/p&gt; &#xA;&lt;p&gt;To publish manually, create a &lt;code&gt;credentials.properties&lt;/code&gt; file in the project&#39;s directory, with &lt;a href=&#34;https://vault.grupozap.io/ui/vault/secrets/squad-quality/show/servicos/jfrog-quality&#34;&gt;the content you can find here&lt;/a&gt;, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt clean compile package publish&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>JohnSnowLabs/spark-nlp</title>
    <updated>2022-05-29T02:22:17Z</updated>
    <id>tag:github.com,2022-05-29:/JohnSnowLabs/spark-nlp</id>
    <link href="https://github.com/JohnSnowLabs/spark-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State of the Art Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark NLP: State of the Art Natural Language Processing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/actions&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/JohnSnowLabs/spark-nlp/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34; alt=&#34;Current Release Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JohnSnowLabs/spark-nlp.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.12&#34; alt=&#34;Maven Central&#34;&gt; &lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.12/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/spark-nlp&#34; alt=&#34;PyPI version&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/spark-nlp.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34; alt=&#34;Anaconda-Cloud&#34;&gt; &lt;img src=&#34;https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/LICENSE&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34; alt=&#34;PyPi downloads&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/spark-nlp?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;performant&lt;/strong&gt; &amp;amp; &lt;strong&gt;accurate&lt;/strong&gt; NLP annotations for machine learning pipelines that &lt;strong&gt;scale&lt;/strong&gt; easily in a distributed environment. Spark NLP comes with &lt;strong&gt;4000+&lt;/strong&gt; pretrained &lt;strong&gt;pipelines&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt; in more than &lt;strong&gt;200+&lt;/strong&gt; languages. It also offers tasks such as &lt;strong&gt;Tokenization&lt;/strong&gt;, &lt;strong&gt;Word Segmentation&lt;/strong&gt;, &lt;strong&gt;Part-of-Speech Tagging&lt;/strong&gt;, Word and Sentence &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Named Entity Recognition&lt;/strong&gt;, &lt;strong&gt;Dependency Parsing&lt;/strong&gt;, &lt;strong&gt;Spell Checking&lt;/strong&gt;, &lt;strong&gt;Text Classification&lt;/strong&gt;, &lt;strong&gt;Sentiment Analysis&lt;/strong&gt;, &lt;strong&gt;Token Classification&lt;/strong&gt;, &lt;strong&gt;Machine Translation&lt;/strong&gt; (+180 languages), &lt;strong&gt;Summarization&lt;/strong&gt; &amp;amp; &lt;strong&gt;Question Answering&lt;/strong&gt;, &lt;strong&gt;Text Generation&lt;/strong&gt;, and many more &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;NLP tasks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark NLP&lt;/strong&gt; is the only open-source NLP library in &lt;strong&gt;production&lt;/strong&gt; that offers state-of-the-art transformers such as &lt;strong&gt;BERT&lt;/strong&gt;, &lt;strong&gt;ALBERT&lt;/strong&gt;, &lt;strong&gt;ELECTRA&lt;/strong&gt;, &lt;strong&gt;XLNet&lt;/strong&gt;, &lt;strong&gt;DistilBERT&lt;/strong&gt;, &lt;strong&gt;RoBERTa&lt;/strong&gt;, &lt;strong&gt;DeBERTa&lt;/strong&gt;, &lt;strong&gt;XLM-RoBERTa&lt;/strong&gt;, &lt;strong&gt;Longformer&lt;/strong&gt;, &lt;strong&gt;ELMO&lt;/strong&gt;, &lt;strong&gt;Universal Sentence Encoder&lt;/strong&gt;, &lt;strong&gt;Google T5&lt;/strong&gt;, &lt;strong&gt;MarianMT&lt;/strong&gt;, and &lt;strong&gt;GPT2&lt;/strong&gt; not only to &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt;, but also to &lt;strong&gt;JVM&lt;/strong&gt; ecosystem (&lt;strong&gt;Java&lt;/strong&gt;, &lt;strong&gt;Scala&lt;/strong&gt;, and &lt;strong&gt;Kotlin&lt;/strong&gt;) at &lt;strong&gt;scale&lt;/strong&gt; by extending &lt;strong&gt;Apache Spark&lt;/strong&gt; natively.&lt;/p&gt; &#xA;&lt;h2&gt;Project&#39;s website&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href=&#34;http://nlp.johnsnowlabs.com/&#34;&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt; &#xA;&lt;h2&gt;Community support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.johnsnowlabs.com/slack-redirect/&#34;&gt;Slack&lt;/a&gt; For live discussion with the Spark NLP community and the team&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp&#34;&gt;GitHub&lt;/a&gt; Bug reports, feature requests, and contributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/discussions&#34;&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/spark-nlp&#34;&gt;Medium&lt;/a&gt; Spark NLP articles&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos&#34;&gt;YouTube&lt;/a&gt; Spark NLP video tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-spark-support&#34;&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala-and-python-support&#34;&gt;Scala &amp;amp; Python Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-support&#34;&gt;Databricks Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-support&#34;&gt;EMR Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#usage&#34;&gt;Using Spark NLP&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-packages&#34;&gt;Spark Packages&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala&#34;&gt;Scala&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#maven&#34;&gt;Maven&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python&#34;&gt;Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipconda&#34;&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-zeppelin&#34;&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#jupyter-notebook-python&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#google-colab-notebook&#34;&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#kaggle-kernel&#34;&gt;Kaggle Kernel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-cluster&#34;&gt;Databricks Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-cluster&#34;&gt;EMR Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#gcp-dataproc&#34;&gt;GCP Dataproc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-nlp-configuration&#34;&gt;Spark NLP Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines-and-models&#34;&gt;Pipelines &amp;amp; Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#offline&#34;&gt;Offline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tokenization&lt;/li&gt; &#xA; &lt;li&gt;Trainable Word Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Stop Words Removal&lt;/li&gt; &#xA; &lt;li&gt;Token Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Document Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Stemmer&lt;/li&gt; &#xA; &lt;li&gt;Lemmatizer&lt;/li&gt; &#xA; &lt;li&gt;NGrams&lt;/li&gt; &#xA; &lt;li&gt;Regex Matching&lt;/li&gt; &#xA; &lt;li&gt;Text Matching&lt;/li&gt; &#xA; &lt;li&gt;Chunking&lt;/li&gt; &#xA; &lt;li&gt;Date Matcher&lt;/li&gt; &#xA; &lt;li&gt;Sentence Detector&lt;/li&gt; &#xA; &lt;li&gt;Deep Sentence Detector (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Dependency parsing (Labeled/unlabeled)&lt;/li&gt; &#xA; &lt;li&gt;Part-of-speech tagging&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Detection (ML models)&lt;/li&gt; &#xA; &lt;li&gt;Spell Checker (ML and DL models)&lt;/li&gt; &#xA; &lt;li&gt;Word Embeddings (GloVe and Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;Doc2Vec (based on Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;BERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;CamemBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa Embeddings (HuggingFace v2 &amp;amp; v3 models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Longformer Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;ALBERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLNet Embeddings&lt;/li&gt; &#xA; &lt;li&gt;ELMO Embeddings (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;Universal Sentence Encoder (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;BERT Sentence Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Sentence Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Chunk Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised keywords extraction&lt;/li&gt; &#xA; &lt;li&gt;Language Detection &amp;amp; Identification (up to 375 languages)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-label Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Text Classification (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;BERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;ALBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLNet for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Longformer for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation (MarianMT)&lt;/li&gt; &#xA; &lt;li&gt;Text-To-Text Transfer Transformer (Google T5)&lt;/li&gt; &#xA; &lt;li&gt;Generative Pre-trained Transformer 2 (OpenAI GPT2)&lt;/li&gt; &#xA; &lt;li&gt;Named entity recognition (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Easy TensorFlow integration&lt;/li&gt; &#xA; &lt;li&gt;GPU Support&lt;/li&gt; &#xA; &lt;li&gt;Full integration with Spark ML functions&lt;/li&gt; &#xA; &lt;li&gt;+3200 pre-trained models in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;+1700 pre-trained pipelines in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To use Spark NLP you need the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8 and 11&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3.2.x, 3.1.x, 3.0.x, 2.4.x, or 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Java 11 is only supported if you are using Spark NLP with Spark/PySpark 3.x and above&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU (optional):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA11 and cuDNN 8.0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ java -version&#xA;# should be Java 8 (Oracle or OpenJDK)&#xA;$ conda create -n sparknlp python=3.7 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python console or Jupyter &lt;code&gt;Python3&lt;/code&gt; kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Spark NLP&#xA;from sparknlp.base import *&#xA;from sparknlp.annotator import *&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;import sparknlp&#xA;&#xA;# Start SparkSession with Spark NLP&#xA;# start() functions has 5 parameters: gpu, spark23, spark24, spark32, and memory&#xA;# sparknlp.start(gpu=True) will start the session with GPU support&#xA;# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed&#xA;# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed&#xA;# sparknlp.start(spark32=True) is when you have Apache Spark 3.2.x installed&#xA;# sparknlp.start(memory=&#34;16G&#34;) to change the default driver memory in SparkSession&#xA;spark = sparknlp.start()&#xA;&#xA;# Download a pre-trained pipeline&#xA;pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;&#xA;# Your testing dataset&#xA;text = &#34;&#34;&#34;&#xA;The Mona Lisa is a 16th century oil painting created by Leonardo.&#xA;It&#39;s held at the Louvre in Paris.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Annotate your testing dataset&#xA;result = pipeline.annotate(text)&#xA;&#xA;# What&#39;s in the pipeline&#xA;list(result.keys())&#xA;Output: [&#39;entities&#39;, &#39;stem&#39;, &#39;checked&#39;, &#39;lemma&#39;, &#39;document&#39;,&#xA;&#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;]&#xA;&#xA;# Check the results&#xA;result[&#39;entities&#39;]&#xA;Output: [&#39;Mona Lisa&#39;, &#39;Leonardo&#39;, &#39;Louvre&#39;, &#39;Paris&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, you can visit our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;repository&lt;/a&gt; to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP &lt;em&gt;3.4.4&lt;/em&gt; has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x, 2.4.x, 3.0.x, 3.1.x, and 3.2.x:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.3.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.4.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.0.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.1.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.2.x&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting 3.0.0 release, the default &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;spark-nlp-gpu&lt;/code&gt; pacakges are based on Scala 2.12 and Apache Spark 3.x by default.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Scala and Python Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.6&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.8&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.9&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Databricks Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following runtimes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5.5 LTS&lt;/li&gt; &#xA; &lt;li&gt;5.5 LTS ML&lt;/li&gt; &#xA; &lt;li&gt;6.4&lt;/li&gt; &#xA; &lt;li&gt;6.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.3&lt;/li&gt; &#xA; &lt;li&gt;7.3 ML&lt;/li&gt; &#xA; &lt;li&gt;7.4&lt;/li&gt; &#xA; &lt;li&gt;7.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.5&lt;/li&gt; &#xA; &lt;li&gt;7.5 ML&lt;/li&gt; &#xA; &lt;li&gt;7.6&lt;/li&gt; &#xA; &lt;li&gt;7.6 ML&lt;/li&gt; &#xA; &lt;li&gt;8.0&lt;/li&gt; &#xA; &lt;li&gt;8.0 ML&lt;/li&gt; &#xA; &lt;li&gt;8.1&lt;/li&gt; &#xA; &lt;li&gt;8.1 ML&lt;/li&gt; &#xA; &lt;li&gt;8.2&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML&lt;/li&gt; &#xA; &lt;li&gt;8.3&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML&lt;/li&gt; &#xA; &lt;li&gt;8.4&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML&lt;/li&gt; &#xA; &lt;li&gt;9.0&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML&lt;/li&gt; &#xA; &lt;li&gt;9.1&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.0&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML&lt;/li&gt; &#xA; &lt;li&gt;10.1&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.2&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML&lt;/li&gt; &#xA; &lt;li&gt;10.3&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML &amp;amp; GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark NLP 3.4.4 is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 8.x and above as listed under GPU.&lt;/p&gt; &#xA;&lt;h2&gt;EMR Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following EMR releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emr-5.20.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.22.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.23.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.25.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.26.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.27.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.28.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.29.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.31.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.32.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.34.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.1.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.2.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.1&lt;/li&gt; &#xA; &lt;li&gt;emr-6.4.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.5.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html&#34;&gt;Amazon EMR 5.x releases&lt;/a&gt; Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html&#34;&gt;Amazon EMR 6.x releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.4.4&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Packages Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;This is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Apache Spark&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on CPU&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on GPU&lt;/th&gt; &#xA;   &lt;th&gt;Start()&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x/3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark32=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark24=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark23=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Spark Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Command line (requires internet connection)&lt;/h3&gt; &#xA;&lt;p&gt;Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, Apache Spark 3.1.x, and Apache Spark 3.2.x. That&#39;s being said, you need to choose the right package name for the right Apache Spark major release:&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.2.x (Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.4.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.3.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x, 3.1.x, and 3.2.x versions. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Maven&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark24_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maven Central: &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects &lt;a href=&#34;https://github.com/maziyarpanahi/spark-nlp-starter&#34;&gt;Spark NLP SBT Starter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Python 3.6.x and above depending on your major PySpark version.&lt;/p&gt; &#xA;&lt;h3&gt;Python without explicit Pyspark installation&lt;/h3&gt; &#xA;&lt;h3&gt;Pip/Conda&lt;/h3&gt; &#xA;&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt; &#xA;&lt;p&gt;Pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34;&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34;&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you&#39;ll have to create a SparkSession either from Spark NLP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;&#xA;spark = sparknlp.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[4]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for comma-delimited jar files. For cluster setups, of course, you&#39;ll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;&#xA;#create or get Spark Session&#xA;&#xA;spark = sparknlp.start()&#xA;&#xA;sparknlp.version()&#xA;spark.version&#xA;&#xA;#download, load and annotate a text by pre-trained pipeline&#xA;&#xA;pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;)&#xA;result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiled JARs&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;h4&gt;spark-nlp&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the jar manually&lt;/h3&gt; &#xA;&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;Maven Central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell --jars spark-nlp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Zeppelin&lt;/h2&gt; &#xA;&lt;p&gt;Use either one of the following options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the following Maven Coordinates to the interpreter&#39;s library list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a path to pre-built jar from &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;here&lt;/a&gt; in the interpreter&#39;s library list making sure the jar is available to driver path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Python in Zeppelin&lt;/h3&gt; &#xA;&lt;p&gt;Apart from the previous step, install the python module through pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt; &#xA;&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier since it includes both scala and python side installation.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook (Python)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recomended:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get this done on Linux and macOS is to simply install &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;pyspark&lt;/code&gt; PyPI packages and launch the Jupyter from the same Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda create -n sparknlp python=3.8 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2 jupyter&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The you can use &lt;code&gt;python3&lt;/code&gt; kernel to run your code with creating SparkSession via &lt;code&gt;spark = sparknlp.start()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_HOME=/path/to/your/spark/folder&#xA;export PYSPARK_PYTHON=python3&#xA;export PYSPARK_DRIVER_PYTHON=jupyter&#xA;export PYSPARK_DRIVER_PYTHON_OPTS=notebook&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If not using pyspark at all, you&#39;ll have to run the instructions pointed &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python-without-explicit-Pyspark-installation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# This is only to setup PySpark and Spark NLP on Colab&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script comes with the two options to define &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-nlp&lt;/code&gt; versions via options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# -p is for pyspark&#xA;# -s is for spark-nlp&#xA;# by default they are set to the latest&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.1.2 -s 3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb&#34;&gt;Spark NLP quick start on Google Colab&lt;/a&gt; is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Kaggle Kernel&lt;/h2&gt; &#xA;&lt;p&gt;Run the following code in Kaggle Kernel and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Let&#39;s setup Kaggle for Spark NLP and PySpark&#xA;!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mozzie/spark-nlp-named-entity-recognition&#34;&gt;Spark NLP quick start on Kaggle Kernel&lt;/a&gt; is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Databricks Cluster&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a cluster if you don&#39;t have one already&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;Libraries&lt;/code&gt; tab inside your cluster you need to follow these steps:&lt;/p&gt; &lt;p&gt;3.1. Install New -&amp;gt; PyPI -&amp;gt; &lt;code&gt;spark-nlp==3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;p&gt;3.2. Install New -&amp;gt; Maven -&amp;gt; Coordinates -&amp;gt; &lt;code&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can attach your notebook to the cluster and use Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: Databrick&#39;s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name for your runtime from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EMR Cluster&lt;/h2&gt; &#xA;&lt;p&gt;To launch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.&lt;/p&gt; &#xA;&lt;p&gt;A sample of your bootstrap script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;#!/bin/bash&#xA;set -x -e&#xA;&#xA;echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3&#xA;export HADOOP_CONF_DIR=/etc/hadoop/conf&#xA;export SPARK_JARS_DIR=/usr/lib/spark/jars&#xA;export SPARK_HOME=/usr/lib/spark&#39; &amp;gt;&amp;gt; $HOME/.bashrc &amp;amp;&amp;amp; source $HOME/.bashrc&#xA;&#xA;sudo python3 -m pip install awscli boto spark-nlp&#xA;&#xA;set +x&#xA;exit 0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of your software configuration in JSON on S3 (must be public access):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.json&#34;&gt;[{&#xA;  &#34;Classification&#34;: &#34;spark-env&#34;,&#xA;  &#34;Configurations&#34;: [{&#xA;    &#34;Classification&#34;: &#34;export&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;PYSPARK_PYTHON&#34;: &#34;/usr/bin/python3&#34;&#xA;    }&#xA;  }]&#xA;},&#xA;{&#xA;  &#34;Classification&#34;: &#34;spark-defaults&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;spark.yarn.stagingDir&#34;: &#34;hdfs:///tmp&#34;,&#xA;      &#34;spark.yarn.preserve.staging.files&#34;: &#34;true&#34;,&#xA;      &#34;spark.kryoserializer.buffer.max&#34;: &#34;2000M&#34;,&#xA;      &#34;spark.serializer&#34;: &#34;org.apache.spark.serializer.KryoSerializer&#34;,&#xA;      &#34;spark.driver.maxResultSize&#34;: &#34;0&#34;,&#xA;      &#34;spark.jars.packages&#34;: &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of AWS CLI to launch EMR cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;aws emr create-cluster \&#xA;--name &#34;Spark NLP 3.4.4&#34; \&#xA;--release-label emr-6.2.0 \&#xA;--applications Name=Hadoop Name=Spark Name=Hive \&#xA;--instance-type m4.4xlarge \&#xA;--instance-count 3 \&#xA;--use-default-roles \&#xA;--log-uri &#34;s3://&amp;lt;S3_BUCKET&amp;gt;/&#34; \&#xA;--bootstrap-actions Path=s3://&amp;lt;S3_BUCKET&amp;gt;/emr-bootstrap.sh,Name=custome \&#xA;--configurations &#34;https://&amp;lt;public_access&amp;gt;/sparknlp-config.json&#34; \&#xA;--ec2-attributes KeyName=&amp;lt;your_ssh_key&amp;gt;,EmrManagedMasterSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt;,EmrManagedSlaveSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt; \&#xA;--profile &amp;lt;aws_profile_credentials&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP Dataproc&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a cluster if you don&#39;t have one already as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At gcloud shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud services enable dataproc.googleapis.com \&#xA;  compute.googleapis.com \&#xA;  storage-component.googleapis.com \&#xA;  bigquery.googleapis.com \&#xA;  bigquerystorage.googleapis.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;ZONE=&amp;lt;zone&amp;gt;&#xA;CLUSTER_NAME=&amp;lt;cluster_name&amp;gt;&#xA;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don&#39;t forget to set the maven coordinates for the jar in properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud dataproc clusters create ${CLUSTER_NAME} \&#xA;  --region=${REGION} \&#xA;  --zone=${ZONE} \&#xA;  --image-version=2.0 \&#xA;  --master-machine-type=n1-standard-4 \&#xA;  --worker-machine-type=n1-standard-2 \&#xA;  --master-boot-disk-size=128GB \&#xA;  --worker-boot-disk-size=128GB \&#xA;  --num-workers=2 \&#xA;  --bucket=${BUCKET_NAME} \&#xA;  --optional-components=JUPYTER \&#xA;  --enable-component-gateway \&#xA;  --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; \&#xA;  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh \&#xA;  --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can attach your notebook to the cluster and use the Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Spark NLP Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can change the following Spark NLP configurations via Spark Configuration:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Property Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.pretrained.cache_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/cache_pretrained&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to download and exctract pretrained &lt;code&gt;Models&lt;/code&gt; and &lt;code&gt;Pipelines&lt;/code&gt;. By default, it will be in User&#39;s Home directory under &lt;code&gt;cache_pretrained&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.storage.cluster_tmp_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of &lt;code&gt;hadoop.tmp.dir&lt;/code&gt; set via Hadoop configuration for Apache Spark. NOTE: &lt;code&gt;S3&lt;/code&gt; is not supported and it must be local, HDFS, or DBFS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.annotator.log_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/annotator_logs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to save logs from annotators during training such as &lt;code&gt;NerDLApproach&lt;/code&gt;, &lt;code&gt;ClassifierDLApproach&lt;/code&gt;, &lt;code&gt;SentimentDLApproach&lt;/code&gt;, &lt;code&gt;MultiClassifierDLApproach&lt;/code&gt;, etc. By default, it will be in User&#39;s Home directory under &lt;code&gt;annotator_logs&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.access_key_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.secret_access_key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.session_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.s3_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.region&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How to set Spark NLP Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparkSession:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.config()&lt;/code&gt; during SparkSession creation to set Spark NLP configurations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;        .master(&#34;local[*]&#34;) \&#xA;        .config(&#34;spark.driver.memory&#34;, &#34;16G&#34;) \&#xA;        .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;        .config(&#34;spark.serializer&#34;, &#34;org.apache.spark.serializer.KryoSerializer&#34;) \&#xA;        .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000m&#34;) \&#xA;        .config(&#34;spark.jsl.settings.pretrained.cache_folder&#34;, &#34;sample_data/pretrained&#34;) \&#xA;        .config(&#34;spark.jsl.settings.storage.cluster_tmp_dir&#34;, &#34;sample_data/storage&#34;) \&#xA;        .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;) \&#xA;        .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-shell:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyspark:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pyspark \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE&#xA;spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE&#xA;spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it.&lt;/p&gt; &#xA;&lt;h3&gt;S3 Integration&lt;/h3&gt; &#xA;&lt;p&gt;In Spark NLP we can define S3 locations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Export log files of training models&lt;/li&gt; &#xA; &lt;li&gt;Store tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.annotator.log_folder&#34;, &#34;s3://my/s3/path/logs&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.s3_bucket&#34;, &#34;my.bucket&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can check the log on your S3 path defined in &lt;em&gt;spark.jsl.settings.annotator.log_folder&lt;/em&gt; property. Make sure to use the prefix &lt;em&gt;s3://&lt;/em&gt;, otherwise it will use the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensorflow Graphs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reference S3 location for downloading graphs. We need to set up AWS credentials&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;MFA Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.session_token&#34;, &#34;MY_TOKEN&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of a bash script that gets temporal AWS credentials can be found &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/scripts/aws_tmp_credentials.sh&#34;&gt;here&lt;/a&gt; This script requires three arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./aws_tmp_credentials.sh iam_user duration serial_number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipelines and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;&#xA;SparkNLP.version()&#xA;&#xA;val testData = spark.createDataFrame(Seq(&#xA;(1, &#34;Google has announced the release of a beta version of the popular TensorFlow machine learning library&#34;),&#xA;(2, &#34;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&#34;)&#xA;)).toDF(&#34;id&#34;, &#34;text&#34;)&#xA;&#xA;val pipeline = PretrainedPipeline(&#34;explain_document_dl&#34;, lang=&#34;en&#34;)&#xA;&#xA;val annotation = pipeline.transform(testData)&#xA;&#xA;annotation.show()&#xA;/*&#xA;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;2.5.0&#xA;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&#xA;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&#xA;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&#xA;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;*/&#xA;&#xA;annotation.select(&#34;entities.result&#34;).show(false)&#xA;&#xA;/*&#xA;+----------------------------------+&#xA;|result                            |&#xA;+----------------------------------+&#xA;|[Google, TensorFlow]              |&#xA;|[Donald John Trump, United States]|&#xA;+----------------------------------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all of the available Pipelines of a particular language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;)&#xA;/*&#xA;+--------------------------------------------+------+---------+&#xA;| Pipeline                                   | lang | version |&#xA;+--------------------------------------------+------+---------+&#xA;| dependency_parse                           |  en  | 2.0.2   |&#xA;| analyze_sentiment_ml                       |  en  | 2.0.2   |&#xA;| check_spelling                             |  en  | 2.1.0   |&#xA;| match_datetime                             |  en  | 2.1.0   |&#xA;                               ...&#xA;| explain_document_ml                        |  en  | 3.1.3   |&#xA;+--------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+---------------------------------------+------+---------+&#xA;| Pipeline                              | lang | version |&#xA;+---------------------------------------+------+---------+&#xA;| dependency_parse                      |  en  | 2.0.2   |&#xA;                               ...&#xA;| clean_slang                           |  en  | 3.0.0   |&#xA;| clean_pattern                         |  en  | 3.0.0   |&#xA;| check_spelling                        |  en  | 3.0.0   |&#xA;| dependency_parse                      |  en  | 3.0.0   |&#xA;+---------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained pipelines&lt;/a&gt; with examples, demos, benchmarks, and more&lt;/h4&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some selected languages:&lt;/strong&gt; &lt;code&gt;Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load NER model trained by deep learning approach and GloVe word embeddings&#xA;ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;)&#xA;# load NER model trained by deep learning approach and BERT word embeddings&#xA;ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// load French POS tagger model trained by Universal Dependencies&#xA;val french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;// load Italain LemmatizerModel&#xA;val italian_lemma = LemmatizerModel.pretrained(&#34;lemma_dxc&#34;, lang=&#34;it&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Models&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;)&#xA;/*&#xA;+---------------------------------------------+------+---------+&#xA;| Model                                       | lang | version |&#xA;+---------------------------------------------+------+---------+&#xA;| onto_100                                    |  en  | 2.1.0   |&#xA;| onto_300                                    |  en  | 2.1.0   |&#xA;| ner_dl_bert                                 |  en  | 2.2.0   |&#xA;| onto_100                                    |  en  | 2.4.0   |&#xA;| ner_conll_elmo                              |  en  | 3.2.2   |&#xA;+---------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+----------------------------+------+---------+&#xA;| Model                      | lang | version |&#xA;+----------------------------+------+---------+&#xA;| onto_100                   |  en  | 2.1.0   |&#xA;| ner_aspect_based_sentiment |  en  | 2.6.2   |&#xA;| ner_weibo_glove_840B_300d  |  en  | 2.6.2   |&#xA;| nerdl_atis_840b_300d       |  en  | 2.7.1   |&#xA;| nerdl_snips_100d           |  en  | 2.7.3   |&#xA;+----------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to see a list of available annotators, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showAvailableAnnotators()&#xA;/*&#xA;AlbertEmbeddings&#xA;AlbertForTokenClassification&#xA;AssertionDLModel&#xA;...&#xA;XlmRoBertaSentenceEmbeddings&#xA;XlnetEmbeddings&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained models&lt;/a&gt; with examples, demo, benchmark, and more&lt;/h4&gt; &#xA;&lt;h2&gt;Offline&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of using the Maven package, you need to load our Fat JAR&lt;/li&gt; &#xA; &lt;li&gt;Instead of using PretrainedPipeline for pretrained pipelines or the &lt;code&gt;.pretrained()&lt;/code&gt; function to download pretrained models, you will need to manually download your pipeline/model from &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;Models Hub&lt;/a&gt;, extract it, and load it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of &lt;code&gt;SparkSession&lt;/code&gt; with Fat JAR to have Spark NLP offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[*]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars&#34;, &#34;/tmp/spark-nlp-assembly-3.4.4.jar&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download provided Fat JARs from each &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/spark-nlp-assembly-3.4.4.jar&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of using pretrained Models and Pipelines in offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# instead of using pretrained() for online:&#xA;# french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;# you download this model, extract it, and use .load&#xA;french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)\&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)\&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&#xA;# example for pipelines&#xA;# instead of using PretrainedPipeline&#xA;# pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;# you download this pipeline, extract it, and use PipelineModel&#xA;PipelineModel.load(&#34;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Need more &lt;strong&gt;examples&lt;/strong&gt;? Check out our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;Spark NLP Showcase&lt;/a&gt; repository to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;p&gt;Also, don&#39;t forget to check &lt;a href=&#34;https://nlp.johnsnowlabs.com/demo&#34;&gt;Spark NLP in Action&lt;/a&gt; built by Streamlit.&lt;/p&gt; &#xA;&lt;h3&gt;All examples: &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;spark-nlp-workshop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nlp.johnsnowlabs.com/learn&#34;&gt;Check our Articles and Videos page here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;We have published a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2665963821000063&#34;&gt;paper&lt;/a&gt; that you can cite for the Spark NLP library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{KOCAMAN2021100058,&#xA;    title = {Spark NLP: Natural language understanding at scale},&#xA;    journal = {Software Impacts},&#xA;    pages = {100058},&#xA;    year = {2021},&#xA;    issn = {2665-9638},&#xA;    doi = {https://doi.org/10.1016/j.simpa.2021.100058},&#xA;    url = {https://www.sciencedirect.com/science/article/pii/S2665963.2.100063},&#xA;    author = {Veysel Kocaman and David Talby},&#xA;    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},&#xA;    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world’s most widely used NLP library in the enterprise.}&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ideas&lt;/li&gt; &#xA; &lt;li&gt;feedback&lt;/li&gt; &#xA; &lt;li&gt;documentation&lt;/li&gt; &#xA; &lt;li&gt;bug reports&lt;/li&gt; &#xA; &lt;li&gt;NLP training and testing corpora&lt;/li&gt; &#xA; &lt;li&gt;Development and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;John Snow Labs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://johnsnowlabs.com&#34;&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>