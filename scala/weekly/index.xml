<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-05T01:41:00Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GoogleCloudDataproc/spark-bigtable-connector</title>
    <updated>2024-05-05T01:41:00Z</updated>
    <id>tag:github.com,2024-05-05:/GoogleCloudDataproc/spark-bigtable-connector</id>
    <link href="https://github.com/GoogleCloudDataproc/spark-bigtable-connector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark SQL connector for Google Bigtable&lt;/h1&gt; &#xA;&lt;p&gt;This connector allows writing Spark SQL DataFrames into &lt;a href=&#34;https://cloud.google.com/bigtable&#34;&gt;Google Bigtable&lt;/a&gt; and reading tables from Bigtable. It uses the&lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources.html&#34;&gt;Spark SQL Data Source API V1&lt;/a&gt; to connect to Bigtable.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;You can access the connector in two different ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;From our &lt;a href=&#34;https://repo1.maven.org/maven2/com/google/cloud/spark/bigtable&#34;&gt;Maven Central repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Through a public GCS bucket, located at &lt;code&gt;gs://spark-lib/bigtable/spark-bigtable_2.12-&amp;lt;version&amp;gt;.jar&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In Java and Scala applications, you can use different dependency management tools (e.g., Maven, sbt, or Gradle) to access the connector &lt;code&gt;com.google.cloud.spark.bigtable:spark-bigtable_2.12:&amp;lt;version&amp;gt;&lt;/code&gt;( initial &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt; is &lt;code&gt;0.1.0&lt;/code&gt;) and package it inside your application JAR using libraries such as Maven Shade Plugin. For PySpark applications, you can use the &lt;code&gt;--jars&lt;/code&gt; flag to pass the GCS address of the connector when submitting it.&lt;/p&gt; &#xA;&lt;p&gt;For Maven, you can add the following snippet to your &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.google.cloud.spark.bigtable&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;spark-bigtable_2.12&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;0.1.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For sbt, you can add the following to your &lt;code&gt;build.sbt&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.google.cloud.spark.bigtable&#34; % &#34;spark-bigtable_2.12&#34; % &#34;0.1.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can add the following to your &lt;code&gt;build.gradle&lt;/code&gt; file when using Gradle:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dependencies {&#xA;implementation group: &#39;com.google.cloud.bigtable&#39;, name: &#39;spark-bigtable_2.12&#39;, version: &#39;0.1.0&#39;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you need plugins such as &lt;a href=&#34;https://maven.apache.org/plugins/maven-shade-plugin/&#34;&gt;Maven Shade Plugin&lt;/a&gt;, &lt;a href=&#34;https://github.com/sbt/sbt-assembly&#34;&gt;sbt-assembly&lt;/a&gt;, or &lt;a href=&#34;https://imperceptiblethoughts.com/shadow/introduction/&#34;&gt;Shadow Plugin&lt;/a&gt; to package the connector inside your JAR in Maven, sbt, and Gradle, respectively.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;About Bigtable&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/bigtable&#34;&gt;Bigtable&lt;/a&gt; is Google&#39;s NoSQL Big Data database service. It&#39;s the same service powering many of Google&#39;s internal applications, e.g., Search, Maps, etc. You can refer to &lt;a href=&#34;https://cloud.google.com/bigtable/docs/instances-clusters-nodes&#34;&gt;Bigtable documentations&lt;/a&gt; to learn more about key concepts, including instances, clusters, nodes, and tablets.&lt;/p&gt; &#xA;&lt;h3&gt;About Apache Spark and Spark SQL&lt;/h3&gt; &#xA;&lt;p&gt;Apache Spark is a distributed computing framework designed for fast and large-scale data processing, where &lt;a href=&#34;https://spark.apache.org/docs/latest/rdd-programming-guide.html&#34;&gt;resilient distributed dataset (RDD)&lt;/a&gt; is the main data model. Spark SQL is a module built on top of Spark that provides a SQL-like interface for querying and manipulating data. This is done through &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;DataFrame and DataSet&lt;/a&gt;, Spark SQL&#39;s data model, built on top of RDDs.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Spark runtime environments and requirements&lt;/h3&gt; &#xA;&lt;p&gt;You can use the connector with Spark locally with the &lt;a href=&#34;https://cloud.google.com/bigtable/docs/emulator&#34;&gt;Bigtable emulator&lt;/a&gt;, as well as in managed environments such as Dataproc cluster or serverless. You need the following depending on the environments you choose to use:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Runtime environment&lt;/th&gt; &#xA;   &lt;th&gt;Bigtable&lt;/th&gt; &#xA;   &lt;th&gt;Dataproc&lt;/th&gt; &#xA;   &lt;th&gt;Cloud Storage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Local Spark w/ &lt;a href=&#34;https://cloud.google.com/bigtable/docs/emulator&#34;&gt;Bigtable emulator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Local Spark&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataproc Cluster&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataproc Serverless&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;   &lt;td&gt;Required&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported Spark versions&lt;/h3&gt; &#xA;&lt;p&gt;The connector supports the following Spark versions &lt;strong&gt;with Scala 2.12&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scala version&lt;/th&gt; &#xA;   &lt;th&gt;Spark versions&lt;/th&gt; &#xA;   &lt;th&gt;Spark Application Languages&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;   &lt;td&gt;2.4.8, 3.1.x, 3.2.x, 3.4.x, 3.5.x&lt;/td&gt; &#xA;   &lt;td&gt;Java, Scala, PySpark (&lt;code&gt;.py&lt;/code&gt; files or Jupyter notebooks)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;p&gt;For a detailed list of features and how to use them, you can refer the official documentation &lt;a href=&#34;https://cloud.google.com/bigtable/docs/use-bigtable-spark-connector&#34;&gt;here&lt;/a&gt;. A list of main features is as follows:&lt;/p&gt; &#xA;&lt;h3&gt;Catalog definition&lt;/h3&gt; &#xA;&lt;p&gt;You can define a catalog as a JSON-formatted string, to convert from the DataFrame&#39;s schema to a format compatible with Bigtable. This is an example of a catalog JSON:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;table&#34;: {&#34;name&#34;: &#34;t1&#34;},&#xA;  &#34;rowkey&#34;: &#34;id_rowkey&#34;,&#xA;  &#34;columns&#34;: {&#xA;    &#34;id&#34;: {&#34;cf&#34;: &#34;rowkey&#34;, &#34;col&#34;: &#34;id_rowkey&#34;, &#34;type&#34;: &#34;string&#34;},&#xA;    &#34;name&#34;: {&#34;cf&#34;: &#34;info&#34;, &#34;col&#34;: &#34;name&#34;, &#34;type&#34;: &#34;string&#34;},&#xA;    &#34;birthYear&#34;: {&#34;cf&#34;: &#34;info&#34;, &#34;col&#34;: &#34;birth_year&#34;, &#34;type&#34;: &#34;long&#34;},&#xA;    &#34;address&#34;: {&#34;cf&#34;: &#34;location&#34;, &#34;col&#34;: &#34;address&#34;, &#34;type&#34;: &#34;string&#34;}&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, the columns &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;birthYear&lt;/code&gt;, and &lt;code&gt;address&lt;/code&gt; from the DataFrame are converted into Bigtable columns and the &lt;code&gt;id&lt;/code&gt; column is used as the row key. Note that you could also specify &lt;em&gt;compound&lt;/em&gt; row keys, which are created by concatenating multiple DataFrame columns together.&lt;/p&gt; &#xA;&lt;h3&gt;Writing to Bigtable&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;bigtable&lt;/code&gt; format along with specifying the Bigtable project and instance id to write to Bigtable. The catalog definition specifies the table destination. This is a sample snippet of writing to Bigtable using Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Dataset&amp;lt;Row&amp;gt; dataFrame;&#xA;// Adding some values to dataFrame.&#xA;dataFrame&#xA;  .write()&#xA;  .format(&#34;bigtable&#34;)&#xA;  .option(&#34;catalog&#34;, catalog)&#xA;  .option(&#34;spark.bigtable.project.id&#34;, projectId)&#xA;  .option(&#34;spark.bigtable.instance.id&#34;, instanceId);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reading from Bigtable&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;bigtable&lt;/code&gt; format and catalog, along with the Bigtable project and instance id to read from Bigtable. This is a sample snippet of reading from Bigtable using Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Dataset&amp;lt;Row&amp;gt; dataFrame = spark&#xA;    .read()&#xA;    .format(&#34;bigtable&#34;)&#xA;    .option(&#34;catalog&#34;, catalog)&#xA;    .option(&#34;spark.bigtable.project.id&#34;, projectId)&#xA;    .option(&#34;spark.bigtable.instance.id&#34;, instanceId)&#xA;    .load();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Runtime configurations&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.option(&amp;lt;config_name&amp;gt;, &amp;lt;config_value&amp;gt;)&lt;/code&gt; in Spark to pass different runtime configurations to the connector. For example, Bigtable project and instance ID or settings for timestamp and timeout configurations. For a full list of configurations, refer to &lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudDataproc/spark-bigtable-connector/main/spark-bigtable_2.12/src/main/scala/com/google/cloud/spark/bigtable/datasources/BigtableSparkConf.scala&#34;&gt;BigtableSparkConf.scala&lt;/a&gt;, where these configs are defined.&lt;/p&gt; &#xA;&lt;h3&gt;Bigtable emulator support&lt;/h3&gt; &#xA;&lt;p&gt;When using the connector locally, you can start a Bigtable emulator server and set the environment variable &lt;code&gt;export BIGTABLE_EMULATOR_HOST=localhost:&amp;lt;emulator_port&amp;gt;&lt;/code&gt; in the same environment where Spark is launched. The connector will use the emulator instead of a real Bigtable instance. You can refer to the &lt;a href=&#34;https://cloud.google.com/bigtable/docs/emulator&#34;&gt;Bigtable emulator documentations&lt;/a&gt; for more details on using it.&lt;/p&gt; &#xA;&lt;h3&gt;Complex data type serialization using Apache Avro&lt;/h3&gt; &#xA;&lt;p&gt;You can specify an Avro schema for columns with a complex Spark SQL type such as &lt;code&gt;ArrayType&lt;/code&gt;, &lt;code&gt;MapType&lt;/code&gt;, or &lt;code&gt;StructType&lt;/code&gt;, to serialize and store them in Bigtable.&lt;/p&gt; &#xA;&lt;h3&gt;Row key filter push down&lt;/h3&gt; &#xA;&lt;p&gt;This connector supports pushing down some of the filters on the row key column in the DataFrame to Bigtable and performing them on the server-side. The list of supported or non-supported filters is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Filter&lt;/th&gt; &#xA;   &lt;th&gt;Push down filter supported&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;EqualTo&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LessThan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GreaterThan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LessThanOrEqual&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GreaterThanOrEqual&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;StringStartsWith&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Or&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;And&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Not&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Compound Row Key&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that when using compound row keys, filter on those columns are &lt;strong&gt;not&lt;/strong&gt; pushed to Bigtable and are performed on the client-side (resulting in a full-table scan). If filtering is required, a workaround is to concatenate the intended columns into a &lt;em&gt;single&lt;/em&gt; DataFrame column of a supported type (e.g., string) and use that column as the row key with one of the supported filters above. One option is using the &lt;code&gt;concat&lt;/code&gt; function, with a sample snippet in Scala as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df&#xA;  .withColumn(&#34;new_row_key&#34;, &#xA;    org.apache.spark.sql.functions.concat(&#xA;      df.col(&#34;first_col&#34;), &#xA;      df.col(&#34;second_col&#34;)&#xA;    ))&#xA;  .drop(&#34;first_col&#34;)&#xA;  .drop(&#34;second_col&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Client-side metrics&lt;/h3&gt; &#xA;&lt;p&gt;Since the Bigtable Spark connector is based on the &lt;a href=&#34;https://github.com/googleapis/java-bigtable&#34;&gt;Bigtable Client for Java&lt;/a&gt;, client-side metrics are enabled inside the connector by default. You can refer to the &lt;a href=&#34;https://cloud.google.com/bigtable/docs/client-side-metrics&#34;&gt;client-side metrics&lt;/a&gt; documentation to find more details on accessing and interpreting these metrics.&lt;/p&gt; &#xA;&lt;h3&gt;Use low-level RDD functions with Bigtable&lt;/h3&gt; &#xA;&lt;p&gt;Since the Bigtable Spark connector is based on the &lt;a href=&#34;https://github.com/googleapis/java-bigtable&#34;&gt;Bigtable client for Java&lt;/a&gt;, you can directly use the client in your Spark applications and perform distributed read or write requests within the low-level RDD functions such as &lt;code&gt;mapPartitions&lt;/code&gt; and &lt;code&gt;foreachPartition&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use the Bigtable client for Java classes, append the &lt;code&gt;com.google.cloud.spark.bigtable.repackaged&lt;/code&gt; prefix to the package names. For example, instead of using the class name as &lt;code&gt;com.google.cloud.bigtable.data.v2.BigtableDataClient&lt;/code&gt;, use &lt;code&gt;com.google.cloud.spark.bigtable.repackaged.com.google.cloud.bigtable.data.v2.BigtableDataClient&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;You can access examples for Java, Scala, and Python inside the &lt;code&gt;examples&lt;/code&gt; directory. Each directory contains a &lt;code&gt;README.md&lt;/code&gt; file with instruction on running the example inside.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Currently, only some types are supported in the connector catalog, as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Catalog support&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;binary&lt;/code&gt; (byte array)&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Support for other types, e.g., &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, etc., will be added in future versions of the connector. The &lt;code&gt;examples&lt;/code&gt; folder contains workaround for converting these types to &lt;code&gt;BinaryType&lt;/code&gt; inside your application, in different languages. Additionally, the columns may be converted to a supported type, e.g., &lt;code&gt;string()&lt;/code&gt;, in SQL when writing and converted back to the intended type on read. For complex types, e.g., &lt;code&gt;ArrayType&lt;/code&gt;, &lt;code&gt;MapType&lt;/code&gt;, and &lt;code&gt;StructType&lt;/code&gt;, you can use Avro for serializations. You can refer to &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-ref-datatypes.html&#34;&gt;this link&lt;/a&gt; for more information on Spark SQL types.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>saig0/mensch-aergere-dich-nicht-fx</title>
    <updated>2024-05-05T01:41:00Z</updated>
    <id>tag:github.com,2024-05-05:/saig0/mensch-aergere-dich-nicht-fx</id>
    <link href="https://github.com/saig0/mensch-aergere-dich-nicht-fx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;board game built with JavaFx&lt;/p&gt;&lt;hr&gt;&lt;p&gt;To package the application, simply run the package-javafx task on sbt.&lt;/p&gt;</summary>
  </entry>
</feed>