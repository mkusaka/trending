<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-14T02:23:07Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/SynapseML</title>
    <updated>2022-08-14T02:23:07Z</updated>
    <id>tag:github.com,2022-08-14:/microsoft/SynapseML</id>
    <link href="https://github.com/microsoft/SynapseML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple and Distributed Machine Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://mmlspark.azureedge.net/icons/mmlspark.svg?sanitize=true&#34; alt=&#34;SynapseML&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Synapse Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;SynapseML (previously MMLSpark) is an open source library to simplify the creation of scalable machine learning pipelines. SynapseML builds on &lt;a href=&#34;https://github.com/apache/spark&#34;&gt;Apache Spark&lt;/a&gt; and SparkML to enable new kinds of machine learning, analytics, and model deployment workflows. SynapseML adds many deep learning and data science tools to the Spark ecosystem, including seamless integration of Spark Machine Learning pipelines with the &lt;a href=&#34;https://onnx.ai&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt;, &lt;a href=&#34;https://github.com/Microsoft/LightGBM&#34;&gt;LightGBM&lt;/a&gt;, &lt;a href=&#34;https://azure.microsoft.com/en-us/services/cognitive-services/&#34;&gt;The Cognitive Services&lt;/a&gt;, &lt;a href=&#34;https://vowpalwabbit.org/&#34;&gt;Vowpal Wabbit&lt;/a&gt;, and &lt;a href=&#34;http://www.opencv.org/&#34;&gt;OpenCV&lt;/a&gt;. These tools enable powerful and highly-scalable predictive and analytical models for a variety of datasources.&lt;/p&gt; &#xA;&lt;p&gt;SynapseML also brings new networking capabilities to the Spark Ecosystem. With the HTTP on Spark project, users can embed &lt;strong&gt;any&lt;/strong&gt; web service into their SparkML models. For production grade deployment, the Spark Serving project enables high throughput, sub-millisecond latency web services, backed by your Spark cluster.&lt;/p&gt; &#xA;&lt;p&gt;SynapseML requires Scala 2.12, Spark 3.2+, and Python 3.6+. See the API documentation &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.10.0/scala/index.html#package&#34;&gt;for Scala&lt;/a&gt; and &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/index.html&#34;&gt;for PySpark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topics&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Build&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://msdata.visualstudio.com/A365/_build/latest?definitionId=17563&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://msdata.visualstudio.com/A365/_apis/build/status/microsoft.SynapseML?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/Microsoft/SynapseML&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/Microsoft/SynapseML/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Version&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/Microsoft/SynapseML/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-0.10.0-blue&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Microsoft/SynapseML/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-notes-blue&#34; alt=&#34;Release Notes&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#sbt&#34;&gt;&lt;img src=&#34;https://mmlspark.blob.core.windows.net/icons/badges/master_version3.svg?sanitize=true&#34; alt=&#34;Snapshot Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Docs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.10.0/scala/index.html#package&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=scala&amp;amp;color=blue&amp;amp;logo=scala&#34; alt=&#34;Scala Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=python&amp;amp;color=blue&amp;amp;logo=python&#34; alt=&#34;PySpark Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.08744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/academic-paper-7fdcf7&#34; alt=&#34;Academic Paper&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Support&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://gitter.im/Microsoft/MMLSpark?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/MMLSpark.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;mailto:synapseml-support@microsoft.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mail-synapseml--support-brightgreen&#34; alt=&#34;Mail&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Binder&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/microsoft/SynapseML/93d7ccf?labpath=notebooks%2Ffeatures&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#documentation-and-examples&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#setup-and-installation&#34;&gt;Setup and installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#papers&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#learn-more&#34;&gt;Learn More&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#contributing--feedback&#34;&gt;Contributing &amp;amp; feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#other-relevant-projects&#34;&gt;Other relevant projects&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/vw-blue-dark-orange.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/decision_tree_recolor.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/mmlspark_serving_recolor.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/vw/about/&#34;&gt;&lt;strong&gt;Vowpal Wabbit on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/cognitive_services/CognitiveServices%20-%20Overview/&#34;&gt;&lt;strong&gt;The Cognitive Services for Big Data&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/&#34;&gt;&lt;strong&gt;LightGBM on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/spark_serving/about/&#34;&gt;&lt;strong&gt;Spark Serving&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fast, Sparse, and Effective Text Analytics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Leverage the Microsoft Cognitive Services at Unprecedented Scales in your existing SparkML pipelines&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Train Gradient Boosted Machines with LightGBM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Serve any Spark Computation as a Web Service with Sub-Millisecond Latency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/microservice_recolor.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/onnxai-ar21_crop.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/scales.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/Readme/bindings.png&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/cognitive_services/CognitiveServices%20-%20Overview/#arbitrary-web-apis&#34;&gt;&lt;strong&gt;HTTP on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/onnx/about/&#34;&gt;&lt;strong&gt;ONNX on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Model%20Interpretation%20on%20Spark/&#34;&gt;&lt;strong&gt;Responsible AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/reference/developer-readme/#packagepython&#34;&gt;&lt;strong&gt;Spark Binding Autogeneration&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;An Integration Between Spark and the HTTP Protocol, enabling Distributed Microservice Orchestration&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Distributed and Hardware Accelerated Model Inference on Spark&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Understand Opaque-box Models and Measure Dataset Biases&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Automatically Generate Spark bindings for PySpark and SparklyR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/isolation forest 3.svg&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/cyberml.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;150&#34; src=&#34;https://mmlspark.blob.core.windows.net/graphics/emails/conditional_knn.svg?sanitize=true&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/documentation/estimators/estimators_core/#isolationforest&#34;&gt;&lt;strong&gt;Isolation Forest on Spark&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/SynapseML/raw/master/notebooks/features/other/CyberML%20-%20Anomalous%20Access%20Detection.ipynb&#34;&gt;&lt;strong&gt;CyberML&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://microsoft.github.io/SynapseML/docs/features/other/ConditionalKNN%20-%20Exploring%20Art%20Across%20Cultures/&#34;&gt;&lt;strong&gt;Conditional KNN&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Distributed Nonlinear Outlier Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Machine Learning Tools for Cyber Security&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scalable KNN Models with Conditional Queries&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documentation and Examples&lt;/h2&gt; &#xA;&lt;p&gt;For quickstarts, documentation, demos, and examples please see our &lt;a href=&#34;https://aka.ms/spark&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup and installation&lt;/h2&gt; &#xA;&lt;p&gt;First select the correct platform that you are installing SynapseML into:&lt;/p&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#synapse-analytics&#34;&gt;Synapse Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#databricks&#34;&gt;Databricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#python-standalone&#34;&gt;Python Standalone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#spark-submit&#34;&gt;Spark Submit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#apache-livy-and-hdinsight&#34;&gt;Apachy Livy and HDInsight&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#r-beta&#34;&gt;R (Beta)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/#building-from-source&#34;&gt;Building from Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;h3&gt;Synapse Analytics&lt;/h3&gt; &#xA;&lt;p&gt;In Azure Synapse notebooks please place the following in the first cell of your notebook.&lt;/p&gt; &#xA;&lt;p&gt;For Spark 3.2 Pools:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;  &#34;name&#34;: &#34;synapseml&#34;,&#xA;  &#34;conf&#34;: {&#xA;      &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.10.0&#34;,&#xA;      &#34;spark.jars.repositories&#34;: &#34;https://mmlspark.azureedge.net/maven&#34;,&#xA;      &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind&#34;,&#xA;      &#34;spark.yarn.user.classpath.first&#34;: &#34;true&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Spark 3.1 Pools:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;  &#34;name&#34;: &#34;synapseml&#34;,&#xA;  &#34;conf&#34;: {&#xA;      &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.9.5-13-d1b51517-SNAPSHOT&#34;,&#xA;      &#34;spark.jars.repositories&#34;: &#34;https://mmlspark.azureedge.net/maven&#34;,&#xA;      &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12&#34;,&#xA;      &#34;spark.yarn.user.classpath.first&#34;: &#34;true&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install at the pool level instead of the notebook level &lt;a href=&#34;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/how-to-set-spark-pyspark-custom-configs-in-synapse-workspace/ba-p/2114434&#34;&gt;add the spark properties listed above to the pool configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Databricks&lt;/h3&gt; &#xA;&lt;p&gt;To install SynapseML on the &lt;a href=&#34;http://community.cloud.databricks.com&#34;&gt;Databricks cloud&lt;/a&gt;, create a new &lt;a href=&#34;https://docs.databricks.com/user-guide/libraries.html#libraries-from-maven-pypi-or-spark-packages&#34;&gt;library from Maven coordinates&lt;/a&gt; in your workspace.&lt;/p&gt; &#xA;&lt;p&gt;For the coordinates use: &lt;code&gt;com.microsoft.azure:synapseml_2.12:0.10.0&lt;/code&gt; with the resolver: &lt;code&gt;https://mmlspark.azureedge.net/maven&lt;/code&gt;. Ensure this library is attached to your target cluster(s).&lt;/p&gt; &#xA;&lt;p&gt;Finally, ensure that your Spark cluster has at least Spark 3.2 and Scala 2.12. If you encounter Netty dependency issues please use DBR 10.1.&lt;/p&gt; &#xA;&lt;p&gt;You can use SynapseML in both your Scala and PySpark notebooks. To get started with our example notebooks import the following databricks archive:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;https://mmlspark.blob.core.windows.net/dbcs/SynapseMLExamplesv0.10.0.dbc&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python Standalone&lt;/h3&gt; &#xA;&lt;p&gt;To try out SynapseML on a Python (or Conda) installation you can get Spark installed via pip with &lt;code&gt;pip install pyspark&lt;/code&gt;. You can then use &lt;code&gt;pyspark&lt;/code&gt; as in the above example, or from python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyspark&#xA;spark = pyspark.sql.SparkSession.builder.appName(&#34;MyApp&#34;) \&#xA;            .config(&#34;spark.jars.packages&#34;, &#34;com.microsoft.azure:synapseml_2.12:0.10.0&#34;) \&#xA;            .getOrCreate()&#xA;import synapse.ml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark Submit&lt;/h3&gt; &#xA;&lt;p&gt;SynapseML can be conveniently installed on existing Spark clusters via the &lt;code&gt;--packages&lt;/code&gt; option, examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark-shell --packages com.microsoft.azure:synapseml_2.12:0.10.0&#xA;pyspark --packages com.microsoft.azure:synapseml_2.12:0.10.0&#xA;spark-submit --packages com.microsoft.azure:synapseml_2.12:0.10.0 MyApp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;If you are building a Spark application in Scala, add the following lines to your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;com.microsoft.azure&#34; % &#34;synapseml_2.12&#34; % &#34;0.10.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Apache Livy and HDInsight&lt;/h3&gt; &#xA;&lt;p&gt;To install SynapseML from within a Jupyter notebook served by Apache Livy the following configure magic can be used. You will need to start a new session after this configure cell is executed.&lt;/p&gt; &#xA;&lt;p&gt;Excluding certain packages from the library may be necessary due to current issues with Livy 0.5.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%%configure -f&#xA;{&#xA;    &#34;name&#34;: &#34;synapseml&#34;,&#xA;    &#34;conf&#34;: {&#xA;        &#34;spark.jars.packages&#34;: &#34;com.microsoft.azure:synapseml_2.12:0.10.0&#34;,&#xA;        &#34;spark.jars.excludes&#34;: &#34;org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to evaluate SynapseML is via our pre-built Docker container. To do so, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it -p 8888:8888 -e ACCEPT_EULA=yes mcr.microsoft.com/mmlspark/release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;http://localhost:8888/&#34;&gt;http://localhost:8888/&lt;/a&gt; in your web browser to run the sample notebooks. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/docs/docker.md&#34;&gt;documentation&lt;/a&gt; for more on Docker use.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To read the EULA for using the docker image, run \ &lt;code&gt;docker run -it -p 8888:8888 mcr.microsoft.com/mmlspark/release eula&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;R (Beta)&lt;/h3&gt; &#xA;&lt;p&gt;To try out SynapseML using the R autogenerated wrappers &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/website/docs/reference/R-setup.md&#34;&gt;see our instructions&lt;/a&gt;. Note: This feature is still under development and some necessary custom wrappers may be missing.&lt;/p&gt; &#xA;&lt;h3&gt;C# (.NET)&lt;/h3&gt; &#xA;&lt;p&gt;To try out SynapseML with .NET, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/website/docs/reference/dotnet-setup.md&#34;&gt;.NET Installation Guide&lt;/a&gt;. Please note that some classes including the &lt;code&gt;AzureSearchWriter&lt;/code&gt;, &lt;code&gt;DiagnosticInfo&lt;/code&gt;, &lt;code&gt;UDPyFParam&lt;/code&gt;, &lt;code&gt;ParamSpaceParam&lt;/code&gt;, &lt;code&gt;BallTreeParam&lt;/code&gt;, &lt;code&gt;ConditionalBallTreeParam&lt;/code&gt;, &lt;code&gt;LightGBMBoosterParam&lt;/code&gt; are still under development and not exposed in .NET yet.&lt;/p&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;SynapseML has recently transitioned to a new build infrastructure. For detailed developer docs please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/website/docs/reference/developer-readme.md&#34;&gt;Developer Readme&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are an existing synapsemldeveloper, you will need to reconfigure your development setup. We now support platform independent development and better integrate with intellij and SBT. If you encounter issues please reach out to our support email!&lt;/p&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.08044&#34;&gt;Large Scale Intelligent Microservices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.07177&#34;&gt;Conditional Image Retrieval&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.08744&#34;&gt;MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.04031&#34;&gt;Flexible and Scalable Deep Learning with SynapseML&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit our &lt;a href=&#34;https://microsoft.github.io/SynapseML/&#34; title=&#34;aka.ms/spark&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Watch our keynote demos at &lt;a href=&#34;https://youtu.be/T_fs4C0aqD0?t=425&#34;&gt;the Spark+AI Summit 2019&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/N3ozCZXeOeU?t=472&#34;&gt;the Spark+AI European Summit 2018&lt;/a&gt;, and &lt;a href=&#34;https://databricks.com/sparkaisummit/north-america/spark-summit-2018-keynotes#Intelligent-cloud&#34; title=&#34;Developing for the Intelligent Cloud and Intelligent Edge&#34;&gt;the Spark+AI Summit 2018&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See how SynapseML is used to &lt;a href=&#34;https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr3&#34; title=&#34;Identifying snow leopards with AI&#34;&gt;help endangered species&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Explore generative adversarial artwork in &lt;a href=&#34;https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr4&#34; title=&#34;Generative art at the MET&#34;&gt;our collaboration with The MET and MIT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Explore &lt;a href=&#34;https://blogs.technet.microsoft.com/machinelearning/2018/03/05/image-data-support-in-apache-spark/&#34; title=&#34;Image Data Support in Apache Spark&#34;&gt;our collaboration with Apache Spark&lt;/a&gt; on image analysis.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing &amp;amp; feedback&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/SynapseML/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for contribution guidelines.&lt;/p&gt; &#xA;&lt;p&gt;To give feedback and/or report an issue, open a &lt;a href=&#34;https://help.github.com/articles/creating-an-issue/&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other relevant projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit&#34;&gt;Vowpal Wabbit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/LightGBM&#34;&gt;LightGBM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/DMTK&#34;&gt;DMTK: Microsoft Distributed Machine Learning Toolkit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/Recommenders&#34;&gt;Recommenders&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/alipay/jpmml-sparkml-lightgbm&#34;&gt;JPMML-SparkML plugin for converting SynapseML LightGBM models to PMML&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Microsoft/CNTK&#34;&gt;Microsoft Cognitive Toolkit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Apache®, Apache Spark, and Spark® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-08-14T02:23:07Z</updated>
    <id>tag:github.com,2022-08-14:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/riscv-torture</title>
    <updated>2022-08-14T02:23:07Z</updated>
    <id>tag:github.com,2022-08-14:/ucb-bar/riscv-torture</id>
    <link href="https://github.com/ucb-bar/riscv-torture" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RISC-V Torture Test&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;=========================================================================== RISC-V Torture Test Generator&lt;/h1&gt; &#xA;&lt;h1&gt;Author: Yunsup Lee and Henry Cook&lt;/h1&gt; &#xA;&lt;h1&gt;Date: January 29th, 2012&lt;/h1&gt; &#xA;&lt;h1&gt;Version: (under version control)&lt;/h1&gt; &#xA;&lt;p&gt;This is the RISC-V torture test generator and framework. This repository contains three sub-projects that build upon one another. The first, [generator], is used to create a single random torture test. The second, [testrun], is used to run a particular test on particular simulators, diffing the resulting signature with the ISA simulator and optionally creating a derivative test subset that pinpoints the divergence. The third, [overnight], wraps testrun, allowing tests to be run repeatedly for a given duration or until a failure count.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Modify &#34;config/default.config&#34; to set the parameters desired for building tests (e.g., setting which instructions to use and in which ratio).&lt;/p&gt; &#xA;&lt;p&gt;Modify &#34;Makefile&#34; as desired to execute the C simulator or RTL simulator of your choice, and to set the other parameters as you require.&lt;/p&gt; &#xA;&lt;p&gt;To build a single test and test it on Spike:&lt;/p&gt; &#xA;&lt;p&gt;$ make igentest&lt;/p&gt; &#xA;&lt;p&gt;To build single test and run it on the C simulator or RTL simulator, use &#34;make cgentest&#34; or &#34;make rgentest&#34;.&lt;/p&gt; &#xA;&lt;p&gt;To run overnight tests, you can use &#34;make cnight&#34; and &#34;make rnight&#34;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Signatures&lt;/h2&gt; &#xA;&lt;p&gt;Torture works by dumping the register state out to memory at the end of the test program execution. This output is then compared against the output from the Spike ISA simulator.&lt;/p&gt; &#xA;&lt;p&gt;The torture program writes the register state to the memory address specified by &#34;xreg_output_data&#34;, which is located in the memory section &#34;.global begin_signature&#34;. The Spike ISA simulator will write out the data found in the &#34;begin_signature&#34; section on exit if provided with the &#34;+signature=&#34; argument:&lt;/p&gt; &#xA;&lt;p&gt;$ spike +signature=my_spike_signature.txt test_binary&lt;/p&gt; &#xA;&lt;p&gt;The Rocket-chip infrastructure uses the &#34;riscv-fesvr&#34; program to control the execution of the C and RTL simulators. The &#34;riscv-fesvr&#34; also accepts the +signature argument too.&lt;/p&gt; &#xA;&lt;p&gt;$ ./csim-rocket-chip +signature=my_rocket_signature.txt test_binary&lt;/p&gt; &#xA;&lt;p&gt;A simple diff between the Spike and chip simulator signatures will tell you if any errors have occurred.&lt;/p&gt; &#xA;&lt;p&gt;$ diff my_spike_signature.txt my_rocket_signature.txt&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PORTING TORTURE TO YOUR OWN RISC-V PROCESSOR:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to use riscv-torture with your own RISC-V processor, you will need to provide a way to dump the &#34;begin_signature&#34; section to a file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Low-level Usage&lt;/h2&gt; &#xA;&lt;p&gt;Some basic use cases are illustrated here (note the Makefile abstracts this for you).&lt;/p&gt; &#xA;&lt;p&gt;Make a single test: % ./sbt generator/run % cd output % make % spike +signature=test.sig test&lt;/p&gt; &#xA;&lt;p&gt;Take an existing test and diff the signatures of ISA and C simulators: % ./sbt &#39;testrun/run -a output/test.S -c /path/to/reference-chip/emulator/emulator&#39;&lt;/p&gt; &#xA;&lt;p&gt;*** Currently, due to the limiation of scala process library, you cannot torture the RTL simulator ***&lt;/p&gt; &#xA;&lt;h1&gt;Generate a random test and diff the signatures of ISA and RTL simulators:&lt;/h1&gt; &#xA;&lt;h1&gt;% ./sbt &#39;testrun/run -r /path/to/reference-chip/vlsi/build/vcs-sim-rtl/simv&#39;&lt;/h1&gt; &#xA;&lt;p&gt;Run tests for 30 minutes, email hcook when done, and save failures to dir: % ./sbt &#39;overnight/run -m 30 -e &lt;a href=&#34;mailto:hcook@eecs.berkeley.edu&#34;&gt;hcook@eecs.berkeley.edu&lt;/a&gt; -p dir&#39;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;% git submodule update --init&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Overnight Overview&lt;/h2&gt; &#xA;&lt;p&gt;This framework utilizes both the test runner and test generator to perform a long terms serach for failing test cases. It takes the following command line arguments:&lt;/p&gt; &#xA;&lt;p&gt;Usage: overnight/run [options] -C &#xA; &lt;file&gt;&#xA;   | --config &#xA;  &lt;file&gt;&#xA;    config file -p &#xA;  &lt;/file&gt;&#xA; &lt;/file&gt;&lt;/p&gt;&#xA;&lt;dir&gt;&#xA;  | --permdir &#xA; &lt;dir&gt;&#xA;   dir to store failing tests -c &#xA;  &lt;file&gt;&#xA;    | --csim &#xA;   &lt;file&gt;&#xA;     C simulator -r &#xA;    &lt;file&gt;&#xA;      | --rtlsim &#xA;     &lt;file&gt;&#xA;       RTL simulator -e &#xA;      &lt;address&gt; | --email &lt;address&gt; email to report to -t &#xA;        &lt;count&gt;&#xA;          | --threshold &#xA;         &lt;count&gt;&#xA;           number of failures to trigger email -m &#xA;          &lt;int&gt;&#xA;            | --minutes &#xA;           &lt;int&gt;&#xA;             number of minutes to run tests&#xA;            &lt;p&gt;&lt;/p&gt; &#xA;            &lt;p&gt;You can only generate tests with one instruction mix at a time (based on the setting in the config file). It doesn&#39;t matter what simulator you use with the -r and -c flags, they just determines the name used to describe whose diff failed.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Testrun Overview&lt;/h2&gt; &#xA;            &lt;p&gt;This utility compares the signatures generated by passing the -testsig flag to the specified simulators. If it encounters a difference, it subdivides the test into many subtests and searches for which exact program segment reveals the failure. It takes the following command line arguments:&lt;/p&gt; &#xA;            &lt;p&gt;Usage: testrun/run [options] -C &#xA;             &lt;file&gt;&#xA;               | --config &#xA;              &lt;file&gt;&#xA;                config file -a &#xA;               &lt;file&gt;&#xA;                 | --asm &#xA;                &lt;file&gt;&#xA;                  input ASM file -c &#xA;                 &lt;file&gt;&#xA;                   | --csim &#xA;                  &lt;file&gt;&#xA;                    C simulator -r &#xA;                   &lt;file&gt;&#xA;                     | --rtlsim &#xA;                    &lt;file&gt;&#xA;                      RTL simulator -s &#xA;                     &lt;boolean&gt;&#xA;                       | --seek &#xA;                      &lt;boolean&gt;&#xA;                        Seek for failing pseg -d &#xA;                       &lt;boolean&gt;&#xA;                         | --dump &#xA;                        &lt;boolean&gt;&#xA;                          Dump mismatched signatures&#xA;                        &lt;/boolean&gt;&#xA;                       &lt;/boolean&gt;&#xA;                      &lt;/boolean&gt;&#xA;                     &lt;/boolean&gt;&#xA;                    &lt;/file&gt;&#xA;                   &lt;/file&gt;&#xA;                  &lt;/file&gt;&#xA;                 &lt;/file&gt;&#xA;                &lt;/file&gt;&#xA;               &lt;/file&gt;&#xA;              &lt;/file&gt;&#xA;             &lt;/file&gt;&lt;/p&gt; &#xA;            &lt;p&gt;If you don&#39;t specify a asm file, a random one will be generated for you. You can only generate tests with one instruction mix at a time (based on the setting in the config file). It doesn&#39;t matter what simulator you use with the -r and -c flags, they just determines the name used to describe whose diff failed. By default, a failed diff will result in the subtest sweep occuring, but this search can be diasbled. Note that the pseg ID reported is actually the pseg following the pseg containing the error. You can optionally dump mistmatched signatures to the dir containing the asm file under test.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Generator Overview&lt;/h2&gt; &#xA;            &lt;p&gt;To generate a random test, the torture test generator randomly generates many test sequences from a set of test sequences that are written by hand, performs a random register allocation for all test sequences, and finally randomly interleaves instructions from these test sequences. To extend the set of tests or coverage, the programmer needs to write new test sequences. It takes the following command line arguments:&lt;/p&gt; &#xA;            &lt;p&gt;Usage: generator/run [options] -o &#xA;             &lt;filename&gt;&#xA;               | --output &#xA;              &lt;filename&gt;&#xA;                output filename -C &#xA;               &lt;file&gt;&#xA;                 | --config &#xA;                &lt;file&gt;&#xA;                  config file&#xA;                &lt;/file&gt;&#xA;               &lt;/file&gt;&#xA;              &lt;/filename&gt;&#xA;             &lt;/filename&gt;&lt;/p&gt; &#xA;            &lt;p&gt;The following sections describe adding new functionality to the generator.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Test sequence example&lt;/h2&gt; &#xA;            &lt;p&gt;Before we talk about how to write a test sequence, let&#39;s look at a very simple example. The following example is a test sequence, which emits an add instruction.&lt;/p&gt; &#xA;            &lt;p&gt;class SeqADD extends Seq { val src1 = reg_read_any() val src2 = reg_read_any() val dest = reg_write(src1, src2) insts += ADD(dest, src1, src2) }&lt;/p&gt; &#xA;            &lt;p&gt;As I hinted in the overview that the test generator will do register allocation you don&#39;t write a string of instructions with architectural registers. You request a virtual registers (i.e., registers that are yet tied down to architectural registers) when you need them, save them in scala values, and use them when you need to (e.g., in an instruction).&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Types of virtual registers&lt;/h2&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt; &lt;p&gt;Hidden (position dependent registers): Registers that will have different values when the code is positioned at a different address. An example is registers that hold addresses. Registers that are hidden should be excluded from the output signature.&lt;/p&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;Visible (position independent registers): Registers that are not hidden, therefore will have the same values when the code is positioned at a different address. These registers should be included as part of the output signature.&lt;/p&gt; &lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;How to write a sequence&lt;/h2&gt; &#xA;            &lt;p&gt;Use the following functions to request a register, and generate a string of instructions (look at Inst.scala to see what instructions are available) that uses these virtual registers, and add them to the insts array.&lt;/p&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt;reg_read_zero(): returns register x0&lt;/li&gt; &#xA;             &lt;li&gt;reg_read_any(): returns any type of register (hidden or visible)&lt;/li&gt; &#xA;             &lt;li&gt;reg_read_visible(): returns a visible register&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_ra(): returns register ra for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_visible(): returns a visible register for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write_hidden(): returns a hidden register for write&lt;/li&gt; &#xA;             &lt;li&gt;reg_write(regs: Reg*): returns a register that matches the type of regs (if any reg in regs are hidden, the output type is hidden)&lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;            &lt;p&gt;Note that the torture test framework is written in scala, you can use any scala functionality to generate instructions. Look at SeqALU.scala, SeqMem.scala, and SeqBranch.scala to get inspired.&lt;/p&gt; &#xA;            &lt;hr&gt; &#xA;            &lt;h2&gt;Future TODO&lt;/h2&gt; &#xA;            &lt;ul&gt; &#xA;             &lt;li&gt; &lt;p&gt;provide support for loops&lt;/p&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;generate statistics of a test to get a sense of coverage&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;statistics should include instruction count of each type&lt;/li&gt; &#xA;               &lt;li&gt;statistics should include register usage&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;complete floating point tests&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;add floating point memory move tests&lt;/li&gt; &#xA;               &lt;li&gt;improve floating point init randomization&lt;/li&gt; &#xA;               &lt;li&gt;add rounding modes tests&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;complete vector tests&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;better randomization&lt;/li&gt; &#xA;               &lt;li&gt;add SeqVOnly: Tests special vf-only instructions&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;             &lt;li&gt; &lt;p&gt;code refactoring&lt;/p&gt; &#xA;              &lt;ul&gt; &#xA;               &lt;li&gt;consolidate RegPool logic&lt;/li&gt; &#xA;               &lt;li&gt;detect and suppress unallocatable sequences&lt;/li&gt; &#xA;              &lt;/ul&gt; &lt;/li&gt; &#xA;            &lt;/ul&gt; &#xA;           &lt;/int&gt;&#xA;          &lt;/int&gt;&#xA;         &lt;/count&gt;&#xA;        &lt;/count&gt;&lt;/address&gt;&lt;/address&gt;&#xA;     &lt;/file&gt;&#xA;    &lt;/file&gt;&#xA;   &lt;/file&gt;&#xA;  &lt;/file&gt;&#xA; &lt;/dir&gt;&#xA;&lt;/dir&gt;</summary>
  </entry>
</feed>