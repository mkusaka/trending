<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T02:01:22Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>databricks/spark-xml</title>
    <updated>2023-10-22T02:01:22Z</updated>
    <id>tag:github.com,2023-10-22:/databricks/spark-xml</id>
    <link href="https://github.com/databricks/spark-xml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;XML data source for Spark SQL and DataFrames&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XML Data Source for Apache Spark 3.x&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;A library for parsing and querying XML data with &lt;a href=&#34;https://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;, for Spark SQL and DataFrames. The structure and test tools are mostly copied from &lt;a href=&#34;https://github.com/databricks/spark-csv&#34;&gt;CSV Data Source for Spark&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This package supports to process format-free XML files in a distributed way, unlike JSON datasource in Spark restricts in-line JSON format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compatible with Spark 3.0 and later with Scala 2.12, and also Spark 3.2 and later with Scala 2.12 or 2.13. Scala 2.11 and Spark 2 support ended with version 0.13.0.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Currently, &lt;code&gt;spark-xml&lt;/code&gt; is planned to &lt;a href=&#34;https://github.com/apache/spark/pull/41832&#34;&gt;become a part of Apache Spark 4.0&lt;/a&gt;. This library will remain in maintenance mode for Spark 3.x versions.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-xml_2.12&#xA;version: 0.17.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.17.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading XML files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: Location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowTag&lt;/code&gt;: The row tag of your xml files to treat as a row. For example, in this xml &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;book&lt;/code&gt;. Default is &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;samplingRatio&lt;/code&gt;: Sampling ratio for inferring schema (0.0 ~ 1). Default is 1. Possible types are &lt;code&gt;StructType&lt;/code&gt;, &lt;code&gt;ArrayType&lt;/code&gt;, &lt;code&gt;StringType&lt;/code&gt;, &lt;code&gt;LongType&lt;/code&gt;, &lt;code&gt;DoubleType&lt;/code&gt;, &lt;code&gt;BooleanType&lt;/code&gt;, &lt;code&gt;TimestampType&lt;/code&gt; and &lt;code&gt;NullType&lt;/code&gt;, unless user provides a schema for this.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;excludeAttribute&lt;/code&gt; : Whether you want to exclude attributes in elements or not. Default is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;treatEmptyValuesAsNulls&lt;/code&gt; : (DEPRECATED: use &lt;code&gt;nullValue&lt;/code&gt; set to &lt;code&gt;&#34;&#34;&lt;/code&gt;) Whether you want to treat whitespaces as a null value. Default is false&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: The mode for dealing with corrupt records during parsing. Default is &lt;code&gt;PERMISSIVE&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt; : &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;When it encounters a corrupted record, it sets all fields to &lt;code&gt;null&lt;/code&gt; and puts the malformed string into a new field configured by &lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;When it encounters a field of the wrong datatype, it sets the offending field to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt; : ignores the whole corrupted records.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt; : throws an exception when it meets corrupted records.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: if &lt;code&gt;true&lt;/code&gt;, attempts to infer an appropriate type for each resulting DataFrame column, like a boolean, numeric or date type. If &lt;code&gt;false&lt;/code&gt;, all resulting columns are of string type. Default is &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;: The name of new field where malformed strings are stored. Default is &lt;code&gt;_corrupt_record&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;attributePrefix&lt;/code&gt;: The prefix for attributes so that we can differentiate attributes and elements. This will be the prefix for field names. Default is &lt;code&gt;_&lt;/code&gt;. Can be empty, but only for reading XML.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valueTag&lt;/code&gt;: The tag used for the value when there are attributes in the element having no child. Default is &lt;code&gt;_VALUE&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: Defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignoreSurroundingSpaces&lt;/code&gt;: Defines whether or not surrounding whitespaces from values being read should be skipped. Default is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wildcardColName&lt;/code&gt;: Name of a column existing in the provided schema which is interpreted as a &#39;wildcard&#39;. It must have type string or array of strings. It will match any XML child element that is not otherwise matched by the schema. The XML of the child becomes the string value of the column. If an array, then all unmatched elements will be returned as an array of strings. As its name implies, it is meant to emulate XSD&#39;s &lt;code&gt;xs:any&lt;/code&gt; type. Default is &lt;code&gt;xs_any&lt;/code&gt;. New in 0.11.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowValidationXSDPath&lt;/code&gt;: Path to an XSD file that is used to validate the XML for each row individually. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided, or inferred. Note that if the same local path is not already also visible on the executors in the cluster, then the XSD and any others it depends on should be added to the Spark executors with &lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@addFile(path:String):Unit&#34;&gt;&lt;code&gt;SparkContext.addFile&lt;/code&gt;&lt;/a&gt;. In this case, to use local XSD &lt;code&gt;/foo/bar.xsd&lt;/code&gt;, call &lt;code&gt;addFile(&#34;/foo/bar.xsd&#34;)&lt;/code&gt; and pass just &lt;code&gt;&#34;bar.xsd&#34;&lt;/code&gt; as &lt;code&gt;rowValidationXSDPath&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignoreNamespace&lt;/code&gt;: If true, namespaces prefixes on XML elements and attributes are ignored. Tags &lt;code&gt;&amp;lt;abc:author&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;def:author&amp;gt;&lt;/code&gt; would, for example, be treated as if both are just &lt;code&gt;&amp;lt;author&amp;gt;&lt;/code&gt;. Note that, at the moment, namespaces cannot be ignored on the &lt;code&gt;rowTag&lt;/code&gt; element, only its children. Note that XML parsing is in general not namespace-aware even if &lt;code&gt;false&lt;/code&gt;. Defaults to &lt;code&gt;false&lt;/code&gt;. New in 0.11.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timestampFormat&lt;/code&gt;: Specifies an additional timestamp format that will be tried when parsing values as &lt;code&gt;TimestampType&lt;/code&gt; columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to try several formats, including &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_INSTANT&#34;&gt;ISO_INSTANT&lt;/a&gt;, including variations with offset timezones or no timezone (defaults to UTC). New in 0.12.0. As of 0.16.0, if a custom format pattern is used without a timezone, the default Spark timezone specified by &lt;code&gt;spark.sql.session.timeZone&lt;/code&gt; will be used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timezone&lt;/code&gt;: identifier of timezone to be used when reading timestamps without a timezone specified. New in 0.16.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: Specifies an additional timestamp format that will be tried when parsing values as &lt;code&gt;DateType&lt;/code&gt; columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_DATE&#34;&gt;ISO_DATE&lt;/a&gt;. New in 0.12.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: Location to write files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rowTag&lt;/code&gt;: The row tag of your xml files to treat as a row. For example, in &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;book&lt;/code&gt;. Default is &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rootTag&lt;/code&gt;: The root tag of your xml files to treat as the root. For example, in &lt;code&gt;&amp;lt;books&amp;gt; &amp;lt;book&amp;gt;&amp;lt;book&amp;gt; ...&amp;lt;/books&amp;gt;&lt;/code&gt;, the appropriate value would be &lt;code&gt;books&lt;/code&gt;. It can include basic attributes by specifying a value like &lt;code&gt;books foo=&#34;bar&#34;&lt;/code&gt; (as of 0.11.0). Default is &lt;code&gt;ROWS&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;declaration&lt;/code&gt;: Content of XML declaration to write at the start of every output XML file, before the &lt;code&gt;rootTag&lt;/code&gt;. For example, a value of &lt;code&gt;foo&lt;/code&gt; causes &lt;code&gt;&amp;lt;?xml foo?&amp;gt;&lt;/code&gt; to be written. Set to empty string to suppress. Defaults to &lt;code&gt;version=&#34;1.0&#34; encoding=&#34;UTF-8&#34; standalone=&#34;yes&#34;&lt;/code&gt;. New in 0.14.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;arrayElementName&lt;/code&gt;: Name of XML element that encloses each element of an array-valued column when writing. Default is &lt;code&gt;item&lt;/code&gt;. New in 0.16.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: The value to write &lt;code&gt;null&lt;/code&gt; value. Default is string &lt;code&gt;null&lt;/code&gt;. When this is &lt;code&gt;null&lt;/code&gt;, it does not write attributes and elements for fields.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;attributePrefix&lt;/code&gt;: The prefix for attributes so that we can differentiating attributes and elements. This will be the prefix for field names. Default is &lt;code&gt;_&lt;/code&gt;. Cannot be empty for writing XML.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valueTag&lt;/code&gt;: The tag used for the value when there are attributes in the element having no child. Default is &lt;code&gt;_VALUE&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;compression&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timestampFormat&lt;/code&gt;: Controls the format used to write &lt;code&gt;TimestampType&lt;/code&gt; format columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_INSTANT&#34;&gt;ISO_INSTANT&lt;/a&gt;. New in 0.12.0. As of 0.16.0, if a custom format pattern is used without a timezone, the default Spark timezone specified by &lt;code&gt;spark.sql.session.timeZone&lt;/code&gt; will be used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timezone&lt;/code&gt;: identifier of timezone to be used when writing timestamps without a timezone specified. New in 0.16.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: Controls the format used to write &lt;code&gt;DateType&lt;/code&gt; format columns. The format is specified as described in &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html&#34;&gt;DateTimeFormatter&lt;/a&gt;. Defaults to &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_DATE&#34;&gt;ISO_DATE&lt;/a&gt;. New in 0.12.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently it supports the shortened name usage. You can use just &lt;code&gt;xml&lt;/code&gt; instead of &lt;code&gt;com.databricks.spark.xml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;XSD Support&lt;/h3&gt; &#xA;&lt;p&gt;Per above, the XML for individual rows can be validated against an XSD using &lt;code&gt;rowValidationXSDPath&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The utility &lt;code&gt;com.databricks.spark.xml.util.XSDToSchema&lt;/code&gt; can be used to extract a Spark DataFrame schema from &lt;em&gt;some&lt;/em&gt; XSD files. It supports only simple, complex and sequence types, and only basic XSD functionality.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.util.XSDToSchema&#xA;import java.nio.file.Paths&#xA;&#xA;val schema = XSDToSchema.read(Paths.get(&#34;/path/to/your.xsd&#34;))&#xA;val df = spark.read.schema(schema)....xml(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parsing Nested XML&lt;/h3&gt; &#xA;&lt;p&gt;Although primarily used to convert (portions of) large XML documents into a &lt;code&gt;DataFrame&lt;/code&gt;, &lt;code&gt;spark-xml&lt;/code&gt; can also parse XML in a string-valued column in an existing DataFrame with &lt;code&gt;from_xml&lt;/code&gt;, in order to add it as a new column with parsed results as a struct.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.functions.from_xml&#xA;import com.databricks.spark.xml.schema_of_xml&#xA;import spark.implicits._&#xA;val df = ... /// DataFrame with XML in column &#39;payload&#39; &#xA;val payloadSchema = schema_of_xml(df.select(&#34;payload&#34;).as[String])&#xA;val parsed = df.withColumn(&#34;parsed&#34;, from_xml($&#34;payload&#34;, payloadSchema))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This can convert arrays of strings containing XML to arrays of parsed structs. Use &lt;code&gt;schema_of_xml_array&lt;/code&gt; instead&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;com.databricks.spark.xml.from_xml_string&lt;/code&gt; is an alternative that operates on a String directly instead of a column, for use in UDFs&lt;/li&gt; &#xA; &lt;li&gt;If you use &lt;code&gt;DROPMALFORMED&lt;/code&gt; mode with &lt;code&gt;from_xml&lt;/code&gt;, then XML values that do not parse correctly will result in a &lt;code&gt;null&lt;/code&gt; value for the column. No rows will be dropped.&lt;/li&gt; &#xA; &lt;li&gt;If you use &lt;code&gt;PERMISSIVE&lt;/code&gt; mode with &lt;code&gt;from_xml&lt;/code&gt; et al, which is the default, then the parse mode will actually instead default to &lt;code&gt;DROPMALFORMED&lt;/code&gt;. If however you include a column in the schema for &lt;code&gt;from_xml&lt;/code&gt; that matches the &lt;code&gt;columnNameOfCorruptRecord&lt;/code&gt;, then &lt;code&gt;PERMISSIVE&lt;/code&gt; mode will still output malformed records to that column in the resulting struct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pyspark notes&lt;/h4&gt; &#xA;&lt;p&gt;The functions above are exposed in the Scala API only, at the moment, as there is no separate Python package for &lt;code&gt;spark-xml&lt;/code&gt;. They can be accessed from Pyspark by manually declaring some helper functions that call into the JVM-based API from Python. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql.column import Column, _to_java_column&#xA;from pyspark.sql.types import _parse_datatype_json_string&#xA;&#xA;def ext_from_xml(xml_column, schema, options={}):&#xA;    java_column = _to_java_column(xml_column.cast(&#39;string&#39;))&#xA;    java_schema = spark._jsparkSession.parseDataType(schema.json())&#xA;    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)&#xA;    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(&#xA;        java_column, java_schema, scala_map)&#xA;    return Column(jc)&#xA;&#xA;def ext_schema_of_xml_df(df, options={}):&#xA;    assert len(df.columns) == 1&#xA;&#xA;    scala_options = spark._jvm.PythonUtils.toScalaMap(options)&#xA;    java_xml_module = getattr(getattr(&#xA;        spark._jvm.com.databricks.spark.xml, &#34;package$&#34;), &#34;MODULE$&#34;)&#xA;    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)&#xA;    return _parse_datatype_json_string(java_schema.json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Structure Conversion&lt;/h2&gt; &#xA;&lt;p&gt;Due to the structure differences between &lt;code&gt;DataFrame&lt;/code&gt; and XML, there are some conversion rules from XML data to &lt;code&gt;DataFrame&lt;/code&gt; and from &lt;code&gt;DataFrame&lt;/code&gt; to XML data. Note that handling attributes can be disabled with the option &lt;code&gt;excludeAttribute&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Conversion from XML to &lt;code&gt;DataFrame&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Attributes&lt;/strong&gt;: Attributes are converted as fields with the heading prefix, &lt;code&gt;attributePrefix&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;one myOneAttrib=&#34;AAAA&#34;&amp;gt;&#xA;    &amp;lt;two&amp;gt;two&amp;lt;/two&amp;gt;&#xA;    &amp;lt;three&amp;gt;three&amp;lt;/three&amp;gt;&#xA;&amp;lt;/one&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root&#xA; |-- _myOneAttrib: string (nullable = true)&#xA; |-- two: string (nullable = true)&#xA; |-- three: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Value in an element that has no child elements but attributes&lt;/strong&gt;: The value is put in a separate field, &lt;code&gt;valueTag&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;one&amp;gt;&#xA;    &amp;lt;two myTwoAttrib=&#34;BBBBB&#34;&amp;gt;two&amp;lt;/two&amp;gt;&#xA;    &amp;lt;three&amp;gt;three&amp;lt;/three&amp;gt;&#xA;&amp;lt;/one&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root&#xA; |-- two: struct (nullable = true)&#xA; |    |-- _VALUE: string (nullable = true)&#xA; |    |-- _myTwoAttrib: string (nullable = true)&#xA; |-- three: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Conversion from &lt;code&gt;DataFrame&lt;/code&gt; to XML&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Element as an array in an array&lt;/strong&gt;: Writing a XML file from &lt;code&gt;DataFrame&lt;/code&gt; having a field &lt;code&gt;ArrayType&lt;/code&gt; with its element as &lt;code&gt;ArrayType&lt;/code&gt; would have an additional nested field for the element. This would not happen in reading and writing XML data but writing a &lt;code&gt;DataFrame&lt;/code&gt; read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a &lt;code&gt;DataFrame&lt;/code&gt; read from other sources is possible to have a different structure.&lt;/p&gt; &lt;p&gt;&lt;code&gt;DataFrame&lt;/code&gt; with a schema below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; |-- a: array (nullable = true)&#xA; |    |-- element: array (containsNull = true)&#xA; |    |    |-- element: string (containsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with data below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;+------------------------------------+&#xA;|                                   a|&#xA;+------------------------------------+&#xA;|[WrappedArray(aa), WrappedArray(bb)]|&#xA;+------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;produces a XML file below:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;a&amp;gt;&#xA;    &amp;lt;item&amp;gt;aa&amp;lt;/item&amp;gt;&#xA;&amp;lt;/a&amp;gt;&#xA;&amp;lt;a&amp;gt;&#xA;    &amp;lt;item&amp;gt;bb&amp;lt;/item&amp;gt;&#xA;&amp;lt;/a&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;These examples use a XML file available for download &lt;a href=&#34;https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;XML data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE books&#xA;USING com.databricks.spark.xml&#xA;OPTIONS (path &#34;books.xml&#34;, rowTag &#34;book&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL. In this case, we do not infer schema.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string)&#xA;USING com.databricks.spark.xml&#xA;OPTIONS (path &#34;books.xml&#34;, rowTag &#34;book&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;Import &lt;code&gt;com.databricks.spark.xml._&lt;/code&gt; to get implicits that add the &lt;code&gt;.xml(...)&lt;/code&gt; method to &lt;code&gt;DataFrame&lt;/code&gt;. You can also use &lt;code&gt;.format(&#34;xml&#34;)&lt;/code&gt; and &lt;code&gt;.load(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SparkSession&#xA;import com.databricks.spark.xml._&#xA;&#xA;val spark = SparkSession.builder().getOrCreate()&#xA;val df = spark.read&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;books.xml&#34;)&#xA;&#xA;val selectedData = df.select(&#34;author&#34;, &#34;_id&#34;)&#xA;selectedData.write&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;newbooks.xml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType}&#xA;import com.databricks.spark.xml._&#xA;&#xA;val spark = SparkSession.builder().getOrCreate()&#xA;val customSchema = StructType(Array(&#xA;  StructField(&#34;_id&#34;, StringType, nullable = true),&#xA;  StructField(&#34;author&#34;, StringType, nullable = true),&#xA;  StructField(&#34;description&#34;, StringType, nullable = true),&#xA;  StructField(&#34;genre&#34;, StringType, nullable = true),&#xA;  StructField(&#34;price&#34;, DoubleType, nullable = true),&#xA;  StructField(&#34;publish_date&#34;, StringType, nullable = true),&#xA;  StructField(&#34;title&#34;, StringType, nullable = true)))&#xA;&#xA;&#xA;val df = spark.read&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .schema(customSchema)&#xA;  .xml(&#34;books.xml&#34;)&#xA;&#xA;val selectedData = df.select(&#34;author&#34;, &#34;_id&#34;)&#xA;selectedData.write&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .xml(&#34;newbooks.xml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SparkSession;&#xA;&#xA;SparkSession spark = SparkSession.builder().getOrCreate();&#xA;DataFrame df = spark.read()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .load(&#34;books.xml&#34;);&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .save(&#34;newbooks.xml&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SparkSession;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SparkSession spark = SparkSession.builder().getOrCreate();&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;  new StructField(&#34;_id&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;author&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;description&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;genre&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;price&#34;, DataTypes.DoubleType, true, Metadata.empty()),&#xA;  new StructField(&#34;publish_date&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;  new StructField(&#34;title&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = spark.read()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .schema(customSchema)&#xA;  .load(&#34;books.xml&#34;);&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write()&#xA;  .format(&#34;xml&#34;)&#xA;  .option(&#34;rootTag&#34;, &#34;books&#34;)&#xA;  .option(&#34;rowTag&#34;, &#34;book&#34;)&#xA;  .save(&#34;newbooks.xml&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;spark = SparkSession.builder.getOrCreate()&#xA;&#xA;df = spark.read.format(&#39;xml&#39;).options(rowTag=&#39;book&#39;).load(&#39;books.xml&#39;)&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;, rootTag=&#39;books&#39;) \&#xA;    .save(&#39;newbooks.xml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;from pyspark.sql.types import *&#xA;&#xA;spark = SparkSession.builder.getOrCreate()&#xA;customSchema = StructType([&#xA;    StructField(&#34;_id&#34;, StringType(), True),&#xA;    StructField(&#34;author&#34;, StringType(), True),&#xA;    StructField(&#34;description&#34;, StringType(), True),&#xA;    StructField(&#34;genre&#34;, StringType(), True),&#xA;    StructField(&#34;price&#34;, DoubleType(), True),&#xA;    StructField(&#34;publish_date&#34;, StringType(), True),&#xA;    StructField(&#34;title&#34;, StringType(), True)])&#xA;&#xA;df = spark.read \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;) \&#xA;    .load(&#39;books.xml&#39;, schema = customSchema)&#xA;&#xA;df.select(&#34;author&#34;, &#34;_id&#34;).write \&#xA;    .format(&#39;xml&#39;) \&#xA;    .options(rowTag=&#39;book&#39;, rootTag=&#39;books&#39;) \&#xA;    .save(&#39;newbooks.xml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;Automatically infer schema (data types)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;sparkR.session(&#34;local[4]&#34;, sparkPackages = c(&#34;com.databricks:spark-xml_2.12:0.17.0&#34;))&#xA;&#xA;df &amp;lt;- read.df(&#34;books.xml&#34;, source = &#34;xml&#34;, rowTag = &#34;book&#34;)&#xA;&#xA;# In this case, `rootTag` is set to &#34;ROWS&#34; and `rowTag` is set to &#34;ROW&#34;.&#xA;write.df(df, &#34;newbooks.csv&#34;, &#34;xml&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;sparkR.session(&#34;local[4]&#34;, sparkPackages = c(&#34;com.databricks:spark-xml_2.12:0.17.0&#34;))&#xA;customSchema &amp;lt;- structType(&#xA;  structField(&#34;_id&#34;, &#34;string&#34;),&#xA;  structField(&#34;author&#34;, &#34;string&#34;),&#xA;  structField(&#34;description&#34;, &#34;string&#34;),&#xA;  structField(&#34;genre&#34;, &#34;string&#34;),&#xA;  structField(&#34;price&#34;, &#34;double&#34;),&#xA;  structField(&#34;publish_date&#34;, &#34;string&#34;),&#xA;  structField(&#34;title&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(&#34;books.xml&#34;, source = &#34;xml&#34;, schema = customSchema, rowTag = &#34;book&#34;)&#xA;&#xA;# In this case, `rootTag` is set to &#34;ROWS&#34; and `rowTag` is set to &#34;ROW&#34;.&#xA;write.df(df, &#34;newbooks.csv&#34;, &#34;xml&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hadoop InputFormat&lt;/h2&gt; &#xA;&lt;p&gt;The library contains a Hadoop input format for reading XML files by a start tag and an end tag. This is similar with &lt;a href=&#34;https://github.com/apache/mahout/raw/9d14053c80a1244bdf7157ab02748a492ae9868a/integration/src/main/java/org/apache/mahout/text/wikipedia/XmlInputFormat.java&#34;&gt;XmlInputFormat.java&lt;/a&gt; in &lt;a href=&#34;https://mahout.apache.org&#34;&gt;Mahout&lt;/a&gt; but supports to read compressed files, different encodings and read elements including attributes, which you may make direct use of as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.databricks.spark.xml.XmlInputFormat&#xA;import org.apache.spark.SparkContext&#xA;import org.apache.hadoop.io.{LongWritable, Text}&#xA;&#xA;val sc: SparkContext = _&#xA;&#xA;// This will detect the tags including attributes&#xA;sc.hadoopConfiguration.set(XmlInputFormat.START_TAG_KEY, &#34;&amp;lt;book&amp;gt;&#34;)&#xA;sc.hadoopConfiguration.set(XmlInputFormat.END_TAG_KEY, &#34;&amp;lt;/book&amp;gt;&#34;)&#xA;&#xA;val records = sc.newAPIHadoopFile(&#xA;  &#34;path&#34;,&#xA;  classOf[XmlInputFormat],&#xA;  classOf[LongWritable],&#xA;  classOf[Text])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;https://www.scala-sbt.org/&#34;&gt;SBT&lt;/a&gt;. To build a JAR file simply run &lt;code&gt;sbt package&lt;/code&gt; from the project root.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project was initially created by &lt;a href=&#34;https://github.com/HyukjinKwon&#34;&gt;HyukjinKwon&lt;/a&gt; and donated to &lt;a href=&#34;https://databricks.com&#34;&gt;Databricks&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neerajgangwar/echo</title>
    <updated>2023-10-22T02:01:22Z</updated>
    <id>tag:github.com,2023-10-22:/neerajgangwar/echo</id>
    <link href="https://github.com/neerajgangwar/echo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;E-Book Search Engine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Echo&lt;/h1&gt; &#xA;&lt;p&gt;E-Book search Engine. Code for indexing and searching.&lt;/p&gt;</summary>
  </entry>
</feed>