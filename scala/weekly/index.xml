<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-31T02:24:06Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gitbucket/gitbucket</title>
    <updated>2022-07-31T02:24:06Z</updated>
    <id>tag:github.com,2022-07-31:/gitbucket/gitbucket</id>
    <link href="https://github.com/gitbucket/gitbucket" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Git platform powered by Scala with easy installation, high extensibility &amp; GitHub API compatibility&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitBucket &lt;a href=&#34;https://gitter.im/gitbucket/gitbucket&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitbucket/gitbucket.svg?sanitize=true&#34; alt=&#34;Gitter chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gitbucket/gitbucket/actions?query=workflow%3Abuild+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/gitbucket/gitbucket/workflows/build/badge.svg?branch=master&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://index.scala-lang.org/gitbucket/gitbucket/gitbucket&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/gitbucket/gitbucket/gitbucket/latest-by-scala-version.svg?sanitize=true&#34; alt=&#34;gitbucket Scala version support&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gitbucket/gitbucket/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;GitBucket is a Git web platform powered by Scala offering:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy installation&lt;/li&gt; &#xA; &lt;li&gt;Intuitive UI&lt;/li&gt; &#xA; &lt;li&gt;High extensibility by plugins&lt;/li&gt; &#xA; &lt;li&gt;API compatibility with GitHub&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gitbucket.github.io/img/screenshots/screenshot-repository_viewer.png&#34; alt=&#34;GitBucket&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can try an &lt;a href=&#34;https://gitbucket.herokuapp.com/&#34;&gt;online demo&lt;/a&gt; &lt;em&gt;(ID: root / Pass: root)&lt;/em&gt; of GitBucket, and also get the latest information at &lt;a href=&#34;https://gitbucket.github.io/gitbucket-news/&#34;&gt;GitBucket News&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The current version of GitBucket provides many features such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public / Private Git repositories (with http/https and ssh access)&lt;/li&gt; &#xA; &lt;li&gt;GitLFS support&lt;/li&gt; &#xA; &lt;li&gt;Repository viewer including an online file editor&lt;/li&gt; &#xA; &lt;li&gt;Issues, Pull Requests and Wiki for repositories&lt;/li&gt; &#xA; &lt;li&gt;Activity timeline and email notifications&lt;/li&gt; &#xA; &lt;li&gt;Account and group management with LDAP integration&lt;/li&gt; &#xA; &lt;li&gt;a Plug-in system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;GitBucket requires &lt;strong&gt;Java8&lt;/strong&gt;. You have to install it, if it is not already installed.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the latest &lt;strong&gt;gitbucket.war&lt;/strong&gt; from &lt;a href=&#34;https://github.com/gitbucket/gitbucket/releases&#34;&gt;the releases page&lt;/a&gt; and run it by &lt;code&gt;java -jar gitbucket.war&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;http://[hostname]:8080/&lt;/code&gt; and log in with ID: &lt;strong&gt;root&lt;/strong&gt; / Pass: &lt;strong&gt;root&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can also deploy &lt;code&gt;gitbucket.war&lt;/code&gt; to a servlet container which supports Servlet 3.0 (like Jetty, Tomcat, JBoss, etc). Note that GitBucket doesn&#39;t support Jakarta EE yet.&lt;/p&gt; &#xA;&lt;p&gt;For more information about installation on Mac or Windows Server (with IIS), or configuration of Apache or Nginx and also integration with other tools or services such as Jenkins or Slack, see &lt;a href=&#34;https://github.com/gitbucket/gitbucket/wiki&#34;&gt;Wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To upgrade GitBucket, replace &lt;code&gt;gitbucket.war&lt;/code&gt; with the new version, after stopping GitBucket. All GitBucket data is stored in &lt;code&gt;HOME/.gitbucket&lt;/code&gt; by default. So if you want to back up GitBucket&#39;s data, copy this directory to the backup location.&lt;/p&gt; &#xA;&lt;h2&gt;Plugins&lt;/h2&gt; &#xA;&lt;p&gt;GitBucket has a plug-in system that allows extra functionality. Officially the following plug-ins are provided:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gitbucket/gitbucket-gist-plugin&#34;&gt;gitbucket-gist-plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gitbucket/gitbucket-emoji-plugin&#34;&gt;gitbucket-emoji-plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gitbucket/gitbucket-pages-plugin&#34;&gt;gitbucket-pages-plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gitbucket/gitbucket-notifications-plugin&#34;&gt;gitbucket-notifications-plugin&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find more plugins made by the community at &lt;a href=&#34;https://gitbucket-plugins.github.io/&#34;&gt;GitBucket community plugins&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building and Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to try the development version of GitBucket, or want to contribute to the project, please see the &lt;a href=&#34;https://github.com/gitbucket/gitbucket/raw/master/doc/readme.md&#34;&gt;Developer&#39;s Guide&lt;/a&gt;. It provides instructions on building from source and on setting up an IDE for debugging. It also contains documentation of the core concepts used within the project.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have any questions about GitBucket, see &lt;a href=&#34;https://github.com/gitbucket/gitbucket/wiki&#34;&gt;Wiki&lt;/a&gt; and check issues whether there is a same question or request in the past.&lt;/li&gt; &#xA; &lt;li&gt;If you can&#39;t find same question and report, send it to our &lt;a href=&#34;https://gitter.im/gitbucket/gitbucket&#34;&gt;Gitter room&lt;/a&gt; before raising an issue.&lt;/li&gt; &#xA; &lt;li&gt;The highest priority of GitBucket is the ease of installation and API compatibility with GitHub, so your feature request might be rejected if they go against those principles.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s New in 4.37.x&lt;/h2&gt; &#xA;&lt;h3&gt;4.37.2 - 16 Jan 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fixed a security issue reported by &lt;a href=&#34;https://www.ptsecurity.com/ww-en/&#34;&gt;Positive Technologies&lt;/a&gt;. Great thanks for their detailed report and close support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4.37.1 - 14 Dec 2021&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update gist-plugin and notification-plugin&lt;/li&gt; &#xA; &lt;li&gt;Fix SSHCommand extension point for apache-sshd 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4.37.0 - 11 Dec 2021&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enhance Git Reference APIs&lt;/li&gt; &#xA; &lt;li&gt;Add milestone data to issue list API&lt;/li&gt; &#xA; &lt;li&gt;Support &#34;all&#34; in issue list API&lt;/li&gt; &#xA; &lt;li&gt;Support EDDSA in signed commit verification&lt;/li&gt; &#xA; &lt;li&gt;Support custom SSH url&lt;/li&gt; &#xA; &lt;li&gt;Relax max passward length limitation&lt;/li&gt; &#xA; &lt;li&gt;Relax max webhook url length limitation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/gitbucket/gitbucket/master/CHANGELOG.md&#34;&gt;change log&lt;/a&gt; for all of the updates.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-07-31T02:24:06Z</updated>
    <id>tag:github.com,2022-07-31:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>meta-soul/LakeSoul</title>
    <updated>2022-07-31T02:24:06Z</updated>
    <id>tag:github.com,2022-07-31:/meta-soul/LakeSoul</id>
    <link href="https://github.com/meta-soul/LakeSoul" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Table Structure Storage on Data Lakes to Unify Batch and Streaming Data Processing&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/LakeSoul/main/README-CN.md&#34;&gt;CN Doc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LakeSoul&lt;/h1&gt; &#xA;&lt;p&gt;LakeSoul is a unified streaming and batch table storage for fast data processing built on top of the Apache Spark engine by the &lt;a href=&#34;https://www.dmetasoul.com&#34;&gt;DMetaSoul&lt;/a&gt; team, and supports scalable metadata management, ACID transactions, efficient and flexible upsert operation, schema evolution, and streaming &amp;amp; batch unification. &lt;img src=&#34;https://raw.githubusercontent.com/meta-soul/LakeSoul/main/doc/LakeSoul.png&#34; alt=&#34;LakeSoul Arch&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LakeSoul implements incremental upserts for both row and column and allows concurrent updates on the same partition. LakeSoul uses LSM-Tree like structure to support updates on hash partitioning table with primary key, and achieve very high write throughput (30MB/s/core) on cloud object store like S3 while providing optimized merge on read performance. LakeSoul scales meta data management by using distributed NoSQL DB Cassandra.&lt;/p&gt; &#xA;&lt;p&gt;More detailed features please refer to our wiki page: &lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/wiki/Home&#34;&gt;Wiki Home&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage Documentations&lt;/h1&gt; &#xA;&lt;p&gt;Please find usage documentations in project&#39;s wiki: &lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/wiki/Usage-Doc&#34;&gt;Usage Doc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/wiki/%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3&#34;&gt;使用文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/wiki/QuickStart&#34;&gt;Quick Start&lt;/a&gt; to quickly set up a test env.&lt;/p&gt; &#xA;&lt;p&gt;Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/LakeSoul/main/examples/cdc_ingestion_debezium&#34;&gt;CDC Ingestion with Debezium and Kafka&lt;/a&gt; example on how to sync LakeSoul table with OLTP dbs like MySQL in a realtime manner.&lt;/p&gt; &#xA;&lt;h1&gt;Feature Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Meta Management (&lt;a href=&#34;https://github.com/meta-soul/LakeSoul/issues/23&#34;&gt;#23&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multiple Level Partitioning: Multiple range partition and at most one hash partition&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Concurrent write with auto conflict resolution&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; MVCC with read isolation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Write transaction through Postgres Transaction&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Table operations &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LSM-Tree style upsert for hash partitioned table&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Merge on read for hash partition with upsert delta file&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Copy on write update for non hash partitioned table&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Compaction&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Spark Integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Table/Dataframe API&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SQL support with catalog except upsert&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Query optimization &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Shuffle/Join elimination for operations on primary key&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Merge UDF (Merge operator)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Merge Into SQL support &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Merge Into SQL with match on Primary Key (Merge on read)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Merge Into SQL with match on non-pk&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Merge Into SQL with match condition and complex expression (Merge on read when match on PK) (depends on &lt;a href=&#34;https://github.com/meta-soul/LakeSoul/issues/66&#34;&gt;#66&lt;/a&gt;)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Flink Integration (&lt;a href=&#34;https://github.com/meta-soul/LakeSoul/issues/57&#34;&gt;#57&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Table API&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Flink CDC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Hive Integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Export to Hive partition after compaction&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Realtime Data Warehousing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CDC ingestion&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Time Travel (Snapshot read)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Snapshot rollback&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MPP Engine Integration (depends on &lt;a href=&#34;https://github.com/meta-soul/LakeSoul/issues/66&#34;&gt;#66&lt;/a&gt;) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Presto&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Apache Doris&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Native IO (&lt;a href=&#34;https://github.com/meta-soul/LakeSoul/issues/66&#34;&gt;#66&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Object storage IO optimization&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Native merge on read&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Cloud Native &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Multi-layer storage classes support with data tiering&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community guidelines&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-soul/LakeSoul/main/community-guideline.md&#34;&gt;Community guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Feedback and Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Please feel free to open an issue or dicussion if you have any questions.&lt;/p&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://join.slack.com/t/dmetasoul-user/shared_invite/zt-1681xagg3-4YouyW0Y4wfhPnvji~OwFg&#34;&gt;slack user group&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contact Us&lt;/h1&gt; &#xA;&lt;p&gt;Email us at &lt;a href=&#34;mailto:opensource@dmetasoul.com&#34;&gt;opensource@dmetasoul.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Opensource License&lt;/h1&gt; &#xA;&lt;p&gt;LakeSoul is opensourced under Apache License v2.0.&lt;/p&gt;</summary>
  </entry>
</feed>