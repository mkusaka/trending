<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:47:12Z</updated>
  <subtitle>Weekly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apache/incubator-linkis</title>
    <updated>2022-10-16T01:47:12Z</updated>
    <id>tag:github.com,2022-10-16:/apache/incubator-linkis</id>
    <link href="https://github.com/apache/incubator-linkis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Linkis helps easily connect to various back-end computation/storage engines(Spark, Python, TiDB...), exposes various interfaces(REST, JDBC, Java ...), with multi-tenancy, high performance, and resource control.&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt; Apache Linkis(Incubating) &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;Linkis builds a computation middleware layer to decouple the upper applications and the underlying data engines, provides standardized interfaces (REST, JDBC, WebSocket etc.) to easily connect to various underlying engines (Spark, Presto, Flink, etc.), while enables cross engine context sharing, unified job&amp;amp; engine governance and orchestration.&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://linkis.apache.org/&#34;&gt;https://linkis.apache.org/&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://linkis.apache.org/docs/latest/introduction/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN docs&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://linkis.apache.org/zh-CN/docs/latest/introduction/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/文档-简体中文-blue.svg&#34; alt=&#34;简体中文文档&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://search.maven.org/search?q=g:org.apache.linkis%20AND%20a:linkis&#34;&gt; &lt;img src=&#34;https://img.shields.io/maven-central/v/org.apache.linkis/linkis.svg?label=maven%20central&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/apache/incubator-linkis/raw/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?label=license&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://www.oracle.com/technetwork/java/javase/downloads/index.html&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/JDK-8-green.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/apache/incubator-linkis/actions&#34;&gt; &lt;img src=&#34;https://github.com/apache/incubator-linkis/actions/workflows/build.yml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/apache/incubator-linkis&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/apache/incubator-linkis.svg?sanitize=true&#34; alt=&#34;github forks&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/apache/incubator-linkis&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/apache/incubator-linkis.svg?sanitize=true&#34; alt=&#34;github stars&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/apache/incubator-linkis&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/apache/incubator-linkis.svg?sanitize=true&#34; alt=&#34;github contributors&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://codecov.io/gh/apache/incubator-linkis&#34;&gt; &lt;img src=&#34;https://codecov.io/gh/apache/incubator-linkis/branch/master/graph/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://badges.toozhao.com/stats/01G7TRNN1PH9PMSCYWDF3EK4QT&#34;&gt; &lt;img src=&#34;https://badges.toozhao.com/badges/01G7TRNN1PH9PMSCYWDF3EK4QT/green.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-linkis/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-linkis/master/README_CN.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Linkis builds a layer of computation middleware between upper applications and underlying engines. By using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc., and achieve the intercommunication of user resources like unified variables, scripts, UDFs, functions and resource files at the same time.&lt;/p&gt; &#xA;&lt;p&gt;As a computation middleware, Linkis provides powerful connectivity, reuse, orchestration, expansion, and governance capabilities. By decoupling the application layer and the engine layer, it simplifies the complex network call relationship, and thus reduces the overall complexity and saves the development and maintenance costs as well.&lt;/p&gt; &#xA;&lt;p&gt;Since the first release of Linkis in 2019, it has accumulated more than &lt;strong&gt;700&lt;/strong&gt; trial companies and &lt;strong&gt;1000+&lt;/strong&gt; sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. Lots of companies have already used Linkis as a unified entrance for the underlying computation and storage engines of the big data platform.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7869972/148767375-aeb11b93-16ca-46d7-a30e-92fbefe2bd5e.png&#34; alt=&#34;linkis-intro-01&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7869972/148767380-c34f44b2-9320-4633-9ec8-662701f41d15.png&#34; alt=&#34;linkis-intro-03&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support for diverse underlying computation storage engines&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Currently supported computation/storage engines: Spark、Hive、Flink、Python、Pipeline、Sqoop、openLooKeng、Presto、ElasticSearch、JDBC, Shell, etc&lt;/li&gt; &#xA;   &lt;li&gt;Computation/storage engines to be supported: Trino (planned 1.3.1), SeaTunnel (planned 1.3.1), etc&lt;/li&gt; &#xA;   &lt;li&gt;Supported scripting languages: SparkSQL、HiveQL、Python、Shell、Pyspark、R、Scala and JDBC, etc&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful task/request governance capabilities&lt;/strong&gt; With services such as Orchestrator, Label Manager and customized Spring Cloud Gateway, Linkis is able to provide multi-level labels based, cross-cluster/cross-IDC fine-grained routing, load balance, multi-tenancy, traffic control, resource control, and orchestration strategies like dual-active, active-standby, etc&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support full stack computation/storage engine&lt;/strong&gt; As a computation middleware, it will receive, execute and manage tasks and requests for various computation storage engines, including batch tasks, interactive query tasks, real-time streaming tasks and storage tasks&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Resource management capabilities&lt;/strong&gt; ResourceManager is not only capable of managing resources for Yarn and Linkis EngineManger, but also able to provide label-based multi-level resource allocation and recycling, allowing itself to have powerful resource management capabilities across mutiple Yarn clusters and mutiple computation resource types&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified Context Service&lt;/strong&gt; Generate Context ID for each task/request, associate and manage user and system resource files (JAR, ZIP, Properties, etc.), result set, parameter variable, function, etc., across user, system, and computing engine. Set in one place, automatic reference everywhere&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified materials&lt;/strong&gt; System and user-level unified material management, which can be shared and transferred across users and systems&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Supported Engine Types&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Engine Name&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Suppor Component Version&lt;br&gt;(Default Dependent Version)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Linkis Version Requirements&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Included in Release Package&lt;br&gt; By Default&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Spark&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache 2.0.0~2.4.7, &lt;br&gt;CDH &amp;gt;= 5.4.0, &lt;br&gt;(default Apache Spark 2.4.3)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Spark EngineConn, supports SQL , Scala, Pyspark and R code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hive&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache &amp;gt;= 1.0.0, &lt;br&gt;CDH &amp;gt;= 5.4.0, &lt;br&gt;(default Apache Hive 2.3.3)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hive EngineConn, supports HiveQL code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Python&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Python &amp;gt;= 2.6, &lt;br&gt;(default Python2*)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Python EngineConn, supports python code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Shell&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Bash &amp;gt;= 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Shell EngineConn, supports Bash shell code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;JDBC&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MySQL &amp;gt;= 5.0, Hive &amp;gt;=1.2.1, &lt;br&gt;(default Hive-jdbc 2.3.4)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;JDBC EngineConn, already supports MySQL and HiveQL, can be extended quickly Support other engines with JDBC Driver package, such as Oracle&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Flink&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Flink &amp;gt;= 1.12.2, &lt;br&gt;(default Apache Flink 1.12.2)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Flink EngineConn, supports FlinkSQL code, also supports starting a new Yarn in the form of Flink Jar Application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pipeline&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pipeline EngineConn, supports file import and export&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;openLooKeng&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;openLooKeng &amp;gt;= 1.5.0, &lt;br&gt;(default openLookEng 1.5.0)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;openLooKeng EngineConn, supports querying data virtualization engine with Sql openLooKeng&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Sqoop&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Sqoop &amp;gt;= 1.4.6, &lt;br&gt;(default Apache Sqoop 1.4.6)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.1.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Sqoop EngineConn, support data migration tool Sqoop engine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Presto&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Presto &amp;gt;= 0.180, &lt;br&gt;(default Presto 0.234)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Presto EngineConn, supports Presto SQL code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ElasticSearch&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ElasticSearch &amp;gt;=6.0, &lt;br&gt;(default ElasticSearch 7.6.2)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&amp;gt;=1.2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ElasticSearch EngineConn, supports SQL and DSL code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Impala&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Impala &amp;gt;= 3.2.0, CDH &amp;gt;=6.3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ongoing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Impala EngineConn, supports Impala SQL code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MLSQL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MLSQL &amp;gt;=1.1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ongoing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MLSQL EngineConn, supports MLSQL code.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hadoop&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache &amp;gt;=2.6.0, &lt;br&gt;CDH &amp;gt;=5.4.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ongoing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hadoop EngineConn, supports Hadoop MR/YARN application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TiSpark&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ongoing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TiSpark EngineConn, supports querying TiDB with SparkSQL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Ecosystem&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Linkis 1.x(recommend 1.1.1) Compatible&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/DataSphereStudio/raw/master/README.md&#34;&gt;&lt;strong&gt;DataSphereStudio&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DSS 1.0.1[released][Linkis recommend 1.1.1]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Scriptis&#34;&gt;&lt;strong&gt;Scriptis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to &lt;a href=&#34;https://github.com/apache/incubator-linkis&#34;&gt;Linkis&lt;/a&gt; to perform data analysis web tools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;In DSS 1.0.1[released]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Schedulis&#34;&gt;&lt;strong&gt;Schedulis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Schedulis0.6.2 [released]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Qualitis&#34;&gt;&lt;strong&gt;Qualitis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Data quality verification tool, providing data verification capabilities such as data integrity and correctness&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qualitis 0.9.1 [released]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Streamis&#34;&gt;&lt;strong&gt;Streamis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Streamis 0.1.0 [released][Linkis recommend 1.1.0]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Exchangis&#34;&gt;&lt;strong&gt;Exchangis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Exchangis 1.0.0 [developing]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Visualis&#34;&gt;&lt;strong&gt;Visualis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Visualis 1.0.0[developing]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/WeBankFinTech/Prophecis&#34;&gt;&lt;strong&gt;Prophecis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis&#39; MLFlow can be connected to DSS workflow through AppConn&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Prophecis 0.3.0 [released]&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Download&lt;/h1&gt; &#xA;&lt;p&gt;Please go to the &lt;a href=&#34;https://linkis.apache.org/download/main&#34;&gt;Linkis Releases Page&lt;/a&gt; to download a compiled distribution or a source code package of Linkis.&lt;/p&gt; &#xA;&lt;h1&gt;Compile and Deploy&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more detailed guidance see:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://linkis.apache.org/docs/latest/development/linkis-compile-and-package&#34;&gt;[Backend Compile]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://linkis.apache.org/docs/latest/development/web-build&#34;&gt;[Management Console Build]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&#xA;Note: If you want use `-Dlinkis.build.web=true` to build  linkis-web image, you need to compile linkis-web first.&#xA;&#xA;## compile backend&#xA;### Mac OS/Linux&#xA;&#xA;# 1. When compiling for the first time, execute the following command first&#xA;./mvnw -N install&#xA;&#xA;# 2. make the linkis distribution package&#xA;# - Option 1: make the linkis distribution package only&#xA;./mvnw clean install -Dmaven.javadoc.skip=true -Dmaven.test.skip=true&#xA;&#xA;# - Option 2: make the linkis distribution package and docker image&#xA;#   - Option 2.1: image without mysql jdbc jars&#xA;./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true&#xA;#   - Option 2.2: image with mysql jdbc jars&#xA;./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true -Dlinkis.build.with.jdbc=true&#xA;&#xA;# - Option 3: linkis distribution package and docker image (included web)&#xA;./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true -Dlinkis.build.web=true&#xA;&#xA;# - Option 4: linkis distribution package and docker image (included web and ldh (hadoop all in one for test))&#xA;./mvnw clean install -Pdocker -Dmaven.javadoc.skip=true -Dmaven.test.skip=true -Dlinkis.build.web=true -Dlinkis.build.ldh=true -Dlinkis.build.with.jdbc=true&#xA;&#xA;### Windows&#xA;mvnw.cmd -N install&#xA;mvnw.cmd clean install -Dmaven.javadoc.skip=true -Dmaven.test.skip=true&#xA;&#xA;## compile web&#xA;cd incubator-linkis/linkis-web&#xA;npm install&#xA;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Bundled with MySQL JDBC Driver&lt;/h3&gt; &#xA;&lt;p&gt;Due to the MySQL licensing restrictions, the MySQL Java Database Connectivity (JDBC) driver is not bundled with the official released linkis image by default. However, at current stage, linkis still relies on this library to work properly. To solve this problem, we provide a script which can help to creating an custom image with mysql jdbc from the official linkis image by yourself, the image created by this tool will be tagged as &lt;code&gt;linkis:with-jdbc&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$&amp;gt; LINKIS_IMAGE=linkis:1.3.0 &#xA;$&amp;gt; ./linkis-dist/docker/scripts/make-linikis-image-with-mysql-jdbc.sh&#xA;&#xA;#          build dir: ...&#xA;#         base image: linkis:1.3.0&#xA;# mysql jdbc version: 8.0.28&#xA;...                                                                                                                                                                                                                                                     0.0s&#xA; =&amp;gt; exporting to image                                                                                                                                                                                                                                                                                                                                                     0.0s&#xA; =&amp;gt; =&amp;gt; exporting layers                                                                                                                                                                                                                                                                                                                                                    0.0s&#xA; =&amp;gt; =&amp;gt; writing image sha256:3870df5500a71fcf879b5b7d5699c3c9804c7e03e33ad842e5d11f3504371fe8                                                                                                                                                                                                                                                                               0.0s&#xA; =&amp;gt; =&amp;gt; naming to docker.io/library/linkis:with-jdbc                                                                                                                                                                                                                                                                                                                        0.0s&#xA;# done, image: linkis:with-jdbc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://linkis.apache.org/docs/latest/deployment/quick-deploy&#34;&gt;Quick Deployment&lt;/a&gt; to do the deployment.&lt;/p&gt; &#xA;&lt;h1&gt;Examples and Guidance&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://linkis.apache.org/docs/latest/user-guide/overview&#34;&gt;User Manual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://linkis.apache.org/docs/latest/engine-usage/overview&#34;&gt;Engine Usage Documents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://linkis.apache.org/docs/latest/api/overview&#34;&gt;API Documents&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation &amp;amp; Vedio&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The documentation of linkis is in &lt;a href=&#34;https://github.com/apache/incubator-linkis-website&#34;&gt;Linkis-Website Git Repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Meetup videos on &lt;a href=&#34;https://space.bilibili.com/598542776?from=search&amp;amp;seid=14344213924133040656&#34;&gt;Bilibili&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p&gt;Linkis services could be divided into three categories: computation governance services, public enhancement services and microservice governance services&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The computation governance services, support the 3 major stages of processing a task/request: submission -&amp;gt; preparation -&amp;gt; execution&lt;/li&gt; &#xA; &lt;li&gt;The public enhancement services, including the material library service, context service, and data source service&lt;/li&gt; &#xA; &lt;li&gt;The microservice governance services, including Spring Cloud Gateway, Eureka and Open Feign&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Below is the Linkis architecture diagram. You can find more detailed architecture docs in &lt;a href=&#34;https://linkis.apache.org/docs/latest/architecture/overview&#34;&gt;Linkis-Doc/Architecture&lt;/a&gt;. &lt;img src=&#34;https://user-images.githubusercontent.com/7869972/148767383-f87e84ba-5baa-4125-8b6e-d0aa4f7d3a66.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Based on Linkis the computation middleware, we&#39;ve built a lot of applications and tools on top of it in the big data platform suite &lt;a href=&#34;https://github.com/WeBankFinTech/WeDataSphere&#34;&gt;WeDataSphere&lt;/a&gt;. Below are the currently available open-source projects. More projects upcoming, please stay tuned.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7869972/148767389-049361df-3609-4c2f-a4e2-c904c273300e.png&#34; alt=&#34;wedatasphere_stack_Linkis&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Contributions are always welcomed, we need more contributors to build Linkis together. either code, or doc, or other supports that could help the community.&lt;br&gt; For code and documentation contributions, please follow the &lt;a href=&#34;https://linkis.apache.org/community/how-to-contribute&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contact Us&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any questions or suggestions please kindly submit an &lt;a href=&#34;https://github.com/apache/incubator-linkis/issues&#34;&gt;issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;By mail &lt;a href=&#34;mailto:dev@linkis.apache.org&#34;&gt;dev@linkis.apache.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can scan the QR code below to join our WeChat group to get more immediate response&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://linkis.apache.org/Images/wedatasphere_contact_01.png&#34; alt=&#34;wechatgroup&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Who is Using Linkis&lt;/h1&gt; &#xA;&lt;p&gt;We opened an issue &lt;a href=&#34;https://github.com/apache/incubator-linkis/issues/23&#34;&gt;[Who is Using Linkis]&lt;/a&gt; for users to feedback and record who is using Linkis.&lt;br&gt; Since the first release of Linkis in 2019, it has accumulated more than &lt;strong&gt;700&lt;/strong&gt; trial companies and &lt;strong&gt;1000+&lt;/strong&gt; sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/gemmini</title>
    <updated>2022-10-16T01:47:12Z</updated>
    <id>tag:github.com,2022-10-16:/ucb-bar/gemmini</id>
    <link href="https://github.com/ucb-bar/gemmini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Berkeley&#39;s Spatial Array Generator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000&#34; src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/full-logo.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Upcoming Tutorial&lt;/h1&gt; &#xA;&lt;p&gt;We will be presenting &lt;a href=&#34;https://sites.google.com/berkeley.edu/gemmini-tutorial-mlsys-2022&#34;&gt;a new tutorial&lt;/a&gt; for Gemmini at MLSys 2022, on August 29th, 2022.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to attend, &lt;strong&gt;then please register online&lt;/strong&gt; &lt;a href=&#34;https://docs.google.com/forms/d/1bdIXegBkEMJY88YuD80HN40haZ9tx_bZgmaN3FON5DI/edit&#34;&gt;at this link&lt;/a&gt;. We&#39;re looking forward to meeting you all!&lt;/p&gt; &#xA;&lt;h1&gt;Gemmini&lt;/h1&gt; &#xA;&lt;p&gt;The Gemmini project is developing a full-system, full-stack DNN hardware exploration and evaluation platform. Gemmini enables architects to make useful insights into how different components of the system and software stack (outside of just the accelerator itself) interact to affect overall DNN performance.&lt;/p&gt; &#xA;&lt;p&gt;Gemmini is part of the &lt;a href=&#34;https://github.com/ucb-bar/chipyard&#34;&gt;Chipyard&lt;/a&gt; ecosystem, and was developed using the &lt;a href=&#34;https://www.chisel-lang.org/&#34;&gt;Chisel&lt;/a&gt; hardware description language.&lt;/p&gt; &#xA;&lt;p&gt;This document is intended to provide information for beginners wanting to try out Gemmini, as well as more advanced in-depth information for those who might want to start hacking on Gemmini&#39;s source code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-system.png&#34; alt=&#34;Gemmini&#39;s high-level architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;We provide here a quick guide to installing Gemmini&#39;s dependencies (Chipyard and Spike), building Gemmini hardware and software, and then running that software on our hardware simulators.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Before beginning, install the &lt;a href=&#34;https://chipyard.readthedocs.io/en/latest/Chipyard-Basics/Initial-Repo-Setup.html#requirements&#34;&gt;Chipyard dependencies&lt;/a&gt; that are described here.&lt;/p&gt; &#xA;&lt;h2&gt;Installing Chipyard and Spike&lt;/h2&gt; &#xA;&lt;p&gt;Run these steps to install Chipyard and Spike (make sure to checkout the correct Chipyard and Spike commits as shown below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ucb-bar/chipyard.git&#xA;cd chipyard&#xA;git checkout 117624d8eea27bafd613eec09e9b9b3e31239e08&#xA;./scripts/init-submodules-no-riscv-tools.sh&#xA;./scripts/build-toolchains.sh esp-tools&#xA;&#xA;source env.sh&#xA;&#xA;cd generators/gemmini&#xA;git fetch &amp;amp;&amp;amp; git checkout v0.6.4&#xA;git submodule update&#xA;&#xA;cd -&#xA;cd toolchains/esp-tools/riscv-isa-sim/build&#xA;git fetch &amp;amp;&amp;amp; git checkout 090e82c473fd28b4eb2011ffcd771ead6076faab&#xA;make &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting Up Gemmini&lt;/h2&gt; &#xA;&lt;p&gt;Run the steps below to set up Gemmini configuration files, symlinks, and subdirectories:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini&#xA;./scripts/setup-paths.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building Gemmini Software&lt;/h2&gt; &#xA;&lt;p&gt;Run the steps below to compile Gemmini programs, including large DNN models like ResNet50, and small matrix-multiplication tests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini/software/gemmini-rocc-tests&#xA;./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, you&#39;ll find RISC-V binaries in &lt;code&gt;build/&lt;/code&gt;, for &#34;baremetal&#34; environments, Linux environments, and &#34;proxy-kernel&#34; environments.&lt;/p&gt; &#xA;&lt;p&gt;Linux binaries are meant to be executed on SoCs that run Linux. These binaries are dynamically linked, and support all syscalls. Typically, our users run them on &lt;a href=&#34;https://fires.im/&#34;&gt;FireSim&lt;/a&gt; simulators.&lt;/p&gt; &#xA;&lt;p&gt;Baremetal binaries are meant to be run in an environment without any operating system available. They lack support for most syscalls, and do not support virtual memory either. Our users typically run them on cycle-accurate simulators like Verilator or VCS.&lt;/p&gt; &#xA;&lt;p&gt;&#34;Proxy-kernel&#34; binaries are meant to be run on a stripped down version of Linux, called the &lt;a href=&#34;https://github.com/riscv-software-src/riscv-pk&#34;&gt;&#34;RISC-V Proxy Kernel.&#34;&lt;/a&gt; These binaries support virtual memory, and are typically run on cycle-accurate simulators like Verilator.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; Proxy-kernel binaries have limited heap space, so some Gemmini programs that work correctly in baremetal or Linux environments may fail on the proxy-kernel.&lt;/p&gt; &#xA;&lt;h2&gt;Building Gemmini Hardware and Cycle-Accurate Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Run the instructions below to build a cycle-accurate Gemmini simulator using Verilator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini&#xA;./scripts/build-verilator.sh&#xA;&#xA;# Or, if you want a simulator that can generate waveforms, run this:&#xA;# ./scripts/build-verilator.sh --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running this, in addition to the cycle-accurate simulator, you will be able to find the Verilog description of your SoC in &lt;code&gt;generated-src/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building Gemmini Functional Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Run the instructions below to build a functional ISA simulator for Gemmini (called &#34;Spike&#34;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini&#xA;./scripts/build-spike.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spike typically runs &lt;em&gt;much&lt;/em&gt; faster than cycle-accurate simulators like Verilator or VCS. However, Spike can only verify functional correctness; it cannot give accurate performance metrics or profiling information.&lt;/p&gt; &#xA;&lt;h2&gt;Run Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Run the instructions below to run the Gemmini RISCV binaries that we built previously, using the simulators that we built above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini&#xA;&#xA;# Run a large DNN workload in the functional simulator&#xA;./scripts/run-spike.sh resnet50&#xA;&#xA;# Run a smaller workload in baremetal mode, on a cycle-accurate simulator&#xA;./scripts/run-verilator.sh template&#xA;&#xA;# Run a smaller workload with the proxy-kernel, on a cycle accurate simulator&#xA;./scripts/run-verilator.sh --pk template&#xA;&#xA;# Or, if you want to generate waveforms in `waveforms/`:&#xA;# ./scripts/run-verilator.sh --pk --debug template&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://sites.google.com/berkeley.edu/gemminitutorialiiswc2021/&#34;&gt;our IISWC 2021 tutorial&lt;/a&gt; to learn how to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;build different types of diverse accelerators using Gemmini.&lt;/li&gt; &#xA; &lt;li&gt;add custom datatypes to Gemmini.&lt;/li&gt; &#xA; &lt;li&gt;write your own Gemmini programs.&lt;/li&gt; &#xA; &lt;li&gt;profile your workloads using Gemmini&#39;s performance counters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, consider learning about &lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/fires.im&#34;&gt;FireSim&lt;/a&gt;, a platform for FPGA-accelerated cycle-accurate simulation. We use FireSim to run end-to-end DNN workloads that would take too long to run on Verilator/VCS. FireSim also allows users to check that their Gemmini hardware/software will work when running on a Linux environment.&lt;/p&gt; &#xA;&lt;p&gt;Or, continue reading the rest of this document for descriptions of Gemmini&#39;s architecture, ISA, and configuration parameters.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p&gt;Gemmini is implemented as a RoCC accelerator with non-standard RISC-V custom instructions. The Gemmini unit uses the RoCC port of a Rocket or BOOM &lt;em&gt;tile&lt;/em&gt;, and by default connects to the memory system through the System Bus (i.e., directly to the L2 cache).&lt;/p&gt; &#xA;&lt;p&gt;At the heart of the accelerator lies a systolic array which performs matrix multiplications. By default, the matrix multiplication support both &lt;em&gt;output-stationary&lt;/em&gt; and &lt;em&gt;weight-stationary&lt;/em&gt; dataflows, which programmers can pick between at runtime. However, the dataflow can also be hardened at elaboration time.&lt;/p&gt; &#xA;&lt;p&gt;The systolic array&#39;s inputs and outputs are stored in an explicity managed scratchpad, made up of banked SRAMs. A DMA engine facilitates the transfer of data between main memory (which is visible to the host CPU) and the scratchpad.&lt;/p&gt; &#xA;&lt;p&gt;Because weight-stationary dataflows require an accumulator outside the systolic array, we add a final SRAM bank, equipped with adder units, which can be conceptually considered an extension of the scratchpad memory space. The systolic array can store results to any address in the accumulator, and can also read new inputs from any address in the accumulator. The DMA engine can also tranfer data directly between the accumulator and main memory, which is often necessary to load in biases.&lt;/p&gt; &#xA;&lt;p&gt;Gemmini also includes peripheral circuitry to optionally apply activation functions such as ReLU or ReLU6, scale results down by powers-of-2 to support quantized workloads, or to transpose matrices before feeding them into the systolic array to support the output-stationary dataflow.&lt;/p&gt; &#xA;&lt;h2&gt;Generator Parameters&lt;/h2&gt; &#xA;&lt;p&gt;Major parameters of interest include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Systolic array dimensions (&lt;code&gt;tileRows&lt;/code&gt;, &lt;code&gt;tileColumns&lt;/code&gt;, &lt;code&gt;meshRows&lt;/code&gt;, &lt;code&gt;meshColumns&lt;/code&gt;): The systolic array is composed of a 2-level hierarchy, in which each tile is fully combinational, while a mesh of tiles has pipeline registers between each tile.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-systolic-array.png&#34; alt=&#34;Gemmini&#39;s systolic two-tiered hierarchy&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Dataflow parameters (&lt;code&gt;dataflow&lt;/code&gt;): Determine whether the systolic array in Gemmini is output-stationary or weight-stationary, or whether it supports both dataflows so that programmers may choose between them at runtime.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Scratchpad and accumulator memory parameters (&lt;code&gt;sp_banks&lt;/code&gt;, &lt;code&gt;sp_capacity&lt;/code&gt;, &lt;code&gt;acc_capacity&lt;/code&gt;): Determine the properties of the Gemmini scratchpad memory: overall capacity of the scratchpad or accumulators (in KiB), and the number of banks the scratchpad is divided into.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Type parameters (&lt;code&gt;inputType&lt;/code&gt;, &lt;code&gt;outputType&lt;/code&gt;, &lt;code&gt;accType&lt;/code&gt;): Determine the data-types flowing through different parts of a Gemmini accelerator. For example, &lt;code&gt;inputType&lt;/code&gt; may be an 8-bit fixed-point number, while &lt;code&gt;accType&lt;/code&gt;, which determines the type of partial accumulations in a matrix multiplication, may be a 32-bit integer. &lt;code&gt;outputType&lt;/code&gt; only determines the type of the data passed between two processing elements (PEs); for example, an 8-bit multiplication may produce a 16-bit result which must be shared between PEs in a systolic array.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Examples of possible datatypes are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;SInt(8.W)&lt;/code&gt; for a signed 8-bit integer&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;UInt(32.W)&lt;/code&gt; for an unsigned 32-bit integer&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Float(8, 24)&lt;/code&gt; for a single-precision IEEE floating point number&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If your datatype is a floating-point number, then you might also want to change the &lt;code&gt;pe_latency&lt;/code&gt; parameter, which specifies how many shift registers to add inside the PEs. This might be necessary if your datatype cannot complete a multiply-accumulate operation within a single cycle.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Access-execute queue parameters (&lt;code&gt;ld_queue_length&lt;/code&gt;, &lt;code&gt;st_queue_length&lt;/code&gt;, &lt;code&gt;ex_queue_length&lt;/code&gt;, &lt;code&gt;rob_entries&lt;/code&gt;): To implement access-execute decoupling, a Gemmini accelerator has a load instruction queue, a store instruction queue, and an execute instruction queue. The relative sizes of these queue determine the level of access-execute decoupling. Gemmini also implements a reorder buffer (ROB) - the number of entries in the ROB determines possible dependency management limitations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;DMA parameters (&lt;code&gt;dma_maxbytes&lt;/code&gt;, &lt;code&gt;dma_buswidth&lt;/code&gt;, &lt;code&gt;mem_pipeline&lt;/code&gt;): Gemmini implements a DMA to move data from main memory to the Gemmini scratchpad, and from the Gemmini accumulators to main memory. The size of these DMA transactions is determined by the DMA parameters. These DMA parameters are tightly coupled with Rocket Chip SoC system parameters: in particular &lt;code&gt;dma_buswidth&lt;/code&gt; is associated with the &lt;code&gt;SystemBusKey&lt;/code&gt; &lt;code&gt;beatBytes&lt;/code&gt; parameter, and &lt;code&gt;dma_maxbytes&lt;/code&gt; is associated with &lt;code&gt;cacheblockbytes&lt;/code&gt; Rocket Chip parameters.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are also optional features, which can be either enabled or left out of Gemmini at elaboration-time. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scaling during &#34;move-in&#34; operations (&lt;code&gt;mvin_scale_args&lt;/code&gt;, &lt;code&gt;mvin_scale_acc_args&lt;/code&gt;): When data is being moved in from DRAM or main memory into Gemmini&#39;s local scratchpad memory, it can optionally be multiplied by a scaling factor. These parameters specify what the datatype of the scaling factor is, and how the scaling is actually done. If these are set to &lt;code&gt;None&lt;/code&gt;, then this optional feature will be disabled at elaboration time. If both the scratchpad inputs are accumulator inputs are to be scaled in the same say, then the &lt;code&gt;mvin_scale_shared&lt;/code&gt; parameter can be set to &lt;code&gt;true&lt;/code&gt; so that the multipliers and functional units are shared.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Major Components&lt;/h2&gt; &#xA;&lt;p&gt;This subsection is aimed towards those who wish to start hacking on Gemmini&#39;s RTL. Here, we briefly describe Gemmini&#39;s main hardware components, and how they fit together. If you have no interest in changing Gemmini&#39;s hardware (besides just changing configuration parameters), then feel free to skip this section.&lt;/p&gt; &#xA;&lt;h3&gt;Decoupled Access/Execute&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini is a decoupled access/execute architecture, which means that &#34;memory-access&#34; and &#34;execute&#34; instructions happen concurrently, in different regions of the hardware. We divide the hardware broadly into three &#34;controllers&#34;: one for &#34;execute&#34; instructions, another for &#34;load&#34; instructions, and a third for &#34;store&#34; instructions. Each of these controllers consume direct ISA commands from the programmer, decode this commands, and execute them, while sharing access to the scratchpad and acccumulator SRAMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ExecuteController&lt;/code&gt;: This module is responsible for executing &#34;execute&#34;-type ISA commands, such as matrix multiplications. It includes a systolic array for dot-products, and a transposer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LoadController&lt;/code&gt;: This module is responsible for all instructions that move data from main memory into Gemmini&#39;s private scratchpad or accumulator.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;StoreController&lt;/code&gt;: This module is responsible for all instructions that move data from Gemmini&#39;s private SRAMs into main memory. This module is also responsible for &#34;max-pooling&#34; instructions, because Gemmini performs pooling when moving unpooled data from the private SRAMs into main memory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Scratchpad and Accumulator&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini stores inputs and outputs for the systolic array in a set of private SRAMs, which we call the &#34;scratchpad&#34; and the &#34;accumulator&#34;. Typically, inputs are stored in the scratchpad, while partial sums and final results are stored in the the accumulator.&lt;/p&gt; &#xA;&lt;p&gt;The scratchpad and accumulator are both instantiated within &lt;code&gt;Scratchpad.scala&lt;/code&gt;. The scratchpad banks are implemented by the &lt;code&gt;ScratchpadBank&lt;/code&gt; module, and the accumulator banks are implemented by the &lt;code&gt;AccumulatorMem&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;Each row of the scratchpad and accumulator SRAMs is &lt;code&gt;DIM&lt;/code&gt; &#34;elements&#34; wide, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs along the width of the systolic array. Each &#34;element&#34; represents a single scalar value that Gemmini operates upon.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;element&#34; in the scratchpad is of type &lt;code&gt;inputType&lt;/code&gt; (which, in the default config, is an 8-bit integer). Each &#34;element&#34; in the acccumulator is of type &lt;code&gt;accType&lt;/code&gt; (which, in the default config, is a 32-bit integer).&lt;/p&gt; &#xA;&lt;p&gt;So, for example, in the default config, which has a 16x16 systolic array, the scratchpad banks have a row-width of &lt;code&gt;16*bits(inputType)=128&lt;/code&gt; bits, and the accumulatorr banks have a row-width of &lt;code&gt;16*bits(accType)=512&lt;/code&gt; bits.&lt;/p&gt; &#xA;&lt;p&gt;Both inputs and outputs to the scratchpad must be of type &lt;code&gt;inputType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Both inputs and outputs from the accumulator can be either of type &lt;code&gt;accType&lt;/code&gt; &lt;em&gt;or&lt;/em&gt; &lt;code&gt;inputType&lt;/code&gt;. If &lt;code&gt;inputType&lt;/code&gt; values are input to the accumulator, they will be cast up to &lt;code&gt;accType&lt;/code&gt;. If &lt;code&gt;inputType&lt;/code&gt; values are output from the accumulator, they will first be &#34;scaled&#34; down to be of type &lt;code&gt;inputType&lt;/code&gt;. The exact &#34;scaling&#34; function can be configured as the as the user wishes, but in the default config, the scaling function is a simple multiplication by a &lt;code&gt;float32&lt;/code&gt; value that casts an &lt;code&gt;int32&lt;/code&gt; down to an &lt;code&gt;int8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The scratchpad banks are very simple, comprising little more than an SRAM and a queue.&lt;/p&gt; &#xA;&lt;p&gt;The accumulator banks are a bit more complex: in addition to the underlying SRAM, they also include a set of adders to support in-place accumulations. In addition, they have a set of &#34;scalers&#34; (described above), and activation function units. The scaling and activation functions are applied when the programmer wishes to transform &lt;code&gt;accType&lt;/code&gt; values down to &lt;code&gt;inputType&lt;/code&gt; values while reading data out of the accumulator. This is typically done to transform the partial-sum outputs of one layer into the low-bitwidth quantized inputs of the next layer.&lt;/p&gt; &#xA;&lt;h3&gt;Systolic Array and Transposer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;MeshWithDelays&lt;/code&gt;, which is instantiated within the &lt;code&gt;ExecuteController&lt;/code&gt;, contains the systolic array (&lt;code&gt;Mesh&lt;/code&gt;), a transposer (&lt;code&gt;Transposer&lt;/code&gt;), and a set of delay registers which shift the inputs to the systolic array. The &lt;code&gt;MeshWithDelays&lt;/code&gt; module takes in three matrices one row at a time per cycle (&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;), and outputs the result &lt;code&gt;C = A * B + D&lt;/code&gt; one row at a time per cycle.&lt;/p&gt; &#xA;&lt;p&gt;In the weight-stationary mode, the &lt;code&gt;B&lt;/code&gt; values are &#34;preloaded&#34; into the systolic array, and &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;D&lt;/code&gt; values are fed through. In the output-stationary mode, the &lt;code&gt;D&lt;/code&gt; values are &#34;preloaded&#34; into the systolic array, and &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; values are fed through.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt; are all of type &lt;code&gt;inputType&lt;/code&gt;, while &lt;code&gt;C&lt;/code&gt; is of type &lt;code&gt;outputType&lt;/code&gt;. If the programmer wishes to write &lt;code&gt;C&lt;/code&gt; into the scratchpad, then &lt;code&gt;C&lt;/code&gt; is cast down to &lt;code&gt;inputType&lt;/code&gt;. However, if the programmer instead wishes to write &lt;code&gt;C&lt;/code&gt; into the scratchpad, then &lt;code&gt;C&lt;/code&gt; is cast up to &lt;code&gt;accType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that in the weight-stationary mode, an &lt;code&gt;inputType&lt;/code&gt; D usually has insufficient bitwidth to accurately represent partial sums. Therefore, in the weight-stationary mode, &lt;code&gt;D&lt;/code&gt; is usually just the 0-matrix, while the &lt;code&gt;accType&lt;/code&gt; accumulator SRAMs are used to accumulate partial sum outputs of the systolic array instead.&lt;/p&gt; &#xA;&lt;p&gt;The inputs (&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;) must be delayed with shift-registers so that each input from one matrix reaches the correct PE at exactly the right time to be multiplied-and-accumulated with the correct input from another matrix. The diagram below shows an example of a 2x2 output-stationary matmul (ignoring &lt;code&gt;D&lt;/code&gt;), with the appropriate delay registers at the inputs and outputs of the systolic array:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/delay-registers.png&#34; alt=&#34;Systolic array with delay registers&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The systolic array itself (implemented in &lt;code&gt;Mesh.scala&lt;/code&gt;), is composed of a two-tier hierarchy of &lt;code&gt;Tiles&lt;/code&gt; and &lt;code&gt;PEs&lt;/code&gt;. The &lt;code&gt;Mesh&lt;/code&gt; is composed of a set of &lt;code&gt;Tiles&lt;/code&gt;, separated by pipeline registers. Every &lt;code&gt;Tile&lt;/code&gt; is composed of a combinational set of &lt;code&gt;PEs&lt;/code&gt;, where each PE performs a single matmul operation, with either the weight-stationary, or output-stationary dataflow.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-systolic-array.png&#34; alt=&#34;Systolic array&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;MeshWithDelays&lt;/code&gt; module also includes a number of counters and configuration registers. &lt;code&gt;MeshWithDelays&lt;/code&gt; assumes that every matmul operation will be exactly of size &lt;code&gt;DIM x DIM&lt;/code&gt;, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs across the width of the systolic array itself (16 in the default config). These counters count up to &lt;code&gt;DIM&lt;/code&gt;, and then update the configuration registers from the inputs to &lt;code&gt;MeshWithDelays&lt;/code&gt;. These configuration registers control which of &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are to be transposed before being fed into the systolic array. They also control whether the preloaded values in the systolic array are to be maintained for the next matmul, or whether they are to be overwritten and replaced.&lt;/p&gt; &#xA;&lt;p&gt;The transposer itself is implemented as a very simple systolic array, which transports inputs from left-to-right for &lt;code&gt;DIM&lt;/code&gt; cycles, and then down-to-up for another &lt;code&gt;DIM&lt;/code&gt; cycles. This is illustrated in the diagram below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/transposer.png&#34; alt=&#34;Transposer&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that for output-stationary matmuls, the transposer is used even when the programmer does not request a transposition. This is because the systolic array expects inputs from the same row of &lt;code&gt;A&lt;/code&gt; to enter the same PE in the output-stationary mode, but all values in a single row of &lt;code&gt;A&lt;/code&gt; are stored within the same scratchpad SRAM row. Therefore, the rows have to be transposed after being read out of the scratchpad, so that elements on the same row can be fed into the same PE one-after-another, rather than being fed into adjacent PEs.&lt;/p&gt; &#xA;&lt;h3&gt;DMA&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini includes two DMAs, one for reading data from main memory into Gemmini&#39;s private SRAMs, and another for moving data from Gemmini&#39;s private SRAMs into main memory. Both these modules are implemented in &lt;code&gt;DMA.scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Both DMAs operate on virtual addresses, and share access to a TLB to translate these into physical main memory addresses. If the TLB misses, it transparently falls back to a PTW that is shared with Gemmini&#39;s host CPU.&lt;/p&gt; &#xA;&lt;p&gt;After physical addresses are obtained from Gemmini&#39;s private TLB, the DMAs break large memory requests up into smaller &lt;a href=&#34;https://sifive.cdn.prismic.io/sifive%2Fcab05224-2df1-4af8-adee-8d9cba3378cd_tilelink-spec-1.8.0.pdf&#34;&gt;TileLink&lt;/a&gt; read and write requests. To satisfy the TileLink protocol, each memory request must be aligned to the number of bytes requested from/to main memory, and the size of each memory request (in bytes) must be a power of 2. The DMAs generally attempt to minimize the number of TileLink requests as much as possible, even if this requires reading a larger total amount of data from main memory. Empirically, we have found that an excessive number TileLink requests can limit performance more than reading a small amount of extra data.&lt;/p&gt; &#xA;&lt;p&gt;The DMAWriter, which writes data from private SRAMs to main memory, also includes a set of &lt;code&gt;&amp;gt;&lt;/code&gt; comparators that are used for max-pooling data during a memory-write operation.&lt;/p&gt; &#xA;&lt;h3&gt;ROB&lt;/h3&gt; &#xA;&lt;p&gt;Due to Gemmini&#39;s decoupled access-execute architecture, instructions in the &lt;code&gt;LoadController&lt;/code&gt;, &lt;code&gt;StoreController&lt;/code&gt;, and &lt;code&gt;ExecuteController&lt;/code&gt; may operate concurrently and out-of-order with respect to instructions in other controllers. Gemmini includes an ROB which is meant to detect hazards between instructions in different controllers. The instructions in the ROB are only issued to their respective controllers once they have no dependencies on instructions in other controllers.&lt;/p&gt; &#xA;&lt;p&gt;Note that instructions that are destined for the same controller are issued in-order. The ROB does not check hazards between instructions within the same controller, because each controller is obligated to handle it&#39;s own dependencies and hazards internally, assuming that it receives it&#39;s own instructions in program-order.&lt;/p&gt; &#xA;&lt;h3&gt;Matmul and Conv Loop Unrollers&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini&#39;s systolic array can only operate on matmuls that are up to &lt;code&gt;DIMxDIM&lt;/code&gt; elements large. When performing matmuls and convolutions that are larger than this, programmers must tile their matmuls into a sequence of smaller &lt;code&gt;DIMxDIM&lt;/code&gt; matmuls.&lt;/p&gt; &#xA;&lt;p&gt;However, tiling these operations efficiently can be difficult for programmers, due to CPU and loop overheads, and the difficulty of unrolling and pipelining software loops.&lt;/p&gt; &#xA;&lt;p&gt;To alleviate this difficulty, Gemmini&#39;s ISA includes high-level CISC-type instructions, which automatically tile and unroll large matmuls and convolutions. These are implemented in the &lt;code&gt;LoopMatmul&lt;/code&gt; and &lt;code&gt;LoopConv&lt;/code&gt; modules.&lt;/p&gt; &#xA;&lt;p&gt;These modules are implemented as FSMs, which double-buffer matmul/conv tiles to maximize performance, and which monitor the proportion of load/store/execute instructions in the ROB to maximize overlap between memory accesses and dot-product computations. For example, if the ROB is dominated by matmul instructions, without leaving any slots for incoming load instructions, then the FSMs will pause the issuing of matmul instructions to allow more space for concurrent load instructions in Gemmini&#39;s datapath.&lt;/p&gt; &#xA;&lt;h1&gt;Software&lt;/h1&gt; &#xA;&lt;p&gt;The Gemmini ISA is specified in the &lt;code&gt;ISA&lt;/code&gt; section below. The ISA includes configuration instructions, data movement instructions (from main memory to/from Gemmini&#39;s private memory), and matrix multiplication execution instructions.&lt;/p&gt; &#xA;&lt;p&gt;Since Gemmini instructions are not exposed through the GNU binutils assembler, several C macros are provided in order to construct the instruction encodings to call these instructions.&lt;/p&gt; &#xA;&lt;p&gt;The Gemmini generator includes a C library which wraps the calls to the custom Gemmini instructions into common DNN operators like matmuls, convolutions (with or without pooling), matrix-additions, etc. The &lt;code&gt;software&lt;/code&gt; directory of the generator includes the aforementioned library and macros, as well as baremetal tests, and some FireMarshal workloads to run the tests in a Linux environment. In particular, the C library can be found in the &lt;code&gt;software/gemmini-rocc-tests/include/gemmini.h&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;The Gemmini generator generates a C header file based on the generator parameters. This header files gets compiled together with the C library to tune library performance. The generated header file can be found under &lt;code&gt;software/gemmini-rocc-tests/include/gemmini_params.h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gemmini can also be used to run ONNX-specified neural-networks through a port of Microsoft&#39;s ONNX-Runtime framework. The port is included as the &lt;a href=&#34;https://github.com/pranav-prakash/onnxruntime-riscv&#34;&gt;onnxruntime-riscv&lt;/a&gt; repository submoduled in the &lt;code&gt;software&lt;/code&gt; directory. To start using ONNX-Runtime, run &lt;code&gt;git submodule update --init --recursive software/onnxruntime-riscv&lt;/code&gt;, and read the documentation &lt;a href=&#34;https://github.com/pranav-prakash/onnxruntime-riscv/raw/systolic/systolic_runner/docs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Run Gemmini Tests&lt;/h2&gt; &#xA;&lt;p&gt;To build the Gemmini tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd software/gemmini-rocc-tests/&#xA;./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, the test binaries will be found in &lt;code&gt;software/gemmini-rocc-tests/build&lt;/code&gt;. Binaries whose names end in &lt;code&gt;-baremetal&lt;/code&gt; are meant to be run in a bare-metal environment, while binaries whose names end in &lt;code&gt;-linux&lt;/code&gt; are meant to run in a Linux environment. You can run the tests either on a cycle-accurate RTL simulator, or on a (much faster) functional ISA simulator called Spike.&lt;/p&gt; &#xA;&lt;p&gt;We use a special fork of Spike, found &lt;a href=&#34;https://github.com/ucb-bar/esp-isa-sim&#34;&gt;here&lt;/a&gt;, which has support for Gemmini instructions. (You can find the required commit hash in &lt;code&gt;SPIKE.hash&lt;/code&gt;). If you are using Chipyard, you can easily build Spike by running &lt;code&gt;./scripts/build-toolchains.sh esp-tools&lt;/code&gt; from Chipyard&#39;s root directory. Then, to run the &lt;code&gt;mvin_mvout&lt;/code&gt; test, which simply moves a matrix into Gemmini&#39;s scratchpad before moving it back out into main memory, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd build/bareMetalC&#xA;spike --extension=gemmini mvin_mvout-baremetal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Writing Your Own Gemmini Tests&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;software/gemmini-rocc-tests/bareMetalC/template.c&lt;/code&gt; is a template Gemmini test that you can base your own Gemmini tests off of. To write your own Gemmini test, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd software/gemmini-rocc-tests/&#xA;cp bareMetalC/template.c bareMetalC/my_test.c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, add &lt;code&gt;my_test&lt;/code&gt; to the &lt;code&gt;tests&lt;/code&gt; list at the top of &lt;code&gt;bareMetalC/Makefile&lt;/code&gt;. Afterwards, running &lt;code&gt;./build.sh&lt;/code&gt; will install &lt;code&gt;my_test-baremetal&lt;/code&gt; in &lt;code&gt;build/bareMetalC&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;DNN Tests&lt;/h2&gt; &#xA;&lt;p&gt;Example DNNs, such as ResNet50, can be found in &lt;code&gt;software/gemmini-rocc-tests/imagenet&lt;/code&gt; and &lt;code&gt;software/gemmini-rocc-tests/mlps&lt;/code&gt;. These tests are built and run the same way as the other tests described above, but they typically take too long to run in a software simulator like VCS or Verilator. We recommend instead that you run these tests through &lt;a href=&#34;https://fires.im/&#34;&gt;Firesim&lt;/a&gt;, an FPGA-accelerated simulation platform, which will reduce your runtime from days to minutes.&lt;/p&gt; &#xA;&lt;p&gt;Note that the DNN tests rely upon our C library of common DNN operators (found in &lt;code&gt;gemmini.h&lt;/code&gt;). They call very few direct Gemmini ISA instructions, and mostly call the wrappers around them found in the C library.&lt;/p&gt; &#xA;&lt;h1&gt;Memory Addressing Scheme&lt;/h1&gt; &#xA;&lt;p&gt;Gemmini&#39;s private memory is &#34;row-addressed&#34;, where each row is &lt;code&gt;DIM&lt;/code&gt; elements wide, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs across the width of the systolic array (16 in the default config). These elements will be of type &lt;code&gt;inputType&lt;/code&gt; in the scratchpad, and of type &lt;code&gt;accType&lt;/code&gt; in the accumulator.&lt;/p&gt; &#xA;&lt;p&gt;Every private Gemmini memory address is 32 bits long. The three most signficant bits are reserved, and have special meanings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bit 31 (the MSB) is 0 if we are addressing the scratchpad, and 1 if we are addressing the accumulator.&lt;/li&gt; &#xA; &lt;li&gt;Bit 30 is ignored if we are addressing the scratchpad, or if we are reading from the accumulator. If, instead, we are writing to the accumulator, then bit 30 is 0 if we want to overwrite the data at that address, and 1 if we want to accumulate on top of the data already at that address.&lt;/li&gt; &#xA; &lt;li&gt;Bit 29 is ignored if we are addressing the scratchpad, or if we are writing to the accumulator. If, instead, we are reading from the accumulator, then bit 29 is 0 if we want to read scaled-down &lt;code&gt;inputType&lt;/code&gt; data from the accumulator, and 1 if we want to read &lt;code&gt;accType&lt;/code&gt; data from the accumulator. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If bit 29 is 1 for an accumulator read address, then we do not apply activation functions or scaling to the output of the accumulator.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The memory addressing scheme for a Gemmini config with a 2x2 systolic array is illustrated below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/memory-addressing.png&#34; alt=&#34;Gemmini&#39;s memory addressing scheme&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gemmini accesses main memory addresses (which are also visible to the CPU) through their software-visible virtual addresses. Physical translation addresses are handled by Gemmini, transparently to the programmer.&lt;/p&gt; &#xA;&lt;h1&gt;ISA&lt;/h1&gt; &#xA;&lt;p&gt;This section describes Gemmini&#39;s assembly-level ISA which is made up of custom RISC-V instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Data Movement&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;mvin&lt;/code&gt; Move Data From Main Memory to Scratchpad&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;mvin rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = virtual DRAM address (byte addressed) to load into scratchpad&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad or accumulator address&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns to load in&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows to load in. Must be less than or equal to &lt;code&gt;DIM&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; Scratchpad[rs2] &amp;lt;= DRAM[Translate[rs1]]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loads a 2D matrix from main memory into Gemmini&#39;s private memory.&lt;/li&gt; &#xA; &lt;li&gt;Load is sequential from the rs1/rs2 base addresses.&lt;/li&gt; &#xA; &lt;li&gt;Main memory stride must be set by the &lt;code&gt;config_mvin&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;If the number of columns we load in are greater than &lt;code&gt;DIM&lt;/code&gt;, then multiple submatrices will be moved in. The private-memory stride between these submatrices is set by the &lt;code&gt;config_mvin&lt;/code&gt; command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The figure below illustrates how the &lt;code&gt;mvin&lt;/code&gt; command works:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/mvin.png&#34; alt=&#34;Gemmini&#39;s mvin command&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition, the figure below illustrates the special case where the number of columns moved-in is greater than &lt;code&gt;DIM&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/block-mvin.png&#34; alt=&#34;Gemmini&#39;s mvin command with many cols&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are actually &lt;strong&gt;three&lt;/strong&gt; &lt;code&gt;mvin&lt;/code&gt; instructions in Gemmini: &lt;code&gt;mvin&lt;/code&gt;, &lt;code&gt;mvin2&lt;/code&gt;, and &lt;code&gt;mvin3&lt;/code&gt;. &lt;code&gt;mvin2&lt;/code&gt; and &lt;code&gt;mvin3&lt;/code&gt; are completely identical to &lt;code&gt;mvin&lt;/code&gt;, except that they have their own independent set of configuration registers. When calling &lt;code&gt;config_mvin&lt;/code&gt; (described below), the programmer can choose which &lt;code&gt;mvin&lt;/code&gt; instruction they want to configure.&lt;/li&gt; &#xA; &lt;li&gt;The reason we have three &lt;code&gt;mvin&lt;/code&gt; instructions is so that the programmer can overlap loads for A, B, and D matrices (for a &lt;code&gt;A*B+D&lt;/code&gt; matmul), where A, B, and D may all have different main-memory-strides.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;mvout&lt;/code&gt; Move Data from Scratchpad to L2/DRAM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;mvout rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = virtual DRAM address (byte addressed) to write to from scratchpad&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns to store&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows to store&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; DRAM[Translate[rs1]] &amp;lt;= Scratchpad[rs2]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stores a 2D matrix from the scratchpad to main-memory&lt;/li&gt; &#xA; &lt;li&gt;Store is sequential from the rs1/rs2 base addresses. Stride must be set by the &lt;code&gt;config_mvout&lt;/code&gt; command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_ex&lt;/code&gt; configures the Execute pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_ex rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;00&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[2]&lt;/code&gt; determines if output (0) or weight (1) stationary&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[4:3]&lt;/code&gt; = activation function: either relu (1), relu6 (2), or no activation function (0)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[8]&lt;/code&gt; = should A be transposed?&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[9]&lt;/code&gt; = should B be transposed?&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:16]&lt;/code&gt; = the stride (in scratchpad addresses) by which the rows of A are fed into the systolic array. &#34;A&#34; in this context refers to the left-hand matrix A in the matmul represented by A * B = C. If this stride is 1, then we feed consecutive rows in the scratchpad, starting from the starting address of A, into the systolic array as the A matrix. If the stride is 2, then we feed every other row into the systolic array instead.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:32]&lt;/code&gt; = the scalar value by which we scale the &lt;code&gt;accType&lt;/code&gt; output of the accumulator down to &lt;code&gt;inputType&lt;/code&gt; values when reading from the accumulator. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In the default config, &lt;code&gt;rs1[63:32]&lt;/code&gt; is of type &lt;code&gt;float32&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = the number of bits by which the accumulated result of a matmul is right-shifted when leaving the systolic array &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This parameter is only relevant in output-stationary mode, when partial sums must be accumulated within the systolic array itself, and scaled-down when leaving the systolic array and being written into the scratchpad.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:32]&lt;/code&gt; = the number of bits by which 6 should be left-shifted before applying relu6 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This parameter is ignored if the relu6 activation function is not being used.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; mode &amp;lt;= rs1(2); shift &amp;lt;= rs2; A_stride &amp;lt;= rs1[31:16]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;As of now, certain combinations of transpose options cannot be performed unless the right dataflow is chosen. This limitation may be lifted in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataflow&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transpose A&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transpose B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Permitted?&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_mvin&lt;/code&gt; configures the Load pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_mvin rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;01&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[2]&lt;/code&gt; is 0 if &lt;code&gt;mvin&lt;/code&gt;s to the accumulator are of type &lt;code&gt;accType&lt;/code&gt;, and 1 if they are &lt;code&gt;inputType&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[4:3]&lt;/code&gt; is 0 if the stride is being set for &lt;code&gt;mvin&lt;/code&gt;, 1 if the stride is being set for &lt;code&gt;mvin2&lt;/code&gt;, and 2 if the stride is being set for &lt;code&gt;mvin3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:32]&lt;/code&gt; is the &#34;scale&#34; by which to multiply data as it&#39;s being moved in to the scratchpad. This is ignored if Gemmini isn&#39;t configured to have the ability to scale values during &lt;code&gt;mvin&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2&lt;/code&gt; = the stride in bytes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; stride &amp;lt;= rs2; scale &amp;lt;= rs1[63:32]&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_mvout&lt;/code&gt; configures the Store pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_mvout rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;10&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2&lt;/code&gt; = the stride in bytes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;During &lt;code&gt;mvout&lt;/code&gt; operations, Gemmini can also perform max-pooling. &lt;strong&gt;This is an experimental feature, and is subject to change.&lt;/strong&gt; This feature assumes that data is stored in the scratchpad or accumulator in NHWC format. The parameters controlling this feature are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[5:4]&lt;/code&gt; = max-pooling stride. If this is 0, then max-pooling is deactivated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[7:6]&lt;/code&gt; = max-pooling window size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[9:8]&lt;/code&gt; = upper zero-padding&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[11:10]&lt;/code&gt; = left zero-padding&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:24]&lt;/code&gt; = output dimension of image after pooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[39:32]&lt;/code&gt; = number of pooled rows to output&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:40]&lt;/code&gt; = number of pooled columns to output&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[55:48]&lt;/code&gt; = number of unpooled rows to pool&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:56]&lt;/code&gt; = number of unpooled columns to pool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; stride &amp;lt;= rs2; max-pooling parameters &amp;lt;= rs1&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;flush&lt;/code&gt; flushes the TLB&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;flush rs1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = If &lt;code&gt;rs1[0]&lt;/code&gt; is 1, then the current TLB request is skipped (if it has hit a page-fault and is waiting for an interrupt). Otherwise, the current TLB request is repeated.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This instruction executes &lt;em&gt;as soon as it is received&lt;/em&gt; without waiting for other instructions which may be queued up. It is the programmer&#39;s responsibility to insert fences if necessary.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Core Matmul Sequences&lt;/h2&gt; &#xA;&lt;p&gt;Every single matrix multiply operation is a combination of &lt;code&gt;matmul.preload&lt;/code&gt; and &lt;code&gt;matmul.compute&lt;/code&gt; (due to the length of a single instruction, it was split into two instructions). &lt;code&gt;matmul.preload&lt;/code&gt; should precede the &lt;code&gt;matmul.compute&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//// OS matmul example ////&#xA;// rs1 = InputD&#xA;// rs2 = OutputC&#xA;// rs3 = InputA&#xA;// rs4 = InputB&#xA;// matmul InputA InputB OutputC InputD&#xA;1. matmul.preload $rs1 $rs2&#xA;2. matmul.compute $rs3 $rs4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; Scratchpad[rs2] &amp;lt;= Scratchpad[rs3] * Scratchpad[rs4] + Scratchpad[rs1]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes on addressing:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For B or D, the address can be replaced with all high bits to input a 0 matrix instead.&lt;/li&gt; &#xA; &lt;li&gt;For A, the address can be replaced with all high bits to input a matrix with undefined garbage data instead.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preloading&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.preload rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:0]&lt;/code&gt; = local scratchpad address of D matrix (when output-stationary), or B matrix (when weight-stationary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:32]&lt;/code&gt; = number of columns of D/B matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:48]&lt;/code&gt; = number of rows of D/B matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address of C matrix. If this is set to all high bits, then C will not be written to the scratchpad or accumulator.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns of C matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows of C matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 6&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Commit Behavior:&lt;/strong&gt; This instruction commits on the cycle after the systolic array receives it. The systolic array remains idle until the subsequent OS/WS specific instructions are seen.&lt;/p&gt; &#xA;&lt;h3&gt;Computing&lt;/h3&gt; &#xA;&lt;h4&gt;Explicitly Preloaded&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.compute.preloaded rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:0]&lt;/code&gt; = local scratchpad address (systolic array single-axis addressed) of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:32]&lt;/code&gt; = number of columns of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:48]&lt;/code&gt; = number of rows of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address (systolic array single-axis addressed) of B matrix (when output-stationary), or D matrix (when weight-stationary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns of B/D matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows of B/D matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 4&lt;/li&gt; &#xA; &lt;li&gt;This instruction will compute on the value preloaded (D if output-stationary, or B if weight-stationary)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Re-use Previous Preloads&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.compute.accumulated rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 5&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; have the same encoding as the &lt;code&gt;matmul.compute.preloaded&lt;/code&gt; encoding&lt;/li&gt; &#xA; &lt;li&gt;If output-stationary, this instruction will compute on the previously computed result (C) in the systolic array, accumulating on top of it&lt;/li&gt; &#xA; &lt;li&gt;If weight-stationary, this instruction will compute on the previously preloaded weights (B) in the systolic array&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Loop Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Gemmini includes CISC-type instructions which can perform matmuls and convolutions on data that is much larger than &lt;code&gt;DIMxDIM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s nothing these CISC instructions do which a programmer couldn&#39;t do by tiling and looping through the other ISA instructions described above; however, these CISC instructions may achieve higher throughput than such tiled loops written by non-expert programmers. The CISC instructions should be considered performance enhancers; they do not give the accelerator any new functionality that it wouldn&#39;t have otherwise.&lt;/p&gt; &#xA;&lt;p&gt;The CISC instructions have too many operands to fit into a single RISC-V custom instruction. Therefore, they are implemented as a sequence of many RISC-V custom instructions which must be called consecutively by the programmer.&lt;/p&gt; &#xA;&lt;p&gt;These instructions can be found &lt;code&gt;software/gemmini-rocc-tests/include/gemmini.h&lt;/code&gt;, together with example usages. We list below their arguments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;These loop instructions are experimental and subject to change.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gemmini_loop_ws&lt;/code&gt; Matmul Loop (WS Dataflow)&lt;/h3&gt; &#xA;&lt;p&gt;This instruction calculates &lt;code&gt;A * B + D = C&lt;/code&gt;, but &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; can all be larger than &lt;code&gt;DIMxDIM&lt;/code&gt;. &lt;code&gt;A&lt;/code&gt;, and &lt;code&gt;B&lt;/code&gt; must be of type &lt;code&gt;inputType&lt;/code&gt;, but both &lt;code&gt;D&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; can be &lt;em&gt;either&lt;/em&gt; &lt;code&gt;inputType&lt;/code&gt; or &lt;code&gt;accType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The sizes of these matrices are represented by &lt;code&gt;I&lt;/code&gt;, &lt;code&gt;J&lt;/code&gt;, and &lt;code&gt;K&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scratchpad rows of A = I * K * DIM&#xA;scratchpad rows of B = K * J * DIM&#xA;accumulator rows of D = I * J * DIM&#xA;accumulator rows of C = I * J * DIM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, the total number of scratchpad rows taken up by a single &lt;code&gt;gemmini_loop_ws&lt;/code&gt; must be at most &lt;strong&gt;half&lt;/strong&gt; of the total scratchpad size, because Gemmini performs double-buffering during CISC instructions. To compute larger matrix multiplies, the loop instructions must also be tiled within an outer loop.&lt;/p&gt; &#xA;&lt;p&gt;To support outer-tiling of the &lt;code&gt;gemmini_loop_ws&lt;/code&gt; instruction, we include an argument called &lt;code&gt;ex_accumulate&lt;/code&gt;, which determines whether to perform a matmul on top of the partial sums that already exist within the accumulator (from previous calls to &lt;code&gt;gemmini_loop_ws&lt;/code&gt; within the same outer-loop).&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; Conv Loop (WS Dataflow)&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini also includes a CISC instruction for convolutions, implemented similarly to the matmul CISC instruction. &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; will perform a convolution with the WS dataflow, and also supports features such as max-pooling, transpose convolutions, and various preprocessing transformations on the weight and input data.&lt;/p&gt; &#xA;&lt;p&gt;Like &lt;code&gt;gemmini_loop_ws&lt;/code&gt;, the inputs to a single &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; call must fit within half of Gemmini&#39;s private memory, to support double-buffering. If the programmer would like to perform larger convolutions, they must tile and wrap &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; within an outer-loop.&lt;/p&gt; &#xA;&lt;h1&gt;Citing Gemmini&lt;/h1&gt; &#xA;&lt;p&gt;If Gemmini helps you in your academic research, you are encouraged to cite our paper. Here is an example bibtex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@INPROCEEDINGS{gemmini-dac,&#xA;  author={Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iyer, Vighnesh and Prakash, Pranav and Zhao, Jerry and Grubb, Daniel and Liew, Harrison and Mao, Howard and Ou, Albert and Schmidt, Colin and Steffl, Samuel and Wright, John and Stoica, Ion and Ragan-Kelley, Jonathan and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},&#xA;  booktitle={Proceedings of the 58th Annual Design Automation Conference (DAC)}, &#xA;  title={Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration}, &#xA;  year={2021},&#xA;  volume={},&#xA;  number={},&#xA;  pages={}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project was, in part, funded by the U.S. Government under the DARPA RTML program (contract FA8650-20-2-7006). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.&lt;/li&gt; &#xA; &lt;li&gt;The Gemmini &lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/full-logo.svg&#34;&gt;logo&lt;/a&gt; was designed by Dima Nikiforov (&lt;a href=&#34;https://github.com/CobbledSteel&#34;&gt;@CobbledSteel&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mdedetrich/akka-apache</title>
    <updated>2022-10-16T01:47:12Z</updated>
    <id>tag:github.com,2022-10-16:/mdedetrich/akka-apache</id>
    <link href="https://github.com/mdedetrich/akka-apache" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build highly concurrent, distributed, and resilient message-driven applications on the JVM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Akka &lt;a href=&#34;https://index.scala-lang.org/akka/akka/akka-actor&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/akka/akka/akka-actor/latest.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://travis-ci.com/github/akka/akka&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/akka/akka.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;We believe that writing correct concurrent &amp;amp; distributed, resilient and elastic applications is too hard. Most of the time it&#39;s because we are using the wrong tools and the wrong level of abstraction.&lt;/p&gt; &#xA;&lt;p&gt;Akka is here to change that.&lt;/p&gt; &#xA;&lt;p&gt;Using the Actor Model we raise the abstraction level and provide a better platform to build correct concurrent and scalable applications. This model is a perfect match for the principles laid out in the &lt;a href=&#34;https://www.reactivemanifesto.org/&#34;&gt;Reactive Manifesto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For resilience, we adopt the &#34;Let it crash&#34; model which the telecom industry has used with great success to build applications that self-heal and systems that never stop.&lt;/p&gt; &#xA;&lt;p&gt;Actors also provide the abstraction for transparent distribution and the basis for truly scalable and fault-tolerant applications.&lt;/p&gt; &#xA;&lt;p&gt;Learn more at &lt;a href=&#34;https://akka.io/&#34;&gt;akka.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reference Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The reference documentation is available at &lt;a href=&#34;https://doc.akka.io&#34;&gt;doc.akka.io&lt;/a&gt;, for &lt;a href=&#34;https://doc.akka.io/docs/akka/current/scala.html&#34;&gt;Scala&lt;/a&gt; and &lt;a href=&#34;https://doc.akka.io/docs/akka/current/java.html&#34;&gt;Java&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;You can join these groups and chats to discuss and ask Akka related questions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: &lt;a href=&#34;https://discuss.akka.io&#34;&gt;discuss.akka.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat room about &lt;em&gt;using&lt;/em&gt; Akka: &lt;a href=&#34;https://gitter.im/akka/akka&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fakka-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Issue tracker: &lt;a href=&#34;https://github.com/akka/akka/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/github%3A-issues-blue.svg?style=flat-square&#34; alt=&#34;github: akka/akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to that, you may enjoy following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog/news-archive.html&#34;&gt;news&lt;/a&gt; section of the page, which is updated whenever a new version is released&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://akka.io/blog/article-archive.html&#34;&gt;Akka Team Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/akkateam&#34;&gt;@akkateam&lt;/a&gt; on Twitter&lt;/li&gt; &#xA; &lt;li&gt;Questions tagged &lt;a href=&#34;https://stackoverflow.com/questions/tagged/akka&#34;&gt;#akka on StackOverflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Projects built with Akka: &lt;a href=&#34;https://index.scala-lang.org/search?q=dependencies:akka/*&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/count.svg?q=dependencies:akka/*&amp;amp;subject=scaladex:&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;akka-dependency-badge&#34; title=&#34;Built with Akka&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributions are &lt;em&gt;very&lt;/em&gt; welcome!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you see an issue that you&#39;d like to see fixed, or want to shape out some ideas, the best way to make it happen is to help out by submitting a pull request implementing it. We welcome contributions from all, even you are not yet familiar with this project, We are happy to get you started, and will guide you through the process once you&#39;ve submitted your PR.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://index.scala-lang.org/search?q=dependencies:akka/*&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/count.svg?q=dependencies:akka/*&amp;amp;subject=scaladex:&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;akka-dependency-badge&#34; title=&#34;Built with Akka&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://github.com/akka/akka/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file for more details about the workflow, and general hints on how to prepare your pull request. You can also ask for clarifications or guidance in GitHub issues directly, or in the akka/dev chat if a more real time communication would be of benefit.&lt;/p&gt; &#xA;&lt;p&gt;A chat room is available for all questions related to &lt;em&gt;developing and contributing&lt;/em&gt; to Akka: &lt;a href=&#34;https://gitter.im/akka/dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter%3A-akka%2Fdev-blue.svg?style=flat-square&#34; alt=&#34;gitter: akka/dev&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Akka is Open Source and available under the Apache 2 License.&lt;/p&gt;</summary>
  </entry>
</feed>