<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-01T01:44:47Z</updated>
  <subtitle>Weekly Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/openai-realtime-agents</title>
    <updated>2025-06-01T01:44:47Z</updated>
    <id>tag:github.com,2025-06-01:/openai/openai-realtime-agents</id>
    <link href="https://github.com/openai/openai-realtime-agents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is a simple demonstration of more advanced, agentic patterns built on top of the Realtime API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Realtime API Agents Demo&lt;/h1&gt; &#xA;&lt;p&gt;This is a demonstration of more advanced patterns for voice agents, using the OpenAI Realtime API. There are two main patterns demonstrated:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat-Supervisor:&lt;/strong&gt; A realtime-based chat agent interacts with the user and handles basic tasks, while a more intelligent, text-based supervisor model (e.g., &lt;code&gt;gpt-4.1&lt;/code&gt;) is used extensively for tool calls and more complex responses. This approach provides an easy onramp and high-quality answers, with a small increase in latency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequential Handoff:&lt;/strong&gt; Specialized agents (powered by realtime api) transfer the user between them to handle specific user intents. This is great for customer service, where user intents can be handled sequentially by specialist models that excel in a specific domains. This helps avoid the model having all instructions and tools in a single agent, which can degrade performance.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is a Next.js typescript app. Install dependencies with &lt;code&gt;npm i&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your env. Either add it to your &lt;code&gt;.bash_profile&lt;/code&gt; or equivalent, or copy &lt;code&gt;.env.sample&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and add it there.&lt;/li&gt; &#xA; &lt;li&gt;Start the server with &lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open your browser to &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;. It should default to the &lt;code&gt;chatSupervisor&lt;/code&gt; Agent Config.&lt;/li&gt; &#xA; &lt;li&gt;You can change examples via the &#34;Scenario&#34; dropdown in the top right.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Agentic Pattern 1: Chat-Supervisor&lt;/h1&gt; &#xA;&lt;p&gt;This is demonstrated in the &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/agentConfigs/chatSupervisor/index.ts&#34;&gt;chatSupervisor&lt;/a&gt; Agent Config. The chat agent uses the realtime model to converse with the user and handle basic tasks, like greeting the user, casual conversation, and collecting information, and a more intelligent, text-based supervisor model (e.g. &lt;code&gt;gpt-4.1&lt;/code&gt;) is used extensively to handle tool calls and more challenging responses. You can control the decision boundary by &#34;opting in&#34; specific tasks to the chat agent as desired.&lt;/p&gt; &#xA;&lt;p&gt;Video walkthrough: &lt;a href=&#34;https://x.com/noahmacca/status/1927014156152058075&#34;&gt;https://x.com/noahmacca/status/1927014156152058075&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/public/screenshot_chat_supervisor.png&#34; alt=&#34;Screenshot of the Chat Supervisor Flow&#34;&gt; &lt;em&gt;In this exchange, note the immediate response to collect the phone number, and the deferral to the supervisor agent to handle the tool call and formulate the response. There ~2s between the end of &#34;give me a moment to check on that.&#34; being spoken aloud and the start of the &#34;Thanks for waiting. Your last bill...&#34;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Schematic&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram&#xA;    participant User&#xA;    participant ChatAgent as Chat Agent&amp;lt;br/&amp;gt;(gpt-4o-realtime-mini)&#xA;    participant Supervisor as Supervisor Agent&amp;lt;br/&amp;gt;(gpt-4.1)&#xA;    participant Tool as Tool&#xA;&#xA;    alt Basic chat or info collection&#xA;        User-&amp;gt;&amp;gt;ChatAgent: User message&#xA;        ChatAgent-&amp;gt;&amp;gt;User: Responds directly&#xA;    else Requires higher intelligence and/or tool call&#xA;        User-&amp;gt;&amp;gt;ChatAgent: User message&#xA;        ChatAgent-&amp;gt;&amp;gt;User: &#34;Let me think&#34;&#xA;        ChatAgent-&amp;gt;&amp;gt;Supervisor: Forwards message/context&#xA;        alt Tool call needed&#xA;            Supervisor-&amp;gt;&amp;gt;Tool: Calls tool&#xA;            Tool-&amp;gt;&amp;gt;Supervisor: Returns result&#xA;        end&#xA;        Supervisor-&amp;gt;&amp;gt;ChatAgent: Returns response&#xA;        ChatAgent-&amp;gt;&amp;gt;User: Delivers response&#xA;    end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benefits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simpler onboarding.&lt;/strong&gt; If you already have a performant text-based chat agent, you can give that same prompt and set of tools to the supervisor agent, and make some tweaks to the chat agent prompt, you&#39;ll have a natural voice agent that will perform on par with your text agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple ramp to a full realtime agent&lt;/strong&gt;: Rather than switching your whole agent to the realtime api, you can move one task at a time, taking time to validate and build trust for each before deploying to production.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High intelligence&lt;/strong&gt;: You benefit from the high intelligence, excellent tool calling and instruction following of models like &lt;code&gt;gpt-4.1&lt;/code&gt; in your voice agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lower cost&lt;/strong&gt;: If your chat agent is only being used for basic tasks, you can use the realtime-mini model, which, even when combined with GPT-4.1, should be cheaper than using the full 4o-realtime model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User experience&lt;/strong&gt;: It&#39;s a more natural conversational experience than using a stitched model architecture, where response latency is often 1.5s or longer after a user has finished speaking. In this architecture, the model responds to the user right away, even if it has to lean on the supervisor agent. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;However, more assistant responses will start with &#34;Let me think&#34;, rather than responding immediately with the full response.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Modifying for your own agent&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Update &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/agentConfigs/chatSupervisorDemo/supervisorAgent.ts&#34;&gt;supervisorAgent&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your existing text agent prompt and tools if you already have them. This should contain the &#34;meat&#34; of your voice agent logic and be very specific with what it should/shouldn&#39;t do and how exactly it should respond. Add this information below &lt;code&gt;==== Domain-Specific Agent Instructions ====&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You should likely update this prompt to be more appropriate for voice, for example with instructions to be concise and avoiding long lists of items.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Update &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/agentConfigs/chatSupervisor/index.ts&#34;&gt;chatAgent&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Customize the chatAgent instructions with your own tone, greeting, etc.&lt;/li&gt; &#xA; &lt;li&gt;Add your tool definitions to &lt;code&gt;chatAgentInstructions&lt;/code&gt;. We recommend a brief yaml description rather than json to ensure the model doesn&#39;t get confused and try calling the tool directly.&lt;/li&gt; &#xA; &lt;li&gt;You can modify the decision boundary by adding new items to the &lt;code&gt;# Allow List of Permitted Actions&lt;/code&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To reduce cost, try using &lt;code&gt;gpt-4o-mini-realtime&lt;/code&gt; for the chatAgent and/or &lt;code&gt;gpt-4.1-mini&lt;/code&gt; for the supervisor model. To maximize intelligence on particularly difficult or high-stakes tasks, consider trading off latency and adding chain-of-thought to your supervisor prompt, or using an additional reasoning model-based supervisor that uses &lt;code&gt;o4-mini&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Agentic Pattern 2: Sequential Handoffs&lt;/h1&gt; &#xA;&lt;p&gt;This pattern is inspired by &lt;a href=&#34;https://github.com/openai/swarm&#34;&gt;OpenAI Swarm&lt;/a&gt; and involves the sequential handoff of a user between specialized agents. Handoffs are decided by the model and coordinated via tool calls, and possible handoffs are defined explicitly in an agent graph. A handoff triggers a session.update event with new instructions and tools. This pattern is effective for handling a variety of user intents with specialist agents, each of which might have long instructions and numerous tools.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a &lt;a href=&#34;https://x.com/OpenAIDevs/status/1880306081517432936&#34;&gt;video walkthrough&lt;/a&gt; showing how it works. You should be able to use this repo to prototype your own multi-agent realtime voice app in less than 20 minutes!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/public/screenshot_handoff.png&#34; alt=&#34;Screenshot of the Realtime API Agents Demo&#34;&gt; &lt;em&gt;In this simple example, the user is transferred from a greeter agent to a haiku agent. See below for the simple, full configuration of this flow.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Configuration in &lt;code&gt;src/app/agentConfigs/simpleExample.ts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import { AgentConfig } from &#34;@/app/types&#34;;&#xA;import { injectTransferTools } from &#34;./utils&#34;;&#xA;&#xA;// Define agents&#xA;const haikuWriter: AgentConfig = {&#xA;  name: &#34;haikuWriter&#34;,&#xA;  publicDescription: &#34;Agent that writes haikus.&#34;, // Context for the agent_transfer tool&#xA;  instructions:&#xA;    &#34;Ask the user for a topic, then reply with a haiku about that topic.&#34;,&#xA;  tools: [],&#xA;};&#xA;&#xA;const greeter: AgentConfig = {&#xA;  name: &#34;greeter&#34;,&#xA;  publicDescription: &#34;Agent that greets the user.&#34;,&#xA;  instructions:&#xA;    &#34;Please greet the user and ask them if they&#39;d like a Haiku. If yes, transfer them to the &#39;haiku&#39; agent.&#34;,&#xA;  tools: [],&#xA;  downstreamAgents: [haikuWriter],&#xA;};&#xA;&#xA;// add the transfer tool to point to downstreamAgents&#xA;const agents = injectTransferTools([greeter, haikuWriter]);&#xA;&#xA;export default agents;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CustomerServiceRetail Flow&lt;/h2&gt; &#xA;&lt;p&gt;This is a more complex, representative implementation that illustrates a customer service flow, with the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A more complex agent graph with agents for user authentication, returns, sales, and a placeholder human agent for escalations.&lt;/li&gt; &#xA; &lt;li&gt;An escalation by the &lt;a href=&#34;https://github.com/openai/openai-realtime-agents/raw/60f4effc50a539b19b2f1fa4c38846086b58c295/src/app/agentConfigs/customerServiceRetail/returns.ts#L233&#34;&gt;returns&lt;/a&gt; agent to &lt;code&gt;o4-mini&lt;/code&gt; to validate and initiate a return, as an example high-stakes decision, using a similar pattern to the above.&lt;/li&gt; &#xA; &lt;li&gt;Prompting models to follow a state machine, for example to accurately collect things like names and phone numbers with confirmation character by character to authenticate a user. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To test this flow, say that you&#39;d like to return your snowboard and go through the necessary prompts!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Configuration in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/agentConfigs/customerServiceRetail/index.ts&#34;&gt;src/app/agentConfigs/customerServiceRetail/index.ts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import authentication from &#34;./authentication&#34;;&#xA;import returns from &#34;./returns&#34;;&#xA;import sales from &#34;./sales&#34;;&#xA;import simulatedHuman from &#34;./simulatedHuman&#34;;&#xA;import { injectTransferTools } from &#34;../utils&#34;;&#xA;&#xA;authentication.downstreamAgents = [returns, sales, simulatedHuman];&#xA;returns.downstreamAgents = [authentication, sales, simulatedHuman];&#xA;sales.downstreamAgents = [authentication, returns, simulatedHuman];&#xA;simulatedHuman.downstreamAgents = [authentication, returns, sales];&#xA;&#xA;const agents = injectTransferTools([&#xA;  authentication,&#xA;  returns,&#xA;  sales,&#xA;  simulatedHuman,&#xA;]);&#xA;&#xA;export default agents;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Schematic&lt;/h2&gt; &#xA;&lt;p&gt;This diagram illustrates a more advanced interaction flow defined in &lt;code&gt;src/app/agentConfigs/customerServiceRetail/&lt;/code&gt;, including detailed events.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Show CustomerServiceRetail Flow Diagram&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram&#xA;    participant User&#xA;    participant WebClient as Next.js Client&#xA;    participant NextAPI as /api/session&#xA;    participant RealtimeAPI as OpenAI Realtime API&#xA;    participant AgentManager as Agents (authentication, returns, sales, simulatedHuman)&#xA;    participant o1mini as &#34;o4-mini&#34; (Escalation Model)&#xA;&#xA;    Note over WebClient: User navigates to ?agentConfig=customerServiceRetail&#xA;    User-&amp;gt;&amp;gt;WebClient: Open Page&#xA;    WebClient-&amp;gt;&amp;gt;NextAPI: GET /api/session&#xA;    NextAPI-&amp;gt;&amp;gt;RealtimeAPI: POST /v1/realtime/sessions&#xA;    RealtimeAPI-&amp;gt;&amp;gt;NextAPI: Returns ephemeral session&#xA;    NextAPI-&amp;gt;&amp;gt;WebClient: Returns ephemeral token (JSON)&#xA;&#xA;    Note right of WebClient: Start RTC handshake&#xA;    WebClient-&amp;gt;&amp;gt;RealtimeAPI: Offer SDP (WebRTC)&#xA;    RealtimeAPI-&amp;gt;&amp;gt;WebClient: SDP answer&#xA;    WebClient-&amp;gt;&amp;gt;WebClient: DataChannel &#34;oai-events&#34; established&#xA;&#xA;    Note over AgentManager: Default agent is &#34;authentication&#34;&#xA;    User-&amp;gt;&amp;gt;WebClient: &#34;Hi, I&#39;d like to return my snowboard.&#34;&#xA;    WebClient-&amp;gt;&amp;gt;AgentManager: conversation.item.create (role=user)&#xA;    WebClient-&amp;gt;&amp;gt;RealtimeAPI: {type: &#34;conversation.item.create&#34;}&#xA;    WebClient-&amp;gt;&amp;gt;RealtimeAPI: {type: &#34;response.create&#34;}&#xA;&#xA;    authentication-&amp;gt;&amp;gt;AgentManager: Requests user info, calls authenticate_user_information()&#xA;    AgentManager--&amp;gt;&amp;gt;WebClient: function_call =&amp;gt; name=&#34;authenticate_user_information&#34;&#xA;    WebClient-&amp;gt;&amp;gt;WebClient: handleFunctionCall =&amp;gt; verifies details&#xA;&#xA;    Note over AgentManager: After user is authenticated&#xA;    authentication-&amp;gt;&amp;gt;AgentManager: transferAgents(&#34;returns&#34;)&#xA;    AgentManager--&amp;gt;&amp;gt;WebClient: function_call =&amp;gt; name=&#34;transferAgents&#34; args={ destination: &#34;returns&#34; }&#xA;    WebClient-&amp;gt;&amp;gt;WebClient: setSelectedAgentName(&#34;returns&#34;)&#xA;&#xA;    Note over returns: The user wants to process a return&#xA;    returns-&amp;gt;&amp;gt;AgentManager: function_call =&amp;gt; checkEligibilityAndPossiblyInitiateReturn&#xA;    AgentManager--&amp;gt;&amp;gt;WebClient: function_call =&amp;gt; name=&#34;checkEligibilityAndPossiblyInitiateReturn&#34;&#xA;&#xA;    Note over WebClient: The WebClient calls /api/chat/completions with model=&#34;o4-mini&#34;&#xA;    WebClient-&amp;gt;&amp;gt;o1mini: &#34;Is this item eligible for return?&#34;&#xA;    o1mini-&amp;gt;&amp;gt;WebClient: &#34;Yes/No (plus notes)&#34;&#xA;&#xA;    Note right of returns: Returns uses the result from &#34;o4-mini&#34;&#xA;    returns-&amp;gt;&amp;gt;AgentManager: &#34;Return is approved&#34; or &#34;Return is denied&#34;&#xA;    AgentManager-&amp;gt;&amp;gt;WebClient: conversation.item.create (assistant role)&#xA;    WebClient-&amp;gt;&amp;gt;User: Displays final verdict&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Other Info&lt;/h1&gt; &#xA;&lt;h2&gt;Next Steps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can copy these templates to make your own multi-agent voice app! Once you make a new agent set config, add it to &lt;code&gt;src/app/agentConfigs/index.ts&lt;/code&gt; and you should be able to select it in the UI in the &#34;Scenario&#34; dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Each agentConfig can define instructions, tools, and toolLogic. By default all tool calls simply return &lt;code&gt;True&lt;/code&gt;, unless you define the toolLogic, which will run your specific tool logic and return an object to the conversation (e.g. for retrieved RAG context).&lt;/li&gt; &#xA; &lt;li&gt;If you want help creating your own prompt using the conventions shown in customerServiceRetail, including defining a state machine, we&#39;ve included a metaprompt &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/agentConfigs/voiceAgentMetaprompt.txt&#34;&gt;here&lt;/a&gt;, or you can use our &lt;a href=&#34;https://chatgpt.com/g/g-678865c9fb5c81918fa28699735dd08e-voice-agent-metaprompt-gpt&#34;&gt;Voice Agent Metaprompter GPT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output Guardrails&lt;/h2&gt; &#xA;&lt;p&gt;Assistant messages are checked for safety and compliance using a guardrail function before being finalized in the transcript. This is implemented in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-realtime-agents/main/src/app/hooks/useHandleServerEvent.ts&#34;&gt;&lt;code&gt;src/app/hooks/useHandleServerEvent.ts&lt;/code&gt;&lt;/a&gt; as the &lt;code&gt;processGuardrail&lt;/code&gt; function, which is invoked on each assistant message (after every 5 incremental words received) to run a moderation/classification check. You can review or customize this logic by editing the &lt;code&gt;processGuardrail&lt;/code&gt; function definition and its invocation inside &lt;code&gt;useHandleServerEvent&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Navigating the UI&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can select agent scenarios in the Scenario dropdown, and automatically switch to a specific agent with the Agent dropdown.&lt;/li&gt; &#xA; &lt;li&gt;The conversation transcript is on the left, including tool calls, tool call responses, and agent changes. Click to expand non-message elements.&lt;/li&gt; &#xA; &lt;li&gt;The event log is on the right, showing both client and server events. Click to see the full payload.&lt;/li&gt; &#xA; &lt;li&gt;On the bottom, you can disconnect, toggle between automated voice-activity detection or PTT, turn off audio playback, and toggle logs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Requests&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to open an issue or pull request and we&#39;ll do our best to review it. The spirit of this repo is to demonstrate the core logic for new agentic flows; PRs that go beyond this core scope will likely not be merged.&lt;/p&gt; &#xA;&lt;h1&gt;Core Contributors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Noah MacCallum - &lt;a href=&#34;https://x.com/noahmacca&#34;&gt;noahmacca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ilan Bigio - &lt;a href=&#34;https://github.com/ibigio&#34;&gt;ibigio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>liriliri/aya</title>
    <updated>2025-06-01T01:44:47Z</updated>
    <id>tag:github.com,2025-06-01:/liriliri/aya</id>
    <link href="https://github.com/liriliri/aya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Android ADB desktop app&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://aya.liriliri.io/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://aya.liriliri.io/icon.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;AYA&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;Android ADB desktop app.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.producthunt.com/posts/aya-1?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-aya-1&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=899538&amp;amp;theme=light&amp;amp;t=1740125747753&#34; alt=&#34;AYA - Open source desktop app for controlling android devices | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/liriliri/aya/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Windows-blue?style=flat-square&amp;amp;logo=windows&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/liriliri/aya/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-macOS-black?style=flat-square&amp;amp;logo=apple&#34; alt=&#34;macOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/liriliri/aya/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linux-yellow?style=flat-square&amp;amp;logo=linux&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/liriliri/aya/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/liriliri/aya/total?style=flat-square&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/liriliri/aya?style=flat-square&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;img src=&#34;https://aya.liriliri.io/screencast.png&#34; style=&#34;width:100%&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aya.liriliri.io/&#34;&gt;AYA&lt;/a&gt; is a desktop application for easily controlling android devices, which can be considered as a GUI wrapper for ADB.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Click &lt;a href=&#34;https://github.com/liriliri/aya/releases/&#34;&gt;here&lt;/a&gt; to download and install AYA. Windows x64, Mac arm64, Mac x64 and Linux x86_64 are supported.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;img src=&#34;https://aya.liriliri.io/screenshot.png&#34; style=&#34;width:100%&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Screen mirror&lt;/li&gt; &#xA; &lt;li&gt;File explorer&lt;/li&gt; &#xA; &lt;li&gt;Application manager&lt;/li&gt; &#xA; &lt;li&gt;Process monitor&lt;/li&gt; &#xA; &lt;li&gt;Layout inspector&lt;/li&gt; &#xA; &lt;li&gt;CPU, memory and FPS monitor&lt;/li&gt; &#xA; &lt;li&gt;Logcat viewer&lt;/li&gt; &#xA; &lt;li&gt;Interactive shell&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more detailed usage instructions, please read the documentation at &lt;a href=&#34;https://aya.liriliri.io&#34;&gt;aya.liriliri.io&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liriliri/licia&#34;&gt;licia&lt;/a&gt;: Utility library used by AYA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liriliri/luna&#34;&gt;luna&lt;/a&gt;: UI components used by AYA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liriliri/vivy&#34;&gt;vivy&lt;/a&gt;: Icon image generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liriliri/echo&#34;&gt;echo&lt;/a&gt;: Harmony OS version of AYA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Read &lt;a href=&#34;https://aya.liriliri.io/guide/contributing.html&#34;&gt;Contributing Guide&lt;/a&gt; for development setup instructions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>heroui-inc/heroui</title>
    <updated>2025-06-01T01:44:47Z</updated>
    <id>tag:github.com,2025-06-01:/heroui-inc/heroui</id>
    <link href="https://github.com/heroui-inc/heroui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀 Beautiful, fast and modern React UI library. (Previously NextUI)&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://ph.heroui.chat?utm_source=https://github.com/heroui-inc/heroui&amp;amp;utm_medium=banner&#34;&gt; &lt;img alt=&#34;HeroUI Chat on Product Hunt&#34; src=&#34;https://heroui-chat-assets.nyc3.cdn.digitaloceanspaces.com/github_banner-round.png&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://heroui.com&#34;&gt; &lt;img width=&#34;20%&#34; src=&#34;https://raw.githubusercontent.com/heroui-inc/heroui/main/apps/docs/public/isotipo.png&#34; alt=&#34;heorui&#34;&gt; &lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;a href=&#34;https://heroui.com&#34;&gt;HeroUI&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;a href=&#34;https://heroui.com&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/heroui-inc/heroui/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/l/@heroui/react?style=flat&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/jrgarciadev/nextui&#34;&gt; &lt;img src=&#34;https://codecov.io/gh/jrgarciadev/nextui/branch/main/graph/badge.svg?token=QJF2QKR5N4&#34; alt=&#34;codecov badge&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://github.com/heroui-inc/heroui/actions/workflows/main.yaml&#34;&gt;&#xA;    &lt;img src=&#34;https://github.com/heroui-inc/heroui/actions/workflows/main.yaml/badge.svg&#34; alt=&#34;CI/CD heroui&#34;&gt;&#xA;  &lt;/a&gt; --&gt; &lt;a href=&#34;https://www.npmjs.com/package/@heroui/react&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dm/@heroui/react.svg?style=flat-round&#34; alt=&#34;npm downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a aria-label=&#34;heroui learn&#34; href=&#34;https://heroui.com/learn&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://heroui.com/guide&#34;&gt;https://heroui.com/guide&lt;/a&gt; to get started with HeroUI.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://heroui.com/docs&#34;&gt;https://heroui.com/docs&lt;/a&gt; to view the full documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Storybook&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://storybook.heroui.com/&#34;&gt;https://storybook.heroui.com&lt;/a&gt; to view the storybook for all components.&lt;/p&gt; &#xA;&lt;h2&gt;Canary Release&lt;/h2&gt; &#xA;&lt;p&gt;Canary versions are available after every merge into &lt;code&gt;canary&lt;/code&gt; branch. You can install the packages with the tag &lt;code&gt;canary&lt;/code&gt; in npm to use the latest changes before the next production release.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://canary.heroui.com/docs&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://canary-sb.heroui.com&#34;&gt;Storybook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re excited to see the community adopt HeroUI, raise issues, and provide feedback. Whether it&#39;s a feature request, bug report, or a project to showcase, please get involved!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/9b6yyZKmH4&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x.com/hero_ui&#34;&gt;X&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/heroui-inc/heroui/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/heroui-inc/heroui/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for ways to get started.&lt;/p&gt; &#xA;&lt;p&gt;Please adhere to this project&#39;s &lt;a href=&#34;https://github.com/heroui-inc/heroui/raw/main/CODE_OF_CONDUCT.md&#34;&gt;CODE_OF_CONDUCT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://choosealicense.com/licenses/mit/&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>