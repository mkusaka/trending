<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-06T01:48:28Z</updated>
  <subtitle>Weekly Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenCTI-Platform/opencti</title>
    <updated>2024-10-06T01:48:28Z</updated>
    <id>tag:github.com,2024-10-06:/OpenCTI-Platform/opencti</id>
    <link href="https://github.com/OpenCTI-Platform/opencti" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Cyber Threat Intelligence Platform&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://opencti.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/.github/img/logo_opencti.png&#34; alt=&#34;OpenCTI&#34;&gt;&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://opencti.io&#34; alt=&#34;Website&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/website-opencti.io-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.opencti.io&#34; alt=&#34;Documentation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/documentation-latest-orange.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://community.filigran.io&#34; alt=&#34;Slack&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-3K%2B%20members-4A154B&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drone.filigran.io/OpenCTI-Platform/opencti&#34;&gt;&lt;img src=&#34;https://drone.filigran.io/api/badges/OpenCTI-Platform/opencti/status.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/OpenCTI-Platform/opencti&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/OpenCTI-Platform/opencti/graph/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepscan.io/dashboard#view=project&amp;amp;tid=4926&amp;amp;pid=6716&amp;amp;bid=57311&#34;&gt;&lt;img src=&#34;https://deepscan.io/api/teams/4926/projects/6716/branches/57311/badge/grade.svg?sanitize=true&#34; alt=&#34;DeepScan grade&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://renovatebot.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/renovate-enabled-brightgreen.svg?sanitize=true&#34; alt=&#34;DeepScan grade&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/u/opencti&#34; alt=&#34;Docker pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/opencti/platform&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;OpenCTI is an open source platform allowing organizations to manage their cyber threat intelligence knowledge and observables. It has been created in order to structure, store, organize and visualize technical and non-technical information about cyber threats.&lt;/p&gt; &#xA;&lt;p&gt;The structuration of the data is performed using a knowledge schema based on the &lt;a href=&#34;https://oasis-open.github.io/cti-documentation/&#34;&gt;STIX2 standards&lt;/a&gt;. It has been designed as a modern web application including a &lt;a href=&#34;https://graphql.org&#34;&gt;GraphQL API&lt;/a&gt; and an UX oriented frontend. Also, OpenCTI can be integrated with other tools and applications such as &lt;a href=&#34;https://github.com/MISP/MISP&#34;&gt;MISP&lt;/a&gt;, &lt;a href=&#34;https://github.com/TheHive-Project/TheHive&#34;&gt;TheHive&lt;/a&gt;, &lt;a href=&#34;https://github.com/mitre/cti&#34;&gt;MITRE ATT&amp;amp;CK&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/.github/img/screenshot.png&#34; alt=&#34;Screenshot&#34; title=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Objective&lt;/h2&gt; &#xA;&lt;p&gt;The goal is to create a comprehensive tool allowing users to capitalize technical (such as TTPs and observables) and non-technical information (such as suggested attribution, victimology etc.) while linking each piece of information to its primary source (a report, a MISP event, etc.), with features such as links between each information, first and last seen dates, levels of confidence, etc. The tool is able to use the &lt;a href=&#34;https://attack.mitre.org&#34;&gt;MITRE ATT&amp;amp;CK framework&lt;/a&gt; (through a &lt;a href=&#34;https://github.com/OpenCTI-Platform/connectors&#34;&gt;dedicated connector&lt;/a&gt;) to help structure the data. The user can also choose to implement their own datasets.&lt;/p&gt; &#xA;&lt;p&gt;Once data has been capitalized and processed by the analysts within OpenCTI, new relations may be inferred from existing ones to facilitate the understanding and the representation of this information. This allows the user to extract and leverage meaningful knowledge from the raw data.&lt;/p&gt; &#xA;&lt;p&gt;OpenCTI not only allows &lt;a href=&#34;https://docs.opencti.io/latest/usage/import-automated/&#34;&gt;imports&lt;/a&gt; but also &lt;a href=&#34;https://docs.opencti.io/latest/usage/feeds/&#34;&gt;exports of data&lt;/a&gt; under different formats (CSV, STIX2 bundles, etc.). &lt;a href=&#34;https://filigran.notion.site/OpenCTI-Ecosystem-868329e9fb734fca89692b2ed6087e76&#34;&gt;Connectors&lt;/a&gt; are currently developed to accelerate interactions between the tool and other platforms.&lt;/p&gt; &#xA;&lt;h2&gt;Editions of the platform&lt;/h2&gt; &#xA;&lt;p&gt;OpenCTI platform has 2 different editions: Community (CE) and Enterprise (EE). The purpose of the Enterprise Edition is to provide &lt;a href=&#34;https://filigran.io/offering/subscribe&#34;&gt;additional and powerful features&lt;/a&gt; which require specific investments in research and development. You can enable the Enterprise Edition directly in the settings of the platform.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenCTI Community Edition, licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/LICENSE&#34;&gt;Apache 2, Version 2.0 license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;OpenCTI Enterprise Edition, licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/LICENSE&#34;&gt;Enterprise Edition license&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To understand what OpenCTI Enterprise Edition brings in terms of features, just check the &lt;a href=&#34;https://filigran.io/offering/subscribe&#34;&gt;Enterprise Editions page&lt;/a&gt; on the Filigran website. You can also try this edition by enabling it in the settings of the platform.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and demonstration&lt;/h2&gt; &#xA;&lt;p&gt;If you want to know more on OpenCTI, you can read the &lt;a href=&#34;https://docs.opencti.io&#34;&gt;documentation on the tool&lt;/a&gt;. If you wish to discover how the OpenCTI platform is working, a &lt;a href=&#34;https://demo.opencti.io&#34;&gt;demonstration instance&lt;/a&gt; is available and open to everyone. This instance is reset every night and is based on reference data maintained by the OpenCTI developers.&lt;/p&gt; &#xA;&lt;h2&gt;Releases download&lt;/h2&gt; &#xA;&lt;p&gt;The releases are available on the &lt;a href=&#34;https://github.com/OpenCTI-Platform/opencti/releases&#34;&gt;Github releases page&lt;/a&gt;. You can also access the &lt;a href=&#34;https://releases.opencti.io&#34;&gt;rolling release package&lt;/a&gt; generated from the master branch of the repository.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;All you need to install the OpenCTI platform can be found in the &lt;a href=&#34;https://docs.opencti.io&#34;&gt;official documentation&lt;/a&gt;. For installation, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.opencti.io/latest/deployment/installation/#using-docker&#34;&gt;Use Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.opencti.io/latest/deployment/installation/#install-manually&#34;&gt;Install manually&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.opencti.io/latest/deployment/installation/#terraform&#34;&gt;Use Terraform (community)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.opencti.io/latest/deployment/installation/#helm-charts&#34;&gt;Use Helm charts (community)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Code of Conduct&lt;/h3&gt; &#xA;&lt;p&gt;OpenCTI has adopted a &lt;a href=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; that we expect project participants to adhere to. Please read the &lt;a href=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CODE_OF_CONDUCT.md&#34;&gt;full text&lt;/a&gt; so that you can understand what actions will and will not be tolerated.&lt;/p&gt; &#xA;&lt;h3&gt;Contributing Guide&lt;/h3&gt; &#xA;&lt;p&gt;Read our &lt;a href=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to OpenCTI.&lt;/p&gt; &#xA;&lt;h3&gt;Beginner friendly issues&lt;/h3&gt; &#xA;&lt;p&gt;To help you get you familiar with our contribution process, we have a list of &lt;a href=&#34;https://github.com/OpenCTI-Platform/opencti/labels/beginner%20friendly%20issue&#34;&gt;beginner friendly issues&lt;/a&gt; which are fairly easy to implement. This is a great place to get started.&lt;/p&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;p&gt;If you want to actively help OpenCTI, we created a &lt;a href=&#34;https://docs.opencti.io/latest/development/environment_ubuntu/&#34;&gt;dedicated documentation&lt;/a&gt; about the deployment of a development environment and how to start the source code modification.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Status &amp;amp; bugs&lt;/h3&gt; &#xA;&lt;p&gt;Currently OpenCTI is under heavy development, if you wish to report bugs or ask for new features, you can directly use the &lt;a href=&#34;https://github.com/OpenCTI-Platform/opencti/issues&#34;&gt;Github issues module&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Discussion&lt;/h3&gt; &#xA;&lt;p&gt;If you need support or you wish to engage a discussion about the OpenCTI platform, feel free to join us on our &lt;a href=&#34;https://community.filigran.io&#34;&gt;Slack channel&lt;/a&gt;. You can also send us an email to &lt;a href=&#34;mailto:contact@filigran.io&#34;&gt;contact@filigran.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;p&gt;OpenCTI is a product designed and developed by the company &lt;a href=&#34;https://filigran.io&#34;&gt;Filigran&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://filigran.io&#34; alt=&#34;Filigran&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenCTI-Platform/opencti/master/.github/img/logo_filigran.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Data Collection&lt;/h3&gt; &#xA;&lt;h4&gt;Usage telemetry&lt;/h4&gt; &#xA;&lt;p&gt;To improve the features and the performances of OpenCTI, the platform collects anonymous statistical data related to its usage and health.&lt;/p&gt; &#xA;&lt;p&gt;You can find all the details on collected data and associated usage in the &lt;a href=&#34;https://docs.opencti.io/latest/reference/usage-telemetry/&#34;&gt;usage telemetry documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;OpenStreetMap server&lt;/h4&gt; &#xA;&lt;p&gt;To provide OpenCTI users with cartography features, the platform uses a dedicated OpenStreetMap server (&lt;a href=&#34;https://map.opencti.io&#34;&gt;https://map.opencti.io&lt;/a&gt;). To monitor usage and adapt services performances, Filigran collects access log to this server (including IP addresses).&lt;/p&gt; &#xA;&lt;p&gt;By using this server, you authorize Filigran to collect this information. Otherwise, you are free to deploy your own OpenStreetMap server and modify the platform configuration accordingly.&lt;/p&gt; &#xA;&lt;p&gt;If you have started using the Filigran server and change your mind, you have the right to access, limit, rectify, erase and receive your data. To exercise your rights, please send your request to &lt;a href=&#34;mailto:privacy@filigran.io&#34;&gt;privacy@filigran.io&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Yonom/assistant-ui</title>
    <updated>2024-10-06T01:48:28Z</updated>
    <id>tag:github.com,2024-10-06:/Yonom/assistant-ui</id>
    <link href="https://github.com/Yonom/assistant-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;React Components for AI Chat 💬 🚀&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://www.assistant-ui.com&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Yonom/assistant-ui/main/.github/assets/header.svg?sanitize=true&#34; alt=&#34;assistant-ui Header&#34; width=&#34;100%&#34; style=&#34;width: 1000px&#34;&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://assistant-ui.com&#34;&gt;Product&lt;/a&gt; · &lt;a href=&#34;https://assistant-ui.com/docs/getting-started&#34;&gt;Documentation&lt;/a&gt; · &lt;a href=&#34;https://assistant-ui.com/examples&#34;&gt;Examples&lt;/a&gt; · &lt;a href=&#34;https://discord.gg/S9dwgCNEFs&#34;&gt;Discord Community&lt;/a&gt; · &lt;a href=&#34;https://cal.com/simon-farshid/assistant-ui&#34;&gt;Contact Sales&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;assistant-ui&lt;/strong&gt; is a set of React components for AI chat, with integrations Langchain, Vercel AI SDK, TailwindCSS, shadcn-ui, react-markdown, react-syntax-highlighter, React Hook Form and more!&lt;/p&gt; &#xA;&lt;p&gt;Wide model provider support (OpenAI, Anthropic, Mistral, Perplexity, AWS Bedrock, Azure, Google Gemini, Hugging Face, Fireworks, Cohere, Replicate, Ollama) out of the box and the ability to integrate custom APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/k6Dc8URmLjk&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Yonom/assistant-ui/main/.github/assets/assistant-ui-starter.gif&#34; alt=&#34;assistant-ui starter template&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Create a new project with &lt;code&gt;assistant-ui&lt;/code&gt; pre-configured:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx assistant-ui@latest create my-app&#xA;cd my-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2: Update the &lt;code&gt;.env&lt;/code&gt; file with your OpenAI API key.&lt;/p&gt; &#xA;&lt;p&gt;Step 3: Run the app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mlc-ai/web-llm</title>
    <updated>2024-10-06T01:48:28Z</updated>
    <id>tag:github.com,2024-10-06:/mlc-ai/web-llm</id>
    <link href="https://github.com/mlc-ai/web-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-performance In-browser LLM Inference Engine&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;WebLLM&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/@mlc-ai/web-llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NPM_Package-Published-cc3534&#34; alt=&#34;NPM Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://chat.webllm.ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WebLLM_Chat-Deployed-%2332a852&#34; alt=&#34;&amp;quot;WebLLM Chat Deployed&amp;quot;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;%22https://discord.gg/9Xpy2HGBuD%22&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Join Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mlc-ai/web-llm-chat/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Related_Repo-WebLLM_Chat-fafbfc?logo=github&#34; alt=&#34;Related Repository: WebLLM Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Related_Repo-MLC_LLM-fafbfc?logo=github&#34; alt=&#34;Related Repository: MLC LLM&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;High-Performance In-Browser LLM Inference Engine.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#get-started&#34;&gt;Get Started&lt;/a&gt; | &lt;a href=&#34;https://blog.mlc.ai/2024/06/13/webllm-a-high-performance-in-browser-llm-inference-engine&#34;&gt;Blogpost&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples&#34;&gt;Examples&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/mlc-llm/docs/deploy/webllm.html&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM is a high-performance in-browser LLM inference engine that brings language model inference directly onto web browsers with hardware acceleration. Everything runs inside the browser with no server support and is accelerated with WebGPU.&lt;/p&gt; &#xA;&lt;p&gt;WebLLM is &lt;strong&gt;fully compatible with &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI API&lt;/a&gt;.&lt;/strong&gt; That is, you can use the same OpenAI API on &lt;strong&gt;any open source models&lt;/strong&gt; locally, with functionalities including streaming, JSON-mode, function-calling (WIP), etc.&lt;/p&gt; &#xA;&lt;p&gt;We can bring a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.&lt;/p&gt; &#xA;&lt;p&gt;You can use WebLLM as a base &lt;a href=&#34;https://www.npmjs.com/package/@mlc-ai/web-llm&#34;&gt;npm package&lt;/a&gt; and build your own web application on top of it by following the examples below. This project is a companion project of &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt;, which enables universal deployment of LLM across hardware environments.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://chat.webllm.ai/&#34;&gt;Check out WebLLM Chat to try it out!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;In-Browser Inference&lt;/strong&gt;: WebLLM is a high-performance, in-browser language model inference engine that leverages WebGPU for hardware acceleration, enabling powerful LLM operations directly within web browsers without server-side processing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#full-openai-compatibility&#34;&gt;&lt;strong&gt;Full OpenAI API Compatibility&lt;/strong&gt;&lt;/a&gt;: Seamlessly integrate your app with WebLLM using OpenAI API with functionalities such as streaming, JSON-mode, logit-level control, seeding, and more.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured JSON Generation&lt;/strong&gt;: WebLLM supports state-of-the-art JSON mode structured generation, implemented in the WebAssembly portion of the model library for optimal performance. Check &lt;a href=&#34;https://huggingface.co/spaces/mlc-ai/WebLLM-JSON-Playground&#34;&gt;WebLLM JSON Playground&lt;/a&gt; on HuggingFace to try generating JSON output with custom JSON schema.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#built-in-models&#34;&gt;&lt;strong&gt;Extensive Model Support&lt;/strong&gt;&lt;/a&gt;: WebLLM natively supports a range of models including Llama 3, Phi 3, Gemma, Mistral, Qwen(通义千问), and many others, making it versatile for various AI tasks. For the complete supported model list, check &lt;a href=&#34;https://mlc.ai/models&#34;&gt;MLC Models&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#custom-models&#34;&gt;&lt;strong&gt;Custom Model Integration&lt;/strong&gt;&lt;/a&gt;: Easily integrate and deploy custom models in MLC format, allowing you to adapt WebLLM to specific needs and scenarios, enhancing flexibility in model deployment.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Plug-and-Play Integration&lt;/strong&gt;: Easily integrate WebLLM into your projects using package managers like NPM and Yarn, or directly via CDN, complete with comprehensive &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/&#34;&gt;examples&lt;/a&gt; and a modular design for connecting with UI components.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streaming &amp;amp; Real-Time Interactions&lt;/strong&gt;: Supports streaming chat completions, allowing real-time output generation which enhances interactive applications like chatbots and virtual assistants.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Web Worker &amp;amp; Service Worker Support&lt;/strong&gt;: Optimize UI performance and manage the lifecycle of models efficiently by offloading computations to separate worker threads or service workers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chrome Extension Support&lt;/strong&gt;: Extend the functionality of web browsers through custom Chrome extensions using WebLLM, with examples available for building both basic and advanced extensions.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Built-in Models&lt;/h2&gt; &#xA;&lt;p&gt;Check the complete list of available models on &lt;a href=&#34;https://mlc.ai/models&#34;&gt;MLC Models&lt;/a&gt;. WebLLM supports a subset of these available models and the list can be accessed at &lt;a href=&#34;https://github.com/mlc-ai/web-llm/raw/main/src/config.ts#L293&#34;&gt;&lt;code&gt;prebuiltAppConfig.model_list&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are the primary families of models currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama&lt;/strong&gt;: Llama 3, Llama 2, Hermes-2-Pro-Llama-3&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Phi&lt;/strong&gt;: Phi 3, Phi 2, Phi 1.5&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gemma&lt;/strong&gt;: Gemma-2B&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mistral&lt;/strong&gt;: Mistral-7B-v0.3, Hermes-2-Pro-Mistral-7B, NeuralHermes-2.5-Mistral-7B, OpenHermes-2.5-Mistral-7B&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Qwen (通义千问)&lt;/strong&gt;: Qwen2 0.5B, 1.5B, 7B&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you need more models, &lt;a href=&#34;https://github.com/mlc-ai/web-llm/issues/new/choose&#34;&gt;request a new model via opening an issue&lt;/a&gt; or check &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#custom-models&#34;&gt;Custom Models&lt;/a&gt; for how to compile and use your own models with WebLLM.&lt;/p&gt; &#xA;&lt;h2&gt;Jumpstart with Examples&lt;/h2&gt; &#xA;&lt;p&gt;Learn how to use WebLLM to integrate large language models into your application and generate chat completions through this simple Chatbot example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jsfiddle.net/neetnestor/4nmgvsa2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Example-JSFiddle-blue?logo=jsfiddle&amp;amp;logoColor=white&#34; alt=&#34;Example Chatbot on JSFiddle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codepen.io/neetnestor/pen/vYwgZaG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Example-Codepen-gainsboro?logo=codepen&#34; alt=&#34;Example Chatbot on Codepen&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For an advanced example of a larger, more complicated project, check &lt;a href=&#34;https://github.com/mlc-ai/web-llm-chat/raw/main/app/client/webllm.ts&#34;&gt;WebLLM Chat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More examples for different use cases are available in the &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM offers a minimalist and modular interface to access the chatbot in the browser. The package is designed in a modular way to hook to any of the UI components.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;Package Manager&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# npm&#xA;npm install @mlc-ai/web-llm&#xA;# yarn&#xA;yarn add @mlc-ai/web-llm&#xA;# or pnpm&#xA;pnpm install @mlc-ai/web-llm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then import the module in your code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// Import everything&#xA;import * as webllm from &#34;@mlc-ai/web-llm&#34;;&#xA;// Or only import what you need&#xA;import { CreateMLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;CDN Delivery&lt;/h4&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.jsdelivr.com/package/npm/@mlc-ai/web-llm&#34;&gt;jsdelivr.com&lt;/a&gt;, WebLLM can be imported directly through URL and work out-of-the-box on cloud development platforms like &lt;a href=&#34;https://jsfiddle.net/&#34;&gt;jsfiddle.net&lt;/a&gt;, &lt;a href=&#34;https://codepen.io/&#34;&gt;Codepen.io&lt;/a&gt;, and &lt;a href=&#34;https://scribbler.live&#34;&gt;Scribbler&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import * as webllm from &#34;https://esm.run/@mlc-ai/web-llm&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It can also be dynamicall imported as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const webllm = await import (&#34;https://esm.run/@mlc-ai/web-llm&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create MLCEngine&lt;/h3&gt; &#xA;&lt;p&gt;Most operations in WebLLM are invoked through the &lt;code&gt;MLCEngine&lt;/code&gt; interface. You can create an &lt;code&gt;MLCEngine&lt;/code&gt; instance and loading the model by calling the &lt;code&gt;CreateMLCEngine()&lt;/code&gt; factory function.&lt;/p&gt; &#xA;&lt;p&gt;(Note that loading models requires downloading and it can take a significant amount of time for the very first run without caching previously. You should properly handle this asynchronous call.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { CreateMLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;// Callback function to update model loading progress&#xA;const initProgressCallback = (initProgress) =&amp;gt; {&#xA;  console.log(initProgress);&#xA;}&#xA;const selectedModel = &#34;Llama-3.1-8B-Instruct-q4f32_1-MLC&#34;;&#xA;&#xA;const engine = await CreateMLCEngine(&#xA;  selectedModel,&#xA;  { initProgressCallback: initProgressCallback }, // engineConfig&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Under the hood, this factory function does the following steps for first creating an engine instance (synchronous) and then loading the model (asynchronous). You can also do them separately in your application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { MLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;// This is a synchronous call that returns immediately&#xA;const engine = new MLCEngine({&#xA;  initProgressCallback: initProgressCallback&#xA;});&#xA;&#xA;// This is an asynchronous call and can take a long time to finish&#xA;await engine.reload(selectedModel);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat Completion&lt;/h3&gt; &#xA;&lt;p&gt;After successfully initializing the engine, you can now invoke chat completions using OpenAI style chat APIs through the &lt;code&gt;engine.chat.completions&lt;/code&gt; interface. For the full list of parameters and their descriptions, check &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#full-openai-compatibility&#34;&gt;section below&lt;/a&gt; and &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;OpenAI API reference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(Note: The &lt;code&gt;model&lt;/code&gt; parameter is not supported and will be ignored here. Instead, call &lt;code&gt;CreateMLCEngine(model)&lt;/code&gt; or &lt;code&gt;engine.reload(model)&lt;/code&gt; instead as shown in the &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#create-mlcengine&#34;&gt;Create MLCEngine&lt;/a&gt; above.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;const messages = [&#xA;  { role: &#34;system&#34;, content: &#34;You are a helpful AI assistant.&#34; },&#xA;  { role: &#34;user&#34;, content: &#34;Hello!&#34; },&#xA;]&#xA;&#xA;const reply = await engine.chat.completions.create({&#xA;  messages,&#xA;});&#xA;console.log(reply.choices[0].message);&#xA;console.log(reply.usage);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;p&gt;WebLLM also supports streaming chat completion generating. To use it, simply pass &lt;code&gt;stream: true&lt;/code&gt; to the &lt;code&gt;engine.chat.completions.create&lt;/code&gt; call.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;const messages = [&#xA;  { role: &#34;system&#34;, content: &#34;You are a helpful AI assistant.&#34; },&#xA;  { role: &#34;user&#34;, content: &#34;Hello!&#34; },&#xA;]&#xA;&#xA;// Chunks is an AsyncGenerator object&#xA;const chunks = await engine.chat.completions.create({&#xA;  messages,&#xA;  temperature: 1,&#xA;  stream: true, // &amp;lt;-- Enable streaming&#xA;  stream_options: { include_usage: true },&#xA;});&#xA;&#xA;let reply = &#34;&#34;;&#xA;for await (const chunk of chunks) {&#xA;  reply += chunk.choices[0]?.delta.content || &#34;&#34;;&#xA;  console.log(reply);&#xA;  if (chunk.usage) {&#xA;    console.log(chunk.usage); // only last chunk has usage&#xA;  }&#xA;}&#xA;&#xA;const fullReply = await engine.getMessage();&#xA;console.log(fullReply);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Using Workers&lt;/h3&gt; &#xA;&lt;p&gt;You can put the heavy computation in a worker script to optimize your application performance. To do so, you need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a handler in the worker thread that communicates with the frontend while handling the requests.&lt;/li&gt; &#xA; &lt;li&gt;Create a Worker Engine in your main application, which under the hood sends messages to the handler in the worker thread.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For detailed implementations of different kinds of Workers, check the following sections.&lt;/p&gt; &#xA;&lt;h4&gt;Dedicated Web Worker&lt;/h4&gt; &#xA;&lt;p&gt;WebLLM comes with API support for WebWorker so you can hook the generation process into a separate worker thread so that the computing in the worker thread won&#39;t disrupt the UI.&lt;/p&gt; &#xA;&lt;p&gt;We create a handler in the worker thread that communicates with the frontend while handling the requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// worker.ts&#xA;import { WebWorkerMLCEngineHandler } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;// A handler that resides in the worker thread&#xA;const handler = new WebWorkerMLCEngineHandler();&#xA;self.onmessage = (msg: MessageEvent) =&amp;gt; {&#xA;  handler.onmessage(msg);&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the main logic, we create a &lt;code&gt;WebWorkerMLCEngine&lt;/code&gt; that implements the same &lt;code&gt;MLCEngineInterface&lt;/code&gt;. The rest of the logic remains the same.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// main.ts&#xA;import { CreateWebWorkerMLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;async function main() {&#xA;  // Use a WebWorkerMLCEngine instead of MLCEngine here&#xA;  const engine = await CreateWebWorkerMLCEngine(&#xA;    new Worker(&#xA;      new URL(&#34;./worker.ts&#34;, import.meta.url), &#xA;      {&#xA;        type: &#34;module&#34;,&#xA;      }&#xA;    ),&#xA;    selectedModel,&#xA;    { initProgressCallback }, // engineConfig&#xA;  );&#xA;&#xA;  // everything else remains the same&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Service Worker&lt;/h3&gt; &#xA;&lt;p&gt;WebLLM comes with API support for ServiceWorker so you can hook the generation process into a service worker to avoid reloading the model in every page visit and optimize your application&#39;s offline experience.&lt;/p&gt; &#xA;&lt;p&gt;(Note, Service Worker&#39;s life cycle is managed by the browser and can be killed any time without notifying the webapp. &lt;code&gt;ServiceWorkerMLCEngine&lt;/code&gt; will try to keep the service worker thread alive by periodically sending heartbeat events, but your application should also include proper error handling. Check &lt;code&gt;keepAliveMs&lt;/code&gt; and &lt;code&gt;missedHeatbeat&lt;/code&gt; in &lt;a href=&#34;https://github.com/mlc-ai/web-llm/raw/main/src/service_worker.ts#L234&#34;&gt;&lt;code&gt;ServiceWorkerMLCEngine&lt;/code&gt;&lt;/a&gt; for more details.)&lt;/p&gt; &#xA;&lt;p&gt;We create a handler in the worker thread that communicates with the frontend while handling the requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// sw.ts&#xA;import { ServiceWorkerMLCEngineHandler } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;let handler: ServiceWorkerMLCEngineHandler;&#xA;&#xA;self.addEventListener(&#34;activate&#34;, function (event) {&#xA;  handler = new ServiceWorkerMLCEngineHandler();&#xA;  console.log(&#34;Service Worker is ready&#34;);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in the main logic, we register the service worker and create the engine using &lt;code&gt;CreateServiceWorkerMLCEngine&lt;/code&gt; function. The rest of the logic remains the same.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// main.ts&#xA;import { MLCEngineInterface, CreateServiceWorkerMLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;if (&#34;serviceWorker&#34; in navigator) {&#xA;  navigator.serviceWorker.register(&#xA;    new URL(&#34;sw.ts&#34;, import.meta.url),  // worker script&#xA;    { type: &#34;module&#34; },&#xA;  );&#xA;}&#xA;&#xA;const engine: MLCEngineInterface =&#xA;  await CreateServiceWorkerMLCEngine(&#xA;    selectedModel,&#xA;    { initProgressCallback }, // engineConfig&#xA;  );&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find a complete example on how to run WebLLM in service worker in &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/service-worker/&#34;&gt;examples/service-worker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chrome Extension&lt;/h3&gt; &#xA;&lt;p&gt;You can also find examples of building Chrome extension with WebLLM in &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/chrome-extension/&#34;&gt;examples/chrome-extension&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/chrome-extension-webgpu-service-worker/&#34;&gt;examples/chrome-extension-webgpu-service-worker&lt;/a&gt;. The latter one leverages service worker, so the extension is persistent in the background.&lt;/p&gt; &#xA;&lt;h2&gt;Full OpenAI Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM is designed to be fully compatible with &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI API&lt;/a&gt;. Thus, besides building a simple chatbot, you can also have the following functionalities with WebLLM:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/streaming&#34;&gt;streaming&lt;/a&gt;: return output as chunks in real-time in the form of an AsyncGenerator&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/json-mode&#34;&gt;json-mode&lt;/a&gt;: efficiently ensure output is in JSON format, see &lt;a href=&#34;https://platform.openai.com/docs/guides/text-generation/chat-completions-api&#34;&gt;OpenAI Reference&lt;/a&gt; for more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/seed-to-reproduce&#34;&gt;seed-to-reproduce&lt;/a&gt;: use seeding to ensure a reproducible output with fields &lt;code&gt;seed&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/function-calling&#34;&gt;function-calling&lt;/a&gt; (WIP): function calling with fields &lt;code&gt;tools&lt;/code&gt; and &lt;code&gt;tool_choice&lt;/code&gt; (with preliminary support); or manual function calling without &lt;code&gt;tools&lt;/code&gt; or &lt;code&gt;tool_choice&lt;/code&gt; (keeps the most flexibility).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Custom Models&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM works as a companion project of &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt; and it supports custom models in MLC format. It reuses the model artifact and builds the flow of MLC LLM. To compile and use your own models with WebLLM, please check out &lt;a href=&#34;https://llm.mlc.ai/docs/deploy/webllm.html&#34;&gt;MLC LLM document&lt;/a&gt; on how to compile and deploy new model weights and libraries to WebLLM.&lt;/p&gt; &#xA;&lt;p&gt;Here, we go over the high-level idea. There are two elements of the WebLLM package that enable new models and weight variants.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Contains a URL to model artifacts, such as weights and meta-data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_lib&lt;/code&gt;: A URL to the web assembly library (i.e. wasm file) that contains the executables to accelerate the model computations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Both are customizable in the WebLLM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { CreateMLCEngine } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;async main() {&#xA;  const appConfig = {&#xA;    &#34;model_list&#34;: [&#xA;      {&#xA;        &#34;model&#34;: &#34;/url/to/my/llama&#34;,&#xA;        &#34;model_id&#34;: &#34;MyLlama-3b-v1-q4f32_0&#34;,&#xA;        &#34;model_lib&#34;: &#34;/url/to/myllama3b.wasm&#34;,&#xA;      }&#xA;    ],&#xA;  };&#xA;  // override default&#xA;  const chatOpts = {&#xA;    &#34;repetition_penalty&#34;: 1.01&#xA;  };&#xA;&#xA;  // load a prebuilt model&#xA;  // with a chat option override and app config&#xA;  // under the hood, it will load the model from myLlamaUrl&#xA;  // and cache it in the browser cache&#xA;  // The chat will also load the model library from &#34;/url/to/myllama3b.wasm&#34;,&#xA;  // assuming that it is compatible to the model in myLlamaUrl.&#xA;  const engine = await CreateMLCEngine(&#xA;    &#34;MyLlama-3b-v1-q4f32_0&#34;,&#xA;    { appConfig }, // engineConfig&#xA;    chatOpts,&#xA;  );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In many cases, we only want to supply the model weight variant, but not necessarily a new model (e.g. &lt;code&gt;NeuralHermes-Mistral&lt;/code&gt; can reuse &lt;code&gt;Mistral&lt;/code&gt;&#39;s model library). For examples of how a model library can be shared by different model variants, see &lt;code&gt;webllm.prebuiltAppConfig&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build WebLLM Package From Source&lt;/h2&gt; &#xA;&lt;p&gt;NOTE: you don&#39;t need to build from source unless you would like to modify the WebLLM package. To use the npm, simply follow &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#get-started&#34;&gt;Get Started&lt;/a&gt; or any of the &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples&#34;&gt;examples&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;To build from source, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, to test the effects of your code change in an example, inside &lt;code&gt;examples/get-started/package.json&lt;/code&gt;, change from &lt;code&gt;&#34;@mlc-ai/web-llm&#34;: &#34;^0.2.73&#34;&lt;/code&gt; to &lt;code&gt;&#34;@mlc-ai/web-llm&#34;: ../..&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/get-started&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that sometimes you would need to switch between &lt;code&gt;file:../..&lt;/code&gt; and &lt;code&gt;../..&lt;/code&gt; to trigger npm to recognize new changes. In the worst case, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/get-started&#xA;rm -rf node_modules dist package-lock.json .parcel-cache&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;In case you need to build TVMjs from source&lt;/h3&gt; &#xA;&lt;p&gt;WebLLM&#39;s runtime largely depends on TVMjs: &lt;a href=&#34;https://github.com/apache/tvm/tree/main/web&#34;&gt;https://github.com/apache/tvm/tree/main/web&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;While it is also available as an npm package: &lt;a href=&#34;https://www.npmjs.com/package/@mlc-ai/web-runtime&#34;&gt;https://www.npmjs.com/package/@mlc-ai/web-runtime&lt;/a&gt;, you can build it from source if needed by following the steps below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://emscripten.org&#34;&gt;emscripten&lt;/a&gt;. It is an LLVM-based compiler that compiles C/C++ source code to WebAssembly.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Follow the &lt;a href=&#34;https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended&#34;&gt;installation instruction&lt;/a&gt; to install the latest emsdk.&lt;/li&gt; &#xA;   &lt;li&gt;Source &lt;code&gt;emsdk_env.sh&lt;/code&gt; by &lt;code&gt;source path/to/emsdk_env.sh&lt;/code&gt;, so that &lt;code&gt;emcc&lt;/code&gt; is reachable from PATH and the command &lt;code&gt;emcc&lt;/code&gt; works.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;We can verify the successful installation by trying out &lt;code&gt;emcc&lt;/code&gt; terminal.&lt;/p&gt; &lt;p&gt;Note: We recently found that using the latest &lt;code&gt;emcc&lt;/code&gt; version may run into issues during runtime. Use &lt;code&gt;./emsdk install 3.1.56&lt;/code&gt; instead of &lt;code&gt;./emsdk install latest&lt;/code&gt; for now as a workaround. The error may look like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Init error, LinkError: WebAssembly.instantiate(): Import #6 module=&#34;wasi_snapshot_preview1&#34;&#xA;function=&#34;proc_exit&#34;: function import requires a callable&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;./package.json&lt;/code&gt;, change from &lt;code&gt;&#34;@mlc-ai/web-runtime&#34;: &#34;0.18.0-dev2&#34;,&lt;/code&gt; to &lt;code&gt;&#34;@mlc-ai/web-runtime&#34;: &#34;file:./tvm_home/web&#34;,&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup necessary environment&lt;/p&gt; &lt;p&gt;Prepare all the necessary dependencies for web build:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./scripts/prep_deps.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this step, if &lt;code&gt;$TVM_SOURCE_DIR&lt;/code&gt; is not defined in the environment, we will execute the following line to build &lt;code&gt;tvmjs&lt;/code&gt; dependency:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mlc-ai/relax 3rdparty/tvm-unity --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This clones the current HEAD of &lt;code&gt;mlc-ai/relax&lt;/code&gt;. However, it may not always be the correct branch or commit to clone. To build a specific npm version from source, refer to the version bump PR, which states which branch (i.e. &lt;code&gt;mlc-ai/relax&lt;/code&gt; or &lt;code&gt;apache/tvm&lt;/code&gt;) and which commit the current WebLLM version depends on. For instance, version 0.2.52, according to its version bump PR &lt;a href=&#34;https://github.com/mlc-ai/web-llm/pull/521&#34;&gt;https://github.com/mlc-ai/web-llm/pull/521&lt;/a&gt;, is built by checking out the following commit &lt;a href=&#34;https://github.com/apache/tvm/commit/e6476847753c80e054719ac47bc2091c888418b6&#34;&gt;https://github.com/apache/tvm/commit/e6476847753c80e054719ac47bc2091c888418b6&lt;/a&gt; in &lt;code&gt;apache/tvm&lt;/code&gt;, rather than the HEAD of &lt;code&gt;mlc-ai/relax&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Besides, &lt;code&gt;--recursive&lt;/code&gt; is necessary and important. Otherwise, you may encounter errors like &lt;code&gt;fatal error: &#39;dlpack/dlpack.h&#39; file not found&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build WebLLM Package&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Validate some of the sub-packages&lt;/p&gt; &lt;p&gt;You can then go to the subfolders in &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples&#34;&gt;examples&lt;/a&gt; to validate some of the sub-packages. We use Parcelv2 for bundling. Although Parcel is not very good at tracking parent directory changes sometimes. When you make a change in the WebLLM package, try to edit the &lt;code&gt;package.json&lt;/code&gt; of the subfolder and save it, which will trigger Parcel to rebuild.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.webllm.ai/&#34;&gt;Demo App: WebLLM Chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to run LLM on native runtime, check out &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You might also be interested in &lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion/&#34;&gt;Web Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is initiated by members from CMU Catalyst, UW SAMPL, SJTU, OctoML, and the MLC community. We would love to continue developing and supporting the open-source ML community.&lt;/p&gt; &#xA;&lt;p&gt;This project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities make these models accessible. We would like to thank the teams behind Vicuna, SentencePiece, LLaMA, and Alpaca. We also would like to thank the WebAssembly, Emscripten, and WebGPU communities. Finally, thanks to Dawn and WebGPU developers.&lt;/p&gt;</summary>
  </entry>
</feed>