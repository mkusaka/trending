<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-07T02:06:07Z</updated>
  <subtitle>Weekly Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>developersdigest/llm-answer-engine</title>
    <updated>2024-04-07T02:06:07Z</updated>
    <id>tag:github.com,2024-04-07:/developersdigest/llm-answer-engine</id>
    <link href="https://github.com/developersdigest/llm-answer-engine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build a Perplexity-Inspired Answer Engine Using Next.js, Groq, Mixtral, Langchain, OpenAI, Brave &amp; Serper&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Perplexity-Inspired LLM Answer Engine&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://twitter.com/dev__digest&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/X/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.youtube.com/@developersdigest&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://trendshift.io/repositories/8642&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/8642&#34; alt=&#34;developersdigest%2Fllm-answer-engine | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/kFC-OWw7G8k&#34;&gt;Watch the tutorial here&lt;/a&gt; for a detailed guide on setting up and running this project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZmJ0ZnhmNjkwYzczZDlqZzM1dDRka2k1MGx6dW02ZHl5dzV0aGQwMiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/mluzeYSMGoAnSXg0ft/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the code and instructions needed to build a sophisticated answer engine that leverages the capabilities of &lt;a href=&#34;https://www.groq.com/&#34;&gt;Groq&lt;/a&gt;, &lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Mistral AI&#39;s Mixtral&lt;/a&gt;, &lt;a href=&#34;https://js.langchain.com/docs/&#34;&gt;Langchain.JS&lt;/a&gt;, &lt;a href=&#34;https://search.brave.com/&#34;&gt;Brave Search&lt;/a&gt;, &lt;a href=&#34;https://serper.dev/&#34;&gt;Serper API&lt;/a&gt;, and &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;. Designed to efficiently return sources, answers, images, videos, and follow-up questions based on user queries, this project is an ideal starting point for developers interested in natural language processing and search technologies.&lt;/p&gt; &#xA;&lt;h2&gt;Technologies Used&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Next.js&lt;/strong&gt;: A React framework for building server-side rendered and static web applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tailwind CSS&lt;/strong&gt;: A utility-first CSS framework for rapidly building custom user interfaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vercel AI SDK&lt;/strong&gt;: The Vercel AI SDK is a library for building AI-powered streaming text and chat UIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq &amp;amp; Mixtral&lt;/strong&gt;: Technologies for processing and understanding user queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Langchain.JS&lt;/strong&gt;: A JavaScript library focused on text operations, such as text splitting and embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brave Search&lt;/strong&gt;: A privacy-focused search engine used for sourcing relevant content and images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Serper API&lt;/strong&gt;: Used for fetching relevant video and image results based on the user&#39;s query.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI Embeddings&lt;/strong&gt;: Used for creating vector representations of text chunks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cheerio&lt;/strong&gt;: Utilized for HTML parsing, allowing the extraction of content from web pages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama (Optional)&lt;/strong&gt;: Used for streaming inference and embeddings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Node.js and npm are installed on your machine.&lt;/li&gt; &#xA; &lt;li&gt;Obtain API keys from OpenAI, Groq, Brave Search, and Serper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Obtaining API Keys&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI API Key&lt;/strong&gt;: &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;Generate your OpenAI API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq API Key&lt;/strong&gt;: &lt;a href=&#34;https://console.groq.com/keys&#34;&gt;Get your Groq API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brave Search API Key&lt;/strong&gt;: &lt;a href=&#34;https://brave.com/search/api/&#34;&gt;Obtain your Brave Search API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Serper API Key&lt;/strong&gt;: &lt;a href=&#34;https://serper.dev/&#34;&gt;Get your Serper API key here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/developersdigest/llm-answer-engine.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install the required dependencies: &lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; or &lt;pre&gt;&lt;code&gt;bun install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root of your project and add your API keys: &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=your_openai_api_key&#xA;GROQ_API_KEY=your_groq_api_key&#xA;BRAVE_SEARCH_API_KEY=your_brave_search_api_key&#xA;SERPER_API=your_serper_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running the Server&lt;/h3&gt; &#xA;&lt;p&gt;To start the server, execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bun run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the server will be listening on the specified port.&lt;/p&gt; &#xA;&lt;h2&gt;Editing the Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The configuration file is located in the &lt;code&gt;app/config.tsx&lt;/code&gt; file. You can modify the following values&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;useOllamaInference: false,&lt;/li&gt; &#xA; &lt;li&gt;useOllamaEmbeddings: false,&lt;/li&gt; &#xA; &lt;li&gt;inferenceModel: &#39;mixtral-8x7b-32768&#39;,&lt;/li&gt; &#xA; &lt;li&gt;inferenceAPIKey: process.env.GROQ_API_KEY,&lt;/li&gt; &#xA; &lt;li&gt;embeddingsModel: &#39;text-embedding-3-small&#39;,&lt;/li&gt; &#xA; &lt;li&gt;textChunkSize: 800,&lt;/li&gt; &#xA; &lt;li&gt;textChunkOverlap: 200,&lt;/li&gt; &#xA; &lt;li&gt;numberOfSimilarityResults: 2,&lt;/li&gt; &#xA; &lt;li&gt;numberOfPagesToScan: 10,&lt;/li&gt; &#xA; &lt;li&gt;nonOllamaBaseURL: &#39;&lt;a href=&#34;https://api.groq.com/openai/v1&#34;&gt;https://api.groq.com/openai/v1&lt;/a&gt;&#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ollama Support (Partially supported)&lt;/h3&gt; &#xA;&lt;p&gt;Currently, streaming text responses are supported for Ollama, but follow-up questions are not yet supported.&lt;/p&gt; &#xA;&lt;p&gt;Embeddings are supported, however, time-to-first-token can be quite long when using both a local embedding model as well as a local model for the streaming inference. I recommended decreasing a number of the RAG values specified in the &lt;code&gt;app/config.tsx&lt;/code&gt; file to decrease the time-to-first-token when using Ollama.&lt;/p&gt; &#xA;&lt;p&gt;To get started, make sure you have the Ollama running model on your local machine and set within the config the model you would like to use and set use OllamaInference and/or useOllamaEmbeddings to true.&lt;/p&gt; &#xA;&lt;p&gt;Note: When &#39;useOllamaInference&#39; is set to true, the model will be used for both text generation, but it will skip the follow-up questions inference step when using Ollama.&lt;/p&gt; &#xA;&lt;p&gt;More info: &lt;a href=&#34;https://ollama.com/blog/openai-compatibility&#34;&gt;https://ollama.com/blog/openai-compatibility&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[In progress] Add support for dynamic and conditionally rendered UI components based on the user&#39;s query&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExN284d3p5azAyNHpubm9mb2F0cnB6MWdtcTdnd2Nkb2d1ZnRtMG0yYiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/OMpt8ZbBsjphZz6mue/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[] Add a settings component to allow users to select the model, embeddings model, and other parameters from the UI&lt;/li&gt; &#xA; &lt;li&gt;[] Add support for follow-up questions when using Ollama&lt;/li&gt; &#xA; &lt;li&gt;[Completed] Add dark mode support based on the user&#39;s system preference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZDQxdHR0NWc4MHl6cDBsNmpiMGNyeWNwbnE4MjZlb29oZGRsODBhMCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QjINYAx6le5PMY020A/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Backend + Node Only Express API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.youtube.com/vi/43ZCeBTcsS8/0.jpg&#34; alt=&#34;Build a Perplexity-Inspired Answer Engine Using Groq, Mixtral, Langchain, Brave &amp;amp; OpenAI in 10 Min&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition to the Next.JS version of the project, there is a backend only version that uses Node.js and Express. Which is located in the &#39;original-express-api&#39; directory. This is a standalone version of the project that can be used as a reference for building a similar API. There is also a readme file in the &#39;original-express-api&#39; directory that explains how to run the backend version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/43ZCeBTcsS8&#34;&gt;Watch the express tutorial here&lt;/a&gt; for a detailed guide on setting up and running this project.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to the project are welcome. Feel free to fork the repository, make your changes, and submit a pull request. You can also open issues to suggest improvements or report bugs.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#developersdigest/llm-answer-engine&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=developersdigest/llm-answer-engine&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m the developer behind Developers Digest. If you find my work helpful or enjoy what I do, consider supporting me. Here are a few ways you can do that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patreon&lt;/strong&gt;: Support me on Patreon at &lt;a href=&#34;https://www.patreon.com/DevelopersDigest&#34;&gt;patreon.com/DevelopersDigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Buy Me A Coffee&lt;/strong&gt;: You can buy me a coffee at &lt;a href=&#34;https://www.buymeacoffee.com/developersdigest&#34;&gt;buymeacoffee.com/developersdigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: Check out my website at &lt;a href=&#34;https://developersdigest.tech&#34;&gt;developersdigest.tech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Github&lt;/strong&gt;: Follow me on GitHub at &lt;a href=&#34;https://github.com/developersdigest&#34;&gt;github.com/developersdigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Twitter&lt;/strong&gt;: Follow me on Twitter at &lt;a href=&#34;https://twitter.com/dev__digest&#34;&gt;twitter.com/dev__digest&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>semanser/codel</title>
    <updated>2024-04-07T02:06:07Z</updated>
    <id>tag:github.com,2024-04-07:/semanser/codel</id>
    <link href="https://github.com/semanser/codel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ú® Fully autonomous AI Agent that can perform complicated tasks and projects using terminal, browser, and editor.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/semanser/codel/main/.github/demo.png&#34;&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; Fully autonomous AI Agent that can perform complicated tasks and projects using terminal, browser, and editor.&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discord: &lt;a href=&#34;https://discord.gg/uMaGSHNjzc&#34;&gt;https://discord.gg/uMaGSHNjzc&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîì Secure. Everything is running in a sandboxed Docker environment.&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Autonomous. Automatically detects the next step and performs it.&lt;/li&gt; &#xA; &lt;li&gt;üîç Built-in browser. Fetches latest information from the web (tutorials, docs, etc.) if needed.&lt;/li&gt; &#xA; &lt;li&gt;üìô Built-in text editor. View all the modified files right in your browser.&lt;/li&gt; &#xA; &lt;li&gt;üß† All the history commands and outputs are saved in the PostgreSQL database.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ Automatic Docker-image picker based on the user task.&lt;/li&gt; &#xA; &lt;li&gt;ü§≥ Self-hosted&lt;/li&gt; &#xA; &lt;li&gt;üíÖ Modern UI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting started&lt;/h1&gt; &#xA;&lt;p&gt;The simplest way to run Codel is to use a pre-built Docker image. You can find the latest image on the &lt;a href=&#34;https://github.com/semanser/codel/pkgs/container/codel&#34;&gt;Github Container Registry&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] You need to use a corresponding environment variable in order to use any of the supported language models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can run the Docker image with the following command. Remove or change the environment variables according to your needs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run \&#xA;  -e OPEN_AI_KEY=your_open_ai_key \&#xA;  -e OPEN_AI_MODEL=gpt-4-0125-preview \&#xA;  -e OLLAMA_MODEL=llama2 \&#xA;  -p 3000:8080 \&#xA;  -v /var/run/docker.sock:/var/run/docker.sock \&#xA;  ghcr.io/semanser/codel:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can create a &lt;code&gt;.env&lt;/code&gt; file and run the Docker image with the &lt;code&gt;--env-file&lt;/code&gt; flag. More information can be found &lt;a href=&#34;https://docs.docker.com/reference/cli/docker/container/run/#env&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now you can visit &lt;a href=&#34;localhost:3000&#34;&gt;localhost:3000&lt;/a&gt; in your browser and start using Codel.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Supported environment variables&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;* `OPEN_AI_KEY` - OpenAI API key. You can get the key [here](https://platform.openai.com/account/api-keys).&#xA;* `OPEN_AI_MODEL` - OpenAI model (default: gpt-4-0125-preview). The list of supported OpenAI models can be found [here](https://pkg.go.dev/github.com/sashabaranov/go-openai#pkg-constants).&#xA;* `OPEN_AI_SERVER_URL` - OpenAI server URL (default: https://api.openai.com/v1). Change this URL if you are using an OpenAI compatible server.&#xA;* `OLLAMA_MODEL` - locally hosted Ollama model (default: https://ollama.com/model). The list of supported Ollama models can be found [here](https://ollama.com/models).&#xA;* `OLLAMA_SERVER_URL` - Ollama server URL (default: https://host.docker.internal:11434). Change this URL if you are using an Ollama compatible server.&#xA;See backend [.env.example](./backend/.env.example) for more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/semanser/codel/main/DEVELOPMENT.md&#34;&gt;DEVELOPMENT.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;You can find the project&#39;s roadmap &lt;a href=&#34;https://github.com/semanser/codel/milestones&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;This project wouldn&#39;t be possible without:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.00352&#34;&gt;https://arxiv.org/abs/2308.00352&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.08299&#34;&gt;https://arxiv.org/abs/2403.08299&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cognition-labs.com/introducing-devin&#34;&gt;https://www.cognition-labs.com/introducing-devin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-rod/rod&#34;&gt;https://github.com/go-rod/rod&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/semanser/JsonGenius&#34;&gt;https://github.com/semanser/JsonGenius&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>midday-ai/midday</title>
    <updated>2024-04-07T02:06:07Z</updated>
    <id>tag:github.com,2024-04-07:/midday-ai/midday</id>
    <link href="https://github.com/midday-ai/midday" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run your business smarter ü™Ñ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/midday-ai/midday/main/github.png&#34; alt=&#34;hero&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;b&gt;Midday&lt;/b&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Run your business smarter &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://go.midday.ai/anPiuRx&#34;&gt;Discord&lt;/a&gt; ¬∑ &lt;a href=&#34;https://midday.ai&#34;&gt;Website&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/midday-ai/midday/issues&#34;&gt;Issues&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Under active development&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We have been working with Midday for the past 4 months and our philosophy has been &#34;make it work, make it right&#34;. Currently, we&#39;re actively improving Midday&#39;s code quality and best practices. In the meantime, fasten your seatbelts and enjoy the ride!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Get started&lt;/h1&gt; &#xA;&lt;p&gt;We are working on the documentation to get started with Midday for local development: &lt;a href=&#34;https://docs.midday.ai/local-development&#34;&gt;https://docs.midday.ai/local-development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;App Architecture&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Monorepo&lt;/li&gt; &#xA; &lt;li&gt;Bun&lt;/li&gt; &#xA; &lt;li&gt;React&lt;/li&gt; &#xA; &lt;li&gt;TypeScript&lt;/li&gt; &#xA; &lt;li&gt;Nextjs&lt;/li&gt; &#xA; &lt;li&gt;Supabase&lt;/li&gt; &#xA; &lt;li&gt;Shadcn&lt;/li&gt; &#xA; &lt;li&gt;ToDesktop&lt;/li&gt; &#xA; &lt;li&gt;Expo&lt;/li&gt; &#xA; &lt;li&gt;TailwindCSS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hosting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supabase (database, storage, realtime, auth)&lt;/li&gt; &#xA; &lt;li&gt;Vercel (Website, edge-config, analytics and metrics)&lt;/li&gt; &#xA; &lt;li&gt;Upstash (redis)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Trigger.dev (background jobs)&lt;/li&gt; &#xA; &lt;li&gt;Resend (email)&lt;/li&gt; &#xA; &lt;li&gt;Novu (notifications)&lt;/li&gt; &#xA; &lt;li&gt;Github Actions (CI/CD)&lt;/li&gt; &#xA; &lt;li&gt;GoCardLess (Bank connection EU)&lt;/li&gt; &#xA; &lt;li&gt;Plaid (Bank connection in Canada and US)&lt;/li&gt; &#xA; &lt;li&gt;Teller (Bank connection in the US)&lt;/li&gt; &#xA; &lt;li&gt;Loops (Marketing email)&lt;/li&gt; &#xA; &lt;li&gt;LogSnag (Events)&lt;/li&gt; &#xA; &lt;li&gt;Dub (Short URLs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Repo Activity&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://repobeats.axiom.co/api/embed/96aae855e5dd87c30d53c1d154b37cf7aa5a89b3.svg?sanitize=true&#34; alt=&#34;Alt&#34; title=&#34;Repobeats analytics image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>