<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-17T02:08:21Z</updated>
  <subtitle>Weekly Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fastapi/full-stack-fastapi-template</title>
    <updated>2025-08-17T02:08:21Z</updated>
    <id>tag:github.com,2025-08-17:/fastapi/full-stack-fastapi-template</id>
    <link href="https://github.com/fastapi/full-stack-fastapi-template" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Full stack, modern web application template. Using FastAPI, React, SQLModel, PostgreSQL, Docker, GitHub Actions, automatic HTTPS and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Full Stack FastAPI Template&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template/actions?query=workflow%3ATest&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/fastapi/full-stack-fastapi-template/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Test&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/full-stack-fastapi-template&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/fastapi/full-stack-fastapi-template.svg?sanitize=true&#34; alt=&#34;Coverage&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Technology Stack and Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;&lt;/a&gt; for the Python backend API. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üß∞ &lt;a href=&#34;https://sqlmodel.tiangolo.com&#34;&gt;SQLModel&lt;/a&gt; for the Python SQL database interactions (ORM).&lt;/li&gt; &#xA;   &lt;li&gt;üîç &lt;a href=&#34;https://docs.pydantic.dev&#34;&gt;Pydantic&lt;/a&gt;, used by FastAPI, for the data validation and settings management.&lt;/li&gt; &#xA;   &lt;li&gt;üíæ &lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; as the SQL database.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://react.dev&#34;&gt;React&lt;/a&gt; for the frontend. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üíÉ Using TypeScript, hooks, Vite, and other parts of a modern frontend stack.&lt;/li&gt; &#xA;   &lt;li&gt;üé® &lt;a href=&#34;https://chakra-ui.com&#34;&gt;Chakra UI&lt;/a&gt; for the frontend components.&lt;/li&gt; &#xA;   &lt;li&gt;ü§ñ An automatically generated frontend client.&lt;/li&gt; &#xA;   &lt;li&gt;üß™ &lt;a href=&#34;https://playwright.dev&#34;&gt;Playwright&lt;/a&gt; for End-to-End testing.&lt;/li&gt; &#xA;   &lt;li&gt;ü¶á Dark mode support.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üêã &lt;a href=&#34;https://www.docker.com&#34;&gt;Docker Compose&lt;/a&gt; for development and production.&lt;/li&gt; &#xA; &lt;li&gt;üîí Secure password hashing by default.&lt;/li&gt; &#xA; &lt;li&gt;üîë JWT (JSON Web Token) authentication.&lt;/li&gt; &#xA; &lt;li&gt;üì´ Email based password recovery.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Tests with &lt;a href=&#34;https://pytest.org&#34;&gt;Pytest&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üìû &lt;a href=&#34;https://traefik.io&#34;&gt;Traefik&lt;/a&gt; as a reverse proxy / load balancer.&lt;/li&gt; &#xA; &lt;li&gt;üö¢ Deployment instructions using Docker Compose, including how to set up a frontend Traefik proxy to handle automatic HTTPS certificates.&lt;/li&gt; &#xA; &lt;li&gt;üè≠ CI (continuous integration) and CD (continuous deployment) based on GitHub Actions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dashboard Login&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/login.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard - Admin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/dashboard.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard - Create User&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/dashboard-create.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard - Items&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/dashboard-items.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard - User Settings&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/dashboard-user-settings.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard - Dark Mode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/dashboard-dark.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Interactive API Documentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/img/docs.png&#34; alt=&#34;API docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How To Use It&lt;/h2&gt; &#xA;&lt;p&gt;You can &lt;strong&gt;just fork or clone&lt;/strong&gt; this repository and use it as is.&lt;/p&gt; &#xA;&lt;p&gt;‚ú® It just works. ‚ú®&lt;/p&gt; &#xA;&lt;h3&gt;How to Use a Private Repository&lt;/h3&gt; &#xA;&lt;p&gt;If you want to have a private repository, GitHub won&#39;t allow you to simply fork it as it doesn&#39;t allow changing the visibility of forks.&lt;/p&gt; &#xA;&lt;p&gt;But you can do the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new GitHub repo, for example &lt;code&gt;my-full-stack&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository manually, set the name with the name of the project you want to use, for example &lt;code&gt;my-full-stack&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:fastapi/full-stack-fastapi-template.git my-full-stack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enter into the new directory:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd my-full-stack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the new origin to your new repository, copy it from the GitHub interface, for example:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git remote set-url origin git@github.com:octocat/my-full-stack.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add this repo as another &#34;remote&#34; to allow you to get updates later:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git remote add upstream git@github.com:fastapi/full-stack-fastapi-template.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Push the code to your new repository:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git push -u origin master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Update From the Original Template&lt;/h3&gt; &#xA;&lt;p&gt;After cloning the repository, and after doing changes, you might want to get the latest changes from this original template.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you added the original repository as a remote, you can check it with:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git remote -v&#xA;&#xA;origin    git@github.com:octocat/my-full-stack.git (fetch)&#xA;origin    git@github.com:octocat/my-full-stack.git (push)&#xA;upstream    git@github.com:fastapi/full-stack-fastapi-template.git (fetch)&#xA;upstream    git@github.com:fastapi/full-stack-fastapi-template.git (push)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pull the latest changes without merging:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull --no-commit upstream master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download the latest changes from this template without committing them, that way you can check everything is right before committing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are conflicts, solve them in your editor.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you are done, commit the changes:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git merge --continue&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configure&lt;/h3&gt; &#xA;&lt;p&gt;You can then update configs in the &lt;code&gt;.env&lt;/code&gt; files to customize your configurations.&lt;/p&gt; &#xA;&lt;p&gt;Before deploying it, make sure you change at least the values for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SECRET_KEY&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FIRST_SUPERUSER_PASSWORD&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can (and should) pass these as environment variables from secrets.&lt;/p&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/deployment.md&#34;&gt;deployment.md&lt;/a&gt; docs for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Generate Secret Keys&lt;/h3&gt; &#xA;&lt;p&gt;Some environment variables in the &lt;code&gt;.env&lt;/code&gt; file have a default value of &lt;code&gt;changethis&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You have to change them with a secret key, to generate secret keys you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &#34;import secrets; print(secrets.token_urlsafe(32))&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy the content and use that as password / secret key. And run that again to generate another secure key.&lt;/p&gt; &#xA;&lt;h2&gt;How To Use It - Alternative With Copier&lt;/h2&gt; &#xA;&lt;p&gt;This repository also supports generating a new project using &lt;a href=&#34;https://copier.readthedocs.io&#34;&gt;Copier&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It will copy all the files, ask you configuration questions, and update the &lt;code&gt;.env&lt;/code&gt; files with your answers.&lt;/p&gt; &#xA;&lt;h3&gt;Install Copier&lt;/h3&gt; &#xA;&lt;p&gt;You can install Copier with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install copier&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or better, if you have &lt;a href=&#34;https://pipx.pypa.io/&#34;&gt;&lt;code&gt;pipx&lt;/code&gt;&lt;/a&gt;, you can run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install copier&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you have &lt;code&gt;pipx&lt;/code&gt;, installing copier is optional, you could run it directly.&lt;/p&gt; &#xA;&lt;h3&gt;Generate a Project With Copier&lt;/h3&gt; &#xA;&lt;p&gt;Decide a name for your new project&#39;s directory, you will use it below. For example, &lt;code&gt;my-awesome-project&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Go to the directory that will be the parent of your project, and run the command with your project&#39;s name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have &lt;code&gt;pipx&lt;/code&gt; and you didn&#39;t install &lt;code&gt;copier&lt;/code&gt;, you can run it directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx run copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; the &lt;code&gt;--trust&lt;/code&gt; option is necessary to be able to execute a &lt;a href=&#34;https://github.com/fastapi/full-stack-fastapi-template/raw/master/.copier/update_dotenv.py&#34;&gt;post-creation script&lt;/a&gt; that updates your &lt;code&gt;.env&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h3&gt;Input Variables&lt;/h3&gt; &#xA;&lt;p&gt;Copier will ask you for some data, you might want to have at hand before generating the project.&lt;/p&gt; &#xA;&lt;p&gt;But don&#39;t worry, you can just update any of that in the &lt;code&gt;.env&lt;/code&gt; files afterwards.&lt;/p&gt; &#xA;&lt;p&gt;The input variables, with their default values (some auto generated) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;project_name&lt;/code&gt;: (default: &lt;code&gt;&#34;FastAPI Project&#34;&lt;/code&gt;) The name of the project, shown to API users (in .env).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stack_name&lt;/code&gt;: (default: &lt;code&gt;&#34;fastapi-project&#34;&lt;/code&gt;) The name of the stack used for Docker Compose labels and project name (no spaces, no periods) (in .env).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;secret_key&lt;/code&gt;: (default: &lt;code&gt;&#34;changethis&#34;&lt;/code&gt;) The secret key for the project, used for security, stored in .env, you can generate one with the method above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;first_superuser&lt;/code&gt;: (default: &lt;code&gt;&#34;admin@example.com&#34;&lt;/code&gt;) The email of the first superuser (in .env).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;first_superuser_password&lt;/code&gt;: (default: &lt;code&gt;&#34;changethis&#34;&lt;/code&gt;) The password of the first superuser (in .env).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;smtp_host&lt;/code&gt;: (default: &#34;&#34;) The SMTP server host to send emails, you can set it later in .env.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;smtp_user&lt;/code&gt;: (default: &#34;&#34;) The SMTP server user to send emails, you can set it later in .env.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;smtp_password&lt;/code&gt;: (default: &#34;&#34;) The SMTP server password to send emails, you can set it later in .env.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;emails_from_email&lt;/code&gt;: (default: &lt;code&gt;&#34;info@example.com&#34;&lt;/code&gt;) The email account to send emails from, you can set it later in .env.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;postgres_password&lt;/code&gt;: (default: &lt;code&gt;&#34;changethis&#34;&lt;/code&gt;) The password for the PostgreSQL database, stored in .env, you can generate one with the method above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sentry_dsn&lt;/code&gt;: (default: &#34;&#34;) The DSN for Sentry, if you are using it, you can set it later in .env.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Backend Development&lt;/h2&gt; &#xA;&lt;p&gt;Backend docs: &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/backend/README.md&#34;&gt;backend/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Frontend Development&lt;/h2&gt; &#xA;&lt;p&gt;Frontend docs: &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/frontend/README.md&#34;&gt;frontend/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Deployment docs: &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/deployment.md&#34;&gt;deployment.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;General development docs: &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/development.md&#34;&gt;development.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This includes using Docker Compose, custom local domains, &lt;code&gt;.env&lt;/code&gt; configurations, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;p&gt;Check the file &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/full-stack-fastapi-template/master/release-notes.md&#34;&gt;release-notes.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Full Stack FastAPI Template is licensed under the terms of the MIT license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ericc-ch/copilot-api</title>
    <updated>2025-08-17T02:08:21Z</updated>
    <id>tag:github.com,2025-08-17:/ericc-ch/copilot-api</id>
    <link href="https://github.com/ericc-ch/copilot-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Turn GitHub Copilot into OpenAI/Anthropic API compatible server. Usable with Claude Code!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Copilot API Proxy&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] This is a reverse-engineered proxy of GitHub Copilot API. It is not supported by GitHub, and may break unexpectedly. Use at your own risk.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] &lt;strong&gt;GitHub Security Notice:&lt;/strong&gt;&lt;br /&gt; Excessive automated or scripted use of Copilot (including rapid or bulk requests, such as via automated tools) may trigger GitHub&#39;s abuse-detection systems.&lt;br /&gt; You may receive a warning from GitHub Security, and further anomalous activity could result in temporary suspension of your Copilot access.&lt;/p&gt; &#xA; &lt;p&gt;GitHub prohibits use of their servers for excessive automated bulk activity or any activity that places undue burden on their infrastructure.&lt;/p&gt; &#xA; &lt;p&gt;Please review:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://docs.github.com/site-policy/acceptable-use-policies/github-acceptable-use-policies#4-spam-and-inauthentic-activity-on-github&#34;&gt;GitHub Acceptable Use Policies&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://docs.github.com/site-policy/github-terms/github-terms-for-additional-products-and-features#github-copilot&#34;&gt;GitHub Copilot Terms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Use this proxy responsibly to avoid account restrictions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/E1E519XS7W&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are using &lt;a href=&#34;https://github.com/sst/opencode&#34;&gt;opencode&lt;/a&gt;, you do not need this project. Opencode supports GitHub Copilot provider out of the box.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Project Overview&lt;/h2&gt; &#xA;&lt;p&gt;A reverse-engineered proxy for the GitHub Copilot API that exposes it as an OpenAI and Anthropic compatible service. This allows you to use GitHub Copilot with any tool that supports the OpenAI Chat Completions API or the Anthropic Messages API, including to power &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/overview&#34;&gt;Claude Code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI &amp;amp; Anthropic Compatibility&lt;/strong&gt;: Exposes GitHub Copilot as an OpenAI-compatible (&lt;code&gt;/v1/chat/completions&lt;/code&gt;, &lt;code&gt;/v1/models&lt;/code&gt;, &lt;code&gt;/v1/embeddings&lt;/code&gt;) and Anthropic-compatible (&lt;code&gt;/v1/messages&lt;/code&gt;) API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Claude Code Integration&lt;/strong&gt;: Easily configure and launch &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/overview&#34;&gt;Claude Code&lt;/a&gt; to use Copilot as its backend with a simple command-line flag (&lt;code&gt;--claude-code&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Usage Dashboard&lt;/strong&gt;: A web-based dashboard to monitor your Copilot API usage, view quotas, and see detailed statistics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rate Limit Control&lt;/strong&gt;: Manage API usage with rate-limiting options (&lt;code&gt;--rate-limit&lt;/code&gt;) and a waiting mechanism (&lt;code&gt;--wait&lt;/code&gt;) to prevent errors from rapid requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Manual Request Approval&lt;/strong&gt;: Manually approve or deny each API request for fine-grained control over usage (&lt;code&gt;--manual&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Token Visibility&lt;/strong&gt;: Option to display GitHub and Copilot tokens during authentication and refresh for debugging (&lt;code&gt;--show-token&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Authentication&lt;/strong&gt;: Authenticate interactively or provide a GitHub token directly, suitable for CI/CD environments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support for Different Account Types&lt;/strong&gt;: Works with individual, business, and enterprise GitHub Copilot plans.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7654b383-669d-4eb9-b23c-06d7aefee8c5&#34;&gt;https://github.com/user-attachments/assets/7654b383-669d-4eb9-b23c-06d7aefee8c5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bun (&amp;gt;= 1.2.x)&lt;/li&gt; &#xA; &lt;li&gt;GitHub account with Copilot subscription (individual, business, or enterprise)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install dependencies, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bun install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Docker&lt;/h2&gt; &#xA;&lt;p&gt;Build image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker build -t copilot-api .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the container&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Create a directory on your host to persist the GitHub token and related data&#xA;mkdir -p ./copilot-data&#xA;&#xA;# Run the container with a bind mount to persist the token&#xA;# This ensures your authentication survives container restarts&#xA;&#xA;docker run -p 4141:4141 -v $(pwd)/copilot-data:/root/.local/share/copilot-api copilot-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The GitHub token and related data will be stored in &lt;code&gt;copilot-data&lt;/code&gt; on your host. This is mapped to &lt;code&gt;/root/.local/share/copilot-api&lt;/code&gt; inside the container, ensuring persistence across restarts.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Docker with Environment Variables&lt;/h3&gt; &#xA;&lt;p&gt;You can pass the GitHub token directly to the container using environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Build with GitHub token&#xA;docker build --build-arg GH_TOKEN=your_github_token_here -t copilot-api .&#xA;&#xA;# Run with GitHub token&#xA;docker run -p 4141:4141 -e GH_TOKEN=your_github_token_here copilot-api&#xA;&#xA;# Run with additional options&#xA;docker run -p 4141:4141 -e GH_TOKEN=your_token copilot-api start --verbose --port 4141&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Compose Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: &#34;3.8&#34;&#xA;services:&#xA;  copilot-api:&#xA;    build: .&#xA;    ports:&#xA;      - &#34;4141:4141&#34;&#xA;    environment:&#xA;      - GH_TOKEN=your_github_token_here&#xA;    restart: unless-stopped&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Docker image includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-stage build for optimized image size&lt;/li&gt; &#xA; &lt;li&gt;Non-root user for enhanced security&lt;/li&gt; &#xA; &lt;li&gt;Health check for container monitoring&lt;/li&gt; &#xA; &lt;li&gt;Pinned base image version for reproducible builds&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using with npx&lt;/h2&gt; &#xA;&lt;p&gt;You can run the project directly using npx:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx copilot-api@latest start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx copilot-api@latest start --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For authentication only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx copilot-api@latest auth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Command Structure&lt;/h2&gt; &#xA;&lt;p&gt;Copilot API now uses a subcommand structure with these main commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;: Start the Copilot API server. This command will also handle authentication if needed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;auth&lt;/code&gt;: Run GitHub authentication flow without starting the server. This is typically used if you need to generate a token for use with the &lt;code&gt;--github-token&lt;/code&gt; option, especially in non-interactive environments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;check-usage&lt;/code&gt;: Show your current GitHub Copilot usage and quota information directly in the terminal (no server required).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;debug&lt;/code&gt;: Display diagnostic information including version, runtime details, file paths, and authentication status. Useful for troubleshooting and support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Command Line Options&lt;/h2&gt; &#xA;&lt;h3&gt;Start Command Options&lt;/h3&gt; &#xA;&lt;p&gt;The following command line options are available for the &lt;code&gt;start&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Alias&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--port&lt;/td&gt; &#xA;   &lt;td&gt;Port to listen on&lt;/td&gt; &#xA;   &lt;td&gt;4141&lt;/td&gt; &#xA;   &lt;td&gt;-p&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--verbose&lt;/td&gt; &#xA;   &lt;td&gt;Enable verbose logging&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;-v&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--account-type&lt;/td&gt; &#xA;   &lt;td&gt;Account type to use (individual, business, enterprise)&lt;/td&gt; &#xA;   &lt;td&gt;individual&lt;/td&gt; &#xA;   &lt;td&gt;-a&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--manual&lt;/td&gt; &#xA;   &lt;td&gt;Enable manual request approval&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--rate-limit&lt;/td&gt; &#xA;   &lt;td&gt;Rate limit in seconds between requests&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;-r&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--wait&lt;/td&gt; &#xA;   &lt;td&gt;Wait instead of error when rate limit is hit&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;-w&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--github-token&lt;/td&gt; &#xA;   &lt;td&gt;Provide GitHub token directly (must be generated using the &lt;code&gt;auth&lt;/code&gt; subcommand)&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;-g&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--claude-code&lt;/td&gt; &#xA;   &lt;td&gt;Generate a command to launch Claude Code with Copilot API config&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;-c&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--show-token&lt;/td&gt; &#xA;   &lt;td&gt;Show GitHub and Copilot tokens on fetch and refresh&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Auth Command Options&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Alias&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--verbose&lt;/td&gt; &#xA;   &lt;td&gt;Enable verbose logging&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;-v&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--show-token&lt;/td&gt; &#xA;   &lt;td&gt;Show GitHub token on auth&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Debug Command Options&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Alias&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--json&lt;/td&gt; &#xA;   &lt;td&gt;Output debug info as JSON&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;API Endpoints&lt;/h2&gt; &#xA;&lt;p&gt;The server exposes several endpoints to interact with the Copilot API. It provides OpenAI-compatible endpoints and now also includes support for Anthropic-compatible endpoints, allowing for greater flexibility with different tools and services.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI Compatible Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;These endpoints mimic the OpenAI API structure.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Endpoint&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST /v1/chat/completions&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Creates a model response for the given chat conversation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET /v1/models&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Lists the currently available models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST /v1/embeddings&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Creates an embedding vector representing the input text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Anthropic Compatible Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;These endpoints are designed to be compatible with the Anthropic Messages API.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Endpoint&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST /v1/messages&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Creates a model response for a given conversation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST /v1/messages/count_tokens&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Calculates the number of tokens for a given set of messages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Usage Monitoring Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;New endpoints for monitoring your Copilot usage and quotas.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Endpoint&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET /usage&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get detailed Copilot usage statistics and quota information.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET /token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get the current Copilot token being used by the API.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;p&gt;Using with npx:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Basic usage with start command&#xA;npx copilot-api@latest start&#xA;&#xA;# Run on custom port with verbose logging&#xA;npx copilot-api@latest start --port 8080 --verbose&#xA;&#xA;# Use with a business plan GitHub account&#xA;npx copilot-api@latest start --account-type business&#xA;&#xA;# Use with an enterprise plan GitHub account&#xA;npx copilot-api@latest start --account-type enterprise&#xA;&#xA;# Enable manual approval for each request&#xA;npx copilot-api@latest start --manual&#xA;&#xA;# Set rate limit to 30 seconds between requests&#xA;npx copilot-api@latest start --rate-limit 30&#xA;&#xA;# Wait instead of error when rate limit is hit&#xA;npx copilot-api@latest start --rate-limit 30 --wait&#xA;&#xA;# Provide GitHub token directly&#xA;npx copilot-api@latest start --github-token ghp_YOUR_TOKEN_HERE&#xA;&#xA;# Run only the auth flow&#xA;npx copilot-api@latest auth&#xA;&#xA;# Run auth flow with verbose logging&#xA;npx copilot-api@latest auth --verbose&#xA;&#xA;# Show your Copilot usage/quota in the terminal (no server needed)&#xA;npx copilot-api@latest check-usage&#xA;&#xA;# Display debug information for troubleshooting&#xA;npx copilot-api@latest debug&#xA;&#xA;# Display debug information in JSON format&#xA;npx copilot-api@latest debug --json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the Usage Viewer&lt;/h2&gt; &#xA;&lt;p&gt;After starting the server, a URL to the Copilot Usage Dashboard will be displayed in your console. This dashboard is a web interface for monitoring your API usage.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the server. For example, using npx: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx copilot-api@latest start&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The server will output a URL to the usage viewer. Copy and paste this URL into your browser. It will look something like this: &lt;code&gt;https://ericc-ch.github.io/copilot-api?endpoint=http://localhost:4141/usage&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you use the &lt;code&gt;start.bat&lt;/code&gt; script on Windows, this page will open automatically.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The dashboard provides a user-friendly interface to view your Copilot usage data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Endpoint URL&lt;/strong&gt;: The dashboard is pre-configured to fetch data from your local server endpoint via the URL query parameter. You can change this URL to point to any other compatible API endpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fetch Data&lt;/strong&gt;: Click the &#34;Fetch&#34; button to load or refresh the usage data. The dashboard will automatically fetch data on load.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Usage Quotas&lt;/strong&gt;: View a summary of your usage quotas for different services like Chat and Completions, displayed with progress bars for a quick overview.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detailed Information&lt;/strong&gt;: See the full JSON response from the API for a detailed breakdown of all available usage statistics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;URL-based Configuration&lt;/strong&gt;: You can also specify the API endpoint directly in the URL using a query parameter. This is useful for bookmarks or sharing links. For example: &lt;code&gt;https://ericc-ch.github.io/copilot-api?endpoint=http://your-api-server/usage&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using with Claude Code&lt;/h2&gt; &#xA;&lt;p&gt;This proxy can be used to power &lt;a href=&#34;https://docs.anthropic.com/en/claude-code&#34;&gt;Claude Code&lt;/a&gt;, an experimental conversational AI assistant for developers from Anthropic.&lt;/p&gt; &#xA;&lt;p&gt;There are two ways to configure Claude Code to use this proxy:&lt;/p&gt; &#xA;&lt;h3&gt;Interactive Setup with &lt;code&gt;--claude-code&lt;/code&gt; flag&lt;/h3&gt; &#xA;&lt;p&gt;To get started, run the &lt;code&gt;start&lt;/code&gt; command with the &lt;code&gt;--claude-code&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx copilot-api@latest start --claude-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be prompted to select a primary model and a &#34;small, fast&#34; model for background tasks. After selecting the models, a command will be copied to your clipboard. This command sets the necessary environment variables for Claude Code to use the proxy.&lt;/p&gt; &#xA;&lt;p&gt;Paste and run this command in a new terminal to launch Claude Code.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Configuration with &lt;code&gt;settings.json&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can configure Claude Code by creating a &lt;code&gt;.claude/settings.json&lt;/code&gt; file in your project&#39;s root directory. This file should contain the environment variables needed by Claude Code. This way you don&#39;t need to run the interactive setup every time.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example &lt;code&gt;.claude/settings.json&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;env&#34;: {&#xA;    &#34;ANTHROPIC_BASE_URL&#34;: &#34;http://localhost:4141&#34;,&#xA;    &#34;ANTHROPIC_AUTH_TOKEN&#34;: &#34;dummy&#34;,&#xA;    &#34;ANTHROPIC_MODEL&#34;: &#34;gpt-4.1&#34;,&#xA;    &#34;ANTHROPIC_SMALL_FAST_MODEL&#34;: &#34;gpt-4.1&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more options here: &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/settings#environment-variables&#34;&gt;Claude Code settings&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also read more about IDE integration here: &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/ide-integrations&#34;&gt;Add Claude Code to your IDE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running from Source&lt;/h2&gt; &#xA;&lt;p&gt;The project can be run from source in several ways:&lt;/p&gt; &#xA;&lt;h3&gt;Development Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bun run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Production Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bun run start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To avoid hitting GitHub Copilot&#39;s rate limits, you can use the following flags: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--manual&lt;/code&gt;: Enables manual approval for each request, giving you full control over when requests are sent.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--rate-limit &amp;lt;seconds&amp;gt;&lt;/code&gt;: Enforces a minimum time interval between requests. For example, &lt;code&gt;copilot-api start --rate-limit 30&lt;/code&gt; will ensure there&#39;s at least a 30-second gap between requests.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--wait&lt;/code&gt;: Use this with &lt;code&gt;--rate-limit&lt;/code&gt;. It makes the server wait for the cooldown period to end instead of rejecting the request with an error. This is useful for clients that don&#39;t automatically retry on rate limit errors.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you have a GitHub business or enterprise plan account with Copilot, use the &lt;code&gt;--account-type&lt;/code&gt; flag (e.g., &lt;code&gt;--account-type business&lt;/code&gt;). See the &lt;a href=&#34;https://docs.github.com/en/enterprise-cloud@latest/copilot/managing-copilot/managing-github-copilot-in-your-organization/managing-access-to-github-copilot-in-your-organization/managing-github-copilot-access-to-your-organizations-network#configuring-copilot-subscription-based-network-routing-for-your-enterprise-or-organization&#34;&gt;official documentation&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>openai/openai-node</title>
    <updated>2025-08-17T02:08:21Z</updated>
    <id>tag:github.com,2025-08-17:/openai/openai-node</id>
    <link href="https://github.com/openai/openai-node" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official JavaScript / TypeScript library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI TypeScript and JavaScript API Library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://npmjs.org/package/openai&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/openai.svg?label=npm%20(stable)&#34; alt=&#34;NPM version&#34; /&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/bundlephobia/minzip/openai&#34; alt=&#34;npm bundle size&#34; /&gt; &lt;a href=&#34;https://jsr.io/@openai/openai&#34;&gt;&lt;img src=&#34;https://jsr.io/badges/@openai/openai&#34; alt=&#34;JSR Version&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library provides convenient access to the OpenAI REST API from TypeScript or JavaScript.&lt;/p&gt; &#xA;&lt;p&gt;It is generated from our &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAPI specification&lt;/a&gt; with &lt;a href=&#34;https://stainlessapi.com/&#34;&gt;Stainless&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To learn how to use the OpenAI API, check out our &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;API Reference&lt;/a&gt; and &lt;a href=&#34;https://platform.openai.com/docs&#34;&gt;Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation from JSR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;deno add jsr:@openai/openai&#xA;npx jsr add @openai/openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These commands will make the module importable from the &lt;code&gt;@openai/openai&lt;/code&gt; scope. You can also &lt;a href=&#34;https://jsr.io/docs/using-packages#importing-with-jsr-specifiers&#34;&gt;import directly from JSR&lt;/a&gt; without an install step if you&#39;re using the Deno JavaScript runtime:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;jsr:@openai/openai&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/api.md&#34;&gt;api.md file&lt;/a&gt; along with many &lt;a href=&#34;https://github.com/openai/openai-node/tree/master/examples&#34;&gt;code examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/responses&#34;&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  apiKey: process.env[&#39;OPENAI_API_KEY&#39;], // This is the default and can be omitted&#xA;});&#xA;&#xA;const response = await client.responses.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  instructions: &#39;You are a coding assistant that talks like a pirate&#39;,&#xA;  input: &#39;Are semicolons optional in JavaScript?&#39;,&#xA;});&#xA;&#xA;console.log(response.output_text);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  apiKey: process.env[&#39;OPENAI_API_KEY&#39;], // This is the default and can be omitted&#xA;});&#xA;&#xA;const completion = await client.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [&#xA;    { role: &#39;developer&#39;, content: &#39;Talk like a pirate.&#39; },&#xA;    { role: &#39;user&#39;, content: &#39;Are semicolons optional in JavaScript?&#39; },&#xA;  ],&#xA;});&#xA;&#xA;console.log(completion.choices[0].message.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming responses&lt;/h2&gt; &#xA;&lt;p&gt;We provide support for streaming responses using Server Sent Events (SSE).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI();&#xA;&#xA;const stream = await client.responses.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  input: &#39;Say &#34;Sheep sleep deep&#34; ten times fast!&#39;,&#xA;  stream: true,&#xA;});&#xA;&#xA;for await (const event of stream) {&#xA;  console.log(event);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;File uploads&lt;/h2&gt; &#xA;&lt;p&gt;Request parameters that correspond to file uploads can be passed in many different forms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;File&lt;/code&gt; (or an object with the same structure)&lt;/li&gt; &#xA; &lt;li&gt;a &lt;code&gt;fetch&lt;/code&gt; &lt;code&gt;Response&lt;/code&gt; (or an object with the same structure)&lt;/li&gt; &#xA; &lt;li&gt;an &lt;code&gt;fs.ReadStream&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;the return value of our &lt;code&gt;toFile&lt;/code&gt; helper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import fs from &#39;fs&#39;;&#xA;import OpenAI, { toFile } from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI();&#xA;&#xA;// If you have access to Node `fs` we recommend using `fs.createReadStream()`:&#xA;await client.files.create({ file: fs.createReadStream(&#39;input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// Or if you have the web `File` API you can pass a `File` instance:&#xA;await client.files.create({ file: new File([&#39;my bytes&#39;], &#39;input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// You can also pass a `fetch` `Response`:&#xA;await client.files.create({ file: await fetch(&#39;https://somesite/input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// Finally, if none of the above are convenient, you can use our `toFile` helper:&#xA;await client.files.create({&#xA;  file: await toFile(Buffer.from(&#39;my bytes&#39;), &#39;input.jsonl&#39;),&#xA;  purpose: &#39;fine-tune&#39;,&#xA;});&#xA;await client.files.create({&#xA;  file: await toFile(new Uint8Array([0, 1, 2]), &#39;input.jsonl&#39;),&#xA;  purpose: &#39;fine-tune&#39;,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Webhook Verification&lt;/h2&gt; &#xA;&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information about webhooks, see &lt;a href=&#34;https://platform.openai.com/docs/guides/webhooks&#34;&gt;the API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; &#xA;&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.webhooks.unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will throw an error if the signature is invalid.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). The &lt;code&gt;.unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { headers } from &#39;next/headers&#39;;&#xA;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.&#xA;});&#xA;&#xA;export async function webhook(request: Request) {&#xA;  const headersList = headers();&#xA;  const body = await request.text();&#xA;&#xA;  try {&#xA;    const event = client.webhooks.unwrap(body, headersList);&#xA;&#xA;    switch (event.type) {&#xA;      case &#39;response.completed&#39;:&#xA;        console.log(&#39;Response completed:&#39;, event.data);&#xA;        break;&#xA;      case &#39;response.failed&#39;:&#xA;        console.log(&#39;Response failed:&#39;, event.data);&#xA;        break;&#xA;      default:&#xA;        console.log(&#39;Unhandled event type:&#39;, event.type);&#xA;    }&#xA;&#xA;    return Response.json({ message: &#39;ok&#39; });&#xA;  } catch (error) {&#xA;    console.error(&#39;Invalid webhook signature:&#39;, error);&#xA;    return new Response(&#39;Invalid signature&#39;, { status: 400 });&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; &#xA;&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.webhooks.verifySignature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;.unwrap()&lt;/code&gt;, this method will throw an error if the signature is invalid.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { headers } from &#39;next/headers&#39;;&#xA;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.&#xA;});&#xA;&#xA;export async function webhook(request: Request) {&#xA;  const headersList = headers();&#xA;  const body = await request.text();&#xA;&#xA;  try {&#xA;    client.webhooks.verifySignature(body, headersList);&#xA;&#xA;    // Parse the body after verification&#xA;    const event = JSON.parse(body);&#xA;    console.log(&#39;Verified event:&#39;, event);&#xA;&#xA;    return Response.json({ message: &#39;ok&#39; });&#xA;  } catch (error) {&#xA;    console.error(&#39;Invalid webhook signature:&#39;, error);&#xA;    return new Response(&#39;Invalid signature&#39;, { status: 400 });&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Handling errors&lt;/h2&gt; &#xA;&lt;p&gt;When the library is unable to connect to the API, or if the API returns a non-success status code (i.e., 4xx or 5xx response), a subclass of &lt;code&gt;APIError&lt;/code&gt; will be thrown:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const job = await client.fineTuning.jobs&#xA;  .create({ model: &#39;gpt-4o&#39;, training_file: &#39;file-abc123&#39; })&#xA;  .catch(async (err) =&amp;gt; {&#xA;    if (err instanceof OpenAI.APIError) {&#xA;      console.log(err.request_id);&#xA;      console.log(err.status); // 400&#xA;      console.log(err.name); // BadRequestError&#xA;      console.log(err.headers); // {server: &#39;nginx&#39;, ...}&#xA;    } else {&#xA;      throw err;&#xA;    }&#xA;  });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Error codes are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Status Code&lt;/th&gt; &#xA;   &lt;th&gt;Error Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;401&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;403&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;404&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;422&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;429&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;gt;=500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Request IDs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more information on debugging requests, see &lt;a href=&#34;https://platform.openai.com/docs/api-reference/debugging-requests&#34;&gt;these docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const completion = await client.chat.completions.create({&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;  model: &#39;gpt-4o&#39;,&#xA;});&#xA;console.log(completion._request_id); // req_123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the Request ID using the &lt;code&gt;.withResponse()&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const { data: stream, request_id } = await openai.chat.completions&#xA;  .create({&#xA;    model: &#39;gpt-4&#39;,&#xA;    messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;    stream: true,&#xA;  })&#xA;  .withResponse();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Realtime API Beta&lt;/h2&gt; &#xA;&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling&lt;/a&gt; through a &lt;code&gt;WebSocket&lt;/code&gt; connection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { OpenAIRealtimeWebSocket } from &#39;openai/beta/realtime/websocket&#39;;&#xA;&#xA;const rt = new OpenAIRealtimeWebSocket({ model: &#39;gpt-4o-realtime-preview-2024-12-17&#39; });&#xA;&#xA;rt.on(&#39;response.text.delta&#39;, (event) =&amp;gt; process.stdout.write(event.delta));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/realtime.md&#34;&gt;realtime.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The Azure API shape slightly differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { AzureOpenAI } from &#39;openai&#39;;&#xA;import { getBearerTokenProvider, DefaultAzureCredential } from &#39;@azure/identity&#39;;&#xA;&#xA;const credential = new DefaultAzureCredential();&#xA;const scope = &#39;https://cognitiveservices.azure.com/.default&#39;;&#xA;const azureADTokenProvider = getBearerTokenProvider(credential, scope);&#xA;&#xA;const openai = new AzureOpenAI({ azureADTokenProvider });&#xA;&#xA;const result = await openai.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say hello!&#39; }],&#xA;});&#xA;&#xA;console.log(result.choices[0]!.message?.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retries&lt;/h3&gt; &#xA;&lt;p&gt;Certain errors will be automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors will all be retried by default.&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;maxRetries&lt;/code&gt; option to configure or disable this:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// Configure the default for all requests:&#xA;const client = new OpenAI({&#xA;  maxRetries: 0, // default is 2&#xA;});&#xA;&#xA;// Or, configure per-request:&#xA;await client.chat.completions.create({ messages: [{ role: &#39;user&#39;, content: &#39;How can I get the name of the current day in JavaScript?&#39; }], model: &#39;gpt-4o&#39; }, {&#xA;  maxRetries: 5,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Timeouts&lt;/h3&gt; &#xA;&lt;p&gt;Requests time out after 10 minutes by default. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// Configure the default for all requests:&#xA;const client = new OpenAI({&#xA;  timeout: 20 * 1000, // 20 seconds (default is 10 minutes)&#xA;});&#xA;&#xA;// Override per-request:&#xA;await client.chat.completions.create({ messages: [{ role: &#39;user&#39;, content: &#39;How can I list all files in a directory using Python?&#39; }], model: &#39;gpt-4o&#39; }, {&#xA;  timeout: 5 * 1000,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On timeout, an &lt;code&gt;APIConnectionTimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; &#xA;&lt;p&gt;Note that requests which time out will be &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/#retries&#34;&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Request IDs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more information on debugging requests, see &lt;a href=&#34;https://platform.openai.com/docs/api-reference/debugging-requests&#34;&gt;these docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const response = await client.responses.create({ model: &#39;gpt-4o&#39;, input: &#39;testing 123&#39; });&#xA;console.log(response._request_id); // req_123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the Request ID using the &lt;code&gt;.withResponse()&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const { data: stream, request_id } = await openai.responses&#xA;  .create({&#xA;    model: &#39;gpt-4o&#39;,&#xA;    input: &#39;Say this is a test&#39;,&#xA;    stream: true,&#xA;  })&#xA;  .withResponse();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Auto-pagination&lt;/h2&gt; &#xA;&lt;p&gt;List methods in the OpenAI API are paginated. You can use the &lt;code&gt;for await ‚Ä¶ of&lt;/code&gt; syntax to iterate through items across all pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;async function fetchAllFineTuningJobs(params) {&#xA;  const allFineTuningJobs = [];&#xA;  // Automatically fetches more pages as needed.&#xA;  for await (const fineTuningJob of client.fineTuning.jobs.list({ limit: 20 })) {&#xA;    allFineTuningJobs.push(fineTuningJob);&#xA;  }&#xA;  return allFineTuningJobs;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can request a single page at a time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;let page = await client.fineTuning.jobs.list({ limit: 20 });&#xA;for (const fineTuningJob of page.data) {&#xA;  console.log(fineTuningJob);&#xA;}&#xA;&#xA;// Convenience methods are provided for manually paginating:&#xA;while (page.hasNextPage()) {&#xA;  page = await page.getNextPage();&#xA;  // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Realtime API Beta&lt;/h2&gt; &#xA;&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling&lt;/a&gt; through a &lt;code&gt;WebSocket&lt;/code&gt; connection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { OpenAIRealtimeWebSocket } from &#39;openai/beta/realtime/websocket&#39;;&#xA;&#xA;const rt = new OpenAIRealtimeWebSocket({ model: &#39;gpt-4o-realtime-preview-2024-12-17&#39; });&#xA;&#xA;rt.on(&#39;response.text.delta&#39;, (event) =&amp;gt; process.stdout.write(event.delta));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/realtime.md&#34;&gt;realtime.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The Azure API shape slightly differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { AzureOpenAI } from &#39;openai&#39;;&#xA;import { getBearerTokenProvider, DefaultAzureCredential } from &#39;@azure/identity&#39;;&#xA;&#xA;const credential = new DefaultAzureCredential();&#xA;const scope = &#39;https://cognitiveservices.azure.com/.default&#39;;&#xA;const azureADTokenProvider = getBearerTokenProvider(credential, scope);&#xA;&#xA;const openai = new AzureOpenAI({&#xA;  azureADTokenProvider,&#xA;  apiVersion: &#39;&amp;lt;The API version, e.g. 2024-10-01-preview&amp;gt;&#39;,&#xA;});&#xA;&#xA;const result = await openai.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say hello!&#39; }],&#xA;});&#xA;&#xA;console.log(result.choices[0]!.message?.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on support for the Azure API, see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/azure.md&#34;&gt;azure.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Accessing raw Response data (e.g., headers)&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;raw&#34; &lt;code&gt;Response&lt;/code&gt; returned by &lt;code&gt;fetch()&lt;/code&gt; can be accessed through the &lt;code&gt;.asResponse()&lt;/code&gt; method on the &lt;code&gt;APIPromise&lt;/code&gt; type that all methods return. This method returns as soon as the headers for a successful response are received and does not consume the response body, so you are free to write custom parsing or streaming logic.&lt;/p&gt; &#xA;&lt;p&gt;You can also use the &lt;code&gt;.withResponse()&lt;/code&gt; method to get the raw &lt;code&gt;Response&lt;/code&gt; along with the parsed data. Unlike &lt;code&gt;.asResponse()&lt;/code&gt; this method consumes the body, returning once it is parsed.&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const client = new OpenAI();&#xA;&#xA;const httpResponse = await client.responses&#xA;  .create({ model: &#39;gpt-4o&#39;, input: &#39;say this is a test.&#39; })&#xA;  .asResponse();&#xA;&#xA;// access the underlying web standard Response object&#xA;console.log(httpResponse.headers.get(&#39;X-My-Header&#39;));&#xA;console.log(httpResponse.statusText);&#xA;&#xA;const { data: modelResponse, response: raw } = await client.responses&#xA;  .create({ model: &#39;gpt-4o&#39;, input: &#39;say this is a test.&#39; })&#xA;  .withResponse();&#xA;console.log(raw.headers.get(&#39;X-My-Header&#39;));&#xA;console.log(modelResponse);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] All log messages are intended for debugging only. The format and content of log messages may change between releases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Log levels&lt;/h4&gt; &#xA;&lt;p&gt;The log level can be configured in two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Via the &lt;code&gt;OPENAI_LOG&lt;/code&gt; environment variable&lt;/li&gt; &#xA; &lt;li&gt;Using the &lt;code&gt;logLevel&lt;/code&gt; client option (overrides the environment variable if set)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  logLevel: &#39;debug&#39;, // Show all log messages&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available log levels, from most to least verbose:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;debug&#39;&lt;/code&gt; - Show debug messages, info, warnings, and errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;info&#39;&lt;/code&gt; - Show info messages, warnings, and errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;warn&#39;&lt;/code&gt; - Show warnings and errors (default)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;error&#39;&lt;/code&gt; - Show only errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;off&#39;&lt;/code&gt; - Disable all logging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At the &lt;code&gt;&#39;debug&#39;&lt;/code&gt; level, all HTTP requests and responses are logged, including headers and bodies. Some authentication-related headers are redacted, but sensitive data in request and response bodies may still be visible.&lt;/p&gt; &#xA;&lt;h4&gt;Custom logger&lt;/h4&gt; &#xA;&lt;p&gt;By default, this library logs to &lt;code&gt;globalThis.console&lt;/code&gt;. You can also provide a custom logger. Most logging libraries are supported, including &lt;a href=&#34;https://www.npmjs.com/package/pino&#34;&gt;pino&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/winston&#34;&gt;winston&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/bunyan&#34;&gt;bunyan&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/consola&#34;&gt;consola&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/signale&#34;&gt;signale&lt;/a&gt;, and &lt;a href=&#34;https://jsr.io/@std/log&#34;&gt;@std/log&lt;/a&gt;. If your logger doesn&#39;t work, please open an issue.&lt;/p&gt; &#xA;&lt;p&gt;When providing a custom logger, the &lt;code&gt;logLevel&lt;/code&gt; option still controls which messages are emitted, messages below the configured level will not be sent to your logger.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import pino from &#39;pino&#39;;&#xA;&#xA;const logger = pino();&#xA;&#xA;const client = new OpenAI({&#xA;  logger: logger.child({ name: &#39;OpenAI&#39; }),&#xA;  logLevel: &#39;debug&#39;, // Send all messages to pino, allowing it to filter&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; &#xA;&lt;p&gt;This library is typed for convenient access to the documented API. If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; &#xA;&lt;p&gt;To make requests to undocumented endpoints, you can use &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other HTTP verbs. Options on the client, such as retries, will be respected when making these requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;await client.post(&#39;/some/path&#39;, {&#xA;  body: { some_prop: &#39;foo&#39; },&#xA;  query: { some_query_arg: &#39;bar&#39; },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Undocumented request params&lt;/h4&gt; &#xA;&lt;p&gt;To make requests using undocumented parameters, you may use &lt;code&gt;// @ts-expect-error&lt;/code&gt; on the undocumented parameter. This library doesn&#39;t validate at runtime that the request matches the type, so any extra values you send will be sent as-is.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;client.chat.completions.create({&#xA;  // ...&#xA;  // @ts-expect-error baz is not yet public&#xA;  baz: &#39;undocumented option&#39;,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For requests with the &lt;code&gt;GET&lt;/code&gt; verb, any extra params will be in the query, all other requests will send the extra param in the body.&lt;/p&gt; &#xA;&lt;p&gt;If you want to explicitly send an extra argument, you can do so with the &lt;code&gt;query&lt;/code&gt;, &lt;code&gt;body&lt;/code&gt;, and &lt;code&gt;headers&lt;/code&gt; request options.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented response properties&lt;/h4&gt; &#xA;&lt;p&gt;To access undocumented response properties, you may access the response object with &lt;code&gt;// @ts-expect-error&lt;/code&gt; on the response object, or cast the response object to the requisite type. Like the request params, we do not validate or strip extra properties from the response from the API.&lt;/p&gt; &#xA;&lt;h3&gt;Customizing the fetch client&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use a different &lt;code&gt;fetch&lt;/code&gt; function, you can either polyfill the global:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import fetch from &#39;my-fetch&#39;;&#xA;&#xA;globalThis.fetch = fetch;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or pass it to the client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import fetch from &#39;my-fetch&#39;;&#xA;&#xA;const client = new OpenAI({ fetch });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fetch options&lt;/h3&gt; &#xA;&lt;p&gt;If you want to set custom &lt;code&gt;fetch&lt;/code&gt; options without overriding the &lt;code&gt;fetch&lt;/code&gt; function, you can provide a &lt;code&gt;fetchOptions&lt;/code&gt; object when instantiating the client or making a request. (Request-specific options override client options.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    // `RequestInit` options&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Configuring proxies&lt;/h4&gt; &#xA;&lt;p&gt;To modify proxy behavior, you can provide custom &lt;code&gt;fetchOptions&lt;/code&gt; that add runtime-specific proxy options to requests:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/node.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Node&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://github.com/nodejs/undici/raw/main/docs/docs/api/ProxyAgent.md#example---proxyagent-with-fetch&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import * as undici from &#39;undici&#39;;&#xA;&#xA;const proxyAgent = new undici.ProxyAgent(&#39;http://localhost:8888&#39;);&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    dispatcher: proxyAgent,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/bun.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Bun&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://bun.sh/guides/http/proxy&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    proxy: &#39;http://localhost:8888&#39;,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/deno.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Deno&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://docs.deno.com/api/deno/~/Deno.createHttpClient&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;npm:openai&#39;;&#xA;&#xA;const httpClient = Deno.createHttpClient({ proxy: { url: &#39;http://localhost:8888&#39; } });&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    client: httpClient,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;h2&gt;Semantic versioning&lt;/h2&gt; &#xA;&lt;p&gt;This package generally follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; &#xA; &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; &#xA;&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&#34;https://www.github.com/openai/openai-node/issues&#34;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;TypeScript &amp;gt;= 4.9 is supported.&lt;/p&gt; &#xA;&lt;p&gt;The following runtimes are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Node.js 20 LTS or later (&lt;a href=&#34;https://endoflife.date/nodejs&#34;&gt;non-EOL&lt;/a&gt;) versions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deno v1.28.0 or higher.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bun 1.0 or later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Cloudflare Workers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Vercel Edge Runtime.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jest 28 or greater with the &lt;code&gt;&#34;node&#34;&lt;/code&gt; environment (&lt;code&gt;&#34;jsdom&#34;&lt;/code&gt; is not supported at this time).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nitro v2.6 or greater.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Web browsers: disabled by default to avoid exposing your secret API credentials. Enable browser support by explicitly setting &lt;code&gt;dangerouslyAllowBrowser&lt;/code&gt; to true&#39;.&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;More explanation&lt;/summary&gt; &#xA;   &lt;h3&gt;Why is this dangerous?&lt;/h3&gt; &#xA;   &lt;p&gt;Enabling the &lt;code&gt;dangerouslyAllowBrowser&lt;/code&gt; option can be dangerous because it exposes your secret API credentials in the client-side code. Web browsers are inherently less secure than server environments, any user with access to the browser can potentially inspect, extract, and misuse these credentials. This could lead to unauthorized access using your credentials and potentially compromise sensitive data or functionality.&lt;/p&gt; &#xA;   &lt;h3&gt;When might this not be dangerous?&lt;/h3&gt; &#xA;   &lt;p&gt;In certain scenarios where enabling browser support might not pose significant risks:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Internal Tools: If the application is used solely within a controlled internal environment where the users are trusted, the risk of credential exposure can be mitigated.&lt;/li&gt; &#xA;    &lt;li&gt;Public APIs with Limited Scope: If your API has very limited scope and the exposed credentials do not grant access to sensitive data or critical operations, the potential impact of exposure is reduced.&lt;/li&gt; &#xA;    &lt;li&gt;Development or debugging purpose: Enabling this feature temporarily might be acceptable, provided the credentials are short-lived, aren&#39;t also used in production environments, or are frequently rotated.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;p&gt;Note that React Native is not supported at this time.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in other runtime environments, please open or upvote an issue on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/CONTRIBUTING.md&#34;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>