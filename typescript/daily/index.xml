<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-19T01:45:11Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>transitive-bullshit/OpenOpenAI</title>
    <updated>2023-11-19T01:45:11Z</updated>
    <id>tag:github.com,2023-11-19:/transitive-bullshit/OpenOpenAI</id>
    <link href="https://github.com/transitive-bullshit/OpenOpenAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Self-hosted version of OpenAIâ€™s new stateful Assistants API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenOpenAI &#xA; &lt;!-- omit in toc --&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Example usage&#34; src=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/media/screenshot.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/actions/workflows/test.yml&#34;&gt;&lt;img alt=&#34;Build Status&#34; src=&#34;https://github.com/transitive-bullshit/OpenOpenAI/actions/workflows/test.yml/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/raw/main/license&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/badge/license-MIT-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://prettier.io&#34;&gt;&lt;img alt=&#34;Prettier Code Formatting&#34; src=&#34;https://img.shields.io/badge/code_style-prettier-brightgreen.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#intro&#34;&gt;Intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#why&#34;&gt;Why?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#stack&#34;&gt;Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#development&#34;&gt;Development&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#environment-variables&#34;&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#services&#34;&gt;Services&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#e2e-examples&#34;&gt;E2E Examples&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#custom-function-example&#34;&gt;Custom Function Example&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#retrieval-tool-example&#34;&gt;Retrieval Tool Example&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#server-routes&#34;&gt;Server routes&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;This project is a self-hosted version of OpenAI&#39;s new stateful Assistants API.&lt;/strong&gt; ðŸ’ª&lt;/p&gt; &#xA;&lt;p&gt;All &lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/src/generated/oai-routes.ts&#34;&gt;API route definitions&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/src/generated/oai.ts&#34;&gt;types&lt;/a&gt; are &lt;strong&gt;100% auto-generated&lt;/strong&gt; from OpenAI&#39;s official OpenAPI spec, so all it takes to switch between the official API and your custom API is changing the &lt;code&gt;baseURL&lt;/code&gt;. ðŸ¤¯&lt;/p&gt; &#xA;&lt;p&gt;This means that all API parameters, responses, and types are wire-compatible with the official OpenAI API, and the fact that they&#39;re auto-generated means that it will be relatively easy to keep them in sync over time.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example using the official Node.js &lt;code&gt;openai&lt;/code&gt; package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;&#xA;&#xA;// The only difference is the `baseURL` pointing to your custom API server ðŸ”¥&#xA;const openai = new OpenAI({&#xA;  baseURL: &#39;http//:localhost:3000&#39;&#xA;})&#xA;&#xA;// Since the custom API is spec-compliant with OpenAI, you can use the sdk normally ðŸ’¯&#xA;const assistant = await openai.beta.assistants.create({&#xA;  model: &#39;gpt-4-1106-preview&#39;,&#xA;  instructions: &#39;You are a helpful assistant.&#39;&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Python example&lt;/summary&gt; &#xA; &lt;p&gt;Here&#39;s the same example using the official Python &lt;code&gt;openai&lt;/code&gt; package:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    base_url: &#34;http//:localhost:3000&#34;&#xA;)&#xA;&#xA;# Now you can use the sdk normally!&#xA;# (only file and beta assistant resources are currently supported)&#xA;# You can even switch back and forth between the official and custom APIs!&#xA;assistant = client.beta.assistants.create(&#xA;    model=&#34;gpt-4-1106-preview&#34;,&#xA;    description=&#34;You are a helpful assistant.&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Note that this project is not meant to be a full recreation of the entire OpenAI API. Rather, &lt;strong&gt;it is focused only on the stateful portions of the new Assistants API&lt;/strong&gt;. The following resource types are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Assistants&lt;/li&gt; &#xA; &lt;li&gt;AssistantFiles&lt;/li&gt; &#xA; &lt;li&gt;Files&lt;/li&gt; &#xA; &lt;li&gt;Messages&lt;/li&gt; &#xA; &lt;li&gt;MessageFiles&lt;/li&gt; &#xA; &lt;li&gt;Threads&lt;/li&gt; &#xA; &lt;li&gt;Runs&lt;/li&gt; &#xA; &lt;li&gt;RunSteps&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the official &lt;a href=&#34;https://platform.openai.com/docs/assistants/how-it-works&#34;&gt;OpenAI Assistants Guide&lt;/a&gt; for more info on how Assistants work.&lt;/p&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;p&gt;Being able to run your own, custom OpenAI Assistants that are &lt;strong&gt;100% compatible with the official OpenAI Assistants&lt;/strong&gt; unlocks all sorts of useful possibilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using OpenAI Assistants with &lt;strong&gt;custom models&lt;/strong&gt; (OSS ftw!) ðŸ’ª&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fully customizable RAG&lt;/strong&gt; via the built-in retrieval tool (LangChain and LlamaIndex integrations &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/2&#34;&gt;coming soon&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Using a &lt;strong&gt;custom code interpreter&lt;/strong&gt; like &lt;a href=&#34;https://github.com/KillianLucas/open-interpreter&#34;&gt;open-interpreter&lt;/a&gt; ðŸ”¥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-hosting / on-premise&lt;/strong&gt; deployments of Assistants&lt;/li&gt; &#xA; &lt;li&gt;Full control over &lt;strong&gt;assistant evals&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Developing &amp;amp; testing GPTs in fully &lt;strong&gt;sandboxed environments&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sandboxed testing of &lt;strong&gt;custom Actions&lt;/strong&gt; before deploying to the OpenAI &#34;GPT Store&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most importantly, if the OpenAI &#34;GPT Store&#34; ends up gaining traction with ChatGPT&#39;s 100M weekly active users, then &lt;strong&gt;the ability to reliably run, debug, and customize OpenAI-compatible Assistants&lt;/strong&gt; will end up being incredibly important in the future.&lt;/p&gt; &#xA;&lt;p&gt;I could even imagine a future Assistant store which is fully compatible with OpenAI&#39;s GPTs, but instead of relying on OpenAI as the gatekeeper, it could be &lt;strong&gt;fully or partially decentralized&lt;/strong&gt;. ðŸ’¯&lt;/p&gt; &#xA;&lt;h2&gt;Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.postgresql.org&#34;&gt;Postgres&lt;/a&gt; - Primary datastore via &lt;a href=&#34;https://www.prisma.io&#34;&gt;Prisma&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/prisma/schema.prisma&#34;&gt;schema file&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://redis.io&#34;&gt;Redis&lt;/a&gt; - Backing store for the async task queue used to process thread runs via &lt;a href=&#34;https://bullmq.io&#34;&gt;BullMQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/s3&#34;&gt;S3&lt;/a&gt; - Stores uploaded files &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Any S3-compatible storage provider is supported, such as &lt;a href=&#34;https://developers.cloudflare.com/r2/&#34;&gt;Cloudflare R2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hono.dev&#34;&gt;Hono&lt;/a&gt; - Serves the REST API via &lt;a href=&#34;https://github.com/honojs/middleware/tree/main/packages/zod-openapi&#34;&gt;@hono/zod-openapi&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We&#39;re using the &lt;a href=&#34;https://hono.dev/getting-started/nodejs&#34;&gt;Node.js&lt;/a&gt; adaptor by default, but Hono supports many environments including CF workers, Vercel, Netlify, Deno, Bun, Lambda, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dexaai/dexter&#34;&gt;Dexter&lt;/a&gt; - Production RAG by &lt;a href=&#34;https://dexa.ai&#34;&gt;Dexa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.typescriptlang.org&#34;&gt;TypeScript&lt;/a&gt; ðŸ’•&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Prerequisites:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en&#34;&gt;node&lt;/a&gt; &amp;gt;= 18&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pnpm.io&#34;&gt;pnpm&lt;/a&gt; &amp;gt;= 8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install deps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate the prisma types locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Postgres&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;DATABASE_URL&lt;/code&gt; - Postgres connection string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://wiki.postgresql.org/wiki/Homebrew&#34;&gt;On macOS&lt;/a&gt;: &lt;code&gt;brew install postgresql &amp;amp;&amp;amp; brew services start postgresql&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You&#39;ll need to run &lt;code&gt;npx prisma db push&lt;/code&gt; to set up your database according to our &lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/prisma/schema.prisma&#34;&gt;prisma schema&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; - OpenAI API key for running the underlying chat completion calls&lt;/li&gt; &#xA;   &lt;li&gt;This is required for now, but depending on &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/1&#34;&gt;how interested people are&lt;/a&gt;, it won&#39;t be hard to add support for local models and other providers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Redis&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://redis.io/docs/install/install-redis/install-redis-on-mac-os/&#34;&gt;On macOS&lt;/a&gt;: &lt;code&gt;brew install redis &amp;amp;&amp;amp; brew services start redis&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If you have a local redis instance running, the default redis env vars should work without touching them&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;REDIS_HOST&lt;/code&gt; - Optional; defaults to &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;REDIS_PORT&lt;/code&gt; - Optional; defaults to &lt;code&gt;6379&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;REDIS_USERNAME&lt;/code&gt; - Optional; defaults to &lt;code&gt;default&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;REDIS_PASSWORD&lt;/code&gt; - Optional&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;S3&lt;/strong&gt; - Required to use file attachments &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Any S3-compatible provider is supported, such as &lt;a href=&#34;https://developers.cloudflare.com/r2/&#34;&gt;Cloudflare R2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Alterantively, you can use a local S3 server like &lt;a href=&#34;https://github.com/minio/minio#homebrew-recommended&#34;&gt;MinIO&lt;/a&gt; or &lt;a href=&#34;https://github.com/localstack/localstack&#34;&gt;LocalStack&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;To run LocalStack on macOS: &lt;code&gt;brew install localstack/tap/localstack-cli &amp;amp;&amp;amp; localstack start -d&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;To run MinIO macOS: &lt;code&gt;brew install minio/stable/minio &amp;amp;&amp;amp; minio server /data&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;I recommend using Cloudflare R2, though â€“&amp;nbsp;it&#39;s amazing and should be free for most use cases!&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;S3_BUCKET&lt;/code&gt; - Required&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;S3_REGION&lt;/code&gt; - Optional; defaults to &lt;code&gt;auto&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;S3_ENDPOINT&lt;/code&gt; - Required; example: &lt;code&gt;https://&amp;lt;id&amp;gt;.r2.cloudflarestorage.com&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ACCESS_KEY_ID&lt;/code&gt; - Required (&lt;a href=&#34;https://developers.cloudflare.com/r2/api/s3/tokens/&#34;&gt;cloudflare R2 docs&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SECRET_ACCESS_KEY&lt;/code&gt; - Required (&lt;a href=&#34;https://developers.cloudflare.com/r2/api/s3/tokens/&#34;&gt;cloudflare R2 docs&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;p&gt;The app is composed of two services: a RESTful API &lt;strong&gt;server&lt;/strong&gt; and an async task &lt;strong&gt;runner&lt;/strong&gt;. Both services are stateless and can be scaled horizontally.&lt;/p&gt; &#xA;&lt;p&gt;There are two ways to run these services locally. The quickest way is via &lt;code&gt;tsx&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start the REST API server in one shell&#xA;npx tsx src/server&#xA;&#xA;# Start an async task queue runner in another shell&#xA;npx tsx src/runner&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can transpile the source TS to JS first, which is preferred for running in production:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm build&#xA;&#xA;# Start the REST API server in one shell&#xA;npx tsx dist/server&#xA;&#xA;# Start an async task queue runner in another shell&#xA;npx tsx dist/runner&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;E2E Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Custom Function Example&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/e2e/index.ts&#34;&gt;This example&lt;/a&gt; contains an end-to-end assistant script which uses a custom &lt;code&gt;get_weather&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;p&gt;You can run it using the official &lt;a href=&#34;https://github.com/openai/openai-node&#34;&gt;openai&lt;/a&gt; client for Node.js against the default OpenAI API hosted at &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx tsx e2e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the same test suite against your local API, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_BASE_URL=&#39;http://127.0.0.1:3000&#39; npx tsx e2e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s pretty cool to see both test suites running the exact same Assistants code using the official OpenAI Node.js client â€“ without any noticeable differences between the two versions. Huzzah! ðŸ¥³&lt;/p&gt; &#xA;&lt;h4&gt;Retrieval Tool Example&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/e2e/retrieval.ts&#34;&gt;This example&lt;/a&gt; contains an end-to-end assistant script which uses the built-in &lt;code&gt;retrieval&lt;/code&gt; tool with this &lt;code&gt;readme.md&lt;/code&gt; file as an attachment.&lt;/p&gt; &#xA;&lt;p&gt;You can run it using the official &lt;a href=&#34;https://github.com/openai/openai-node&#34;&gt;openai&lt;/a&gt; client for Node.js against the default OpenAI API hosted at &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx tsx e2e/retrieval.ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the same test suite against your local API, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_BASE_URL=&#39;http://127.0.0.1:3000&#39; npx tsx e2e/retrieval.ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output will likely differ slightly due to differences in OpenAI&#39;s built-in retrieval implementation and &lt;a href=&#34;https://raw.githubusercontent.com/transitive-bullshit/OpenOpenAI/main/src/lib/retrieval.ts&#34;&gt;our default, naive retrieval implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/raw/main/src/lib/retrieval.ts&#34;&gt;current &lt;code&gt;retrieval&lt;/code&gt; implementation&lt;/a&gt; only support text files like &lt;code&gt;text/plain&lt;/code&gt; and markdown, as no preprocessing or conversions are done at the moment. We also use a very naive retrieval method at the moment which always returns the full file contents as opposed to pre-processing them and only returning the most semantically relevant chunks. See &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/2&#34;&gt;this issue&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;h3&gt;Server routes&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET       /files&#xA;POST      /files&#xA;DELETE    /files/:file_id&#xA;GET       /files/:file_id&#xA;GET       /files/:file_id/content&#xA;GET       /assistants&#xA;POST      /assistants&#xA;GET       /assistants/:assistant_id&#xA;POST      /assistants/:assistant_id&#xA;DELETE    /assistants/:assistant_id&#xA;GET       /assistants/:assistant_id/files&#xA;GET       /assistants/:assistant_id/files&#xA;POST      /assistants/:assistant_id/files&#xA;DELETE    /assistants/:assistant_id/files/:file_id&#xA;GET       /assistants/:assistant_id/files/:file_id&#xA;POST      /threads&#xA;GET       /threads/:thread_id&#xA;POST      /threads/:thread_id&#xA;DELETE    /threads/:thread_id&#xA;GET       /threads/:thread_id/messages&#xA;POST      /threads/:thread_id/messages&#xA;GET       /threads/:thread_id/messages/:message_id&#xA;POST      /threads/:thread_id/messages/:message_id&#xA;GET       /threads/:thread_id/messages/:message_id/files&#xA;GET       /threads/:thread_id/messages/:message_id/files/:file_id&#xA;GET       /threads/:thread_id/runs&#xA;POST      /threads/runs&#xA;POST      /threads/:thread_id/runs&#xA;GET       /threads/:thread_id/runs/:run_id&#xA;POST      /threads/:thread_id/runs/:run_id&#xA;POST      /threads/:thread_id/runs/:run_id/submit_tool_outputs&#xA;POST      /threads/:thread_id/runs/:run_id/cancel&#xA;GET       /threads/:thread_id/runs/:run_id/steps&#xA;GET       /threads/:thread_id/runs/:run_id/steps/:step_id&#xA;GET       /openapi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the server&#39;s auto-generated openapi spec by running the server and then visiting &lt;code&gt;http://127.0.0.1:3000/openapi&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Status&lt;/strong&gt;: All API routes have been tested side-by-side with the official OpenAI API and are working as expected. The only missing features at the moment are support for the built-in &lt;code&gt;code_interpreter&lt;/code&gt; tool (&lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/3&#34;&gt;issue&lt;/a&gt;) and support for non-text files with the built-in &lt;code&gt;retrieval&lt;/code&gt; tool (&lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/2&#34;&gt;issue&lt;/a&gt;). All other functionality should be fully supported and wire-compatible with the official API.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hosted demo (bring your own OpenAI API key?)&lt;/li&gt; &#xA; &lt;li&gt;get hosted redis working&lt;/li&gt; &#xA; &lt;li&gt;handle locking thread and messages &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;not sure how this works exactly, but according to the &lt;a href=&#34;https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps&#34;&gt;OpenAI Assistants Guide&lt;/a&gt;, threads are locked while runs are being processed&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;built-in &lt;code&gt;code_interpreter&lt;/code&gt; tool (&lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/3&#34;&gt;issue&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;support non-text files w/ built-in &lt;code&gt;retrieval&lt;/code&gt; tool (&lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/2&#34;&gt;issue&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;openai uses prefix IDs for its resources, which would be great, except it&#39;s a pain to get working with Prisma (&lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/issues/7&#34;&gt;issue&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;figure out why localhost resolution wasn&#39;t working for &lt;a href=&#34;https://github.com/transitive-bullshit/OpenOpenAI/pull/6&#34;&gt;#6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;handle context overflows (truncation for now)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT Â© &lt;a href=&#34;https://transitivebullsh.it&#34;&gt;Travis Fischer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you found this project useful, please consider &lt;a href=&#34;https://github.com/sponsors/transitive-bullshit&#34;&gt;sponsoring me&lt;/a&gt; or &lt;a href=&#34;https://twitter.com/transitive_bs&#34;&gt;following me on twitter &lt;img src=&#34;https://storage.googleapis.com/saasify-assets/twitter-logo.svg?sanitize=true&#34; alt=&#34;twitter&#34; height=&#34;24px&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>