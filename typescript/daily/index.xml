<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-22T01:41:34Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AntonioErdeljac/next13-lms-platform</title>
    <updated>2023-09-22T01:41:34Z</updated>
    <id>tag:github.com,2023-09-22:/AntonioErdeljac/next13-lms-platform</id>
    <link href="https://github.com/AntonioErdeljac/next13-lms-platform" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build an LMS Platform: Next.js 13, React, Stripe, Mux, Prisma, Tailwind, MySQL | Full Course 2023&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AntonioErdeljac/next13-lms-platform/assets/23248726/fa077fca-bb74-419a-84de-54ac103bb026&#34; alt=&#34;Copy of Copy of Copy of Copy of Fullstack Twitter Clone (9)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a repository for Build an LMS Platform: Next.js 13, React, Stripe, Mux, Prisma, Tailwind, MySQL | Full Course 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Big_aFLmekI&#34;&gt;VIDEO TUTORIAL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Key Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Browse &amp;amp; Filter Courses&lt;/li&gt; &#xA; &lt;li&gt;Purchase Courses using Stripe&lt;/li&gt; &#xA; &lt;li&gt;Mark Chapters as Completed or Uncompleted&lt;/li&gt; &#xA; &lt;li&gt;Progress Calculation of each Course&lt;/li&gt; &#xA; &lt;li&gt;Student Dashboard&lt;/li&gt; &#xA; &lt;li&gt;Teacher mode&lt;/li&gt; &#xA; &lt;li&gt;Create new Courses&lt;/li&gt; &#xA; &lt;li&gt;Create new Chapters&lt;/li&gt; &#xA; &lt;li&gt;Easily reorder chapter position with drag n’ drop&lt;/li&gt; &#xA; &lt;li&gt;Upload thumbnails, attachments and videos using UploadThing&lt;/li&gt; &#xA; &lt;li&gt;Video processing using Mux&lt;/li&gt; &#xA; &lt;li&gt;HLS Video player using Mux&lt;/li&gt; &#xA; &lt;li&gt;Rich text editor for chapter description&lt;/li&gt; &#xA; &lt;li&gt;Authentication using Clerk&lt;/li&gt; &#xA; &lt;li&gt;ORM using Prisma&lt;/li&gt; &#xA; &lt;li&gt;MySQL database using Planetscale&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Node version 18.x.x&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cloning the repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/AntonioErdeljac/next13-lms-platform.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install packages&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup .env file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=&#xA;CLERK_SECRET_KEY=&#xA;NEXT_PUBLIC_CLERK_SIGN_IN_URL=&#xA;NEXT_PUBLIC_CLERK_SIGN_UP_URL=&#xA;NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=&#xA;NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=&#xA;&#xA;DATABASE_URL=&#xA;&#xA;UPLOADTHING_SECRET=&#xA;UPLOADTHING_APP_ID=&#xA;&#xA;MUX_TOKEN_ID=&#xA;MUX_TOKEN_SECRET=&#xA;&#xA;STRIPE_API_KEY=&#xA;NEXT_PUBLIC_APP_URL=http://localhost:3000&#xA;STRIPE_WEBHOOK_SECRET=&#xA;&#xA;NEXT_PUBLIC_TEACHER_ID=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup Prisma&lt;/h3&gt; &#xA;&lt;p&gt;Add MySQL Database (I used PlanetScale)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npx prisma generate&#xA;npx prisma db push&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start the app&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available commands&lt;/h2&gt; &#xA;&lt;p&gt;Running commands with npm &lt;code&gt;npm run [command]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;command&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;dev&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Starts a development instance of the app&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/aws-genai-llm-chatbot</title>
    <updated>2023-09-22T01:41:34Z</updated>
    <id>tag:github.com,2023-09-22:/aws-samples/aws-genai-llm-chatbot</id>
    <link href="https://github.com/aws-samples/aws-genai-llm-chatbot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modular and comprehensive solution to deploy a multi LLM powered chatbot (Amazon Bedrock, Anthropic, HuggingFace, OpenAI, AI21, Cohere) using AWS CDK on AWS&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deploy a multi LLM and multi RAG powered chatbot using AWS CDK on AWS&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/sample.gif&#34; alt=&#34;sample&#34; title=&#34;AWS GenAI Chatbot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#precautions&#34;&gt;Precautions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#service-quotas-and-preview-access&#34;&gt;Preview Access and Service Quotas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#providers&#34;&gt;Models Providers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#amazon-bedrock-preview&#34;&gt;Amazon Bedrock&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#self-hosted-models-on-sagemaker&#34;&gt;Self-hosted models on SageMaker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#3p-models-providers&#34;&gt;3P models providers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#retrieval-augmented-generation-rag-cdk-constructs&#34;&gt;RAG Sources&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#amazon-aurora-with-pgvector&#34;&gt;Aurora Serverless with pgvector&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#amazon-opensearch-vectorsearch-requires-bedrock-preview-access&#34;&gt;Amazon OpenSearch VectorSearch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#amazon-kendra&#34;&gt;Amazon Kendra&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#deploy&#34;&gt;Deploy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#clean-up&#34;&gt;Clean up&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;h2&gt;Modular, comprehensive and ready to use&lt;/h2&gt; &#xA;&lt;p&gt;This sample provides code ready to use so you can start &lt;strong&gt;experimenting with different LLMs and prompts.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Supported models providers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt; (&lt;em&gt;currently in preview&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;Amazon SageMaker&lt;/a&gt; self hosted models from Foundation, Jumpstart and HuggingFace.&lt;/li&gt; &#xA; &lt;li&gt;External providers via API such as AI21 Labs, Cohere, OpenAI, etc. &lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/&#34;&gt;See available langchain integrations&lt;/a&gt; for a comprehensive list.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Multiple RAG sources&lt;/h2&gt; &#xA;&lt;p&gt;This sample provides comes with CDK constructs to allow you to optionally deploy one or more of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-aurora-postgresql-pgvector-vector-storage-similarity-search/&#34;&gt;Amazon Aurora Serverless with pgvector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/&#34;&gt;Amazon OpenSearch VectorSearch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/kendra/&#34;&gt;Amazon Kendra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/kendra-rag-sample.gif&#34; width=&#34;95%&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/opensearchvectorsearch-rag-sample.gif&#34; with=&#34;97%&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Example with Kendra as RAG source&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Example with Amazon OpenSearch Vector Search as RAG source&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Full-fledged User Interface&lt;/h2&gt; &#xA;&lt;p&gt;The repository includes a CDK construct to deploy a &lt;strong&gt;full-fledged UI&lt;/strong&gt; built with &lt;a href=&#34;https://react.dev/&#34;&gt;React&lt;/a&gt; to interact with the deployed LLMs as chatbots. Hosted on &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;Amazon S3&lt;/a&gt; and distributed with &lt;a href=&#34;https://aws.amazon.com/cloudfront/&#34;&gt;Amazon CloudFront&lt;/a&gt;. Protected with &lt;a href=&#34;https://aws.amazon.com/cognito/&#34;&gt;Amazon Cognito Authentication&lt;/a&gt; to help you interact and experiment with multiple LLMs, multiple RAG sources, conversational history support and documents upload. The interface layer between the UI and backend is build on top of &lt;a href=&#34;https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html&#34;&gt;Amazon API Gateway WebSocket APIs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Build on top of &lt;a href=&#34;https://cloudscape.design/&#34;&gt;AWS Cloudscape Design System&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p&gt;This repository comes with several reusable CDK constructs. Giving you freedom to decide what the deploy and what not.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an overview:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/architecture.png&#34; alt=&#34;sample&#34; title=&#34;Architecture Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Available CDK Constructs&lt;/h2&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/authentication/index.ts&#34;&gt;CDK constructs&lt;/a&gt; provides necessary Amazon Cognito resources to support user authentication.&lt;/p&gt; &#xA;&lt;h3&gt;Websocket Interface&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/websocket-interface/index.ts&#34;&gt;CDK constructs&lt;/a&gt; deployes a websocket based interface layer to allow two-way communication between the user interface and the model interface.&lt;/p&gt; &#xA;&lt;h3&gt;Main Topic and Queues - FIFO&lt;/h3&gt; &#xA;&lt;p&gt;This is not a CDK construct but it&#39;s important to note that messages are delivered via &lt;a href=&#34;https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html&#34;&gt;Amazon SQS FIFO&lt;/a&gt; queues and routed via an &lt;a href=&#34;https://aws.amazon.com/blogs/aws/introducing-amazon-sns-fifo-first-in-first-out-pub-sub-messaging/&#34;&gt;Amazon SNS FIFO Topic&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FIFO is used to ensure the correct order of messages inflow/outflow to keep a &#34;chatbot conversation&#34; always consistent for both user and LLM. Also to ensure that, where streaming tokens, is used the order of tokens is also always respected.&lt;/p&gt; &#xA;&lt;h3&gt;Model Interface&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/&#34;&gt;CDK constructs&lt;/a&gt; which deploye resources, dependencies and data storage to integrate with multiple LLM sources and providers. To facilitate further integrations and future updates and reduce amount of customization required, we provide code built with known existing LLM oriented frameworks.&lt;/p&gt; &#xA;&lt;p&gt;Pre-built model interafaces:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/index.ts&#34;&gt;LangchainModelInterface&lt;/a&gt;: python-centric and built on top of &lt;a href=&#34;https://python.langchain.com/docs/get_started/introduction.html&#34;&gt;Langchain framework&lt;/a&gt; and leveraging &lt;a href=&#34;https://aws.amazon.com/dynamodb/&#34;&gt;Amazon DynamoDB&lt;/a&gt; as &lt;a href=&#34;https://python.langchain.com/docs/integrations/memory/dynamodb_chat_message_history&#34;&gt;LangChain Memory&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Model Adapters&lt;/h4&gt; &#xA;&lt;p&gt;The model interface carries a concept of &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/functions/request-handler/adapters/base/base.py&#34;&gt;ModelAdapter&lt;/a&gt; with it. It&#39;s a class that you can inherit and ovveride specific methods to integrate with different models that might have different requirements in terms of prompt structure or parameters.&lt;/p&gt; &#xA;&lt;p&gt;It also natively support subscription to &lt;a href=&#34;https://python.langchain.com/docs/modules/callbacks/&#34;&gt;LangChain Callback Handlers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository provides some &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/functions/request-handler/adapters/&#34;&gt;sample adapetrs&lt;/a&gt; that you can take inspiration from to integrate with other models. Read more about it &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SageMaker Model&lt;/h3&gt; &#xA;&lt;p&gt;A prupose-built CDK Construct, &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/sagemaker-model/index.ts&#34;&gt;SageMakerModel&lt;/a&gt;, which helps facilitate the deployment of model to SageMaker, you can use this layer to deploy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Models from SageMaker Foundation Models/Jumpstart&lt;/li&gt; &#xA; &lt;li&gt;Model supported by &lt;a href=&#34;https://huggingface.co/blog/sagemaker-huggingface-llm&#34;&gt;HuggingFace LLM Inference container&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Models from HuggingFace with custom inference code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Layer&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/layer/index.ts&#34;&gt;Layer construct&lt;/a&gt; in CDK provides an easier mechanism to manage and deploy AWS Lambda layers. You can specify dependencies and requirements in a local folder and the layer will pack, zip and upload the depedencies autonomously to S3 and generate the Lambda Layer.&lt;/p&gt; &#xA;&lt;h3&gt;VPC&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/vpc/index.ts&#34;&gt;CDK construct&lt;/a&gt; simply deploys public, private, and isolated subnets. Additionally, this stack deploys VPC endpoints for SageMaker endpoints, AWS Secrets Manager, S3, and Amazon DynamoDB, ensuring that traffic stays within the VPC when appropriate.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieval Augmented Generation (RAG) CDK Constructs&lt;/h2&gt; &#xA;&lt;p&gt;This repo also comes with &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/rag-sources/&#34;&gt;CDK constructs&lt;/a&gt; to help you getting started with pre-built RAG sources.&lt;/p&gt; &#xA;&lt;p&gt;All RAG constructs leverages the same pattern of implementing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An ingestion queue to recieve upload/delete S3 events for documents&lt;/li&gt; &#xA; &lt;li&gt;An ingestion, converstion and storage mechanism which is specific to the RAG source&lt;/li&gt; &#xA; &lt;li&gt;An API endpoint to expose RAG results to consumers, in our case the model interface.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In this sample each RAG sources is exposes endpoints and formats results in order to be used as &lt;a href=&#34;https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/remote-retriever&#34;&gt;LangChain RemoteRetriever&lt;/a&gt; from the Model Interface as part of a &lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db&#34;&gt;ConversationalRetrievalChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This aims to allow seamless integration with Langchain chains and workflows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/rag.png&#34; alt=&#34;sample&#34; title=&#34;Architecture Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Amazon Aurora with pgvector&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/rag-sources/aurora-pgvector/index.ts&#34;&gt;CDK construct&lt;/a&gt; deployes a &lt;strong&gt;vector database&lt;/strong&gt; on &lt;strong&gt;Amazon Aurora PostgreSQL&lt;/strong&gt; with &lt;strong&gt;pgvector&lt;/strong&gt; and embeddings.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Embeddings Model&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&#34;&gt;sentence-transformers/all-MiniLM-L6-v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Ranking Model&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2&#34;&gt;cross-encoder/ms-marco-MiniLM-L-12-v2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hybrid search&lt;/strong&gt; is performed with a combination of&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Similiary Search&lt;/li&gt; &#xA; &lt;li&gt;Full Text Search&lt;/li&gt; &#xA; &lt;li&gt;Reranking of results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/assets/vectordb-query.jpg&#34; alt=&#34;sample&#34; title=&#34;Life of a query&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L251&#34;&gt;here&lt;/a&gt; to learn how to enable it in the stack.&lt;/p&gt; &#xA;&lt;h3&gt;Amazon OpenSearch VectorSearch (requires Bedrock Preview Access)&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/rag-sources/opensearch-vectorsearch/index.ts&#34;&gt;CDK construct&lt;/a&gt; deployes a AOSS vector database capabilities with required collection, VPC endpoints, data access, encryption policies and a an index that can be used with embeddings produced by &lt;a href=&#34;https://aws.amazon.com/bedrock/titan/&#34;&gt;Amazon Titan Embeddings&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Embeddings Model&lt;/code&gt;: &lt;a href=&#34;https://aws.amazon.com/bedrock/titan/&#34;&gt;Amazon Titan Embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L232&#34;&gt;here&lt;/a&gt; to learn how to enable it in the stack.&lt;/p&gt; &#xA;&lt;h3&gt;Amazon Kendra&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/rag-sources/kendra-search/index.ts&#34;&gt;CDK Construct&lt;/a&gt; deployes an &lt;a href=&#34;https://docs.aws.amazon.com/kendra/latest/dg/hiw-index.html&#34;&gt;Amazon Kendra Index&lt;/a&gt; and necessary resoures to ingest documents and search them via &lt;a href=&#34;https://python.langchain.com/docs/integrations/retrievers/amazon_kendra_retriever&#34;&gt;LangChain Amazon Kendra Index Retriever&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to review &lt;a href=&#34;https://aws.amazon.com/kendra/pricing/&#34;&gt;Amazon Kendra Pricing&lt;/a&gt; before deploying it.&lt;/p&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L206&#34;&gt;here&lt;/a&gt; to learn how to enable it in the stack.&lt;/p&gt; &#xA;&lt;h1&gt;⚠️ Precautions ⚠️&lt;/h1&gt; &#xA;&lt;p&gt;Before you begin using the sample, there are certain precautions you must take into account:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cost Management with self hosted models&lt;/strong&gt;: Be mindful of the costs associated with AWS resources, especially with SageMaker models which are billed by the hour. While the sample is designed to be cost-effective, leaving serverful resources running for extended periods or deploying numerous LLMs can quickly lead to increased costs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Licensing obligations&lt;/strong&gt;: If you choose to use any datasets or models alongside the provided samples, ensure you check LLM code and comply with all licensing obligations attached to them.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;This is a sample&lt;/strong&gt;: the code provided as part of this repository shouldn&#39;t be used for production workloads without further reviews and adaptation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Preview Access and Service Quotas&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Amazon Bedrock&lt;/strong&gt; If you are looking to interact with models from Amazon Bedrock FMs, you need to request preview access from the AWS console. Futhermore, make sure which regions are currently supported for Amazon Bedrock.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Instance type quota increase&lt;/strong&gt; You might consider requesting an increase in service quota for specific SageMaker instance types such as the &lt;code&gt;ml.g5&lt;/code&gt; instance type. This will give access to latest generation of GPU/Multi-GPU instances types. You can do this from the AWS console.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Foundation Models Preview Access&lt;/strong&gt; If you are looking to deploy models from SageMaker foundation models, you need to request preview access from the AWS console. Futhermore, make sure which regions are currently supported for SageMaker foundation models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Providers&lt;/h1&gt; &#xA;&lt;h2&gt;Amazon Bedrock (Preview)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt; is a fully managed service that makes foundation models (FMs) from Amazon and leading AI startups available through an API, so you can choose from various FMs to find the model that&#39;s best suited for your use case. With the Amazon Bedrock serverless experience, you can quickly get started, easily experiment with FMs, privately customize FMs with your own data, and seamlessly integrate and deploy them into your applications using AWS tools and capabilities.&lt;/p&gt; &#xA;&lt;p&gt;If your account has access to Amazon Bedrock, there&#39;s no additional action required and you can deploy this sample as it is and Bedrock models will appear in your model list.&lt;/p&gt; &#xA;&lt;h2&gt;Self Hosted Models on SageMaker&lt;/h2&gt; &#xA;&lt;p&gt;This sample comes with a prupose-built CDK Construct, &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/sagemaker-model/index.ts&#34;&gt;SageMakerModel&lt;/a&gt;, which helps abstracting 3 different types of model deployments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Models from SageMaker Foundation Models/Jumpstart.&lt;/li&gt; &#xA; &lt;li&gt;Model supported by HuggingFace LLM Inference container.&lt;/li&gt; &#xA; &lt;li&gt;Models from HuggingFace with custom inference code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Read more details &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/sagemaker-model/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3P Models Providers&lt;/h2&gt; &#xA;&lt;p&gt;You can also interact with external providers via their API such as AI21 Labs, Cohere, OpenAI, etc.&lt;/p&gt; &#xA;&lt;p&gt;The provider must be supported in the &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/functions/request-handler/index.py&#34;&gt;Model Interface&lt;/a&gt;, &lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/&#34;&gt;see available langchain integrations&lt;/a&gt; for a comprehensive list of providers.&lt;/p&gt; &#xA;&lt;p&gt;Usually an &lt;code&gt;API_KEY&lt;/code&gt; is required to integrated with 3P models. To do so, the &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/index.ts&#34;&gt;Model Interface&lt;/a&gt; deployes a Secrets in &lt;a href=&#34;https://aws.amazon.com/secrets-manager/&#34;&gt;AWS Secrets Manager&lt;/a&gt;, intially with an empty JSON &lt;code&gt;{}&lt;/code&gt;, where you can add your API KEYS for one or more providers.&lt;/p&gt; &#xA;&lt;p&gt;These keys will be injected at runtime into the Lambda function Environment Variables, they won&#39;t be visibile in the AWS Lambda Console.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you wish to be able to interact with AI21 Labs., OpenAI&#39;s and Cohere endponts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/model-interfaces/langchain/index.ts#L38&#34;&gt;Model Interface Keys Secret&lt;/a&gt; in Secrets Manager. You can find the secret name in the stack output too.&lt;/li&gt; &#xA; &lt;li&gt;Update the Secrets by adding a key to the JSON&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;AI21_API_KEY&#34;: &#34;xxxxx&#34;,&#xA;  &#34;OPENAI_API_KEY&#34;: &#34;sk-xxxxxxxxxxxxxxx&#34;,&#xA;  &#34;COHERE_API_KEY&#34;: &#34;xxxxx&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;N.B: In case of no keys needs, the secret value must be an empty JSON &lt;code&gt;{}&lt;/code&gt;, NOT an empty string &lt;code&gt;&#39;&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;make sure that the environment variable matches what is expected by the framework in use, like Langchain (&lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/&#34;&gt;see available langchain integrations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Deploy&lt;/h1&gt; &#xA;&lt;h3&gt;1. IMPORTANT Prerequisites for models providers&lt;/h3&gt; &#xA;&lt;p&gt;⚠️ IMPORTANT: Depending on the Model Provider you want to use there are different prerequisites. ⚠️&lt;/p&gt; &#xA;&lt;h3&gt;WITH Amazon Bedrock&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use Amazon Bedrock you must sign up for preview access from the AWS console.&lt;/p&gt; &#xA;&lt;p&gt;If access is granted you need to add the &lt;code&gt;region&lt;/code&gt; and &lt;code&gt;endpoint_url&lt;/code&gt; provided as part of the preview access in &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L36&#34;&gt;lib/aws-genai-llm-chatbot-stack.ts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;const bedrockRegion = &#39;region&#39;;&#xA;const bedrockEndpointUrl = &#39;https://endpoint-url&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this you can jump to the next step: &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#2-environment-setup&#34;&gt;Enviroment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;WITHOUT Amazon Bedrock&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t have access to Amazon Bedrock you can choose to:&lt;/p&gt; &#xA;&lt;h4&gt;a. Deploy a self hosted model on Sagemaker.&lt;/h4&gt; &#xA;&lt;p&gt;To facilitate this steps there are &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L96&#34;&gt;2 commented examples&lt;/a&gt; on how to deploy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L98&#34;&gt;amazon/FalconLite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/aws-genai-llm-chatbot-stack.ts#L98&#34;&gt;LLama2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More instructions on how to deploy other models &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/lib/sagemaker-model/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;b. Interact with a 3P models providers&lt;/h4&gt; &#xA;&lt;p&gt;You can find how &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#3p-models-providers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;(Optional) If using AWS Cloud9&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to use &lt;a href=&#34;https://aws.amazon.com/cloud9/&#34;&gt;AWS Cloud9&lt;/a&gt; to deploy the solution from you will need the following before proceeding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;at least &lt;code&gt;m5.large&lt;/code&gt; as Instance type.&lt;/li&gt; &#xA; &lt;li&gt;use &lt;code&gt;Amazon Linux 2&lt;/code&gt; as the platform.&lt;/li&gt; &#xA; &lt;li&gt;increase the instance&#39;s EBS volume size to at least 100GB. To do this, run the following commands from the Cloud9 terminal. See the documentation for more details &lt;a href=&#34;https://docs.aws.amazon.com/cloud9/latest/user-guide/move-environment.html#move-environment-resize&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;./assets/cloud9-resize.sh 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Environment setup&lt;/h3&gt; &#xA;&lt;p&gt;Verify that your environment satisfies the following prerequisites:&lt;/p&gt; &#xA;&lt;p&gt;You have:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/&#34;&gt;AWS account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AdministratorAccess&lt;/code&gt; policy granted to your AWS account (for production, we recommend restricting access as needed)&lt;/li&gt; &#xA; &lt;li&gt;Both console and programmatic access&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34;&gt;NodeJS 16 or 18&lt;/a&gt; installed &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you are using &lt;a href=&#34;https://github.com/nvm-sh/nvm&#34;&gt;&lt;code&gt;nvm&lt;/code&gt;&lt;/a&gt; you can run the following before proceeding&lt;/li&gt; &#xA;   &lt;li&gt; &lt;pre&gt;&lt;code&gt;nvm install 16 &amp;amp;&amp;amp; nvm use 16&#xA;&#xA;or&#xA;&#xA;nvm install 18 &amp;amp;&amp;amp; nvm use 18&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/cli/&#34;&gt;AWS CLI&lt;/a&gt; installed and configured to use with your AWS account&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.typescriptlang.org/download&#34;&gt;Typescript 3.8+&lt;/a&gt; installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html&#34;&gt;AWS CDK CLI&lt;/a&gt; installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt; installed &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;N.B. &lt;a href=&#34;https://github.com/docker/buildx&#34;&gt;&lt;code&gt;buildx&lt;/code&gt;&lt;/a&gt; is also required. For Windows and macOS &lt;code&gt;buildx&lt;/code&gt; &lt;a href=&#34;https://github.com/docker/buildx#windows-and-macos&#34;&gt;is included&lt;/a&gt; in &lt;a href=&#34;https://docs.docker.com/desktop/&#34;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3+&lt;/a&gt; installed&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;3. Prepare CDK&lt;/h3&gt; &#xA;&lt;p&gt;The solution will be deployed into your AWS account using infrastructure-as-code wih the &lt;a href=&#34;https://aws.amazon.com/cdk/&#34;&gt;AWS Cloud Development Kit&lt;/a&gt; (CDK).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/aws-samples/aws-genai-llm-chatbot.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Navigate to this project on your computer using your terminal:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd aws-genai-llm-chatbot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the project dependencies by running this command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Bootstrap AWS CDK on the target account and regioon&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is required if you have never used AWS CDK before on this account and region combination. (&lt;a href=&#34;https://docs.aws.amazon.com/cdk/latest/guide/cli.html#cli-bootstrap&#34;&gt;More information on CDK bootstrapping&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx cdk bootstrap aws://{targetAccountId}/{targetRegion}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Deploy the solution to your AWS Account&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Verify that Docker is running with the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you get an error like the one below, then Docker is not running and need to be restarted:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Deploy the sample using the following CDK command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx cdk deploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This step duration can vary a lot, depending on the Constructs you are deploying. Can go from 6m with basic usage with Amazon Bedrock to 40m deploying all RAG sources an self hosted models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;You can view the progress of your CDK deployment in the &lt;a href=&#34;https://console.aws.amazon.com/cloudformation/home&#34;&gt;CloudFormation console&lt;/a&gt; in the selected region.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once deployed, take note of the &lt;code&gt;User Interface&lt;/code&gt;, &lt;code&gt;User Pool&lt;/code&gt; and, if you want to interact with &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/#3p-models-providers&#34;&gt;3P models providers&lt;/a&gt; the &lt;code&gt;Secret&lt;/code&gt; that will hold the various &lt;code&gt;API_KEYS&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;...&#xA;Outputs:&#xA;AwsGenaiLllmChatbotStack.WebInterfaceUserInterfaceUrlXXXXX = dxxxxxxxxxxxxx.cloudfront.net&#xA;AwsGenaiLllmChatbotStack.AuthenticationUserPoolLinkXXXXX = https://xxxxx.console.aws.amazon.com/cognito/v2/idp/user-pools/xxxxx_XXXXX/users?region=xxxxx&#xA;AwsGenaiLllmChatbotStack1.LangchainInterfaceKeysSecretsNameXXXX = LangchainInterfaceKeySecret-xxxxxx&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the generated &lt;strong&gt;Cognito User Pool&lt;/strong&gt; Link from outputs above i.e. &lt;code&gt;https://xxxxx.console.aws.amazon.com/cognito/v2/idp/user-pools/xxxxx_XXXXX/users?region=xxxxx&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add a user that will be used to login into the web interface.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;User Interface&lt;/code&gt; Url frin the outputs above i.e. &lt;code&gt;dxxxxxxxxxxxxx.cloudfront.net&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Login with the user created in .6, you will be asked to change the password and you&#39;ll be logged in in the main page.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Clean up&lt;/h1&gt; &#xA;&lt;p&gt;You can remove the stacks and all the associated resources created in your AWS account by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx cdk destroy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/bigadsoleiman/&#34;&gt;Bigad Soleiman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/spugachev/&#34;&gt;Sergey Pugachev&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;This sample was made possible thanks to the following libraries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/get_started/introduction.html&#34;&gt;langchain&lt;/a&gt; from &lt;a href=&#34;https://github.com/langchain-ai&#34;&gt;LangChain AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Unstructured-IO/unstructured&#34;&gt;unstructured&lt;/a&gt; from &lt;a href=&#34;https://github.com/Unstructured-IO/unstructured&#34;&gt;Unstructured-IO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;pgvector&lt;/a&gt; from &lt;a href=&#34;https://github.com/ankane&#34;&gt;Andrew Kane&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This library is licensed under the MIT-0 License. See the LICENSE file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt; of the project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/LICENSE&#34;&gt;License&lt;/a&gt; of the project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; of the project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-genai-llm-chatbot/main/CONTRIBUTING.md#security-issue-notifications&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>