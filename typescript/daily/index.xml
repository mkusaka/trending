<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-09T01:47:50Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alexy201/awsdocsgpt</title>
    <updated>2023-07-09T01:47:50Z</updated>
    <id>tag:github.com,2023-07-09:/alexy201/awsdocsgpt</id>
    <link href="https://github.com/alexy201/awsdocsgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Codebase for www.awsdocsgpt.com (AI-powered Search and Chat for AWS Documentation)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AWS Docs GPT&lt;/h1&gt; &#xA;&lt;p&gt;AI-powered search and chat for &lt;a href=&#34;https://docs.aws.amazon.com/&#34;&gt;AWS Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;AWS Docs GPT provides 2 things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A search interface.&lt;/li&gt; &#xA; &lt;li&gt;A chat interface.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Search&lt;/h3&gt; &#xA;&lt;p&gt;Search was created with &lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI Embeddings&lt;/a&gt; (&lt;code&gt;text-embedding-ada-002&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;First, we loop over the documentation urls and generate embeddings for each chunk of text in the page.&lt;/p&gt; &#xA;&lt;p&gt;Then in the app we take the user&#39;s search query, generate an embedding, and use the result to find the pages that contain similar content&lt;/p&gt; &#xA;&lt;p&gt;The comparison is done using cosine similarity across our database of vectors.&lt;/p&gt; &#xA;&lt;p&gt;Results are then ranked by similarity score and returned to the user.&lt;/p&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;Chat builds on top of search. It uses search results to create a prompt that is fed into GPT-3.5-turbo.&lt;/p&gt; &#xA;&lt;p&gt;This allows for a chat-like experience where the user can ask questions about AWS documentation and get answers.&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a quick overview of how to run it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set up OpenAI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You&#39;ll need an OpenAI API key to generate embeddings (locally).&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Set up a local image of PostgreSQL (I recommend the pgvector docker image)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There is a setup.sql file in the root of the repo that you can use to set up the database.&lt;/p&gt; &#xA;&lt;p&gt;Run that in a SQL editor.&lt;/p&gt; &#xA;&lt;p&gt;Note: Or, connect to any PostgreSQL server using the env variables defined below&lt;/p&gt; &#xA;&lt;h3&gt;Repo Setup&lt;/h3&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Clone repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/alexy201/awsdocsgpt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;npm i&#xA;cd ../backend&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Set up environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create a .env.local file in the root of the frontend folder with the following variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NEXT_PUBLIC_SEARCH_ENDPOINT =&#xA;NEXT_PUBLIC_CHAT_ENDPOINT = &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a .env file in the root of the backend folder with the following variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY = &#xA;POSTGRES_HOST = &#xA;POSTGRES_DB_NAME = &#xA;POSTGRES_USERNAME = &#xA;POSTGRES_TABLE_NAME = #if you used setup.sql, this should be &#34;aws_chunks&#34;&#xA;POSTGRES_SEARCH_FUNCTION = #if you used setup.sql, this should be &#34;aws_gpt_search&#34;&#xA;POSTGRES_PASSWORD = &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run parsing script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: The data-upload.py script requires the same environment variables as the backend folder. Add AWS documentation links to the additional.txt file (one url on each line). This will import chunks + embeddings from those urls to the PostgreSQL DB specified in .env.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 data/data-upload.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please be patient! Depending on the number of links inputted, this process will take anywhere from 30 minutes to multiple hours.&lt;/p&gt; &#xA;&lt;h3&gt;App&lt;/h3&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Run entire app&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd backend&#xA;uvicorn app.main:app --reload&#xA;cd ../frontend&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/mckaywrigley&#34;&gt;Mckay Wrigley&lt;/a&gt; for inspiring this project.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/sima_alexx&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>