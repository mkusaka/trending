<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-08T01:37:41Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cloudflare/serverless-registry</title>
    <updated>2024-09-08T01:37:41Z</updated>
    <id>tag:github.com,2024-09-08:/cloudflare/serverless-registry</id>
    <link href="https://github.com/cloudflare/serverless-registry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Docker registry backed by Workers and R2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Container Registry in Workers&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains a container registry implementation in Workers that uses R2.&lt;/p&gt; &#xA;&lt;p&gt;It supports all pushing and pulling workflows. It also supports Username/Password and public key JWT based authentication.&lt;/p&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;You have to install all the dependencies with &lt;a href=&#34;https://pnpm.io/installation&#34;&gt;pnpm&lt;/a&gt; (other package managers may work, but only pnpm is supported.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation, there is a few steps to actually deploy the registry into production:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Have your own &lt;code&gt;wrangler&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp wrangler.toml.example wrangler.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Setup the R2 Bucket for this registry&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ npx wrangler --env production r2 bucket create r2-registry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add this to your &lt;code&gt;wrangler.toml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;r2_buckets = [&#xA;    { binding = &#34;REGISTRY&#34;, bucket_name = &#34;r2-registry&#34;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Deploy your image registry&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ npx wrangler deploy --env production&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your registry should be up and running. It will refuse any requests if you don&#39;t setup credentials.&lt;/p&gt; &#xA;&lt;h3&gt;Adding username password based authentication&lt;/h3&gt; &#xA;&lt;p&gt;Set the USERNAME and PASSWORD as secrets with &lt;code&gt;npx wrangler secret put USERNAME --env production&lt;/code&gt; and &lt;code&gt;npx wrangler secret put PASSWORD --env production&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Adding JWT authentication with public key&lt;/h3&gt; &#xA;&lt;p&gt;You can add a base64 encoded JWT public key to verify passwords (or token) that are signed by the private key. &lt;code&gt;npx wrangler secret put JWT_REGISTRY_TOKENS_PUBLIC_KEY --env production&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using with Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can use this registry with Docker to push and pull images.&lt;/p&gt; &#xA;&lt;p&gt;Example using &lt;code&gt;docker push&lt;/code&gt; and &lt;code&gt;docker pull&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export REGISTRY_URL=your-url-here&#xA;&#xA;# Replace $PASSWORD and $USERNAME with the actual credentials&#xA;echo $PASSWORD | docker login --username $USERNAME --password-stdin $REGISTRY_URL&#xA;docker pull ubuntu:latest&#xA;docker tag ubuntu:latest $REGISTRY_URL/ubuntu:latest&#xA;docker push $REGISTRY_URL/ubuntu:latest&#xA;&#xA;# Check that pulls work&#xA;docker rmi ubuntu:latest $REGISTRY_URL/ubuntu:latest&#xA;docker pull $REGISTRY_URL/ubuntu:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring Pull fallback&lt;/h3&gt; &#xA;&lt;p&gt;You can configure the R2 regitry to fallback to another registry if it doesn&#39;t exist in your R2 bucket. It will download from the registry and copy it into the R2 bucket. In the next pull it will be able to pull it directly from R2.&lt;/p&gt; &#xA;&lt;p&gt;This is very useful for migrating from one registry to &lt;code&gt;serverless-registry&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It supports both Basic and Bearer authentications as explained in the &lt;a href=&#34;https://distribution.github.io/distribution/spec/auth/token/&#34;&gt;registry spec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the wrangler.toml file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[env.production]&#xA;REGISTRIES_JSON = &#34;[{ \&#34;registry\&#34;: \&#34;https://url-to-other-registry\&#34;, \&#34;password_env\&#34;: \&#34;REGISTRY_TOKEN\&#34;, \&#34;username\&#34;: \&#34;username-to-use\&#34; }]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set as a secret the registry token of the registry you want to setup pull fallback in.&lt;/p&gt; &#xA;&lt;p&gt;For example &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/reference/docker-api&#34;&gt;gcr&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/settings/tokens&#34;&gt;Github&lt;/a&gt; for example uses a simple token that you can copy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The trick is always looking for how you would login in Docker for the target registry and setup the credentials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Never put a registry password/token inside the wrangler.toml, please always use &lt;code&gt;wrangler secrets put&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Known limitations&lt;/h3&gt; &#xA;&lt;p&gt;Right now there is some limitations with this container registry.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pushing with docker is limited to images that have layers of maximum size 500MB. Refer to maximum request body sizes in your Workers plan.&lt;/li&gt; &#xA; &lt;li&gt;To circumvent that limitation, you can manually add the layer and the manifest into the R2 bucket or use a client that is able to chunk uploads in sizes less than 500MB (or the limit that you have in your Workers plan).&lt;/li&gt; &#xA; &lt;li&gt;If you use &lt;code&gt;npx wrangler dev&lt;/code&gt; and push to the R2 registry with docker, the R2 registry will have to buffer the request on the Worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The project is licensed under the &lt;a href=&#34;https://opensource.org/licenses/apache-2.0/&#34;&gt;Apache License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;code&gt;CONTRIBUTING.md&lt;/code&gt; for contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/langgraphjs</title>
    <updated>2024-09-08T01:37:41Z</updated>
    <id>tag:github.com,2024-09-08:/langchain-ai/langgraphjs</id>
    <link href="https://github.com/langchain-ai/langgraphjs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ö° Build language agents as graphs ‚ö°&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶úüï∏Ô∏èLangGraph.js&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/npm/v/@langchain/langgraph?logo=npm&#34; alt=&#34;Version&#34;&gt;&lt;br&gt; &lt;a href=&#34;https://www.npmjs.com/package/@langchain/langgraph&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/@langchain/langgraph&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langchain-ai/langgraphjs/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/langchain-ai/langgraphjs&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö° Building language agents as graphs ‚ö°&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/&#34;&gt;LangGraph.js&lt;/a&gt; is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.&lt;/p&gt; &#xA;&lt;p&gt;LangGraph is inspired by &lt;a href=&#34;https://research.google/pubs/pub37252/&#34;&gt;Pregel&lt;/a&gt; and &lt;a href=&#34;https://beam.apache.org/&#34;&gt;Apache Beam&lt;/a&gt;. The public interface draws inspiration from &lt;a href=&#34;https://networkx.org/documentation/latest/&#34;&gt;NetworkX&lt;/a&gt;. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cycles and Branching&lt;/strong&gt;: Implement loops and conditionals in your apps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human-in-the-Loop&lt;/strong&gt;: Interrupt graph execution to approve or edit next action planned by the agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Support&lt;/strong&gt;: Stream outputs as they are produced by each node (including token streaming).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integration with LangChain&lt;/strong&gt;: LangGraph integrates seamlessly with &lt;a href=&#34;https://github.com/langchain-ai/langchainjs/&#34;&gt;LangChain.js&lt;/a&gt; and &lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt; (but does not require them).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @langchain/langgraph @langchain/core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;One of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s take a look at an example of an agent that can use a search tool.&lt;/p&gt; &#xA;&lt;p&gt;First install the required dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @langchain/anthropic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then set the required environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ANTHROPIC_API_KEY=sk-...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, set up &lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt; for best-in-class observability:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LANGCHAIN_TRACING_V2=true&#xA;export LANGCHAIN_API_KEY=ls__...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s define our agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { AIMessage, BaseMessage, HumanMessage } from &#34;@langchain/core/messages&#34;;&#xA;import { tool } from &#34;@langchain/core/tools&#34;;&#xA;import { z } from &#34;zod&#34;;&#xA;import { ChatAnthropic } from &#34;@langchain/anthropic&#34;;&#xA;import { StateGraph, StateGraphArgs } from &#34;@langchain/langgraph&#34;;&#xA;import { MemorySaver, Annotation } from &#34;@langchain/langgraph&#34;;&#xA;import { ToolNode } from &#34;@langchain/langgraph/prebuilt&#34;;&#xA;&#xA;// Define the graph state&#xA;// See here for more info: https://langchain-ai.github.io/langgraphjs/how-tos/define-state/&#xA;const StateAnnotation = Annotation.Root({&#xA;  messages: Annotation&amp;lt;BaseMessage[]&amp;gt;({&#xA;    reducer: (x, y) =&amp;gt; x.concat(y),&#xA;  })&#xA;})&#xA;&#xA;// Define the tools for the agent to use&#xA;const weatherTool = tool(async ({ query }) =&amp;gt; {&#xA;  // This is a placeholder for the actual implementation&#xA;  if (query.toLowerCase().includes(&#34;sf&#34;) || query.toLowerCase().includes(&#34;san francisco&#34;)) {&#xA;    return &#34;It&#39;s 60 degrees and foggy.&#34;&#xA;  }&#xA;  return &#34;It&#39;s 90 degrees and sunny.&#34;&#xA;}, {&#xA;  name: &#34;weather&#34;,&#xA;  description:&#xA;    &#34;Call to get the current weather for a location.&#34;,&#xA;  schema: z.object({&#xA;    query: z.string().describe(&#34;The query to use in your search.&#34;),&#xA;  }),&#xA;});&#xA;&#xA;const tools = [weatherTool];&#xA;const toolNode = new ToolNode(tools);&#xA;&#xA;const model = new ChatAnthropic({&#xA;  model: &#34;claude-3-5-sonnet-20240620&#34;,&#xA;  temperature: 0,&#xA;}).bindTools(tools);&#xA;&#xA;// Define the function that determines whether to continue or not&#xA;// We can extract the state typing via `StateAnnotation.State`&#xA;function shouldContinue(state: typeof StateAnnotation.State) {&#xA;  const messages = state.messages;&#xA;  const lastMessage = messages[messages.length - 1] as AIMessage;&#xA;&#xA;  // If the LLM makes a tool call, then we route to the &#34;tools&#34; node&#xA;  if (lastMessage.tool_calls?.length) {&#xA;    return &#34;tools&#34;;&#xA;  }&#xA;  // Otherwise, we stop (reply to the user)&#xA;  return &#34;__end__&#34;;&#xA;}&#xA;&#xA;// Define the function that calls the model&#xA;async function callModel(state: typeof StateAnnotation.State) {&#xA;  const messages = state.messages;&#xA;  const response = await model.invoke(messages);&#xA;&#xA;  // We return a list, because this will get added to the existing list&#xA;  return { messages: [response] };&#xA;}&#xA;&#xA;// Define a new graph&#xA;const workflow = new StateGraph(StateAnnotation)&#xA;  .addNode(&#34;agent&#34;, callModel)&#xA;  .addNode(&#34;tools&#34;, toolNode)&#xA;  .addEdge(&#34;__start__&#34;, &#34;agent&#34;)&#xA;  .addConditionalEdges(&#34;agent&#34;, shouldContinue)&#xA;  .addEdge(&#34;tools&#34;, &#34;agent&#34;);&#xA;&#xA;// Initialize memory to persist state between graph runs&#xA;const checkpointer = new MemorySaver();&#xA;&#xA;// Finally, we compile it!&#xA;// This compiles it into a LangChain Runnable.&#xA;// Note that we&#39;re (optionally) passing the memory when compiling the graph&#xA;const app = workflow.compile({ checkpointer });&#xA;&#xA;// Use the Runnable&#xA;const finalState = await app.invoke(&#xA;  { messages: [new HumanMessage(&#34;what is the weather in sf&#34;)] },&#xA;  { configurable: { thread_id: &#34;42&#34; } }&#xA;);&#xA;&#xA;console.log(finalState.messages[finalState.messages.length - 1].content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Based on the information I received, the current weather in San Francisco is:&#xA;&#xA;Temperature: 60 degrees Fahrenheit&#xA;Conditions: Foggy&#xA;&#xA;San Francisco is known for its foggy weather, especially during certain times of the year. The moderate temperature of 60¬∞F (about 15.5¬∞C) is quite typical for the city, which generally has mild weather year-round due to its coastal location.&#xA;&#xA;Is there anything else you&#39;d like to know about the weather in San Francisco or any other location?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now when we pass the same &lt;code&gt;&#34;thread_id&#34;&lt;/code&gt;, the conversation context is retained via the saved state (i.e. stored list of messages):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;const nextState = await app.invoke(&#xA;  { messages: [new HumanMessage(&#34;what about ny&#34;)] },&#xA;  { configurable: { thread_id: &#34;42&#34; } }&#xA;);&#xA;console.log(nextState.messages[nextState.messages.length - 1].content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Based on the information I received, the current weather in New York is:&#xA;&#xA;Temperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)&#xA;Conditions: Sunny&#xA;&#xA;New York is experiencing quite warm weather today. A temperature of 90¬∞F is considered hot for most people, and it&#39;s significantly warmer than the San Francisco weather we just checked. The sunny conditions suggest it&#39;s a clear day without cloud cover, which can make it feel even warmer.&#xA;&#xA;On a day like this in New York, it would be advisable for people to stay hydrated, seek shade when possible, and use sun protection if spending time outdoors.&#xA;&#xA;Is there anything else you&#39;d like to know about the weather in New York or any other location?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step-by-step Breakdown&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Initialize the model and tools.&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;We use &lt;code&gt;ChatAnthropic&lt;/code&gt; as our LLM. &lt;strong&gt;NOTE:&lt;/strong&gt; We need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for Anthropic tool calling using the &lt;code&gt;.bindTools()&lt;/code&gt; method.&lt;/li&gt; &#xA;    &lt;li&gt;We define the tools we want to use -- a weather tool in our case. See the documentation &lt;a href=&#34;https://js.langchain.com/docs/modules/agents/tools/dynamic&#34;&gt;here&lt;/a&gt; on how to create your own tools.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Initialize graph with state.&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;We initialize the graph (&lt;code&gt;StateGraph&lt;/code&gt;) by passing the state interface (&lt;code&gt;AgentState&lt;/code&gt;).&lt;/li&gt; &#xA;    &lt;li&gt;The &lt;code&gt;StateAnnotation&lt;/code&gt; object defines how updates from each node should be merged into the graph&#39;s state.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Define graph nodes.&lt;/summary&gt; &#xA;   &lt;p&gt;There are two main nodes we need:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;The &lt;code&gt;agent&lt;/code&gt; node: responsible for deciding what (if any) actions to take.&lt;/li&gt; &#xA;    &lt;li&gt;The &lt;code&gt;tools&lt;/code&gt; node that invokes tools: if the agent decides to take an action, this node will then execute that action.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Define entry point and graph edges.&lt;/summary&gt; &#xA;   &lt;p&gt;First, we need to set the entry point for graph execution - the &lt;code&gt;agent&lt;/code&gt; node.&lt;/p&gt; &#xA;   &lt;p&gt;Then we define one normal and one conditional edge. A conditional edge means that the destination depends on the contents of the graph&#39;s state (&lt;code&gt;AgentState&lt;/code&gt;). In our case, the destination is not known until the agent (LLM) decides.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Conditional edge: after the agent is called, we should either: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;a. Run tools if the agent said to take an action, OR&lt;/li&gt; &#xA;      &lt;li&gt;b. Finish (respond to the user) if the agent did not ask to run tools&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Compile the graph.&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;When we compile the graph, we turn it into a LangChain &lt;a href=&#34;https://js.langchain.com/docs/expression_language/&#34;&gt;Runnable&lt;/a&gt;, which automatically enables calling &lt;code&gt;.invoke()&lt;/code&gt;, &lt;code&gt;.stream()&lt;/code&gt; and &lt;code&gt;.batch()&lt;/code&gt; with your inputs.&lt;/li&gt; &#xA;    &lt;li&gt;We can also optionally pass a checkpointer object for persisting state between graph runs, enabling memory, human-in-the-loop workflows, time travel and more. In our case we use &lt;code&gt;MemorySaver&lt;/code&gt; - a simple in-memory checkpointer.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Execute the graph.&lt;/summary&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt; &lt;p&gt;LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, &lt;code&gt;&#34;agent&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;The &lt;code&gt;&#34;agent&#34;&lt;/code&gt; node executes, invoking the chat model.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;The chat model returns an &lt;code&gt;AIMessage&lt;/code&gt;. LangGraph adds this to the state.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;The graph cycles through the following steps until there are no more &lt;code&gt;tool_calls&lt;/code&gt; on the &lt;code&gt;AIMessage&lt;/code&gt;:&lt;/p&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;If &lt;code&gt;AIMessage&lt;/code&gt; has &lt;code&gt;tool_calls&lt;/code&gt;, the &lt;code&gt;&#34;tools&#34;&lt;/code&gt; node executes.&lt;/li&gt; &#xA;      &lt;li&gt;The &lt;code&gt;&#34;agent&#34;&lt;/code&gt; node executes again and returns an &lt;code&gt;AIMessage&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;Execution progresses to the special &lt;code&gt;__end__&lt;/code&gt; value and outputs the final state. As a result, we get a list of all our chat messages as output.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ol&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/tutorials/&#34;&gt;Tutorials&lt;/a&gt;: Learn to build with LangGraph through guided examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/how-tos/&#34;&gt;How-to Guides&lt;/a&gt;: Accomplish specific things within LangGraph, from streaming, to adding memory &amp;amp; persistence, to common design patterns (branching, subgraphs, etc.). These are the place to go if you want to copy and run a specific code snippet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/concepts/&#34;&gt;Conceptual Guides&lt;/a&gt;: In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraphjs/reference/&#34;&gt;API Reference&lt;/a&gt;: Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running Example Jupyter Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the *.ipynb notebooks in the &lt;code&gt;examples/&lt;/code&gt; folder require &lt;a href=&#34;https://github.com/yunabe/tslab?tab=readme-ov-file&#34;&gt;tslab&lt;/a&gt; to be installed. In order to run these notebooks in VSCode, you will also need the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter&#34;&gt;Jupyter&lt;/a&gt; VSCode Extension installed. After cloning this repository, you can run &lt;code&gt;yarn build&lt;/code&gt; in the root. You should then be all set!&lt;/p&gt; &#xA;&lt;p&gt;If you are still having trouble, try adding the following &lt;code&gt;tsconfig.json&lt;/code&gt; file to the &lt;code&gt;examples/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;compilerOptions&#34;: {&#xA;    &#34;esModuleInterop&#34;: true,&#xA;    &#34;moduleResolution&#34;: &#34;node&#34;,&#xA;    &#34;target&#34;: &#34;ES2020&#34;,&#xA;    &#34;module&#34;: &#34;ES2020&#34;,&#xA;    &#34;lib&#34;: [&#xA;      &#34;ES2020&#34;&#xA;    ],&#xA;    &#34;strict&#34;: true,&#xA;    &#34;baseUrl&#34;: &#34;.&#34;,&#xA;    &#34;paths&#34;: {&#xA;      &#34;@langchain/langgraph&#34;: [&#xA;        &#34;../langgraph/src&#34;&#xA;      ]&#xA;    }&#xA;  },&#xA;  &#34;include&#34;: [&#xA;    &#34;./**/*.ts&#34;,&#xA;    &#34;./**/*.tsx&#34;&#xA;  ],&#xA;  &#34;exclude&#34;: [&#xA;    &#34;node_modules&#34;&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>