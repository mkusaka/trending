<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-07T01:37:56Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dzhng/deep-research</title>
    <updated>2025-05-07T01:37:56Z</updated>
    <id>tag:github.com,2025-05-07:/dzhng/deep-research</id>
    <link href="https://github.com/dzhng/deep-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI-powered research assistant that performs iterative, deep research on any topic by combining search engines, web scraping, and large language models. The goal of this repo is to provide the simplest implementation of a deep research agent - e.g. an agent that can refine its research direction overtime and deep dive into a topic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; &#xA;&lt;p&gt;An AI-powered research assistant that performs iterative, deep research on any topic by combining search engines, web scraping, and large language models.&lt;/p&gt; &#xA;&lt;p&gt;The goal of this repo is to provide the simplest implementation of a deep research agent - e.g. an agent that can refine its research direction over time and deep dive into a topic. Goal is to keep the repo size at &amp;lt;500 LoC so it is easy to understand and build on top of.&lt;/p&gt; &#xA;&lt;p&gt;If you like this project, please consider starring it and giving me a follow on &lt;a href=&#34;https://x.com/dzhng&#34;&gt;X/Twitter&lt;/a&gt;. This project is sponsored by &lt;a href=&#34;https://aomni.com&#34;&gt;Aomni&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB&#xA;    subgraph Input&#xA;        Q[User Query]&#xA;        B[Breadth Parameter]&#xA;        D[Depth Parameter]&#xA;    end&#xA;&#xA;    DR[Deep Research] --&amp;gt;&#xA;    SQ[SERP Queries] --&amp;gt;&#xA;    PR[Process Results]&#xA;&#xA;    subgraph Results[Results]&#xA;        direction TB&#xA;        NL((Learnings))&#xA;        ND((Directions))&#xA;    end&#xA;&#xA;    PR --&amp;gt; NL&#xA;    PR --&amp;gt; ND&#xA;&#xA;    DP{depth &amp;gt; 0?}&#xA;&#xA;    RD[&#34;Next Direction:&#xA;    - Prior Goals&#xA;    - New Questions&#xA;    - Learnings&#34;]&#xA;&#xA;    MR[Markdown Report]&#xA;&#xA;    %% Main Flow&#xA;    Q &amp;amp; B &amp;amp; D --&amp;gt; DR&#xA;&#xA;    %% Results to Decision&#xA;    NL &amp;amp; ND --&amp;gt; DP&#xA;&#xA;    %% Circular Flow&#xA;    DP --&amp;gt;|Yes| RD&#xA;    RD --&amp;gt;|New Context| DR&#xA;&#xA;    %% Final Output&#xA;    DP --&amp;gt;|No| MR&#xA;&#xA;    %% Styling&#xA;    classDef input fill:#7bed9f,stroke:#2ed573,color:black&#xA;    classDef process fill:#70a1ff,stroke:#1e90ff,color:black&#xA;    classDef recursive fill:#ffa502,stroke:#ff7f50,color:black&#xA;    classDef output fill:#ff4757,stroke:#ff6b81,color:black&#xA;    classDef results fill:#a8e6cf,stroke:#3b7a57,color:black&#xA;&#xA;    class Q,B,D input&#xA;    class DR,SQ,PR process&#xA;    class DP,RD recursive&#xA;    class MR output&#xA;    class NL,ND results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Iterative Research&lt;/strong&gt;: Performs deep research by iteratively generating search queries, processing results, and diving deeper based on findings&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligent Query Generation&lt;/strong&gt;: Uses LLMs to generate targeted search queries based on research goals and previous findings&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Depth &amp;amp; Breadth Control&lt;/strong&gt;: Configurable parameters to control how wide (breadth) and deep (depth) the research goes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Follow-up&lt;/strong&gt;: Generates follow-up questions to better understand research needs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Reports&lt;/strong&gt;: Produces detailed markdown reports with findings and sources&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent Processing&lt;/strong&gt;: Handles multiple searches and result processing in parallel for efficiency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js environment&lt;/li&gt; &#xA; &lt;li&gt;API keys for: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Firecrawl API (for web search and content extraction)&lt;/li&gt; &#xA;   &lt;li&gt;OpenAI API (for o3 mini model)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Node.js&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set up environment variables in a &lt;code&gt;.env.local&lt;/code&gt; file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FIRECRAWL_KEY=&#34;your_firecrawl_key&#34;&#xA;# If you want to use your self-hosted Firecrawl, add the following below:&#xA;# FIRECRAWL_BASE_URL=&#34;http://localhost:3002&#34;&#xA;&#xA;OPENAI_KEY=&#34;your_openai_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use local LLM, comment out &lt;code&gt;OPENAI_KEY&lt;/code&gt; and instead uncomment &lt;code&gt;OPENAI_ENDPOINT&lt;/code&gt; and &lt;code&gt;OPENAI_MODEL&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set &lt;code&gt;OPENAI_ENDPOINT&lt;/code&gt; to the address of your local server (eg.&#34;&lt;a href=&#34;http://localhost:1234/v1&#34;&gt;http://localhost:1234/v1&lt;/a&gt;&#34;)&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;OPENAI_MODEL&lt;/code&gt; to the name of the model loaded in your local server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rename &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env.local&lt;/code&gt; and set your API keys&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;docker build -f Dockerfile&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the Docker image:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;npm run docker&lt;/code&gt; in the docker service:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it deep-research npm run docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run the research assistant:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll be prompted to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Enter your research query&lt;/li&gt; &#xA; &lt;li&gt;Specify research breadth (recommended: 3-10, default: 4)&lt;/li&gt; &#xA; &lt;li&gt;Specify research depth (recommended: 1-5, default: 2)&lt;/li&gt; &#xA; &lt;li&gt;Answer follow-up questions to refine the research direction&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The system will then:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate and execute search queries&lt;/li&gt; &#xA; &lt;li&gt;Process and analyze search results&lt;/li&gt; &#xA; &lt;li&gt;Recursively explore deeper based on findings&lt;/li&gt; &#xA; &lt;li&gt;Generate a comprehensive markdown report&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The final report will be saved as &lt;code&gt;report.md&lt;/code&gt; or &lt;code&gt;answer.md&lt;/code&gt; in your working directory, depending on which modes you selected.&lt;/p&gt; &#xA;&lt;h3&gt;Concurrency&lt;/h3&gt; &#xA;&lt;p&gt;If you have a paid version of Firecrawl or a local version, feel free to increase the &lt;code&gt;ConcurrencyLimit&lt;/code&gt; by setting the &lt;code&gt;CONCURRENCY_LIMIT&lt;/code&gt; environment variable so it runs faster.&lt;/p&gt; &#xA;&lt;p&gt;If you have a free version, you may sometimes run into rate limit errors, you can reduce the limit to 1 (but it will run a lot slower).&lt;/p&gt; &#xA;&lt;h3&gt;DeepSeek R1&lt;/h3&gt; &#xA;&lt;p&gt;Deep research performs great on R1! We use &lt;a href=&#34;http://fireworks.ai&#34;&gt;Fireworks&lt;/a&gt; as the main provider for the R1 model. To use R1, simply set a Fireworks API key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FIREWORKS_KEY=&#34;api_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The system will automatically switch over to use R1 instead of &lt;code&gt;o3-mini&lt;/code&gt; when the key is detected.&lt;/p&gt; &#xA;&lt;h3&gt;Custom endpoints and models&lt;/h3&gt; &#xA;&lt;p&gt;There are 2 other optional env vars that lets you tweak the endpoint (for other OpenAI compatible APIs like OpenRouter or Gemini) as well as the model string.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_ENDPOINT=&#34;custom_endpoint&#34;&#xA;CUSTOM_MODEL=&#34;custom_model&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Initial Setup&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Takes user query and research parameters (breadth &amp;amp; depth)&lt;/li&gt; &#xA;   &lt;li&gt;Generates follow-up questions to understand research needs better&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Research Process&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generates multiple SERP queries based on research goals&lt;/li&gt; &#xA;   &lt;li&gt;Processes search results to extract key learnings&lt;/li&gt; &#xA;   &lt;li&gt;Generates follow-up research directions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Recursive Exploration&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If depth &amp;gt; 0, takes new research directions and continues exploration&lt;/li&gt; &#xA;   &lt;li&gt;Each iteration builds on previous learnings&lt;/li&gt; &#xA;   &lt;li&gt;Maintains context of research goals and findings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Report Generation&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compiles all findings into a comprehensive markdown report&lt;/li&gt; &#xA;   &lt;li&gt;Includes all sources and references&lt;/li&gt; &#xA;   &lt;li&gt;Organizes information in a clear, readable format&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License - feel free to use and modify as needed.&lt;/p&gt;</summary>
  </entry>
</feed>