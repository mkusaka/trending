<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-18T01:36:40Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>musistudio/claude-code-router</title>
    <updated>2025-06-18T01:36:40Z</updated>
    <id>tag:github.com,2025-06-18:/musistudio/claude-code-router</id>
    <link href="https://github.com/musistudio/claude-code-router" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Router&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This is a tool for routing Claude Code requests to different models, and you can customize any request.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/screenshoots/claude-code.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Claude Code&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @anthropic-ai/claude-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Claude Code Router&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @musistudio/claude-code-router&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start Claude Code by claude-code-router&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ccr code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Configure routing[optional]&lt;br&gt; Set up your &lt;code&gt;~/.claude-code-router/config.json&lt;/code&gt; file like this:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;OPENAI_API_KEY&#34;: &#34;sk-xxx&#34;,&#xA;  &#34;OPENAI_BASE_URL&#34;: &#34;https://api.deepseek.com&#34;,&#xA;  &#34;OPENAI_MODEL&#34;: &#34;deepseek-chat&#34;,&#xA;  &#34;Providers&#34;: [&#xA;    {&#xA;      &#34;name&#34;: &#34;openrouter&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://openrouter.ai/api/v1&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#xA;        &#34;google/gemini-2.5-pro-preview&#34;,&#xA;        &#34;anthropic/claude-sonnet-4&#34;,&#xA;        &#34;anthropic/claude-3.5-sonnet&#34;,&#xA;        &#34;anthropic/claude-3.7-sonnet:thinking&#34;&#xA;      ]&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;deepseek&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://api.deepseek.com&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#34;deepseek-reasoner&#34;]&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;ollama&#34;,&#xA;      &#34;api_base_url&#34;: &#34;http://localhost:11434/v1&#34;,&#xA;      &#34;api_key&#34;: &#34;ollama&#34;,&#xA;      &#34;models&#34;: [&#34;qwen2.5-coder:latest&#34;]&#xA;    }&#xA;  ],&#xA;  &#34;Router&#34;: {&#xA;    &#34;background&#34;: &#34;ollama,qwen2.5-coder:latest&#34;,&#xA;    &#34;think&#34;: &#34;deepseek,deepseek-reasoner&#34;,&#xA;    &#34;longContext&#34;: &#34;openrouter,google/gemini-2.5-pro-preview&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;background&lt;/code&gt;&lt;br&gt; This model will be used to handle some background tasks(&lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/costs#background-token-usage&#34;&gt;background-token-usage&lt;/a&gt;). Based on my tests, it doesn’t require high intelligence. I’m using the qwen-coder-2.5:7b model running locally on my MacBook Pro M1 (32GB) via Ollama. If your computer can’t run Ollama, you can also use some free models, such as qwen-coder-2.5:3b.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;think&lt;/code&gt;&lt;br&gt; This model will be used when enabling Claude Code to perform reasoning. However, reasoning budget control has not yet been implemented (since the DeepSeek-R1 model does not support it), so there is currently no difference between using UltraThink and Think modes. It is worth noting that Plan Mode also use this model to achieve better planning results.&lt;br&gt; Note: The reasoning process via the official DeepSeek API may be very slow, so you may need to wait for an extended period of time.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;longContext&lt;/code&gt;&lt;br&gt; This model will be used when the context length exceeds 32K (this value may be modified in the future). You can route the request to a model that performs well with long contexts (I’ve chosen google/gemini-2.5-pro-preview). This scenario has not been thoroughly tested yet, so if you encounter any issues, please submit an issue.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;model command&lt;br&gt; You can also switch models within Claude Code by using the &lt;code&gt;/model&lt;/code&gt; command. The format is: &lt;code&gt;provider,model&lt;/code&gt;, like this:&lt;br&gt; &lt;code&gt;/model openrouter,anthropic/claude-3.5-sonnet&lt;/code&gt;&lt;br&gt; This will use the anthropic/claude-3.5-sonnet model provided by OpenRouter to handle all subsequent tasks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Plugins&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support change models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support scheduled tasks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Some tips:&lt;/h2&gt; &#xA;&lt;p&gt;Now you can use deepseek-v3 models directly without using any plugins.&lt;/p&gt; &#xA;&lt;p&gt;If you’re using the DeepSeek API provided by the official website, you might encounter an “exceeding context” error after several rounds of conversation (since the official API only supports a 64K context window). In this case, you’ll need to discard the previous context and start fresh. Alternatively, you can use ByteDance’s DeepSeek API, which offers a 128K context window and supports KV cache.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/screenshoots/contexterror.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: claude code consumes a huge amount of tokens, but thanks to DeepSeek’s low cost, you can use claude code at a fraction of Claude’s price, and you don’t need to subscribe to the Claude Max plan.&lt;/p&gt; &#xA;&lt;p&gt;Some interesting points: Based on my testing, including a lot of context information can help narrow the performance gap between these LLM models. For instance, when I used Claude-4 in VSCode Copilot to handle a Flutter issue, it messed up the files in three rounds of conversation, and I had to roll everything back. However, when I used claude code with DeepSeek, after three or four rounds of conversation, I finally managed to complete my task—and the cost was less than 1 RMB!&lt;/p&gt; &#xA;&lt;h2&gt;Some articles:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/project-motivation-and-how-it-works.md&#34;&gt;Project Motivation and Principles&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/zh/%E9%A1%B9%E7%9B%AE%E5%88%9D%E8%A1%B7%E5%8F%8A%E5%8E%9F%E7%90%86.md&#34;&gt;中文版看这里&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Buy me a coffee&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful, you can choose to sponsor the author with a cup of coffee. &lt;a href=&#34;http://paypal.me/musistudio1999&#34;&gt;Buy me a coffee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the following sponsors:&lt;/p&gt; &#xA;&lt;p&gt;@Simon Leischnig (If you see this, feel free to contact me and I can update it with your GitHub information)&lt;/p&gt;</summary>
  </entry>
</feed>