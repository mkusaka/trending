<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-07T01:38:59Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AsyncFuncAI/deepwiki-open</title>
    <updated>2025-08-07T01:38:59Z</updated>
    <id>tag:github.com,2025-08-07:/AsyncFuncAI/deepwiki-open</id>
    <link href="https://github.com/AsyncFuncAI/deepwiki-open" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepWiki-Open&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Deepwiki.png&#34; alt=&#34;DeepWiki Banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepWiki&lt;/strong&gt; is my own implementation attempt of DeepWiki, automatically creates beautiful, interactive wikis for any GitHub, GitLab, or BitBucket repository! Just enter a repo name, and DeepWiki will:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Analyze the code structure&lt;/li&gt; &#xA; &lt;li&gt;Generate comprehensive documentation&lt;/li&gt; &#xA; &lt;li&gt;Create visual diagrams to explain how everything works&lt;/li&gt; &#xA; &lt;li&gt;Organize it all into an easy-to-navigate wiki&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://buymeacoffee.com/sheing&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;&amp;quot;Buy Me A Coffee&amp;quot;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tip.md/sng-asyncfunc&#34;&gt;&lt;img src=&#34;https://tip.md/badge.svg?sanitize=true&#34; alt=&#34;Tip in Crypto&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/sashimikun_void&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white&#34; alt=&#34;Twitter/X&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/VQMBGR8u5v&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh-tw.md&#34;&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ja.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.es.md&#34;&gt;Espa√±ol&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.kr.md&#34;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.vi.md&#34;&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.pt-br.md&#34;&gt;Portugu√™s Brasileiro&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.fr.md&#34;&gt;Fran√ßais&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ru.md&#34;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instant Documentation&lt;/strong&gt;: Turn any GitHub, GitLab or BitBucket repo into a wiki in seconds&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Private Repository Support&lt;/strong&gt;: Securely access private repositories with personal access tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt;: AI-powered understanding of code structure and relationships&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Beautiful Diagrams&lt;/strong&gt;: Automatic Mermaid diagrams to visualize architecture and data flow&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy Navigation&lt;/strong&gt;: Simple, intuitive interface to explore the wiki&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ask Feature&lt;/strong&gt;: Chat with your repository using RAG-powered AI to get accurate answers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DeepResearch&lt;/strong&gt;: Multi-turn research process that thoroughly investigates complex topics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Model Providers&lt;/strong&gt;: Support for Google Gemini, OpenAI, OpenRouter, and local Ollama models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start (Super Easy!)&lt;/h2&gt; &#xA;&lt;h3&gt;Option 1: Using Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/AsyncFuncAI/deepwiki-open.git&#xA;cd deepwiki-open&#xA;&#xA;# Create a .env file with your API keys&#xA;echo &#34;GOOGLE_API_KEY=your_google_api_key&#34; &amp;gt; .env&#xA;echo &#34;OPENAI_API_KEY=your_openai_api_key&#34; &amp;gt;&amp;gt; .env&#xA;# Optional: Add OpenRouter API key if you want to use OpenRouter models&#xA;echo &#34;OPENROUTER_API_KEY=your_openrouter_api_key&#34; &amp;gt;&amp;gt; .env&#xA;# Optional: Add Ollama host if not local. defaults to http://localhost:11434&#xA;echo &#34;OLLAMA_HOST=your_ollama_host&#34; &amp;gt;&amp;gt; .env&#xA;# Optional: Add Azure API key, endpoint and version if you want to use azure openai models&#xA;echo &#34;AZURE_OPENAI_API_KEY=your_azure_openai_api_key&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;AZURE_OPENAI_VERSION=your_azure_openai_version&#34; &amp;gt;&amp;gt; .env&#xA;# Run with Docker Compose&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed instructions on using DeepWiki with Ollama and Docker, see &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/Ollama-instruction.md&#34;&gt;Ollama Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí° &lt;strong&gt;Where to get these keys:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Get a Google API key from &lt;a href=&#34;https://makersuite.google.com/app/apikey&#34;&gt;Google AI Studio&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Get an OpenAI API key from &lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;OpenAI Platform&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Get Azure OpenAI credentials from &lt;a href=&#34;https://portal.azure.com/&#34;&gt;Azure Portal&lt;/a&gt; - create an Azure OpenAI resource and get the API key, endpoint, and API version&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Option 2: Manual Setup (Recommended)&lt;/h3&gt; &#xA;&lt;h4&gt;Step 1: Set Up Your API Keys&lt;/h4&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project root with these keys:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GOOGLE_API_KEY=your_google_api_key&#xA;OPENAI_API_KEY=your_openai_api_key&#xA;# Optional: Add this if you want to use OpenRouter models&#xA;OPENROUTER_API_KEY=your_openrouter_api_key&#xA;# Optional: Add this if you want to use Azure OpenAI models&#xA;AZURE_OPENAI_API_KEY=your_azure_openai_api_key&#xA;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint&#xA;AZURE_OPENAI_VERSION=your_azure_openai_version&#xA;# Optional: Add Ollama host if not local. default: http://localhost:11434&#xA;OLLAMA_HOST=your_ollama_host&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2: Start the Backend&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install Python dependencies&#xA;pip install -r api/requirements.txt&#xA;&#xA;# Start the API server&#xA;python -m api.main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3: Start the Frontend&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install JavaScript dependencies&#xA;npm install&#xA;# or&#xA;yarn install&#xA;&#xA;# Start the web app&#xA;npm run dev&#xA;# or&#xA;yarn dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 4: Use DeepWiki!&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser&lt;/li&gt; &#xA; &lt;li&gt;Enter a GitHub, GitLab, or Bitbucket repository (like &lt;code&gt;https://github.com/openai/codex&lt;/code&gt;, &lt;code&gt;https://github.com/microsoft/autogen&lt;/code&gt;, &lt;code&gt;https://gitlab.com/gitlab-org/gitlab&lt;/code&gt;, or &lt;code&gt;https://bitbucket.org/redradish/atlassian_app_versions&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;For private repositories, click &#34;+ Add access tokens&#34; and enter your GitHub or GitLab personal access token&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Generate Wiki&#34; and watch the magic happen!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üîç How It Works&lt;/h2&gt; &#xA;&lt;p&gt;DeepWiki uses AI to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone and analyze the GitHub, GitLab, or Bitbucket repository (including private repos with token authentication)&lt;/li&gt; &#xA; &lt;li&gt;Create embeddings of the code for smart retrieval&lt;/li&gt; &#xA; &lt;li&gt;Generate documentation with context-aware AI (using Google Gemini, OpenAI, OpenRouter, Azure OpenAI, or local Ollama models)&lt;/li&gt; &#xA; &lt;li&gt;Create visual diagrams to explain code relationships&lt;/li&gt; &#xA; &lt;li&gt;Organize everything into a structured wiki&lt;/li&gt; &#xA; &lt;li&gt;Enable intelligent Q&amp;amp;A with the repository through the Ask feature&lt;/li&gt; &#xA; &lt;li&gt;Provide in-depth research capabilities with DeepResearch&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD&#xA;    A[User inputs GitHub/GitLab/Bitbucket repo] --&amp;gt; AA{Private repo?}&#xA;    AA --&amp;gt;|Yes| AB[Add access token]&#xA;    AA --&amp;gt;|No| B[Clone Repository]&#xA;    AB --&amp;gt; B&#xA;    B --&amp;gt; C[Analyze Code Structure]&#xA;    C --&amp;gt; D[Create Code Embeddings]&#xA;&#xA;    D --&amp;gt; M{Select Model Provider}&#xA;    M --&amp;gt;|Google Gemini| E1[Generate with Gemini]&#xA;    M --&amp;gt;|OpenAI| E2[Generate with OpenAI]&#xA;    M --&amp;gt;|OpenRouter| E3[Generate with OpenRouter]&#xA;    M --&amp;gt;|Local Ollama| E4[Generate with Ollama]&#xA;    M --&amp;gt;|Azure| E5[Generate with Azure]&#xA;&#xA;    E1 --&amp;gt; E[Generate Documentation]&#xA;    E2 --&amp;gt; E&#xA;    E3 --&amp;gt; E&#xA;    E4 --&amp;gt; E&#xA;    E5 --&amp;gt; E&#xA;&#xA;    D --&amp;gt; F[Create Visual Diagrams]&#xA;    E --&amp;gt; G[Organize as Wiki]&#xA;    F --&amp;gt; G&#xA;    G --&amp;gt; H[Interactive DeepWiki]&#xA;&#xA;    classDef process stroke-width:2px;&#xA;    classDef data stroke-width:2px;&#xA;    classDef result stroke-width:2px;&#xA;    classDef decision stroke-width:2px;&#xA;&#xA;    class A,D data;&#xA;    class AA,M decision;&#xA;    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;&#xA;    class H result;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Project Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepwiki/&#xA;‚îú‚îÄ‚îÄ api/                  # Backend API server&#xA;‚îÇ   ‚îú‚îÄ‚îÄ main.py           # API entry point&#xA;‚îÇ   ‚îú‚îÄ‚îÄ api.py            # FastAPI implementation&#xA;‚îÇ   ‚îú‚îÄ‚îÄ rag.py            # Retrieval Augmented Generation&#xA;‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.py  # Data processing utilities&#xA;‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt  # Python dependencies&#xA;‚îÇ&#xA;‚îú‚îÄ‚îÄ src/                  # Frontend Next.js app&#xA;‚îÇ   ‚îú‚îÄ‚îÄ app/              # Next.js app directory&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx      # Main application page&#xA;‚îÇ   ‚îî‚îÄ‚îÄ components/       # React components&#xA;‚îÇ       ‚îî‚îÄ‚îÄ Mermaid.tsx   # Mermaid diagram renderer&#xA;‚îÇ&#xA;‚îú‚îÄ‚îÄ public/               # Static assets&#xA;‚îú‚îÄ‚îÄ package.json          # JavaScript dependencies&#xA;‚îî‚îÄ‚îÄ .env                  # Environment variables (create this)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ñ Provider-Based Model Selection System&lt;/h2&gt; &#xA;&lt;p&gt;DeepWiki now implements a flexible provider-based model selection system supporting multiple LLM providers:&lt;/p&gt; &#xA;&lt;h3&gt;Supported Providers and Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Default &lt;code&gt;gemini-2.0-flash&lt;/code&gt;, also supports &lt;code&gt;gemini-1.5-flash&lt;/code&gt;, &lt;code&gt;gemini-1.0-pro&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-4o&lt;/code&gt;, also supports &lt;code&gt;o4-mini&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;: Access to multiple models via a unified API, including Claude, Llama, Mistral, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-4o&lt;/code&gt;, also supports &lt;code&gt;o4-mini&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Support for locally running open-source models like &lt;code&gt;llama3&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;p&gt;Each provider requires its corresponding API key environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# API Keys&#xA;GOOGLE_API_KEY=your_google_api_key        # Required for Google Gemini models&#xA;OPENAI_API_KEY=your_openai_api_key        # Required for OpenAI models&#xA;OPENROUTER_API_KEY=your_openrouter_api_key # Required for OpenRouter models&#xA;AZURE_OPENAI_API_KEY=your_azure_openai_api_key  #Required for Azure OpenAI models&#xA;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  #Required for Azure OpenAI models&#xA;AZURE_OPENAI_VERSION=your_azure_openai_version  #Required for Azure OpenAI models&#xA;&#xA;# OpenAI API Base URL Configuration&#xA;OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, for custom OpenAI API endpoints&#xA;&#xA;# Ollama host&#xA;OLLAMA_HOST=your_ollama_host # Optional, if Ollama is not local. default: http://localhost:11434&#xA;&#xA;# Configuration Directory&#xA;DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, for custom config file location&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration Files&lt;/h3&gt; &#xA;&lt;p&gt;DeepWiki uses JSON configuration files to manage various aspects of the system:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;generator.json&lt;/code&gt;&lt;/strong&gt;: Configuration for text generation models&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Defines available model providers (Google, OpenAI, OpenRouter, Azure, Ollama)&lt;/li&gt; &#xA;   &lt;li&gt;Specifies default and available models for each provider&lt;/li&gt; &#xA;   &lt;li&gt;Contains model-specific parameters like temperature and top_p&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;embedder.json&lt;/code&gt;&lt;/strong&gt;: Configuration for embedding models and text processing&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Defines embedding models for vector storage&lt;/li&gt; &#xA;   &lt;li&gt;Contains retriever configuration for RAG&lt;/li&gt; &#xA;   &lt;li&gt;Specifies text splitter settings for document chunking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;repo.json&lt;/code&gt;&lt;/strong&gt;: Configuration for repository handling&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Contains file filters to exclude certain files and directories&lt;/li&gt; &#xA;   &lt;li&gt;Defines repository size limits and processing rules&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By default, these files are located in the &lt;code&gt;api/config/&lt;/code&gt; directory. You can customize their location using the &lt;code&gt;DEEPWIKI_CONFIG_DIR&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h3&gt;Custom Model Selection for Service Providers&lt;/h3&gt; &#xA;&lt;p&gt;The custom model selection feature is specifically designed for service providers who need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can offer multiple AI model choices to users within your organization&lt;/li&gt; &#xA; &lt;li&gt;You can quickly adapt to the rapidly evolving LLM landscape without code changes&lt;/li&gt; &#xA; &lt;li&gt;You can support specialized or fine-tuned models that aren&#39;t in the predefined list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Service providers can implement their model offerings by selecting from the predefined options or entering custom model identifiers in the frontend interface.&lt;/p&gt; &#xA;&lt;h3&gt;Base URL Configuration for Enterprise Private Channels&lt;/h3&gt; &#xA;&lt;p&gt;The OpenAI Client&#39;s base_url configuration is designed primarily for enterprise users with private API channels. This feature:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enables connection to private or enterprise-specific API endpoints&lt;/li&gt; &#xA; &lt;li&gt;Allows organizations to use their own self-hosted or custom-deployed LLM services&lt;/li&gt; &#xA; &lt;li&gt;Supports integration with third-party OpenAI API-compatible services&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coming Soon&lt;/strong&gt;: In future updates, DeepWiki will support a mode where users need to provide their own API keys in requests. This will allow enterprise customers with private channels to use their existing API arrangements without sharing credentials with the DeepWiki deployment.&lt;/p&gt; &#xA;&lt;h2&gt;üß© Using OpenAI-Compatible Embedding Models (e.g., Alibaba Qwen)&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use embedding models compatible with the OpenAI API (such as Alibaba Qwen), follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Replace the contents of &lt;code&gt;api/config/embedder.json&lt;/code&gt; with those from &lt;code&gt;api/config/embedder_openai_compatible.json&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In your project root &lt;code&gt;.env&lt;/code&gt; file, set the relevant environment variables, for example: &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=your_api_key&#xA;OPENAI_BASE_URL=your_openai_compatible_endpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The program will automatically substitute placeholders in embedder.json with the values from your environment variables.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This allows you to seamlessly switch to any OpenAI-compatible embedding service without code changes.&lt;/p&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;DeepWiki uses Python&#39;s built-in &lt;code&gt;logging&lt;/code&gt; module for diagnostic output. You can configure the verbosity and log file destination via environment variables:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).&lt;/td&gt; &#xA;   &lt;td&gt;INFO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LOG_FILE_PATH&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to the log file. If set, logs will be written to this file.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;api/logs/application.log&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To enable debug logging and direct logs to a custom file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LOG_LEVEL=DEBUG&#xA;export LOG_FILE_PATH=./debug.log&#xA;python -m api.main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with Docker Compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running with Docker Compose, the container&#39;s &lt;code&gt;api/logs&lt;/code&gt; directory is bind-mounted to &lt;code&gt;./api/logs&lt;/code&gt; on your host (see the &lt;code&gt;volumes&lt;/code&gt; section in &lt;code&gt;docker-compose.yml&lt;/code&gt;), ensuring log files persist across restarts.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can store these settings in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LOG_LEVEL=DEBUG&#xA;LOG_FILE_PATH=./debug.log&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging Path Security Considerations:&lt;/strong&gt; In production environments, ensure the &lt;code&gt;api/logs&lt;/code&gt; directory and any custom log file path are secured with appropriate filesystem permissions and access controls. The application enforces that &lt;code&gt;LOG_FILE_PATH&lt;/code&gt; resides within the project&#39;s &lt;code&gt;api/logs&lt;/code&gt; directory to prevent path traversal or unauthorized writes.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Advanced Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Google Gemini API key for AI generation&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use Google Gemini models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI API key for embeddings&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Note: This is required even if you&#39;re not using OpenAI models, as it&#39;s used for embeddings.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OpenRouter API key for alternative models&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use OpenRouter models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Azure OpenAI API key&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Azure OpenAI endpoint&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_VERSION&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Azure OpenAI version&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OLLAMA_HOST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ollama Host (default: &lt;a href=&#34;http://localhost:11434&#34;&gt;http://localhost:11434&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Required only if you want to use external Ollama server&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Port for the API server (default: 8001)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;If you host API and frontend on the same machine, make sure change port of &lt;code&gt;SERVER_BASE_URL&lt;/code&gt; accordingly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SERVER_BASE_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Base URL for the API server (default: &lt;a href=&#34;http://localhost:8001&#34;&gt;http://localhost:8001&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Set to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; to enable authorization mode.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Defaults to &lt;code&gt;false&lt;/code&gt;. If enabled, &lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt; is required.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The secret code required for wiki generation when &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is enabled.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Only used if &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you&#39;re not using ollama mode, you need to configure an OpenAI API key for embeddings. Other API keys are only required when configuring and using models from the corresponding providers.&lt;/p&gt; &#xA;&lt;h2&gt;Authorization Mode&lt;/h2&gt; &#xA;&lt;p&gt;DeepWiki can be configured to run in an authorization mode, where wiki generation requires a valid authorization code. This is useful if you want to control who can use the generation feature. Restricts frontend initiation and protects cache deletion, but doesn&#39;t fully prevent backend generation if API endpoints are hit directly.&lt;/p&gt; &#xA;&lt;p&gt;To enable authorization mode, set the following environment variables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;: Set this to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. When enabled, the frontend will display an input field for the authorization code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;: Set this to the desired secret code. Restricts frontend initiation and protects cache deletion, but doesn&#39;t fully prevent backend generation if API endpoints are hit directly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is not set or is set to &lt;code&gt;false&lt;/code&gt; (or any other value than &lt;code&gt;true&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;), the authorization feature will be disabled, and no code will be required.&lt;/p&gt; &#xA;&lt;h3&gt;Docker Setup&lt;/h3&gt; &#xA;&lt;p&gt;You can use Docker to run DeepWiki:&lt;/p&gt; &#xA;&lt;h4&gt;Running the Container&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Pull the image from GitHub Container Registry&#xA;docker pull ghcr.io/asyncfuncai/deepwiki-open:latest&#xA;&#xA;# Run the container with environment variables&#xA;docker run -p 8001:8001 -p 3000:3000 \&#xA;  -e GOOGLE_API_KEY=your_google_api_key \&#xA;  -e OPENAI_API_KEY=your_openai_api_key \&#xA;  -e OPENROUTER_API_KEY=your_openrouter_api_key \&#xA;  -e OLLAMA_HOST=your_ollama_host \&#xA;  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \&#xA;  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \&#xA;  -e AZURE_OPENAI_VERSION=your_azure_openai_version \&#xA;&#xA;  -v ~/.adalflow:/root/.adalflow \&#xA;  ghcr.io/asyncfuncai/deepwiki-open:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; &#xA;&lt;p&gt;Or use the provided &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Edit the .env file with your API keys first&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(The &lt;code&gt;docker-compose.yml&lt;/code&gt; file is pre-configured to mount &lt;code&gt;~/.adalflow&lt;/code&gt; for data persistence, similar to the &lt;code&gt;docker run&lt;/code&gt; command above.)&lt;/p&gt; &#xA;&lt;h4&gt;Using a .env file with Docker&lt;/h4&gt; &#xA;&lt;p&gt;You can also mount a .env file to the container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a .env file with your API keys&#xA;echo &#34;GOOGLE_API_KEY=your_google_api_key&#34; &amp;gt; .env&#xA;echo &#34;OPENAI_API_KEY=your_openai_api_key&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;OPENROUTER_API_KEY=your_openrouter_api_key&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;AZURE_OPENAI_API_KEY=your_azure_openai_api_key&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint&#34; &amp;gt;&amp;gt; .env&#xA;echo &#34;AZURE_OPENAI_VERSION=your_azure_openai_version&#34;  &amp;gt;&amp;gt;.env&#xA;echo &#34;OLLAMA_HOST=your_ollama_host&#34; &amp;gt;&amp;gt; .env&#xA;&#xA;# Run the container with the .env file mounted&#xA;docker run -p 8001:8001 -p 3000:3000 \&#xA;  -v $(pwd)/.env:/app/.env \&#xA;  -v ~/.adalflow:/root/.adalflow \&#xA;  ghcr.io/asyncfuncai/deepwiki-open:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; &#xA;&lt;h4&gt;Building the Docker image locally&lt;/h4&gt; &#xA;&lt;p&gt;If you want to build the Docker image locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/AsyncFuncAI/deepwiki-open.git&#xA;cd deepwiki-open&#xA;&#xA;# Build the Docker image&#xA;docker build -t deepwiki-open .&#xA;&#xA;# Run the container&#xA;docker run -p 8001:8001 -p 3000:3000 \&#xA;  -e GOOGLE_API_KEY=your_google_api_key \&#xA;  -e OPENAI_API_KEY=your_openai_api_key \&#xA;  -e OPENROUTER_API_KEY=your_openrouter_api_key \&#xA;  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \&#xA;  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \&#xA;  -e AZURE_OPENAI_VERSION=your_azure_openai_version \&#xA;  -e OLLAMA_HOST=your_ollama_host \&#xA;  deepwiki-open&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Self-Signed Certificates in Docker&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re in an environment that uses self-signed certificates, you can include them in the Docker build:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a directory for your certificates (default is &lt;code&gt;certs&lt;/code&gt; in your project root)&lt;/li&gt; &#xA; &lt;li&gt;Copy your &lt;code&gt;.crt&lt;/code&gt; or &lt;code&gt;.pem&lt;/code&gt; certificate files into this directory&lt;/li&gt; &#xA; &lt;li&gt;Build the Docker image:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build with default certificates directory (certs)&#xA;docker build .&#xA;&#xA;# Or build with a custom certificates directory&#xA;docker build --build-arg CUSTOM_CERT_DIR=my-custom-certs .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API Server Details&lt;/h3&gt; &#xA;&lt;p&gt;The API server provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Repository cloning and indexing&lt;/li&gt; &#xA; &lt;li&gt;RAG (Retrieval Augmented Generation)&lt;/li&gt; &#xA; &lt;li&gt;Streaming chat completions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see the &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/api/README.md&#34;&gt;API README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîå OpenRouter Integration&lt;/h2&gt; &#xA;&lt;p&gt;DeepWiki now supports &lt;a href=&#34;https://openrouter.ai/&#34;&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to hundreds of AI models through a single API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Model Options&lt;/strong&gt;: Access models from OpenAI, Anthropic, Google, Meta, Mistral, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Configuration&lt;/strong&gt;: Just add your OpenRouter API key and select the model you want to use&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: Choose models that fit your budget and performance needs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy Switching&lt;/strong&gt;: Toggle between different models without changing your code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to Use OpenRouter with DeepWiki&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Get an API Key&lt;/strong&gt;: Sign up at &lt;a href=&#34;https://openrouter.ai/&#34;&gt;OpenRouter&lt;/a&gt; and get your API key&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add to Environment&lt;/strong&gt;: Add &lt;code&gt;OPENROUTER_API_KEY=your_key&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enable in UI&lt;/strong&gt;: Check the &#34;Use OpenRouter API&#34; option on the homepage&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Select Model&lt;/strong&gt;: Choose from popular models like GPT-4o, Claude 3.5 Sonnet, Gemini 2.0, and more&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;OpenRouter is particularly useful if you want to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try different models without signing up for multiple services&lt;/li&gt; &#xA; &lt;li&gt;Access models that might be restricted in your region&lt;/li&gt; &#xA; &lt;li&gt;Compare performance across different model providers&lt;/li&gt; &#xA; &lt;li&gt;Optimize for cost vs. performance based on your needs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ñ Ask &amp;amp; DeepResearch Features&lt;/h2&gt; &#xA;&lt;h3&gt;Ask Feature&lt;/h3&gt; &#xA;&lt;p&gt;The Ask feature allows you to chat with your repository using Retrieval Augmented Generation (RAG):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context-Aware Responses&lt;/strong&gt;: Get accurate answers based on the actual code in your repository&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RAG-Powered&lt;/strong&gt;: The system retrieves relevant code snippets to provide grounded responses&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-Time Streaming&lt;/strong&gt;: See responses as they&#39;re generated for a more interactive experience&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conversation History&lt;/strong&gt;: The system maintains context between questions for more coherent interactions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepResearch Feature&lt;/h3&gt; &#xA;&lt;p&gt;DeepResearch takes repository analysis to the next level with a multi-turn research process:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;In-Depth Investigation&lt;/strong&gt;: Thoroughly explores complex topics through multiple research iterations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Process&lt;/strong&gt;: Follows a clear research plan with updates and a comprehensive conclusion&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Continuation&lt;/strong&gt;: The AI automatically continues research until reaching a conclusion (up to 5 iterations)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research Stages&lt;/strong&gt;: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Research Plan&lt;/strong&gt;: Outlines the approach and initial findings&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Research Updates&lt;/strong&gt;: Builds on previous iterations with new insights&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Final Conclusion&lt;/strong&gt;: Provides a comprehensive answer based on all iterations&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use DeepResearch, simply toggle the &#34;Deep Research&#34; switch in the Ask interface before submitting your question.&lt;/p&gt; &#xA;&lt;h2&gt;üì± Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Interface.png&#34; alt=&#34;DeepWiki Main Interface&#34;&gt; &lt;em&gt;The main interface of DeepWiki&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/privaterepo.png&#34; alt=&#34;Private Repository Support&#34;&gt; &lt;em&gt;Access private repositories with personal access tokens&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/DeepResearch.png&#34; alt=&#34;DeepResearch Feature&#34;&gt; &lt;em&gt;DeepResearch conducts multi-turn investigations for complex topics&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo Video&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/zGANs8US8B4&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/zGANs8US8B4/0.jpg&#34; alt=&#34;DeepWiki Demo Video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Watch DeepWiki in action!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ùì Troubleshooting&lt;/h2&gt; &#xA;&lt;h3&gt;API Key Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Missing environment variables&#34;&lt;/strong&gt;: Make sure your &lt;code&gt;.env&lt;/code&gt; file is in the project root and contains the required API keys&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;API key not valid&#34;&lt;/strong&gt;: Check that you&#39;ve copied the full key correctly with no extra spaces&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;OpenRouter API error&#34;&lt;/strong&gt;: Verify your OpenRouter API key is valid and has sufficient credits&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Azure OpenAI API error&#34;&lt;/strong&gt;: Verify your Azure OpenAI credentials (API key, endpoint, and version) are correct and the service is properly deployed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Connection Problems&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Cannot connect to API server&#34;&lt;/strong&gt;: Make sure the API server is running on port 8001&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;CORS error&#34;&lt;/strong&gt;: The API is configured to allow all origins, but if you&#39;re having issues, try running both frontend and backend on the same machine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generation Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Error generating wiki&#34;&lt;/strong&gt;: For very large repositories, try a smaller one first&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Invalid repository format&#34;&lt;/strong&gt;: Make sure you&#39;re using a valid GitHub, GitLab or Bitbucket URL format&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Could not fetch repository structure&#34;&lt;/strong&gt;: For private repositories, ensure you&#39;ve entered a valid personal access token with appropriate permissions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Diagram rendering error&#34;&lt;/strong&gt;: The app will automatically try to fix broken diagrams&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Common Solutions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restart both servers&lt;/strong&gt;: Sometimes a simple restart fixes most issues&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Check console logs&lt;/strong&gt;: Open browser developer tools to see any JavaScript errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Check API logs&lt;/strong&gt;: Look at the terminal where the API is running for Python errors&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Feel free to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open issues for bugs or feature requests&lt;/li&gt; &#xA; &lt;li&gt;Submit pull requests to improve the code&lt;/li&gt; &#xA; &lt;li&gt;Share your feedback and ideas&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#AsyncFuncAI/deepwiki-open&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>