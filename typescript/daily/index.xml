<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-17T01:33:37Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sugarforever/chat-ollama</title>
    <updated>2024-05-17T01:33:37Z</updated>
    <id>tag:github.com,2024-05-17:/sugarforever/chat-ollama</id>
    <link href="https://github.com/sugarforever/chat-ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatOllama is an open source chatbot based on LLMs. It supports a wide range of language models, and knowledge base management.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/README.zh-Hans.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ChatOllama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;ChatOllama&lt;/code&gt; is an open source chatbot based on LLMs. It supports a wide range of language models including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ollama served models&lt;/li&gt; &#xA; &lt;li&gt;OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Azure OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Anthropic&lt;/li&gt; &#xA; &lt;li&gt;Moonshot&lt;/li&gt; &#xA; &lt;li&gt;Gemini&lt;/li&gt; &#xA; &lt;li&gt;Groq&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;ChatOllama&lt;/code&gt; supports multiple types of chat:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Free chat with LLMs&lt;/li&gt; &#xA; &lt;li&gt;Chat with LLMs based on knowledge base&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;ChatOllama&lt;/code&gt; feature list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ollama models management&lt;/li&gt; &#xA; &lt;li&gt;Knowledge bases management&lt;/li&gt; &#xA; &lt;li&gt;Chat&lt;/li&gt; &#xA; &lt;li&gt;Commercial LLMs API keys management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Join Our Community&lt;/h2&gt; &#xA;&lt;p&gt;If you are a user, contributor, or even just new to &lt;code&gt;ChatOllama&lt;/code&gt;, you are more than welcome to join our community on Discord by clicking the &lt;a href=&#34;https://discord.gg/TjhZGYv5pC&#34;&gt;invite link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are a contributor, the channel &lt;code&gt;technical-discussion&lt;/code&gt; is for you, where we discuss technical stuff.&lt;/p&gt; &#xA;&lt;p&gt;If you have any issue in &lt;code&gt;ChatOllama&lt;/code&gt; usage, please report to channel &lt;code&gt;customer-support&lt;/code&gt;. We will help you out as soon as we can.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;As a user of &lt;code&gt;ChatOllama&lt;/code&gt;, please walk through the document below, to make sure you get all the components up and running before starting using &lt;code&gt;ChatOllama&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Vector Databases&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ChatOllama&lt;/code&gt; supported 2 types of vector databases: Milvus and Chroma.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the &lt;code&gt;.env.example&lt;/code&gt; for how to work with your vector database setup.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Supported values: chroma, milvus&#xA;VECTOR_STORE=chroma&#xA;CHROMADB_URL=http://localhost:8000&#xA;MILVUS_URL=http://localhost:19530&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default &lt;code&gt;ChatOllama&lt;/code&gt; is using Chroma. If you&#39;d like to use Milvus, set &lt;code&gt;VECTOR_STORE&lt;/code&gt; to &lt;code&gt;milvus&lt;/code&gt; and specify the corresponding URL. It works both in the development server and Docker containers.&lt;/p&gt; &#xA;&lt;h3&gt;Use with Nuxt 3 Development Server&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to run with the latest code base and apply changes as needed, you can clone this repository and follow the steps below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install and run Ollama server&lt;/p&gt; &lt;p&gt;You will need an Ollama server running. Follow the installation guide of &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;. By default, it&#39;s running on &lt;a href=&#34;http://localhost:11434&#34;&gt;http://localhost:11434&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Chroma&lt;/p&gt; &lt;p&gt;Please refer to &lt;a href=&#34;https://docs.trychroma.com/getting-started&#34;&gt;https://docs.trychroma.com/getting-started&lt;/a&gt; for Chroma installation.&lt;/p&gt; &lt;p&gt;We recommend you run it in a docker container:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#https://hub.docker.com/r/chromadb/chroma/tags&#xA;&#xA;docker pull chromadb/chroma&#xA;docker run -d -p 8000:8000 chromadb/chroma&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, ChromaDB is running on &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ChatOllama Setup&lt;/p&gt; &lt;p&gt;Now, we can complete the necessary setup to run ChatOllama.&lt;/p&gt; &lt;p&gt;3.1 Copy the &lt;code&gt;.env.example&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.2 Make sure to install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.3 Run a migration to create your database tables with Prisma Migrate&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm prisma-migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch Development Server&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Make sure both &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/#ollama-server&#34;&gt;Ollama Server&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/#install-chromadb-and-startup&#34;&gt;ChromaDB&lt;/a&gt;&lt;/strong&gt; are running.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;Start the development server on &lt;code&gt;http://localhost:3000&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Use with Docker&lt;/h3&gt; &#xA;&lt;p&gt;This is the easist way to use &lt;code&gt;ChatOllama&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The only thing you need is a copy of &lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt;. Please download it and execute the command below to launch &lt;code&gt;ChatOllama&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As &lt;code&gt;ChatOllama&lt;/code&gt; is running within a docker container, you should set Ollama server to &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt; in the Settings section, assuming your Ollama server is running locally with default port.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you initialize the SQLite database as below if you are launching the dockerized &lt;code&gt;ChatOllama&lt;/code&gt; for the first time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker compose exec chatollama npx prisma migrate dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Prerequisites for knowledge bases&lt;/h4&gt; &#xA;&lt;p&gt;When using KnowledgeBases, we need a valid embedding model in place. It can be one of the models downloaded by Ollama or from 3rd party service provider for example, OpenAI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ollama Managed Embedding Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We recommend you download &lt;code&gt;nomic-embed-text&lt;/code&gt; model for embedding purpose.&lt;/p&gt; &#xA;&lt;p&gt;You can do so on Models page &lt;a href=&#34;http://localhost:3000/models&#34;&gt;http://localhost:3000/models&lt;/a&gt;, or via CLI as below if you are using Docker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# In the folder of docker-compose.yaml&#xA;&#xA;$ docker compose exec ollama ollama pull nomic-embed-text:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenAI Embedding Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to use OpenAI, please make sure you set a valid OpenAI API Key in Settings, and fill with one of the OpenAI embedding models listed below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text-embedding-3-large&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text-embedding-3-small&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Data Storage with Docker Containers&lt;/h4&gt; &#xA;&lt;p&gt;There are 2 types of data storage, vector data and relational data. See the summary below and for more details, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt; for the settings.&lt;/p&gt; &#xA;&lt;h5&gt;Vector data&lt;/h5&gt; &#xA;&lt;p&gt;With &lt;code&gt;docker-compose.yaml&lt;/code&gt;, a dockerized Chroma database is run side by side with &lt;code&gt;ChatOllama&lt;/code&gt;. The data is persisted in a docker volume.&lt;/p&gt; &#xA;&lt;h5&gt;Relational data&lt;/h5&gt; &#xA;&lt;p&gt;The relational data including knowledge base records and their associated files are stored in a SQLite database file persisted and mounted from &lt;code&gt;~/.chatollama/chatollama.sqlite&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Proxy&lt;/h4&gt; &#xA;&lt;p&gt;We have provided a proxy configuration feature. For specific usage, please click &lt;a href=&#34;https://raw.githubusercontent.com/sugarforever/chat-ollama/main/docs/proxy-usage.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Developers Guide&lt;/h2&gt; &#xA;&lt;p&gt;As ChatOllama is still under active development, features, interfaces and database schema may be changed. Please follow the instructions below in your every &lt;code&gt;git pull&lt;/code&gt; to make sure your dependencies and database schema are always in sync.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the latest dependencies &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pnpm install&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Prisma migrate &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pnpm prisma-migrate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>