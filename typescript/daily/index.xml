<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-30T01:48:22Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>agnaistic/agnai</title>
    <updated>2023-05-30T01:48:22Z</updated>
    <id>tag:github.com,2023-05-30:/agnaistic/agnai</id>
    <link href="https://github.com/agnaistic/agnai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI Agnostic (Multi-user and Multi-bot) Chat with Personalised Characters. Designed with scale in mind.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agnaistic&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;AI Agnostic Chat with Personalised Characters. Self-hosted or Multi-tenant.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;AI Agnostic Chat&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://agnai.chat&#34;&gt;Live Version&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/luminai&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://github.com/users/sceuick/projects/1&#34;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can visit the hosted version at &lt;a href=&#34;https://agnai.chat&#34;&gt;Agnai.chat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div style=&#34;display: flex; flex-direction: row; gap: 0.5rem;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/agnaistic/agnai/dev/screenshots/chat.png&#34; height=&#34;150&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/agnaistic/agnai/dev/screenshots/persona.png&#34; height=&#34;150&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/agnaistic/agnai/dev/screenshots/settings.png&#34; height=&#34;150&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Based upon the early work of &lt;a href=&#34;https://github.com/PygmalionAI/galatea-ui&#34;&gt;https://github.com/PygmalionAI/galatea-ui&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;(Requires Node.JS)&lt;br&gt; Agnaistic is bundled as an NPM package and can be installed globally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Install or update:&#xA;&amp;gt; npm install agnai -g&#xA;&amp;gt; agnai&#xA;&#xA;# View launch options:&#xA;&amp;gt; agnai help&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using the NPM package, your images and JSON files will be stored in: &lt;code&gt;HOME_FOLDER/.agnai&lt;/code&gt;.&lt;br&gt; E.g. &lt;code&gt;/home/sceuick/.agnai/json&lt;/code&gt; &lt;code&gt;/home/sceuick/.agnai/assets&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Group Conversations&lt;/strong&gt;: Multiple users with one character/bot&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple AI services&lt;/strong&gt;: Support for Kobold, Novel, AI Horde, LuminAI, OpenAI, Claude&lt;/li&gt; &#xA; &lt;li&gt;Multiple persona schema formats: W++, Square bracket format (SBF), Boostyle, Plain text&lt;/li&gt; &#xA; &lt;li&gt;Multi-tenancy: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;User authentication&lt;/li&gt; &#xA;   &lt;li&gt;User settings: Which AI service to use and their own settings&lt;/li&gt; &#xA;   &lt;li&gt;User generation settings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Chat specific overrides: AI Service, Character, Generation Settings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running with Docker&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the project&lt;/li&gt; &#xA; &lt;li&gt;With MongoDB: &lt;code&gt;docker compose -p agnai -f self-host.docker-compose.yml up -d&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Without MongoDB: &lt;code&gt;docker run -dt --restart=always -p 3001:3001 ghcr.io/agnaistic/agnaistic:latest&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;-dt&lt;/code&gt; Run the container detached&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--restart=always&lt;/code&gt; Restart at start up or if the server crashes&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-p 3001:3001&lt;/code&gt; Expose port 3001. Access the app at &lt;code&gt;http://localhost:3001&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running Manually&lt;/h2&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://nodejs.org/en/download/&#34;&gt;Node.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.mongodb.com/docs/manual/installation/&#34;&gt;MongoDB&lt;/a&gt; &lt;strong&gt;Optional&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The database is optional. Agnaistic will run in &lt;code&gt;anonymous-only&lt;/code&gt; mode if there is no database available.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Anonymous&lt;/code&gt; users have their data saved to the browser&#39;s local storage. Your data will &#34;persist&#34;, but not be shareable between devices or other browsers. Clearing your browser&#39;s application data/cookies will delete this data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Download the project: &lt;code&gt;git clone https://github.com/agnaistic/agnai&lt;/code&gt; or &lt;a href=&#34;https://github.com/agnaistic/agnai/archive/refs/heads/dev.zip&#34;&gt;download it&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;From inside the cloned/unpacked folder in your terminal/console: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;npm run deps&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Do this every time you update AgnAI, just in case.&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;This will install the dependencies using &lt;code&gt;pnpm v6&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;npm run build:all&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Build and run the project in watch mode: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Mac/Linux: &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Windows: &lt;code&gt;npm run start:win&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Build and run the project with Local Tunnel: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Mac/Linux: &lt;code&gt;npm run start:public&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Windows: &lt;code&gt;npm run start:public:win&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Design Goals&lt;/h2&gt; &#xA;&lt;p&gt;This project quickly deviated from the upstream project. This project is not intended to be a SaaS nor be centered around the Pygmalion model.&lt;br&gt; Ultimately the design goals for this project are my own.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High quality codebase&lt;/li&gt; &#xA; &lt;li&gt;AI Services: Transparently use a variety of AI models and services to converse with &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Initial AI services: Kobold, AI Horde, and Novel&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Supporting additional AI services should be low friction&lt;/li&gt; &#xA; &lt;li&gt;Lightweight to self-host&lt;/li&gt; &#xA; &lt;li&gt;Avoiding native dependencies and Docker to be easy for non-technical people to install and run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Self-Hosting Settings&lt;/h2&gt; &#xA;&lt;p&gt;To try and cater for the small tweaks and tuning that people need for their specific needs at an application level we have &lt;code&gt;settings.json&lt;/code&gt;.&lt;br&gt; You can create a file called &lt;code&gt;settings.json&lt;/code&gt; at the root level to apply some changes across the entire application.&lt;br&gt; If you have a specific need for your application, this is the place to ask to have it catered for.&lt;/p&gt; &#xA;&lt;p&gt;I will try and find a balance between catering to these requests and not having them get out of control in the codebase.&lt;/p&gt; &#xA;&lt;p&gt;Examples of requests that are suited for this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I want a &#34;default memory book&#34; applied to all users.&lt;/li&gt; &#xA; &lt;li&gt;I want to use a different set of end tokens than the ones provided.&lt;/li&gt; &#xA; &lt;li&gt;I want to disable anonymous access&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;settings.json&lt;/h3&gt; &#xA;&lt;p&gt;You can copy or look at &lt;code&gt;template.settings.json&lt;/code&gt; for an example of all of the available settings. You will need to restart Agnai for changes to take effect.&lt;/p&gt; &#xA;&lt;p&gt;Currently supported custom settings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;baseEndTokens&lt;/code&gt;: Add extra response end tokens to the base set.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;For Developers&lt;/h2&gt; &#xA;&lt;h3&gt;Recommended Development Tooling&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;d highly recommend using &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;VSCode&lt;/a&gt; with the following extensions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Prettier - Code formatter&lt;/code&gt;: For auto-formatting&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tailwind CSS Intellisense&lt;/code&gt;: For auto-completion and intellisense with Tailwind CSS classes&lt;/li&gt; &#xA; &lt;li&gt;And adding &lt;code&gt;&#34;editor.formatOnSave&#34;: true&lt;/code&gt; to your VSCode &lt;code&gt;settings.json&lt;/code&gt; to auto-format with Prettier&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When using &lt;code&gt;pnpm start&lt;/code&gt;, the Node.JS server is run using &lt;code&gt;--inspect&lt;/code&gt;. This means you can use various &lt;a href=&#34;https://nodejs.org/en/docs/guides/debugging-getting-started/#inspector-clients&#34;&gt;Inspector Clients&lt;/a&gt; for debugging.&lt;/p&gt; &#xA;&lt;h3&gt;Tech Stack&lt;/h3&gt; &#xA;&lt;p&gt;The important parts of the stack are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mongodb.com/docs/manual/installation/&#34;&gt;MongoDB&lt;/a&gt; for persistence&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://redis.io&#34;&gt;Redis&lt;/a&gt; for distributed messaging for websockets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.solidjs.com/&#34;&gt;SolidJS&lt;/a&gt; for interactivity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com/&#34;&gt;TailwindCSS&lt;/a&gt; for styling&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pnpm.io/&#34;&gt;pnpm&lt;/a&gt; for dependency management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Starting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies - Always run this after pulling changes&#xA;&amp;gt; npm run deps&#xA;&#xA;# Run MongoDB using Docker&#xA;&amp;gt; npm run up&#xA;&#xA;# Start the frontend, backend, and python service&#xA;# Mac/Linux&#xA;&amp;gt; npm start&#xA;&#xA;# Windows&#xA;&amp;gt; npm run start:win&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Recommended Developer Tooling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Redux Dev Tools &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The front-end application state is wired up to the &#34;Redux Dev Tools&#34; Chrome extension.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;NodeJS debugger &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The &lt;code&gt;pnpm start&lt;/code&gt; script launchs the NodeJS API using the &lt;code&gt;--inspect&lt;/code&gt; flag&lt;/li&gt; &#xA;   &lt;li&gt;Attach using the default launch task in VSCode (&lt;code&gt;F5&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Or go to the url &lt;code&gt;chrome://inspect&lt;/code&gt; to use the debugger&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Format and Type Checking&lt;/h3&gt; &#xA;&lt;p&gt;The project uses ESLint for linting, Prettier for enforcing code style and TypeScript to check for type errors. When opening a PR, please make sure you&#39;re not introducing any new errors in any of these checks by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# auto-fixes any style problems&#xA;$ pnpm run format:fix&#xA;&#xA;# runs the TypeScript compiler so any type errors will be shown&#xA;$ pnpm run typecheck&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mlc-ai/web-llm</title>
    <updated>2023-05-30T01:48:22Z</updated>
    <id>tag:github.com,2023-05-30:/mlc-ai/web-llm</id>
    <link href="https://github.com/mlc-ai/web-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing large-language models and chat to web browsers. Everything runs inside the browser with no server support.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Web LLM&lt;/h1&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://www.npmjs.com/package/@mlc-ai/web-llm&#34;&gt;NPM Package&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/#get-started&#34;&gt;Get Started&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples&#34;&gt;Examples&lt;/a&gt;| &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/9Xpy2HGBuD&#34;&gt;Discord&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;WebLLM is a modular, customizable javascript package that directly brings language model chats directly onto web browsers with hardware acceleration. &lt;strong&gt;Everything runs inside the browser with no server support and accelerated with WebGPU.&lt;/strong&gt; We can bring a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://mlc.ai/web-llm/&#34;&gt;Check out our demo webpage to try out!&lt;/a&gt;&lt;/strong&gt; This project is a companion project of &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt;, our companion project that runs LLMs natively on iPhone and other native local environments.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/site/img/fig/demo.gif&#34;&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM offers a minimalist and modular interface to access the chatbot in the browser. The WebLLM package itself does not come with UI, and is designed in a modular way to hook to any of the UI components. The following code snippet demonstrate a simple example that generates a streaming response on a webpage. You can check out &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/get-started/&#34;&gt;examples/get-started&lt;/a&gt; to see the complete example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import * as webllm from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;// We use label to intentionally keep it simple&#xA;function setLabel(id: string, text: string) {&#xA;  const label = document.getElementById(id);&#xA;  if (label == null) {&#xA;    throw Error(&#34;Cannot find label &#34; + id);&#xA;  }&#xA;  label.innerText = text;&#xA;}&#xA;&#xA;async function main() {&#xA;  // create a ChatModule,&#xA;  const chat = new webllm.ChatModule();&#xA;  // This callback allows us to report initialization progress&#xA;  chat.setInitProgressCallback((report: webllm.InitProgressReport) =&amp;gt; {&#xA;    setLabel(&#34;init-label&#34;, report.text);&#xA;  });&#xA;  // You can also try out &#34;RedPajama-INCITE-Chat-3B-v1-q4f32_0&#34;&#xA;  await chat.reload(&#34;vicuna-v1-7b-q4f32_0&#34;);&#xA;&#xA;  const generateProgressCallback = (_step: number, message: string) =&amp;gt; {&#xA;    setLabel(&#34;generate-label&#34;, message);&#xA;  };&#xA;&#xA;  const prompt0 = &#34;What is the capital of Canada?&#34;;&#xA;  setLabel(&#34;prompt-label&#34;, prompt0);&#xA;  const reply0 = await chat.generate(prompt0, generateProgressCallback);&#xA;  console.log(reply0);&#xA;&#xA;  const prompt1 = &#34;Can you write a poem about it?&#34;;&#xA;  setLabel(&#34;prompt-label&#34;, prompt1);&#xA;  const reply1 = await chat.generate(prompt1, generateProgressCallback);&#xA;  console.log(reply1);&#xA;&#xA;  console.log(await chat.runtimeStatsText());&#xA;}&#xA;&#xA;main();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Web Worker&lt;/h3&gt; &#xA;&lt;p&gt;WebLLM comes with API support for WebWorker so you can hook the generation process into a separate worker thread so that the compute in the webworker won&#39;t disrupt the UI.&lt;/p&gt; &#xA;&lt;p&gt;We first create a worker script that created a ChatModule and hook it up to a handler that handles requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// worker.ts&#xA;import { ChatWorkerHandler, ChatModule } from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;// Hookup a chat module to a worker handler&#xA;const chat = new ChatModule();&#xA;const handler = new ChatWorkerHandler(chat);&#xA;self.onmessage = (msg: MessageEvent) =&amp;gt; {&#xA;  handler.onmessage(msg);&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in the main logic, we create a &lt;code&gt;ChatWorkerClient&lt;/code&gt; that implements the same &lt;code&gt;ChatInterface&lt;/code&gt;. The rest of the logic remains the same.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;// main.ts&#xA;import * as webllm from &#34;@mlc-ai/web-llm&#34;;&#xA;&#xA;async function main() {&#xA;  // Use a chat worker client instead of ChatModule here&#xA;  const chat = new webllm.ChatWorkerClient(new Worker(&#xA;    new URL(&#39;./worker.ts&#39;, import.meta.url),&#xA;    {type: &#39;module&#39;}&#xA;  ));&#xA;  // everything else remains the same&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build a ChatApp&lt;/h3&gt; &#xA;&lt;p&gt;You can find a complete a complete chat app example in &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples/simple-chat/&#34;&gt;examples/simple-chat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Customized Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM works as a companion project of &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt;. It reuses the model artifact and builds flow of MLC LLM, please check out MLC LLM document on how to build new model weights and libraries (MLC LLM document will come in the incoming weeks). To generate the wasm needed by WebLLM, you can run with &lt;code&gt;--target webgpu&lt;/code&gt; in the mlc llm build. There are two elements of the WebLLM package that enables new models and weight variants.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model_url: Contains a URL to model artifacts, such as weights and meta-data.&lt;/li&gt; &#xA; &lt;li&gt;model_lib: The web assembly libary that contains the executables to accelerate the model computations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Both are customizable in the WebLLM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;async main() {&#xA;  const myLlamaUrl = &#34;/url/to/my/llama&#34;;&#xA;  const appConfig = {&#xA;  &#34;model_list&#34;: [&#xA;    {&#xA;      &#34;model_url&#34;: myLlamaUrl,&#xA;      &#34;local_id&#34;: &#34;MyLlama-3b-v1-q4f32_0&#34;&#xA;    }&#xA;  ],&#xA;  &#34;model_lib_map&#34;: {&#xA;    &#34;llama-v1-3b-q4f32_0&#34;: &#34;/url/to/myllama3b.wasm&#34;,&#xA;  };&#xA;  // override default&#xA;  const chatOpts = {&#xA;    &#34;repetition_penalty&#34;: 1.01&#xA;  };&#xA;&#xA;  const chat = new ChatModule();&#xA;  // load a prebuilt model&#xA;  // with a chat option override and app config&#xA;  // under the hood, it will load the model from myLlamaUrl&#xA;  // and cache it in the browser cache&#xA;  //&#xA;  // Let us assume that myLlamaUrl/mlc-config.json contains a model_lib&#xA;  // field that points to &#34;llama-v1-3b-q4f32_0&#34;&#xA;  // then chat module will initialize with these information&#xA;  await chat.reload(&#34;MyLlama-3b-v1-q4f32_0&#34;, chatOpts, appConfig);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In many cases, we only want to supply the model weight variant, but not necessarily a new model. In such cases, we can reuse the model lib. In such cases, we can just pass in the &lt;code&gt;model_list&lt;/code&gt; field and skip the model lib, and make sure the &lt;code&gt;mlc-chat-config.json&lt;/code&gt; in the model url has a model lib that points to a prebuilt version, right now the prebuilt lib includes&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;vicuna-v1-7b-q4f32_0&lt;/code&gt;: llama-7b models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RedPajama-INCITE-Chat-3B-v1-q4f32_0&lt;/code&gt;: RedPajama-3B variant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build WebLLM Package From Source&lt;/h2&gt; &#xA;&lt;p&gt;WebLLM package is a web runtime designed for &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install all the prerequisites for compilation:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://emscripten.org&#34;&gt;emscripten&lt;/a&gt;. It is an LLVM-based compiler that compiles C/C++ source code to WebAssembly. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Follow the &lt;a href=&#34;https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended&#34;&gt;installation instruction&lt;/a&gt; to install the latest emsdk.&lt;/li&gt; &#xA;     &lt;li&gt;Source &lt;code&gt;emsdk_env.sh&lt;/code&gt; by &lt;code&gt;source path/to/emsdk_env.sh&lt;/code&gt;, so that &lt;code&gt;emcc&lt;/code&gt; is reachable from PATH and the command &lt;code&gt;emcc&lt;/code&gt; works.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Install jekyll by following the &lt;a href=&#34;https://jekyllrb.com/docs/installation/&#34;&gt;official guides&lt;/a&gt;. It is the package we use for website.&lt;/li&gt; &#xA;   &lt;li&gt;Install jekyll-remote-theme by command. Try &lt;a href=&#34;https://gems.ruby-china.com/&#34;&gt;gem mirror&lt;/a&gt; if install blocked. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gem install jekyll-remote-theme&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;We can verify the successful installation by trying out &lt;code&gt;emcc&lt;/code&gt; and &lt;code&gt;jekyll&lt;/code&gt; in terminal, respectively.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup necessary environment&lt;/p&gt; &lt;p&gt;Prepare all the necessary dependencies for web build:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./scripts/prep_deps.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Buld WebLLM Package&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Validate some of the sub-packages&lt;/p&gt; &lt;p&gt;You can then go to the subfolders in &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/web-llm/main/examples&#34;&gt;examples&lt;/a&gt; to validate some of the sub-packages. We use Parcelv2 for bundling. Although Parcel is not very good at tracking parent directory changes sometimes. When you make a change in the WebLLM package, try to edit the &lt;code&gt;package.json&lt;/code&gt; of the subfolder and save it, which will trigger Parcel to rebuild.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlc.ai/web-llm/&#34;&gt;Demo page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to run LLM on native runtime, check out &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You might also be interested in &lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion/&#34;&gt;Web Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is initiated by members from CMU catalyst, UW SAMPL, SJTU, OctoML and the MLC community. We would love to continue developing and supporting the open-source ML community.&lt;/p&gt; &#xA;&lt;p&gt;This project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities make these models accessible. We would like to thank the teams behind vicuna, SentencePiece, LLaMA, Alpaca. We also would like to thank the WebAssembly, Emscripten, and WebGPU communities. Finally, thanks to Dawn and WebGPU developers.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vercel/nextjs-planetscale-nextauth-tailwindcss-template</title>
    <updated>2023-05-30T01:48:22Z</updated>
    <id>tag:github.com,2023-05-30:/vercel/nextjs-planetscale-nextauth-tailwindcss-template</id>
    <link href="https://github.com/vercel/nextjs-planetscale-nextauth-tailwindcss-template" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Admin dashboard template.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/9113740/201498864-2a900c64-d88f-4ed4-b5cf-770bcb57e1f5.png&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://user-images.githubusercontent.com/9113740/201498152-b171abb8-9225-487a-821c-6ff49ee48579.png&#34;&gt; &#xA; &lt;img alt=&#34;Shows all of the tools in the stack for this template, also listed in the README file.&#34; src=&#34;https://user-images.githubusercontent.com/9113740/201498152-b171abb8-9225-487a-821c-6ff49ee48579.png&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;strong&gt;Next.js 13 Admin Dashboard Template&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; Built with the Next.js App Router&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;http://admin-dash-template.vercel.sh/&#34;&gt;Demo&lt;/a&gt; &#xA; &lt;span&gt; · &lt;/span&gt; &#xA; &lt;a href=&#34;https://vercel.com/templates/next.js/admin-dashboard-tailwind-planetscale-react-nextjs&#34;&gt;Clone &amp;amp; Deploy&lt;/a&gt; &#xA; &lt;span&gt; &lt;/span&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This is a starter template using the following stack:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Framework - &lt;a href=&#34;https://nextjs.org/13&#34;&gt;Next.js 13&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Language - &lt;a href=&#34;https://www.typescriptlang.org&#34;&gt;TypeScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Auth - &lt;a href=&#34;https://next-auth.js.org&#34;&gt;NextAuth.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Database - &lt;a href=&#34;https://planetscale.com&#34;&gt;PlanetScale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deployment - &lt;a href=&#34;https://vercel.com/docs/concepts/next.js/overview&#34;&gt;Vercel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Styling - &lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Components - &lt;a href=&#34;https://www.tremor.so&#34;&gt;Tremor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Analytics - &lt;a href=&#34;https://vercel.com/analytics&#34;&gt;Vercel Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linting - &lt;a href=&#34;https://eslint.org&#34;&gt;ESLint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Formatting - &lt;a href=&#34;https://prettier.io&#34;&gt;Prettier&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This template uses the new Next.js App Router. This includes support for enhanced layouts, colocation of components, tests, and styles, component-level data fetching, and more.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;After creating an account with PlanetScale, you&#39;ll need to create a new database and retrieve the &lt;code&gt;DATABASE_URL&lt;/code&gt;. Optionally, you can use Vercel integration, which will add the &lt;code&gt;DATABASE_URL&lt;/code&gt; to the environment variables for your project.&lt;/p&gt; &#xA;&lt;p&gt;This is the provided &lt;code&gt;.env.local.example&lt;/code&gt; file, which you&#39;ll want to use to create your own &lt;code&gt;.env.local&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# https://vercel.com/integrations/planetscale&#xA;DATABASE_URL=&#xA;&#xA;NEXTAUTH_URL=http://localhost:3000&#xA;NEXTAUTH_SECRET= # Linux: `openssl rand -hex 32` or go to https://generate-secret.now.sh/32&#xA;&#xA;# https://next-auth.js.org/providers/github&#xA;GITHUB_ID=&#xA;GITHUB_SECRET=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, inside PlanetScale, create a users table based on the schema defined in this repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CREATE TABLE `users` (&#xA;  `id` int NOT NULL AUTO_INCREMENT,&#xA;  `email` varchar(255) NOT NULL,&#xA;  `name` varchar(255),&#xA;  `username` varchar(255),&#xA;  PRIMARY KEY (`id`)&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Insert a row for testing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;INSERT INTO `users` (`id`, `email`, `name`, `username`) VALUES (1, &#39;me@site.com&#39;, &#39;Me&#39;, &#39;username&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run the following commands to start the development server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pnpm install&#xA;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should now be able to access the application at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>