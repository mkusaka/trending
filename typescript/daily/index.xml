<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-03T01:38:36Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lmstudio-ai/lmstudio.js</title>
    <updated>2025-02-03T01:38:36Z</updated>
    <id>tag:github.com,2025-02-03:/lmstudio-ai/lmstudio.js</id>
    <link href="https://github.com/lmstudio-ai/lmstudio.js" rel="alternate"></link>
    <summary type="html">&lt;p&gt;👾 LM Studio TypeScript SDK (pre-release public alpha)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/lmstudio-ai/lmstudio.js/assets/3611042/dd0b2298-beec-4dfe-9019-7d4dc5427e40&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/lmstudio-ai/lmstudio.js/assets/3611042/70f24e8f-302b-465d-8607-8c3f36cd4934&#34;&gt; &#xA;  &lt;img alt=&#34;lmstudio javascript library logo&#34; src=&#34;https://github.com/lmstudio-ai/lmstudio.js/assets/3611042/70f24e8f-302b-465d-8607-8c3f36cd4934&#34; width=&#34;290&#34; height=&#34;86&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;code&gt;Use local LLMs in JS/TS/Node&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;LM Studio Client SDK - Pre-Release&lt;/i&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Pre-Release Alpha&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;lmstudio.js&lt;/code&gt; is in pre-release alpha, and is undergoing rapid and continuous development. Expect breaking changes!&lt;/p&gt; &#xA;&lt;p&gt;Follow along for our upcoming announcements about &lt;code&gt;lmstudio.js&lt;/code&gt; on &lt;a href=&#34;https://lmstudio.ai/LMStudioAI&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://discord.gg/aPQfnNkxGC&#34;&gt;Discord&lt;/a&gt;. Read the &lt;a href=&#34;https://lmstudio.ai/docs&#34;&gt;Docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Discuss all things lmstudio.js in &lt;a href=&#34;https://discord.gg/aPQfnNkxGC&#34;&gt;#dev-chat&lt;/a&gt; in LM Studio&#39;s Community Discord server.&lt;/p&gt; &#xA;&lt;a href=&#34;https://discord.gg/aPQfnNkxGC&#34;&gt;&lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1110598183144399058?logo=discord&amp;amp;style=flat&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install @lmstudio/sdk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick project setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npx lmstudio install-cli # open a new terminal window after installation...&#xA;lms create&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { LMStudioClient } from &#34;@lmstudio/sdk&#34;;&#xA;&#xA;const client = new LMStudioClient();&#xA;&#xA;async function main() {&#xA;  const modelPath = &#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;;&#xA;  const llama3 = await client.llm.load(modelPath, { config: { gpuOffload: &#34;max&#34; } });&#xA;  const prediction = llama3.respond([&#xA;    { role: &#34;system&#34;, content: &#34;Always answer in rhymes.&#34; },&#xA;    { role: &#34;user&#34;, content: &#34;Please introduce yourself.&#34; },&#xA;  ]);&#xA;&#xA;  for await (const { content } of prediction) {&#xA;    process.stdout.write(content);&#xA;  }&#xA;&#xA;  const { stats } = await prediction;&#xA;  console.log(stats);&#xA;}&#xA;&#xA;main();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Set up &lt;code&gt;lms&lt;/code&gt; (CLI)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;lms&lt;/code&gt; is the CLI tool for LM Studio. It is shipped with the latest versions of &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LM Studio&lt;/a&gt;. To set it up, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npx lmstudio install-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To check if the bootstrapping was successful, run the following in a &lt;strong&gt;👉 new terminal window 👈&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;lms&lt;/code&gt; is only shipped with the latest version of &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LM Studio&lt;/a&gt; (v0.2.22 and onwards). Please make sure you have the latest version installed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Start the local LLM server&lt;/h2&gt; &#xA;&lt;h3&gt;Node.js script&lt;/h3&gt; &#xA;&lt;p&gt;Start the server by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lms server start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web app&lt;/h3&gt; &#xA;&lt;p&gt;If you are developing a web application and/or need to enable CORS (Cross Origin Resource Sharing), run this instead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lms server start --cors=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Override the default port&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lms server start --port 12345&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Loading an LLM and Predicting with It&lt;/h3&gt; &#xA;&lt;p&gt;This example loads a model &lt;code&gt;&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;&lt;/code&gt; and predicts text with it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { LMStudioClient } from &#34;@lmstudio/sdk&#34;;&#xA;&#xA;const client = new LMStudioClient();&#xA;&#xA;// Load a model&#xA;const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;);&#xA;&#xA;// Create a text completion prediction&#xA;const prediction = llama3.complete(&#34;The meaning of life is&#34;);&#xA;&#xA;// Stream the response&#xA;for await (const { content } of prediction) {&#xA;  process.stdout.write(content);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;About &lt;code&gt;process.stdout.write&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;process.stdout.write&lt;/code&gt; is a &lt;a href=&#34;https://nodejs.org/api/process.html#processstdout&#34;&gt;Node.js-specific function&lt;/a&gt; that allows you to print text without a newline.&lt;/p&gt; &#xA; &lt;p&gt;On the browser, you might want to do something like:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// Get the element where you want to display the output&#xA;const outputElement = document.getElementById(&#34;output&#34;);&#xA;&#xA;for await (const { content } of prediction) {&#xA;  outputElement.textContent += content;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using a Non-Default LM Studio Server Port&lt;/h3&gt; &#xA;&lt;p&gt;This example shows how to connect to LM Studio running on a different port (e.g., 8080).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { LMStudioClient } from &#34;@lmstudio/sdk&#34;;&#xA;&#xA;const client = new LMStudioClient({&#xA;  baseUrl: &#34;ws://127.0.0.1:8080&#34;,&#xA;});&#xA;&#xA;// client.llm.load(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading a Model and Keeping It Loaded After Client Exit (daemon mode)&lt;/h3&gt; &#xA;&lt;p&gt;By default, when your client disconnects from LM Studio, all models loaded by that client are unloaded. You can prevent this by setting the &lt;code&gt;noHup&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  noHup: true,&#xA;});&#xA;&#xA;// The model stays loaded even after the client disconnects&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Giving a Loaded Model a Friendly Name&lt;/h3&gt; &#xA;&lt;p&gt;You can set an identifier for a model when loading it. This identifier can be used to refer to the model later.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  identifier: &#34;my-model&#34;,&#xA;});&#xA;&#xA;// You can refer to the model later using the identifier&#xA;const myModel = await client.llm.get(&#34;my-model&#34;);&#xA;// myModel.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading a Model with a Custom Configuration&lt;/h3&gt; &#xA;&lt;p&gt;By default, the load configuration for a model comes from the preset associated with the model (Can be changed on the &#34;My Models&#34; page in LM Studio).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  config: {&#xA;    contextLength: 1024,&#xA;    gpuOffload: 0.5, // Offloads 50% of the computation to the GPU&#xA;  },&#xA;});&#xA;&#xA;// llama3.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading a Model with a Specific Preset&lt;/h3&gt; &#xA;&lt;p&gt;The preset determines the default load configuration and the default inference configuration for a model. By default, the preset associated with the model is used. (Can be changed on the &#34;My Models&#34; page in LM Studio). You can change the preset used by specifying the &lt;code&gt;preset&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  preset: &#34;My ChatML&#34;,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Custom Loading Progress&lt;/h3&gt; &#xA;&lt;p&gt;You can track the loading progress of a model by providing an &lt;code&gt;onProgress&lt;/code&gt; callback.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  verbose: false, // Disables the default progress logging&#xA;  onProgress: progress =&amp;gt; {&#xA;    console.log(`Progress: ${(progress * 100).toFixed(1)}%`);&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing all Models that can be Loaded&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to find all models that are available to be loaded, you can use the &lt;code&gt;listDownloadedModel&lt;/code&gt; method on the &lt;code&gt;system&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const downloadedModels = await client.system.listDownloadedModels();&#xA;const downloadedLLMs = downloadedModels.filter(model =&amp;gt; model.type === &#34;llm&#34;);&#xA;&#xA;// Load the first model&#xA;const model = await client.llm.load(downloadedLLMs[0].path);&#xA;// model.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Canceling a Load&lt;/h3&gt; &#xA;&lt;p&gt;You can cancel a load by using an AbortController.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const controller = new AbortController();&#xA;&#xA;try {&#xA;  const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;    signal: controller.signal,&#xA;  });&#xA;  // llama3.complete(...);&#xA;} catch (error) {&#xA;  console.error(error);&#xA;}&#xA;&#xA;// Somewhere else in your code:&#xA;controller.abort();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;About &lt;code&gt;AbortController&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;AbortController is a standard JavaScript API that allows you to cancel asynchronous operations. It is supported in modern browsers and Node.js. For more information, see the &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/AbortController&#34;&gt;MDN Web Docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Unloading a Model&lt;/h3&gt; &#xA;&lt;p&gt;You can unload a model by calling the &lt;code&gt;unload&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const llama3 = await client.llm.load(&#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;, {&#xA;  identifier: &#34;my-model&#34;,&#xA;});&#xA;&#xA;// ...Do stuff...&#xA;&#xA;await client.llm.unload(&#34;my-model&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, by default, all models loaded by a client are unloaded when the client disconnects. Therefore, unless you want to precisely control the lifetime of a model, you do not need to unload them manually.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Keeping a Model Loaded After Disconnection&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;If you wish to keep a model loaded after disconnection, you can set the &lt;code&gt;noHup&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt; when loading the model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using an Already Loaded Model&lt;/h3&gt; &#xA;&lt;p&gt;To look up an already loaded model by its identifier, use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const myModel = await client.llm.get({ identifier: &#34;my-model&#34; });&#xA;// Or just&#xA;const myModel = await client.llm.get(&#34;my-model&#34;);&#xA;&#xA;// myModel.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To look up an already loaded model by its path, use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// Matches any quantization&#xA;const llama3 = await client.llm.get({ path: &#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34; });&#xA;&#xA;// Or if a specific quantization is desired:&#xA;const llama3 = await client.llm.get({&#xA;  path: &#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf&#34;,&#xA;});&#xA;&#xA;// llama3.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using any Loaded Model&lt;/h3&gt; &#xA;&lt;p&gt;If you do not have a specific model in mind, and just want to use any loaded model, you can simply pass in an empty object to &lt;code&gt;client.llm.get&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const anyModel = await client.llm.get({});&#xA;// anyModel.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing All Loaded Models&lt;/h3&gt; &#xA;&lt;p&gt;To list all loaded models, use the &lt;code&gt;client.llm.listLoaded&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const loadedModels = await client.llm.listLoaded();&#xA;&#xA;if (loadedModels.length === 0) {&#xA;  throw new Error(&#34;No models loaded&#34;);&#xA;}&#xA;&#xA;// Use the first one&#xA;const firstModel = await client.llm.get({ identifier: loadedModels[0].identifier });&#xA;// firstModel.complete(...);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example loadedModels Response:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;[&#xA;  {&#xA;    &#34;identifier&#34;: &#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;,&#xA;    &#34;path&#34;: &#34;lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF&#34;,&#xA;  },&#xA;  {&#xA;    &#34;identifier&#34;: &#34;microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf&#34;,&#xA;    &#34;path&#34;: &#34;microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf&#34;,&#xA;  },&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Text Completion&lt;/h3&gt; &#xA;&lt;p&gt;To perform text completion, use the &lt;code&gt;complete&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = model.complete(&#34;The meaning of life is&#34;);&#xA;&#xA;for await (const { content } of prediction) {&#xA;  process.stdout.write(content);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the inference parameters in the preset is used for the prediction. You can override them like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = anyModel.complete(&#34;Meaning of life is&#34;, {&#xA;  contextOverflowPolicy: &#34;stopAtLimit&#34;,&#xA;  maxPredictedTokens: 100,&#xA;  prePrompt: &#34;Some pre-prompt&#34;,&#xA;  stopStrings: [&#34;\n&#34;],&#xA;  temperature: 0.7,&#xA;});&#xA;&#xA;// ...Do stuff with the prediction...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Conversation&lt;/h3&gt; &#xA;&lt;p&gt;To perform a conversation, use the &lt;code&gt;respond&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = anyModel.respond([&#xA;  { role: &#34;system&#34;, content: &#34;Answer the following questions.&#34; },&#xA;  { role: &#34;user&#34;, content: &#34;What is the meaning of life?&#34; },&#xA;]);&#xA;&#xA;for await (const { content } of prediction) {&#xA;  process.stdout.write(content);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, you can override the inference parameters for the conversation (Note the available options are different from text completion):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = anyModel.respond(&#xA;  [&#xA;    { role: &#34;system&#34;, content: &#34;Answer the following questions.&#34; },&#xA;    { role: &#34;user&#34;, content: &#34;What is the meaning of life?&#34; },&#xA;  ],&#xA;  {&#xA;    contextOverflowPolicy: &#34;stopAtLimit&#34;,&#xA;    maxPredictedTokens: 100,&#xA;    stopStrings: [&#34;\n&#34;],&#xA;    temperature: 0.7,&#xA;    inputPrefix: &#34;Q: &#34;,&#xA;    inputSuffix: &#34;\nA:&#34;,&#xA;  },&#xA;);&#xA;&#xA;// ...Do stuff with the prediction...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Always Provide the Full History/Context&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;LLMs are &lt;em&gt;stateless&lt;/em&gt;. They do not remember or retain information from previous inputs. Therefore, when predicting with an LLM, you should always provide the full history/context.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Getting Prediction Stats&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to get the prediction statistics, you can await on the prediction object to get a &lt;code&gt;PredictionResult&lt;/code&gt;, through which you can access the stats via the &lt;code&gt;stats&lt;/code&gt; property.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = model.complete(&#34;The meaning of life is&#34;);&#xA;&#xA;for await (const { content } of prediction) {&#xA;  process.stdout.write(content);&#xA;}&#xA;&#xA;const { stats } = await prediction;&#xA;console.log(stats);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;No Extra Waiting&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;When you have already consumed the prediction stream, awaiting on the prediction object will not cause any extra waiting, as the result is cached within the prediction object.&lt;/p&gt; &#xA; &lt;p&gt;On the other hand, if you only care about the final result, you don&#39;t need to iterate through the stream. Instead, you can await on the prediction object directly to get the final result.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = model.complete(&#34;The meaning of life is&#34;);&#xA;const result = await prediction;&#xA;const content = result.content;&#xA;const stats = result.stats;&#xA;&#xA;// Or just:&#xA;&#xA;const { content, stats } = await model.complete(&#34;The meaning of life is&#34;);&#xA;&#xA;console.log(stats);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example output for stats:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{&#xA;  &#34;stopReason&#34;: &#34;eosFound&#34;,&#xA;  &#34;tokensPerSecond&#34;: 26.644333102146646,&#xA;  &#34;numGpuLayers&#34;: 33,&#xA;  &#34;timeToFirstTokenSec&#34;: 0.146,&#xA;  &#34;promptTokensCount&#34;: 5,&#xA;  &#34;predictedTokensCount&#34;: 694,&#xA;  &#34;totalTokensCount&#34;: 699&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Producing JSON (Structured Output)&lt;/h3&gt; &#xA;&lt;p&gt;LM Studio supports structured prediction, which will force the model to produce content that conforms to a specific structure. To enable structured prediction, you should set the &lt;code&gt;structured&lt;/code&gt; field. It is available for both &lt;code&gt;complete&lt;/code&gt; and &lt;code&gt;respond&lt;/code&gt; methods.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of how to use structured prediction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = model.complete(&#34;Here is a joke in JSON:&#34;, {&#xA;  maxPredictedTokens: 100,&#xA;  structured: { type: &#34;json&#34; },&#xA;});&#xA;&#xA;const result = await prediction;&#xA;try {&#xA;  // Although the LLM is guaranteed to only produce valid JSON, when it is interrupted, the&#xA;  // partial result might not be. Always check for errors. (See caveats below)&#xA;  const parsed = JSON.parse(result.content);&#xA;  console.info(parsed);&#xA;} catch (e) {&#xA;  console.error(e);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example output:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{&#xA; &#34;title&#34;: &#34;The Shawshank Redemption&#34;,&#xA; &#34;genre&#34;: [ &#34;drama&#34;, &#34;thriller&#34; ],&#xA; &#34;release_year&#34;: 1994,&#xA; &#34;cast&#34;: [&#xA;   { &#34;name&#34;: &#34;Tim Robbins&#34;, &#34;role&#34;: &#34;Andy Dufresne&#34; },&#xA;   { &#34;name&#34;: &#34;Morgan Freeman&#34;, &#34;role&#34;: &#34;Ellis Boyd&#34; }&#xA; ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Sometimes, any JSON is not enough. You might want to enforce a specific JSON schema. You can do this by providing a JSON schema to the &lt;code&gt;structured&lt;/code&gt; field. Read more about JSON schema at &lt;a href=&#34;https://json-schema.org/&#34;&gt;json-schema.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const bookSchema = {&#xA;  type: &#34;object&#34;,&#xA;  properties: {&#xA;    bookTitle: { type: &#34;string&#34; },&#xA;    author: { type: &#34;string&#34; },&#xA;    genre: { type: &#34;string&#34; },&#xA;    pageCount: { type: &#34;number&#34; },&#xA;  },&#xA;  required: [&#34;bookTitle&#34;, &#34;author&#34;, &#34;genre&#34;],&#xA;};&#xA;&#xA;const prediction = model.complete(&#34;Books that were turned into movies:&#34;, {&#xA;  maxPredictedTokens: 100,&#xA;  structured: { type: &#34;json&#34;, jsonSchema: bookSchema },&#xA;});&#xA;&#xA;const result = await prediction;&#xA;try {&#xA;  const parsed = JSON.parse(result.content);&#xA;&#xA;  console.info(parsed); // see example response below&#xA;  console.info(&#34;The bookTitle is&#34;, parsed.bookTitle); // The bookTitle is The Help&#xA;  console.info(&#34;The author is&#34;, parsed.author); // The author is Tina&#xA;  console.info(&#34;The genre is&#34;, parsed.genre); // The genre is Historical Fiction&#xA;  console.info(&#34;The pageCount is&#34;, parsed.pageCount); // The pageCount is 320&#xA;} catch (e) {&#xA;  console.error(e);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example response for parsed:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{&#xA;  &#34;author&#34;: &#34;J.K. Rowling&#34;,&#xA;  &#34;bookTitle&#34;: &#34;Harry Potter and the Philosopher&#39;s Stone&#34;,&#xA;  &#34;genre&#34;: &#34;Fantasy&#34;,&#xA;  &#34;pageCount&#34;: 320&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Caveats with Structured Prediction&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Although the model is forced to generate predictions that conform to the specified structure, the prediction may be interrupted (for example, if the user stops the prediction). When that happens, the partial result may not conform to the specified structure. Thus, always check the prediction result before using it, for example, by wrapping the &lt;code&gt;JSON.parse&lt;/code&gt; inside a try-catch block.&lt;/li&gt; &#xA;  &lt;li&gt;In certain cases, the model may get stuck. For example, when forcing it to generate valid JSON, it may generate a opening brace &lt;code&gt;{&lt;/code&gt; but never generate a closing brace &lt;code&gt;}&lt;/code&gt;. In such cases, the prediction will go on forever until the context length is reached, which can take a long time. Therefore, it is recommended to always set a &lt;code&gt;maxPredictedTokens&lt;/code&gt; limit. This also contributes to the point above.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Canceling/Aborting a Prediction&lt;/h3&gt; &#xA;&lt;p&gt;A prediction may be canceled by calling the &lt;code&gt;cancel&lt;/code&gt; method on the prediction object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const prediction = model.complete(&#34;The meaning of life is&#34;);&#xA;&#xA;// ...Do stuff...&#xA;&#xA;prediction.cancel();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When a prediction is canceled, the prediction will stop normally but with &lt;code&gt;stopReason&lt;/code&gt; set to &lt;code&gt;&#34;userStopped&#34;&lt;/code&gt;. You can detect cancellation like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;for await (const { content } of prediction) {&#xA;  process.stdout.write(content);&#xA;}&#xA;const { stats } = await prediction;&#xA;if (stats.stopReason === &#34;userStopped&#34;) {&#xA;  console.log(&#34;Prediction was canceled by the user&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>coinbase/agentkit</title>
    <updated>2025-02-03T01:38:36Z</updated>
    <id>tag:github.com,2025-02-03:/coinbase/agentkit</id>
    <link href="https://github.com/coinbase/agentkit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://docs.cdp.coinbase.com/agentkit/docs/welcome&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/agentkit_banner.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;h1 style=&#34;font-size: 3em; margin-bottom: 20px;&#34;&gt; AgentKit &lt;/h1&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/agent_k.webp&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA; &lt;p style=&#34;font-size: 1.2em; max-width: 600px; margin: 0 auto 20px;&#34;&gt; Every agent deserves a wallet. &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypistats.org/packages/cdp-agentkit-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/cdp-agentkit-core?style=flat-square&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@coinbase/agentkit&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/@coinbase/cdp-agentkit-core?style=flat-square&#34; alt=&#34;npm downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#coinbase/agentkit&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/coinbase/cdp-agentkit?style=flat-square&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/coinbase/agentkit/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/coinbase/cdp-agentkit?style=flat-square&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-overview&#34;&gt;📖 Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-quickstart&#34;&gt;🚀 Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#nodejs&#34;&gt;Node.js&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#python&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-repository-structure&#34;&gt;🗂 Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-contributing&#34;&gt;🤝 Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-documentation&#34;&gt;📜 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-security-and-bug-reports&#34;&gt;🚨 Security and bug reports&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-contact&#34;&gt;📧 Contact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-license&#34;&gt;📝 License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#-legal-and-privacy&#34;&gt;🔒 Legal and Privacy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📖 Overview&lt;/h2&gt; &#xA;&lt;p&gt;AgentKit is &lt;a href=&#34;https://docs.cdp.coinbase.com&#34;&gt;Coinbase Developer Platform&#39;s&lt;/a&gt; framework for easily enabling AI agents to take actions onchain. It is designed to be framework-agnostic, so you can use it with any AI framework, and wallet-agnostic, so you can use it with any wallet. AgentKit is actively being built out, and &lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/#contributing&#34;&gt;welcomes community contributions!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=-R_mKpdepRE&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/-R_mKpdepRE/maxresdefault.jpg&#34; alt=&#34;Video Title&#34; style=&#34;max-width: 600px;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🚀 Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Node.js&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34;&gt;Node.js 18+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cdp.coinbase.com/get-started/docs/cdp-api-keys#creating-secret-api-keys&#34;&gt;CDP Secret API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key&#34;&gt;OpenAI API Key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get your agent running:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/coinbase/agentkit.git&#xA;&#xA;# Navigate to the langchain-cdp-chatbot example&#xA;cd agentkit/typescript/examples/langchain-cdp-chatbot&#xA;&#xA;# At this point, fill in your CDP API key name, private key, and OpenAI API key in&#xA;# the .env.example file.&#xA;# Then, rename the .env.example file to .env&#xA;mv .env.example .env&#xA;&#xA;# Install dependencies&#xA;npm install&#xA;&#xA;# Run the chatbot&#xA;npm run start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Select &#34;1. chat mode&#34; and start telling your Agent to do things onchain!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Prompt: Fund my wallet with some testnet ETH.&#xA;-------------------&#xA;Wallet: ccaf1dbf-3a90-4e52-ad34-89a07aad9e8b on network: base-sepolia with default address: 0xD9b990c7b0079c1c3733D2918Ee50b68f29FCFD5&#xA;-------------------&#xA;&#xA;-------------------&#xA;Received eth from the faucet. Transaction: https://sepolia.basescan.org/tx/0x03e82934cd04be5b725927729b517c606f6f744611f0f36e834f21ad742ad7ca&#xA;-------------------&#xA;Your wallet has been successfully funded with testnet ETH. You can view the transaction [here](https://sepolia.basescan.org/tx/0x03e82934cd04be5b725927729b517c606f6f744611f0f36e834f21ad742ad7ca).&#xA;-------------------&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.10+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python-poetry.org/docs/&#34;&gt;Poetry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cdp.coinbase.com/get-started/docs/cdp-api-keys#creating-secret-api-keys&#34;&gt;CDP Secret API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key&#34;&gt;OpenAI API Key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get your agent running:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/coinbase/agentkit.git&#xA;&#xA;# Navigate to the chatbot-python example&#xA;cd agentkit/python/examples/cdp-langchain-chatbot &#xA;&#xA;# At this point, fill in your CDP API key name, private key, and OpenAI API key in the&#xA;# .env.example file.&#xA;# Then, rename the .env.example file to .env&#xA;mv .env.example .env&#xA;&#xA;# Install dependencies&#xA;poetry install&#xA;&#xA;# Run the chatbot&#xA;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Select &#34;1. chat mode&#34; and start telling your Agent to do things onchain!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Prompt: Fund my wallet with some testnet ETH.&#xA;-------------------&#xA;Wallet: ccaf1dbf-3a90-4e52-ad34-89a07aad9e8b on network: base-sepolia with default address: 0xD9b990c7b0079c1c3733D2918Ee50b68f29FCFD5&#xA;-------------------&#xA;&#xA;-------------------&#xA;Received eth from the faucet. Transaction: https://sepolia.basescan.org/tx/0x03e82934cd04be5b725927729b517c606f6f744611f0f36e834f21ad742ad7ca&#xA;-------------------&#xA;Your wallet has been successfully funded with testnet ETH. You can view the transaction [here](https://sepolia.basescan.org/tx/0x03e82934cd04be5b725927729b517c606f6f744611f0f36e834f21ad742ad7ca).&#xA;-------------------&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗂 Repository Structure&lt;/h2&gt; &#xA;&lt;p&gt;AgentKit is organized as a monorepo that contains multiple packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./&#xA;├── typescript/&#xA;│   ├── agentkit/&#xA;│   ├── framework-extensions/&#xA;│   |   └── langchain/&#xA;│   └── examples/&#xA;│       ├── langchain-cdp-chatbot/&#xA;│       ├── langchain-farcaster-chatbot/&#xA;│       └── langchain-twitter-chatbot/&#xA;├── python/&#xA;│   ├── cdp-agentkit-core/&#xA;│   ├── cdp-langchain/&#xA;│   ├── twitter-langchain/&#xA;│   └── examples/&#xA;│       ├── cdp-langchain-chatbot/&#xA;│       └── twitter-langchain-chatbot/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤝 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;AgentKit is actively looking for community contributions!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To see a list of actions and frameworks we&#39;d love to see open-source contributions for, see &lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/WISHLIST.md&#34;&gt;WISHLIST.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To understand the process for contributing to AgentKit, see &lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📜 Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cdp.coinbase.com/agentkit/docs/welcome&#34;&gt;AgentKit Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python API References &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://coinbase.github.io/agentkit/cdp-agentkit-core/python/index.html&#34;&gt;AgentKit Core&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://coinbase.github.io/agentkit/cdp-langchain/python/index.html&#34;&gt;AgentKit Langchain Extension&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Node.js API References &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://coinbase.github.io/agentkit/agentkit/typescript/index.html&#34;&gt;AgentKit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://coinbase.github.io/agentkit/agentkit-langchain/typescript/index.html&#34;&gt;AgentKit Langchain Extension&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚨 Security and Bug Reports&lt;/h2&gt; &#xA;&lt;p&gt;The AgentKit team takes security seriously. See &lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/SECURITY.md&#34;&gt;SECURITY.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;📧 Contact&lt;/h2&gt; &#xA;&lt;p&gt;For feature requests, feedback, or questions, please reach out to us via the &lt;a href=&#34;https://discord.com/channels/1220414409550336183/1304126107876069376&#34;&gt;Coinbase Developer Platform Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📝 License&lt;/h2&gt; &#xA;&lt;p&gt;AgentKit is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/coinbase/agentkit/master/LICENSE.md&#34;&gt;Apache-2.0&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h2&gt;🔒 Legal and Privacy&lt;/h2&gt; &#xA;&lt;p&gt;The AgentKit software is novel and experimental, and is therefore provided on an AS-IS basis. The software is intended to be used only for the purposes of assisting with designing blockchain transactions and enabling other API integrations using natural language inputs, and is not intended to provide (i) an offer, or solicitation of an offer, to invest in, or to buy or sell, any interests or shares, or to participate in any investment or trading strategy, (ii) accounting, legal, tax advice, investment recommendations or other professional advice or (iii) an official statement of Coinbase. Acts proposed or performed by an agent through AgentKit software are NOT acts of Coinbase. You should consult with a professional advisor before making any decisions based on the information provided by the software. No representation or warranty is made, expressed or implied, with respect to the accuracy, completeness, reliability, security, or suitability of the software or to any information provided in connection with the software. The risk of loss through use of the software can be substantial, and you assume any and all risks of loss and liability. The software may produce output that is inaccurate, incorrect, unpredictable or undesirable, and it is the user&#39;s exclusive responsibility to evaluate the output and the use-case and determine whether it is appropriate. The right to use the software is contingent on your agreement to the &lt;a href=&#34;https://www.coinbase.com/legal/developer-platform/terms-of-service&#34;&gt;CDP Terms of Service&lt;/a&gt; (except to the extent it conflicts with the Apache-2.0 license).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ChatGPTNextWeb/NextChat</title>
    <updated>2025-02-03T01:38:36Z</updated>
    <id>tag:github.com,2025-02-03:/ChatGPTNextWeb/NextChat</id>
    <link href="https://github.com/ChatGPTNextWeb/NextChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;✨ Local and Fast AI Assistant. Support: Web | iOS | MacOS | Android | Linux | Windows&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://nextchat.dev/chat&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/287c510f-f508-478e-ade3-54d30453dc18&#34; width=&#34;1000&#34; alt=&#34;icon&#34;&gt; &lt;/a&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;NextChat (ChatGPT Next Web)&lt;/h1&gt; &#xA; &lt;p&gt;English / &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/5973&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/5973&#34; alt=&#34;ChatGPTNextWeb%2FChatGPT-Next-Web | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;One-Click to get a well-designed cross-platform ChatGPT web UI, with Claude, GPT4 &amp;amp; Gemini Pro support.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://nextchat.dev/chat?utm_source=readme&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NextChat-Saas-green?logo=microsoftedge&#34; alt=&#34;Saas&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.nextchat.dev/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Web-PWA-orange?logo=microsoftedge&#34; alt=&#34;Web&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Windows-blue?logo=windows&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-MacOS-black?logo=apple&#34; alt=&#34;MacOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linux-333?logo=ubuntu&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://nextchat.dev/chat?utm_source=readme&#34;&gt;NextChatAI&lt;/a&gt; / &lt;a href=&#34;https://app.nextchat.dev&#34;&gt;Web App Demo&lt;/a&gt; / &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/releases&#34;&gt;Desktop App&lt;/a&gt; / &lt;a href=&#34;https://discord.gg/YCkeafCafC&#34;&gt;Discord&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/#enterprise-edition&#34;&gt;Enterprise Edition&lt;/a&gt; / &lt;a href=&#34;https://twitter.com/NextChatDev&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FChatGPTNextWeb%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;env=CODE&amp;amp;project-name=nextchat&amp;amp;repository-name=NextChat&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy on Vercel&#34; height=&#34;30&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zeabur.com/templates/ZBUEFA&#34;&gt;&lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34; height=&#34;30&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitpod.io/#https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34; height=&#34;30&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.bt.cn/new/download.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/BT_Deploy-Install-20a53a&#34; alt=&#34;BT Deply Install&#34; height=&#34;30&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://monica.im/?utm=nxcrp&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/903482d4-3e87-4134-9af1-f2588fa90659&#34; height=&#34;60&#34; width=&#34;288&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🥳 Cheer for DeepSeek, China&#39;s AI star!&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Purpose-Built UI for DeepSeek Reasoner Model&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img src=&#34;https://github.com/user-attachments/assets/f3952210-3af1-4dc0-9b81-40eaa4847d9a&#34;&gt; &#xA;&lt;h2&gt;🫣 NextChat Support MCP !&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Before build, please set env ENABLE_MCP=true&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img src=&#34;https://github.com/user-attachments/assets/d8851f40-4e36-4335-b1a4-ec1e11488c7e&#34;&gt; &#xA;&lt;h2&gt;Enterprise Edition&lt;/h2&gt; &#xA;&lt;p&gt;Meeting Your Company&#39;s Privatization and Customization Deployment Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brand Customization&lt;/strong&gt;: Tailored VI/UI to seamlessly align with your corporate brand image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource Integration&lt;/strong&gt;: Unified configuration and management of dozens of AI resources by company administrators, ready for use by team members.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Permission Control&lt;/strong&gt;: Clearly defined member permissions, resource permissions, and knowledge base permissions, all controlled via a corporate-grade Admin Panel.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Integration&lt;/strong&gt;: Combining your internal knowledge base with AI capabilities, making it more relevant to your company&#39;s specific business needs compared to general AI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Security Auditing&lt;/strong&gt;: Automatically intercept sensitive inquiries and trace all historical conversation records, ensuring AI adherence to corporate information security standards.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Private Deployment&lt;/strong&gt;: Enterprise-level private deployment supporting various mainstream private cloud solutions, ensuring data security and privacy protection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous Updates&lt;/strong&gt;: Ongoing updates and upgrades in cutting-edge capabilities like multimodal AI, ensuring consistent innovation and advancement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For enterprise inquiries, please contact: &lt;strong&gt;&lt;a href=&#34;mailto:business@nextchat.dev&#34;&gt;business@nextchat.dev&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/settings.png&#34; alt=&#34;Settings&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/more.png&#34; alt=&#34;More&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deploy for free with one-click&lt;/strong&gt; on Vercel in under 1 minute&lt;/li&gt; &#xA; &lt;li&gt;Compact client (~5MB) on Linux/Windows/MacOS, &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/releases&#34;&gt;download it now&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fully compatible with self-deployed LLMs, recommended for use with &lt;a href=&#34;https://github.com/josStorer/RWKV-Runner&#34;&gt;RWKV-Runner&lt;/a&gt; or &lt;a href=&#34;https://github.com/go-skynet/LocalAI&#34;&gt;LocalAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Privacy first, all data is stored locally in the browser&lt;/li&gt; &#xA; &lt;li&gt;Markdown support: LaTex, mermaid, code highlight, etc.&lt;/li&gt; &#xA; &lt;li&gt;Responsive design, dark mode and PWA&lt;/li&gt; &#xA; &lt;li&gt;Fast first screen loading speed (~100kb), support streaming response&lt;/li&gt; &#xA; &lt;li&gt;New in v2: create, share and debug your chat tools with prompt templates (mask)&lt;/li&gt; &#xA; &lt;li&gt;Awesome prompts powered by &lt;a href=&#34;https://github.com/PlexPt/awesome-chatgpt-prompts-zh&#34;&gt;awesome-chatgpt-prompts-zh&lt;/a&gt; and &lt;a href=&#34;https://github.com/f/awesome-chatgpt-prompts&#34;&gt;awesome-chatgpt-prompts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatically compresses chat history to support long conversations while also saving your tokens&lt;/li&gt; &#xA; &lt;li&gt;I18n: English, 简体中文, 繁体中文, 日本語, Français, Español, Italiano, Türkçe, Deutsch, Tiếng Việt, Русский, Čeština, 한국어, Indonesia&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/cover.png&#34; alt=&#34;主界面&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; System Prompt: pin a user defined prompt as system prompt &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/issues/138&#34;&gt;#138&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; User Prompt: user can edit and save custom prompts to prompt list&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Prompt Template: create a new chat with pre-defined in-context prompts &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/issues/993&#34;&gt;#993&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Share as image, share to ShareGPT &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/pull/1741&#34;&gt;#1741&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Desktop App with tauri&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Self-host Model: Fully compatible with &lt;a href=&#34;https://github.com/josStorer/RWKV-Runner&#34;&gt;RWKV-Runner&lt;/a&gt;, as well as server deployment of &lt;a href=&#34;https://github.com/go-skynet/LocalAI&#34;&gt;LocalAI&lt;/a&gt;: llama/gpt4all/rwkv/vicuna/koala/gpt4all-j/cerebras/falcon/dolly etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Artifacts: Easily preview, copy and share generated content/webpages through a separate window &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/pull/5092&#34;&gt;#5092&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Plugins: support network search, calculator, any other apis etc. &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/issues/165&#34;&gt;#165&lt;/a&gt; &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5353&#34;&gt;#5353&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; network search, calculator, any other apis etc. &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/issues/165&#34;&gt;#165&lt;/a&gt; &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5353&#34;&gt;#5353&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supports Realtime Chat &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5672&#34;&gt;#5672&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; local knowledge base&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚀 v2.15.8 Now supports Realtime Chat &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5672&#34;&gt;#5672&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.15.4 The Application supports using Tauri fetch LLM API, MORE SECURITY! &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5379&#34;&gt;#5379&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.15.0 Now supports Plugins! Read this: &lt;a href=&#34;https://github.com/ChatGPTNextWeb/NextChat-Awesome-Plugins&#34;&gt;NextChat-Awesome-Plugins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.14.0 Now supports Artifacts &amp;amp; SD&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.10.1 support Google Gemini Pro model.&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.9.11 you can use azure endpoint now.&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.8 now we have a client that runs across all platforms!&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.7 let&#39;s share conversations as image, or share to ShareGPT!&lt;/li&gt; &#xA; &lt;li&gt;🚀 v2.0 is released, now you can create prompt templates, turn your ideas into reality! Read this: &lt;a href=&#34;https://www.allabtai.com/prompt-engineering-tips-zero-one-and-few-shot-prompting/&#34;&gt;ChatGPT Prompt Engineering Tips: Zero, One and Few Shot Prompting&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API Key&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FYidadaa%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;env=CODE&amp;amp;project-name=chatgpt-next-web&amp;amp;repository-name=ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;, remember that &lt;code&gt;CODE&lt;/code&gt; is your page password;&lt;/li&gt; &#xA; &lt;li&gt;Enjoy :)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/faq-en.md&#34;&gt;English &amp;gt; FAQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Keep Updated&lt;/h2&gt; &#xA;&lt;p&gt;If you have deployed your own project with just one click following the steps above, you may encounter the issue of &#34;Updates Available&#34; constantly showing up. This is because Vercel will create a new project for you by default instead of forking this project, resulting in the inability to detect updates correctly.&lt;/p&gt; &#xA;&lt;p&gt;We recommend that you follow the steps below to re-deploy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Delete the original repository;&lt;/li&gt; &#xA; &lt;li&gt;Use the fork button in the upper right corner of the page to fork this project;&lt;/li&gt; &#xA; &lt;li&gt;Choose and deploy in Vercel again, &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/vercel-cn.md&#34;&gt;please see the detailed tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enable Automatic Updates&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you encounter a failure of Upstream Sync execution, please &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/README.md#manually-updating-code&#34;&gt;manually update code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After forking the project, due to the limitations imposed by GitHub, you need to manually enable Workflows and Upstream Sync Action on the Actions page of the forked project. Once enabled, automatic updates will be scheduled every hour:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/enable-actions.jpg&#34; alt=&#34;Automatic Updates&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/enable-actions-sync.jpg&#34; alt=&#34;Enable Automatic Updates&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Manually Updating Code&lt;/h3&gt; &#xA;&lt;p&gt;If you want to update instantly, you can check out the &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork&#34;&gt;GitHub documentation&lt;/a&gt; to learn how to synchronize a forked project with upstream code.&lt;/p&gt; &#xA;&lt;p&gt;You can star or watch this project or follow author to get release notifications in time.&lt;/p&gt; &#xA;&lt;h2&gt;Access Password&lt;/h2&gt; &#xA;&lt;p&gt;This project provides limited access control. Please add an environment variable named &lt;code&gt;CODE&lt;/code&gt; on the vercel environment variables page. The value should be passwords separated by comma like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code1,code2,code3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding or modifying this environment variable, please redeploy the project for the changes to take effect.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Variables&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;CODE&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Access password, separated by comma.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (required)&lt;/h3&gt; &#xA;&lt;p&gt;Your openai api key, join multiple api keys with comma.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BASE_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: &lt;code&gt;https://api.openai.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Examples: &lt;code&gt;http://your-openai-proxy.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Override openai api request base url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;OPENAI_ORG_ID&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Specify OpenAI organization ID.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;AZURE_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example: https://{azure-resource-url}/openai&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Azure deploy url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Azure Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Azure Api Version, find it at &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions&#34;&gt;Azure Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Google Gemini Pro Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;GOOGLE_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Google Gemini Pro Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;anthropic claude Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ANTHROPIC_API_VERSION&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;anthropic claude Api version.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ANTHROPIC_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;anthropic claude Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BAIDU_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Baidu Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BAIDU_SECRET_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Baidu Secret Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BAIDU_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Baidu Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BYTEDANCE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;ByteDance Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;BYTEDANCE_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;ByteDance Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ALIBABA_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Alibaba Cloud Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ALIBABA_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Alibaba Cloud Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;IFLYTEK_URL&lt;/code&gt; (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;iflytek Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;IFLYTEK_API_KEY&lt;/code&gt; (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;iflytek Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;IFLYTEK_API_SECRET&lt;/code&gt; (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;iflytek Api Secret.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;CHATGLM_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;ChatGLM Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;CHATGLM_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;ChatGLM Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;DeepSeek Api Key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;DEEPSEEK_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;DeepSeek Api Url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;HIDE_USER_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you do not want users to input their own API key, set this value to 1.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;DISABLE_GPT4&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you do not want users to use GPT-4, set this value to 1.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ENABLE_BALANCE_QUERY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you do want users to query balance, set this value to 1.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;DISABLE_FAST_LINK&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want to disable parse settings from url, set this to 1.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;CUSTOM_MODELS&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty Example: &lt;code&gt;+llama,+claude-2,-gpt-3.5-turbo,gpt-4-1106-preview=gpt-4-turbo&lt;/code&gt; means add &lt;code&gt;llama, claude-2&lt;/code&gt; to model list, and remove &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; from list, and display &lt;code&gt;gpt-4-1106-preview&lt;/code&gt; as &lt;code&gt;gpt-4-turbo&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To control custom models, use &lt;code&gt;+&lt;/code&gt; to add a custom model, use &lt;code&gt;-&lt;/code&gt; to hide a model, use &lt;code&gt;name=displayName&lt;/code&gt; to customize model name, separated by comma.&lt;/p&gt; &#xA;&lt;p&gt;User &lt;code&gt;-all&lt;/code&gt; to disable all default models, &lt;code&gt;+all&lt;/code&gt; to enable all default models.&lt;/p&gt; &#xA;&lt;p&gt;For Azure: use &lt;code&gt;modelName@Azure=deploymentName&lt;/code&gt; to customize model name and deployment name.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example: &lt;code&gt;+gpt-3.5-turbo@Azure=gpt35&lt;/code&gt; will show option &lt;code&gt;gpt35(Azure)&lt;/code&gt; in model list. If you only can use Azure model, &lt;code&gt;-all,+gpt-3.5-turbo@Azure=gpt35&lt;/code&gt; will &lt;code&gt;gpt35(Azure)&lt;/code&gt; the only option in model list.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For ByteDance: use &lt;code&gt;modelName@bytedance=deploymentName&lt;/code&gt; to customize model name and deployment name.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Example: &lt;code&gt;+Doubao-lite-4k@bytedance=ep-xxxxx-xxx&lt;/code&gt; will show option &lt;code&gt;Doubao-lite-4k(ByteDance)&lt;/code&gt; in model list.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;code&gt;DEFAULT_MODEL&lt;/code&gt; （optional）&lt;/h3&gt; &#xA;&lt;p&gt;Change default model&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;VISION_MODELS&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Default: Empty Example: &lt;code&gt;gpt-4-vision,claude-3-opus,my-custom-model&lt;/code&gt; means add vision capabilities to these models in addition to the default pattern matches (which detect models containing keywords like &#34;vision&#34;, &#34;claude-3&#34;, &#34;gemini-1.5&#34;, etc).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Add additional models to have vision capabilities, beyond the default pattern matching. Multiple models should be separated by commas.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;WHITE_WEBDAV_ENDPOINTS&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;You can use this option if you want to increase the number of webdav service addresses you are allowed to access, as required by the format：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Each address must be a complete endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;https://xxxx/yyy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multiple addresses are connected by &#39;, &#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;DEFAULT_INPUT_TEMPLATE&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Customize the default template used to initialize the User Input Preprocessing configuration item in Settings.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;STABILITY_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Stability API key.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;STABILITY_URL&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Customize Stability API url.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ENABLE_MCP&lt;/code&gt; (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Enable MCP（Model Context Protocol）Feature&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;NodeJS &amp;gt;= 18, Docker &amp;gt;= 20&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before starting development, you must create a new &lt;code&gt;.env.local&lt;/code&gt; file at project root, and place your api key into it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;lt;your api key here&amp;gt;&#xA;&#xA;# if you are not able to access openai service, use this BASE_URL&#xA;BASE_URL=https://chatgpt1.nextweb.fun/api/proxy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local Development&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 1. install nodejs and yarn first&#xA;# 2. config local env vars in `.env.local`&#xA;# 3. run&#xA;yarn install&#xA;yarn dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;h3&gt;Docker (Recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull yidadaa/chatgpt-next-web&#xA;&#xA;docker run -d -p 3000:3000 \&#xA;   -e OPENAI_API_KEY=sk-xxxx \&#xA;   -e CODE=your-password \&#xA;   yidadaa/chatgpt-next-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can start service behind a proxy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d -p 3000:3000 \&#xA;   -e OPENAI_API_KEY=sk-xxxx \&#xA;   -e CODE=your-password \&#xA;   -e PROXY_URL=http://localhost:7890 \&#xA;   yidadaa/chatgpt-next-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your proxy needs password, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;-e PROXY_URL=&#34;http://127.0.0.1:7890 user pass&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If enable MCP, use：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 \&#xA;   -e OPENAI_API_KEY=sk-xxxx \&#xA;   -e CODE=your-password \&#xA;   -e ENABLE_MCP=true \&#xA;   yidadaa/chatgpt-next-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Shell&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash &amp;lt;(curl -s https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/scripts/setup.sh)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Synchronizing Chat Records (UpStash)&lt;/h2&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-cn.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-en.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-es.md&#34;&gt;Italiano&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-ja.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-ko.md&#34;&gt;한국어&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Please go to the [docs][./docs] directory for more documentation instructions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/cloudflare-pages-en.md&#34;&gt;Deploy with cloudflare (Deprecated)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/faq-en.md&#34;&gt;Frequent Ask Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/translation.md&#34;&gt;How to add a new translation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/vercel-cn.md&#34;&gt;How to use Vercel (No English)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/user-manual-cn.md&#34;&gt;User Manual (Only Chinese, WIP)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Translation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to add a new translation, read this &lt;a href=&#34;https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/translation.md&#34;&gt;document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Donation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/yidadaa&#34;&gt;Buy Me a Coffee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Special Thanks&lt;/h2&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=ChatGPTNextWeb/ChatGPT-Next-Web&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/license/mit/&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>