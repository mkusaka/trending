<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TypeScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-12T01:38:50Z</updated>
  <subtitle>Daily Trending of TypeScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cvlab-epfl/gaussian-splatting-web</title>
    <updated>2023-09-12T01:38:50Z</updated>
    <id>tag:github.com,2023-09-12:/cvlab-epfl/gaussian-splatting-web</id>
    <link href="https://github.com/cvlab-epfl/gaussian-splatting-web" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WebGPU viewer for Gaussian Splatting nerfs&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvlab-epfl/gaussian-splatting-web/main/teaser-image.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the source for an interactive web viewer of NeRFs crated with the code available from &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;INRIA&lt;/a&gt;. The app with instructions is hosted at &lt;a href=&#34;https://jatentaki.github.io/portfolio/gaussian-splatting/&#34;&gt;jatentaki.github.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;This project has been created using &lt;strong&gt;webpack-cli&lt;/strong&gt;. Before the first build, go to the code directory and execute &lt;code&gt;npm install&lt;/code&gt; to install dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Afterwards, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to bundle the application or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to have a live-updating server.&lt;/p&gt; &#xA;&lt;h2&gt;Browser compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The official compatiblity table of WebGPU can be found &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API#browser_compatibility&#34;&gt;here&lt;/a&gt;. In practice, the following are known to work:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MacOS&lt;/strong&gt;: works with recent (version 115+) Chrome/Chromium browsers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;: works with Edge 116+, most likely with Chrome/Chromium as well (it&#39;s the same thing but I was not able to test).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: works with Chrome dev version and custom flags. The steps are as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://www.google.com/chrome/dev/&#34;&gt;Chrome dev&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Launch from command line with extra flags: &lt;code&gt;google-chrome-unstable --enable-features=Vulkan,UseSkiaRenderer&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;chrome://flags/#enable-unsafe-webgpu&lt;/code&gt; and enable webgpu. Restart the browser for the change to take effect, make sure to use the flags from the previous step as well.&lt;/li&gt; &#xA; &lt;li&gt;The Gaussian viewer should work.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Firefox&lt;/strong&gt;: the nightly channel is supposed to support webGPU experimentally but in practice it fails on parsing my shaders across MacOS/Ubuntu.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you succeed with any other configuration or fail with the ones described above, please &lt;a href=&#34;https://github.com/cvlab-epfl/gaussian-splatting-web/issues&#34;&gt;open an issue&lt;/a&gt; and tell us.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Unlike the original paper, this code doesn&#39;t use computer shaders to compute each pixel value independently but instead maps the problem to a standard rasterization technique, where each Gaussian is a flat rectangle facing the camera, with the actual content drawn via a fragment shader. I found this approach to yield substantially better framerates than compute shaders, although both are available in WebGPU.&lt;/p&gt; &#xA;&lt;p&gt;This was my first substantial webdev project, therefore the code is far from idiomatic. I&#39;m happy to receive PRs both to improve performance and to clean up the codebase.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Koenkk/zigbee-herdsman-converters</title>
    <updated>2023-09-12T01:38:50Z</updated>
    <id>tag:github.com,2023-09-12:/Koenkk/zigbee-herdsman-converters</id>
    <link href="https://github.com/Koenkk/zigbee-herdsman-converters" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of device converters to be used with zigbee-herdsman&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://nodei.co/npm/zigbee-herdsman-converters/&#34;&gt;&lt;img src=&#34;https://nodei.co/npm/zigbee-herdsman-converters.png&#34; alt=&#34;NPM&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;zigbee-herdsman-converters&lt;/h1&gt; &#xA;&lt;p&gt;Collection of device converters to be used with zigbee-herdsman.&lt;/p&gt; &#xA;&lt;h2&gt;Breaking changes&lt;/h2&gt; &#xA;&lt;p&gt;15.0.0&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OTA &lt;code&gt;isUpdateAvailable&lt;/code&gt; now returns an object instead of a boolean (e.g. &lt;code&gt;{available: true, currentFileVersion: 120, otaFileVersion: 125}&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;OTA &lt;code&gt;updateToLatest&lt;/code&gt; now returns a number (&lt;code&gt;fileVersion&lt;/code&gt; of the new OTA) instead of a void&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.zigbee2mqtt.io/advanced/support-new-devices/01_support_new_devices.html&#34;&gt;Zigbee2MQTT how to support new devices&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a pull request&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you should run the following commands to ensure your changes will pass the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install&#xA;npm run lint&#xA;npm test&#xA;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If any of those commands finish with an error your PR won&#39;t pass the tests and will likely be rejected.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation of definition meta property&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;multiEndpoint&lt;/code&gt;: enables the multi endpoint functionallity in e.g. fromZigbee.on_off, example: normally this converter would return {&#34;state&#34;: &#34;OFF&#34;}, when multiEndpoint is enabled the &#39;endpoint&#39; method of the device definition will be called to determine the endpoint name which is then used as key e.g. {&#34;state_left&#34;: &#34;OFF&#34;}. Only needed when device sends the same attribute from multiple endpoints. (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multiEndpointSkip&lt;/code&gt;: array of attributes to not suffix with the endpoint name&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multiEndpointEnforce&lt;/code&gt;: enforce a certain endpoint for an attribute, e.g. {&#34;power&#34;: 4} see utils.enforceEndpoint()&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;disableDefaultResponse&lt;/code&gt;: used by toZigbee converters to disable the default response of some devices as they don&#39;t provide one. (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;applyRedFix&lt;/code&gt;: see toZigbee.light_color (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;supportsEnhancedHue&lt;/code&gt;: see toZigbee.light_color (default: true)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;supportsHueAndSaturation&lt;/code&gt;: see toZigbee.light_color (default: true), usually set by light_* extends via options.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timeout&lt;/code&gt;: timeout for commands to this device used in toZigbee. (default: 10000)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;coverInverted&lt;/code&gt;: Set to true for cover controls that report position=100 as open (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;coverStateFromTilt&lt;/code&gt;: Set cover state based on tilt&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;turnsOffAtBrightness1&lt;/code&gt;: Indicates light turns off when brightness 1 is set (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pinCodeCount&lt;/code&gt;: Amount of pincodes the lock can handle&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;disableActionGroup&lt;/code&gt;: Prevents some converters adding the action_group to the payload (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tuyaThermostatSystemMode&lt;/code&gt;/&lt;code&gt;tuyaThermostatPreset&lt;/code&gt;: TuYa specific thermostat options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;thermostat&lt;/code&gt;: see e.g. HT-08 definition &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;{dontMapPIHeatingDemand: true}&lt;/code&gt;: do not map &lt;code&gt;pIHeatingDemand&lt;/code&gt;/&lt;code&gt;pICoolingDemand&lt;/code&gt; from 0-255 -&amp;gt; 0-100, see fromZigbee.thermostat (default: false)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;battery&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;{dontDividePercentage: true}&lt;/code&gt;: prevents batteryPercentageRemainig from being divided (ZCL 200=100%, but some report 100=100%) (default: false)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;{voltageToPercentage: &#39;3V_2100&#39;}&lt;/code&gt;: convert voltage to percentage using specified option. See utils.batteryVoltageToPercentage() (default: null, no voltage to percentage conversion)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;fanStateOn&lt;/code&gt;: value used for fan_mode when using fan_state=&#34;ON&#34;, the default is &#34;on&#34;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>promptfoo/promptfoo</title>
    <updated>2023-09-12T01:38:50Z</updated>
    <id>tag:github.com,2023-09-12:/promptfoo/promptfoo</id>
    <link href="https://github.com/promptfoo/promptfoo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Test your prompts. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;promptfoo: test your prompts&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://npmjs.com/package/promptfoo&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/promptfoo&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/typpo/promptfoo/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/typpo/promptfoo/main.yml&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/typpo/promptfoo&#34; alt=&#34;MIT license&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;promptfoo&lt;/code&gt; is a tool for testing and evaluating LLM output quality.&lt;/p&gt; &#xA;&lt;p&gt;With promptfoo, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Systematically test prompts &amp;amp; models&lt;/strong&gt; against predefined test cases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluate quality and catch regressions&lt;/strong&gt; by comparing LLM outputs side-by-side&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speed up evaluations&lt;/strong&gt; with caching and concurrency&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Score outputs automatically&lt;/strong&gt; by defining test cases&lt;/li&gt; &#xA; &lt;li&gt;Use as a CLI, library, or in CI/CD&lt;/li&gt; &#xA; &lt;li&gt;Use OpenAI, Anthropic, Azure, Google, open-source models like Llama, or integrate custom API providers for any LLM API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The goal: &lt;strong&gt;test-driven prompt engineering&lt;/strong&gt;, rather than trial-and-error.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://promptfoo.dev/docs/intro&#34;&gt;» View full documentation «&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;promptfoo produces matrix views that let you quickly evaluate outputs across many prompts.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of a side-by-side comparison of multiple prompts and inputs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/promptfoo/promptfoo/assets/310310/ce5a7817-da82-4484-b26d-32474f1cabc5&#34; alt=&#34;prompt evaluation matrix - web viewer&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It works on the command line too:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/typpo/promptfoo/assets/310310/480e1114-d049-40b9-bd5f-f81c15060284&#34; alt=&#34;Prompt evaluation&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Workflow&lt;/h2&gt; &#xA;&lt;p&gt;Start by establishing a handful of test cases - core use cases and failure cases that you want to ensure your prompt can handle.&lt;/p&gt; &#xA;&lt;p&gt;As you explore modifications to the prompt, use &lt;code&gt;promptfoo eval&lt;/code&gt; to rate all outputs. This ensures the prompt is actually improving overall.&lt;/p&gt; &#xA;&lt;p&gt;As you collect more examples and establish a user feedback loop, continue to build the pool of test cases.&lt;/p&gt; &#xA;&lt;img width=&#34;772&#34; alt=&#34;LLM ops&#34; src=&#34;https://github.com/typpo/promptfoo/assets/310310/cf0461a7-2832-4362-9fbb-4ebd911d06ff&#34;&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To get started, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx promptfoo init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create some placeholders in your current directory: &lt;code&gt;prompts.txt&lt;/code&gt; and &lt;code&gt;promptfooconfig.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After editing the prompts and variables to your liking, run the eval command to kick off an evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx promptfoo eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;The YAML configuration format runs each prompt through a series of example inputs (aka &#34;test case&#34;) and checks if they meet requirements (aka &#34;assert&#34;).&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.promptfoo.dev/docs/configuration/guide&#34;&gt;Configuration docs&lt;/a&gt; for a detailed guide.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prompts: [prompt1.txt, prompt2.txt]&#xA;providers: [openai:gpt-3.5-turbo, ollama:llama2:70b]&#xA;tests:&#xA;  - description: &#39;Test translation to French&#39;&#xA;    vars:&#xA;      language: French&#xA;      input: Hello world&#xA;    assert:&#xA;      - type: contains-json&#xA;      - type: javascript&#xA;        value: output.length &amp;lt; 100&#xA;&#xA;  - description: &#39;Test translation to German&#39;&#xA;    vars:&#xA;      language: German&#xA;      input: How&#39;s it going?&#xA;    assert:&#xA;      - type: model-graded-closedqa&#xA;        value: does not describe self as an AI, model, or chatbot&#xA;      - type: similar&#xA;        value: was geht&#xA;        threshold: 0.6 # cosine similarity&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supported assertion types&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://promptfoo.dev/docs/configuration/expected-outputs&#34;&gt;Test assertions&lt;/a&gt; for full details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Assertion Type&lt;/th&gt; &#xA;   &lt;th&gt;Returns true if...&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output matches exactly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output contains substring&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;icontains&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output contains substring, case insensitive&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;regex&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output matches regex&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;starts-with&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output starts with string&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;contains-any &lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output contains any of the listed substrings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;contains-all&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output contains all list of substrings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;is-json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output is valid json (optional json schema validation)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;contains-json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;output contains valid json (optional json schema validation)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;javascript&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;provided Javascript function validates the output&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;python&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;provided Python function validates the output&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;webhook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;provided webhook returns &lt;code&gt;{pass: true}&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;similar&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;embeddings and cosine similarity are above a threshold&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;llm-rubric&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLM output matches a given rubric, using a Language Model to grade output&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;model-graded-factuality&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLM output adheres to the given facts, using Factuality method from OpenAI eval&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;model-graded-closedqa&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLM output adheres to given criteria, using Closed QA method from OpenAI eval&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;rouge-n&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Rouge-N score is above a given threshold&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;levenshtein&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Levenshtein distance is below a threshold&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Every test type can be negated by prepending &lt;code&gt;not-&lt;/code&gt;. For example, &lt;code&gt;not-equals&lt;/code&gt; or &lt;code&gt;not-regex&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tests from spreadsheet&lt;/h3&gt; &#xA;&lt;p&gt;Some people prefer to configure their LLM tests in a CSV. In that case, the config is pretty simple:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prompts: [prompts.txt]&#xA;providers: [openai:gpt-3.5-turbo]&#xA;tests: tests.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/typpo/promptfoo/raw/main/examples/simple-test/tests.csv&#34;&gt;example CSV&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Command-line&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re looking to customize your usage, you have a wide set of parameters at your disposal.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-p, --prompts &amp;lt;paths...&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Paths to &lt;a href=&#34;https://promptfoo.dev/docs/configuration/parameters#prompt-files&#34;&gt;prompt files&lt;/a&gt;, directory, or glob&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-r, --providers &amp;lt;name or path...&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;One of: openai:chat, openai:completion, openai:model-name, localai:chat:model-name, localai:completion:model-name. See &lt;a href=&#34;https://promptfoo.dev/docs/configuration/providers&#34;&gt;API providers&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-o, --output &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to &lt;a href=&#34;https://promptfoo.dev/docs/configuration/parameters#output-file&#34;&gt;output file&lt;/a&gt; (csv, json, yaml, html)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--tests &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to &lt;a href=&#34;https://promptfoo.dev/docs/configurationexpected-outputsassertions#load-an-external-tests-file&#34;&gt;external test file&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-c, --config &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to &lt;a href=&#34;https://promptfoo.dev/docs/configuration/guide&#34;&gt;configuration file&lt;/a&gt;. &lt;code&gt;promptfooconfig.js/json/yaml&lt;/code&gt; is automatically loaded if present&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-j, --max-concurrency &amp;lt;number&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of concurrent API calls&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--table-cell-max-length &amp;lt;number&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Truncate console table cells to this length&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--prompt-prefix &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This prefix is prepended to every prompt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--prompt-suffix &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This suffix is append to every prompt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--grader&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://promptfoo.dev/docs/configuration/providers&#34;&gt;Provider&lt;/a&gt; that will conduct the evaluation, if you are &lt;a href=&#34;https://promptfoo.dev/docs/configuration/expected-outputs#llm-evaluation&#34;&gt;using LLM to grade your output&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;After running an eval, you may optionally use the &lt;code&gt;view&lt;/code&gt; command to open the web viewer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx promptfoo view&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Prompt quality&lt;/h4&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/typpo/promptfoo/tree/main/examples/assistant-cli&#34;&gt;this example&lt;/a&gt;, we evaluate whether adding adjectives to the personality of an assistant bot affects the responses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx promptfoo eval -p prompts.txt -r openai:gpt-3.5-turbo -t tests.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&lt;img width=&#34;1362&#34; alt=&#34;Side-by-side evaluation of LLM prompt quality, terminal output&#34; src=&#34;https://user-images.githubusercontent.com/310310/235329207-e8c22459-5f51-4fee-9714-1b602ac3d7ca.png&#34;&gt;&#xA;&#xA;![Side-by-side evaluation of LLM prompt quality, html output](https://user-images.githubusercontent.com/310310/235483444-4ddb832d-e103-4b9c-a862-b0d6cc11cdc0.png)&#xA;--&gt; &#xA;&lt;p&gt;This command will evaluate the prompts in &lt;code&gt;prompts.txt&lt;/code&gt;, substituing the variable values from &lt;code&gt;vars.csv&lt;/code&gt;, and output results in your terminal.&lt;/p&gt; &#xA;&lt;p&gt;You can also output a nice &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1nanoj3_TniWrDl1Sj-qYqIMD6jwm5FBy15xPFdUTsmI/edit?usp=sharing&#34;&gt;spreadsheet&lt;/a&gt;, &lt;a href=&#34;https://github.com/typpo/promptfoo/raw/main/examples/simple-cli/output.json&#34;&gt;JSON&lt;/a&gt;, YAML, or an HTML file:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/310310/235483444-4ddb832d-e103-4b9c-a862-b0d6cc11cdc0.png&#34; alt=&#34;Table output&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Model quality&lt;/h4&gt; &#xA;&lt;p&gt;In the &lt;a href=&#34;https://github.com/typpo/promptfoo/tree/main/examples/gpt-3.5-vs-4&#34;&gt;next example&lt;/a&gt;, we evaluate the difference between GPT 3 and GPT 4 outputs for a given prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx promptfoo eval -p prompts.txt -r openai:gpt-3.5-turbo openai:gpt-4 -o output.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Produces this HTML table:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/310310/235490527-e0c31f40-00a0-493a-8afc-8ed6322bb5ca.png&#34; alt=&#34;Side-by-side evaluation of LLM model quality, gpt3 vs gpt4, html output&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage (node package)&lt;/h2&gt; &#xA;&lt;p&gt;You can also use &lt;code&gt;promptfoo&lt;/code&gt; as a library in your project by importing the &lt;code&gt;evaluate&lt;/code&gt; function. The function takes the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;testSuite&lt;/code&gt;: the Javascript equivalent of the promptfooconfig.yaml&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;interface EvaluateTestSuite {&#xA;  providers: string[]; // Valid provider name (e.g. openai:gpt-3.5-turbo)&#xA;  prompts: string[]; // List of prompts&#xA;  tests: string | TestCase[]; // Path to a CSV file, or list of test cases&#xA;&#xA;  defaultTest?: Omit&amp;lt;TestCase, &#39;description&#39;&amp;gt;; // Optional: add default vars and assertions on test case&#xA;  outputPath?: string; // Optional: write results to file&#xA;}&#xA;&#xA;interface TestCase {&#xA;  // Optional description of what you&#39;re testing&#xA;  description?: string;&#xA;&#xA;  // Key-value pairs to substitute in the prompt&#xA;  vars?: Record&amp;lt;string, string | string[] | object&amp;gt;;&#xA;&#xA;  // Optional list of automatic checks to run on the LLM output&#xA;  assert?: Assertion[];&#xA;&#xA;  // Additional configuration settings for the prompt&#xA;  options?: PromptConfig &amp;amp; OutputConfig &amp;amp; GradingConfig;&#xA;&#xA;  // The required score for this test case.  If not provided, the test case is graded pass/fail.&#xA;  threshold?: number;&#xA;}&#xA;&#xA;interface Assertion {&#xA;  type: string;&#xA;  value?: string;&#xA;  threshold?: number; // Required score for pass&#xA;  weight?: number; // The weight of this assertion compared to other assertions in the test case. Defaults to 1.&#xA;  provider?: ApiProvider; // For assertions that require an LLM provider&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;options&lt;/code&gt;: misc options related to how the tests are run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;interface EvaluateOptions {&#xA;  maxConcurrency?: number;&#xA;  showProgressBar?: boolean;&#xA;  generateSuggestions?: boolean;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;promptfoo&lt;/code&gt; exports an &lt;code&gt;evaluate&lt;/code&gt; function that you can use to run prompt evaluations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import promptfoo from &#39;promptfoo&#39;;&#xA;&#xA;const results = await promptfoo.evaluate({&#xA;  prompts: [&#39;Rephrase this in French: {{body}}&#39;, &#39;Rephrase this like a pirate: {{body}}&#39;],&#xA;  providers: [&#39;openai:gpt-3.5-turbo&#39;],&#xA;  tests: [&#xA;    {&#xA;      vars: {&#xA;        body: &#39;Hello world&#39;,&#xA;      },&#xA;    },&#xA;    {&#xA;      vars: {&#xA;        body: &#34;I&#39;m hungry&#34;,&#xA;      },&#xA;    },&#xA;  ],&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This code imports the &lt;code&gt;promptfoo&lt;/code&gt; library, defines the evaluation options, and then calls the &lt;code&gt;evaluate&lt;/code&gt; function with these options.&lt;/p&gt; &#xA;&lt;p&gt;See the full example &lt;a href=&#34;https://github.com/typpo/promptfoo/tree/main/examples/simple-import&#34;&gt;here&lt;/a&gt;, which includes an example results object.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://promptfoo.dev/docs/configuration/guide&#34;&gt;Main guide&lt;/a&gt;&lt;/strong&gt;: Learn about how to configure your YAML file, setup prompt files, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://promptfoo.dev/docs/configuration/expected-outputs&#34;&gt;Configuring test cases&lt;/a&gt;&lt;/strong&gt;: Learn more about how to configure expected outputs and test assertions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;strong&gt;&lt;a href=&#34;https://promptfoo.dev/docs/installation&#34;&gt;installation docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;API Providers&lt;/h2&gt; &#xA;&lt;p&gt;We support OpenAI&#39;s API as well as a number of open-source models. It&#39;s also to set up your own custom API provider. &lt;strong&gt;&lt;a href=&#34;https://promptfoo.dev/docs/configuration/providers&#34;&gt;See Provider documentation&lt;/a&gt;&lt;/strong&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please feel free to submit a pull request or open an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;promptfoo&lt;/code&gt; includes several npm scripts to make development easier and more efficient. To use these scripts, run &lt;code&gt;npm run &amp;lt;script_name&amp;gt;&lt;/code&gt; in the project directory.&lt;/p&gt; &#xA;&lt;p&gt;Here are some of the available scripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;build&lt;/code&gt;: Transpile TypeScript files to JavaScript&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;build:watch&lt;/code&gt;: Continuously watch and transpile TypeScript files on changes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;test&lt;/code&gt;: Run test suite&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;test:watch&lt;/code&gt;: Continuously run test suite on changes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://promptfoo.dev/docs/intro&#34;&gt;» View full documentation «&lt;/a&gt;&lt;/h1&gt;</summary>
  </entry>
</feed>