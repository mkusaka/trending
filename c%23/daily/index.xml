<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-19T01:32:13Z</updated>
  <subtitle>Daily Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sqrtZeroKnowledge/CVE-2023-23397_EXPLOIT_0DAY</title>
    <updated>2023-03-19T01:32:13Z</updated>
    <id>tag:github.com,2023-03-19:/sqrtZeroKnowledge/CVE-2023-23397_EXPLOIT_0DAY</id>
    <link href="https://github.com/sqrtZeroKnowledge/CVE-2023-23397_EXPLOIT_0DAY" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Exploit for the CVE-2023-23397&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2023-23397_EXPLOIT_0DAY&lt;/h1&gt; &#xA;&lt;p&gt;Exploit for the CVE-2023-23397 Credit to domchell&lt;/p&gt; &#xA;&lt;p&gt;EML/MSG Checker for the exploit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3&#xA;&#xA;from extract_msg import AppointmentMeeting&#xA;from ..helpers import Status&#xA;from ..task import Task&#xA;from ..report import Report&#xA;from .base import BaseWorker&#xA;class OutlookMSG(BaseWorker):&#xA;def analyse(self, task: Task, report: Report, manual_trigger: bool=False):&#xA;print(task.file.msg_data)&#xA;if not task.file.msg_data or not isinstance(task.file.msg_data, AppointmentMeeting):&#xA;report.status = Status.NOTAPPLICABLE&#xA;return&#xA;self.logger.debug(f&#39;analysing AppontmentMeeting in {task.file.path}...&#39;)&#xA;if task.file.msg_data.reminderFileParameter is not None:&#xA;report.status = Status.ALERT&#xA;# suspicious for cve-2023-23397: https://www.mdsec.co.uk/2023/03/exploiting-cve-2023-23397-microsoft-outlook-elevation-of-privilege-vulnerability/&#xA;report.add_details(&#39;CVE-2023-23397&#39;, f&#39;A parameter used to exploit this vulnerability is present in the mail: &#34;{task.file.msg_data.reminderFileParameter}&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pandora-analysis/pandora/raw/0dd6b01956b0501c28e4a7c1128298dcd6a499b8/pandora/workers/outlookmsg.py&#34;&gt;Based on Pandora Framework&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RageAgainstThePixel/OpenAI-DotNet</title>
    <updated>2023-03-19T01:32:13Z</updated>
    <id>tag:github.com,2023-03-19:/RageAgainstThePixel/OpenAI-DotNet</id>
    <link href="https://github.com/RageAgainstThePixel/OpenAI-DotNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast lightweight DotNet library for OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI-DotNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/xQgMW9ufN4&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/855294214065487932.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.nuget.org/packages/OpenAI-DotNet/&#34;&gt;&lt;img src=&#34;https://img.shields.io/nuget/v/OpenAI-DotNet.svg?label=OpenAI-DotNet&amp;amp;logo=nuget&#34; alt=&#34;NuGet version (OpenAI-DotNet)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RageAgainstThePixel/OpenAI-DotNet/actions/workflows/Publish-Nuget.yml&#34;&gt;&lt;img src=&#34;https://github.com/RageAgainstThePixel/OpenAI-DotNet/actions/workflows/Publish-Nuget.yml/badge.svg?sanitize=true&#34; alt=&#34;Nuget Publish&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A simple C# .NET client library for &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; to use use chat-gpt, GPT-4, GPT-3.5-Turbo and Dall-E though their RESTful API (currently in beta). Independently developed, this is not an official library and I am not affiliated with OpenAI. An OpenAI API account is required.&lt;/p&gt; &#xA;&lt;p&gt;Forked from &lt;a href=&#34;https://github.com/OkGoDoIt/OpenAI-API-dotnet&#34;&gt;OpenAI-API-dotnet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More context &lt;a href=&#34;https://rogerpincombe.com/openai-dotnet-api&#34;&gt;on Roger Pincombe&#39;s blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository is available to transfer to the OpenAI organization if they so choose to accept it.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This library targets .NET 6.0 and above.&lt;/li&gt; &#xA; &lt;li&gt;It should work across console apps, winforms, wpf, asp.net, etc.&lt;/li&gt; &#xA; &lt;li&gt;It should also work across Windows, Linux, and Mac.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Install from NuGet&lt;/h3&gt; &#xA;&lt;p&gt;Install package &lt;a href=&#34;https://www.nuget.org/packages/OpenAI-DotNet/&#34;&gt;&lt;code&gt;OpenAI-DotNet&lt;/code&gt; from Nuget&lt;/a&gt;. Here&#39;s how via command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Install-Package OpenAI-DotNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Looking to &lt;a href=&#34;https://github.com/RageAgainstThePixel/com.openai.unity&#34;&gt;use OpenAI-DotNet in the Unity Game Engine&lt;/a&gt;? Check out our unity package on OpenUPM:&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://openupm.com/packages/com.openai.unity/&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/com.openai.unity?label=openupm&amp;amp;registry_uri=https://package.openupm.com&#34; alt=&#34;openupm&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#authentication&#34;&gt;Authentication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#azure-openai&#34;&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#new-openai-api-proxy&#34;&gt;OpenAI API Proxy&lt;/a&gt; &lt;span&gt;ðŸ†•&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#models&#34;&gt;Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#list-models&#34;&gt;List Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#retrieve-model&#34;&gt;Retrieve Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#delete-fine-tuned-model&#34;&gt;Delete Fine Tuned Model&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#completions&#34;&gt;Completions&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#completion-streaming&#34;&gt;Streaming&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#chat&#34;&gt;Chat&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#chat-completions&#34;&gt;Chat Completions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#chat-streaming&#34;&gt;Streaming&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#edits&#34;&gt;Edits&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-edit&#34;&gt;Create Edit&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#embeddings&#34;&gt;Embeddings&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-embeddings&#34;&gt;Create Embedding&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#audio&#34;&gt;Audio&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-transcription&#34;&gt;Create Transcription&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-translation&#34;&gt;Create Translation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#images&#34;&gt;Images&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-image&#34;&gt;Create Image&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#edit-image&#34;&gt;Edit Image&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-image-variation&#34;&gt;Create Image Variation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#files&#34;&gt;Files&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#list-files&#34;&gt;List Files&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#upload-file&#34;&gt;Upload File&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#delete-file&#34;&gt;Delete File&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#retrieve-file-info&#34;&gt;Retrieve File Info&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#download-file-content&#34;&gt;Download File Content&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#fine-tuning&#34;&gt;Fine Tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-fine-tune-job&#34;&gt;Create Fine Tune Job&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#list-fine-tune-jobs&#34;&gt;List Fine Tune Jobs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#retrieve-fine-tune-job-info&#34;&gt;Retrieve Fine Tune Job Info&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#cancel-fine-tune-job&#34;&gt;Cancel Fine Tune Job&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#list-fine-tune-events&#34;&gt;List Fine Tune Events&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#stream-fine-tune-events&#34;&gt;Stream Fine Tune Events&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#moderations&#34;&gt;Moderations&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#create-moderation&#34;&gt;Create Moderation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;There are 3 ways to provide your API keys, in order of precedence:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#pass-keys-directly-with-constructor&#34;&gt;Pass keys directly with constructor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#load-key-from-configuration-file&#34;&gt;Load key from configuration file&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#use-system-environment-variables&#34;&gt;Use System Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You use the &lt;code&gt;OpenAIAuthentication&lt;/code&gt; when you initialize the API as shown:&lt;/p&gt; &#xA;&lt;h4&gt;Pass keys directly with constructor&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient(&#34;sk-apiKey&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or create a &lt;code&gt;OpenAIAuthentication&lt;/code&gt; object manually&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient(new OpenAIAuthentication(&#34;sk-apiKey&#34;, &#34;org-yourOrganizationId&#34;));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Load key from configuration file&lt;/h4&gt; &#xA;&lt;p&gt;Attempts to load api keys from a configuration file, by default &lt;code&gt;.openai&lt;/code&gt; in the current directory, optionally traversing up the directory tree or in the user&#39;s home directory.&lt;/p&gt; &#xA;&lt;p&gt;To create a configuration file, create a new text file named &lt;code&gt;.openai&lt;/code&gt; and containing the line:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Organization entry is optional.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h5&gt;Json format&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;apiKey&#34;: &#34;sk-aaaabbbbbccccddddd&#34;,&#xA;  &#34;organization&#34;: &#34;org-yourOrganizationId&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Deprecated format&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;OPENAI_KEY=sk-aaaabbbbbccccddddd&#xA;ORGANIZATION=org-yourOrganizationId&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also load the file directly with known path by calling a static method in Authentication:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient(OpenAIAuthentication.LoadFromDirectory(&#34;your/path/to/.openai&#34;));;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Use System Environment Variables&lt;/h4&gt; &#xA;&lt;p&gt;Use your system&#39;s environment variables specify an api key and organization to use.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; for your api key.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;OPENAI_ORGANIZATION_ID&lt;/code&gt; to specify an organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient(OpenAIAuthentication.LoadFromEnv());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/&#34;&gt;Azure OpenAI&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can also choose to use Microsoft&#39;s Azure OpenAI deployments as well. To setup the client to use your deployment, you&#39;ll need to pass in &lt;code&gt;OpenAIClientSettings&lt;/code&gt; into the client constructor.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var auth = new OpenAIAuthentication(&#34;sk-apiKey&#34;);&#xA;var settings = new OpenAIClientSettings(resourceName: &#34;your-resource&#34;, deploymentId: &#34;your-deployment-id&#34;);&#xA;var api = new OpenAIClient(auth, settings);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;ðŸ†•&lt;/span&gt; OpenAI API Proxy&lt;/h3&gt; &#xA;&lt;p&gt;Using either the &lt;a href=&#34;https://github.com/RageAgainstThePixel/OpenAI-DotNet&#34;&gt;OpenAI-DotNet&lt;/a&gt; or &lt;a href=&#34;https://github.com/RageAgainstThePixel/com.openai.unity&#34;&gt;com.openai.unity&lt;/a&gt; packages directly in your front-end app may expose your API keys and other sensitive information. To mitigate this risk, it is recommended to set up an intermediate API that makes requests to OpenAI on behalf of your front-end app. This library can be utilized for both front-end and intermediary host configurations, ensuring secure communication with the OpenAI API.&lt;/p&gt; &#xA;&lt;h4&gt;Front End Example&lt;/h4&gt; &#xA;&lt;p&gt;In the front end example, you will need to securely authenticate your users using your preferred OAuth provider. Once the user is authenticated, exchange your custom auth token with your API key on the backend.&lt;/p&gt; &#xA;&lt;p&gt;Follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup a new project using either the &lt;a href=&#34;https://github.com/RageAgainstThePixel/OpenAI-DotNet&#34;&gt;OpenAI-DotNet&lt;/a&gt; or &lt;a href=&#34;https://github.com/RageAgainstThePixel/com.openai.unity&#34;&gt;com.openai.unity&lt;/a&gt; packages.&lt;/li&gt; &#xA; &lt;li&gt;Authenticate users with your OAuth provider.&lt;/li&gt; &#xA; &lt;li&gt;After successful authentication, create a new &lt;code&gt;OpenAIAuthentication&lt;/code&gt; object and pass in the custom token with the prefix &lt;code&gt;sess-&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create a new &lt;code&gt;OpenAIClientSettings&lt;/code&gt; object and specify the domain where your intermediate API is located.&lt;/li&gt; &#xA; &lt;li&gt;Pass your new &lt;code&gt;auth&lt;/code&gt; and &lt;code&gt;settings&lt;/code&gt; objects to the &lt;code&gt;OpenAIClient&lt;/code&gt; constructor when you create the client instance.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to set up the front end:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var authToken = await LoginAsync();&#xA;var auth = new OpenAIAuthentication($&#34;sess-{authToken}&#34;);&#xA;var settings = new OpenAIClientSettings(domain: &#34;api.your-custom-domain.com&#34;);&#xA;var api = new OpenAIClient(auth, settings);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This setup allows your front end application to securely communicate with your backend that will be using the proxy service, which then forwards requests to the OpenAI API. This ensures that your OpenAI API keys and other sensitive information remain secure throughout the process.&lt;/p&gt; &#xA;&lt;h4&gt;Back End Example&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we demonstrate how to set up and use &lt;code&gt;OpenAIProxyStartup&lt;/code&gt; in a new ASP.NET Core web app. The proxy server will handle authentication and forward requests to the OpenAI API, ensuring that your API keys and other sensitive information remain secure.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a new &lt;a href=&#34;https://learn.microsoft.com/en-us/aspnet/core/tutorials/min-web-api?view=aspnetcore-6.0&#34;&gt;ASP.NET Core minimal web API&lt;/a&gt; project.&lt;/li&gt; &#xA; &lt;li&gt;Add the OpenAI-DotNet nuget package to your project. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Powershell install: &lt;code&gt;Install-Package OpenAI-DotNet&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Manually editing .csproj: &lt;code&gt;&amp;lt;PackageReference Include=&#34;OpenAI-DotNet&#34; /&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a new class that inherits from &lt;code&gt;AbstractAuthenticationFilter&lt;/code&gt; and override the &lt;code&gt;ValidateAuthentication&lt;/code&gt; method. This will implement the &lt;code&gt;IAuthenticationFilter&lt;/code&gt; that you will use to check user session token against your internal server.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;Program.cs&lt;/code&gt;, create a new proxy web application by calling &lt;code&gt;OpenAIProxyStartup.CreateDefaultHost&lt;/code&gt; method, passing your custom &lt;code&gt;AuthenticationFilter&lt;/code&gt; as a type argument.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;code&gt;OpenAIAuthentication&lt;/code&gt; and &lt;code&gt;OpenAIClientSettings&lt;/code&gt; as you would normally with your API keys, org id, or Azure settings.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;public partial class Program&#xA;{&#xA;    private class AuthenticationFilter : AbstractAuthenticationFilter&#xA;    {&#xA;        public override void ValidateAuthentication(IHeaderDictionary request)&#xA;        {&#xA;            // You will need to implement your own class to properly test&#xA;            // custom issued tokens you&#39;ve setup for your end users.&#xA;            if (!request.Authorization.ToString().Contains(userToken))&#xA;            {&#xA;                throw new AuthenticationException(&#34;User is not authorized&#34;);&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    public static void Main(string[] args)&#xA;    {&#xA;        var auth = OpenAIAuthentication.LoadFromEnv();&#xA;        var settings = new OpenAIClientSettings(/* your custom settings if using Azure OpenAI */);&#xA;        var openAIClient = new OpenAIClient(auth, settings);&#xA;        var proxy = OpenAIProxyStartup.CreateDefaultHost&amp;lt;AuthenticationFilter&amp;gt;(args, openAIClient);&#xA;        proxy.Run();&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you have set up your proxy server, your end users can now make authenticated requests to your proxy api instead of directly to the OpenAI API. The proxy server will handle authentication and forward requests to the OpenAI API, ensuring that your API keys and other sensitive information remain secure.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/models&#34;&gt;Models&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;List and describe the various models available in the API. You can refer to the &lt;a href=&#34;https://beta.openai.com/docs/models&#34;&gt;Models documentation&lt;/a&gt; to understand what models are available and the differences between them.&lt;/p&gt; &#xA;&lt;p&gt;The Models API is accessed via &lt;code&gt;OpenAIClient.ModelsEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/models/list&#34;&gt;List models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Lists the currently available models, and provides basic information about each one such as the owner and availability.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var models = await api.ModelsEndpoint.GetModelsAsync();&#xA;&#xA;foreach (var model in models)&#xA;{&#xA;    Console.WriteLine(model.ToString());&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/models/retrieve&#34;&gt;Retrieve model&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Retrieves a model instance, providing basic information about the model such as the owner and permissioning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var model = await api.ModelsEndpoint.GetModelDetailsAsync(&#34;text-davinci-003&#34;);&#xA;Console.WriteLine(model.ToString());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/delete-model&#34;&gt;Delete Fine Tuned Model&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Delete a fine-tuned model. You must have the Owner role in your organization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var result = await api.ModelsEndpoint.DeleteFineTuneModelAsync(&#34;your-fine-tuned-model&#34;);&#xA;Assert.IsTrue(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/completions&#34;&gt;Completions&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var result = await api.CompletionsEndpoint.CreateCompletionAsync(&#34;One Two Three One Two&#34;, temperature: 0.1, model: Model.Davinci);&#xA;Console.WriteLine(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To get the &lt;code&gt;CompletionResult&lt;/code&gt; (which is mostly metadata), use its implicit string operator to get the text if all you want is the completion choice.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Completion Streaming&lt;/h4&gt; &#xA;&lt;p&gt;Streaming allows you to get results are they are generated, which can help your application feel more responsive, especially on slow models like Davinci.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;&#xA;await api.CompletionsEndpoint.StreamCompletionAsync(result =&amp;gt;&#xA;{&#xA;    foreach (var choice in result.Completions)&#xA;    {&#xA;        Console.WriteLine(choice);&#xA;    }&#xA;}, &#34;My name is Roger and I am a principal software engineer at Salesforce.  This is my resume:&#34;, maxTokens: 200, temperature: 0.5, presencePenalty: 0.1, frequencyPenalty: 0.1, model: Model.Davinci);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if using &lt;a href=&#34;https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.iasyncenumerable-1?view=net-5.0&#34;&gt;&lt;code&gt;IAsyncEnumerable{T}&lt;/code&gt;&lt;/a&gt; (&lt;a href=&#34;https://docs.microsoft.com/en-us/archive/msdn-magazine/2019/november/csharp-iterating-with-async-enumerables-in-csharp-8&#34;&gt;C# 8.0+&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;await foreach (var token in api.CompletionsEndpoint.StreamCompletionEnumerableAsync(&#34;My name is Roger and I am a principal software engineer at Salesforce.  This is my resume:&#34;, maxTokens: 200, temperature: 0.5, presencePenalty: 0.1, frequencyPenalty: 0.1, model: Model.Davinci))&#xA;{&#xA;  Console.WriteLine(token);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;Chat&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Given a chat conversation, the model will return a chat completion response.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;Chat Completions&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates a completion for the chat message&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var chatPrompts = new List&amp;lt;ChatPrompt&amp;gt;&#xA;{&#xA;    new ChatPrompt(&#34;system&#34;, &#34;You are a helpful assistant.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Who won the world series in 2020?&#34;),&#xA;    new ChatPrompt(&#34;assistant&#34;, &#34;The Los Angeles Dodgers won the World Series in 2020.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Where was it played?&#34;),&#xA;};&#xA;var chatRequest = new ChatRequest(chatPrompts);&#xA;var result = await api.ChatEndpoint.GetCompletionAsync(chatRequest);&#xA;Console.WriteLine(result.FirstChoice);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create#chat/create-stream&#34;&gt;Chat Streaming&lt;/a&gt;&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var chatPrompts = new List&amp;lt;ChatPrompt&amp;gt;&#xA;{&#xA;    new ChatPrompt(&#34;system&#34;, &#34;You are a helpful assistant.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Who won the world series in 2020?&#34;),&#xA;    new ChatPrompt(&#34;assistant&#34;, &#34;The Los Angeles Dodgers won the World Series in 2020.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Where was it played?&#34;),&#xA;};&#xA;var chatRequest = new ChatRequest(chatPrompts, Model.GPT3_5_Turbo);&#xA;&#xA;await api.ChatEndpoint.StreamCompletionAsync(chatRequest, result =&amp;gt;&#xA;{&#xA;    Console.WriteLine(result.FirstChoice);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if using &lt;a href=&#34;https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.iasyncenumerable-1?view=net-5.0&#34;&gt;&lt;code&gt;IAsyncEnumerable{T}&lt;/code&gt;&lt;/a&gt; (&lt;a href=&#34;https://docs.microsoft.com/en-us/archive/msdn-magazine/2019/november/csharp-iterating-with-async-enumerables-in-csharp-8&#34;&gt;C# 8.0+&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var chatPrompts = new List&amp;lt;ChatPrompt&amp;gt;&#xA;{&#xA;    new ChatPrompt(&#34;system&#34;, &#34;You are a helpful assistant.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Who won the world series in 2020?&#34;),&#xA;    new ChatPrompt(&#34;assistant&#34;, &#34;The Los Angeles Dodgers won the World Series in 2020.&#34;),&#xA;    new ChatPrompt(&#34;user&#34;, &#34;Where was it played?&#34;),&#xA;};&#xA;var chatRequest = new ChatRequest(chatPrompts, Model.GPT3_5_Turbo);&#xA;&#xA;await foreach (var result in api.ChatEndpoint.StreamCompletionEnumerableAsync(chatRequest))&#xA;{&#xA;    Console.WriteLine(result.FirstChoice);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/edits&#34;&gt;Edits&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Given a prompt and an instruction, the model will return an edited version of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;The Edits API is accessed via &lt;code&gt;OpenAIClient.EditsEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/edits/create&#34;&gt;Create Edit&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates a new edit for the provided input, instruction, and parameters using the provided input and instruction.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var request = new EditRequest(&#34;What day of the wek is it?&#34;, &#34;Fix the spelling mistakes&#34;);&#xA;var result = await api.EditsEndpoint.CreateEditAsync(request);&#xA;Console.WriteLine(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/embeddings&#34;&gt;Embeddings&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Related guide: &lt;a href=&#34;https://beta.openai.com/docs/guides/embeddings&#34;&gt;Embeddings&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Edits API is accessed via &lt;code&gt;OpenAIClient.EmbeddingsEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/embeddings/create&#34;&gt;Create Embeddings&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates an embedding vector representing the input text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var model = await api.ModelsEndpoint.GetModelDetailsAsync(&#34;text-embedding-ada-002&#34;);&#xA;var result = await api.EmbeddingsEndpoint.CreateEmbeddingAsync(&#34;The food was delicious and the waiter...&#34;, model);&#xA;Console.WriteLine(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/audio&#34;&gt;Audio&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Converts audio into text.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio/create&#34;&gt;Create Transcription&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Transcribes audio into the input language.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var request = new AudioTranscriptionRequest(Path.GetFullPath(audioAssetPath), language: &#34;en&#34;);&#xA;var result = await api.AudioEndpoint.CreateTranscriptionAsync(request);&#xA;Console.WriteLine(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio/create&#34;&gt;Create Translation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Translates audio into into English.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var request = new AudioTranslationRequest(Path.GetFullPath(audioAssetPath));&#xA;var result = await api.AudioEndpoint.CreateTranslationAsync(request);&#xA;Console.WriteLine(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/images&#34;&gt;Images&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Given a prompt and/or an input image, the model will generate a new image.&lt;/p&gt; &#xA;&lt;p&gt;The Images API is accessed via &lt;code&gt;OpenAIClient.ImagesEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/images/create&#34;&gt;Create Image&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates an image given a prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var results = await api.ImagesEndPoint.GenerateImageAsync(&#34;A house riding a velociraptor&#34;, 1, ImageSize.Small);&#xA;&#xA;foreach (var result in results)&#xA;{&#xA;    Console.WriteLine(result);&#xA;    // result == file://path/to/image.png&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/images/create-edit&#34;&gt;Edit Image&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates an edited or extended image given an original image and a prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var results = await api.ImagesEndPoint.CreateImageEditAsync(Path.GetFullPath(imageAssetPath), Path.GetFullPath(maskAssetPath), &#34;A sunlit indoor lounge area with a pool containing a flamingo&#34;, 1, ImageSize.Small);&#xA;&#xA;foreach (var result in results)&#xA;{&#xA;    Console.WriteLine(result);&#xA;    // result == file://path/to/image.png&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/images/create-variation&#34;&gt;Create Image Variation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates a variation of a given image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var results = await api.ImagesEndPoint.CreateImageVariationAsync(Path.GetFullPath(imageAssetPath), 1, ImageSize.Small);&#xA;&#xA;foreach (var result in results)&#xA;{&#xA;    Console.WriteLine(result);&#xA;    // result == file://path/to/image.png&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files&#34;&gt;Files&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Files are used to upload documents that can be used with features like &lt;a href=&#34;https://raw.githubusercontent.com/RageAgainstThePixel/OpenAI-DotNet/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Files API is accessed via &lt;code&gt;OpenAIClient.FilesEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files/list&#34;&gt;List Files&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns a list of files that belong to the user&#39;s organization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var files = await api.FilesEndpoint.ListFilesAsync();&#xA;&#xA;foreach (var file in files)&#xA;{&#xA;    Console.WriteLine($&#34;{file.Id} -&amp;gt; {file.Object}: {file.FileName} | {file.Size} bytes&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files/upload&#34;&gt;Upload File&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var fileData = await api.FilesEndpoint.UploadFileAsync(&#34;path/to/your/file.jsonl&#34;, &#34;fine-tune&#34;);&#xA;Console.WriteLine(fileData.Id);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files/delete&#34;&gt;Delete File&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Delete a file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var result = await api.FilesEndpoint.DeleteFileAsync(fileData);&#xA;Assert.IsTrue(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files/retrieve&#34;&gt;Retrieve File Info&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns information about a specific file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var fileData = await GetFileInfoAsync(fileId);&#xA;Console.WriteLine($&#34;{fileData.Id} -&amp;gt; {fileData.Object}: {fileData.FileName} | {fileData.Size} bytes&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/files/retrieve-content&#34;&gt;Download File Content&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Downloads the specified file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var downloadedFilePath = await api.FilesEndpoint.DownloadFileAsync(fileId, &#34;path/to/your/save/directory&#34;);&#xA;Console.WriteLine(downloadedFilePath);&#xA;Assert.IsTrue(File.Exists(downloadedFilePath));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes&#34;&gt;Fine Tuning&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Manage fine-tuning jobs to tailor a model to your specific training data.&lt;/p&gt; &#xA;&lt;p&gt;Related guide: &lt;a href=&#34;https://beta.openai.com/docs/guides/fine-tuning&#34;&gt;Fine-tune models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Files API is accessed via &lt;code&gt;OpenAIClient.FineTuningEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/create&#34;&gt;Create Fine Tune Job&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Creates a job that fine-tunes a specified model from a given dataset.&lt;/p&gt; &#xA;&lt;p&gt;Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var request = new CreateFineTuneRequest(fileData);&#xA;var fineTuneJob = await api.FineTuningEndpoint.CreateFineTuneJobAsync(request);&#xA;Console.WriteLine(fineTuneJob.Id);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/list&#34;&gt;List Fine Tune Jobs&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;List your organization&#39;s fine-tuning jobs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var fineTuneJobs = await api.FineTuningEndpoint.ListFineTuneJobsAsync();&#xA;&#xA;foreach (var job in fineTuneJobs)&#xA;{&#xA;    Console.WriteLine($&#34;{job.Id} -&amp;gt; {job.Status}&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/retrieve&#34;&gt;Retrieve Fine Tune Job Info&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Gets info about the fine-tune job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var result = await api.FineTuningEndpoint.RetrieveFineTuneJobInfoAsync(fineTuneJob);&#xA;Console.WriteLine($&#34;{result.Id} -&amp;gt; {result.Status}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/cancel&#34;&gt;Cancel Fine Tune Job&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Immediately cancel a fine-tune job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var result = await api.FineTuningEndpoint.CancelFineTuneJobAsync(fineTuneJob);&#xA;Assert.IsTrue(result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/events&#34;&gt;List Fine Tune Events&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Get fine-grained status updates for a fine-tune job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var fineTuneEvents = await api.FineTuningEndpoint.ListFineTuneEventsAsync(fineTuneJob);&#xA;Console.WriteLine($&#34;{fineTuneJob.Id} -&amp;gt; status: {fineTuneJob.Status} | event count: {fineTuneEvents.Count}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/fine-tunes/events#fine-tunes/events-stream&#34;&gt;Stream Fine Tune Events&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;await api.FineTuningEndpoint.StreamFineTuneEventsAsync(fineTuneJob, fineTuneEvent =&amp;gt;&#xA;{&#xA;    Console.WriteLine($&#34;  {fineTuneEvent.CreatedAt} [{fineTuneEvent.Level}] {fineTuneEvent.Message}&#34;);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if using &lt;a href=&#34;https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.iasyncenumerable-1?view=net-5.0&#34;&gt;&lt;code&gt;IAsyncEnumerable{T}&lt;/code&gt;&lt;/a&gt; (&lt;a href=&#34;https://docs.microsoft.com/en-us/archive/msdn-magazine/2019/november/csharp-iterating-with-async-enumerables-in-csharp-8&#34;&gt;C# 8.0+&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;await foreach (var fineTuneEvent in api.FineTuningEndpoint.StreamFineTuneEventsEnumerableAsync(fineTuneJob))&#xA;{&#xA;    Console.WriteLine($&#34;  {fineTuneEvent.CreatedAt} [{fineTuneEvent.Level}] {fineTuneEvent.Message}&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/moderations&#34;&gt;Moderations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Given a input text, outputs if the model classifies it as violating OpenAI&#39;s content policy.&lt;/p&gt; &#xA;&lt;p&gt;Related guide: &lt;a href=&#34;https://beta.openai.com/docs/guides/moderation&#34;&gt;Moderations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Moderations API can be accessed via &lt;code&gt;OpenAIClient.ModerationsEndpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://beta.openai.com/docs/api-reference/moderations/create&#34;&gt;Create Moderation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Classifies if text violates OpenAI&#39;s Content Policy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var api = new OpenAIClient();&#xA;var response = await api.ModerationsEndpoint.GetModerationAsync(&#34;I want to kill them.&#34;);&#xA;Assert.IsTrue(response);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://licensebuttons.net/p/zero/1.0/88x31.png&#34; alt=&#34;CC-0 Public Domain&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library is licensed CC-0, in the public domain. You can use it for whatever you want, publicly or privately, without worrying about permission or licensing or whatever. It&#39;s just a wrapper around the OpenAI API, so you still need to get access to OpenAI from them directly. I am not affiliated with OpenAI and this library is not endorsed by them, I just have beta access and wanted to make a C# library to access it more easily. Hopefully others find this useful as well. Feel free to open a PR if there&#39;s anything you want to contribute.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>VRCFury/VRCFury</title>
    <updated>2023-03-19T01:32:13Z</updated>
    <id>tag:github.com,2023-03-19:/VRCFury/VRCFury</id>
    <link href="https://github.com/VRCFury/VRCFury" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Non-Destructive Tools for VRChat Avatars&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VRC Fury - Non-Destructive Tools for VRChat Avatars&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clothing Attacher // Toggle Builder // Gesture Manager // Controller Merger // Avatar Optimizer // Modular Setup // All Reversible!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;h3&gt;&amp;gt;&amp;gt; &lt;a href=&#34;https://vrcfury.com/download&#34;&gt;Download VRCFury&lt;/a&gt; || &lt;a href=&#34;https://vrcfury.com/discord&#34;&gt;Support Discord&lt;/a&gt; &amp;lt;&amp;lt;&lt;/h3&gt; &#xA;&lt;h2&gt;Benefits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy to use&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Define toggles, gestures, item modes and more using a simple GUI in Unity&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No more layers, no more menu editing&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The animation controller, VRC Menu, and synced parameters are all generated automatically by VRC Fury.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Great for asset artists&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Prefabs can contain their own VRC Fury definitions&lt;/li&gt; &#xA;   &lt;li&gt;Distribute your avatar addons with everything needed for the user to add your animations / props to their menu.&lt;/li&gt; &#xA;   &lt;li&gt;No more complicated &#34;copy my layers into your fx&#34;! If their project has VRC Fury, they can just drop your prefab into their project, and upload!&lt;/li&gt; &#xA;   &lt;li&gt;For more details, check out &lt;a href=&#34;https://gitlab.com/VRCFury/VRCFury/-/wikis/VRCFury-for-Artists&#34;&gt;VRCFury for Artists&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No more absolute paths for animations&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;VRC Fury props defined in prefabs will automatically have their clips rewritten to work properly, no matter where in the hierarchy they are ultimately placed.&lt;/li&gt; &#xA;   &lt;li&gt;Write your animation clips from the root of the prefab, and VRCFury will handle rewriting it when it winds up on an avatar.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No clips? No problem!&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Just want to toggle a game object or a blend shape? No worries!&lt;/li&gt; &#xA;   &lt;li&gt;VRC Fury can create these toggles for you, without you needing to touch animation clips whatsoever.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gestures, Idle Animation Support, and more!&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fury isn&#39;t just about props. It also has logic to build every single animation layer that I personally use myself for my avatars. If it can&#39;t build a layer the way you want, let us know and maybe we can support it!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Already got your avatar perfect?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;VRC Fury still works perfectly with avatars that already have animations.&lt;/li&gt; &#xA;   &lt;li&gt;Your existing layers, parameters, and menus will be untouched, and VRC Fury will keep its work totally separate from yours.&lt;/li&gt; &#xA;   &lt;li&gt;This also means VRC Fury will not clobber the work of TPS, VRCLens, etc.&lt;/li&gt; &#xA;   &lt;li&gt;VRCFury does its work just before your avatar uploads. It makes a copy of your work, adds the features you&#39;ve requested, then ships that off! This means your animation controller files are never touched, and if you&#39;re unhappy with the results, you can just remove it and your next upload will be like it was never there.&lt;/li&gt; &#xA;   &lt;li&gt;Note: VRC Fury works with existing controllers using &lt;em&gt;either&lt;/em&gt; Write Defaults ON or OFF.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No more write defaults pain&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Every layer generated by VRC Fury will automatically have its default states calculated and maintained, based on the resting state of your avatar in the editor. This includes animation clips you give VRC Fury, so now you only have to make &#34;on&#34; animations, no more &#34;off&#34;!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;And more!&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We&#39;re constantly adding more and more non-destructive avatar features to VRCFury. Keep an eye out for updates!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and import the package above (it may take a bit to finish importing)&lt;/li&gt; &#xA; &lt;li&gt;On your main avatar object, or the prop you are trying to setup, click &lt;code&gt;Add Component&lt;/code&gt; -&amp;gt; &lt;code&gt;VRC Fury&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add features using the &lt;code&gt;+&lt;/code&gt; button on the component. See the &lt;code&gt;Feature Modules&lt;/code&gt; section below for information about each type of feature.&lt;/li&gt; &#xA; &lt;li&gt;You&#39;re done! There&#39;s no &#34;building&#34; to do. VRCFury will non-destructively update your controllers, VRC menus and params automatically before each upload.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Feature Modules&lt;/h2&gt; &#xA;&lt;p&gt;Once you add a VRCFury component to your avatar (or prop), you can add any combination of these feature modules. These are in alphabetical order, but one of the most popular is the &lt;code&gt;Toggle&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Advanced Visemes&lt;/h3&gt; &#xA;&lt;p&gt;This feature allows you to use VRCFury actions as visemes.&lt;/p&gt; &#xA;&lt;p&gt;Benefits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use animation clips, material flipbooks, or any other VRCFury action as a speech viseme.&lt;/li&gt; &#xA; &lt;li&gt;You can use bone transforms in your visemes, meaning you can open your jaw rather than using an &#34;open mouth&#34; blend shape.&lt;/li&gt; &#xA; &lt;li&gt;This can enhance some features, such as tongue movement, while your mouth is open during speech.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Anchor Override Fix&lt;/h3&gt; &#xA;&lt;p&gt;Adding this feature ensures that all of your avatar meshes use the same anchor override, which ensures different meshes receive the same environment lighting level.&lt;/p&gt; &#xA;&lt;h3&gt;Armature Link (Attach clothing, props, etc)&lt;/h3&gt; &#xA;&lt;p&gt;Is your prop a skinned mesh that &#34;attaches&#34; to the bones of the root avatar? Armature Link is what you need! Give it the root bone (hips) within your prop and the path to that same root bone in the avatar, and VRCFury will automatically attach the mesh to the avatar&#39;s bones. Just make sure the bones you want to link are all in the same order and have the same names.&lt;/p&gt; &#xA;&lt;p&gt;You can also use this feature to attach an object to an avatar&#39;s bone. For instance, if your prop contains a empty that must be on the avatar&#39;s hand, you can use Armature Link to place it there.&lt;/p&gt; &#xA;&lt;h3&gt;Blendshape Link&lt;/h3&gt; &#xA;&lt;p&gt;Add this feature, specify a clothing mesh, and a path to the Body object on the avatar, and the blendshapes from the clothing will automatically be linked to the avatar body. This means any sliders / toggles affecting the body will be reflected in the clothing as well automatically.&lt;/p&gt; &#xA;&lt;h3&gt;Blendshape Optimizer&lt;/h3&gt; &#xA;&lt;p&gt;Automatically bakes all non-animated blendshapes into your avatar&#39;s meshes. Reduces your VRAM usage for free, no configuration required!&lt;/p&gt; &#xA;&lt;h3&gt;Blink Controller&lt;/h3&gt; &#xA;&lt;p&gt;Include a single-frame animation of your avatar with its eyes closed (or click the plus and give it the blend shape name), and VRC Fury will drive your avatar&#39;s blink cycle.&lt;/p&gt; &#xA;&lt;p&gt;Benefits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Blinking will stop automatically when your avatar performs vrcfury gestures affecting its eyes. This means no more &#39;double-blinking&#39;.&lt;/li&gt; &#xA; &lt;li&gt;Unlike vrc&#39;s built-in eye tracking disable feature, your eyes will not freeze closed, partially closed, unfreeze unexpectedly due to combo-gestures.&lt;/li&gt; &#xA; &lt;li&gt;Your eye blink will be synchronized with all other clients (I&#39;m unsure if the default vrc eye blink is synced or not).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Bounding Box Fix&lt;/h3&gt; &#xA;&lt;p&gt;This feature ensures that every mesh on your avatar has a suitably large bounding box. This prevents the issue when some objects on your avatar dissappear when viewed at extreme angles.&lt;/p&gt; &#xA;&lt;h3&gt;Breathing Controller&lt;/h3&gt; &#xA;&lt;p&gt;Automatically creates an animation for your avatar&#39;s breathing cycle. Provide either a gameobject (which will be scaled between the provided &#34;min&#34; and &#34;max&#34; scale), or a blendshape, which will be animated between 0 and 1.&lt;/p&gt; &#xA;&lt;h3&gt;Cross-Eye Fix&lt;/h3&gt; &#xA;&lt;p&gt;VRChat introduces roll to your eye bones in some circumstances, making it appear that you&#39;ve gone cross-eyed. Adding this fix will solve this problem automatically through a combination of rotation constraints to eliminate roll.&lt;/p&gt; &#xA;&lt;h3&gt;Direct Tree Optimizer&lt;/h3&gt; &#xA;&lt;p&gt;If you add this feature to your avatar, VRCFury will attempt to convert all &#34;plain&#34; toggle layers on your avatar into a compressed direct blend-tree. This reduces the number of animator layers on your avatar, which can have a meaningful improvement on performance (frame time).&lt;/p&gt; &#xA;&lt;h3&gt;Fix Write Defaults&lt;/h3&gt; &#xA;&lt;p&gt;This feature will automatically align Write Defaults for every state on your avatar. If will automatically prefer whichever your avatar is &#34;closest to,&#34; meaning it will select On or Off depending on which requires the fewest changes to your avatar. If you&#39;d like, you can override the selection and Force Off or Force On. Yes, it&#39;s magic.&lt;/p&gt; &#xA;&lt;h3&gt;Force Object State&lt;/h3&gt; &#xA;&lt;p&gt;This feature can activate, deactivate, or delete an object on your avatar during the upload process. Useful if you want to show clothing on your avatar in the editor but have it &#34;off&#34; during the upload for toggles to work properly. Also useful if you want to delete an object from a prefab without having to unpack it.&lt;/p&gt; &#xA;&lt;h3&gt;Full Controller&lt;/h3&gt; &#xA;&lt;p&gt;This is usually only useful for prefab artists. Provide a controller, menu, and params, and it will be merged into your client&#39;s avatar automatically. If you&#39;re working on your own avatar, you should usually just add these things to your avatar&#39;s own controller, menu, and params instead.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Animation clips in your specified controller should have animated paths relative to your prop&#39;s root. VRCF will automatically add prefixes to all the animations so they work wherever it is installed on the avatar. If you wish to animate properties on the avatar itself (outside of your prop), you can specify a path from the root of the avatar (as you traditionally would), and they will still work. VRCFury will automatically determine if the path is from the avatar root or from the prop root, and rewrite them appropriately if needed.&lt;/p&gt; &#xA;&lt;h3&gt;Gestures&lt;/h3&gt; &#xA;&lt;p&gt;This is the one-stop shop for adding hand gestures to your avatar! For each hand gesture, choose which hand is used for the gesture, and which hand sign needs to be acted. If you select COMBO, the gesture will only activate if you do the given gesture on both hands simultaneously. Use the + (Plus symbol) to set the animation clip / blend shape that your gesture will activate.&lt;/p&gt; &#xA;&lt;p&gt;Advanced options for each gesture:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Disable Blinking when Active&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If enabled, this gesture will automatically prevent your avatar from blinking while the gesture is active. Useful if the gesture does something like sad eyes or eyes closed. Note: Only works if your avatar&#39;s blinking is controlled by the VrcFury Blink Controller.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Customize Transition Time&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default, gestures will active in 0.1 seconds. You can adjust this with this option.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Gesture Lock&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Set a menu path here, and an item will be created in your expression menu at the given path. The menu item will allow you to easily lock the gesture &#34;on&#34; without having to hold the hand sign.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Exclusive Tag&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If multiple gestures contain the same Exclusive Tag, only one can be active at a time. For instance, if you have a Sad and an Angry gesture, you could give them both the same tag, and the system will prevent them from being active simultaneously. The &#34;highest&#34; one in the list wins. A gesture can have multiple Exclusive Tags separated by commas.&lt;/p&gt; &#xA;&lt;h3&gt;Gizmo&lt;/h3&gt; &#xA;&lt;p&gt;All this does is show an editor gizmo. It does nothing in game. Useful primarily for prop artists who wish to identify something on their prop without including it in the upload.&lt;/p&gt; &#xA;&lt;h3&gt;Move Menu Item&lt;/h3&gt; &#xA;&lt;p&gt;Can move a menu item, either already on the avatar, or one created by VRCFury. Simply enter the path you&#39;d like to move from and move to. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;From: &lt;code&gt;My Folder/Clothing/Shirt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;To: &lt;code&gt;Cool Stuff/Clothes/Shirt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Override Menu Icon&lt;/h3&gt; &#xA;&lt;p&gt;Will override the icon for the given item in your menu.&lt;/p&gt; &#xA;&lt;h3&gt;Override Menu Settings&lt;/h3&gt; &#xA;&lt;p&gt;Allows you to change VRCFury&#39;s default &#34;Next&#34; menu item, when there are too many items to fit on a page.&lt;/p&gt; &#xA;&lt;h3&gt;Remove Hand Gestures&lt;/h3&gt; &#xA;&lt;p&gt;When present, this feature eliminates any features in your avatar&#39;s non-vrcfury controllers that use hand gestures. This is useful if you&#39;d like to implement your own hand gestures with VRCFury, don&#39;t want them to conflict with ones that came with a base avatar, and don&#39;t want to edit them manually.&lt;/p&gt; &#xA;&lt;h3&gt;Security Lock&lt;/h3&gt; &#xA;&lt;p&gt;This feature allows you to set a pin number which, when entered in your avatar&#39;s Security submenu, will unlock any Toggleable Props which you&#39;ve marked with the Security flag.&lt;/p&gt; &#xA;&lt;h3&gt;Senky Gesture Driver&lt;/h3&gt; &#xA;&lt;p&gt;This feature sets up gestures the way that Senky likes them! Probably not super useful for you, unless you want this very specific gesture hand layout.&lt;/p&gt; &#xA;&lt;h3&gt;Slot 4 Fix&lt;/h3&gt; &#xA;&lt;p&gt;Allows you to animate materials in slot 4 on meshes within your avatar. Typically, attempting to do this results in corruption of the material in slot 2 due to a unity bug. This feature resolves the issue by moving all slot 4 references to a new slot at the end of the list.&lt;/p&gt; &#xA;&lt;h3&gt;Toes Puppet&lt;/h3&gt; &#xA;&lt;p&gt;Given an animation for up, down, and out, this creates a puppet for toe control in your menu.&lt;/p&gt; &#xA;&lt;h3&gt;Toggle&lt;/h3&gt; &#xA;&lt;p&gt;Use this for a normal &#34;on / off&#34; prop. For simple object props, click the plus, choose Object toggle, and then drag the object into the field. If you choose blendshape, the blendshape will be set to 100 when &#34;on&#34; (only works on root skinned meshes). For more advanced &#34;on&#34; states, you can provide an animation clip instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Menu Entry&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The name you put in the prop&#39;s text field will be used as the name of the toggle in your VRChat menu. If you wish to put the prop in a sub-menu, use slashes. Ex: &lt;code&gt;Props/My Cool Piano&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Default On&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Want to add an idle animation or &#34;default prop&#34; to your avatar? Create a new prop, click the &lt;code&gt;*&lt;/code&gt; and select &lt;code&gt;Default On&lt;/code&gt;. Your idle animation or prop will now be on all the time (but you can also trigger it back off in game!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Show in Rest Pose&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If set, this toggle will be enabled in the avatar&#39;s &#34;Rest Pose&#34;. This means the toggle will shown &#34;on&#34; in the in-game avatar selector, during full body calibration, and for users who have disabled your avatar&#39;s animations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Slider&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Select &lt;code&gt;Slider&lt;/code&gt; from the &lt;code&gt;*&lt;/code&gt; menu, and VRC Fury will make the prop into a slider rather than a toggle. 0 will be the avatar default state, and 100% will be your &#34;enabled&#34; state. If &lt;code&gt;Default On&lt;/code&gt; is also set, an arbitrary starting value can be set.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Saved&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Not everything in VRC Fury has to be a temporary prop. Want to save your clothes (or anything else?) across worlds? Select &lt;code&gt;Saved between worlds&lt;/code&gt; in the &lt;code&gt;*&lt;/code&gt; menu.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Security&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;When a prop is flagged with the &lt;code&gt;Security&lt;/code&gt; flag, it can only be enabled when the Security Lock feature is unlocked on your avatar (see the Security Lock section for more details).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Physbone Reset&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Got an animation that changes parameters on a physbone?&lt;/p&gt; &#xA;&lt;p&gt;Click the advanced &lt;code&gt;*&lt;/code&gt; button on the VRC Fury prop for the animation, then click &lt;code&gt;Add PhysBone to Reset&lt;/code&gt;. Drag the object for the physbone into the box (it should be on an empty by itself). VRC Fury will automatically flip the bone off and on any time your animation is run or reset, causing the physbone to reload your changed settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Exclusive Tags&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If multiple toggles contain the same Exclusive Tag, only one can be active at a time. For example, if you have multiple sets of clothing which interfere with each other, you can give them the same tag. When one is enabled, all other toggles with the same tag will be disabled. Multiple tags can be given, separated by commas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Exclusive Tag Off State&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If set, this toggle will automatically be activated when all other toggles with the same &lt;code&gt;Exclusive Tags&lt;/code&gt; are disabled. This makes it usable as an &#34;Off&#34; state for a set of conflicting toggles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Separate Local State&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If set, this creates a separate animation for local and remote machines. The local state will be seen by the user in the avatar, and the remote state will be seen by everyone else.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Enable Transition State&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If set, this will create 2 additional states for animating a transition between the off and on state. The transition animation will be played forwards when transitioning from off to on and backwards when transitioning from on to off when &lt;code&gt;Transition Out is reverse of Transition In&lt;/code&gt; is on, otherwise an separate out transition can be set. If &lt;code&gt;Separate Local State&lt;/code&gt; is also on, separate local transitions can also be set.&lt;/p&gt; &#xA;&lt;h3&gt;When-Talking State&lt;/h3&gt; &#xA;&lt;p&gt;This is a very simple feature which activates the given animation only while the user is &#34;talking&#34; (with any viseme).&lt;/p&gt; &#xA;&lt;h2&gt;Additional Features&lt;/h2&gt; &#xA;&lt;h3&gt;Controller-Less Setup&lt;/h3&gt; &#xA;&lt;p&gt;Your avatar doesn&#39;t even need to have a FX layer, menu, or params! If these are unset, VRCFury will create them automatically, and manage them fully (meaning it will be deleted and recreated from scratch before each upload). Beware of this! If you want to make your own changes to your controller, menu, or params, then you should create one yourself outside of the vrcf temp directory.&lt;/p&gt; &#xA;&lt;h3&gt;VRCF Global Colliders&lt;/h3&gt; &#xA;&lt;p&gt;VRCFury can be used to add globally-synced colliders to any bone on your avatar. This means you can put one on your foot, your nose, or anywhere else you can imagine, then bap people with them! Simply create an empty on the bone you&#39;d like to add a collider to, then add a &lt;code&gt;VRCF Global Collider&lt;/code&gt; component to that empty.&lt;/p&gt; &#xA;&lt;p&gt;Beware that this feature steals colliders from your fingers, so the more you add, the fewer contacts there will be on your fingers. It will try to steal from the least important fingers first. You&#39;ve been warned!&lt;/p&gt; &#xA;&lt;h3&gt;D4rk Avatar Optimizer Integration&lt;/h3&gt; &#xA;&lt;p&gt;If your avatar uses the D4rk Optimizer, VRCFury will automatically run the optimizer during your avatar upload. No need to run it manually anymore!&lt;/p&gt; &#xA;&lt;h3&gt;Write Defaults Auto-Fix&lt;/h3&gt; &#xA;&lt;p&gt;VRCFury will detect if your avatar has a mixture of Write Defaults, and will offer to fix it for you on your first upload. Don&#39;t worry, this change isn&#39;t destructive -- it simply adds a &lt;code&gt;Fix Write Defaults&lt;/code&gt; VRCFury component to your avatar root, which you can always remove to undo if you choose.&lt;/p&gt; &#xA;&lt;h3&gt;Action Controller Conflict Resolution&lt;/h3&gt; &#xA;&lt;p&gt;If you install multiple independent packages of avatar &#34;dances&#34; using vrcfury, they will be rewritten to work together. For instance, you can install GogoLoco AND CuteDancer using VRCFury, and will be able to use the dances from each. Typically this is impossible as the animations from one will override the other, however VRCFury rewrites the playable layer weight drivers to affect only the layers owned by each individual package. Hooray!&lt;/p&gt; &#xA;&lt;h2&gt;How to remove / uninstall&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;gt; VRCFury &amp;gt; Uninstall VRCFury&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If that doesn&#39;t work, or the menu is missing entirely, go into Unity&#39;s &lt;code&gt;Package Manager&lt;/code&gt; tab and remove all the VRCFury packages. &lt;strong&gt;Remove the VRCFury Updater FIRST&lt;/strong&gt; so it doesn&#39;t try to reinstall the rest.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>