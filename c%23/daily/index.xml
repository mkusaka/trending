<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-01T01:32:07Z</updated>
  <subtitle>Daily Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>starik222/BooruDatasetTagManager</title>
    <updated>2023-03-01T01:32:07Z</updated>
    <id>tag:github.com,2023-03-01:/starik222/BooruDatasetTagManager</id>
    <link href="https://github.com/starik222/BooruDatasetTagManager" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BooruDatasetTagManager&lt;/h1&gt; &#xA;&lt;p&gt;A simple tag editor for a dataset created in &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; with the deepdanbooru option enabled, or creating a dataset from scratch.&lt;/p&gt; &#xA;&lt;h1&gt;Using&lt;/h1&gt; &#xA;&lt;p&gt;You need a dataset like the following:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;You can also specify a dataset without text files if you want to create tags from scratch. In this case, text files will be created on save.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1236582/198582869-be2938a7-f7b2-4ad9-8e8c-a53604a24c2d.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the program, select &#34;File-&amp;gt;Load folder&#34; and specify the directory with the dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1236582/218164012-6b9a474c-dca6-4556-a608-ba89f132d600.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the left column, tags are edited for the selected image, in the right column, tags are edited for all images of the dataset.&lt;/p&gt; &#xA;&lt;p&gt;After editing, you will select &#34;File-&amp;gt;Save all changes&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Also, you can load loss statistics after training. After pressing the &#34;Interrupt&#34; button, in the console you will see the loss statistics for each image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1236582/198585578-1a958600-cc95-466e-b926-3cfed44b28e4.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Copy all text to file. File should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Loss statistics for file C:\NAI\stable-diffusion-webui\train\NishinoOut2\00006-0-00003-0-98028336_p0.png&#xA;loss:0.045±(0.002)&#xA;recent 32 loss:0.055±(0.007)&#xA;Loss statistics for file C:\NAI\stable-diffusion-webui\train\NishinoOut2\00014-0-00007-0-98909113_p0.png&#xA;loss:0.045±(0.002)&#xA;recent 32 loss:0.048±(0.007)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the program, select &#34;File-&amp;gt;Load loss from file&#34;, and you will see:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1236582/198586476-6094d32f-b31d-48a2-8ad7-f043417cd78c.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can automatically translate tags into the language you need. Specify the code of the language you need in the setting.json file. In the program select &#34;View-&amp;gt;Translate tags&#34;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/CodeXGLUE</title>
    <updated>2023-03-01T01:32:07Z</updated>
    <id>tag:github.com,2023-03-01:/microsoft/CodeXGLUE</id>
    <link href="https://github.com/microsoft/CodeXGLUE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CodeXGLUE&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://evansdata.com/press/viewRelease.php?pressID=278&#34;&gt;Evans Data Corporation&lt;/a&gt;, there are 23.9 million professional developers in 2019, and the population is expected to reach 28.7 million in 2024. With the growing population of developers, code intelligence, which aims to leverage AI to help software developers improve the productivity of the development process, is growing increasingly important in both communities of software engineering and artificial intelligence.&lt;/p&gt; &#xA;&lt;p&gt;When developers want to find code written by others with the same intent, &lt;a href=&#34;https://arxiv.org/abs/1909.09436&#34;&gt;code search&lt;/a&gt; systems can help automatically retrieve semantically relevant code given natural language queries. When developers are confused about what to write next, &lt;a href=&#34;https://arxiv.org/abs/1912.00742&#34;&gt;code completion&lt;/a&gt; systems can help by automatically completing the following tokens given the context of the edits being made. When developers want to implement Java code with the same function of some existing body of Python code, &lt;a href=&#34;https://arxiv.org/abs/2006.03511&#34;&gt;code-to-code translation&lt;/a&gt; systems can help translate from one programming language (Python) to another (Java).&lt;/p&gt; &#xA;&lt;p&gt;Code intelligence therefore plays a vital role in Microsoft’s mission to empower developers. As highlighted by Microsoft CEO Satya Nadella at Microsoft &lt;a href=&#34;https://mybuild.microsoft.com/sessions/23912de2-1531-4684-b85a-d57ac30af09e&#34;&gt;Build 2020&lt;/a&gt;, the role of developers is more important than ever. GitHub is increasingly the default home for source code, and Visual Studio Code is the most popular code editor. Microsoft offers the most complete toolchain for developers, bringing together the best of GitHub, Visual Studio, and Microsoft Azure to help developers to go from idea to code and code to cloud.&lt;/p&gt; &#xA;&lt;p&gt;Recent years have seen a surge of applying of statistical models, including neural nets, to code intelligence tasks. Very recently, pre-trained models learned from big programming language data have been inspired by the great success of large pre-trained models like &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1908.09203&#34;&gt;GPT&lt;/a&gt; in natural language processing (NLP). These models, including &lt;a href=&#34;https://arxiv.org/pdf/2005.08025.pdf&#34;&gt;IntelliCode&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2002.08155.pdf&#34;&gt;CodeBERT&lt;/a&gt;, obtain further improvements on code understanding and generation problems. However, the area of code intelligence lacks a benchmark suite that covers a wide range of tasks. We have seen that a diversified benchmark dataset is significant for the growth of an area of applied AI research, like &lt;a href=&#34;http://image-net.org/&#34;&gt;ImageNet&lt;/a&gt; for computer vision and &lt;a href=&#34;https://gluebenchmark.com/&#34;&gt;GLUE&lt;/a&gt; for NLP.&lt;/p&gt; &#xA;&lt;p&gt;To address this, researchers from Microsoft Research Asia, Developer Division, and Bing introduce CodeXGLUE, a benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison. CodeXGLUE stands for General Language Understanding Evaluation benchmark for CODE. It includes 14 datasets for 10 diversified code intelligence tasks covering the following scenarios:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code&#34;&gt;code-code&lt;/a&gt;&lt;/strong&gt; (clone detection, defect detection, cloze test, code completion, code repair, and code-to-code translation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code&#34;&gt;text-code&lt;/a&gt;&lt;/strong&gt; (natural language code search, text-to-code generation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/&#34;&gt;code-text&lt;/a&gt;&lt;/strong&gt; (code summarization)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/microsoft/CodeXGLUE/tree/main/Text-Text&#34;&gt;text-text&lt;/a&gt;&lt;/strong&gt; (documentation translation)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A brief summary of CodeXGLUE is given below, including tasks, datasets, language, sizes in various states, baseline systems, providers, and short definitions of each task. Datasets highlighted in BLUE are newly introduced. &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/tasks.jpg&#34; alt=&#34;A brief summary of CodeXGLUE, including tasks, datasets, baseline systems, etc.&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To make it easy for participants, we provide three baseline models to support these tasks, including a BERT-style pre-trained model (in this case, CodeBERT), which is good at understanding problems. We also include a GPT-style pre-trained model, which we call CodeGPT, to support completion and generation problems. Finally, we include an Encoder-Decoder framework that supports sequence-to-sequence generation problems.&lt;/p&gt; &#xA;&lt;p&gt;Three pipelines including &lt;a href=&#34;https://github.com/microsoft/CodeBERT&#34;&gt;CodeBERT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2&#34;&gt;CodeGPT&lt;/a&gt;, and Encoder-Decoder are provided to make it easy for participants. &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/baselines.jpg&#34; alt=&#34;baselines&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With CodeXGLUE, we seek to support the development of models that can be applied to various code intelligence problems, with the goal of increasing the productivity of software developers. We encourage researchers to participate in the open challenges to continue progress in code intelligence. Moving forward, we’ll extend CodeXGLUE to more programming languages and downstream tasks while continuing to push forward pre-trained models by exploring new model structures, introducing new pre-training tasks, using different types of data, and more.&lt;/p&gt; &#xA;&lt;h1&gt;Relevant Links&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://microsoft.github.io/CodeXGLUE/&#34;&gt;Leaderboard&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2102.04664.pdf&#34;&gt;CodeXGLUE paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/datasets?search=code_x_glue&#34;&gt;Access from HuggingFace datasets&lt;/a&gt; &lt;img alt=&#34;Hugging Face Datasets&#34; src=&#34;https://img.shields.io/badge/-%F0%9F%A4%97%20datasets-blue&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Tasks and Datasets&lt;/h1&gt; &#xA;&lt;p&gt;Below, we elaborate on the task definition for each task and newly introduced datasets that are highlighted in the table above.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone detection (BigCloneBench, POJ-104). A model is tasked with measure the semantic similarity between codes. Two existing datasets are included. One is for binary classification between code and the other is for retrieving semantically similar code given code as the query.&lt;/li&gt; &#xA; &lt;li&gt;Defect detection (Devign). A model is tasked with identifying whether a body of source code contains defects that may be used to attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack. An existing dataset is included.&lt;/li&gt; &#xA; &lt;li&gt;Cloze test (CT-all, CT-max/min). A model is tasked with predicting the masked token from code, formulated as a multi-choice classification problem. The two datasets are newly created, one with candidates from the (filtered) vocabulary and the other with candidates among “max” and “min”.&lt;/li&gt; &#xA; &lt;li&gt;Code completion (PY150, GitHub Java Corpus). A model is tasked with predicting following tokens given a code context. Both token-level and line-level completion are covered. The token-level task is analogous to language modeling, and we include two influential datasets here. Line-level datasets are newly created to test a model’s ability to autocomplete a line.&lt;/li&gt; &#xA; &lt;li&gt;Code translation (CodeTrans). A model is tasked with translating the code in one programming language to the code in another one. A dataset between Java and C# is newly created.&lt;/li&gt; &#xA; &lt;li&gt;Code search (CodeSearchNet, AdvTest; CodeSearchNet, WebQueryTest). ). A model is given the task of measuring semantic similarity between text and code. In the retrieval scenario, a test set is newly created where function names and variables in test sets are replaced to test the generalization ability of a model. In text-code classification scenario, a test set where natural language queries come from Bing query log is created to test on real user queries.&lt;/li&gt; &#xA; &lt;li&gt;Code repair (Bugs2Fix). A model is tasked with trying to automatically refine the code, which could be buggy or complex. An existing dataset is included.&lt;/li&gt; &#xA; &lt;li&gt;Text-to-code generation (CONCODE). A model is given the task to generate code given natural language description. An existing dataset is included.&lt;/li&gt; &#xA; &lt;li&gt;Code summarization (CodeSearchNet). A model is given the task to generate natural language comments for a code. Existing datasets are included.&lt;/li&gt; &#xA; &lt;li&gt;Documentation translation (Microsoft Docs). A model is given the task to translate code documentation between human languages. A dataset, focusing on low-resource multilingual translation, is newly created.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Submission Instructions&lt;/h1&gt; &#xA;&lt;p&gt;Once you have built a model that meets your expectations on evaluation with the dev set, you can submit your test results to get official evaluation on the test set. To ensure the integrity of the official test results, we do not release the correct answers for test set to the public. To submit your model for official evaluation on the test set, follow the below steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate your prediction output for the dev set.&lt;/li&gt; &#xA; &lt;li&gt;Run the official evaluation methodologies found in the task specific git repo and verify your systems are running as expected.&lt;/li&gt; &#xA; &lt;li&gt;Generate your prediction output for the test set.&lt;/li&gt; &#xA; &lt;li&gt;Submit the following information by emailing to &lt;code&gt;codexglue@microsoft.com&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your email should include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prediction results on test set. &lt;strong&gt;[Required]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prediction results on dev set. &lt;strong&gt;[Recommended]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Individual/Team Name: Name of the individual or the team to appear in the leaderboard. &lt;strong&gt;[Required]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Individual/Team Institution: Name of the institution of the individual or the team to appear in the leaderboard. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Model code: Training code for the model. &lt;strong&gt;[Recommended]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Model information: Name of the model/technique to appear in the leaderboard. &lt;strong&gt;[Required]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paper Information: Name, Citation, URL of the paper if model is from a published work to appear in the leaderboard. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To avoid &#34;P-hacking&#34; we discourage too many submissions from the same group in a short period of time.&lt;/p&gt; &#xA;&lt;h1&gt;Training and Inference Time Cost&lt;/h1&gt; &#xA;&lt;p&gt;We calculate the training and inference time cost for each dataset with 2 P100 GPUs. Results are shared in the following table. &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/time-cost.jpg&#34; alt=&#34;time-cost&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LICENSE&lt;/h1&gt; &#xA;&lt;p&gt;Our codes follow MIT License.&lt;/p&gt; &#xA;&lt;p&gt;Our datasets follow Computational Use of Data Agreement (C-UDA) License.&lt;/p&gt; &#xA;&lt;h1&gt;Reference&lt;/h1&gt; &#xA;&lt;p&gt;If you use this code or CodeXGLUE, please consider citing us.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{DBLP:journals/corr/abs-2102-04664,&#xA;  author    = {Shuai Lu and&#xA;               Daya Guo and&#xA;               Shuo Ren and&#xA;               Junjie Huang and&#xA;               Alexey Svyatkovskiy and&#xA;               Ambrosio Blanco and&#xA;               Colin B. Clement and&#xA;               Dawn Drain and&#xA;               Daxin Jiang and&#xA;               Duyu Tang and&#xA;               Ge Li and&#xA;               Lidong Zhou and&#xA;               Linjun Shou and&#xA;               Long Zhou and&#xA;               Michele Tufano and&#xA;               Ming Gong and&#xA;               Ming Zhou and&#xA;               Nan Duan and&#xA;               Neel Sundaresan and&#xA;               Shao Kun Deng and&#xA;               Shengyu Fu and&#xA;               Shujie Liu},&#xA;  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding&#xA;               and Generation},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2102.04664},&#xA;  year      = {2021}&#xA;}&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This research was conducted by Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Daya Guo, Duyu Tang, Junjie Huang, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shuai Lu, Shujie Liu, and Shuo Ren.&lt;/p&gt;</summary>
  </entry>
</feed>