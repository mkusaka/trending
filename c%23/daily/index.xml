<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-04T01:29:24Z</updated>
  <subtitle>Daily Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>thebookisclosed/AmperageKit</title>
    <updated>2024-06-04T01:29:24Z</updated>
    <id>tag:github.com,2024-06-04:/thebookisclosed/AmperageKit</id>
    <link href="https://github.com/thebookisclosed/AmperageKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;One stop shop for enabling Recall in Windows 11 version 24H2 on unsupported devices&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amperage&lt;/h1&gt; &#xA;&lt;p&gt;Amperage is a console Windows app designed to help you enable Recall on devices that aren&#39;t officially supported.&lt;/p&gt; &#xA;&lt;p&gt;At the moment, Amperage can only enable recall if your machine has an &lt;em&gt;Arm64 CPU / SoC&lt;/em&gt;. That is, any &lt;strong&gt;Qualcomm Snapdragon&lt;/strong&gt;, &lt;strong&gt;Microsoft SQ&lt;/strong&gt;, or &lt;strong&gt;Ampere&lt;/strong&gt; chipset.&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you&#39;re running Windows 11 version 24H2 &lt;strong&gt;build 26100.712&lt;/strong&gt; before continuing. Older builds, as well as newer betas (builds 26200-26217) do not include the necessary OS level components for Recall.&lt;/p&gt; &#xA;&lt;p&gt;Most x86_64 users will have to wait until Microsoft publishes AI Components for their platform. &lt;em&gt;However&lt;/em&gt;, if you&#39;re feeling particularly adventurous you can try &lt;a href=&#34;https://github.com/thebookisclosed/AmperageKit/raw/main/ArmOnX86_64.md&#34;&gt;Emulating Arm64 Windows on your x86_64 Windows PC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;26100.712 is also available on &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/arm/create-arm-vm&#34;&gt;Azure ARM VMs&lt;/a&gt; by using a 24H2 image (currently 26100.560) and installing KB5037850.&lt;/p&gt; &#xA;&lt;h1&gt;How do I get started?&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Head to the &lt;a href=&#34;https://github.com/thebookisclosed/AmperageKit/releases&#34;&gt;Releases&lt;/a&gt; page and download the latest version&lt;/li&gt; &#xA; &lt;li&gt;Download the AI Components (Machine Learning workloads) for Arm64 from &lt;a href=&#34;https://archive.org/details/windows-workloads-0.3.252.0-arm-64.7z&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Unpack the release you&#39;ve just downloaded&lt;/li&gt; &#xA; &lt;li&gt;Unpack the contents of the Workloads archive into the &lt;code&gt;WorkloadComponents&lt;/code&gt; folder next to &lt;code&gt;Amperage.exe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fire up Command Prompt as Administrator and navigate to the directory you extracted Amperage to&lt;/li&gt; &#xA; &lt;li&gt;Type in &lt;code&gt;amperage /install&lt;/code&gt; and hit Enter&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The tool will now guide you through Recall installation. It should all finish in one fell swoop but in case you happened to misplace any files, it will let you know as soon as you run it. &lt;img src=&#34;https://github.com/thebookisclosed/AmperageKit/assets/13197516/722ffccb-3c16-4d3e-bf4c-b959d01588e3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awaescher/OllamaSharp</title>
    <updated>2024-06-04T01:29:24Z</updated>
    <id>tag:github.com,2024-06-04:/awaescher/OllamaSharp</id>
    <link href="https://github.com/awaescher/OllamaSharp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ollama API bindings for .NET&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OllamaSharp ðŸ¦™&lt;/h1&gt; &#xA;&lt;p&gt;OllamaSharp is a .NET binding for the &lt;a href=&#34;https://github.com/jmorganca/ollama/raw/main/docs/api.md&#34;&gt;Ollama API&lt;/a&gt;, making it easy to interact with Ollama using your favorite .NET languages.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intuitive API client: Set up and interact with Ollama in just a few lines of code.&lt;/li&gt; &#xA; &lt;li&gt;API endpoint coverage: Support for all Ollama API endpoints including chats, embeddings, listing models, pulling and creating new models, and more.&lt;/li&gt; &#xA; &lt;li&gt;Real-time streaming: Stream responses directly to your application.&lt;/li&gt; &#xA; &lt;li&gt;Progress reporting: Get real-time progress feedback on tasks like model pulling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/awaescher/OllamaSharp/main/#api-console&#34;&gt;API Console&lt;/a&gt;: A ready-to-use API console to chat and manage your Ollama host remotely&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;OllamaSharp wraps every Ollama API endpoint in awaitable methods that fully support response streaming.&lt;/p&gt; &#xA;&lt;p&gt;The follow list shows a few examples to get a glimpse on how easy it is to use. The list is not complete.&lt;/p&gt; &#xA;&lt;h3&gt;Initializing&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;// set up the client&#xA;var uri = new Uri(&#34;http://localhost:11434&#34;);&#xA;var ollama = new OllamaApiClient(uri);&#xA;&#xA;// select a model which should be used for further operations&#xA;ollama.SelectedModel = &#34;llama2&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing all models that are available locally&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var models = await ollama.ListLocalModels();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pulling a model and reporting progress&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;await ollama.PullModel(&#34;mistral&#34;, status =&amp;gt; Console.WriteLine($&#34;({status.Percent}%) {status.Status}&#34;));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Streaming a completion directly into the console&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;// keep reusing the context to keep the chat topic going&#xA;ConversationContext context = null;&#xA;context = await ollama.StreamCompletion(&#34;How are you today?&#34;, context, stream =&amp;gt; Console.Write(stream.Response));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building interactive chats&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;// uses the /chat api from Ollama 0.1.14&#xA;// messages including their roles will automatically be tracked within the chat object&#xA;var chat = ollama.Chat(stream =&amp;gt; Console.WriteLine(stream.Message?.Content ?? &#34;&#34;));&#xA;while (true)&#xA;{&#xA;    var message = Console.ReadLine();&#xA;    await chat.Send(message);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Api Console&lt;/h2&gt; &#xA;&lt;p&gt;This project ships a full-featured demo console for all endpoints the Ollama API is exposing.&lt;/p&gt; &#xA;&lt;p&gt;This is not only a great &lt;a href=&#34;https://raw.githubusercontent.com/awaescher/OllamaSharp/main/OllamaApiConsole/Demos&#34;&gt;resource to learn&lt;/a&gt; about OllamaSharp, it can also be used to manage and chat with the Ollama host remotely. &lt;a href=&#34;https://github.com/awaescher/OllamaSharp/raw/main/docs/imagechat.png&#34;&gt;Image chat&lt;/a&gt; is supported for multi modal models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/awaescher/OllamaSharp/raw/main/docs/demo.gif&#34; alt=&#34;Api Console Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Icon and name were reused from the amazing &lt;a href=&#34;https://github.com/jmorganca/ollama&#34;&gt;Ollama project&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>