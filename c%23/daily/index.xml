<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-23T01:34:57Z</updated>
  <subtitle>Daily Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ppy/osu</title>
    <updated>2022-06-23T01:34:57Z</updated>
    <id>tag:github.com,2022-06-23:/ppy/osu</id>
    <link href="https://github.com/ppy/osu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;rhythm is just a *click* away!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; alt=&#34;osu! logo&#34; src=&#34;https://raw.githubusercontent.com/ppy/osu/master/assets/lazer.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;osu!&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ppy/osu/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/ppy/osu/actions/workflows/ci.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ppy/osu/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/ppy/osu.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/ppy/osu&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/ppy/osu/badge&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/ppy&#34;&gt;&lt;img src=&#34;https://discordapp.com/api/guilds/188630481301012481/widget.png?style=shield&#34; alt=&#34;dev chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crowdin.com/project/osu-web&#34;&gt;&lt;img src=&#34;https://d322cqt584bo4o.cloudfront.net/osu-web/localized.svg?sanitize=true&#34; alt=&#34;Crowdin&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A free-to-win rhythm game. Rhythm is just a &lt;em&gt;click&lt;/em&gt; away!&lt;/p&gt; &#xA;&lt;p&gt;The future of &lt;a href=&#34;https://osu.ppy.sh&#34;&gt;osu!&lt;/a&gt; and the beginning of an open era! Currently known by and released under the release codename &#34;&lt;em&gt;lazer&lt;/em&gt;&#34;. As in sharper than cutting-edge.&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is under heavy development, but is in a stable state. Users are encouraged to try it out and keep it installed alongside the stable &lt;em&gt;osu!&lt;/em&gt; client. It will continue to evolve to the point of eventually replacing the existing stable client as an update.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; Gameplay mechanics (and other features which you may have come to know and love) are in a constant state of flux. Game balance and final quality-of-life passes come at the end of development, preceded by experimentation and changes which may potentially &lt;strong&gt;reduce playability or usability&lt;/strong&gt;. This is done in order to allow us to move forward as developers and designers more efficiently. If this offends you, please consider sticking to the stable releases of osu! (found on the website). We are not yet open to heated discussion over game mechanics and will not be using github as a forum for such discussions just yet.&lt;/p&gt; &#xA;&lt;p&gt;We are accepting bug reports (please report with as much detail as possible and follow the existing issue templates). Feature requests are also welcome, but understand that our focus is on completing the game to feature parity before adding new features. A few resources are available as starting points to getting involved and understanding the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detailed release changelogs are available on the &lt;a href=&#34;https://osu.ppy.sh/home/changelog/lazer&#34;&gt;official osu! site&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can learn more about our approach to &lt;a href=&#34;https://github.com/ppy/osu/wiki/Project-management&#34;&gt;project management&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read peppy&#39;s &lt;a href=&#34;https://blog.ppy.sh/a-definitive-lazer-faq/&#34;&gt;blog post&lt;/a&gt; exploring where the project is currently and the roadmap going forward.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running osu!&lt;/h2&gt; &#xA;&lt;p&gt;If you are looking to install or test osu! without setting up a development environment, you can consume our &lt;a href=&#34;https://github.com/ppy/osu/releases&#34;&gt;binary releases&lt;/a&gt;. Handy links below will download the latest version for your operating system of choice:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latest build:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/ppy/osu/releases/latest/download/install.exe&#34;&gt;Windows 8.1+ (x64)&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;macOS 10.15+ (&lt;a href=&#34;https://github.com/ppy/osu/releases/latest/download/osu.app.Intel.zip&#34;&gt;Intel&lt;/a&gt;, &lt;a href=&#34;https://github.com/ppy/osu/releases/latest/download/osu.app.Apple.Silicon.zip&#34;&gt;Apple Silicon&lt;/a&gt;)&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/ppy/osu/releases/latest/download/osu.AppImage&#34;&gt;Linux (x64)&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://osu.ppy.sh/home/testflight&#34;&gt;iOS 10+&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/ppy/osu/releases/latest/download/sh.ppy.osulazer.apk&#34;&gt;Android 5+&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The iOS testflight link may fill up (Apple has a hard limit of 10,000 users). We reset it occasionally when this happens. Please do not ask about this. Check back regularly for link resets or follow &lt;a href=&#34;https://twitter.com/ppy&#34;&gt;peppy&lt;/a&gt; on twitter for announcements of link resets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If your platform is not listed above, there is still a chance you can manually build it by following the instructions below.&lt;/p&gt; &#xA;&lt;h2&gt;Developing a custom ruleset&lt;/h2&gt; &#xA;&lt;p&gt;osu! is designed to have extensible modular gameplay modes, called &#34;rulesets&#34;. Building one of these allows a developer to harness the power of osu! for their own game style. To get started working on a ruleset, we have some templates available &lt;a href=&#34;https://github.com/ppy/osu/tree/master/Templates&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can see some examples of custom rulesets by visiting the &lt;a href=&#34;https://github.com/ppy/osu/discussions/13096&#34;&gt;custom ruleset directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Developing osu!&lt;/h2&gt; &#xA;&lt;p&gt;Please make sure you have the following prerequisites:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A desktop platform with the &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;.NET 6.0 SDK&lt;/a&gt; installed.&lt;/li&gt; &#xA; &lt;li&gt;When developing with mobile, &lt;a href=&#34;https://docs.microsoft.com/en-us/xamarin/&#34;&gt;Xamarin&lt;/a&gt; is required, which is shipped together with Visual Studio or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/mac/&#34;&gt;Visual Studio for Mac&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;When working with the codebase, we recommend using an IDE with intelligent code completion and syntax highlighting, such as the latest version of &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt;, &lt;a href=&#34;https://www.jetbrains.com/rider/&#34;&gt;JetBrains Rider&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;Visual Studio Code&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;When running on Linux, please have a system-wide FFmpeg installation available to support video decoding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Downloading the source code&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ppy/osu&#xA;cd osu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the source code to the latest commit, run the following command inside the &lt;code&gt;osu&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Build configurations for the recommended IDEs (listed above) are included. You should use the provided Build/Run functionality of your IDE to get things going. When testing or building new components, it&#39;s highly encouraged you use the &lt;code&gt;VisualTests&lt;/code&gt; project/configuration. More information on this is provided &lt;a href=&#34;https://raw.githubusercontent.com/ppy/osu/master/#contributing&#34;&gt;below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visual Studio / Rider users should load the project via one of the platform-specific &lt;code&gt;.slnf&lt;/code&gt; files, rather than the main &lt;code&gt;.sln&lt;/code&gt;. This will allow access to template run configurations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also build and run &lt;em&gt;osu!&lt;/em&gt; from the command-line with a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dotnet run --project osu.Desktop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not interested in debugging &lt;em&gt;osu!&lt;/em&gt;, you can add &lt;code&gt;-c Release&lt;/code&gt; to gain performance. In this case, you must replace &lt;code&gt;Debug&lt;/code&gt; with &lt;code&gt;Release&lt;/code&gt; in any commands mentioned in this document.&lt;/p&gt; &#xA;&lt;p&gt;If the build fails, try to restore NuGet packages with &lt;code&gt;dotnet restore&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Due to a historical feature gap between .NET Core and Xamarin, running &lt;code&gt;dotnet&lt;/code&gt; CLI from the root directory will not work for most commands. This can be resolved by specifying a target &lt;code&gt;.csproj&lt;/code&gt; or the helper project at &lt;code&gt;build/Desktop.proj&lt;/code&gt;. Configurations have been provided to work around this issue for all supported IDEs mentioned above.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Testing with resource/framework modifications&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes it may be necessary to cross-test changes in &lt;a href=&#34;https://github.com/ppy/osu-resources&#34;&gt;osu-resources&lt;/a&gt; or &lt;a href=&#34;https://github.com/ppy/osu-framework&#34;&gt;osu-framework&lt;/a&gt;. This can be achieved by running some commands as documented on the &lt;a href=&#34;https://github.com/ppy/osu-resources/wiki/Testing-local-resources-checkout-with-other-projects&#34;&gt;osu-resources&lt;/a&gt; and &lt;a href=&#34;https://github.com/ppy/osu-framework/wiki/Testing-local-framework-checkout-with-other-projects&#34;&gt;osu-framework&lt;/a&gt; wiki pages.&lt;/p&gt; &#xA;&lt;h3&gt;Code analysis&lt;/h3&gt; &#xA;&lt;p&gt;Before committing your code, please run a code formatter. This can be achieved by running &lt;code&gt;dotnet format&lt;/code&gt; in the command line, or using the &lt;code&gt;Format code&lt;/code&gt; command in your IDE.&lt;/p&gt; &#xA;&lt;p&gt;We have adopted some cross-platform, compiler integrated analyzers. They can provide warnings when you are editing, building inside IDE or from command line, as-if they are provided by the compiler itself.&lt;/p&gt; &#xA;&lt;p&gt;JetBrains ReSharper InspectCode is also used for wider rule sets. You can run it from PowerShell with &lt;code&gt;.\InspectCode.ps1&lt;/code&gt;. Alternatively, you can install ReSharper or use Rider to get inline support in your IDE of choice.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;When it comes to contributing to the project, the two main things you can do to help out are reporting issues and submitting pull requests. Based on past experiences, we have prepared a &lt;a href=&#34;https://raw.githubusercontent.com/ppy/osu/master/CONTRIBUTING.md&#34;&gt;list of contributing guidelines&lt;/a&gt; that should hopefully ease you into our collaboration process and answer the most frequently-asked questions.&lt;/p&gt; &#xA;&lt;p&gt;Note that while we already have certain standards in place, nothing is set in stone. If you have an issue with the way code is structured, with any libraries we are using, or with any processes involved with contributing, &lt;em&gt;please&lt;/em&gt; bring it up. We welcome all feedback so we can make contributing to this project as painless as possible.&lt;/p&gt; &#xA;&lt;p&gt;If you wish to help with localisation efforts, head over to &lt;a href=&#34;https://crowdin.com/project/osu-web&#34;&gt;crowdin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For those interested, we love to reward quality contributions via &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1jNXfj_S3Pb5PErA-czDdC9DUu4IgUbe1Lt8E7CYUJuE/view?&amp;amp;rm=minimal#gid=523803337&#34;&gt;bounties&lt;/a&gt;, paid out via PayPal or osu!supporter tags. Don&#39;t hesitate to &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSet_8iFAgPMG526pBZ2Kic6HSh7XPM3fE8xPcnWNkMzINDdYg/viewform&#34;&gt;request a bounty&lt;/a&gt; for your work on this project.&lt;/p&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;osu!&lt;/em&gt;&#39;s code and framework are licensed under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT licence&lt;/a&gt;. Please see &lt;a href=&#34;https://raw.githubusercontent.com/ppy/osu/master/LICENCE&#34;&gt;the licence file&lt;/a&gt; for more information. &lt;a href=&#34;https://tldrlegal.com/license/mit-license&#34;&gt;tl;dr&lt;/a&gt; you can do whatever you want as long as you include the original copyright and license notice in any copy of the software/source.&lt;/p&gt; &#xA;&lt;p&gt;Please note that this &lt;em&gt;does not cover&lt;/em&gt; the usage of the &#34;osu!&#34; or &#34;ppy&#34; branding in any software, resources, advertising or promotion, as this is protected by trademark law.&lt;/p&gt; &#xA;&lt;p&gt;Please also note that game resources are covered by a separate licence. Please see the &lt;a href=&#34;https://github.com/ppy/osu-resources&#34;&gt;ppy/osu-resources&lt;/a&gt; repository for clarifications.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azure-functions-dotnet-worker</title>
    <updated>2022-06-23T01:34:57Z</updated>
    <id>tag:github.com,2022-06-23:/Azure/azure-functions-dotnet-worker</id>
    <link href="https://github.com/Azure/azure-functions-dotnet-worker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Azure Functions out-of-process .NET language worker&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Azure/azure-functions-cli/master/src/Azure.Functions.Cli/npm/assets/azure-functions-logo-color-raster.png&#34; alt=&#34;Azure Functions Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Branch&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;main&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://azfunc.visualstudio.com/Azure%20Functions/_build/latest?definitionId=45&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://azfunc.visualstudio.com/Azure%20Functions/_apis/build/status/.NET%20Worker/.NET%20Worker?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;release/1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://azfunc.visualstudio.com/Azure%20Functions/_build/latest?definitionId=45&amp;amp;branchName=release%2F1.x&#34;&gt;&lt;img src=&#34;https://azfunc.visualstudio.com/Azure%20Functions/_apis/build/status/.NET%20Worker/.NET%20Worker?branchName=release%2F1.x&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Azure Functions .NET Worker&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Azure Functions .NET Worker Repository. The .NET Worker provides .NET 5 support in Azure Functions, introducing an &lt;strong&gt;Isolated Model&lt;/strong&gt;, running as an out-of-process language worker that is separate from the Azure Functions runtime. This allows you to have full control over your application&#39;s dependencies as well as other new features like a middleware pipeline.&lt;/p&gt; &#xA;&lt;p&gt;A .NET Isolated function app works differently than a .NET Core 3.1 function app. For .NET Isolated, you build an executable that imports the .NET Isolated language worker as a NuGet package. Your app includes a &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-functions-dotnet-worker/main/samples/FunctionApp/Program.cs&#34;&gt;&lt;code&gt;Program.cs&lt;/code&gt;&lt;/a&gt; that starts the worker.&lt;/p&gt; &#xA;&lt;h2&gt;Binding Model&lt;/h2&gt; &#xA;&lt;p&gt;.NET Isolated introduces a new binding model, slightly different from the binding model exposed in .NET Core 3 Azure Functions. More information can be &lt;a href=&#34;https://github.com/Azure/azure-functions-dotnet-worker/wiki/.NET-Worker-bindings&#34;&gt;found here&lt;/a&gt;. Please review our samples for usage information.&lt;/p&gt; &#xA;&lt;h2&gt;Middleware&lt;/h2&gt; &#xA;&lt;p&gt;The Azure Functions .NET Isolated supports middleware registration, following a model similar to what exists in ASP.NET and giving you the ability to inject logic into the invocation pipeline, pre and post function executions.&lt;/p&gt; &#xA;&lt;h2&gt;Samples&lt;/h2&gt; &#xA;&lt;p&gt;You can find samples on how to use different features of the .NET Worker under &lt;code&gt;samples&lt;/code&gt; (&lt;a href=&#34;https://github.com/Azure/azure-functions-dotnet-worker/tree/main/samples&#34;&gt;link&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Create and run .NET Isolated functions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Visual Studio and Visual Studio Code support is on the way. In the meantime, please use &lt;code&gt;azure-functions-core-tools&lt;/code&gt; or the sample projects as a starting point.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Install .NET 5.0&lt;/h3&gt; &#xA;&lt;p&gt;Download .NET 5.0 &lt;a href=&#34;https://dotnet.microsoft.com/download/dotnet/5.0&#34;&gt;from here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Install the Azure Functions Core Tools&lt;/h3&gt; &#xA;&lt;p&gt;To download Core Tools, please check out our docs at &lt;a href=&#34;https://github.com/Azure/azure-functions-core-tools&#34;&gt;Azure Functions Core Tools&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Create a .NET Isolated project&lt;/h3&gt; &#xA;&lt;p&gt;In an empty directory, run &lt;code&gt;func init&lt;/code&gt; and select &lt;code&gt;dotnet (Isolated Process)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a function&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;func new&lt;/code&gt; and select any trigger (&lt;code&gt;HttpTrigger&lt;/code&gt; is a good one to start). Fill in the function name.&lt;/p&gt; &#xA;&lt;h3&gt;Run functions locally&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;func host start&lt;/code&gt; in the sample app directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you selected a trigger different from &lt;code&gt;HttpTrigger&lt;/code&gt;, you may need to setup local connection strings or emulator for the trigger service.&lt;/p&gt; &#xA;&lt;h3&gt;Debugging&lt;/h3&gt; &#xA;&lt;h4&gt;Visual Studio&lt;/h4&gt; &#xA;&lt;p&gt;Debugging for the Isolated model is supported in Visual Studio 2019 and 2022 with the Azure Development workloads support installed.&lt;/p&gt; &#xA;&lt;h4&gt;JetBrains Rider&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: To debug your Worker, you must be using the Azure Functions Core Tools version 3.0.3381 or higher. You must also have the &lt;a href=&#34;https://plugins.jetbrains.com/plugin/11220-azure-toolkit-for-rider&#34;&gt;Azure Toolkit for Rider&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In Rider, make sure a Run Configuration is generated for your Azure Functions project is active. You can also create a custom Run Configuration from the &lt;strong&gt;Run | Edit Configurations...&lt;/strong&gt; menu.&lt;/p&gt; &#xA;&lt;p&gt;To start debugging, select the run configuration and start debugging. This will compile your project, run the Core Tools, and attach the debugger to your project.&lt;/p&gt; &#xA;&lt;p&gt;Under the hood, Rider launches the Core Tools with the &lt;code&gt;--dotnet-isolated-debug&lt;/code&gt; argument, and attached to the process ID for your worker process.&lt;/p&gt; &#xA;&lt;p&gt;You can place a breakpoint in any function, and inspect your code as it is running. Note that &lt;a href=&#34;https://github.com/Azure/azure-functions-dotnet-worker/issues/434&#34;&gt;debugging startup code may timeout (#434)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running E2E Tests&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.2&#34;&gt;Powershell 7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cosmos-db/local-emulator?tabs=ssl-netstd21&#34;&gt;CosmosDb Emulator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Azurite (the set up script will download this automatically)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run &lt;code&gt;setup-e2e-tests.ps1&lt;/code&gt;. Once the build succeeds and the emulators are started correctly, you are done with the setup.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;run-e2e-tests.ps1&lt;/code&gt; to run the tests or use the Test Explorer in VS.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Do &lt;strong&gt;not&lt;/strong&gt; add the switch to skip the core-tools download when running &lt;code&gt;set-up-e2e-tests.ps1&lt;/code&gt; as it will lead to an incomplete setup.&lt;/p&gt; &#xA;&lt;h2&gt;Deploying to Azure&lt;/h2&gt; &#xA;&lt;h3&gt;Create the Azure resources&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;To deploy the app, first ensure that you&#39;ve installed the Azure CLI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Login to the CLI.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;az login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If necessary, use &lt;code&gt;az account set&lt;/code&gt; to select the subscription you want to use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a resource group, Storage account, and Azure Functions app. If you would like to use an existing Windows .NET Core 3 function app, please skip this step.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;az group create --name AzureFunctionsQuickstart-rg --location westeurope&#xA;az storage account create --name &amp;lt;STORAGE_NAME&amp;gt; --location westeurope --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS&#xA;az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location westeurope --runtime dotnet-isolated --functions-version 3 --name &amp;lt;APP_NAME&amp;gt; --storage-account &amp;lt;STORAGE_NAME&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deploy the app&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you are in your functions project folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy the app.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;func azure functionapp publish &amp;lt;APP_NAME&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Known issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimizations are not all in place in the consumption plan and you may experience longer cold starts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Please create issues in this repo. Thanks!&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Unity-Technologies/arfoundation-samples</title>
    <updated>2022-06-23T01:34:57Z</updated>
    <id>tag:github.com,2022-06-23:/Unity-Technologies/arfoundation-samples</id>
    <link href="https://github.com/Unity-Technologies/arfoundation-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Example content for Unity projects based on AR Foundation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AR Foundation Samples&lt;/h1&gt; &#xA;&lt;p&gt;Example projects that use &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html&#34;&gt;&lt;em&gt;AR Foundation 5.0&lt;/em&gt;&lt;/a&gt; and demonstrate its functionality with sample assets and components.&lt;/p&gt; &#xA;&lt;p&gt;This set of samples relies on three Unity packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google ARCore XR Plug-in (&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arcore@5.0/manual/index.html&#34;&gt;documentation&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Apple ARKit XR Plug-in (&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arkit@5.0/manual/index.html&#34;&gt;documentation&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ARFoundation (&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html&#34;&gt;documentation&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What version should I use?&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Unity Version&lt;/th&gt; &#xA;   &lt;th&gt;ARFoundation Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/1.5-preview&#34;&gt;1.5 (preview)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/2.1&#34;&gt;2.1 (verified)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/4.1&#34;&gt;4.1 (verified)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/4.2&#34;&gt;4.2 (verified)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022.1&lt;/td&gt; &#xA;   &lt;td&gt;5.0 (prerelease)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022.2&lt;/td&gt; &#xA;   &lt;td&gt;5.0 (prerelease)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ARSubsystems&lt;/h2&gt; &#xA;&lt;p&gt;ARFoundation is built on &#34;&lt;a href=&#34;https://docs.unity3d.com/2021.3/Documentation/ScriptReference/Subsystem.html&#34;&gt;subsystems&lt;/a&gt;&#34; and depends on subsystems defined in &lt;code&gt;UnityEngine.XR.ARSubsystems&lt;/code&gt; namespace. This namespace defines an interface, and the platform-specific implementations are in the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arcore@5.0/manual/index.html&#34;&gt;Google ARCore&lt;/a&gt; and &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arkit@5.0/manual/index.html&#34;&gt;Apple ARKit&lt;/a&gt; packages. ARFoundation turns the AR data provided by ARSubsystems into Unity &lt;code&gt;GameObject&lt;/code&gt;s and &lt;code&gt;MonoBehavour&lt;/code&gt;s.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;main&lt;/code&gt; branch is compatible with Unity 2021.2 and later. For earlier versions, see the table above.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions for installing AR Foundation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the latest version of Unity 2021.2 or later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open Unity, and load the project at the root of the &lt;em&gt;arfoundation-samples&lt;/em&gt; repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open your choice of sample scene.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html&#34;&gt;AR Foundation Documentation&lt;/a&gt; for usage instructions and more information.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Samples&lt;/h1&gt; &#xA;&lt;h2&gt;SimpleAR&lt;/h2&gt; &#xA;&lt;p&gt;This is a good starting sample that enables point cloud visualization and plane detection. There are buttons on screen that let you pause, resume, reset, and reload the ARSession.&lt;/p&gt; &#xA;&lt;p&gt;When a plane is detected, you can tap on the detected plane to place a cube on it. This uses the &lt;code&gt;ARRaycastManager&lt;/code&gt; to perform a raycast against the plane. If the plane is in &lt;code&gt;TrackingState.Limited&lt;/code&gt;, it will highlight red. In the case of ARCore, this means that raycasting will not be available until the plane is in &lt;code&gt;TrackingState.Tracking&lt;/code&gt; again.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Action&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pause&lt;/td&gt; &#xA;   &lt;td&gt;Pauses the ARSession, meaning device tracking and trackable detection (e.g., plane detection) is temporarily paused. While paused, the ARSession does not consume CPU resources.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Resume&lt;/td&gt; &#xA;   &lt;td&gt;Resumes a paused ARSession. The device will attempt to relocalize and previously detected objects may shift around as tracking is reestablished.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reset&lt;/td&gt; &#xA;   &lt;td&gt;Clears all detected trackables and effectively begins a new ARSession.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reload&lt;/td&gt; &#xA;   &lt;td&gt;Completely destroys the ARSession GameObject and re-instantiates it. This simulates the behavior you might experience during scene switching.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Check Support&lt;/h2&gt; &#xA;&lt;p&gt;Demonstrates checking for AR support and logs the results to the screen. The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/SupportChecker.cs&#34;&gt;&lt;code&gt;SupportChecker.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;LightEstimation&lt;/h2&gt; &#xA;&lt;h3&gt;BasicLightEstimation&lt;/h3&gt; &#xA;&lt;p&gt;Demonstrates basic light estimation information from the camera frame. You should see values for &#34;Ambient Intensity&#34; and &#34;Ambient Color&#34; on screen. The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/BasicLightEstimation.cs&#34;&gt;&lt;code&gt;BasicLightEstimation.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;h3&gt;HDRLightEstimation&lt;/h3&gt; &#xA;&lt;p&gt;This sample attempts to read HDR lighting information. You should see values for &#34;Ambient Intensity&#34;, &#34;Ambient Color&#34;, &#34;Main Light Direction&#34;, &#34;Main Light Intensity Lumens&#34;, &#34;Main Light Color&#34;, and &#34;Spherical Harmonics&#34;. Most devices only support a subset of these 6, so some will be listed as &#34;Unavailable.&#34; The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/HDRLightEstimation.cs&#34;&gt;&lt;code&gt;HDRLightEstimation.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;On iOS, this is only available when face tracking is enabled and requires a device that supports face tracking (such as an iPhone X, XS or 11). When available, a virtual arrow appears in front of the camera which indicates the estimated main light direction. The virtual light direction is also updated, so that virtual content appears to be lit from the direction of the real light source.&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;HDRLightEstimation&lt;/code&gt;, the sample will automatically pick the supported camera facing direction for you, for example &lt;code&gt;World&lt;/code&gt; on Android and &lt;code&gt;User&lt;/code&gt; on iOS, so it does not matter which facing direction you select in the &lt;code&gt;ARCameraManager&lt;/code&gt; component.&lt;/p&gt; &#xA;&lt;h2&gt;Anchors&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows how to create anchors as the result of a raycast hit. The &#34;Clear Anchors&#34; button removes all created anchors. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/AnchorCreator.cs&#34;&gt;&lt;code&gt;AnchorCreator.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This script can create two kinds of anchors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If a feature point is hit, it creates a normal anchor at the hit pose using the &lt;code&gt;GameObject.AddComponent&amp;lt;ARAnchor&amp;gt;()&lt;/code&gt; method.&lt;/li&gt; &#xA; &lt;li&gt;If a plane is hit, it creates an anchor &#34;attached&#34; to the plane using the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARFoundation.ARAnchorManager.html#UnityEngine_XR_ARFoundation_ARAnchorManager_AttachAnchor_UnityEngine_XR_ARFoundation_ARPlane_Pose_&#34;&gt;AttachAnchor&lt;/a&gt; method.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;CpuImages&lt;/h2&gt; &#xA;&lt;p&gt;This samples shows how to acquire and manipulate textures obtained from ARFoundation on the CPU. Most textures in ARFoundation (e.g., the pass-through video supplied by the &lt;code&gt;ARCameraManager&lt;/code&gt;, and the human depth and human stencil buffers provided by the &lt;code&gt;AROcclusionManager&lt;/code&gt;) are GPU textures. Computer vision or other CPU-based applications often require the pixel buffers on the CPU, which would normally involve an expensive GPU readback. ARFoundation provides an API for obtaining these textures on the CPU for further processing, without incurring the costly GPU readback.&lt;/p&gt; &#xA;&lt;p&gt;The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/CpuImageSample.cs&#34;&gt;&lt;code&gt;CpuImageSample.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The resolution of the camera image is affected by the camera&#39;s configuration. The current configuration is indicated at the bottom left of the screen inside a dropdown box which lets you select one of the supported camera configurations. The &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/CameraConfigController.cs&#34;&gt;&lt;code&gt;CameraConfigController.cs&lt;/code&gt;&lt;/a&gt; demonstrates enumerating and selecting a camera configuration. It is on the &lt;code&gt;CameraConfigs&lt;/code&gt; GameObject.&lt;/p&gt; &#xA;&lt;p&gt;Where available (currently iOS 13+ only), the human depth and human stencil textures are also available on the CPU. These appear inside two additional boxes underneath the camera&#39;s image.&lt;/p&gt; &#xA;&lt;h2&gt;TogglePlaneDetection&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows how to toggle plane detection on and off. When off, it will also hide all previously detected planes by disabling their GameObjects. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/PlaneDetectionController.cs&#34;&gt;&lt;code&gt;PlaneDetectionController.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PlaneClassification&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows how to query for a plane&#39;s classification. Some devices attempt to classify planes into categories such as &#34;door&#34;, &#34;seat&#34;, &#34;window&#34;, and &#34;floor&#34;. This scene enables plane detection using the &lt;code&gt;ARPlaneManager&lt;/code&gt;, and uses a prefab which includes a component which displays the plane&#39;s classification, or &#34;none&#34; if it cannot be classified.&lt;/p&gt; &#xA;&lt;h2&gt;FeatheredPlanes&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates basic plane detection, but uses a better looking prefab for the &lt;code&gt;ARPlane&lt;/code&gt;. Rather than being drawn as exactly defined, the plane fades out towards the edges.&lt;/p&gt; &#xA;&lt;h2&gt;PlaneOcclusion&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates basic plane detection, but uses an occlusion shader for the plane&#39;s material. This makes the plane appear invisible, but virtual objects behind the plane are culled. This provides an additional level of realism when, for example, placing objects on a table.&lt;/p&gt; &#xA;&lt;p&gt;Move the device around until a plane is detected (its edges are still drawn) and then tap on the plane to place/move content.&lt;/p&gt; &#xA;&lt;h2&gt;UX&lt;/h2&gt; &#xA;&lt;p&gt;A sample demonstrating UI that may be useful when guiding new users through an AR application is available in the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-demos#ux--also-available-on-the-asset-store-here&#34;&gt;ARFoundation Demos&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;EnvironmentProbes&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates environment probes, a feature which attempts to generate a 3D texture from the real environment and applies it to reflection probes in the scene. The scene includes several spheres which start out completely black, but will change to shiny spheres which reflect the real environment when possible.&lt;/p&gt; &#xA;&lt;h2&gt;ARWorldMap&lt;/h2&gt; &#xA;&lt;p&gt;An &lt;code&gt;ARWorldMap&lt;/code&gt; is an ARKit-specific feature which lets you save a scanned area. ARKit can optionally relocalize to a saved world map at a later time. This can be used to synchronize multiple devices to a common space, or for curated experiences specific to a location, such as a museum exhibition or other special installation. Read more about world maps &lt;a href=&#34;https://developer.apple.com/documentation/arkit/arworldmap&#34;&gt;here&lt;/a&gt;. A world map will store most types of trackables, such as reference points and planes.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARWorldMapController.cs&#34;&gt;&lt;code&gt;ARWorldMapController.cs&lt;/code&gt;&lt;/a&gt; performs most of the logic in this sample.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 12.&lt;/p&gt; &#xA;&lt;h2&gt;ARCollaborationData&lt;/h2&gt; &#xA;&lt;p&gt;Similar to an &lt;code&gt;ARWorldMap&lt;/code&gt;, a &#34;collaborative session&#34; is an ARKit-specific feature which allows multiple devices to share session information in real time. Each device will periodically produce &lt;code&gt;ARCollaborationData&lt;/code&gt; which should be sent to all other devices in the collaborative session. ARKit will share each participant&#39;s pose and all reference points. Other types of trackables, such as detected planes, are not shared.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARCollaborationData/CollaborativeSession.cs&#34;&gt;&lt;code&gt;CollaborativeSession.cs&lt;/code&gt;&lt;/a&gt;. Note there are two types of collaboration data: &#34;Critical&#34; and &#34;Optional&#34;. &#34;Critical&#34; data is available periodically and should be sent to all other devices reliably. &#34;Optional&#34; data is available nearly every frame and may be sent unreliably. Data marked as &#34;optional&#34; includes data about the device&#39;s location, which is why it is produced very frequently (i.e., every frame).&lt;/p&gt; &#xA;&lt;p&gt;Note that ARKit&#39;s support for collaborative sessions does not include any networking; it is up to the developer to manage the connection and send data to other participants in the collaborative session. For this sample, we used Apple&#39;s &lt;a href=&#34;https://developer.apple.com/documentation/multipeerconnectivity&#34;&gt;MultipeerConnectivity Framework&lt;/a&gt;. Our implementation can be found &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scripts/Multipeer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can create reference points by tapping on the screen. Reference points are created when the tap results in a raycast which hits a point in the point cloud.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;ARKitCoachingOverlay&lt;/h2&gt; &#xA;&lt;p&gt;The coaching overlay is an ARKit-specific feature which will overlay a helpful UI guiding the user to perform certain actions to achieve some &#34;goal&#34;, such as finding a horizontal plane.&lt;/p&gt; &#xA;&lt;p&gt;The coaching overlay can be activated automatically or manually, and you can set its goal. In this sample, we&#39;ve set the goal to be &#34;Any plane&#34;, and for it to activate automatically. This will display a special UI on the screen until a plane is found. There is also a button to activate it manually.&lt;/p&gt; &#xA;&lt;p&gt;The sample includes a MonoBehavior to define the settings of the coaching overlay. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARKitCoachingOverlay/ARKitCoachingOverlay.cs&#34;&gt;&lt;code&gt;ARKitCoachingOverlay.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample also shows how to subscribe to ARKit session callbacks. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARKitCoachingOverlay/CustomSessionDelegate.cs&#34;&gt;CustomSessionDelegate&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;ARKitGeoAnchors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developer.apple.com/documentation/arkit/argeoanchor?language=objc&#34;&gt;ARKit&#39;s ARGeoAnchors&lt;/a&gt; are not yet supported by ARFoundation, but you can still access this feature with a bit of Objective-C. This sample uses a custom &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARSubsystems.ConfigurationChooser.html&#34;&gt;ConfigurationChooser&lt;/a&gt; to instruct the Apple ARKit XR Plug-in to use an &lt;a href=&#34;https://developer.apple.com/documentation/arkit/argeotrackingconfiguration?language=objc&#34;&gt;ARGeoTrackingConfiguration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample also shows how to interpret the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARSubsystems.XRSessionSubsystem.html#UnityEngine_XR_ARSubsystems_XRSessionSubsystem_nativePtr&#34;&gt;nativePtr&lt;/a&gt; provided by the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARSubsystems.XRSessionSubsystem.html&#34;&gt;XRSessionSubsystem&lt;/a&gt; as an ARKit &lt;a href=&#34;https://developer.apple.com/documentation/arkit/arsession?language=objc&#34;&gt;ARSession&lt;/a&gt; pointer.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires an iOS device running iOS 14.0 or later, an A12 chip or later, location services enabled, and cellular capability.&lt;/p&gt; &#xA;&lt;h2&gt;ImageTracking&lt;/h2&gt; &#xA;&lt;p&gt;There are two samples demonstrating image tracking. The image tracking samples are supported on ARCore and ARKit. To enable image tracking, you must first create an &lt;code&gt;XRReferenceImageLibrary&lt;/code&gt;. This is the set of images to look for in the environment. &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/arsubsystems/image-tracking.html&#34;&gt;Click here&lt;/a&gt; for instructions on creating one.&lt;/p&gt; &#xA;&lt;p&gt;You can also add images to the reference image library at runtime. This sample includes a button that adds the images &lt;code&gt;one.png&lt;/code&gt; and &lt;code&gt;two.png&lt;/code&gt; to the reference image library. See the script &lt;code&gt;DynamicLibrary.cs&lt;/code&gt; for example code.&lt;/p&gt; &#xA;&lt;p&gt;Run the sample on an ARCore or ARKit-capable device and point your device at one of the images in &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scenes/ImageTracking/Images&#34;&gt;&lt;code&gt;Assets/Scenes/ImageTracking/Images&lt;/code&gt;&lt;/a&gt;. They can be displayed on a computer monitor; they do not need to be printed out.&lt;/p&gt; &#xA;&lt;h3&gt;BasicImageTracking&lt;/h3&gt; &#xA;&lt;p&gt;At runtime, ARFoundation will generate an &lt;code&gt;ARTrackedImage&lt;/code&gt; for each detected reference image. This sample uses the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/BasicImageTracking/TrackedImageInfoManager.cs&#34;&gt;&lt;code&gt;TrackedImageInfoManager.cs&lt;/code&gt;&lt;/a&gt; script to overlay the original image on top of the detected image, along with some meta data.&lt;/p&gt; &#xA;&lt;h3&gt;ImageTrackingWithMultiplePrefabs&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/ImageTrackingWithMultiplePrefabs/PrefabImagePairManager.cs&#34;&gt;&lt;code&gt;PrefabImagePairManager.cs&lt;/code&gt;&lt;/a&gt; script, you can assign different prefabs for each image in the reference image library.&lt;/p&gt; &#xA;&lt;p&gt;You can also change prefabs at runtime. This sample includes a button that switch between the original and alternative prefab for the first image in the reference image library. See the script &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/ImageTrackingWithMultiplePrefabs/DynamicPrefab.cs&#34;&gt;&lt;code&gt;DynamicPrefab.cs&lt;/code&gt;&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;h2&gt;ObjectTracking&lt;/h2&gt; &#xA;&lt;p&gt;Similar to the image tracking sample, this sample detects a 3D object from a set of reference objects in an &lt;code&gt;XRReferenceObjectLibrary&lt;/code&gt;. &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/arsubsystems/object-tracking.html&#34;&gt;Click here&lt;/a&gt; for instructions on creating one.&lt;/p&gt; &#xA;&lt;p&gt;To use this sample, you must have a physical object the device can recognize. The sample&#39;s reference object library is built using two reference objects. The sample includes &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scenes/Object%20Tracking/Printable%20Templates&#34;&gt;printable templates&lt;/a&gt; which can be printed on 8.5x11 inch paper and folded into a cube and cylinder.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can &lt;a href=&#34;https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects&#34;&gt;scan your own objects&lt;/a&gt; and add them to the reference object library.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 12 and is not supported on Android.&lt;/p&gt; &#xA;&lt;h2&gt;Face Tracking&lt;/h2&gt; &#xA;&lt;p&gt;There are several samples showing different face tracking features. Some are ARCore specific and some are ARKit specific.&lt;/p&gt; &#xA;&lt;h3&gt;FacePose&lt;/h3&gt; &#xA;&lt;p&gt;This is the simplest face tracking sample and simply draws an axis at the detected face&#39;s pose.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;FaceMesh&lt;/h3&gt; &#xA;&lt;p&gt;This sample instantiates and updates a mesh representing the detected face. Information about the device support (e.g., number of faces that can be simultaneously tracked) is displayed on the screen.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;ARKitFaceBlendShapes&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Blend shapes&#34; are an ARKit-specific feature which provides information about various facial features on a scale of 0..1. For instance, &#34;wink&#34; and &#34;frown&#34;. In this sample, blend shapes are used to puppet a cartoon face which is displayed over the detected face. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARKitBlendShapeVisualizer.cs&#34;&gt;&lt;code&gt;ARKitBlendShapeVisualizer.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;ARCoreFaceRegions&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Face regions&#34; are an ARCore-specific feature which provides pose information for specific &#34;regions&#34; on the detected face, e.g., left eyebrow. In this example, axes are drawn at each face region. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARCoreFaceRegionManager.cs&#34;&gt;&lt;code&gt;ARCoreFaceRegionManager.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;EyeLasers, EyePoses, FixationPoint&lt;/h3&gt; &#xA;&lt;p&gt;These samples demonstrate eye and fixation point tracking. Eye tracking produces a pose (position and rotation) for each eye in the detected face, and the &#34;fixation point&#34; is the point the face is looking at (i.e., fixated upon). &lt;code&gt;EyeLasers&lt;/code&gt; uses the eye pose to draw laser beams emitted from the detected face.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera and requires an iOS device with a TrueDepth camera.&lt;/p&gt; &#xA;&lt;h3&gt;WorldCameraWithUserFacingFaceTracking&lt;/h3&gt; &#xA;&lt;p&gt;iOS 13 adds support for face tracking while the world-facing (i.e., rear) camera is active. This means the user-facing (i.e., front) camera is used for face tracking, but the pass through video uses the world-facing camera. To enable this mode in ARFoundation, you must enable an &lt;code&gt;ARFaceManager&lt;/code&gt;, set the &lt;code&gt;ARSession&lt;/code&gt; tracking mode to &#34;Position and Rotation&#34; or &#34;Don&#39;t Care&#34;, and set the &lt;code&gt;ARCameraManager&lt;/code&gt;&#39;s facing direction to &#34;World&#34;. Tap the screen to toggle between the user-facing and world-facing cameras.&lt;/p&gt; &#xA;&lt;p&gt;The sample code in &lt;code&gt;DisplayFaceInfo.OnEnable&lt;/code&gt; shows how to detect support for these face tracking features.&lt;/p&gt; &#xA;&lt;p&gt;When using the world-facing camera, a cube is displayed in front of the camera whose orientation is driven by the face in front of the user-facing camera.&lt;/p&gt; &#xA;&lt;p&gt;This feature requires a device with a TrueDepth camera and an A12 bionic chip running iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;HumanBodyTracking2D&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates 2D screen space body tracking. A 2D skeleton is generated when a person is detected. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ScreenSpaceJointVisualizer.cs&#34;&gt;&lt;code&gt;ScreenSpaceJointVisualizer.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device with an A12 bionic chip running iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;HumanBodyTracking3D&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates 3D world space body tracking. A 3D skeleton is generated when a person is detected. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/HumanBodyTracker.cs&#34;&gt;&lt;code&gt;HumanBodyTracker.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device with an A12 bionic chip running iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;DepthImages&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates raw texture depth images from different methods.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Environment depth (certain Android devices and Apple devices with the LiDAR sensor)&lt;/li&gt; &#xA; &lt;li&gt;Human stencil (Apple devices with an A12 bionic chip (or later) running iOS 13 or later)&lt;/li&gt; &#xA; &lt;li&gt;Human depth (Apple devices with an A12 bionic chip (or later) running iOS 13 or later)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SimpleOcclusion&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates occlusion of virtual content by real world content through the use of environment depth images on supported Android and iOS devices.&lt;/p&gt; &#xA;&lt;h2&gt;AllPointCloudPoints&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows all feature points over time, not just the current frame&#39;s feature points as the &#34;AR Default Point Cloud&#34; prefab does. It does this by using a slightly modified version of the &lt;code&gt;ARPointCloudParticleVisualizer&lt;/code&gt; component that stores all the feature points in a Dictionary. Since each feature point has a unique identifier, it can look up the stored point and update its position in the dictionary if it already exists. This can be a useful starting point for custom solutions that require the entire map of point cloud points, e.g., for custom mesh reconstruction techniques.&lt;/p&gt; &#xA;&lt;p&gt;This sample has two UI components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A button in the lower left which allows you to switch between visualizing &#34;All&#34; the points and just those in the &#34;Current Frame&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Text in the upper right which displays the number of points in each point cloud (ARCore &amp;amp; ARKit will only ever have one).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CameraGrain&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates the camera grain effect. Once a plane is detected, you can place a cube on it with a material that simulates the camera grain noise in the camera feed. See the &lt;code&gt;CameraGrain.cs&lt;/code&gt; script. Also see &lt;code&gt;CameraGrain.shader&lt;/code&gt; which animates and applies the camera grain texture (through linear interpolation) in screenspace.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device running iOS 13 and Unity 2020.2 or later.&lt;/p&gt; &#xA;&lt;h2&gt;Meshing&lt;/h2&gt; &#xA;&lt;p&gt;These meshing scenes use features of some devices to construct meshes from scanned data of real world surfaces. These meshing scenes will not work on all devices.&lt;/p&gt; &#xA;&lt;p&gt;For ARKit, this functionality requires at least iPadOS 13.4 running on a device with a LiDAR scanner.&lt;/p&gt; &#xA;&lt;h3&gt;ClassificationMeshes&lt;/h3&gt; &#xA;&lt;p&gt;This scene demonstrates mesh classification functionality. With mesh classification enabled, each triangle in the mesh surface is identified as one of several surface types. This sample scene creates submeshes for each classification type and renders each mesh type with a different color.&lt;/p&gt; &#xA;&lt;p&gt;This scene only works on ARKit.&lt;/p&gt; &#xA;&lt;h3&gt;NormalMeshes&lt;/h3&gt; &#xA;&lt;p&gt;This scene renders an overlay on top of the real world scanned geometry illustrating the normal of the surface.&lt;/p&gt; &#xA;&lt;h3&gt;OcclusionMeshes&lt;/h3&gt; &#xA;&lt;p&gt;At first, this scene may appear to be doing nothing. However, it is rendering a depth texture on top of the scene based on the real world geometry. This allows for the real world to occlude virtual content. The scene has a script on it that fires a red ball into the scene when you tap. You will see the occlusion working by firing the red balls into a space which you can then move the iPad camera behind some other real world object to see that the virtual red balls are occluded by the real world object.&lt;/p&gt; &#xA;&lt;h2&gt;Interaction&lt;/h2&gt; &#xA;&lt;p&gt;This sample scene demonstrates the functionality of the &lt;code&gt;XR Interaction Toolkit&lt;/code&gt; package. In the scene, you are able to place a cube on a plane which you can translate, rotate and scale with gestures. See the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.0/manual/index.html&#34;&gt;&lt;code&gt;XR Interaction Toolkit Documentation&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Input System&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates a version of the SimpleAR scene using Unity&#39;s new Input System. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/InputSystem/ARController.inputactions&#34;&gt;&lt;code&gt;ARController.inputactions&lt;/code&gt;&lt;/a&gt; for an example of an action map. For a demonstration on how these action map bindings are used, see the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/InputSystem/InputSystem_PlaceOnPlane.cs&#34;&gt;&lt;code&gt;InputSystem_PlaceOnPlane.cs&lt;/code&gt;&lt;/a&gt; script. For more details, see the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.inputsystem@1.3/manual/index.html&#34;&gt;&lt;code&gt;Input System Documentation&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Thermal State&lt;/h2&gt; &#xA;&lt;p&gt;This sample contains the code required to query for an iOS device&#39;s thermal state so that the thermal state may be used with C# game code. This sample illustrates how the thermal state may be used to disable AR Foundation features to reduce the thermal state of the device.&lt;/p&gt; &#xA;&lt;h2&gt;ARCoreSessionRecording&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates the session recording and playback functionality available in ARCore. This feature allows you to record the sensor and camera telemetry during a live session, and then reply it at later time. When replayed, ARCore runs on the target device using the recorded telemetry rather than live data. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/ARCore/ARCoreSessionRecorder.cs&#34;&gt;ARCoreSessionRecorder.cs&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;h1&gt;Community and Feedback&lt;/h1&gt; &#xA;&lt;p&gt;The intention of this reposititory is to provide a means for getting started with the features in ARFoundation. The samples are intentionally simplistic with a focus on teaching basic scene setup and APIs. If you you have a question, find a bug, or would like to request a new feature concerning any of the ARFoundation packages or these samples please &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/issues&#34;&gt;submit a GitHub issue&lt;/a&gt;. New issues are reviewed regularly.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Pull Requests&lt;/h2&gt; &#xA;&lt;p&gt;We are not accepting pull requests at this time. If you find an issue with the samples, or would like to request a new sample, please &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/issues&#34;&gt;submit a GitHub issue&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>