<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-25T01:53:24Z</updated>
  <subtitle>Weekly Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Stealerium/Stealerium</title>
    <updated>2023-06-25T01:53:24Z</updated>
    <id>tag:github.com,2023-06-25:/Stealerium/Stealerium</id>
    <link href="https://github.com/Stealerium/Stealerium" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stealer + Clipper + Keylogger&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/73314940/227033966-765bde5a-438d-4b97-844b-f70c67ac6352.jpg&#34;&gt; &lt;br&gt; &lt;b&gt;Stealer + Clipper + Keylogger&lt;/b&gt; &lt;br&gt; &lt;i&gt;Stealer written on C#, logs will be sent to your Discord channel using a webhook.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Stealerium/Stealerium/actions/workflows/dotnet.yml&#34;&gt;&lt;img src=&#34;https://github.com/Stealerium/Stealerium/actions/workflows/dotnet.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Stealerium&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;ðŸš§&lt;/span&gt; Disclaimer&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;This program is for educational purposes only.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;How you use this program is your responsibility.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;I will not be held accountable for any illegal activities.&lt;/p&gt; &#xA;&lt;h1&gt;ðŸ”± Data extraction:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AntiAnalysis (VirtualBox, SandBox, Debugger, VirusTotal, Any.Run)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Get system info (Version, CPU, GPU, RAM, IPs, BSSID, Location, Screen metrics, Installed apps)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Chromium based browsers (passwords, credit cards, cookies, history, autofill, bookmarks)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Firefox based browsers (db files, cookies, history, bookmarks)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Internet explorer/Edge (passwords)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Saved wifi networks &amp;amp; scan networks around device (SSID, BSSID)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; File grabber (Documents, Images, Source codes, Databases, USB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Detect banking &amp;amp; cryptocurrency services in browsers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Steam, Uplay, Battle.Net, Minecraft session&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Install keylogger &amp;amp; clipper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Desktop &amp;amp; Webcam screenshot&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ProtonVPN, OpenVPN, NordVPN&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Crypto Wallets &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Zcash, Armory, Bytecoin, Jaxx, Exodus, Ethereum, Electrum, AtomicWallet, Guarda, Coinomi, Litecoin, Dash, Bitcoin&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Crypto Wallet Extensions from Chrome &amp;amp; Edge &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Binance, coin98, Phantom, Mobox, XinPay, Math10, Metamask, BitApp, Guildwallet, iconx, Sollet, Slope Wallet, Starcoin, Swash, Finnie, KEPLR, Crocobit, OXYGEN, Nifty, Liquality, Auvitas wallet, Math wallet, MTV wallet, Rabet wallet, Ronin wallet, Yoroi wallet, ZilPay wallet, Exodus, Terra Station, Jaxx.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Messenger Sessions, Accounts, Tokens &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Discord, Telegram, ICQ, Skype, Pidgin, Outlook, Tox, Element, Signal&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Directories structure&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Filezilla hosts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Process list&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Product key&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Autorun module&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; Features:&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;These functions will become available in the builder only if you have autorun enabled.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ðŸŽ¹&lt;/span&gt; Keylogger:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The keylogger will turn on if the user is texting in the chat or using the bank&#39;s website.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ðŸ“‹&lt;/span&gt; Clipper:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Clipper turns on and replaces crypto wallet addresses in the clipboard when a user makes a transaction.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ðŸ“·&lt;/span&gt; Webcam screenshots:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Webcam screenshots will be taken if the user is watching something obscene on the Internet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;ðŸ”¨&lt;/span&gt; Builder:&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/73314940/165985151-6f74dd66-c9d8-4063-a3e2-fe80d4a4f34a.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;ðŸ“¢&lt;/span&gt; Channel Webhook:&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/73314940/165986700-8109a5ab-a1e1-4e50-8e91-90e72eb41af1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Requirements:&lt;/h1&gt; &#xA;&lt;p&gt;If you want to build from the source these are the requirements.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio 2022 (v17.*)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dotnet.microsoft.com/en-us/download/dotnet/6.0&#34;&gt;NET SDK 6.0.*&lt;/a&gt; (included in Visual Studio 2022)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dotnet.microsoft.com/en-us/download/dotnet-framework/net48&#34;&gt;NET Framework SDK 4.8&lt;/a&gt; (included in Visual Studio 2022)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Runtime requirements.&lt;/h1&gt; &#xA;&lt;p&gt;Only needed if u download the release from &lt;a href=&#34;https://github.com/Stealerium/Stealerium/releases&#34;&gt;Releases&lt;/a&gt; (stealerium.zip)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Builder.exe (&lt;a href=&#34;https://dotnet.microsoft.com/en-us/download/dotnet/6.0&#34;&gt;NET Runtime 6.0.*&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Stub (&lt;a href=&#34;https://dotnet.microsoft.com/en-us/download/dotnet-framework/net48&#34;&gt;NET Framework 4.8&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Tichau/FileConverter</title>
    <updated>2023-06-25T01:53:24Z</updated>
    <id>tag:github.com,2023-06-25:/Tichau/FileConverter</id>
    <link href="https://github.com/Tichau/FileConverter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;File Converter is a very simple tool which allows you to convert and compress one or several file(s) using the context menu in windows explorer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;File Converter&lt;/h1&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;File Converter&lt;/strong&gt; is a very simple tool which allows you to convert and compress one or several file(s) using the context menu of windows explorer.&lt;/p&gt; &#xA;&lt;p&gt;You can download it here: &lt;a href=&#34;https://file-converter.org/?from=readme.md&#34;&gt;www.file-converter.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can find more information about what&#39;s in File converter and how to use it on the &lt;a href=&#34;https://github.com/Tichau/FileConverter/wiki&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Donate&lt;/h2&gt; &#xA;&lt;p&gt;File Converter is a personal open source project started in 2014. I have put hundreds of hours adding, refining and tuning File Converter with the goal of making the conversion and compression of files an easy task for everyone.&lt;/p&gt; &#xA;&lt;p&gt;You can help me by &lt;a href=&#34;https://github.com/Tichau/FileConverter/wiki#contribute&#34;&gt;contributing to the project&lt;/a&gt;, by &lt;a href=&#34;https://www.paypal.com/donate/?cmd=_donations&amp;amp;business=3BDWQTYTTA3D8&amp;amp;item_name=File+Converter+Donations&amp;amp;currency_code=EUR&amp;amp;Z3JncnB0=&#34;&gt;making a donation&lt;/a&gt; or just by &lt;a href=&#34;https://saythanks.io/to/Tichau&#34;&gt;saying thanksâ€‹&lt;/a&gt; :).&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter any problem with File Converter, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the already known problems in the &lt;a href=&#34;https://github.com/Tichau/FileConverter/wiki/Troubleshooting&#34;&gt;troubleshooting section of the documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Or report an issue on the &lt;a href=&#34;https://github.com/Tichau/FileConverter/issues&#34;&gt;bug tracker&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you report an issue, please join the following informations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Registry.xml&lt;/li&gt; &#xA; &lt;li&gt;Settings.user.xml&lt;/li&gt; &#xA; &lt;li&gt;The Diagnostics folder of the session that encountered the issue.&lt;/li&gt; &#xA; &lt;li&gt;A screenshot (if possible) and a description that shows/explain the issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will find the xml files and diagnostics folder in &lt;code&gt;c:\Users\[UserName]\AppData\Local\FileConverter\&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup development environement&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;For File Converter and its explorer extension:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visual Studio 2017&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For the installer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wixtoolset.org/&#34;&gt;Wix toolset build tool v3.11 and visual studio extension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.microsoft.com/fr-fr/windows/downloads/windows-10-sdk&#34;&gt;Windows SDK Signing Tools for Desktop Apps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to all the contributors of File Converter project.&lt;/p&gt; &#xA;&lt;h3&gt;Localization&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to &lt;strong&gt;Khidreal&lt;/strong&gt; for the Portuguese localization. Thanks to &lt;strong&gt;Marhc&lt;/strong&gt; for the Brazilian localization. Thanks to &lt;strong&gt;Chachak&lt;/strong&gt; for the Spanish localization. Thanks to &lt;strong&gt;Davide&lt;/strong&gt; for the Italian localization. Thanks to &lt;strong&gt;nikotschierske&lt;/strong&gt; for the German localization. Thanks to &lt;strong&gt;Snoopy1866&lt;/strong&gt; for the Simplified Chinese localization. Thanks to &lt;strong&gt;MayaC0re&lt;/strong&gt; for the Turkish localization. Thanks to &lt;strong&gt;vishveshjain&lt;/strong&gt; for the Hindi localization.&lt;/p&gt; &#xA;&lt;h2&gt;Middlewares&lt;/h2&gt; &#xA;&lt;p&gt;File converter uses the following middlewares:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; as file conversion software. Thanks to ffmpeg devs for this awesome open source file conversion tool. &lt;a href=&#34;https://ffmpeg.org&#34;&gt;Web site link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageMagick&lt;/strong&gt; as image edition and conversion software. Thanks to image magick devs for this awesome open source image edition software suite. &lt;a href=&#34;http://imagemagick.net&#34;&gt;Web site link&lt;/a&gt; And thanks to dlemstra for the C# wrapper of this software. &lt;a href=&#34;https://github.com/ImageMagick/ImageMagick&#34;&gt;Github link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ghostscript&lt;/strong&gt; as pdf edition software. Thanks to ghostscript devs. &lt;a href=&#34;https://www.ghostscript.com/download/gsdnld.html&#34;&gt;Download link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SharpShell&lt;/strong&gt; to easily create windows context menu extensions. Thanks to Dave Kerr for his work on SharpShell. &lt;a href=&#34;https://sharpshell.codeplex.com&#34;&gt;CodePlex link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ripper&lt;/strong&gt; and &lt;strong&gt;yeti.mmedia&lt;/strong&gt; for CD Audio extraction. Thanks to Idael Cardoso for his work on CD Audio ripper. &lt;a href=&#34;https://www.codeproject.com/Articles/5458/C-Sharp-Ripper&#34;&gt;Code project link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Markdown.XAML&lt;/strong&gt; for markdown rendering in the wpf application. Thanks to Bevan Arps for his work on Markdown.XAML. &lt;a href=&#34;https://github.com/theunrepentantgeek/Markdown.XAML&#34;&gt;GitHub link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WpfAnimatedGif&lt;/strong&gt; for animated gif rendering in the wpf application. Thanks to Thomas Levesque for his work on WpfAnimatedGif. &lt;a href=&#34;https://github.com/XamlAnimatedGif/WpfAnimatedGif&#34;&gt;GitHub link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;File Converter is licensed under the GPL version 3 License. For more information check the LICENSE.md file in your installation folder or the &lt;a href=&#34;https://www.gnu.org/licenses/gpl.html&#34;&gt;gnu website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Unity-Technologies/arfoundation-samples</title>
    <updated>2023-06-25T01:53:24Z</updated>
    <id>tag:github.com,2023-06-25:/Unity-Technologies/arfoundation-samples</id>
    <link href="https://github.com/Unity-Technologies/arfoundation-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Example content for Unity projects based on AR Foundation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AR Foundation Samples&lt;/h1&gt; &#xA;&lt;p&gt;Example AR scenes that use &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/manual/index.html&#34;&gt;AR Foundation 5.1&lt;/a&gt; and demonstrate its features. Each feature is used in a minimal sample scene with example code that you can modify or copy into your project.&lt;/p&gt; &#xA;&lt;p&gt;This sample project depends on four Unity packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/manual/index.html&#34;&gt;AR Foundation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arcore@5.1/manual/index.html&#34;&gt;Google ARCore XR Plug-in&lt;/a&gt; on Android&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arkit@5.1/manual/index.html&#34;&gt;Apple ARKit XR Plug-in&lt;/a&gt; on iOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.openxr@1.5/manual/index.html&#34;&gt;OpenXR Plug-in&lt;/a&gt; on HoloLens 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Which version should I use?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;main&lt;/code&gt; branch of this repository uses AR Foundation 5.1 and is compatible with Unity 2021.2 and later. To access sample scenes for previous versions of AR Foundation, refer to the table below for links to other branches.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Unity Version&lt;/th&gt; &#xA;   &lt;th&gt;AR Foundation Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023.2 (alpha)&lt;/td&gt; &#xA;   &lt;td&gt;5.1 (prerelease)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023.1 (beta)&lt;/td&gt; &#xA;   &lt;td&gt;5.1 (prerelease)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/5.0&#34;&gt;5.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/4.2&#34;&gt;4.2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/4.1&#34;&gt;4.1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to use these samples&lt;/h2&gt; &#xA;&lt;h3&gt;Build and run on device&lt;/h3&gt; &#xA;&lt;p&gt;You can build the AR Foundation Samples project directly to device, which can be a helpful introduction to using AR Foundation features for the first time.&lt;/p&gt; &#xA;&lt;p&gt;To build to device, follow the steps below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Unity 2021.2 or later and clone this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the Unity project at the root of this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;As with any other Unity project, go to &lt;a href=&#34;https://docs.unity3d.com/Manual/BuildSettings.html&#34;&gt;Build Settings&lt;/a&gt;, select your target platform, and build this project.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Understand the sample code&lt;/h3&gt; &#xA;&lt;p&gt;All sample scenes in this project can be found in the &lt;code&gt;Assets/Scenes&lt;/code&gt; folder. To learn more about the AR Foundation components used in each scene, see the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/manual/index.html&#34;&gt;AR Foundation Documentation&lt;/a&gt;. Each scene is explained in more detail below.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Sample scene(s)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#simple-ar&#34;&gt;Simple AR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates basic Plane detection and Raycasting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#camera&#34;&gt;Camera&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Camera features&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#plane-detection&#34;&gt;Plane detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Plane detection&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#image-tracking&#34;&gt;Image tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Image tracking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#object-tracking&#34;&gt;Object tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates Object tracking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#face-tracking&#34;&gt;Face tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Face tracking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#body-tracking&#34;&gt;Body tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Body tracking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#point-clouds&#34;&gt;Point clouds&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates Point clouds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#anchors&#34;&gt;Anchors&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates Anchors&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#meshing&#34;&gt;Meshing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Meshing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#environment-probes&#34;&gt;Environment Probes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates Environment Probes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#occlusion&#34;&gt;Occlusion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scenes that demonstrate Occlusion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#check-support&#34;&gt;Check support&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates checking for AR support on device&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#interaction&#34;&gt;Interation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates AR Foundation paired with the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@latest&#34;&gt;XR Interaction Toolkit&lt;/a&gt; package&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#configuration-chooser&#34;&gt;Configuration Chooser&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates AR Foundation&#39;s Configuration Chooser&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#debug-menu&#34;&gt;Debug Menu&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Visualize trackables and configurations on device&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#arkit&#34;&gt;ARKit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;iOS-specific sample scenes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Unity-Technologies/arfoundation-samples/main/#arcore-record-session&#34;&gt;ARCore Record Session&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Demonstrates ARCore&#39;s session recording feature&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Simple AR&lt;/h2&gt; &#xA;&lt;p&gt;This is a good starting sample that enables point cloud visualization and plane detection. There are buttons on screen that let you pause, resume, reset, and reload the ARSession.&lt;/p&gt; &#xA;&lt;p&gt;When a plane is detected, you can tap on the detected plane to place a cube on it. This uses the &lt;code&gt;ARRaycastManager&lt;/code&gt; to perform a raycast against the plane. If the plane is in &lt;code&gt;TrackingState.Limited&lt;/code&gt;, it will highlight red. In the case of ARCore, this means that raycasting will not be available until the plane is in &lt;code&gt;TrackingState.Tracking&lt;/code&gt; again.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Action&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pause&lt;/td&gt; &#xA;   &lt;td&gt;Pauses the ARSession, meaning device tracking and trackable detection (e.g., plane detection) is temporarily paused. While paused, the ARSession does not consume CPU resources.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Resume&lt;/td&gt; &#xA;   &lt;td&gt;Resumes a paused ARSession. The device will attempt to relocalize and previously detected objects may shift around as tracking is reestablished.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reset&lt;/td&gt; &#xA;   &lt;td&gt;Clears all detected trackables and effectively begins a new ARSession.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reload&lt;/td&gt; &#xA;   &lt;td&gt;Completely destroys the ARSession GameObject and re-instantiates it. This simulates the behavior you might experience during scene switching.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Camera&lt;/h2&gt; &#xA;&lt;h3&gt;CPU Images&lt;/h3&gt; &#xA;&lt;p&gt;This samples shows how to acquire and manipulate textures obtained from AR Foundation on the CPU. Most textures in ARFoundation (e.g., the pass-through video supplied by the &lt;code&gt;ARCameraManager&lt;/code&gt;, and the human depth and human stencil buffers provided by the &lt;code&gt;AROcclusionManager&lt;/code&gt;) are GPU textures. Computer vision or other CPU-based applications often require the pixel buffers on the CPU, which would normally involve an expensive GPU readback. AR Foundation provides an API for obtaining these textures on the CPU for further processing, without incurring the costly GPU readback.&lt;/p&gt; &#xA;&lt;p&gt;The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/CpuImageSample.cs&#34;&gt;&lt;code&gt;CpuImageSample.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The resolution of the camera image is affected by the camera&#39;s configuration. The current configuration is indicated at the bottom left of the screen inside a dropdown box which lets you select one of the supported camera configurations. The &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/CameraConfigController.cs&#34;&gt;&lt;code&gt;CameraConfigController.cs&lt;/code&gt;&lt;/a&gt; demonstrates enumerating and selecting a camera configuration. It is on the &lt;code&gt;CameraConfigs&lt;/code&gt; GameObject.&lt;/p&gt; &#xA;&lt;p&gt;Where available (currently iOS 13+ only), the human depth and human stencil textures are also available on the CPU. These appear inside two additional boxes underneath the camera&#39;s image.&lt;/p&gt; &#xA;&lt;h3&gt;Basic Light Estimation&lt;/h3&gt; &#xA;&lt;p&gt;Demonstrates basic light estimation information from the camera frame. You should see values for &#34;Ambient Intensity&#34; and &#34;Ambient Color&#34; on screen. The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/BasicLightEstimation.cs&#34;&gt;&lt;code&gt;BasicLightEstimation.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;h3&gt;HDR Light Estimation&lt;/h3&gt; &#xA;&lt;p&gt;This sample attempts to read HDR lighting information. You should see values for &#34;Ambient Intensity&#34;, &#34;Ambient Color&#34;, &#34;Main Light Direction&#34;, &#34;Main Light Intensity Lumens&#34;, &#34;Main Light Color&#34;, and &#34;Spherical Harmonics&#34;. Most devices only support a subset of these 6, so some will be listed as &#34;Unavailable.&#34; The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/HDRLightEstimation.cs&#34;&gt;&lt;code&gt;HDRLightEstimation.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;On iOS, this is only available when face tracking is enabled and requires a device that supports face tracking (such as an iPhone X, XS or 11). When available, a virtual arrow appears in front of the camera which indicates the estimated main light direction. The virtual light direction is also updated, so that virtual content appears to be lit from the direction of the real light source.&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;HDRLightEstimation&lt;/code&gt;, the sample will automatically pick the supported camera facing direction for you, for example &lt;code&gt;World&lt;/code&gt; on Android and &lt;code&gt;User&lt;/code&gt; on iOS, so it does not matter which facing direction you select in the &lt;code&gt;ARCameraManager&lt;/code&gt; component.&lt;/p&gt; &#xA;&lt;h3&gt;Background Rendering Order&lt;/h3&gt; &#xA;&lt;p&gt;Produces a visual example of how changing the background rendering between &lt;code&gt;BeforeOpaqueGeometry&lt;/code&gt; and &lt;code&gt;AfterOpaqueGeometry&lt;/code&gt; would effect a rudimentary AR application. Leverages Occlusion where available to display &lt;code&gt;AfterOpaqueGeometry&lt;/code&gt; support for AR Occlusion.&lt;/p&gt; &#xA;&lt;h3&gt;Camera Grain (ARKit)&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates the camera grain effect. Once a plane is detected, you can place a cube on it with a material that simulates the camera grain noise in the camera feed. See the &lt;code&gt;CameraGrain.cs&lt;/code&gt; script. Also see &lt;code&gt;CameraGrain.shader&lt;/code&gt; which animates and applies the camera grain texture (through linear interpolation) in screenspace.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device running iOS 13 or later and Unity 2020.2 or later.&lt;/p&gt; &#xA;&lt;h3&gt;EXIF Data&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates how to access camera frame&#39;s EXIF metadata. You should see values for all the supported EXIF tags on screen. Refer to &lt;code&gt;ExifDataLogger.cs&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 16 or newer.&lt;/p&gt; &#xA;&lt;h2&gt;Plane Detection&lt;/h2&gt; &#xA;&lt;h3&gt;Feathered Planes&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates basic plane detection, but uses a better looking prefab for the &lt;code&gt;ARPlane&lt;/code&gt;. Rather than being drawn as exactly defined, the plane fades out towards the edges.&lt;/p&gt; &#xA;&lt;h3&gt;Toggle Plane Detection&lt;/h3&gt; &#xA;&lt;p&gt;This sample shows how to toggle plane detection on and off. When off, it will also hide all previously detected planes by disabling their GameObjects. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/PlaneDetectionController.cs&#34;&gt;&lt;code&gt;PlaneDetectionController.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Plane Classification&lt;/h3&gt; &#xA;&lt;p&gt;This sample shows how to query for a plane&#39;s classification. Some devices attempt to classify planes into categories such as &#34;door&#34;, &#34;seat&#34;, &#34;window&#34;, and &#34;floor&#34;. This scene enables plane detection using the &lt;code&gt;ARPlaneManager&lt;/code&gt;, and uses a prefab which includes a component which displays the plane&#39;s classification, or &#34;none&#34; if it cannot be classified.&lt;/p&gt; &#xA;&lt;h3&gt;Plane Masking&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates basic plane detection, but uses an occlusion shader for the plane&#39;s material. This makes the plane appear invisible, but virtual objects behind the plane are culled. This provides an additional level of realism when, for example, placing objects on a table.&lt;/p&gt; &#xA;&lt;p&gt;Move the device around until a plane is detected (its edges are still drawn) and then tap on the plane to place/move content.&lt;/p&gt; &#xA;&lt;h2&gt;Image Tracking&lt;/h2&gt; &#xA;&lt;p&gt;There are two samples demonstrating image tracking. The image tracking samples are supported on ARCore and ARKit. To enable image tracking, you must first create an &lt;code&gt;XRReferenceImageLibrary&lt;/code&gt;. This is the set of images to look for in the environment. &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/manual/features/image-tracking.html&#34;&gt;Click here&lt;/a&gt; for instructions on creating one.&lt;/p&gt; &#xA;&lt;p&gt;You can also add images to the reference image library at runtime. This sample includes a button that adds the images &lt;code&gt;one.png&lt;/code&gt; and &lt;code&gt;two.png&lt;/code&gt; to the reference image library. See the script &lt;code&gt;DynamicLibrary.cs&lt;/code&gt; for example code.&lt;/p&gt; &#xA;&lt;p&gt;Run the sample on an ARCore or ARKit-capable device and point your device at one of the images in &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scenes/ImageTracking/Images&#34;&gt;&lt;code&gt;Assets/Scenes/ImageTracking/Images&lt;/code&gt;&lt;/a&gt;. They can be displayed on a computer monitor; they do not need to be printed out.&lt;/p&gt; &#xA;&lt;h3&gt;Basic Image Tracking&lt;/h3&gt; &#xA;&lt;p&gt;At runtime, ARFoundation will generate an &lt;code&gt;ARTrackedImage&lt;/code&gt; for each detected reference image. This sample uses the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/BasicImageTracking/TrackedImageInfoManager.cs&#34;&gt;&lt;code&gt;TrackedImageInfoManager.cs&lt;/code&gt;&lt;/a&gt; script to overlay the original image on top of the detected image, along with some meta data.&lt;/p&gt; &#xA;&lt;h3&gt;Image Tracking With Multiple Prefabs&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/ImageTrackingWithMultiplePrefabs/PrefabImagePairManager.cs&#34;&gt;&lt;code&gt;PrefabImagePairManager.cs&lt;/code&gt;&lt;/a&gt; script, you can assign different prefabs for each image in the reference image library.&lt;/p&gt; &#xA;&lt;p&gt;You can also change prefabs at runtime. This sample includes a button that switch between the original and alternative prefab for the first image in the reference image library. See the script &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ImageTracking/ImageTrackingWithMultiplePrefabs/DynamicPrefab.cs&#34;&gt;&lt;code&gt;DynamicPrefab.cs&lt;/code&gt;&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;h2&gt;Object Tracking&lt;/h2&gt; &#xA;&lt;p&gt;Similar to the image tracking sample, this sample detects a 3D object from a set of reference objects in an &lt;code&gt;XRReferenceObjectLibrary&lt;/code&gt;. &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/manual/features/object-tracking.html&#34;&gt;Click here&lt;/a&gt; for instructions on creating one.&lt;/p&gt; &#xA;&lt;p&gt;To use this sample, you must have a physical object the device can recognize. The sample&#39;s reference object library is built using two reference objects. The sample includes &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scenes/Object%20Tracking/Printable%20Templates&#34;&gt;printable templates&lt;/a&gt; which can be printed on 8.5x11 inch paper and folded into a cube and cylinder.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can &lt;a href=&#34;https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects&#34;&gt;scan your own objects&lt;/a&gt; and add them to the reference object library.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 12 or above.&lt;/p&gt; &#xA;&lt;h2&gt;Face Tracking&lt;/h2&gt; &#xA;&lt;p&gt;There are several samples showing different face tracking features. Some are ARCore specific and some are ARKit specific.&lt;/p&gt; &#xA;&lt;h3&gt;Face Pose&lt;/h3&gt; &#xA;&lt;p&gt;This is the simplest face tracking sample and simply draws an axis at the detected face&#39;s pose.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;Face Mesh&lt;/h3&gt; &#xA;&lt;p&gt;This sample instantiates and updates a mesh representing the detected face. Information about the device support (e.g., number of faces that can be simultaneously tracked) is displayed on the screen.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;Face Regions (ARCore)&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Face regions&#34; are an ARCore-specific feature which provides pose information for specific &#34;regions&#34; on the detected face, e.g., left eyebrow. In this example, axes are drawn at each face region. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARCoreFaceRegionManager.cs&#34;&gt;&lt;code&gt;ARCoreFaceRegionManager.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;Blend Shapes (ARKit)&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Blend shapes&#34; are an ARKit-specific feature which provides information about various facial features on a scale of 0..1. For instance, &#34;wink&#34; and &#34;frown&#34;. In this sample, blend shapes are used to puppet a cartoon face which is displayed over the detected face. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARKitBlendShapeVisualizer.cs&#34;&gt;&lt;code&gt;ARKitBlendShapeVisualizer.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera.&lt;/p&gt; &#xA;&lt;h3&gt;Eye Lasers, Eye Poses, and Fixation Point (ARKit)&lt;/h3&gt; &#xA;&lt;p&gt;These samples demonstrate eye and fixation point tracking. Eye tracking produces a pose (position and rotation) for each eye in the detected face, and the &#34;fixation point&#34; is the point the face is looking at (i.e., fixated upon). &lt;code&gt;EyeLasers&lt;/code&gt; uses the eye pose to draw laser beams emitted from the detected face.&lt;/p&gt; &#xA;&lt;p&gt;This sample uses the front-facing (i.e., selfie) camera and requires an iOS device with a TrueDepth camera.&lt;/p&gt; &#xA;&lt;h3&gt;Rear Camera (ARKit)&lt;/h3&gt; &#xA;&lt;p&gt;iOS 13 adds support for face tracking while the world-facing (i.e., rear) camera is active. This means the user-facing (i.e., front) camera is used for face tracking, but the pass through video uses the world-facing camera. To enable this mode in ARFoundation, you must enable an &lt;code&gt;ARFaceManager&lt;/code&gt;, set the &lt;code&gt;ARSession&lt;/code&gt; tracking mode to &#34;Position and Rotation&#34; or &#34;Don&#39;t Care&#34;, and set the &lt;code&gt;ARCameraManager&lt;/code&gt;&#39;s facing direction to &#34;World&#34;. Tap the screen to toggle between the user-facing and world-facing cameras.&lt;/p&gt; &#xA;&lt;p&gt;The sample code in &lt;code&gt;DisplayFaceInfo.OnEnable&lt;/code&gt; shows how to detect support for these face tracking features.&lt;/p&gt; &#xA;&lt;p&gt;When using the world-facing camera, a cube is displayed in front of the camera whose orientation is driven by the face in front of the user-facing camera.&lt;/p&gt; &#xA;&lt;p&gt;This feature requires a device with a TrueDepth camera and an A12 bionic chip running iOS 13.&lt;/p&gt; &#xA;&lt;h2&gt;Body Tracking&lt;/h2&gt; &#xA;&lt;h3&gt;Body Tracking 2D&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates 2D screen space body tracking. A 2D skeleton is generated when a person is detected. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ScreenSpaceJointVisualizer.cs&#34;&gt;&lt;code&gt;ScreenSpaceJointVisualizer.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device with an A12 bionic chip running iOS 13 or above.&lt;/p&gt; &#xA;&lt;h3&gt;Body Tracking 3D&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates 3D world space body tracking. A 3D skeleton is generated when a person is detected. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/HumanBodyTracker.cs&#34;&gt;&lt;code&gt;HumanBodyTracker.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires a device with an A12 bionic chip running iOS 13 or above.&lt;/p&gt; &#xA;&lt;h2&gt;Point Clouds&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows all feature points over time, not just the current frame&#39;s feature points as the &#34;AR Default Point Cloud&#34; prefab does. It does this by using a slightly modified version of the &lt;code&gt;ARPointCloudParticleVisualizer&lt;/code&gt; component that stores all the feature points in a Dictionary. Since each feature point has a unique identifier, it can look up the stored point and update its position in the dictionary if it already exists. This can be a useful starting point for custom solutions that require the entire map of point cloud points, e.g., for custom mesh reconstruction techniques.&lt;/p&gt; &#xA;&lt;p&gt;This sample has two UI components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A button in the lower left which allows you to switch between visualizing &#34;All&#34; the points and just those in the &#34;Current Frame&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Text in the upper right which displays the number of points in each point cloud (ARCore &amp;amp; ARKit will only ever have one).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Anchors&lt;/h2&gt; &#xA;&lt;p&gt;This sample shows how to create anchors as the result of a raycast hit. The &#34;Clear Anchors&#34; button removes all created anchors. See the &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/AnchorCreator.cs&#34;&gt;&lt;code&gt;AnchorCreator.cs&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;This script can create two kinds of anchors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If a feature point is hit, it creates a normal anchor at the hit pose using the &lt;code&gt;GameObject.AddComponent&amp;lt;ARAnchor&amp;gt;()&lt;/code&gt; method.&lt;/li&gt; &#xA; &lt;li&gt;If a plane is hit, it creates an anchor &#34;attached&#34; to the plane using the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/api/UnityEngine.XR.ARFoundation.ARAnchorManager.html#UnityEngine_XR_ARFoundation_ARAnchorManager_AttachAnchor_UnityEngine_XR_ARFoundation_ARPlane_Pose_&#34;&gt;AttachAnchor&lt;/a&gt; method.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Meshing&lt;/h2&gt; &#xA;&lt;p&gt;These meshing scenes use features of some devices to construct meshes from scanned data of real world surfaces. These meshing scenes will not work on all devices.&lt;/p&gt; &#xA;&lt;p&gt;For ARKit, this functionality requires at least iPadOS 13.4 running on a device with a LiDAR scanner.&lt;/p&gt; &#xA;&lt;h3&gt;Classification Meshes&lt;/h3&gt; &#xA;&lt;p&gt;This scene demonstrates mesh classification functionality. With mesh classification enabled, each triangle in the mesh surface is identified as one of several surface types. This sample scene creates submeshes for each classification type and renders each mesh type with a different color.&lt;/p&gt; &#xA;&lt;p&gt;This scene only works on ARKit.&lt;/p&gt; &#xA;&lt;h3&gt;Normal Meshes&lt;/h3&gt; &#xA;&lt;p&gt;This scene renders an overlay on top of the real world scanned geometry illustrating the normal of the surface.&lt;/p&gt; &#xA;&lt;h3&gt;Occlusion Meshes&lt;/h3&gt; &#xA;&lt;p&gt;At first, this scene may appear to be doing nothing. However, it is rendering a depth texture on top of the scene based on the real world geometry. This allows for the real world to occlude virtual content. The scene has a script on it that fires a red ball into the scene when you tap. You will see the occlusion working by firing the red balls into a space which you can then move the iPad camera behind some other real world object to see that the virtual red balls are occluded by the real world object.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Probes&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates environment probes, a feature which attempts to generate a 3D texture from the real environment and applies it to reflection probes in the scene. The scene includes several spheres which start out completely black, but will change to shiny spheres which reflect the real environment when possible.&lt;/p&gt; &#xA;&lt;h2&gt;Occlusion&lt;/h2&gt; &#xA;&lt;h3&gt;SimpleOcclusion&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates occlusion of virtual content by real world content through the use of environment depth images on supported Android and iOS devices.&lt;/p&gt; &#xA;&lt;h3&gt;Depth Images&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates raw texture depth images from different methods.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Environment depth (certain Android devices and Apple devices with the LiDAR sensor)&lt;/li&gt; &#xA; &lt;li&gt;Human stencil (Apple devices with an A12 bionic chip (or later) running iOS 13 or later)&lt;/li&gt; &#xA; &lt;li&gt;Human depth (Apple devices with an A12 bionic chip (or later) running iOS 13 or later)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Check Support&lt;/h2&gt; &#xA;&lt;p&gt;Demonstrates checking for AR support and logs the results to the screen. The relevant script is &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/SupportChecker.cs&#34;&gt;&lt;code&gt;SupportChecker.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interaction&lt;/h2&gt; &#xA;&lt;p&gt;This sample scene demonstrates the functionality of the &lt;code&gt;XR Interaction Toolkit&lt;/code&gt; package. In the scene, you are able to place a cube on a plane which you can translate, rotate and scale with gestures. See the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@latest&#34;&gt;&lt;code&gt;XR Interaction Toolkit Documentation&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration Chooser&lt;/h2&gt; &#xA;&lt;p&gt;Demonstrates how to use the AR Foundation session&#39;s ConfigurationChooser to swap between rear and front-facing camera configurations.&lt;/p&gt; &#xA;&lt;h2&gt;Debug Menu&lt;/h2&gt; &#xA;&lt;p&gt;The AR Foundation Debug Menu allows you to visualize trackables and configurations on device.&lt;/p&gt; &#xA;&lt;h2&gt;ARKit&lt;/h2&gt; &#xA;&lt;p&gt;These samples are only available on iOS devices.&lt;/p&gt; &#xA;&lt;h3&gt;Coaching Overlay&lt;/h3&gt; &#xA;&lt;p&gt;The coaching overlay is an ARKit-specific feature which will overlay a helpful UI guiding the user to perform certain actions to achieve some &#34;goal&#34;, such as finding a horizontal plane.&lt;/p&gt; &#xA;&lt;p&gt;The coaching overlay can be activated automatically or manually, and you can set its goal. In this sample, we&#39;ve set the goal to be &#34;Any plane&#34;, and for it to activate automatically. This will display a special UI on the screen until a plane is found. There is also a button to activate it manually.&lt;/p&gt; &#xA;&lt;p&gt;The sample includes a MonoBehavior to define the settings of the coaching overlay. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARKitCoachingOverlay/ARKitCoachingOverlay.cs&#34;&gt;&lt;code&gt;ARKitCoachingOverlay.cs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample also shows how to subscribe to ARKit session callbacks. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARKitCoachingOverlay/CustomSessionDelegate.cs&#34;&gt;CustomSessionDelegate&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 13 or above.&lt;/p&gt; &#xA;&lt;h3&gt;Thermal State&lt;/h3&gt; &#xA;&lt;p&gt;This sample contains the code required to query for an iOS device&#39;s thermal state so that the thermal state may be used with C# game code. This sample illustrates how the thermal state may be used to disable AR Foundation features to reduce the thermal state of the device.&lt;/p&gt; &#xA;&lt;h3&gt;AR World Map&lt;/h3&gt; &#xA;&lt;p&gt;An &lt;code&gt;ARWorldMap&lt;/code&gt; is an ARKit-specific feature which lets you save a scanned area. ARKit can optionally relocalize to a saved world map at a later time. This can be used to synchronize multiple devices to a common space, or for curated experiences specific to a location, such as a museum exhibition or other special installation. Read more about world maps &lt;a href=&#34;https://developer.apple.com/documentation/arkit/arworldmap&#34;&gt;here&lt;/a&gt;. A world map will store most types of trackables, such as reference points and planes.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scripts/ARWorldMapController.cs&#34;&gt;&lt;code&gt;ARWorldMapController.cs&lt;/code&gt;&lt;/a&gt; performs most of the logic in this sample.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 12 or above.&lt;/p&gt; &#xA;&lt;h3&gt;Geo Anchors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developer.apple.com/documentation/arkit/argeoanchor?language=objc&#34;&gt;ARKit&#39;s ARGeoAnchors&lt;/a&gt; are not yet supported by ARFoundation, but you can still access this feature with a bit of Objective-C. This sample uses a custom &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.1/api/UnityEngine.XR.ARSubsystems.ConfigurationChooser.html&#34;&gt;ConfigurationChooser&lt;/a&gt; to instruct the Apple ARKit XR Plug-in to use an &lt;a href=&#34;https://developer.apple.com/documentation/arkit/argeotrackingconfiguration?language=objc&#34;&gt;ARGeoTrackingConfiguration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This sample also shows how to interpret the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARSubsystems.XRSessionSubsystem.html#UnityEngine_XR_ARSubsystems_XRSessionSubsystem_nativePtr&#34;&gt;nativePtr&lt;/a&gt; provided by the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARSubsystems.XRSessionSubsystem.html&#34;&gt;XRSessionSubsystem&lt;/a&gt; as an ARKit &lt;a href=&#34;https://developer.apple.com/documentation/arkit/arsession?language=objc&#34;&gt;ARSession&lt;/a&gt; pointer.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires an iOS device running iOS 14.0 or later, an A12 chip or later, location services enabled, and cellular capability.&lt;/p&gt; &#xA;&lt;h3&gt;AR Collaboration Data&lt;/h3&gt; &#xA;&lt;p&gt;Similar to an &lt;code&gt;ARWorldMap&lt;/code&gt;, a &#34;collaborative session&#34; is an ARKit-specific feature which allows multiple devices to share session information in real time. Each device will periodically produce &lt;code&gt;ARCollaborationData&lt;/code&gt; which should be sent to all other devices in the collaborative session. ARKit will share each participant&#39;s pose and all reference points. Other types of trackables, such as detected planes, are not shared.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/master/Assets/Scenes/ARKit/ARCollaborationData/CollaborativeSession.cs&#34;&gt;&lt;code&gt;CollaborativeSession.cs&lt;/code&gt;&lt;/a&gt;. Note there are two types of collaboration data: &#34;Critical&#34; and &#34;Optional&#34;. &#34;Critical&#34; data is available periodically and should be sent to all other devices reliably. &#34;Optional&#34; data is available nearly every frame and may be sent unreliably. Data marked as &#34;optional&#34; includes data about the device&#39;s location, which is why it is produced very frequently (i.e., every frame).&lt;/p&gt; &#xA;&lt;p&gt;Note that ARKit&#39;s support for collaborative sessions does not include any networking; it is up to the developer to manage the connection and send data to other participants in the collaborative session. For this sample, we used Apple&#39;s &lt;a href=&#34;https://developer.apple.com/documentation/multipeerconnectivity&#34;&gt;MultipeerConnectivity Framework&lt;/a&gt;. Our implementation can be found &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/tree/master/Assets/Scripts/Multipeer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can create reference points by tapping on the screen. Reference points are created when the tap results in a raycast which hits a point in the point cloud.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 13 or above.&lt;/p&gt; &#xA;&lt;h3&gt;High Resolution CPU Image&lt;/h3&gt; &#xA;&lt;p&gt;This sample demonstrates high resolution CPU image capture on iOS 16 and newer. See the &lt;a href=&#34;https://docs.unity3d.com/Packages/com.unity.xr.arkit@5.1/manual/arkit-camera.html#high-resolution-cpu-image&#34;&gt;High Resolution CPU Image&lt;/a&gt; package documentation to learn more about this feature.&lt;/p&gt; &#xA;&lt;h3&gt;Camera Exposure&lt;/h3&gt; &#xA;&lt;p&gt;This sample shows how to lock the device camera and set the camera exposure mode, duration, and ISO. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/ARKit/CameraExposure/CameraExposureController.cs&#34;&gt;CameraExposureController.cs&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 16 or newer and a device with an ultra-wide camera.&lt;/p&gt; &#xA;&lt;h3&gt;Camera White Balance&lt;/h3&gt; &#xA;&lt;p&gt;This sample shows how to lock the device camera and set the camera white balance mode and gains. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/ARKit/CameraWhiteBalance/CameraWhiteBalanceController.cs&#34;&gt;CameraWhiteBalanceController.cs&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 16 or newer and a device with an ultra-wide camera.&lt;/p&gt; &#xA;&lt;h3&gt;Camera Focus&lt;/h3&gt; &#xA;&lt;p&gt;This sample shows how to lock the device camera and set the camera focus mode and lens position. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/ARKit/CameraFocus/CameraFocusController.cs&#34;&gt;CameraFocusController.cs&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;p&gt;This sample requires iOS 16 or newer and a device with an ultra-wide camera.&lt;/p&gt; &#xA;&lt;h2&gt;ARCore Record Session&lt;/h2&gt; &#xA;&lt;p&gt;This sample demonstrates the session recording and playback functionality available in ARCore. This feature allows you to record the sensor and camera telemetry during a live session, and then reply it at later time. When replayed, ARCore runs on the target device using the recorded telemetry rather than live data. See &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/raw/main/Assets/Scenes/ARCore/ARCoreSessionRecorder.cs&#34;&gt;ARCoreSessionRecorder.cs&lt;/a&gt; for example code.&lt;/p&gt; &#xA;&lt;h2&gt;Additional demos&lt;/h2&gt; &#xA;&lt;p&gt;While no longer actively maintained, Unity has a separate &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-demos&#34;&gt;AR Foundation Demos&lt;/a&gt; repository that contains some larger samples including localization, mesh placement, shadows, and user onboarding UX.&lt;/p&gt; &#xA;&lt;h1&gt;Community and Feedback&lt;/h1&gt; &#xA;&lt;p&gt;The intention of this reposititory is to provide a means for getting started with the features in AR Foundation. The samples are intentionally simplistic with a focus on teaching basic scene setup and APIs. If you you have a question, find a bug, or would like to request a new feature concerning any of the AR Foundation packages or these samples please &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/issues&#34;&gt;submit a GitHub issue&lt;/a&gt;. New issues are reviewed regularly.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Pull Requests&lt;/h2&gt; &#xA;&lt;p&gt;We are not accepting pull requests at this time. If you find an issue with the samples, or would like to request a new sample, please &lt;a href=&#34;https://github.com/Unity-Technologies/arfoundation-samples/issues&#34;&gt;submit a GitHub issue&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>