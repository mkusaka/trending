<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C# Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-11T01:36:10Z</updated>
  <subtitle>Weekly Trending of C# in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/openai-dotnet</title>
    <updated>2024-08-11T01:36:10Z</updated>
    <id>tag:github.com,2024-08-11:/openai/openai-dotnet</id>
    <link href="https://github.com/openai/openai-dotnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official .NET library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI .NET API library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nuget.org/packages/OpenAI/absoluteLatest&#34;&gt;&lt;img src=&#34;https://img.shields.io/nuget/vpre/openai.svg?sanitize=true&#34; alt=&#34;NuGet version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The OpenAI .NET library provides convenient access to the OpenAI REST API from .NET applications.&lt;/p&gt; &#xA;&lt;p&gt;It is generated from our &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAPI specification&lt;/a&gt; in collaboration with Microsoft.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#getting-started&#34;&gt;Getting started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#install-the-nuget-package&#34;&gt;Install the NuGet package&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#using-the-client-library&#34;&gt;Using the client library&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#namespace-organization&#34;&gt;Namespace organization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#using-the-async-api&#34;&gt;Using the async API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#using-the-openaiclient-class&#34;&gt;Using the &lt;code&gt;OpenAIClient&lt;/code&gt; class&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-use-chat-completions-with-streaming&#34;&gt;How to use chat completions with streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-use-chat-completions-with-tools-and-function-calling&#34;&gt;How to use chat completions with tools and function calling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-generate-text-embeddings&#34;&gt;How to generate text embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-generate-images&#34;&gt;How to generate images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-transcribe-audio&#34;&gt;How to transcribe audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-use-assistants-with-retrieval-augmented-generation-rag&#34;&gt;How to use assistants with retrieval augmented generation (RAG)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-use-streaming-and-gpt-4o-vision-with-assistants&#34;&gt;How to use streaming and GPT-4o vision with assistants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#how-to-work-with-azure-openai&#34;&gt;How to work with Azure OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#advanced-scenarios&#34;&gt;Advanced scenarios&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#using-protocol-methods&#34;&gt;Using protocol methods&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#automatically-retrying-errors&#34;&gt;Automatically retrying errors&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/#observability&#34;&gt;Observability&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;To call the OpenAI REST API, you will need an API key. To obtain one, first &lt;a href=&#34;https://platform.openai.com/signup&#34;&gt;create a new OpenAI account&lt;/a&gt; or &lt;a href=&#34;https://platform.openai.com/login&#34;&gt;log in&lt;/a&gt;. Next, navigate to the &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;API key page&lt;/a&gt; and select &#34;Create new secret key&#34;, optionally naming the key. Make sure to save your API key somewhere safe and do not share it with anyone.&lt;/p&gt; &#xA;&lt;h3&gt;Install the NuGet package&lt;/h3&gt; &#xA;&lt;p&gt;Add the client library to your .NET project with &lt;a href=&#34;https://www.nuget.org/&#34;&gt;NuGet&lt;/a&gt; using your IDE or the dotnet CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cli&#34;&gt;dotnet add package OpenAI --prerelease&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the code examples included below were written using &lt;a href=&#34;https://dotnet.microsoft.com/download/dotnet/8.0&#34;&gt;.NET 8&lt;/a&gt;. The OpenAI .NET library is compatible with all .NET Standard 2.0 applications but some code examples in this document may depend on newer language features.&lt;/p&gt; &#xA;&lt;h2&gt;Using the client library&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in the &lt;a href=&#34;https://github.com/openai/openai-dotnet/raw/main/api/api.md&#34;&gt;api.md&lt;/a&gt; file, and there are many &lt;a href=&#34;https://github.com/openai/openai-dotnet/tree/main/examples&#34;&gt;code examples&lt;/a&gt; to help. For instance, the following snippet illustrates the basic use of the chat completions API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI.Chat;&#xA;&#xA;ChatClient client = new(model: &#34;gpt-4o&#34;, Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&#xA;ChatCompletion completion = client.CompleteChat(&#34;Say &#39;this is a test.&#39;&#34;);&#xA;&#xA;Console.WriteLine($&#34;[ASSISTANT]: {completion}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While you can pass your API key directly as a string, it is highly recommended to keep it in a secure location and instead access it via an environment variable or configuration file as shown above to avoid storing it in source control.&lt;/p&gt; &#xA;&lt;h3&gt;Namespace organization&lt;/h3&gt; &#xA;&lt;p&gt;The library is organized into several namespaces corresponding to OpenAI feature areas. Each namespace contains a corresponding client class.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Namespace&lt;/th&gt; &#xA;   &lt;th&gt;Client class&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Assistants&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AssistantClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[Experimental]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Audio&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AudioClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Batch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;BatchClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ChatClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Embeddings&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;EmbeddingClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.FineTuning&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;FineTuningClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Files&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;FileClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Images&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ImageClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Models&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ModelClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.Moderations&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ModerationClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OpenAI.VectorStores&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VectorStoreClient&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[Experimental]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Using the async API&lt;/h3&gt; &#xA;&lt;p&gt;Every client method that performs a synchronous API call has an asynchronous variant in the same client class. For instance, the asynchronous variant of the &lt;code&gt;ChatClient&lt;/code&gt;&#39;s &lt;code&gt;CompleteChat&lt;/code&gt; method is &lt;code&gt;CompleteChatAsync&lt;/code&gt;. To rewrite the call above using the asynchronous counterpart, simply &lt;code&gt;await&lt;/code&gt; the call to the corresponding async variant:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;ChatCompletion completion = await client.CompleteChatAsync(&#34;Say &#39;this is a test.&#39;&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the &lt;code&gt;OpenAIClient&lt;/code&gt; class&lt;/h3&gt; &#xA;&lt;p&gt;In addition to the namespaces mentioned above, there is also the parent &lt;code&gt;OpenAI&lt;/code&gt; namespace itself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This namespace contains the &lt;code&gt;OpenAIClient&lt;/code&gt; class, which offers certain conveniences when you need to work with multiple feature area clients. Specifically, you can use an instance of this class to create instances of the other clients and have them share the same implementation details, which might be more efficient.&lt;/p&gt; &#xA;&lt;p&gt;You can create an &lt;code&gt;OpenAIClient&lt;/code&gt; by specifying the API key that all clients will use for authentication:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;OpenAIClient client = new(Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, to create an instance of an &lt;code&gt;AudioClient&lt;/code&gt;, for example, you can call the &lt;code&gt;OpenAIClient&lt;/code&gt;&#39;s &lt;code&gt;GetAudioClient&lt;/code&gt; method by passing the OpenAI model that the &lt;code&gt;AudioClient&lt;/code&gt; will use, just as if you were using the &lt;code&gt;AudioClient&lt;/code&gt; constructor directly. If necessary, you can create additional clients of the same type to target different models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;AudioClient ttsClient = client.GetAudioClient(&#34;tts-1&#34;);&#xA;AudioClient whisperClient = client.GetAudioClient(&#34;whisper-1&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use chat completions with streaming&lt;/h2&gt; &#xA;&lt;p&gt;When you request a chat completion, the default behavior is for the server to generate it in its entirety before sending it back in a single response. Consequently, long chat completions can require waiting for several seconds before hearing back from the server. To mitigate this, the OpenAI REST API supports the ability to stream partial results back as they are being generated, allowing you to start processing the beginning of the completion before it is finished.&lt;/p&gt; &#xA;&lt;p&gt;The client library offers a convenient approach to working with streaming chat completions. If you wanted to re-write the example from the previous section using streaming, rather than calling the &lt;code&gt;ChatClient&lt;/code&gt;&#39;s &lt;code&gt;CompleteChat&lt;/code&gt; method, you would call its &lt;code&gt;CompleteChatStreaming&lt;/code&gt; method instead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;CollectionResult&amp;lt;StreamingChatCompletionUpdate&amp;gt; updates&#xA;    = client.CompleteChatStreaming(&#34;Say &#39;this is a test.&#39;&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that the returned value is a &lt;code&gt;CollectionResult&amp;lt;StreamingChatCompletionUpdate&amp;gt;&lt;/code&gt; instance, which can be enumerated to process the streaming response chunks as they arrive:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;Console.WriteLine($&#34;[ASSISTANT]:&#34;);&#xA;foreach (StreamingChatCompletionUpdate update in updates)&#xA;{&#xA;    foreach (ChatMessageContentPart updatePart in update.ContentUpdate)&#xA;    {&#xA;        Console.Write(updatePart);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can do this asynchronously by calling the &lt;code&gt;CompleteChatStreamingAsync&lt;/code&gt; method to get an &lt;code&gt;AsyncCollectionResult&amp;lt;StreamingChatCompletionUpdate&amp;gt;&lt;/code&gt; and enumerate it using &lt;code&gt;await foreach&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;AsyncCollectionResult&amp;lt;StreamingChatCompletionUpdate&amp;gt; updates&#xA;    = client.CompleteChatStreamingAsync(&#34;Say &#39;this is a test.&#39;&#34;);&#xA;&#xA;Console.WriteLine($&#34;[ASSISTANT]:&#34;);&#xA;await foreach (StreamingChatCompletionUpdate update in updates)&#xA;{&#xA;    foreach (ChatMessageContentPart updatePart in update.ContentUpdate)&#xA;    {&#xA;        Console.Write(updatePart.Text);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use chat completions with tools and function calling&lt;/h2&gt; &#xA;&lt;p&gt;In this example, you have two functions. The first function can retrieve a user&#39;s current geographic location (e.g., by polling the location service APIs of the user&#39;s device), while the second function can query the weather in a given location (e.g., by making an API call to some third-party weather service). You want chat completions to be able to call these functions if the model deems it necessary to have this information in order to respond to a user request. For illustrative purposes, consider the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;private static string GetCurrentLocation()&#xA;{&#xA;    // Call the location API here.&#xA;    return &#34;San Francisco&#34;;&#xA;}&#xA;&#xA;private static string GetCurrentWeather(string location, string unit = &#34;celsius&#34;)&#xA;{&#xA;    // Call the weather API here.&#xA;    return $&#34;31 {unit}&#34;;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start by creating two &lt;code&gt;ChatTool&lt;/code&gt; instances using the static &lt;code&gt;CreateFunctionTool&lt;/code&gt; method to describe each function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;private static readonly ChatTool getCurrentLocationTool = ChatTool.CreateFunctionTool(&#xA;    functionName: nameof(GetCurrentLocation),&#xA;    functionDescription: &#34;Get the user&#39;s current location&#34;&#xA;);&#xA;&#xA;private static readonly ChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(&#xA;    functionName: nameof(GetCurrentWeather),&#xA;    functionDescription: &#34;Get the current weather in a given location&#34;,&#xA;    functionParameters: BinaryData.FromString(&#34;&#34;&#34;&#xA;        {&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: {&#xA;                &#34;location&#34;: {&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;description&#34;: &#34;The city and state, e.g. Boston, MA&#34;&#xA;                },&#xA;                &#34;unit&#34;: {&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;enum&#34;: [ &#34;celsius&#34;, &#34;fahrenheit&#34; ],&#xA;                    &#34;description&#34;: &#34;The temperature unit to use. Infer this from the specified location.&#34;&#xA;                }&#xA;            },&#xA;            &#34;required&#34;: [ &#34;location&#34; ]&#xA;        }&#xA;        &#34;&#34;&#34;)&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create a &lt;code&gt;ChatCompletionOptions&lt;/code&gt; instance and add both to its &lt;code&gt;Tools&lt;/code&gt; property. You will pass the &lt;code&gt;ChatCompletionOptions&lt;/code&gt; as an argument in your calls to the &lt;code&gt;ChatClient&lt;/code&gt;&#39;s &lt;code&gt;CompleteChat&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;List&amp;lt;ChatMessage&amp;gt; messages = [&#xA;    new UserChatMessage(&#34;What&#39;s the weather like today?&#34;),&#xA;];&#xA;&#xA;ChatCompletionOptions options = new()&#xA;{&#xA;    Tools = { getCurrentLocationTool, getCurrentWeatherTool },&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the resulting &lt;code&gt;ChatCompletion&lt;/code&gt; has a &lt;code&gt;FinishReason&lt;/code&gt; property equal to &lt;code&gt;ChatFinishReason.ToolCalls&lt;/code&gt;, it means that the model has determined that one or more tools must be called before the assistant can respond appropriately. In those cases, you must first call the function specified in the &lt;code&gt;ChatCompletion&lt;/code&gt;&#39;s &lt;code&gt;ToolCalls&lt;/code&gt; and then call the &lt;code&gt;ChatClient&lt;/code&gt;&#39;s &lt;code&gt;CompleteChat&lt;/code&gt; method again while passing the function&#39;s result as an additional &lt;code&gt;ChatRequestToolMessage&lt;/code&gt;. Repeat this process as needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;bool requiresAction;&#xA;&#xA;do&#xA;{&#xA;    requiresAction = false;&#xA;    ChatCompletion chatCompletion = client.CompleteChat(messages, options);&#xA;&#xA;    switch (chatCompletion.FinishReason)&#xA;    {&#xA;        case ChatFinishReason.Stop:&#xA;            {&#xA;                // Add the assistant message to the conversation history.&#xA;                messages.Add(new AssistantChatMessage(chatCompletion));&#xA;                break;&#xA;            }&#xA;&#xA;        case ChatFinishReason.ToolCalls:&#xA;            {&#xA;                // First, add the assistant message with tool calls to the conversation history.&#xA;                messages.Add(new AssistantChatMessage(chatCompletion));&#xA;&#xA;                // Then, add a new tool message for each tool call that is resolved.&#xA;                foreach (ChatToolCall toolCall in chatCompletion.ToolCalls)&#xA;                {&#xA;                    switch (toolCall.FunctionName)&#xA;                    {&#xA;                        case nameof(GetCurrentLocation):&#xA;                            {&#xA;                                string toolResult = GetCurrentLocation();&#xA;                                messages.Add(new ToolChatMessage(toolCall.Id, toolResult));&#xA;                                break;&#xA;                            }&#xA;&#xA;                        case nameof(GetCurrentWeather):&#xA;                            {&#xA;                                // The arguments that the model wants to use to call the function are specified as a&#xA;                                // stringified JSON object based on the schema defined in the tool definition. Note that&#xA;                                // the model may hallucinate arguments too. Consequently, it is important to do the&#xA;                                // appropriate parsing and validation before calling the function.&#xA;                                using JsonDocument argumentsJson = JsonDocument.Parse(toolCall.FunctionArguments);&#xA;                                bool hasLocation = argumentsJson.RootElement.TryGetProperty(&#34;location&#34;, out JsonElement location);&#xA;                                bool hasUnit = argumentsJson.RootElement.TryGetProperty(&#34;unit&#34;, out JsonElement unit);&#xA;&#xA;                                if (!hasLocation)&#xA;                                {&#xA;                                    throw new ArgumentNullException(nameof(location), &#34;The location argument is required.&#34;);&#xA;                                }&#xA;&#xA;                                string toolResult = hasUnit&#xA;                                    ? GetCurrentWeather(location.GetString(), unit.GetString())&#xA;                                    : GetCurrentWeather(location.GetString());&#xA;                                messages.Add(new ToolChatMessage(toolCall.Id, toolResult));&#xA;                                break;&#xA;                            }&#xA;&#xA;                        default:&#xA;                            {&#xA;                                // Handle other unexpected calls.&#xA;                                throw new NotImplementedException();&#xA;                            }&#xA;                    }&#xA;                }&#xA;&#xA;                requiresAction = true;&#xA;                break;&#xA;            }&#xA;&#xA;        case ChatFinishReason.Length:&#xA;            throw new NotImplementedException(&#34;Incomplete model output due to MaxTokens parameter or token limit exceeded.&#34;);&#xA;&#xA;        case ChatFinishReason.ContentFilter:&#xA;            throw new NotImplementedException(&#34;Omitted content due to a content filter flag.&#34;);&#xA;&#xA;        case ChatFinishReason.FunctionCall:&#xA;            throw new NotImplementedException(&#34;Deprecated in favor of tool calls.&#34;);&#xA;&#xA;        default:&#xA;            throw new NotImplementedException(chatCompletion.FinishReason.ToString());&#xA;    }&#xA;} while (requiresAction);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to generate text embeddings&lt;/h2&gt; &#xA;&lt;p&gt;In this example, you want to create a trip-planning website that allows customers to write a prompt describing the kind of hotel that they are looking for and then offers hotel recommendations that closely match this description. To achieve this, it is possible to use text embeddings to measure the relatedness of text strings. In summary, you can get embeddings of the hotel descriptions, store them in a vector database, and use them to build a search index that you can query using the embedding of a given customer&#39;s prompt.&lt;/p&gt; &#xA;&lt;p&gt;To generate a text embedding, use &lt;code&gt;EmbeddingClient&lt;/code&gt; from the &lt;code&gt;OpenAI.Embeddings&lt;/code&gt; namespace:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI.Embeddings;&#xA;&#xA;EmbeddingClient client = new(model: &#34;text-embedding-3-small&#34;, Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&#xA;string description = &#34;Best hotel in town if you like luxury hotels. They have an amazing infinity pool, a spa,&#34;&#xA;    + &#34; and a really helpful concierge. The location is perfect -- right downtown, close to all the tourist&#34;&#xA;    + &#34; attractions. We highly recommend this hotel.&#34;;&#xA;&#xA;Embedding embedding = client.GenerateEmbedding(description);&#xA;ReadOnlyMemory&amp;lt;float&amp;gt; vector = embedding.Vector;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that the resulting embedding is a list (also called a vector) of floating point numbers represented as an instance of &lt;code&gt;ReadOnlyMemory&amp;lt;float&amp;gt;&lt;/code&gt;. By default, the length of the embedding vector will be 1536 when using the &lt;code&gt;text-embedding-3-small&lt;/code&gt; model or 3072 when using the &lt;code&gt;text-embedding-3-large&lt;/code&gt; model. Generally, larger embeddings perform better, but using them also tends to cost more in terms of compute, memory, and storage. You can reduce the dimensions of the embedding by creating an instance of the &lt;code&gt;EmbeddingGenerationOptions&lt;/code&gt; class, setting the &lt;code&gt;Dimensions&lt;/code&gt; property, and passing it as an argument in your call to the &lt;code&gt;GenerateEmbedding&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;EmbeddingGenerationOptions options = new() { Dimensions = 512 };&#xA;&#xA;Embedding embedding = client.GenerateEmbedding(description, options);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to generate images&lt;/h2&gt; &#xA;&lt;p&gt;In this example, you want to build an app to help interior designers prototype new ideas based on the latest design trends. As part of the creative process, an interior designer can use this app to generate images for inspiration simply by describing the scene in their head as a prompt. As expected, high-quality, strikingly dramatic images with finer details deliver the best results for this application.&lt;/p&gt; &#xA;&lt;p&gt;To generate an image, use &lt;code&gt;ImageClient&lt;/code&gt; from the &lt;code&gt;OpenAI.Images&lt;/code&gt; namespace:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI.Images;&#xA;&#xA;ImageClient client = new(model: &#34;dall-e-3&#34;, Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generating an image always requires a &lt;code&gt;prompt&lt;/code&gt; that describes what should be generated. To further tailor the image generation to your specific needs, you can create an instance of the &lt;code&gt;ImageGenerationOptions&lt;/code&gt; class and set the &lt;code&gt;Quality&lt;/code&gt;, &lt;code&gt;Size&lt;/code&gt;, and &lt;code&gt;Style&lt;/code&gt; properties accordingly. Note that you can also set the &lt;code&gt;ResponseFormat&lt;/code&gt; property of &lt;code&gt;ImageGenerationOptions&lt;/code&gt; to &lt;code&gt;GeneratedImageFormat.Bytes&lt;/code&gt; in order to receive the resulting PNG as &lt;code&gt;BinaryData&lt;/code&gt; (instead of the default remote &lt;code&gt;Uri&lt;/code&gt;) if this is convenient for your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;string prompt = &#34;The concept for a living room that blends Scandinavian simplicity with Japanese minimalism for&#34;&#xA;    + &#34; a serene and cozy atmosphere. It&#39;s a space that invites relaxation and mindfulness, with natural light&#34;&#xA;    + &#34; and fresh air. Using neutral tones, including colors like white, beige, gray, and black, that create a&#34;&#xA;    + &#34; sense of harmony. Featuring sleek wood furniture with clean lines and subtle curves to add warmth and&#34;&#xA;    + &#34; elegance. Plants and flowers in ceramic pots adding color and life to a space. They can serve as focal&#34;&#xA;    + &#34; points, creating a connection with nature. Soft textiles and cushions in organic fabrics adding comfort&#34;&#xA;    + &#34; and softness to a space. They can serve as accents, adding contrast and texture.&#34;;&#xA;&#xA;ImageGenerationOptions options = new()&#xA;{&#xA;    Quality = GeneratedImageQuality.High,&#xA;    Size = GeneratedImageSize.W1792xH1024,&#xA;    Style = GeneratedImageStyle.Vivid,&#xA;    ResponseFormat = GeneratedImageFormat.Bytes&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, call the &lt;code&gt;ImageClient&lt;/code&gt;&#39;s &lt;code&gt;GenerateImage&lt;/code&gt; method by passing the prompt and the &lt;code&gt;ImageGenerationOptions&lt;/code&gt; instance as arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;GeneratedImage image = client.GenerateImage(prompt, options);&#xA;BinaryData bytes = image.ImageBytes;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For illustrative purposes, you could then save the generated image to local storage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using FileStream stream = File.OpenWrite($&#34;{Guid.NewGuid()}.png&#34;);&#xA;bytes.ToStream().CopyTo(stream);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to transcribe audio&lt;/h2&gt; &#xA;&lt;p&gt;In this example, an audio file is transcribed using the Whisper speech-to-text model, including both word- and audio-segment-level timestamp information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI.Audio;&#xA;&#xA;AudioClient client = new(model: &#34;whisper-1&#34;, Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&#xA;string audioFilePath = Path.Combine(&#34;Assets&#34;, &#34;audio_houseplant_care.mp3&#34;);&#xA;&#xA;AudioTranscriptionOptions options = new()&#xA;{&#xA;    ResponseFormat = AudioTranscriptionFormat.Verbose,&#xA;    Granularities = AudioTimestampGranularities.Word | AudioTimestampGranularities.Segment,&#xA;};&#xA;&#xA;AudioTranscription transcription = client.TranscribeAudio(audioFilePath, options);&#xA;&#xA;Console.WriteLine(&#34;Transcription:&#34;);&#xA;Console.WriteLine($&#34;{transcription.Text}&#34;);&#xA;&#xA;Console.WriteLine();&#xA;Console.WriteLine($&#34;Words:&#34;);&#xA;foreach (TranscribedWord word in transcription.Words)&#xA;{&#xA;    Console.WriteLine($&#34;  {word.Word,15} : {word.Start.TotalMilliseconds,5:0} - {word.End.TotalMilliseconds,5:0}&#34;);&#xA;}&#xA;&#xA;Console.WriteLine();&#xA;Console.WriteLine($&#34;Segments:&#34;);&#xA;foreach (TranscribedSegment segment in transcription.Segments)&#xA;{&#xA;    Console.WriteLine($&#34;  {segment.Text,90} : {segment.Start.TotalMilliseconds,5:0} - {segment.End.TotalMilliseconds,5:0}&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use assistants with retrieval augmented generation (RAG)&lt;/h2&gt; &#xA;&lt;p&gt;In this example, you have a JSON document with the monthly sales information of different products, and you want to build an assistant capable of analyzing it and answering questions about it.&lt;/p&gt; &#xA;&lt;p&gt;To achieve this, use both &lt;code&gt;FileClient&lt;/code&gt; from the &lt;code&gt;OpenAI.Files&lt;/code&gt; namespace and &lt;code&gt;AssistantClient&lt;/code&gt; from the &lt;code&gt;OpenAI.Assistants&lt;/code&gt; namespace.&lt;/p&gt; &#xA;&lt;p&gt;Important: The Assistants REST API is currently in beta. As such, the details are subject to change, and correspondingly the &lt;code&gt;AssistantClient&lt;/code&gt; is attributed as &lt;code&gt;[Experimental]&lt;/code&gt;. To use it, suppress the &lt;code&gt;OPENAI001&lt;/code&gt; warning at either the project level or, as below, in the code itself.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using OpenAI.Assistants;&#xA;using OpenAI.Files;&#xA;&#xA;// Assistants is a beta API and subject to change; acknowledge its experimental status by suppressing the matching warning.&#xA;#pragma warning disable OPENAI001&#xA;OpenAIClient openAIClient = new(Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;FileClient fileClient = openAIClient.GetFileClient();&#xA;AssistantClient assistantClient = openAIClient.GetAssistantClient();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is an example of what the JSON document might look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;using Stream document = BinaryData.FromString(&#34;&#34;&#34;&#xA;    {&#xA;        &#34;description&#34;: &#34;This document contains the sale history data for Contoso products.&#34;,&#xA;        &#34;sales&#34;: [&#xA;            {&#xA;                &#34;month&#34;: &#34;January&#34;,&#xA;                &#34;by_product&#34;: {&#xA;                    &#34;113043&#34;: 15,&#xA;                    &#34;113045&#34;: 12,&#xA;                    &#34;113049&#34;: 2&#xA;                }&#xA;            },&#xA;            {&#xA;                &#34;month&#34;: &#34;February&#34;,&#xA;                &#34;by_product&#34;: {&#xA;                    &#34;113045&#34;: 22&#xA;                }&#xA;            },&#xA;            {&#xA;                &#34;month&#34;: &#34;March&#34;,&#xA;                &#34;by_product&#34;: {&#xA;                    &#34;113045&#34;: 16,&#xA;                    &#34;113055&#34;: 5&#xA;                }&#xA;            }&#xA;        ]&#xA;    }&#xA;    &#34;&#34;&#34;).ToStream();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upload this document to OpenAI using the &lt;code&gt;FileClient&lt;/code&gt;&#39;s &lt;code&gt;UploadFile&lt;/code&gt; method, ensuring that you use &lt;code&gt;FileUploadPurpose.Assistants&lt;/code&gt; to allow your assistant to access it later:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;OpenAIFileInfo salesFile = fileClient.UploadFile(&#xA;    document,&#xA;    &#34;monthly_sales.json&#34;,&#xA;    FileUploadPurpose.Assistants);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a new assistant using an instance of the &lt;code&gt;AssistantCreationOptions&lt;/code&gt; class to customize it. Here, we use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A friendly &lt;code&gt;Name&lt;/code&gt; for the assistant, as will display in the Playground&lt;/li&gt; &#xA; &lt;li&gt;Tool definition instances for the tools that the assistant should have access to; here, we use &lt;code&gt;FileSearchToolDefinition&lt;/code&gt; to process the sales document we just uploaded and &lt;code&gt;CodeInterpreterToolDefinition&lt;/code&gt; so we can analyze and visualize the numeric data&lt;/li&gt; &#xA; &lt;li&gt;Resources for the assistant to use with its tools, here using the &lt;code&gt;VectorStoreCreationHelper&lt;/code&gt; type to automatically make a new vector store that indexes the sales file; alternatively, you could use &lt;code&gt;VectorStoreClient&lt;/code&gt; to manage the vector store separately&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;AssistantCreationOptions assistantOptions = new()&#xA;{&#xA;    Name = &#34;Example: Contoso sales RAG&#34;,&#xA;    Instructions =&#xA;        &#34;You are an assistant that looks up sales data and helps visualize the information based&#34;&#xA;        + &#34; on user queries. When asked to generate a graph, chart, or other visualization, use&#34;&#xA;        + &#34; the code interpreter tool to do so.&#34;,&#xA;    Tools =&#xA;    {&#xA;        new FileSearchToolDefinition(),&#xA;        new CodeInterpreterToolDefinition(),&#xA;    },&#xA;    ToolResources = new()&#xA;    {&#xA;        FileSearch = new()&#xA;        {&#xA;            NewVectorStores =&#xA;            {&#xA;                new VectorStoreCreationHelper([salesFile.Id]),&#xA;            }&#xA;        }&#xA;    },&#xA;};&#xA;&#xA;Assistant assistant = assistantClient.CreateAssistant(&#34;gpt-4o&#34;, assistantOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create a new thread. For illustrative purposes, you could include an initial user message asking about the sales information of a given product and then use the &lt;code&gt;AssistantClient&lt;/code&gt;&#39;s &lt;code&gt;CreateThreadAndRun&lt;/code&gt; method to get it started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;ThreadCreationOptions threadOptions = new()&#xA;{&#xA;    InitialMessages = { &#34;How well did product 113045 sell in February? Graph its trend over time.&#34; }&#xA;};&#xA;&#xA;ThreadRun threadRun = assistantClient.CreateThreadAndRun(assistant.Id, threadOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Poll the status of the run until it is no longer queued or in progress:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;do&#xA;{&#xA;    Thread.Sleep(TimeSpan.FromSeconds(1));&#xA;    threadRun = assistantClient.GetRun(threadRun.ThreadId, threadRun.Id);&#xA;} while (!threadRun.Status.IsTerminal);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything went well, the terminal status of the run will be &lt;code&gt;RunStatus.Completed&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, you can use the &lt;code&gt;AssistantClient&lt;/code&gt;&#39;s &lt;code&gt;GetMessages&lt;/code&gt; method to retrieve the messages associated with this thread, which now include the responses from the assistant to the initial user message.&lt;/p&gt; &#xA;&lt;p&gt;For illustrative purposes, you could print the messages to the console and also save any images produced by the assistant to local storage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;PageCollection&amp;lt;ThreadMessage&amp;gt; messagePages = assistantClient.GetMessages(threadRun.ThreadId, new MessageCollectionOptions() { Order = ListOrder.OldestFirst });&#xA;IEnumerable&amp;lt;ThreadMessage&amp;gt; messages = messagePages.GetAllValues();&#xA;&#xA;foreach (ThreadMessage message in messages)&#xA;{&#xA;    Console.Write($&#34;[{message.Role.ToString().ToUpper()}]: &#34;);&#xA;    foreach (MessageContent contentItem in message.Content)&#xA;    {&#xA;        if (!string.IsNullOrEmpty(contentItem.Text))&#xA;        {&#xA;            Console.WriteLine($&#34;{contentItem.Text}&#34;);&#xA;&#xA;            if (contentItem.TextAnnotations.Count &amp;gt; 0)&#xA;            {&#xA;                Console.WriteLine();&#xA;            }&#xA;&#xA;            // Include annotations, if any.&#xA;            foreach (TextAnnotation annotation in contentItem.TextAnnotations)&#xA;            {&#xA;                if (!string.IsNullOrEmpty(annotation.InputFileId))&#xA;                {&#xA;                    Console.WriteLine($&#34;* File citation, file ID: {annotation.InputFileId}&#34;);&#xA;                }&#xA;                if (!string.IsNullOrEmpty(annotation.OutputFileId))&#xA;                {&#xA;                    Console.WriteLine($&#34;* File output, new file ID: {annotation.OutputFileId}&#34;);&#xA;                }&#xA;            }&#xA;        }&#xA;        if (!string.IsNullOrEmpty(contentItem.ImageFileId))&#xA;        {&#xA;            OpenAIFileInfo imageInfo = fileClient.GetFile(contentItem.ImageFileId);&#xA;            BinaryData imageBytes = fileClient.DownloadFile(contentItem.ImageFileId);&#xA;            using FileStream stream = File.OpenWrite($&#34;{imageInfo.Filename}.png&#34;);&#xA;            imageBytes.ToStream().CopyTo(stream);&#xA;&#xA;            Console.WriteLine($&#34;&amp;lt;image: {imageInfo.Filename}.png&amp;gt;&#34;);&#xA;        }&#xA;    }&#xA;    Console.WriteLine();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And it would yield something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[USER]: How well did product 113045 sell in February? Graph its trend over time.&#xA;&#xA;[ASSISTANT]: Product 113045 sold 22 units in February【4:0†monthly_sales.json】.&#xA;&#xA;Now, I will generate a graph to show its sales trend over time.&#xA;&#xA;* File citation, file ID: file-hGOiwGNftMgOsjbynBpMCPFn&#xA;&#xA;[ASSISTANT]: &amp;lt;image: 015d8e43-17fe-47de-af40-280f25452280.png&amp;gt;&#xA;The sales trend for Product 113045 over the past three months shows that:&#xA;&#xA;- In January, 12 units were sold.&#xA;- In February, 22 units were sold, indicating significant growth.&#xA;- In March, sales dropped slightly to 16 units.&#xA;&#xA;The graph above visualizes this trend, showing a peak in sales during February.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use streaming and GPT-4o vision with assistants&lt;/h2&gt; &#xA;&lt;p&gt;This example shows how to use the v2 Assistants API to provide image data to an assistant and then stream the run&#39;s response.&lt;/p&gt; &#xA;&lt;p&gt;As before, you will use a &lt;code&gt;FileClient&lt;/code&gt; and an &lt;code&gt;AssistantClient&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;// Assistants is a beta API and subject to change; acknowledge its experimental status by suppressing the matching warning.&#xA;#pragma warning disable OPENAI001&#xA;OpenAIClient openAIClient = new(Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;FileClient fileClient = openAIClient.GetFileClient();&#xA;AssistantClient assistantClient = openAIClient.GetAssistantClient();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For this example, we will use both image data from a local file as well as an image located at a URL. For the local data, we upload the file with the &lt;code&gt;Vision&lt;/code&gt; upload purpose, which would also allow it to be downloaded and retrieved later.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;OpenAIFileInfo pictureOfAppleFile = fileClient.UploadFile(&#xA;    &#34;picture-of-apple.jpg&#34;,&#xA;    FileUploadPurpose.Vision);&#xA;Uri linkToPictureOfOrange = new(&#34;https://platform.openai.com/fictitious-files/picture-of-orange.png&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create a new assistant with a vision-capable model like &lt;code&gt;gpt-4o&lt;/code&gt; and a thread with the image information referenced:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;Assistant assistant = assistantClient.CreateAssistant(&#xA;    model: &#34;gpt-4o&#34;,&#xA;    new AssistantCreationOptions()&#xA;    {&#xA;        Instructions = &#34;When asked a question, attempt to answer very concisely. &#34;&#xA;            + &#34;Prefer one-sentence answers whenever feasible.&#34;&#xA;    });&#xA;&#xA;AssistantThread thread = assistantClient.CreateThread(new ThreadCreationOptions()&#xA;{&#xA;    InitialMessages =&#xA;    {&#xA;        new ThreadInitializationMessage(&#xA;        [&#xA;            &#34;Hello, assistant! Please compare these two images for me:&#34;,&#xA;            MessageContent.FromImageFileId(pictureOfAppleFile.Id),&#xA;            MessageContent.FromImageUrl(linkToPictureOfOrange),&#xA;        ]),&#xA;    }&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the assistant and thread prepared, use the &lt;code&gt;CreateRunStreaming&lt;/code&gt; method to get an enumerable &lt;code&gt;CollectionResult&amp;lt;StreamingUpdate&amp;gt;&lt;/code&gt;. You can then iterate over this collection with &lt;code&gt;foreach&lt;/code&gt;. For async calling patterns, use &lt;code&gt;CreateRunStreamingAsync&lt;/code&gt; and iterate over the &lt;code&gt;AsyncCollectionResult&amp;lt;StreamingUpdate&amp;gt;&lt;/code&gt; with &lt;code&gt;await foreach&lt;/code&gt;, instead. Note that streaming variants also exist for &lt;code&gt;CreateThreadAndRunStreaming&lt;/code&gt; and &lt;code&gt;SubmitToolOutputsToRunStreaming&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;CollectionResult&amp;lt;StreamingUpdate&amp;gt; streamingUpdates = assistantClient.CreateRunStreaming(&#xA;    thread,&#xA;    assistant,&#xA;    new RunCreationOptions()&#xA;    {&#xA;        AdditionalInstructions = &#34;When possible, try to sneak in puns if you&#39;re asked to compare things.&#34;,&#xA;    });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, to handle the &lt;code&gt;StreamingUpdates&lt;/code&gt; as they arrive, you can use the &lt;code&gt;UpdateKind&lt;/code&gt; property on the base &lt;code&gt;StreamingUpdate&lt;/code&gt; and/or downcast to a specifically desired update type, like &lt;code&gt;MessageContentUpdate&lt;/code&gt; for &lt;code&gt;thread.message.delta&lt;/code&gt; events or &lt;code&gt;RequiredActionUpdate&lt;/code&gt; for streaming tool calls.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;foreach (StreamingUpdate streamingUpdate in streamingUpdates)&#xA;{&#xA;    if (streamingUpdate.UpdateKind == StreamingUpdateReason.RunCreated)&#xA;    {&#xA;        Console.WriteLine($&#34;--- Run started! ---&#34;);&#xA;    }&#xA;    if (streamingUpdate is MessageContentUpdate contentUpdate)&#xA;    {&#xA;        Console.Write(contentUpdate.Text);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will yield streamed output from the run like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;--- Run started! ---&#xA;The first image shows a red apple with a smooth skin and a single leaf, while the second image depicts an orange with a rough, textured skin and a leaf with droplets of water. Comparing them might seem impossible - it&#39;s like apples and oranges!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to work with Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;For Azure OpenAI scenarios use the &lt;a href=&#34;https://github.com/Azure/azure-sdk-for-net&#34;&gt;Azure SDK&lt;/a&gt; and more specifically the &lt;a href=&#34;https://github.com/Azure/azure-sdk-for-net/raw/main/sdk/openai/Azure.AI.OpenAI/README.md&#34;&gt;Azure OpenAI client library for .NET&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Azure OpenAI client library for .NET is a companion to this library and all common capabilities between OpenAI and Azure OpenAI share the same scenario clients, methods, and request/response types. It is designed to make Azure specific scenarios straightforward, with extensions for Azure-specific concepts like Responsible AI content filter results and On Your Data integration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c#&#34;&gt;AzureOpenAIClient azureClient = new(&#xA;    new Uri(&#34;https://your-azure-openai-resource.com&#34;),&#xA;    new DefaultAzureCredential());&#xA;ChatClient chatClient = azureClient.GetChatClient(&#34;my-gpt-35-turbo-deployment&#34;);&#xA;&#xA;ChatCompletion completion = chatClient.CompleteChat(&#xA;    [&#xA;        // System messages represent instructions or other guidance about how the assistant should behave&#xA;        new SystemChatMessage(&#34;You are a helpful assistant that talks like a pirate.&#34;),&#xA;        // User messages represent user input, whether historical or the most recen tinput&#xA;        new UserChatMessage(&#34;Hi, can you help me?&#34;),&#xA;        // Assistant messages in a request represent conversation history for responses&#xA;        new AssistantChatMessage(&#34;Arrr! Of course, me hearty! What can I do for ye?&#34;),&#xA;        new UserChatMessage(&#34;What&#39;s the best way to train a parrot?&#34;),&#xA;    ]);&#xA;&#xA;Console.WriteLine($&#34;{completion.Role}: {completion.Content[0].Text}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced scenarios&lt;/h2&gt; &#xA;&lt;h3&gt;Using protocol methods&lt;/h3&gt; &#xA;&lt;p&gt;In addition to the client methods that use strongly-typed request and response objects, the .NET library also provides &lt;em&gt;protocol methods&lt;/em&gt; that enable more direct access to the REST API. Protocol methods are &#34;binary in, binary out&#34; accepting &lt;code&gt;BinaryContent&lt;/code&gt; as request bodies and providing &lt;code&gt;BinaryData&lt;/code&gt; as response bodies.&lt;/p&gt; &#xA;&lt;p&gt;For example, to use the protocol method variant of the &lt;code&gt;ChatClient&lt;/code&gt;&#39;s &lt;code&gt;CompleteChat&lt;/code&gt; method, pass the request body as &lt;code&gt;BinaryContent&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;ChatClient client = new(&#34;gpt-4o&#34;, Environment.GetEnvironmentVariable(&#34;OPENAI_API_KEY&#34;));&#xA;&#xA;BinaryData input = BinaryData.FromBytes(&#34;&#34;&#34;&#xA;{&#xA;    &#34;model&#34;: &#34;gpt-4o&#34;,&#xA;    &#34;messages&#34;: [&#xA;       {&#xA;           &#34;role&#34;: &#34;user&#34;,&#xA;           &#34;content&#34;: &#34;How does AI work? Explain it in simple terms.&#34;&#xA;       }&#xA;    ]&#xA;}&#xA;&#34;&#34;&#34;u8.ToArray());&#xA;&#xA;using BinaryContent content = BinaryContent.Create(input);&#xA;ClientResult result = client.CompleteChat(content);&#xA;BinaryData output = result.GetRawResponse().Content;&#xA;&#xA;using JsonDocument outputAsJson = JsonDocument.Parse(output);&#xA;string message = outputAsJson.RootElement&#xA;    .GetProperty(&#34;choices&#34;u8)[0]&#xA;    .GetProperty(&#34;message&#34;u8)&#xA;    .GetProperty(&#34;content&#34;u8)&#xA;    .GetString();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice how you can then call the resulting &lt;code&gt;ClientResult&lt;/code&gt;&#39;s &lt;code&gt;GetRawResponse&lt;/code&gt; method and retrieve the response body as &lt;code&gt;BinaryData&lt;/code&gt; via the &lt;code&gt;PipelineResponse&lt;/code&gt;&#39;s &lt;code&gt;Content&lt;/code&gt; property.&lt;/p&gt; &#xA;&lt;h3&gt;Automatically retrying errors&lt;/h3&gt; &#xA;&lt;p&gt;By default, the client classes will automatically retry the following errors up to three additional times using exponential backoff:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;408 Request Timeout&lt;/li&gt; &#xA; &lt;li&gt;429 Too Many Requests&lt;/li&gt; &#xA; &lt;li&gt;500 Internal Server Error&lt;/li&gt; &#xA; &lt;li&gt;502 Bad Gateway&lt;/li&gt; &#xA; &lt;li&gt;503 Service Unavailable&lt;/li&gt; &#xA; &lt;li&gt;504 Gateway Timeout&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Observability&lt;/h3&gt; &#xA;&lt;p&gt;OpenAI .NET library supports experimental distributed tracing and metrics with OpenTelemetry. Check out &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-dotnet/main/docs/observability.md&#34;&gt;Observability with OpenTelemetry&lt;/a&gt; for more details.&lt;/p&gt;</summary>
  </entry>
</feed>