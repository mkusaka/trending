<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Perl Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-03T01:37:17Z</updated>
  <subtitle>Daily Trending of Perl in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pawjy/perl-promised-mysqld</title>
    <updated>2022-11-03T01:37:17Z</updated>
    <id>tag:github.com,2022-11-03:/pawjy/perl-promised-mysqld</id>
    <link href="https://github.com/pawjy/perl-promised-mysqld" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;=head1 NAME&lt;/p&gt; &#xA;&lt;p&gt;Promised::Mysqld - MySQL server wrapper for development and testing&lt;/p&gt; &#xA;&lt;p&gt;=head1 SYNOPSIS&lt;/p&gt; &#xA;&lt;p&gt;use Promised::Mysqld; $mysqld = Promised::Mysqld-&amp;gt;new; $mysqld-&amp;gt;start-&amp;gt;then (sub { ...; return $mysqld-&amp;gt;stop; })-&amp;gt;then (sub { warn &#34;done&#34;; });&lt;/p&gt; &#xA;&lt;p&gt;=head1 DESCRIPTION&lt;/p&gt; &#xA;&lt;p&gt;The C&lt;a href=&#34;Promised::Mysqld&#34;&gt;Promised::Mysqld&lt;/a&gt; class provides a L&#xA; &lt;promise&gt;&#xA;  -aware interface to start and stop C&#xA;  &lt;mysqld&gt;&#xA;    process, for the purpose of development and testing of Perl applications. It creates a database in the temporary directory (or in the directory specified by the application) and starts a MySQL server instance using it.&#xA;  &lt;/mysqld&gt;&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use of this module is B&#xA; &lt;deprecated&gt;&#xA;   in favor of MariaDB Docker image with L&#xA;  &lt;a href=&#34;Promised::Command::Docker&#34;&gt;Promised::Command::Docker&lt;/a&gt;.&#xA; &lt;/deprecated&gt;&lt;/p&gt; &#xA;&lt;p&gt;=head1 METHODS&lt;/p&gt; &#xA;&lt;p&gt;There are following methods:&lt;/p&gt; &#xA;&lt;p&gt;=over 4&lt;/p&gt; &#xA;&lt;p&gt;=item $mysqld = Promised::Mysqld-&amp;gt;new&lt;/p&gt; &#xA;&lt;p&gt;Create a new instance.&lt;/p&gt; &#xA;&lt;p&gt;=item $promise = $mysqld-&amp;gt;start&lt;/p&gt; &#xA;&lt;p&gt;Start the MySQL server instance and return a promise (L&#xA; &lt;promise&gt;&#xA;  ), which is resolved when the server is ready to accept SQL queries. The promise is rejected if the server failed to start.&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $promise = $mysqld-&amp;gt;stop&lt;/p&gt; &#xA;&lt;p&gt;0Stop the MySQL server instance and return a promise (L&#xA; &lt;promise&gt;&#xA;  ), which is resolved after the server is shutdown.&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked after the C&#xA; &lt;start&gt;&#xA;   method is invoked. It can be invoked even after the C&#xA;  &lt;start&gt;&#xA;   &#39;s promise is rejected.&#xA;  &lt;/start&gt;&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $mysqld-&amp;gt;set_mysqld_and_mysql_install_db ($mysqld, $mysql_install_db)&lt;/p&gt; &#xA;&lt;p&gt;Set the paths to the C&#xA; &lt;mysqld&gt;&#xA;   and C&amp;lt;mysql_install_db&amp;gt; commands.&#xA; &lt;/mysqld&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked before the C&#xA; &lt;start&gt;&#xA;   method is invoked. If not invoked, directories in C&#xA;  &lt;path&gt;&#xA;    and well-known locations are used to find these commands.&#xA;  &lt;/path&gt;&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $mysqld-&amp;gt;set_db_dir ($path)&lt;/p&gt; &#xA;&lt;p&gt;Set the path to the directory used to create files for the MySQL server. It can be an existing database directory, an empty directory, or a path to directory that does not exist yet. Please note that any existing file in the directory, especially C&amp;lt;etc/my.cnf&amp;gt;, can be modified by the module and/or C&#xA; &lt;mysqld&gt;&#xA;  .&#xA; &lt;/mysqld&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked before the C&#xA; &lt;start&gt;&#xA;   method is invoked. If not invoked, a temporary directory is created and used. The temporary directory is removed after the MySQL server is shutdown, unless the C&amp;lt;PROMISED_MYSQLD_DEBUG&amp;gt; environment variable is set to a true value.&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $hashref = $mysqld-&amp;gt;my_cnf&lt;/p&gt; &#xA;&lt;p&gt;Return a hash reference, which is used to generate the C&amp;lt;my.cnf&amp;gt; configuration file for the C&#xA; &lt;mysqld&gt;&#xA;   process.&#xA; &lt;/mysqld&gt;&lt;/p&gt; &#xA;&lt;p&gt;The hash can contain zero or more name/value character string pairs, which is used as name/value pairs in the generated C&amp;lt;my.cnf&amp;gt; file. If the value is the empty string or C&#xA; &lt;undef&gt;&#xA;  , only the name is inserted in the file.&#xA; &lt;/undef&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some of name/value pairs are set by default (or to be set implicitly when the C&#xA; &lt;start&gt;&#xA;   method is invoked). Unless otherwise specified, the C&#xA;  &lt;mysqld&gt;&#xA;    process is configured such that any file read or written (including database files) is located within the directory specified by the C&amp;lt;db_dir&amp;gt; method and that no TCP/IP port is listen but a Unix domain socket is created in the C&amp;lt;db_dir&amp;gt;&#39;s directory.&#xA;  &lt;/mysqld&gt;&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want C&#xA; &lt;mysqld&gt;&#xA;   to listen a TCP port, following lines should be executed before the C&#xA;  &lt;start&gt;&#xA;    method invocation:&#xA;  &lt;/start&gt;&#xA; &lt;/mysqld&gt;&lt;/p&gt; &#xA;&lt;p&gt;$mysqld-&amp;gt;my_cnf-&amp;gt;{port} = $port; delete $mysqld-&amp;gt;my_cnf-&amp;gt;{&#39;skip-networking&#39;};&lt;/p&gt; &#xA;&lt;p&gt;=item $time = $mysqld-&amp;gt;start_timeout&lt;/p&gt; &#xA;&lt;p&gt;=item $mysqld-&amp;gt;start_timeout ($time)&lt;/p&gt; &#xA;&lt;p&gt;Get or set the timeout for the C&#xA; &lt;start&gt;&#xA;  , i.e. the maximum time interval between the invocation of the C&#xA;  &lt;mysqld&gt;&#xA;    command and when the server becomes ready to accept queries.&#xA;  &lt;/mysqld&gt;&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked before the C&#xA; &lt;start&gt;&#xA;   method is invoked.&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $hashref = $mysqld-&amp;gt;get_dsn_options&lt;/p&gt; &#xA;&lt;p&gt;Return a hash reference, which contains name/value pairs that can be use to construct the options component of the DSN string for the L&#xA; &lt;dbi&gt;&#xA;   (L&#xA;  &lt;a href=&#34;DBD::mysql&#34;&gt;DBD::mysql&lt;/a&gt;) module used to connect to the database. Note that the C&#xA;  &lt;dbname&gt;&#xA;    option is set to C&#xA;   &lt;mysql&gt;&#xA;    , which is the only database in the server when the server is created with no initial data.&#xA;   &lt;/mysql&gt;&#xA;  &lt;/dbname&gt;&#xA; &lt;/dbi&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked after the C&#xA; &lt;start&gt;&#xA;  &#39;s promise is resolved.&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $dsn = $mysqld-&amp;gt;get_dsn_string (NAME =&amp;gt; VALUE, ...)&lt;/p&gt; &#xA;&lt;p&gt;Return a DSN string which can be use to connect to the database using the L&#xA; &lt;dbi&gt;&#xA;   (L&#xA;  &lt;a href=&#34;DBD::mysql&#34;&gt;DBD::mysql&lt;/a&gt;) module. By default the C&#xA;  &lt;dbname&gt;&#xA;    option is set to C&#xA;   &lt;mysql&gt;&#xA;    , which is the only database in the server when the server is created with no initial data. This can be replaced by specifying the C&#xA;    &lt;dbname&gt;&#xA;      named argument to the method. Likewise, other options such as C&#xA;     &lt;user&gt;&#xA;       and C&#xA;      &lt;password&gt;&#xA;        can also be specified as named arguments.&#xA;      &lt;/password&gt;&#xA;     &lt;/user&gt;&#xA;    &lt;/dbname&gt;&#xA;   &lt;/mysql&gt;&#xA;  &lt;/dbname&gt;&#xA; &lt;/dbi&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked after the C&#xA; &lt;start&gt;&#xA;  &#39;s promise is resolved.&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;=item $mysqld-&amp;gt;client_connect (NAME =&amp;gt; VALUE, ...)-&amp;gt;then (sub { $client = shift })&lt;/p&gt; &#xA;&lt;p&gt;Connect to the MySQL server as a client and return a promise (L&#xA; &lt;promise&gt;&#xA;  ), which is resolved with the L&#xA;  &lt;a href=&#34;AnyEvent::MySQL::Client&#34;&gt;AnyEvent::MySQL::Client&lt;/a&gt; object which is connected to the database. By default it is connected to the C&#xA;  &lt;mysql&gt;&#xA;    database on the server, which is the only database when the database is initialized without data. This can be altered by specifying the C&#xA;   &lt;dbname&gt;&#xA;     named argument to the method.&#xA;   &lt;/dbname&gt;&#xA;  &lt;/mysql&gt;&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked after the C&#xA; &lt;start&gt;&#xA;  &#39;s promise is resolved. The C&#xA;  &lt;disconnect&gt;&#xA;    method of the client object does not have to be explicitly invoked. The C&#xA;   &lt;stop&gt;&#xA;     method will invoke the method before shutting down the MySQL server, if necessary.&#xA;   &lt;/stop&gt;&#xA;  &lt;/disconnect&gt;&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method requires the L&lt;a href=&#34;AnyEvent::MySQL::Client&#34;&gt;AnyEvent::MySQL::Client&lt;/a&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;=item $promise = $mysqld-&amp;gt;create_db_and_execute_sqls ($dbname, [$sql, $sql, ...])&lt;/p&gt; &#xA;&lt;p&gt;Execute a set of SQL stagements and return a promise (L&#xA; &lt;promise&gt;&#xA;  ), which is resolved after the SQL statements specified by the arguments are executed on the created database. The promise is rejected if the execution is failed.&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;The first argument must be the database name. If there is no database with the specified name on the server, a new database is created (by an SQL C&#xA; &lt;create database&gt;&#xA;   statement).&#xA; &lt;/create&gt;&lt;/p&gt; &#xA;&lt;p&gt;The second argument must be an array reference of zero or more strings, which are sent to the MySQL server as SQL statements, in order.&lt;/p&gt; &#xA;&lt;p&gt;This method must be invoked after the C&#xA; &lt;start&gt;&#xA;  &#39;s promise is resolved.&#xA; &lt;/start&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method requires the L&lt;a href=&#34;AnyEvent::MySQL::Client&#34;&gt;AnyEvent::MySQL::Client&lt;/a&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;=back&lt;/p&gt; &#xA;&lt;p&gt;During the server is running, signal handlers for C&#xA; &lt;sigint&gt;&#xA;  , C&#xA;  &lt;sigterm&gt;&#xA;   , and C&#xA;   &lt;sigquit&gt;&#xA;     are installed such that these signal will terminate the server (and the current script). If the script wants to handle signal in other ways, the handling should be specified using the L&#xA;    &lt;a href=&#34;Promised::Command::Signals&#34;&gt;Promised::Command::Signals&lt;/a&gt; API from &#xA;    &lt;a href=&#34;https://github.com/wakaba/perl-promised-command&#34;&gt;https://github.com/wakaba/perl-promised-command&lt;/a&gt; to avoid confliction of signal handlers.&#xA;   &lt;/sigquit&gt;&#xA;  &lt;/sigterm&gt;&#xA; &lt;/sigint&gt;&lt;/p&gt; &#xA;&lt;p&gt;=head1 DEPENDENCY&lt;/p&gt; &#xA;&lt;p&gt;This module requires Perl 5.12 or later on Linux, or the latest version of Perl 5 on Mac OS X. The latest version of Perl 5 on Linux is recommended.&lt;/p&gt; &#xA;&lt;p&gt;The module requires L&#xA; &lt;promise&gt; &#xA;  &lt;a href=&#34;https://github.com/wakaba/perl-promise&#34;&gt;https://github.com/wakaba/perl-promise&lt;/a&gt;, L&#xA;  &lt;a href=&#34;Promised::Command&#34;&gt;Promised::Command&lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/wakaba/perl-promised-command&#34;&gt;https://github.com/wakaba/perl-promised-command&lt;/a&gt;, L&#xA;  &lt;a href=&#34;Promised::File&#34;&gt;Promised::File&lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/wakaba/perl-promised-file&#34;&gt;https://github.com/wakaba/perl-promised-file&lt;/a&gt;, and L&#xA;  &lt;anyevent&gt;&#xA;   .&#xA;  &lt;/anyevent&gt;&#xA; &lt;/promise&gt;&lt;/p&gt; &#xA;&lt;p&gt;As described in the earlier section, some methods require L&lt;a href=&#34;AnyEvent::MySQL::Client&#34;&gt;AnyEvent::MySQL::Client&lt;/a&gt; &lt;a href=&#34;https://github.com/wakaba/perl-anyevent-mysql-client&#34;&gt;https://github.com/wakaba/perl-anyevent-mysql-client&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It also requires MySQL version 5 (C&#xA; &lt;mysqld&gt;&#xA;   and C&amp;lt;mysql_install_db&amp;gt; commands).&#xA; &lt;/mysqld&gt;&lt;/p&gt; &#xA;&lt;p&gt;=head1 AUTHOR&lt;/p&gt; &#xA;&lt;p&gt;Wakaba &lt;a href=&#34;mailto:wakaba@suikawiki.org&#34;&gt;wakaba@suikawiki.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;=head1 HISTORY&lt;/p&gt; &#xA;&lt;p&gt;This module is inspired by L&lt;a href=&#34;Test::mysqld&#34;&gt;Test::mysqld&lt;/a&gt; &lt;a href=&#34;https://github.com/kazuho/p5-test-mysqld&#34;&gt;https://github.com/kazuho/p5-test-mysqld&lt;/a&gt; and L&lt;a href=&#34;Test::MySQL::CreateDatabase&#34;&gt;Test::MySQL::CreateDatabase&lt;/a&gt; &lt;a href=&#34;https://github.com/wakaba/perl-rdb-utils/tree/master/lib/Test/MySQL&#34;&gt;https://github.com/wakaba/perl-rdb-utils/tree/master/lib/Test/MySQL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This Git repository was located at &lt;a href=&#34;https://github.com/wakaba/perl-promised-mysqld&#34;&gt;https://github.com/wakaba/perl-promised-mysqld&lt;/a&gt; until 7 March, 2022.&lt;/p&gt; &#xA;&lt;p&gt;=head1 LICENSE&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2015-2022 Wakaba &lt;a href=&#34;mailto:wakaba@suikawiki.org&#34;&gt;wakaba@suikawiki.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This library is free software; you can redistribute it and/or modify it under the same terms as Perl itself.&lt;/p&gt; &#xA;&lt;p&gt;=cut&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>github4bhavin/Forex</title>
    <updated>2022-11-03T01:37:17Z</updated>
    <id>tag:github.com,2022-11-03:/github4bhavin/Forex</id>
    <link href="https://github.com/github4bhavin/Forex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Foreign Exchange Rates from OpenExchangeRates.org&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NAME&lt;/h1&gt; &#xA;&lt;p&gt;Forex - Historic Foreign Exchange Rates from Open Exchange Rates&lt;/p&gt; &#xA;&lt;h1&gt;VERSION&lt;/h1&gt; &#xA;&lt;p&gt;1.3&lt;/p&gt; &#xA;&lt;h1&gt;SYNOPSIS&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;use Forex;&#xA;&#xA;&#xA;my $forex = new Forex( &#39;APP_ID&#39; =&amp;gt; $app_id, &#39;BASE&#39; =&amp;gt; &#39;USD&#39;);&#xA;&#xA;&#xA;#__fetch and initialize daily rates from $from_date to $to_date in yyyy-mm-dd &#xA;$forex-&amp;gt;get_rates_from ( $from_date, $to_date );&#xA;&#xA;#___ fetches rates for $date (yyyy-mm-dd)&#xA;$forex-&amp;gt;get_rates( $date );&#xA; &#xA;&#xA;#___ fetches reates for today &#xA;$forex-&amp;gt;get_rates();&#xA;&#xA;&#xA;#___ gives AUD on 2012-09-01 in USD Base currency&#xA;$usd = $forex-&amp;gt;get_rate_of ( &#39;AUD&#39;, &#39;2012-09-01&#39; );  &#xA;&#xA;&#xA;if ($Forex::LASTERROR )&#xA;   { print &#34;\n Something went wrong&#34; , $oxr-&amp;gt;last_error_message(); }  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;DESCRIPTION&lt;/h1&gt; &#xA;&lt;h1&gt;METHODS&lt;/h1&gt; &#xA;&lt;h2&gt;Constructors&lt;/h2&gt; &#xA;&lt;h3&gt;new Forex()&lt;/h3&gt; &#xA;&lt;p&gt;returns new Forex object with defaults values for&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;OXR_HOME = &#39;&lt;a href=&#34;http://openexchangerates.org&#34;&gt;http://openexchangerates.org&lt;/a&gt;&#39;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;API_HOOK = &#39;api&#39;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;APP_ID = &#39;temp-e091fc14b3884a516d6cc2c299a&#39;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;BASE = &#39;USD&#39;&lt;/p&gt; &lt;p&gt;my $oxe = new Forex( OXR_HOME =&amp;gt; &#39;&lt;a href=&#34;https://openexchangerates.org&#34;&gt;https://openexchangerates.org&lt;/a&gt;&#39;, API_HOOK =&amp;gt; &#39;api&#39;, APP_ID =&amp;gt; &#39;temp-e091fc14b3884a516d6cc2c299a&#39;, BASE =&amp;gt; &#39;AUD&#39;);&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;get_rate_of( $currency, &amp;lt;$date&amp;gt; )&lt;/h3&gt; &#xA;&lt;p&gt;This method returns forex rate for &lt;code&gt;$currency&lt;/code&gt; on $date in BASE currency, $date should be in &lt;code&gt;yyyy-mm-dd&lt;/code&gt; format.&lt;/p&gt; &#xA;&lt;p&gt;my $AUD = $oxr-&amp;gt;get_rate_of( &#39;AUD&#39; , &#39;2012-09-10&#39; );&lt;/p&gt; &#xA;&lt;h3&gt;get_rates_from ( $from_date , $to_date )&lt;/h3&gt; &#xA;&lt;p&gt;downloads and fills CURRENCIES hash for $from_date to $to_date both dates should be in &lt;code&gt;yyyy-mm-dd&lt;/code&gt; formate&lt;/p&gt; &#xA;&lt;h3&gt;get_rates ( &amp;lt;$day&amp;gt; )&lt;/h3&gt; &#xA;&lt;p&gt;downloads and fills in values for all currencies in the CURRENCIES hash for given &lt;code&gt;$day&lt;/code&gt; $day should be in &lt;code&gt;yyyy-mm-dd&lt;/code&gt; formate if $day is skipped , it uses &lt;code&gt;todays date&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;h3&gt;get_currency_symbols()&lt;/h3&gt; &#xA;&lt;p&gt;this method downloads and initializes all currency symbols from openexchangerates site. this method should be run before either &lt;code&gt;get_rates&lt;/code&gt; or &lt;code&gt;get_rates_from&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;base_currency( &amp;lt;$BASE_CURRENCY&amp;gt; )&lt;/h3&gt; &#xA;&lt;p&gt;sets BASE currency so that succeeding request will request the rates with base currency as specified by $BASE_CURRENCY. if the parameter is omitted it return the current BASE_CURRENCY value;&lt;/p&gt; &#xA;&lt;p&gt;Note: you will have to flush the $CURRENCIES hash if you change the BASE currency. with &lt;code&gt;flush_values()&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;oxr_home ( &amp;lt;$OXR_HOME&amp;gt; )&lt;/h3&gt; &#xA;&lt;p&gt;sets OXR_HOME parameter to $OXR_HOME value , if the parameter is omitted it returns the current value of the OXR_HOME. OXR_HOME value should cuntaion &#34;http://&#34; .&lt;/p&gt; &#xA;&lt;p&gt;Note: you could you it to change the default &#34;http://&#34; to &#34;https://&#34; if you have enterprise APP_ID&lt;/p&gt; &#xA;&lt;h3&gt;app_id ( &amp;lt;$APP_ID&amp;gt; )&lt;/h3&gt; &#xA;&lt;p&gt;sets app_id for all succeeding requests. return current app_id if the parameter is omitted.&lt;/p&gt; &#xA;&lt;h3&gt;get_currencies()&lt;/h3&gt; &#xA;&lt;p&gt;returns all the currencies in the currencies hash&lt;/p&gt; &#xA;&lt;h3&gt;last_error()&lt;/h3&gt; &#xA;&lt;p&gt;returns last error object as returned by Open Exchange Rates API&lt;/p&gt; &#xA;&lt;h3&gt;last_error_message ()&lt;/h3&gt; &#xA;&lt;p&gt;returns last error message&lt;/p&gt; &#xA;&lt;h3&gt;ERROR&lt;/h3&gt; &#xA;&lt;p&gt;on errors module sets $LASTERROR global variable which can be accessed by $Forex::LASTERROR. And error message can be accessed via last_error_message() or last_error()&lt;/p&gt; &#xA;&lt;h3&gt;KNOWN BUGS&lt;/h3&gt; &#xA;&lt;h3&gt;SUPPORT&lt;/h3&gt; &#xA;&lt;p&gt;please submit known issues or bugs to &lt;a href=&#34;mailto:mail4bhavin@yahoo.com&#34;&gt;mail4bhavin@yahoo.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AUTHOR&lt;/h3&gt; &#xA;&lt;p&gt;Bhavin Patel&lt;/p&gt; &#xA;&lt;h3&gt;COPYRIGHT AND LICENSE&lt;/h3&gt; &#xA;&lt;p&gt;This Software is free to use , licensed under:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;The Artistic License 2.0 (GPL Compatible)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Ensembl/ensembl-analysis</title>
    <updated>2022-11-03T01:37:17Z</updated>
    <id>tag:github.com,2022-11-03:/Ensembl/ensembl-analysis</id>
    <link href="https://github.com/Ensembl/ensembl-analysis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modules to interface with tools used in Ensembl Gene Annotation Process and scripts to run pipelines&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ensembl-analysis&lt;/h1&gt; &#xA;&lt;p&gt;The EnsEMBL Genome Annotation System&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Softwares&lt;/h3&gt; &#xA;&lt;p&gt;We use Linuxbrew to install all our software you will need to tap cask from &lt;a href=&#34;https://github.com/Ensembl/homebrew-cask&#34;&gt;https://github.com/Ensembl/homebrew-cask&lt;/a&gt; before being able to install &lt;code&gt;ensembl/cask/genebuild-annotation&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will need to ask for a licence for some of the software we use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;crossmatch, part of phred/phrap: used by RepeatMasker, you could use NCBI blast instead&lt;/li&gt; &#xA; &lt;li&gt;trf&lt;/li&gt; &#xA; &lt;li&gt;repbase: you could use RepeatModeler to generate a repeat library. However the generated library could mask some genes coding for some protein families&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Perl EnsEMBL repositories you need to have&lt;/h3&gt; &#xA;&lt;p&gt;We recommend that you clone all the repositories into one directory&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Repository name&lt;/th&gt; &#xA;   &lt;th&gt;branch&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl.git&#34;&gt;https://github.com/Ensembl/ensembl.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-hive&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-hive.git&#34;&gt;https://github.com/Ensembl/ensembl-hive.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-compara&lt;/td&gt; &#xA;   &lt;td&gt;release/98&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-compara.git&#34;&gt;https://github.com/Ensembl/ensembl-compara.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-variation&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-variation.git&#34;&gt;https://github.com/Ensembl/ensembl-variation.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-production&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-production.git&#34;&gt;https://github.com/Ensembl/ensembl-production.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-taxonomy&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-taxonomy.git&#34;&gt;https://github.com/Ensembl/ensembl-taxonomy.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-orm&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-orm.git&#34;&gt;https://github.com/Ensembl/ensembl-orm.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-killlist&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-killlist.git&#34;&gt;https://github.com/Ensembl/ensembl-killlist.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-datacheck&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-datacheck.git&#34;&gt;https://github.com/Ensembl/ensembl-datacheck.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-metadata&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-metadata.git&#34;&gt;https://github.com/Ensembl/ensembl-metadata.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-io&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-io.git&#34;&gt;https://github.com/Ensembl/ensembl-io.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For each of these repository, you will need to install their dependencies using the cpanfile provided in their Git repositories&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/Ensembl/ensembl-git-tools&#34;&gt;Ensembl git commands&lt;/a&gt; and run the following command to clone the repositories&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git ensembl --clone genebuild&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python EnsEMBL repositories you need to have&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Repository name&lt;/th&gt; &#xA;   &lt;th&gt;branch&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ensembl-genes&lt;/td&gt; &#xA;   &lt;td&gt;default&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ensembl/ensembl-genes.git&#34;&gt;https://github.com/Ensembl/ensembl-genes.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Python virtual environment&lt;/h3&gt; &#xA;&lt;p&gt;You will need to create two virtual environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;genebuild&lt;/code&gt; using the requirements.txt file; it needs to be activated for the pipeline to run&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;genebuild-mirna&lt;/code&gt; using the requirements_p36_ncrna.txt file; it will be used directly from the analysis, no need to activate it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Shell environment&lt;/h3&gt; &#xA;&lt;p&gt;If you are not part of the Ensembl Genebuild team, you will need to set some shell environment variables to avoid having to provide the information to the configuration files. We will assume you are using your home directory&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;   &lt;th&gt;Hive configuration parameter&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ENSCODE&lt;/td&gt; &#xA;   &lt;td&gt;$HOME&lt;/td&gt; &#xA;   &lt;td&gt;-enscode_root_dir&lt;/td&gt; &#xA;   &lt;td&gt;Directory path where you cloned all the Perl repositories&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ENSEMBL_SOFTWARE_HOME&lt;/td&gt; &#xA;   &lt;td&gt;$HOME&lt;/td&gt; &#xA;   &lt;td&gt;-software_base_path&lt;/td&gt; &#xA;   &lt;td&gt;Directory where pyenv, plenv and linuxbrew are installed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LINUXBREW_HOME&lt;/td&gt; &#xA;   &lt;td&gt;$HOME/.linuxbrew&lt;/td&gt; &#xA;   &lt;td&gt;-linuxbrew_home_path&lt;/td&gt; &#xA;   &lt;td&gt;Base directory for your Linuxbrew installation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PYTHONPATH&lt;/td&gt; &#xA;   &lt;td&gt;$HOME/ensembl-genes/ensembl_genes:$HOME/ensembl-hive/wrappers/python3/&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;It needs to be set until the package can be installed properly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLASTDB_DIR&lt;/td&gt; &#xA;   &lt;td&gt;$HOME&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;It will be used to find the path to the entry_loc file which is the list of accession from SwissProt which would be located at $HOME/uniprot/2021_03/entry_loc&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MySQL&lt;/h3&gt; &#xA;&lt;p&gt;We currently use MySQL databases to store our data. To avoid having to do many changes to the configuration files we recommend having one read-only user and one read-write user. It is also better to use different servers for keeping the eHive pipeline database, the DNA database and the &#34;data&#34; databases.&lt;/p&gt; &#xA;&lt;h2&gt;Running the EnsEMBL Genome Annotation System&lt;/h2&gt; &#xA;&lt;p&gt;There is a main configuration file, &lt;code&gt;Bio::EnsEMBL::Analysis::Hive::Config::Genome_annotation_conf&lt;/code&gt;, which will generate a set of pipelines to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;load the DNA into the &lt;code&gt;dna_db&lt;/code&gt; database&lt;/li&gt; &#xA; &lt;li&gt;mask repeats&lt;/li&gt; &#xA; &lt;li&gt;align multiple sets of data to the genome to produce gene models depending on the data available&lt;/li&gt; &#xA; &lt;li&gt;select the best transcripts for each loci&lt;/li&gt; &#xA; &lt;li&gt;produce the finalised databases for an Ensembl release The whole system is explained in more details below&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Initialising the pipeline&lt;/h3&gt; &#xA;&lt;p&gt;You will need to activate the genebuild virtual environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyenv activate genebuild&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Filling the main configuration automatically&lt;/h4&gt; &#xA;&lt;p&gt;If you are operating within an environment prepared for Ensembl with the assembly registry you can use the &lt;code&gt;$ENSCODE/ensembl-analysis/scripts/genebuild/create_annotation_configs.pl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You would need to edit &lt;code&gt;$ENSCODE/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Hive/Config/genome_annotation.ini&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-analysis/scripts/genebuild/create_annotation_configs.pl --config_file $ENSCODE/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Hive/Config/genome_annotation.ini --assembly_registry_host &amp;lt;host_name&amp;gt; --assembly_registry_port &amp;lt;port&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Filling the main configuration manually&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re setup do not work with the create_annotation_configs.pl script, you would need to edit &lt;code&gt;$ENSCODE/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Hive/Config/Genome_annotation_conf.pm&lt;/code&gt; and fill in any information according to you environment&lt;/p&gt; &#xA;&lt;p&gt;Then you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-hive/scripts/init_Pipeline.pl Bio::EnsEMBL::Analysis::Hive::Config::Genome_annotation_conf [extra parameters]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running the pipeline&lt;/h3&gt; &#xA;&lt;p&gt;To start the pipeline you need the URL to your pipeline database which will be provided when running the init_Pipeline.pl script. If you initialised the pipeline automatically, you need to look at the command file created in your &lt;code&gt;working_dir&lt;/code&gt; directory to retrieve the information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export EHIVE_URL=mysql://readwrite_user:password@host:port/dbname&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now start the pipeline with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-hive/scripts/beekeeper.pl -url $EHIVE_URL -loop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you only want to run some analyses, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-hive/scripts/beekeeper.pl -url $EHIVE_URL -loop -analyses_pattern 1..5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Monitoring the pipeline&lt;/h3&gt; &#xA;&lt;h4&gt;GuiHive&lt;/h4&gt; &#xA;&lt;p&gt;To follow the pipeline steps, it is better to use GuiHive, a graphical interface to ensembl-hive, which allows you to change parameters, debug your problems and much more &lt;a href=&#34;https://github.com/Ensembl/guiHive&#34;&gt;https://github.com/Ensembl/guiHive&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;What to do when the main pipeline fails&lt;/h4&gt; &#xA;&lt;p&gt;You should first look at the job tab to know the reason of the failure&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Insufficient memory: you can either use a different resource or add a new one more suited to your needs&lt;/li&gt; &#xA; &lt;li&gt;Error in the code: I&#39;m afraid you will need to do proper debugging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you are happy with your fix, you would need to reset the jobs with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-hive/scripts/beekeeper.pl -url $EHIVE_URL -reset_failed_jobs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and restart the pipeline&lt;/p&gt; &#xA;&lt;h4&gt;How can I debug a job&lt;/h4&gt; &#xA;&lt;p&gt;By default ensembl-hive redirect all output to &lt;code&gt;/dev/null&lt;/code&gt; unless you used some logging parameters.&lt;/p&gt; &#xA;&lt;p&gt;You will need to run the problematic job with runWorker. First you will need to retrieve the job id using GuiHive or the pipeline database. Then you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;perl $ENSCODE/ensembl-hive/scripts/runWorker.pl -url $EHIVE_URL -debug 1 -job_id XX&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using a higher value for &lt;code&gt;-debug&lt;/code&gt; is usually not useful as it is mostly seen as a boolean flag.&lt;/p&gt; &#xA;&lt;h4&gt;What do to when a subpipeline fails&lt;/h4&gt; &#xA;&lt;p&gt;First you would need to go on the job tab and do a middle click/right click on the &lt;code&gt;guihive&lt;/code&gt; link. You would need to insert the password twice, this is normal and not a scam.&lt;/p&gt; &#xA;&lt;p&gt;Then from the &#34;GuiHive&#34; of the sub pipeline, you need to make your changes/debugging the same way as a normal pipeline. Once this is done, you can simply reset the main pipeline and restart the main pipeline&lt;/p&gt; &#xA;&lt;h2&gt;What is the difference between a &#34;main&#34; pipeline and a &#34;sub&#34; pipeline&lt;/h2&gt; &#xA;&lt;p&gt;We use the main/sub terminology to define the different part of the system. We could have used parent/child&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &#34;main&#34; pipeline will run multiple analyses and will initialise at least one sub pipeline and run this sub pipeline&lt;/li&gt; &#xA; &lt;li&gt;The &#34;sub&#34; pipeline will run multiple analyses and can potentially be the &#34;main&#34; pipeline of a different pipeline. A sub pipeline can be run on its own&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A pipeline needs three analyses to be called a &#34;main&#34; pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;create_&amp;lt;sub pipeline name&amp;gt;_jobs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;initialise_&amp;lt;sub pipeline name&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;run_&amp;lt;sub pipeline name&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;create_&amp;lt;sub pipeline name&amp;gt;_jobs&lt;/h3&gt; &#xA;&lt;p&gt;The main reason for this analysis is to provide an easy access to the sub pipeline guiHive and the pipeline database details&lt;/p&gt; &#xA;&lt;p&gt;It is usually a &lt;code&gt;Bio::EnsEMBL::Hive::RunnableDB::JobFactory&lt;/code&gt; where the input list will have at least one element with four key value pair. The reason for this analysis it&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ehive_url: the sub pipeline database connection URI, &lt;code&gt;mysql://rw\_user:password@host:port/database&lt;/code&gt;, useful for debugging as it can be used&lt;/li&gt; &#xA; &lt;li&gt;external_link: the sub pipeline guiHive URL&lt;/li&gt; &#xA; &lt;li&gt;meta_pipeline_db: the sub pipeline database connection hash, &lt;code&gt;{&#39;-dbname&#39; =&amp;gt; &#39;database&#39;,&#39;-driver&#39; =&amp;gt; &#39;mysql&#39;,&#39;-host&#39; =&amp;gt; &#39;host&#39;,&#39;-pass&#39; =&amp;gt; &#39;password&#39;,&#39;-port&#39; =&amp;gt; 3306,&#39;-user&#39; =&amp;gt; &#39;rw_user&#39;}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;pipeline_name: the name of the sub pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example analysis config&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  -logic_name =&amp;gt; &#39;create_rnaseq_db_pipeline_job&#39;,&#xA;  -module =&amp;gt; &#39;Bio::EnsEMBL::Hive::RunnableDB::JobFactory&#39;,&#xA;  -parameters =&amp;gt; {&#xA;    column_names =&amp;gt; [&#39;meta_pipeline_db&#39;, &#39;ehive_url&#39;, &#39;pipeline_name&#39;, &#39;external_link&#39;],&#xA;    inputlist =&amp;gt; [[$rnaseq_db_pipe_db, $rnaseq_db_pipe_url, &#39;rnaseq_db_&#39;.$self-&amp;gt;o(&#39;production_name&#39;), $rnaseq_db_guihive]],&#xA;    guihive_host =&amp;gt; $self-&amp;gt;o(&#39;guihive_host&#39;),&#xA;    guihive_port =&amp;gt; $self-&amp;gt;o(&#39;guihive_port&#39;),&#xA;  },&#xA;  -rc_name =&amp;gt; &#39;default&#39;,&#xA;  -max_retry_count =&amp;gt; 0,&#xA;  -flow_into =&amp;gt; {&#xA;    2 =&amp;gt; [&#39;initialise_rnaseq_db&#39;],&#xA;  }&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;initialise_&amp;lt;sub pipeline name&amp;gt;&lt;/h3&gt; &#xA;&lt;p&gt;This analysis will initialise the sub pipeline.&lt;/p&gt; &#xA;&lt;p&gt;It must be a &lt;code&gt;Bio::EnsEMBL::Analysis::Hive::RunnableDB::HiveMetaPipelineInit&lt;/code&gt;, it will simply generate the commandline which will be run with &lt;code&gt;$ENSCODE/ensembl-hive/scripts/init_pipeline.pl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is a set of parameters which are needed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hive_config: the name of the config to use&lt;/li&gt; &#xA; &lt;li&gt;databases: list of keys which contains the connection details of any database to be used such as &#39;dna_db&#39;. The key value pairs should not be in &lt;code&gt;extra_parameters&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;extra_parameters: any parameter to be provided to the initialisation script. &lt;code&gt;species_name =&amp;gt; &#39;homo_sapiens&#39;&lt;/code&gt; would be used as &lt;code&gt;-species_name homo_sapiens&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;metadata_pipeline_db: the connection details of the pipeline database, this would usually be provided by the &lt;code&gt;create_&amp;lt;sub pipeline name&amp;gt;_jobs&lt;/code&gt; analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There is one special parameter which is not required but can be helpful, &lt;code&gt;commandline_params&lt;/code&gt;, which will not process the value associated. It can be used for re-initialising a sub pipeline with something like &lt;code&gt;-hive_force_init 1&lt;/code&gt; or &lt;code&gt;-hive_force_init 1 -drop_databases 1&lt;/code&gt; if the pipeline has already been started/run and you want to drop any databases created by the pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Arrays and hashes cannot be easily passed to the init script on the command line. Also Hive does not overwrite arrays, it always add elements to the parameter if the array already exist.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example analysis config&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  -logic_name =&amp;gt; &#39;initialise_rnaseq_db&#39;,&#xA;  -module     =&amp;gt; &#39;Bio::EnsEMBL::Analysis::Hive::RunnableDB::HiveMetaPipelineInit&#39;,&#xA;  -parameters =&amp;gt; {&#xA;    hive_config =&amp;gt; $self-&amp;gt;o(&#39;hive_rnaseq_db_config&#39;),&#xA;    databases =&amp;gt; [&#39;rnaseq_db&#39;, &#39;rnaseq_refine_db&#39;, &#39;rnaseq_blast_db&#39;, &#39;dna_db&#39;],&#xA;    rnaseq_db =&amp;gt; $self-&amp;gt;o(&#39;rnaseq_db&#39;),&#xA;    rnaseq_refine_db =&amp;gt; $self-&amp;gt;o(&#39;rnaseq_refine_db&#39;),&#xA;    rnaseq_blast_db =&amp;gt; $self-&amp;gt;o(&#39;rnaseq_blast_db&#39;),&#xA;    dna_db =&amp;gt; $self-&amp;gt;o(&#39;dna_db&#39;),&#xA;    enscode_root_dir =&amp;gt; $self-&amp;gt;o(&#39;enscode_root_dir&#39;),&#xA;    extra_parameters =&amp;gt; {&#xA;      output_path =&amp;gt; $self-&amp;gt;o(&#39;output_path&#39;),&#xA;      user_r =&amp;gt; $self-&amp;gt;o(&#39;user_r&#39;),&#xA;      dna_db_host =&amp;gt; $self-&amp;gt;o(&#39;dna_db_host&#39;),&#xA;      dna_db_port =&amp;gt; $self-&amp;gt;o(&#39;dna_db_port&#39;),&#xA;      databases_host =&amp;gt; $self-&amp;gt;o(&#39;databases_host&#39;),&#xA;      databases_port =&amp;gt; $self-&amp;gt;o(&#39;databases_port&#39;),&#xA;      release_number =&amp;gt; $self-&amp;gt;o(&#39;release_number&#39;),&#xA;      production_name =&amp;gt; $self-&amp;gt;o(&#39;production_name&#39;),&#xA;      species_name =&amp;gt; $self-&amp;gt;o(&#39;species_name&#39;),&#xA;      registry_host =&amp;gt; $self-&amp;gt;o(&#39;registry_host&#39;),&#xA;      registry_port =&amp;gt; $self-&amp;gt;o(&#39;registry_port&#39;),&#xA;      registry_db =&amp;gt; $self-&amp;gt;o(&#39;registry_db&#39;),&#xA;      assembly_name =&amp;gt; $self-&amp;gt;o(&#39;assembly_name&#39;),&#xA;      assembly_accession =&amp;gt; $self-&amp;gt;o(&#39;assembly_accession&#39;),&#xA;    },&#xA;  },&#xA;  -rc_name      =&amp;gt; &#39;default&#39;,&#xA;  -max_retry_count =&amp;gt; 0,&#xA;  -flow_into =&amp;gt; {&#xA;    1 =&amp;gt; [&#39;run_rnaseq_db&#39;],&#xA;  },&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;run_&amp;lt;sub pipeline name&amp;gt;&lt;/h3&gt; &#xA;&lt;p&gt;This analysis will run the sub pipeline&lt;/p&gt; &#xA;&lt;p&gt;It must be a &lt;code&gt;Bio::EnsEMBL::Analysis::Hive::RunnableDB::HiveMetaPipelineRun&lt;/code&gt;, it will run beekeeper in a loop. If the job is a retry, it will first reset all failed jobs in the sub pipeline and start beekeeper.&lt;/p&gt; &#xA;&lt;p&gt;There is only one parameter to be passed, &lt;code&gt;beekeeper_script&lt;/code&gt;, it is not required but recommended.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example analysis config&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  -logic_name =&amp;gt; &#39;run_rnaseq_db&#39;,&#xA;  -module     =&amp;gt; &#39;Bio::EnsEMBL::Analysis::Hive::RunnableDB::HiveMetaPipelineRun&#39;,&#xA;  -parameters =&amp;gt; {&#xA;    beekeeper_script =&amp;gt; $self-&amp;gt;o(&#39;hive_beekeeper_script&#39;),&#xA;  },&#xA;  -rc_name      =&amp;gt; &#39;default&#39;,&#xA;  -max_retry_count =&amp;gt; 1,&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The different parts of the EnsEMBL Genome Annotation System&lt;/h2&gt; &#xA;&lt;h3&gt;The main pipeline&lt;/h3&gt; &#xA;&lt;p&gt;The main pipeline will query ENA to retrieve the possible short read and long read accessions which would be used in the corresponding sub pipelines. Then start each sub pipeline below in the order they appear in this document.&lt;/p&gt; &#xA;&lt;p&gt;By default, ENA is queried using the NCBI taxonomy id of the species, but you can provide either a single project id or a list of project ids to &lt;code&gt;study_accession&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;It will create a registry file which will be used for the whole genome alignment and the DataChecks&lt;/p&gt; &#xA;&lt;p&gt;It will provide stats on the assembly and on the repeat masking which will be sent to the email provided.&lt;/p&gt; &#xA;&lt;p&gt;Before starting the alignments sub pipelines, it will reset a set of arrays in the &#34;Transcript selection&#34; sub pipeline to allow any related sub pipeline to provide the database connection details some analyses will need.&lt;/p&gt; &#xA;&lt;h3&gt;Loading the assembly&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will create the core database which is referenced as &lt;code&gt;dna_db&lt;/code&gt;, &lt;code&gt;reference_db&lt;/code&gt; and sometimes &lt;code&gt;core_db&lt;/code&gt; in configuration files. It will load a set of static tables. It will process the assembly report file to load the sequence names and synonyms and the dna. If UCSC sequence accessions exist in the report, they will be loaded as sequence synonyms&lt;/p&gt; &#xA;&lt;p&gt;When there is a known mitochondrial sequence, it will load the mitochondrial dna and genes. Finally, it will dump the genome in a fasta file and index it for faidx access.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;If RefSeq has not created a sister assembly (GCF_*), we will not load any mitochondrial data even if a RefSeq mitochondrial annotation exists. Because the mitochodrial sequence is not used in the annotation process it can be checked before the gene set is released with a query on the NCBI website, searching for a mitocondrial annotation from RefSeq for the species of interest.&lt;/p&gt; &#xA;&lt;h3&gt;RefSeq annotation import&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It checks on the assembly report if a corresponding RefSeq assembly exists. It will then load the RefSeq sequence accessions as sequence synonyms and the RefSeq gene set using their GFF3 annotation file&lt;/p&gt; &#xA;&lt;p&gt;The gene set is not used for annotation purpose. It can be used to compare the gene set generated and Ensmbl users appreciate the possibilty of looking at both gene sets in one location.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Repeat masking the genome&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will run RepeatMasker using RepBase with the closest clade library and it will run dustmasker and TRF.&lt;/p&gt; &#xA;&lt;p&gt;It will verify the presence of a RepeatModeler library file and run RepeatMasker with this library if needed.&lt;/p&gt; &#xA;&lt;p&gt;It will run Red, a different repeat masking program if &lt;code&gt;replace_repbase_with_red_to_mask&lt;/code&gt; is set to 1.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;Red and RepeatMasker with a RepeatModeler library may mask the genome where protein gene families could be. However in the absence of a RepBase entry for the species of interest, they will be helpful.&lt;/p&gt; &#xA;&lt;h3&gt;Whole genome alignment against a high quality assembly with LastZ&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It uses an Ensembl Compara pipeline to do the whole genome alignment of your species genome against a high quality assembly. We usually use human GRCh38 for all species except the rodents where we would use mouse GRCm39. You can choose a different species for the source. You will need to check the quality of the assembly and the quality of the annotation.&lt;/p&gt; &#xA;&lt;p&gt;You will need to use a Compara master database.&lt;/p&gt; &#xA;&lt;p&gt;It is possible to avoid running this part by setting &lt;code&gt;skip_projection&lt;/code&gt; to 1.&lt;/p&gt; &#xA;&lt;p&gt;The results of this sub pipeline will be used for the projection sub pipeline.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;The pipeline provides the best result when the two species are not too divergent. For example, it does not provide good information when used with any fish if human is the source.&lt;/p&gt; &#xA;&lt;h3&gt;Projecting a high quality annotation on the species of interest&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will use the whole genome alignment of the LastZ sub pipeline to project the high quality source annotation onto the genome of interest. It will also use CESAR2.0 to project the high quality source annotation onto the genome of interest. Then we select the best model for each projected gene based on coverage and identity. The selected model can be used later in the transcript selection sub pipeline.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;proj_sel database to transcript selection sub pipeline &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Aligning proteins from related species&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;We align a controlled set of proteins grouped by species/family/clade depending on the clade of the species of interest using GenBlast. The models are classified based on their coverage and identity. The models with the highest coverage and the highest identity will be preferably used when selecting the transcripts.&lt;/p&gt; &#xA;&lt;p&gt;The controlled sets are found in modules/Bio/EnsEMBL/Analysis/Hive/Config/UniProtCladeDownloadStatic.pm&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;genblast_nr database to transcript selection sub pipeline &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Aligning proteins and cDNAs from the species of interest&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;We align the cDNAs specific to the species onto the genome using Exonerate. First we align all possible sequences, they will be used later to add UTRs. Then we only align the full length cDNAs using the first alignments to reduce the search space and using the reported CDS start and CDS end. When internal stops are found, the model is removed unless we found only one. In this case we replace the stop codon with an intron to avoid any assembly error to interfere.&lt;/p&gt; &#xA;&lt;p&gt;We also align the species specific proteins retrieved from UniProt with protein existence (PE) level 1 and 2 (protein evidence and transcript evidence). We use PMatch to reduce the search space then Exonerate and GeneWise in two different analyses. We ran a final Exonerate on the whole genome as back up.&lt;/p&gt; &#xA;&lt;p&gt;We align the species specific seleno proteins and mark the internal stop with an attribute&lt;/p&gt; &#xA;&lt;p&gt;The final step will select the best model for each protein of a locus with the following ranking:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cdna&lt;/li&gt; &#xA; &lt;li&gt;seleno protein&lt;/li&gt; &#xA; &lt;li&gt;edited cdna&lt;/li&gt; &#xA; &lt;li&gt;protein&lt;/li&gt; &#xA; &lt;li&gt;backup protein&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;bt database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;cdna database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;run_utr_addition&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_10GB&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_30GB&lt;/li&gt; &#xA;   &lt;li&gt;utr_memory_failover&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Creating an IG/TR annotation&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It uses a known set of human IG/TR genes and align them using GenBlast with customised parameters to restrict the size of the introns. It collapses the models created at each locus&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;igtr database to transcript selection sub pipeline &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Generating a short non coding gene set&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It aligns sequence from the RFam database to the genome using Blast and uses the Infernal software suite to assess the short non coding genes.&lt;/p&gt; &#xA;&lt;p&gt;Sequences from miRBase are aligned to the genome and are filtered using a&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;genebuild-mirna&lt;/code&gt; virtual environment is needed but do not need to be activacted as we use the full path to Python.&lt;/p&gt; &#xA;&lt;h3&gt;Short read alignments&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;We use STAR to align the set of Illumina short reads downloaded. Then we generate models with Scallop based on the STAR alignments. We determine the coding potential of the models using blast on a sub set of UniProt proteins, PE 1 and 2. For non vertebrate we also use CPC2 and Samba as UniProt does not have enough data. The models are collapsed by loci for the transcript selection pipeline.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;rnalayer_nr database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;rnalayer_nr database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;run_utr_addition&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_10GB&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_30GB&lt;/li&gt; &#xA;   &lt;li&gt;utr_memory_failover&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;rnalayer_nr database to homology rnaseq sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;genblast_rnaseq_support&lt;/li&gt; &#xA;   &lt;li&gt;genblast_rnaseq_support_himem&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;scallop_blast database to main pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;initialise_rnaseq&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;The fastq files downloaded will be deleted to free disk space as it can easily be above the terabyte&lt;/p&gt; &#xA;&lt;h3&gt;Homology intron check&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will use all the splice sites found by the short read alignment step and compare them with the introns found in the protein models from GenBlast. The models will be ranked based on the completeness of the intron support.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;gb_rnaseq_nr database to transcript selection sub pipeline &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;It is not run when there is no short read data&lt;/p&gt; &#xA;&lt;h3&gt;Long read alignments&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;We use Minimap2 to align the long reads to the genome and the resulting models are collapsed to help reduce the number of isoforms when there is a slight difference in final exons for example. It also try to find the correct canonical splice sites using the redundancy. The default data type is PacBio but ONT can be used too.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;lrfinal database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;create_toplevel_slices&lt;/li&gt; &#xA;   &lt;li&gt;split_slices_on_intergenic&lt;/li&gt; &#xA;   &lt;li&gt;layer_annotation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;lrfinal database to transcript selection sub pipeline&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;run_utr_addition&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_10GB&lt;/li&gt; &#xA;   &lt;li&gt;run_utr_addition_30GB&lt;/li&gt; &#xA;   &lt;li&gt;utr_memory_failover&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;The fastq files downloaded will be deleted to free disk space as it can easily be above the terabyte&lt;/p&gt; &#xA;&lt;h3&gt;Transcript selection&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will look at all the protein coding data generated by sub pipelines and using a layer mechanism it will decide which model to use for each loci. It will also add the UTR regions to the models based on the transcriptomic data sets; cDNA, RNA-seq and long reads. It will trim the UTR regions when necessary.&lt;/p&gt; &#xA;&lt;p&gt;It will look for pseudogenes. The criterion are single exon models which have a good blast hit with a multi exon model or if the model is within an LTR region. Multi exon models with very short introns are also seen as pseudogenes.&lt;/p&gt; &#xA;&lt;p&gt;Long non coding RNA will be labelled but we need transcriptomic data.&lt;/p&gt; &#xA;&lt;p&gt;Short non coding RNA will be copied over unless they overlap protein coding genes.&lt;/p&gt; &#xA;&lt;p&gt;Some cleaning is done to remove low quality models which haven&#39;t been labelled as pseudogenes or lncRNA or when more information was needed about the gene set.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h3&gt;Gene set finalisation&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;We copy the final gene set into the core database while making a last check for readthroughs and bad UTRs.&lt;/p&gt; &#xA;&lt;p&gt;The stable id needs to be generated, either with the simple script when it&#39;s a new species or with the stable id mapping pipeline when it is an update to the assembly. If it is an update to the assembly, the &lt;code&gt;mapping_required&lt;/code&gt; parameter of run_stable_ids should be set to 1. Once the mapping has been done you can add a &lt;code&gt;skip_analysis&lt;/code&gt; parameter to the same analysis and set it to 1 to restart the pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Then the database is prepared to be ready for handover.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;The external db ids of the supporting evidences are set using a script which when it breaks can brak many things. So if the analysis load_external_db_ids_and_optimise_af_tables fails, you have to be very careful rerunning the job.&lt;/p&gt; &#xA;&lt;p&gt;The keys of the meta table are set for RapidRelease. If you want to handover for the Main website, you will need to manually do some tweaks&lt;/p&gt; &#xA;&lt;h3&gt;_otherfeatures_ database creation&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will create an empty database from the core database and copy the genes from the following databases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cdna&lt;/li&gt; &#xA; &lt;li&gt;refseq&lt;/li&gt; &#xA; &lt;li&gt;lrinitial&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The database is then prepared for handover.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;When this pipeline is started after a successful core handover and the hopefully unlikely modification have been done on the database from the pipeline, the meta table should not need any updates&lt;/p&gt; &#xA;&lt;h3&gt;_rnaseq_ database creation&lt;/h3&gt; &#xA;&lt;h4&gt;What it does&lt;/h4&gt; &#xA;&lt;p&gt;It will copy the database provided by the short read pipeline and sort the dna_align_feature table which contains the introns. This is necessary to speed up the MySQL queries as the table contains millions of rows. It will the assign the external db id for protein evidences and make sure the data_file table which contains the filename of the BAM/BigWig files. It will upload the web_data and analyses descriptions to the Production database for the configuration matrix to work as expected on the website. It will generate BigWig files from the tissue BAM files and generate a README and a md5sum file for the FTP. And finally prepare the database for handover.&lt;/p&gt; &#xA;&lt;h4&gt;Notifications&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;Caveats&lt;/h4&gt; &#xA;&lt;p&gt;When this pipeline is started after a successful core handover and the hopefully unlikely modification have been done on the database from the pipeline, the meta table should not need any updates&lt;/p&gt;</summary>
  </entry>
</feed>