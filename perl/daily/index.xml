<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Perl Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-14T01:37:34Z</updated>
  <subtitle>Daily Trending of Perl in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rabbitXIII/cmsc423-proj3</title>
    <updated>2022-09-14T01:37:34Z</updated>
    <id>tag:github.com,2022-09-14:/rabbitXIII/cmsc423-proj3</id>
    <link href="https://github.com/rabbitXIII/cmsc423-proj3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;project 3 for bioinformatics algorithms class&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Project 3 Rohit Gopal&lt;/p&gt; &#xA;&lt;p&gt;CMSC423 0101&lt;/p&gt; &#xA;&lt;p&gt;The files included for this submission are: proj3_rgopal.pl&lt;/p&gt; &#xA;&lt;p&gt;proj3_rgopal.pl This is my submission for the project. It uses BioPerl for reading in the files and then performs a local alignment on the first sequence in each file passed in as arguments.&lt;/p&gt; &#xA;&lt;p&gt;To Run the Project:&lt;/p&gt; &#xA;&lt;p&gt;./proj3_rgopal.pl &#xA; &lt;fasta file&gt; &#xA;  &lt;fasta file&gt; &#xA;   &lt;hoxd2 file&gt;&#xA;     &amp;lt;gap_open&amp;gt; &amp;lt;gap_extend&amp;gt;&#xA;   &lt;/hoxd2&gt;&#xA;  &lt;/fasta&gt;&#xA; &lt;/fasta&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project reads in the HOXD2 Format.&lt;/p&gt; &#xA;&lt;p&gt;The output is to stdout, and assumes that the naming schema is&lt;/p&gt; &#xA;&lt;p&gt;seq1_id start_index &#xA; &lt;basepairs&gt;&#xA;   end_index &#xA;  &lt;alignment&gt;&#xA;    seq2_id start_index &#xA;   &lt;basepairs&gt;&#xA;     end_index&#xA;   &lt;/basepairs&gt;&#xA;  &lt;/alignment&gt;&#xA; &lt;/basepairs&gt;&lt;/p&gt; &#xA;&lt;p&gt;where seq1_id is the id for the first sequence in the first fasta file and seq2_id is the corresponding id for the second sequence. The output should only be up to 80 characters in width.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>brentgwalker/spider.io-Top-Websites-Test</title>
    <updated>2022-09-14T01:37:34Z</updated>
    <id>tag:github.com,2022-09-14:/brentgwalker/spider.io-Top-Websites-Test</id>
    <link href="https://github.com/brentgwalker/spider.io-Top-Websites-Test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A script to determine potentially nefarious sites linked to in top 100,000 sites.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This project is a response to Problem 1 from the spider.io test questions.&lt;/p&gt; &#xA;&lt;p&gt;The project consists of two pieces: the main perl script (&#34;test1.pl&#34;), together with an associated module, containing various subroutines (&#34;Test1.pm&#34;).&lt;/p&gt; &#xA;&lt;p&gt;The script downloads the daily list of the top 1,000,000 websites generated by Alexa.com from the link: &lt;a href=&#34;http://s3.amazonaws.com/alexa-static/top-1m.csv.zip&#34;&gt;http://s3.amazonaws.com/alexa-static/top-1m.csv.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It uncompresses this, and extracts the top 100,000 sites.&lt;/p&gt; &#xA;&lt;p&gt;It then uses the database of &#34;bugs&#34; maintained by the developers of the Ghostery plugin (&lt;a href=&#34;http://www.ghostery.com&#34;&gt;www.ghostery.com&lt;/a&gt;). This database is available at: &lt;a href=&#34;http://www.ghostery.com/update/all?format=json&#34;&gt;http://www.ghostery.com/update/all?format=json&lt;/a&gt;. This file is downloaded to &#34;bugs.js&#34; in the local directory.&lt;/p&gt; &#xA;&lt;p&gt;The bugs file is parsed for the list of bugs currently known to Ghostery. For each bug, Ghostery has an information page located at a link of the form: &lt;a href=&#34;http://www.ghostery.com/apps/Bug_Name&#34;&gt;http://www.ghostery.com/apps/Bug_Name&lt;/a&gt;. These data files contain a listing of example websites on which the bug has been found. The script goes through the list of bugs from the Ghostery database, and for each downloads the Ghostery information page. It then parses the downloaded information page to extract the list of example sites for each bug. For each bug, the examples listed by Ghostery are then cross-referenced with the list of top 100,000 sites. The bugs which have an example appearing in the top sites list are then printed.&lt;/p&gt; &#xA;&lt;p&gt;The outputs of the code are two files: &#34;top100000.txt&#34;, which simply contains a list of the top 100,000 sites extracted from the Alexa csv file; and &#34;bugs_in_top_100000_websites.txt&#34;, which is a list of the bugs about which Ghostery knows that appear on the top 100,000 sites.&lt;/p&gt; &#xA;&lt;p&gt;The script will not overwrite existing copies of the top 1 million sites &#34;top-1m.csv.zip&#34;, the Ghostery bugs database (&#34;bug.js&#34;), or any of the downloaded bug webpages (&#34;Bug_Name.html&#34;). All these files are left behind after running the script.&lt;/p&gt; &#xA;&lt;p&gt;Brent Walker, Feb. 2012.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jonhoo/netcfg-wifi-scan</title>
    <updated>2022-09-14T01:37:34Z</updated>
    <id>tag:github.com,2022-09-14:/jonhoo/netcfg-wifi-scan</id>
    <link href="https://github.com/jonhoo/netcfg-wifi-scan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scan for and create wireless profiles for NetCFG&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Connecting to wireless networks is a bit of a hassle at the moment with netcfg. First, you have to run iwlist scan, then analyze its output, copy one of the /etc/network.d/examples/wireless-* files into /etc/network.d and then modify it to fit your connection. This becomes cumbersome when you&#39;ve done it a couple of times, so I&#39;ve written a fairly simple Perl script that does all the heavy lifting for you. It even shows a pretty list of all available networks, with signal level and authentication type. No dependencies apart from perl and iwlist (and netcfg to do the connecting).&lt;/p&gt;</summary>
  </entry>
</feed>