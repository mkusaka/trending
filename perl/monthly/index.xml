<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Perl Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-01T02:07:09Z</updated>
  <subtitle>Monthly Trending of Perl in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ttfnrob/SatelliteTracking</title>
    <updated>2024-02-01T02:07:09Z</updated>
    <id>tag:github.com,2024-02-01:/ttfnrob/SatelliteTracking</id>
    <link href="https://github.com/ttfnrob/SatelliteTracking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tracking satellites using TLEs and Perl (KML/Google Earth output)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Tracking Satellites on Google Earth&lt;/h2&gt; &#xA;&lt;p&gt;Tracking satellites using TLEs and Perl (KML/Google Earth output). This single Perl file is designed to sit on your server and by default returns a KML file tracking the International Space Station over the next 2 hours. The default TLW source file is at &lt;a href=&#34;http://celestrak.com/NORAD/elements/visual.txt&#34;&gt;http://celestrak.com/NORAD/elements/visual.txt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;File useage&lt;/h3&gt; &#xA;&lt;p&gt;With a few simple arguments the file can slightly alter the returned output in KML. Defaults shown in brackets.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;url:&lt;/em&gt; URL of TLW data file (&lt;a href=&#34;http://celestrak.com/NORAD/elements/visual.txt&#34;&gt;http://celestrak.com/NORAD/elements/visual.txt&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;id:&lt;/em&gt; Comma-sperated list of TLE ids from the file (25544)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;hor&lt;/em&gt; Boolean Y or N to show satellite&#39;s horizon (Y)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;path&lt;/em&gt; Integer number of hours to shown future path of satellite (2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;ex&lt;/em&gt; Boolean Y or N to show line connecting satellite to ground (N)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;icon&lt;/em&gt; URL path to icon for the satellite in Google Earth (&lt;a href=&#34;http://resources.orbitingfrog.com/ID_OF_SATELLITE.png&#34;&gt;http://resources.orbitingfrog.com/ID_OF_SATELLITE.png&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Perl Packages Required&lt;/h3&gt; &#xA;&lt;p&gt;CGI, Math, POSIX, LWP and Astro::Coord&lt;/p&gt; &#xA;&lt;p&gt;It was written in Perl because it utilises the &lt;em&gt;Astro-satpass&lt;/em&gt; package by Tom Wyant (found at &lt;a href=&#34;http://search.cpan.org/dist/Astro-satpass/&#34;&gt;http://search.cpan.org/dist/Astro-satpass/&lt;/a&gt;). Perl is not my favorutie package but this is a speedy library that does most of the grunt work. Big thank you to Tom for all his hard work. :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>brentgwalker/spider.io-Top-Websites-Test</title>
    <updated>2024-02-01T02:07:09Z</updated>
    <id>tag:github.com,2024-02-01:/brentgwalker/spider.io-Top-Websites-Test</id>
    <link href="https://github.com/brentgwalker/spider.io-Top-Websites-Test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A script to determine potentially nefarious sites linked to in top 100,000 sites.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This project is a response to Problem 1 from the spider.io test questions.&lt;/p&gt; &#xA;&lt;p&gt;The project consists of two pieces: the main perl script (&#34;test1.pl&#34;), together with an associated module, containing various subroutines (&#34;Test1.pm&#34;).&lt;/p&gt; &#xA;&lt;p&gt;The script downloads the daily list of the top 1,000,000 websites generated by Alexa.com from the link: &lt;a href=&#34;http://s3.amazonaws.com/alexa-static/top-1m.csv.zip&#34;&gt;http://s3.amazonaws.com/alexa-static/top-1m.csv.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It uncompresses this, and extracts the top 100,000 sites.&lt;/p&gt; &#xA;&lt;p&gt;It then uses the database of &#34;bugs&#34; maintained by the developers of the Ghostery plugin (&lt;a href=&#34;http://www.ghostery.com&#34;&gt;www.ghostery.com&lt;/a&gt;). This database is available at: &lt;a href=&#34;http://www.ghostery.com/update/all?format=json&#34;&gt;http://www.ghostery.com/update/all?format=json&lt;/a&gt;. This file is downloaded to &#34;bugs.js&#34; in the local directory.&lt;/p&gt; &#xA;&lt;p&gt;The bugs file is parsed for the list of bugs currently known to Ghostery. For each bug, Ghostery has an information page located at a link of the form: &lt;a href=&#34;http://www.ghostery.com/apps/Bug_Name&#34;&gt;http://www.ghostery.com/apps/Bug_Name&lt;/a&gt;. These data files contain a listing of example websites on which the bug has been found. The script goes through the list of bugs from the Ghostery database, and for each downloads the Ghostery information page. It then parses the downloaded information page to extract the list of example sites for each bug. For each bug, the examples listed by Ghostery are then cross-referenced with the list of top 100,000 sites. The bugs which have an example appearing in the top sites list are then printed.&lt;/p&gt; &#xA;&lt;p&gt;The outputs of the code are two files: &#34;top100000.txt&#34;, which simply contains a list of the top 100,000 sites extracted from the Alexa csv file; and &#34;bugs_in_top_100000_websites.txt&#34;, which is a list of the bugs about which Ghostery knows that appear on the top 100,000 sites.&lt;/p&gt; &#xA;&lt;p&gt;The script will not overwrite existing copies of the top 1 million sites &#34;top-1m.csv.zip&#34;, the Ghostery bugs database (&#34;bug.js&#34;), or any of the downloaded bug webpages (&#34;Bug_Name.html&#34;). All these files are left behind after running the script.&lt;/p&gt; &#xA;&lt;p&gt;Brent Walker, Feb. 2012.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lockefox/EVE_appreview</title>
    <updated>2024-02-01T02:07:09Z</updated>
    <id>tag:github.com,2024-02-01:/lockefox/EVE_appreview</id>
    <link href="https://github.com/lockefox/EVE_appreview" rel="alternate"></link>
    <summary type="html">&lt;p&gt;API cruncher for applicants trying to join an EVE Online corp&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EVE_appreview&lt;/h1&gt; &#xA;&lt;p&gt;API cruncher for applicants trying to join an EVE Online corp&lt;/p&gt; &#xA;&lt;p&gt;APIcrunch.pl -id=### -vcode=hashcode Takes EVE Online API key and crunches data, printing out a report for the applicant. Core application in suite. Downloads local copies of relevant personal APIs for data crunching.&lt;/p&gt; &#xA;&lt;p&gt;CORPstanding.pl Controls/downloads corp standing list for flagging connections&lt;/p&gt; &#xA;&lt;p&gt;Wallet_crunch.pl Follow the money. Checks donations (and private contracts, TBD). Also flags against standing list for critical issues&lt;/p&gt; &#xA;&lt;p&gt;Standing_crunch.pl Processes standing/mail info. Flags messages according to corp standing flags.&lt;/p&gt; &#xA;&lt;p&gt;Skill_crunch.pl Builds report of relevant skills. Custom certificate program. Also breaks down PVP/Industry/Other SP counts&lt;/p&gt; &#xA;&lt;p&gt;Ship_crunch.pl Crunches data against doctrine ships. Builds report for &#34;can fly&#34; &#34;can fly well&#34;&lt;/p&gt; &#xA;&lt;h1&gt;Control Files&lt;/h1&gt; &#xA;&lt;p&gt;cert.xml XML file of custom certifications. Completely customizable for various types of corp certs.&lt;br&gt; Default includes both industry and combat certs.&lt;/p&gt; &#xA;&lt;p&gt;standing.xml Standing data from crunched corpAPI data. Includes tags to be human readable.&lt;/p&gt; &#xA;&lt;p&gt;*.sql SQL files from EVE Online to do conversions to/from human readable outputs.&lt;/p&gt; &#xA;&lt;h1&gt;Sample Files&lt;/h1&gt; &#xA;&lt;p&gt;cert-sample.xml Includes comments for formatting. Basic sample rubric&lt;/p&gt; &#xA;&lt;p&gt;Lockefox.txt Sample output report&lt;/p&gt;</summary>
  </entry>
</feed>