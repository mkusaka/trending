<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-01T02:01:25Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>frappe/erpnext</title>
    <updated>2025-02-01T02:01:25Z</updated>
    <id>tag:github.com,2025-02-01:/frappe/erpnext</id>
    <link href="https://github.com/frappe/erpnext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://erpnext.com&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/erpnext.svg?sanitize=true&#34; alt=&#34;ERPNext Logo&#34; height=&#34;80px&#34; width=&#34;80xp&#34;&gt; &lt;/a&gt; &#xA; &lt;h2&gt;ERPNext&lt;/h2&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;Powerful, Intuitive and Open-Source ERP&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml&#34;&gt;&lt;img src=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/frappe/erpnext-worker&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true&#34; alt=&#34;docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/hero_image.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://erpnext-demo.frappe.cloud/app/home&#34;&gt;Live Demo&lt;/a&gt; - &#xA; &lt;a href=&#34;https://erpnext.com&#34;&gt;Website&lt;/a&gt; - &#xA; &lt;a href=&#34;https://docs.erpnext.com&#34;&gt;Documentation&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ERPNext&lt;/h2&gt; &#xA;&lt;p&gt;100% Open-Source ERP system to help you run your business.&lt;/p&gt; &#xA;&lt;h3&gt;Motivation&lt;/h3&gt; &#xA;&lt;p&gt;Running a business is a complex task - handling invoices, tracking stock, managing personnel and even more ad-hoc activities. In a market where software is sold separately to manage each of these tasks, ERPNext does all of the above and more, for free.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accounting&lt;/strong&gt;: All the tools you need to manage cash flow in one place, right from recording transactions to summarizing and analyzing financial reports.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Order Management&lt;/strong&gt;: Track inventory levels, replenish stock, and manage sales orders, customers, suppliers, shipments, deliverables, and order fulfillment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Manufacturing&lt;/strong&gt;: Simplifies the production cycle, helps track material consumption, exhibits capacity planning, handles subcontracting, and more!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asset Management&lt;/strong&gt;: From purchase to perishment, IT infrastructure to equipment. Cover every branch of your organization, all in one centralized system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Projects&lt;/strong&gt;: Delivery both internal and external Projects on time, budget and Profitability. Track tasks, timesheets, and issues by project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;img src=&#34;https://erpnext.com/files/v16_bom.png&#34;&gt; &#xA; &lt;img src=&#34;https://erpnext.com/files/v16_stock_summary.png&#34;&gt; &#xA; &lt;img src=&#34;https://erpnext.com/files/v16_job_card.png&#34;&gt; &#xA; &lt;img src=&#34;https://erpnext.com/files/v16_tasks.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Under the Hood&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/frappe/frappe&#34;&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/frappe/frappe-ui&#34;&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Production Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Managed Hosting&lt;/h3&gt; &#xA;&lt;p&gt;You can try &lt;a href=&#34;https://frappecloud.com&#34;&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href=&#34;https://github.com/frappe/press&#34;&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; &#xA;&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://erpnext-demo.frappe.cloud/app/home&#34; target=&#34;_blank&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://frappe.io/files/try-on-fc-white.png&#34;&gt; &#xA;   &lt;img src=&#34;https://frappe.io/files/try-on-fc-black.png&#34; alt=&#34;Try on Frappe Cloud&#34; height=&#34;28&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Self-Hosted&lt;/h3&gt; &#xA;&lt;h4&gt;Docker&lt;/h4&gt; &#xA;&lt;p&gt;Prerequisites: docker, docker-compose, git. Refer &lt;a href=&#34;https://docs.docker.com&#34;&gt;Docker Documentation&lt;/a&gt; for more details on Docker setup.&lt;/p&gt; &#xA;&lt;p&gt;Run following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/frappe_docker&#xA;cd frappe_docker&#xA;docker compose -f pwd.yml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After a couple of minutes, site should be accessible on your localhost port: 8080. Use below default login credentials to access the site.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Username: Administrator&lt;/li&gt; &#xA; &lt;li&gt;Password: admin&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/frappe/frappe_docker?tab=readme-ov-file#to-run-on-arm64-architecture-follow-this-instructions&#34;&gt;Frappe Docker&lt;/a&gt; for ARM based docker setup.&lt;/p&gt; &#xA;&lt;h2&gt;Development Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Manual Install&lt;/h3&gt; &#xA;&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href=&#34;https://github.com/frappe/bench&#34;&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;New passwords will be created for the ERPNext &#34;Administrator&#34; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;p&gt;To setup the repository locally follow the steps mentioned below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup bench by following the &lt;a href=&#34;https://frappeframework.com/docs/user/en/installation&#34;&gt;Installation Steps&lt;/a&gt; and start the server&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bench start&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In a separate terminal window, run the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a new site&#xA;bench new-site erpnext.dev&#xA;&#xA;# Map your site to localhost&#xA;bench --site erpnext.dev add-to-hosts&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the ERPNext app and install it&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Get the ERPNext app&#xA;bench get-app https://github.com/frappe/erpnext&#xA;&#xA;# Install the app&#xA;bench --site erpnext.dev install-app erpnext&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the URL &lt;code&gt;http://erpnext.dev:8000/app&lt;/code&gt; in your browser, you should see the app running&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Learning and community&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://frappe.school&#34;&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.erpnext.com/&#34;&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discuss.erpnext.com/&#34;&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext_public.t.me&#34;&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Issue-Guidelines&#34;&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/security&#34;&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Contribution-Guidelines&#34;&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; &#xA;&lt;p&gt;Please read our &lt;a href=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md&#34;&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;padding-top: 0.75rem;&#34;&gt; &#xA; &lt;a href=&#34;https://frappe.io&#34; target=&#34;_blank&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://frappe.io/files/Frappe-white.png&#34;&gt; &#xA;   &lt;img src=&#34;https://frappe.io/files/Frappe-black.png&#34; alt=&#34;Frappe Technologies&#34; height=&#34;28&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>browser-use/browser-use</title>
    <updated>2025-02-01T02:01:25Z</updated>
    <id>tag:github.com,2025-02-01:/browser-use/browser-use</id>
    <link href="https://github.com/browser-use/browser-use" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make websites accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./static/browser-use-dark.png&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;./static/browser-use.png&#34;&gt; &#xA; &lt;img alt=&#34;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&#34; src=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png&#34; width=&#34;full&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Enable AI to control your browser ü§ñ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gregpr07/browser-use/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gregpr07/browser-use?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue&#34; alt=&#34;Cloud&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üåê Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; &#xA;&lt;p&gt;üí° See what others are building and share your projects in our &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt; - we&#39;d love to see what you create!&lt;/p&gt; &#xA;&lt;p&gt;üå©Ô∏è Skip the setup - try our hosted version for instant browser automation! &lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;Try it now&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install browser-use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;install playwright:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spin up your agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain_openai import ChatOpenAI&#xA;from browser_use import Agent&#xA;import asyncio&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;async def main():&#xA;    agent = Agent(&#xA;        task=&#34;Go to Reddit, search for &#39;browser-use&#39;, click on the first post and return the first comment.&#34;,&#xA;        llm=ChatOpenAI(model=&#34;gpt-4o&#34;),&#xA;    )&#xA;    result = await agent.run()&#xA;    print(result)&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other settings, models, and more, check out the &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;documentation üìï&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Test with UI&lt;/h3&gt; &#xA;&lt;p&gt;You can test &lt;a href=&#34;https://github.com/browser-use/web-ui&#34;&gt;browser-use with a UI repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or simply run the gradio example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;uv pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/ui/gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py&#34;&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa&#34; alt=&#34;Letter to Papa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py&#34;&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&#34;&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Find flights on kayak.com from Zurich to Beijing from 25.12.2024 to 02.02.2025.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/ea605d4a-90e6-481e-a569-f0e0db7e6390&#34; alt=&#34;flight search 8x 10fps&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py&#34;&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&#34;&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;For more examples see the &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/examples&#34;&gt;examples&lt;/a&gt; folder or join the &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt; and show off your project.&lt;/p&gt; &#xA;&lt;h1&gt;Vision&lt;/h1&gt; &#xA;&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve memory management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance planning capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve self-correction&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tune the model for better performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create datasets for complex tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sandbox browser-use for specific websites&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement deterministic script rerun with LLM fallback&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cloud-hosted version&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add stop/pause functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve authentication handling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce token consumption&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement long-term memory&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Handle repetitive tasks reliably&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Third-party integrations (Slack, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Include more interactive elements&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Human-in-the-loop execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Benchmark various models against each other&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Let the user record a workflow and browser-use will execute it&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the generated GIF quality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create various demos for tutorial execution, job application, QA testing, social media, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Local Setup&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about the library, check out the &lt;a href=&#34;https://docs.browser-use.com/development/local-setup&#34;&gt;local setup üìï&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cooperations&lt;/h2&gt; &#xA;&lt;p&gt;We are forming a commission to define best practices for UI/UX design for browser agents. Together, we&#39;re exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.&lt;/p&gt; &#xA;&lt;p&gt;Email &lt;a href=&#34;mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A&#34;&gt;Toby&lt;/a&gt; to apply for a seat on the committee.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{browser_use2024,&#xA;  author = {M√ºller, Magnus and ≈Ωuniƒç, Gregor},&#xA;  title = {Browser Use: Enable AI to control your browser},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  url = {https://github.com/browser-use/browser-use}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/402b2129-b6ac-44d3-a217-01aea3277dce&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Made with ‚ù§Ô∏è in Zurich and San Francisco &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/Janus</title>
    <updated>2025-02-01T02:01:25Z</updated>
    <id>tag:github.com,2025-02-01:/deepseek-ai/Janus</id>
    <link href="https://github.com/deepseek-ai/Janus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üöÄ Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt;  &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;images/qr.jpeg&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#2-model-download&#34;&gt;&lt;b&gt;üì• Model Download&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#3-quick-start&#34;&gt;&lt;b&gt;‚ö° Quick Start&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#4-license&#34;&gt;&lt;b&gt;üìú License&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-citation&#34;&gt;&lt;b&gt;üìñ Citation&lt;/b&gt;&lt;/a&gt; &lt;br&gt; &#xA; &lt;!-- üìÑ Paper Link (&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) | --&gt; ü§ó Online Demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;&lt;b&gt;Janus-Pro-7B&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2025.01.27&lt;/strong&gt;: Janus-Pro is released, an advanced version of Janus, improving both multimodal understanding and visual generation significantly. See &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.11.13&lt;/strong&gt;: JanusFlow is released, a new unified model with rectified flow for image generation. See &lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;demo&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepseek-ai/Janus?tab=readme-ov-file#janusflow&#34;&gt;usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.23&lt;/strong&gt;: Evaluation code for reproducing the multimodal understanding results from the paper has been added to VLMEvalKit. Please refer to &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/pull/541&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.20&lt;/strong&gt;: (1) Fix a bug in &lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B/blob/main/tokenizer_config.json&#34;&gt;tokenizer_config.json&lt;/a&gt;. The previous version caused classifier-free guidance to not function properly, resulting in relatively poor visual generation quality. (2) Release Gradio demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;online demo&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#gradio-demo&#34;&gt;local&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;&lt;b&gt;Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus-Pro&lt;/strong&gt; is an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_januspro.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus&lt;/strong&gt; is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;&lt;b&gt;JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JanusFlow&lt;/strong&gt; introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_janusflow.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;2. Model Download&lt;/h2&gt; &#xA;&lt;p&gt;We release Janus to the public to support a broader and more diverse range of research within both academic and commercial communities. Please note that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JanusFlow-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/JanusFlow-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-1B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-1B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-7B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;3. Quick Start&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus-Pro&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: f&#34;&amp;lt;image_placeholder&amp;gt;\n{question}&#34;,&#xA;        &#34;images&#34;: [image],&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_januspro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA; &lt;h3&gt;FastAPI Demo&lt;/h3&gt; &#xA; &lt;p&gt;It&#39;s easy to run a FastAPI server to host an API server running the same functions as gradio.&lt;/p&gt; &#xA; &lt;p&gt;To start FastAPI server, run the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To test the server, you can open another terminal and run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_client.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;JanusFlow&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;pip install diffusers[torch]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;ü§ó Huggingface Online Demo&lt;/h3&gt; &#xA; &lt;p&gt;Check out the demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;import torchvision&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;from diffusers.models import AutoencoderKL&#xA;# remember to use bfloat16 dtype, this vae doesn&#39;t work with fp16&#xA;vae = AutoencoderKL.from_pretrained(&#34;stabilityai/sdxl-vae&#34;)&#xA;vae = vae.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_gen_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    cfg_weight: float = 5.0,&#xA;    num_inference_steps: int = 30,&#xA;    batchsize: int = 5&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;    &#xA;    tokens = torch.stack([input_ids] * 2 * batchsize).cuda()&#xA;    tokens[batchsize:, 1:] = vl_chat_processor.pad_id&#xA;    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    # we remove the last &amp;lt;bog&amp;gt; token and replace it with t_emb later&#xA;    inputs_embeds = inputs_embeds[:, :-1, :] &#xA;    &#xA;    # generate with rectified flow ode&#xA;    # step 1: encode with vision_gen_enc&#xA;    z = torch.randn((batchsize, 4, 48, 48), dtype=torch.bfloat16).cuda()&#xA;    &#xA;    dt = 1.0 / num_inference_steps&#xA;    dt = torch.zeros_like(z).cuda().to(torch.bfloat16) + dt&#xA;    &#xA;    # step 2: run ode&#xA;    attention_mask = torch.ones((2*batchsize, inputs_embeds.shape[1]+577)).to(vl_gpt.device)&#xA;    attention_mask[batchsize:, 1:inputs_embeds.shape[1]] = 0&#xA;    attention_mask = attention_mask.int()&#xA;    for step in range(num_inference_steps):&#xA;        # prepare inputs for the llm&#xA;        z_input = torch.cat([z, z], dim=0) # for cfg&#xA;        t = step / num_inference_steps * 1000.&#xA;        t = torch.tensor([t] * z_input.shape[0]).to(dt)&#xA;        z_enc = vl_gpt.vision_gen_enc_model(z_input, t)&#xA;        z_emb, t_emb, hs = z_enc[0], z_enc[1], z_enc[2]&#xA;        z_emb = z_emb.view(z_emb.shape[0], z_emb.shape[1], -1).permute(0, 2, 1)&#xA;        z_emb = vl_gpt.vision_gen_enc_aligner(z_emb)&#xA;        llm_emb = torch.cat([inputs_embeds, t_emb.unsqueeze(1), z_emb], dim=1)&#xA;&#xA;        # input to the llm&#xA;        # we apply attention mask for CFG: 1 for tokens that are not masked, 0 for tokens that are masked.&#xA;        if step == 0:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=None)&#xA;            past_key_values = []&#xA;            for kv_cache in past_key_values:&#xA;                k, v = kv_cache[0], kv_cache[1]&#xA;                past_key_values.append((k[:, :, :inputs_embeds.shape[1], :], v[:, :, :inputs_embeds.shape[1], :]))&#xA;            past_key_values = tuple(past_key_values)&#xA;        else:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=past_key_values)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        # transform hidden_states back to v&#xA;        hidden_states = vl_gpt.vision_gen_dec_aligner(vl_gpt.vision_gen_dec_aligner_norm(hidden_states[:, -576:, :]))&#xA;        hidden_states = hidden_states.reshape(z_emb.shape[0], 24, 24, 768).permute(0, 3, 1, 2)&#xA;        v = vl_gpt.vision_gen_dec_model(hidden_states, hs, t_emb)&#xA;        v_cond, v_uncond = torch.chunk(v, 2)&#xA;        v = cfg_weight * v_cond - (cfg_weight-1.) * v_uncond&#xA;        z = z + dt * v&#xA;        &#xA;    # step 3: decode with vision_gen_dec and sdxl vae&#xA;    decoded_image = vae.decode(z / vae.config.scaling_factor).sample&#xA;    &#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    save_path = os.path.join(&#39;generated_samples&#39;, &#34;img.jpg&#34;)&#xA;    torchvision.utils.save_image(decoded_image.clip_(-1.0, 1.0)*0.5+0.5, save_path)&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;    cfg_weight=2.0,&#xA;    num_inference_steps=30,&#xA;    batchsize=5&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_janusflow.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;4. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of Janus models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;DeepSeek Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;5. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2025januspro,&#xA;      title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling}, &#xA;      author={Xiaokang Chen and Zhiyu Wu and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan},&#xA;      year={2025},&#xA;}&#xA;&#xA;@article{wu2024janus,&#xA;  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},&#xA;  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},&#xA;  journal={arXiv preprint arXiv:2410.13848},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{ma2024janusflow,&#xA;      title={JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation}, &#xA;      author={Yiyang Ma and Xingchao Liu and Xiaokang Chen and Wen Liu and Chengyue Wu and Zhiyu Wu and Zizheng Pan and Zhenda Xie and Haowei Zhang and Xingkai yu and Liang Zhao and Yisong Wang and Jiaying Liu and Chong Ruan},&#xA;      journal={arXiv preprint arXiv:2411.07975},&#xA;      year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>