<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-01T01:55:00Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hacksider/Deep-Live-Cam</title>
    <updated>2024-09-01T01:55:00Z</updated>
    <id>tag:github.com,2024-09-01:/hacksider/Deep-Live-Cam</id>
    <link href="https://github.com/hacksider/Deep-Live-Cam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.&lt;/p&gt; &#xA;&lt;p&gt;The developers of this software are aware of its possible unethical applications and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media including but not limited to nudity, graphic content, sensitive material such as war footage etc. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.&lt;/p&gt; &#xA;&lt;p&gt;Users of this software are expected to use this software responsibly while abiding by local laws. If the face of a real person is being used, users are required to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.&lt;/p&gt; &#xA;&lt;h2&gt;How do I install it?&lt;/h2&gt; &#xA;&lt;h3&gt;Basic: It is more likely to work on your computer but it will also be very slow. You can follow instructions for the basic install (This usually runs via &lt;strong&gt;CPU&lt;/strong&gt;)&lt;/h3&gt; &#xA;&lt;h4&gt;1.Setup your platform&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python (3.10 recommended)&lt;/li&gt; &#xA; &lt;li&gt;pip&lt;/li&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OlNWCpFdVMA&#34;&gt;ffmpeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;visual studio 2022 runtimes (windows)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Clone Repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/hacksider/Deep-Live-Cam.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Download Models&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth&#34;&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx&#34;&gt;inswapper_128_fp16.onnx&lt;/a&gt; &lt;em&gt;(Note: Use this &lt;a href=&#34;https://github.com/facefusion/facefusion-assets/releases/download/models/inswapper_128_fp16.onnx&#34;&gt;replacement version&lt;/a&gt; if an issue occurs on your computer)&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Then put those 2 files on the &#34;&lt;strong&gt;models&lt;/strong&gt;&#34; folder&lt;/p&gt; &#xA;&lt;h4&gt;4. Install dependency&lt;/h4&gt; &#xA;&lt;p&gt;We highly recommend to work with a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For MAC OS, You have to install or upgrade python-tk package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install python-tk@3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;DONE!!! If you don&#39;t have any GPU, You should be able to run roop using &lt;code&gt;python run.py&lt;/code&gt; command. Keep in mind that while running the program for first time, it will download some models which can take time depending on your network connection.&lt;/h5&gt; &#xA;&lt;h4&gt;5. Proceed if you want to use GPU acceleration (optional)&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see the details&lt;/summary&gt; &#xA; &lt;h3&gt;CUDA Execution Provider (Nvidia)*&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;CUDA Toolkit 11.8&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-gpu&#xA;pip install onnxruntime-gpu==1.16.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;python run.py --execution-provider cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-silicon&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Silicon)&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-silicon&#xA;pip install onnxruntime-silicon==1.13.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-legacy&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Legacy)&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-coreml&#xA;pip install onnxruntime-coreml==1.13.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#directml-execution-provider-windows&#34;&gt;&lt;/a&gt;DirectML Execution Provider (Windows)&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-directml&#xA;pip install onnxruntime-directml==1.15.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;python run.py --execution-provider directml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#openvino-execution-provider-intel&#34;&gt;&lt;/a&gt;OpenVINO‚Ñ¢ Execution Provider (Intel)&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-openvino&#xA;pip install onnxruntime-openvino==1.15.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;python run.py --execution-provider openvino&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;How do I use it?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: When you run this program for the first time, it will download some models ~300MB in size.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Executing &lt;code&gt;python run.py&lt;/code&gt; command will launch this window: &lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/instruction.png&#34; alt=&#34;gui-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on &lt;code&gt;Start&lt;/code&gt;. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named &lt;code&gt;&amp;lt;video_title&amp;gt;&lt;/code&gt; where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;For the webcam mode&lt;/h2&gt; &#xA;&lt;p&gt;Just follow the clicks on the screenshot&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select a face&lt;/li&gt; &#xA; &lt;li&gt;Click live&lt;/li&gt; &#xA; &lt;li&gt;Wait for a few seconds (it takes a longer time, usually 10 to 30 seconds before the preview shows up)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just use your favorite screencapture to stream like OBS&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: In case you want to change your face, just select another picture, the preview mode will then restart (so just wait a bit).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Additional command line arguments are given below. To learn out what they do, check &lt;a href=&#34;https://github.com/s0md3v/roop/wiki/Advanced-Options&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;options:&#xA;  -h, --help                                               show this help message and exit&#xA;  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image&#xA;  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video&#xA;  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory&#xA;  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)&#xA;  --keep-fps                                               keep original fps&#xA;  --keep-audio                                             keep original audio&#xA;  --keep-frames                                            keep temporary frames&#xA;  --many-faces                                             process every face&#xA;  --nsfw-filter                                            filter the NSFW image or video&#xA;  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder&#xA;  --video-quality [0-51]                                   adjust output video quality&#xA;  --live-mirror                                            the live camera display as you see it in the front-facing camera frame&#xA;  --live-resizable                                         the live camera frame is resizable&#xA;  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB&#xA;  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)&#xA;  --execution-threads EXECUTION_THREADS                    number of execution threads&#xA;  -v, --version                                            show program&#39;s version number and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; &#xA;&lt;h3&gt;Webcam mode on Windows 11 using WSL2 Ubuntu (optional)&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see the details&lt;/summary&gt; &#xA; &lt;p&gt;If you want to use WSL2 on Windows 11 you will notice, that Ubuntu WSL2 doesn&#39;t come with USB-Webcam support in the Kernel. You need to do two things: Compile the Kernel with the right modules integrated and forward your USB Webcam from Windows to Ubuntu with the usbipd app. Here are detailed Steps:&lt;/p&gt; &#xA; &lt;p&gt;This tutorial will guide you through the process of setting up WSL2 Ubuntu with USB webcam support, rebuilding the kernel, and preparing the environment for the Deep-Live-Cam project.&lt;/p&gt; &#xA; &lt;h4&gt;1. Install WSL2 Ubuntu&lt;/h4&gt; &#xA; &lt;p&gt;Install WSL2 Ubuntu from the Microsoft Store or using PowerShell:&lt;/p&gt; &#xA; &lt;h4&gt;2. Enable USB Support in WSL2&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Install the USB/IP tool for Windows:&lt;br&gt; &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/connect-usb&#34;&gt;https://learn.microsoft.com/en-us/windows/wsl/connect-usb&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;In Windows PowerShell (as Administrator), connect your webcam to WSL:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;usbipd list  &#xA;usbipd bind --busid x-x # Replace x-x with your webcam&#39;s bus ID  &#xA;usbipd attach --wsl --busid x-x # Replace x-x with your webcam&#39;s bus ID  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You need to redo the above every time you reboot wsl or re-connect your webcam/usb device.&lt;/p&gt; &#xA; &lt;h4&gt;3. Rebuild WSL2 Ubuntu Kernel with USB and Webcam Modules&lt;/h4&gt; &#xA; &lt;p&gt;Follow these steps to rebuild the kernel:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Start with this guide: &lt;a href=&#34;https://github.com/PINTO0309/wsl2_linux_kernel_usbcam_enable_conf&#34;&gt;https://github.com/PINTO0309/wsl2_linux_kernel_usbcam_enable_conf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;When you reach the &lt;code&gt;sudo wget [github.com](http://github.com/)...PINTO0309&lt;/code&gt; step, which won&#39;t work for newer kernel versions, follow this video instead or alternatively follow the video tutorial from the beginning: &lt;a href=&#34;https://www.youtube.com/watch?v=t_YnACEPmrM&#34;&gt;https://www.youtube.com/watch?v=t_YnACEPmrM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Additional info: &lt;a href=&#34;https://askubuntu.com/questions/1413377/camera-not-working-in-cheese-in-wsl2&#34;&gt;https://askubuntu.com/questions/1413377/camera-not-working-in-cheese-in-wsl2&lt;/a&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;After rebuilding, restart WSL with the new kernel.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;4. Set Up Deep-Live-Cam Project&lt;/h4&gt; &#xA; &lt;p&gt;Within Ubuntu:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone [https://github.com/hacksider/Deep-Live-Cam](https://github.com/hacksider/Deep-Live-Cam)  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Follow the installation instructions in the repository, including cuda toolkit 11.8, make 100% sure it&#39;s not cuda toolkit 12.x.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;5. Verify and Load Kernel Modules&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Check if USB and webcam modules are built into the kernel:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;zcat /proc/config.gz | grep -i &#34;CONFIG_USB_VIDEO_CLASS&#34;  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;If modules are loadable (m), not built-in (y), check if the file exists:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /lib/modules/$(uname -r)/kernel/drivers/media/usb/uvc/  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Load the module and check for errors (optional if built-in):&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo modprobe uvcvideo  &#xA;dmesg | tail  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Verify video devices:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ls -al /dev/video*  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;6. Set Up Permissions&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Add user to video group and set permissions:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo usermod -a -G video $USER  &#xA;sudo chgrp video /dev/video0 /dev/video1  &#xA;sudo chmod 660 /dev/video0 /dev/video1  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Create a udev rule for permanent permissions:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo nano /etc/udev/rules.d/81-webcam.rules  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Add this content:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;KERNEL==&#34;video[0-9]*&#34;, GROUP=&#34;video&#34;, MODE=&#34;0660&#34;  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Reload udev rules:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo udevadm control --reload-rules &amp;amp;&amp;amp; sudo udevadm trigger  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt; &lt;p&gt;Log out and log back into your WSL session.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Start Deep-Live-Cam with &lt;code&gt;python run.py --execution-provider cuda --max-memory 8&lt;/code&gt; where 8 can be changed to the number of GB VRAM of your GPU has, minus 1-2GB. If you have a RTX3080 with 10GB I suggest adding 8GB. Leave some left for Windows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;Final Notes&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Steps 6 and 7 may be optional if the modules are built into the kernel and permissions are already set correctly.&lt;/li&gt; &#xA;  &lt;li&gt;Always ensure you&#39;re using compatible versions of CUDA, ONNX, and other dependencies.&lt;/li&gt; &#xA;  &lt;li&gt;If issues persist, consider checking the Deep-Live-Cam project&#39;s specific requirements and troubleshooting steps.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;By following these steps, you should have a WSL2 Ubuntu environment with USB webcam support ready for the Deep-Live-Cam project. If you encounter any issues, refer back to the specific error messages and troubleshooting steps provided.&lt;/p&gt; &#xA; &lt;h4&gt;Troubleshooting CUDA Issues&lt;/h4&gt; &#xA; &lt;p&gt;If you encounter this error:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;[ONNXRuntimeError] : 1 : FAIL : Failed to load library [libonnxruntime_providers_cuda.so](http://libonnxruntime_providers_cuda.so/) with error: libcufft.so.10: cannot open shared object file: No such file or directory  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install CUDA Toolkit 11.8 (ONNX 1.16.3 requires CUDA 11.x, not 12.x):&lt;br&gt; &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;https://developer.nvidia.com/cuda-11-8-0-download-archive&lt;/a&gt;&lt;br&gt; select: Linux, x86_64, WSL-Ubuntu, 2.0, deb (local)&lt;/li&gt; &#xA;  &lt;li&gt;Check CUDA version:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/usr/local/cuda/bin/nvcc --version  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt; &lt;p&gt;If the wrong version is installed, remove it completely:&lt;br&gt; &lt;a href=&#34;https://askubuntu.com/questions/530043/removing-nvidia-cuda-toolkit-and-installing-new-one&#34;&gt;https://askubuntu.com/questions/530043/removing-nvidia-cuda-toolkit-and-installing-new-one&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Install CUDA Toolkit 11.8 again &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;https://developer.nvidia.com/cuda-11-8-0-download-archive&lt;/a&gt;, select: Linux, x86_64, WSL-Ubuntu, 2.0, deb (local)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get -y install cuda-toolkit-11-8  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Want the Next Update Now?&lt;/h2&gt; &#xA;&lt;p&gt;If you want the latest and greatest build, or want to see some new great features, go to our &lt;a href=&#34;https://github.com/hacksider/Deep-Live-Cam/tree/experimental&#34;&gt;experimental branch&lt;/a&gt; and experience what the contributors have given.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support multiple faces feature&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Develop a version for web app/service&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; UI/UX enhancements for desktop app&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Speed up model loading&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Speed up real-time face swapping&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: This is an open-source project, and we‚Äôre working on it in our free time. Therefore, features, replies, bug fixes, etc., might be delayed. We hope you understand. Thanks.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;: for making video related operations easy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight&#34;&gt;deepinsight&lt;/a&gt;: for their &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt; project which provided a well-made library and models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/havok2-htwo&#34;&gt;havok2-htwo&lt;/a&gt; : for sharing the code for webcam&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GosuDRM/nsfw-roop&#34;&gt;GosuDRM&lt;/a&gt; : for uncensoring roop&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vic4key&#34;&gt;vic4key&lt;/a&gt; : For supporting/contributing on this project&lt;/li&gt; &#xA; &lt;li&gt;and &lt;a href=&#34;https://github.com/hacksider/Deep-Live-Cam/graphs/contributors&#34;&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; &#xA; &lt;li&gt;Foot Note: &lt;a href=&#34;https://github.com/hacksider/roop-cam&#34;&gt;This is originally roop-cam, see the full history of the code here.&lt;/a&gt; Please be informed that the base author of the code is &lt;a href=&#34;https://github.com/s0md3v/roop&#34;&gt;s0md3v&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>infiniflow/ragflow</title>
    <updated>2024-09-01T01:55:00Z</updated>
    <id>tag:github.com,2024-09-01:/infiniflow/ragflow</id>
    <link href="https://github.com/infiniflow/ragflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://demo.ragflow.io/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.png&#34; width=&#34;520&#34; alt=&#34;ragflow logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md&#34;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release&#34; alt=&#34;Latest Release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://demo.ragflow.io&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/Online-Demo-4e6b99&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/infiniflow/ragflow&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/docker_pull-ragflow:v0.10.0-brightgreen&#34; alt=&#34;docker pull infiniflow/ragflow:v0.10.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/raw/main/LICENSE&#34;&gt; &lt;img height=&#34;21&#34; src=&#34;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://ragflow.io/docs/dev/&#34;&gt;Document&lt;/a&gt; | &lt;a href=&#34;https://github.com/infiniflow/ragflow/issues/162&#34;&gt;Roadmap&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/infiniflowai&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/4XxujFgUN7&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://demo.ragflow.io&#34;&gt;Demo&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;üìï Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;üí° &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow&#34;&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üéÆ &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üìå &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates&#34;&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üåü &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üîé &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture&#34;&gt;System Architecture&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üé¨ &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started&#34;&gt;Get Started&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üîß &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations&#34;&gt;Configurations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-from-source&#34;&gt;Build from source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source&#34;&gt;Launch service from source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üìö &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üìú &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üèÑ &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üôå &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üí° What is RAGFlow?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ragflow.io/&#34;&gt;RAGFlow&lt;/a&gt; is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.&lt;/p&gt; &#xA;&lt;h2&gt;üéÆ Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try our demo at &lt;a href=&#34;https://demo.ragflow.io&#34;&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top:20px;margin-bottom:20px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/7248/2f6baa3e-1092-4f11-866d-36f6a9d075e5&#34; width=&#34;1200&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/12318111/b083d173-dadc-4ea9-bdeb-180d7df514eb&#34; width=&#34;1200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üî• Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-08-22 Support text to SQL statements through RAG.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-08-02 Supports GraphRAG inspired by &lt;a href=&#34;https://github.com/microsoft/graphrag&#34;&gt;graphrag&lt;/a&gt; and mind map.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-07-23 Supports audio file parsing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-07-21 Supports more LLMs (LocalAI, OpenRouter, StepFun, and Nvidia).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-07-18 Adds more components (Wikipedia, PubMed, Baidu, and Duckduckgo) to the graph.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-07-08 Supports workflow based on &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/graph/README.md&#34;&gt;Graph&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-27 Supports Markdown and Docx in the Q&amp;amp;A parsing method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-27 Supports extracting images from Docx files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-27 Supports extracting tables from Markdown files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-06 Supports &lt;a href=&#34;https://huggingface.co/papers/2310.11511&#34;&gt;Self-RAG&lt;/a&gt;, which is enabled by default in dialog settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-30 Integrates &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCE&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding&#34;&gt;BGE&lt;/a&gt; reranker models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-23 Supports &lt;a href=&#34;https://arxiv.org/html/2401.18059v1&#34;&gt;RAPTOR&lt;/a&gt; for better text retrieval.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-15 Integrates OpenAI GPT-4o.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;üç≠ &lt;strong&gt;&#34;Quality in, quality out&#34;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md&#34;&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; &#xA; &lt;li&gt;Finds &#34;needle in a data haystack&#34; of literally unlimited tokens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üç± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intelligent and explainable.&lt;/li&gt; &#xA; &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üå± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; &#xA; &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üçî &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üõÄ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; &#xA; &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; &#xA; &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; &#xA; &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîé System Architecture&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top:20px;margin-bottom:20px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485&#34; width=&#34;1000&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé¨ Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;üìù Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; &#xA; &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; &#xA; &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; &#xA; &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1 &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üöÄ Start up the server&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl vm.max_map_count&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# In this case, we set it to 262144:&#xA;$ sudo sysctl -w vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the pre-built Docker images and start up the server:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Running the following commands automatically downloads the &lt;em&gt;dev&lt;/em&gt; version RAGFlow Docker image. To download and run a specified Docker version, update &lt;code&gt;RAGFLOW_VERSION&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to the intended version, for example &lt;code&gt;RAGFLOW_VERSION=v0.10.0&lt;/code&gt;, before running the following commands.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;The core image is about 9 GB in size and may take a while to load.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker logs -f ragflow-server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    ____                 ______ __&#xA;   / __ \ ____ _ ____ _ / ____// /____  _      __&#xA;  / /_/ // __ `// __ `// /_   / // __ \| | /| / /&#xA; / _, _// /_/ // /_/ // __/  / // /_/ /| |/ |/ /&#xA;/_/ |_| \__,_/ \__, //_/    /_/ \____/ |__/|__/&#xA;              /____/&#xA;&#xA; * Running on all addresses (0.0.0.0)&#xA; * Running on http://127.0.0.1:9380&#xA; * Running on http://x.x.x.x:9380&#xA; INFO:werkzeug:Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anomaly&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;See &lt;a href=&#34;https://ragflow.io/docs/dev/llm_api_key_setup&#34;&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is now on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üîß Configurations&lt;/h2&gt; &#xA;&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;: Configures the back-end services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You must ensure that changes to the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt; file are in line with what are in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations, and you are REQUIRED to ensure that all environment settings listed in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file are aligned with the corresponding configurations in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Updates to all system configurations require a system reboot to take effect:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Build from source&lt;/h2&gt; &#xA;&lt;p&gt;To build the Docker images from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;$ docker build -t infiniflow/ragflow:dev .&#xA;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Launch service from source&lt;/h2&gt; &#xA;&lt;p&gt;To launch the service from source:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtual environment, ensuring that Anaconda or Miniconda is installed:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda create -n ragflow python=3.11.0&#xA;$ conda activate ragflow&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# If your CUDA version is higher than 12.0, run the following additional commands:&#xA;$ pip uninstall -y onnxruntime-gpu&#xA;$ pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the entry script and configure environment variables:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get the Python path:&#xA;$ which python&#xA;# Get the ragflow project path:&#xA;$ pwd&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp docker/entrypoint.sh .&#xA;$ vi entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Adjust configurations according to your actual situation (the following two export commands are newly added):&#xA;# - Assign the result of `which python` to `PY`.&#xA;# - Assign the result of `pwd` to `PYTHONPATH`.&#xA;# - Comment out `LD_LIBRARY_PATH`, if it is configured.&#xA;# - Optional: Add Hugging Face mirror.&#xA;PY=${PY}&#xA;export PYTHONPATH=${PYTHONPATH}&#xA;export HF_ENDPOINT=https://hf-mirror.com&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the third-party services (MinIO, Elasticsearch, Redis, and MySQL):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd docker&#xA;$ docker compose -f docker-compose-base.yml up -d &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the configuration files, ensuring that:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The settings in &lt;strong&gt;docker/.env&lt;/strong&gt; match those in &lt;strong&gt;conf/service_conf.yaml&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The IP addresses and ports for related services in &lt;strong&gt;service_conf.yaml&lt;/strong&gt; match the local machine IP and ports exposed by the container.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the RAGFlow backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x ./entrypoint.sh&#xA;$ bash ./entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd web&#xA;$ npm install --registry=https://registry.npmmirror.com --force&#xA;$ vim .umirc.ts&#xA;# Update proxy.target to http://127.0.0.1:9380&#xA;$ npm run dev &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy the frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd web&#xA;$ npm install --registry=https://registry.npmmirror.com --force&#xA;$ umi build&#xA;$ mkdir -p /ragflow/web&#xA;$ cp -r dist /ragflow/web&#xA;$ apt install nginx -y&#xA;$ cp ../docker/nginx/proxy.conf /etc/nginx&#xA;$ cp ../docker/nginx/nginx.conf /etc/nginx&#xA;$ cp ../docker/nginx/ragflow.conf /etc/nginx/conf.d&#xA;$ systemctl start nginx&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìö Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/category/user-guides&#34;&gt;User guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/category/references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìú Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/infiniflow/ragflow/issues/162&#34;&gt;RAGFlow Roadmap 2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèÑ Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/4XxujFgUN7&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/infiniflowai&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/orgs/infiniflow/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôå Contributing&lt;/h2&gt; &#xA;&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/references/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LLaVA-VL/LLaVA-NeXT</title>
    <updated>2024-09-01T01:55:00Z</updated>
    <id>tag:github.com,2024-09-01:/LLaVA-VL/LLaVA-NeXT</id>
    <link href="https://github.com/LLaVA-VL/LLaVA-NeXT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LLaVA-NeXT: Open Large Multimodal Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-paper-green&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io/blog/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-blog-green&#34; alt=&#34;llava_next-blog&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava-onevision.lmms-lab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-demo-red&#34; alt=&#34;llava_onevision-demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_demo-red&#34; alt=&#34;llava_next-interleave_demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_demo-red&#34; alt=&#34;llava_next-video_demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-checkpoints-blue&#34; alt=&#34;llava_onevision-checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_checkpoints-blue&#34; alt=&#34;llava_next-interleave_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_checkpoints-blue&#34; alt=&#34;llava_next-video_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-image_checkpoints-blue&#34; alt=&#34;llava_next-image_checkpoints&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/08/06] üî• &lt;strong&gt;üöÄ &lt;a href=&#34;https://llava-vl.github.io/blog/2024-08-05-llava-onevision/&#34;&gt;LLaVA-OneVision (OV)&lt;/a&gt;!&lt;/strong&gt; The new LLaVA-OV models (0.5B/7B/72B) achieve new state-of-the-art performance across single-image, multi-image, and video benchmarks, sometimes rivaling top commercial models on 47 diverse benchmarks. üìÑ Explore More:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;[Paper]&lt;/a&gt;: In-depth insights, new emegerging scenarios, ie, strong video understadning through task transfer from images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md&#34;&gt;[LLaVA-OV Doc]&lt;/a&gt;: Model inference and evaluation guidance.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/scripts/train&#34;&gt;[Scripts]&lt;/a&gt;: Start training models on your single-image/multi-image/video data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07/16] üî• &lt;strong&gt;LLaVA-NeXT-Video&lt;/strong&gt; has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video_0716.md&#34;&gt;this page&lt;/a&gt; for details, refer to &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;llava_next-video_demo&lt;/a&gt; for demo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/06/23] üî• &lt;strong&gt;LLaVA-NeXT-Interleave&lt;/strong&gt; is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve &lt;strong&gt;SoTA&lt;/strong&gt; performance on a wide range of benchmarks. Check out &lt;a href=&#34;https://arxiv.org/pdf/2407.07895&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/&#34;&gt;blog&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;checkpoints&lt;/a&gt; to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An all-round LLM for multi-image, video, and 3D with strong performance [&lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;Construct interleave training data &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data&#34;&gt;&lt;strong&gt;M4-Instruct&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Construct multi-image benchmark &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench&#34;&gt;&lt;strong&gt;LLaVA-Interleave Bench&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/25] üî• Wondering &#34;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;What Else Influences Visual Instruction Tuning Beyond Data?&lt;/a&gt;&#34; Our new &lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;blog&lt;/a&gt; summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K&#34;&gt;[COCO]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K&#34;&gt;[LCS]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M&#34;&gt;[CC3M]&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Architectures (LMM &amp;amp; Vision Encoder)&lt;/li&gt; &#xA;   &lt;li&gt;Visual Representations (Resolution &amp;amp; # Tokens)&lt;/li&gt; &#xA;   &lt;li&gt;Training Strategies (High-quality data &amp;amp; Trainable modules)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/&#34;&gt;blog&lt;/a&gt;] and [&lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;checkpoints&lt;/a&gt;] to see improved performance!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-04-30-llava-next-video/&#34;&gt;Blog&lt;/a&gt;], [&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;checkpoints&lt;/a&gt;] and [&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/01/30] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the &lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;blog post&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. Training/eval data and scripts coming soon.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024/03/10] üî• Releasing &lt;strong&gt;LMMs-Eval&lt;/strong&gt;, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [&lt;a href=&#34;https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/&#34;&gt;Blog&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;Codebase&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/10] &lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;LLaVA-Plus&lt;/a&gt; is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [&lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavaplus.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Plus-Codebase&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.05437&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/02] &lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;LLaVA-Interactive&lt;/a&gt; is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [&lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavainteractive.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Interactive-Demo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.00571&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/26] üî• LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15&#34;&gt;ckpts&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;script&lt;/a&gt;). We also provide a &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href=&#34;https://huggingface.co/spaces/etri-vilab/Ko-LLaVA&#34;&gt;ü§ó Demo&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/05] üî• LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;technical report&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href=&#34;https://llava-rlhf.github.io/&#34;&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/22] &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/06] Support &lt;strong&gt;Intel&lt;/strong&gt; dGPU and CPU platforms. &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel&#34;&gt;More details here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] LLaVA is now supported in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/10] &lt;a href=&#34;https://blog.roboflow.com/first-impressions-with-llava-1-5/&#34;&gt;Roboflow Deep Dive&lt;/a&gt;: First Impressions with LLaVA-1.5.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href=&#34;https://arxiv.org/abs/2309.09958&#34;&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href=&#34;https://arxiv.org/abs/2309.10020&#34;&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#39;&#39;.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2023/07/19] üî• We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/01] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/06] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/02] üî• We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/17] üî• We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI Terms of Use&lt;/a&gt; for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama-1/2 community license&lt;/a&gt; for LLaMA-2 and Vicuna-v1.5, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE&#34;&gt;Tongyi Qianwen RESEARCH LICENSE AGREEMENT&lt;/a&gt; and &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama-3 Research License&lt;/a&gt;). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.&lt;/p&gt; &#xA;&lt;h2&gt;Models &amp;amp; Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;1. &lt;strong&gt;Clone this repository and navigate to the LLaVA folder:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LLaVA-VL/LLaVA-NeXT&#xA;cd LLaVA-NeXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. &lt;strong&gt;Install the inference package:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # Enable PEP 660 support.&#xA;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Project Navigation&lt;/h3&gt; &#xA;&lt;p&gt;Please checkout the following page for more inference &amp;amp; evaluation details.&lt;/p&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-OneVision: Easy Task Transfer&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/%5B./docs/LLaVA-NeXT.md%5D(https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md)&#34;&gt;LLaVA-OneVision&lt;/a&gt;: for demo inference. The evaluation code is in &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT.md&#34;&gt;LLaVA-NeXT-Image&lt;/a&gt;: for image demo inference and evaluation of stronger LMMs using &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video.md&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;: for video inference and evaluation scripts. We recommend to use &lt;a href=&#34;https://lmms-lab.github.io/posts/lmms-eval-0.2/&#34;&gt;LMMs-video&lt;/a&gt; for evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Interleave.md&#34;&gt;LLaVA-NeXT-Interleave&lt;/a&gt;: for multi-image demo and evaluation scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SGLang for SpeedUp Inference and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;: Following the instruction in the &lt;a href=&#34;https://github.com/sgl-project/sglang?tab=readme-ov-file#install&#34;&gt;sglang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT/OneVision&lt;/h3&gt; &#xA;&lt;p&gt;Checkout the HTTP Post/Get and SRT usage at &lt;a href=&#34;https://github.com/sgl-project/sglang/tree/main/examples/runtime/llava_onevision&#34;&gt;sglang/examples/runtime/llava_onevision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT (Video)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Launch and Run on (K) Nodes&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to sglang project &lt;pre&gt;&lt;code&gt;cd PATH_TO/sglang&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;First node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;(e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Second node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The K node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2024llava,&#xA;  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},&#xA;  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},&#xA;  journal={arXiv preprint arXiv:2407.07895},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-ablations,&#xA;&#x9;title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},&#xA;&#x9;url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},&#xA;&#x9;author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},&#xA;&#x9;month={May},&#xA;&#x9;year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-strong,&#xA;    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},&#xA;    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},&#xA;    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},&#xA;    month={May},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{zhang2024llavanext-video,&#xA;  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},&#xA;  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},&#xA;  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},&#xA;  month={April},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{liu2024llavanext,&#xA;    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},&#xA;    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},&#xA;    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},&#xA;    month={January},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{liu2023improvedllava,&#xA;      title={Improved Baselines with Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},&#xA;      publisher={arXiv:2310.03744},&#xA;      year={2023},&#xA;}&#xA;&#xA;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={NeurIPS},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA; &lt;li&gt;The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): &lt;a href=&#34;https://brianboli.com/&#34;&gt;Bo Li&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dongguoset/&#34;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=ybRe9GcAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=en&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg&#34;&gt;Kaichen Zhang&lt;/a&gt;, &lt;a href=&#34;https://zrrskywalker.github.io/&#34;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&#34;https://zhangyuanhan-ai.github.io/&#34;&gt;Yuanhan Zhang&lt;/a&gt;, led by &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt; and with the guidance and help from &lt;a href=&#34;https://hliu.cc/&#34;&gt;Haotian Liu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;Ôªølmms-eval&lt;/code&gt; framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>