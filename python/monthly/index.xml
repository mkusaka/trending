<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-01T01:54:02Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/diffusers</title>
    <updated>2022-09-01T01:54:02Z</updated>
    <id>tag:github.com,2022-09-01:/huggingface/diffusers</id>
    <link href="https://github.com/huggingface/diffusers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ó Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/imgs/diffusers_library.jpg&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/huggingface/datasets.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/huggingface/diffusers.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/diffusers/main/CODE_OF_CONDUCT.md&#34;&gt; &lt;img alt=&#34;Contributor Covenant&#34; src=&#34;https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;ü§ó Diffusers provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models.&lt;/p&gt; &#xA;&lt;p&gt;More precisely, ü§ó Diffusers offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;State-of-the-art diffusion pipelines that can be run in inference with just a couple of lines of code (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines&#34;&gt;src/diffusers/pipelines&lt;/a&gt;). Check &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/README.md#pipelines-summary&#34;&gt;this overview&lt;/a&gt; to see all supported pipelines and their corresponding official papers.&lt;/li&gt; &#xA; &lt;li&gt;Various noise schedulers that can be used interchangeably for the prefered speed vs. quality trade-off in inference (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers&#34;&gt;src/diffusers/schedulers&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Multiple types of models, such as UNet, can be used as building blocks in an end-to-end diffusion system (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/models&#34;&gt;src/diffusers/models&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Training examples to show how to train the most popular diffusion model tasks (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples&#34;&gt;examples&lt;/a&gt;, &lt;em&gt;e.g.&lt;/em&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/unconditional_image_generation&#34;&gt;unconditional-image-generation&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;In order to get started, we recommend taking a look at two notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;Getting started with Diffusers&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; notebook, which showcases an end-to-end example of usage for diffusion models, schedulers and pipelines. Take a look at this notebook to learn how to use the pipeline abstraction, which takes care of everything (model, scheduler, noise handling) for you, and also to understand each independent building block in the library.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb&#34;&gt;Training a diffusers model&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; notebook summarizes diffusion models training methods. This notebook takes a step-by-step approach to training your diffusion models on an image dataset, with explanatory graphics.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;New&lt;/strong&gt; Stable Diffusion is now fully compatible with &lt;code&gt;diffusers&lt;/code&gt;!&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from &lt;a href=&#34;https://github.com/CompVis&#34;&gt;CompVis&lt;/a&gt;, &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;. It&#39;s trained on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-3&#34;&gt;model card&lt;/a&gt;, read the license and tick the checkbox if you agree. You have to be a registered user in ü§ó Hugging Face Hub, and you&#39;ll also need to use an access token for the code to work. For more information on access tokens, please refer to &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;this section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Image generation with Stable Diffusion&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# make sure you&#39;re logged in with `huggingface-cli login`&#xA;from torch import autocast&#xA;import torch&#xA;from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler&#xA;&#xA;lms = LMSDiscreteScheduler(&#xA;    beta_start=0.00085, &#xA;    beta_end=0.012, &#xA;    beta_schedule=&#34;scaled_linear&#34;&#xA;)&#xA;&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;    &#34;CompVis/stable-diffusion-v1-4&#34;, &#xA;    revision=&#34;fp16&#34;, &#xA;    torch_dtype=torch.float16,&#xA;    scheduler=lms,&#xA;    use_auth_token=True&#xA;)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a photo of an astronaut riding a horse on mars&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    image = pipe(prompt)[&#34;sample&#34;][0]  &#xA;    &#xA;image.save(&#34;astronaut_rides_horse.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image-to-Image text-guided generation with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;StableDiffusionImg2ImgPipeline&lt;/code&gt; lets you pass a text prompt and an initial image to condition the generation of new images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch import autocast&#xA;import requests&#xA;import torch&#xA;from PIL import Image&#xA;from io import BytesIO&#xA;&#xA;from diffusers import StableDiffusionImg2ImgPipeline&#xA;&#xA;# load the pipeline&#xA;device = &#34;cuda&#34;&#xA;pipe = StableDiffusionImg2ImgPipeline.from_pretrained(&#xA;    &#34;CompVis/stable-diffusion-v1-4&#34;,&#xA;    revision=&#34;fp16&#34;, &#xA;    torch_dtype=torch.float16,&#xA;    use_auth_token=True&#xA;)&#xA;pipe = pipe.to(device)&#xA;&#xA;# let&#39;s download an initial image&#xA;url = &#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg&#34;&#xA;&#xA;response = requests.get(url)&#xA;init_image = Image.open(BytesIO(response.content)).convert(&#34;RGB&#34;)&#xA;init_image = init_image.resize((768, 512))&#xA;&#xA;prompt = &#34;A fantasy landscape, trending on artstation&#34;&#xA;&#xA;with autocast(&#34;cuda&#34;):&#xA;    images = pipe(prompt=prompt, init_image=init_image, strength=0.75, guidance_scale=7.5)[&#34;sample&#34;]&#xA;&#xA;images[0].save(&#34;fantasy_landscape.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run this example on colab &lt;a href=&#34;https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;In-painting using Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;StableDiffusionInpaintPipeline&lt;/code&gt; lets you edit specific parts of an image by providing a mask and text prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from io import BytesIO&#xA;&#xA;from torch import autocast&#xA;import torch&#xA;import requests&#xA;import PIL&#xA;&#xA;from diffusers import StableDiffusionInpaintPipeline&#xA;&#xA;def download_image(url):&#xA;    response = requests.get(url)&#xA;    return PIL.Image.open(BytesIO(response.content)).convert(&#34;RGB&#34;)&#xA;&#xA;img_url = &#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png&#34;&#xA;mask_url = &#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png&#34;&#xA;&#xA;init_image = download_image(img_url).resize((512, 512))&#xA;mask_image = download_image(mask_url).resize((512, 512))&#xA;&#xA;device = &#34;cuda&#34;&#xA;pipe = StableDiffusionInpaintPipeline.from_pretrained(&#xA;    &#34;CompVis/stable-diffusion-v1-4&#34;,&#xA;    revision=&#34;fp16&#34;, &#xA;    torch_dtype=torch.float16,&#xA;    use_auth_token=True&#xA;)&#xA;pipe = pipe.to(device)&#xA;&#xA;prompt = &#34;a cat sitting on a bench&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    images = pipe(prompt=prompt, init_image=init_image, mask_image=mask_image, strength=0.75)[&#34;sample&#34;]&#xA;&#xA;images[0].save(&#34;cat_on_bench.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tweak prompts reusing seeds and latents&lt;/h3&gt; &#xA;&lt;p&gt;You can generate your own latents to reproduce results, or tweak your prompt on a specific result you liked. &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/diffusers/main/stable-diffusion-seeds.ipynb&#34;&gt;This notebook&lt;/a&gt; shows how to do it step by step. You can also run it in Google Colab &lt;a href=&#34;https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details, check out &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb&#34;&gt;the Stable Diffusion notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; and have a look into the &lt;a href=&#34;https://github.com/huggingface/diffusers/releases/tag/v0.2.0&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to try running Diffusers! Here we outline code-focused tools (primarily using &lt;code&gt;DiffusionPipeline&lt;/code&gt;s and Google Colab) and interactive web-tools.&lt;/p&gt; &#xA;&lt;h3&gt;Running Code&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run the code yourself üíª, you can try out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CompVis/ldm-text2im-large-256&#34;&gt;Text-to-Image Latent Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install diffusers transformers&#xA;from diffusers import DiffusionPipeline&#xA;&#xA;model_id = &#34;CompVis/ldm-text2im-large-256&#34;&#xA;&#xA;# load model and scheduler&#xA;ldm = DiffusionPipeline.from_pretrained(model_id)&#xA;&#xA;# run pipeline in inference (sample random noise and denoise)&#xA;prompt = &#34;A painting of a squirrel eating a burger&#34;&#xA;images = ldm([prompt], num_inference_steps=50, eta=0.3, guidance_scale=6)[&#34;sample&#34;]&#xA;&#xA;# save images&#xA;for idx, image in enumerate(images):&#xA;    image.save(f&#34;squirrel-{idx}.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/ddpm-celebahq-256&#34;&gt;Unconditional Diffusion with discrete scheduler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install diffusers&#xA;from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline&#xA;&#xA;model_id = &#34;google/ddpm-celebahq-256&#34;&#xA;&#xA;# load model and scheduler&#xA;ddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference&#xA;&#xA;# run pipeline in inference (sample random noise and denoise)&#xA;image = ddpm()[&#34;sample&#34;]&#xA;&#xA;# save image&#xA;image[0].save(&#34;ddpm_generated_image.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CompVis/ldm-celebahq-256&#34;&gt;Unconditional Latent Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/ncsnpp-ffhq-1024&#34;&gt;Unconditional Diffusion with continous scheduler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other Notebooks&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb&#34;&gt;image-to-image generation with Stable Diffusion&lt;/a&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb&#34;&gt;tweak images via repeated Stable Diffusion seeds&lt;/a&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web Demos&lt;/h3&gt; &#xA;&lt;p&gt;If you just want to play around with some web demos, you can try out the following üöÄ Spaces:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-to-Image Latent Diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/CompVis/text2img-latent-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Faces generator&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/CompVis/celeba-latent-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DDPM with different schedulers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/fusing/celeba-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conditional generation from sketch&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/huggingface/diffuse-the-rest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Composable diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/Shuang59/Composable-Diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Definitions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;: Neural network that models $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$ (see image below) and is trained end-to-end to &lt;em&gt;denoise&lt;/em&gt; a noisy input to an image. &lt;em&gt;Examples&lt;/em&gt;: UNet, Conditioned UNet, 3D UNet, Transformer UNet&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt; Figure from DDPM paper (https://arxiv.org/abs/2006.11239). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Schedulers&lt;/strong&gt;: Algorithm class for both &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;training&lt;/strong&gt;. The class provides functionality to compute previous image according to alpha, beta schedule as well as predict noise for training. &lt;em&gt;Examples&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;DDPM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.02502&#34;&gt;DDIM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PNDM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2204.13902&#34;&gt;DEIS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174349706-53d58acc-a4d1-4cda-b3e8-432d9dc7ad38.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt; Sampling and training algorithms. Figure from DDPM paper (https://arxiv.org/abs/2006.11239). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Diffusion Pipeline&lt;/strong&gt;: End-to-end pipeline that includes multiple diffusion models, possible text encoders, ... &lt;em&gt;Examples&lt;/em&gt;: Glide, Latent-Diffusion, Imagen, DALL-E 2&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174348898-481bd7c2-5457-4830-89bc-f0907756f64c.jpeg&#34; width=&#34;550&#34;&gt; &lt;br&gt; &lt;em&gt; Figure from ImageGen (https://imagen.research.google/). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Philosophy&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Readability and clarity is prefered over highly optimized code. A strong importance is put on providing readable, intuitive and elementary code design. &lt;em&gt;E.g.&lt;/em&gt;, the provided &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers&#34;&gt;schedulers&lt;/a&gt; are separated from the provided &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/models&#34;&gt;models&lt;/a&gt; and provide well-commented code that can be read alongside the original paper.&lt;/li&gt; &#xA; &lt;li&gt;Diffusers is &lt;strong&gt;modality independent&lt;/strong&gt; and focuses on providing pretrained models and tools to build systems that generate &lt;strong&gt;continous outputs&lt;/strong&gt;, &lt;em&gt;e.g.&lt;/em&gt; vision and audio.&lt;/li&gt; &#xA; &lt;li&gt;Diffusion models and schedulers are provided as concise, elementary building blocks. In contrast, diffusion pipelines are a collection of end-to-end diffusion systems that can be used out-of-the-box, should stay as close as possible to their original implementation and can include components of another library, such as text-encoders. Examples for diffusion pipelines are &lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Glide&lt;/a&gt; and &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade diffusers  # should install diffusers 0.2.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install -c conda-forge diffusers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;In the works&lt;/h2&gt; &#xA;&lt;p&gt;For the first release, ü§ó Diffusers focuses on text-to-image diffusion techniques. However, diffusers can be used for much more than that! Over the upcoming releases, we&#39;ll be focusing on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Diffusers for audio&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for reinforcement learning (initial work happening in &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/105&#34;&gt;https://github.com/huggingface/diffusers/pull/105&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for video generation&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for molecule generation (initial work happening in &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/54&#34;&gt;https://github.com/huggingface/diffusers/pull/54&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A few pipeline components are already being worked on, namely:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BDDMPipeline for spectrogram-to-sound vocoding&lt;/li&gt; &#xA; &lt;li&gt;GLIDEPipeline to support OpenAI&#39;s GLIDE model&lt;/li&gt; &#xA; &lt;li&gt;Grad-TTS for text to audio generation / conditional audio generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We want diffusers to be a toolbox useful for diffusers models in general; if you find yourself limited in any way by the current API, or would like to see additional models, schedulers, or techniques, please open a &lt;a href=&#34;https://github.com/huggingface/diffusers/issues&#34;&gt;GitHub issue&lt;/a&gt; mentioning what you would like to see.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We&#39;d like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@CompVis&#39; latent diffusion models library, available &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@hojonathanho original DDPM implementation, available &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;here&lt;/a&gt; as well as the extremely useful translation into PyTorch by @pesser, available &lt;a href=&#34;https://github.com/pesser/pytorch_diffusion&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@ermongroup&#39;s DDIM implementation, available &lt;a href=&#34;https://github.com/ermongroup/ddim&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;@yang-song&#39;s Score-VE and Score-VP implementations, available &lt;a href=&#34;https://github.com/yang-song/score_sde_pytorch&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available &lt;a href=&#34;https://github.com/heejkoo/Awesome-Diffusion-Models&#34;&gt;here&lt;/a&gt; as well as @crowsonkb and @rromb for useful discussions and insights.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xinntao/Real-ESRGAN</title>
    <updated>2022-09-01T01:54:02Z</updated>
    <id>tag:github.com,2022-09-01:/xinntao/Real-ESRGAN</id>
    <link href="https://github.com/xinntao/Real-ESRGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-ESRGAN aims at developing Practical Algorithms for General Image/Video Restoration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/realesrgan_logo.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;üëÄ&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-demos-videos&#34;&gt;&lt;strong&gt;Demos&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üö©&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-updates&#34;&gt;&lt;strong&gt;Updates&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ö°&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-quick-inference&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üè∞&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/model_zoo.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üîß&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-dependencies-and-installation&#34;&gt;Install&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üíª&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Train&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ùì&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üé®&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/CONTRIBUTING.md&#34;&gt;Contribution&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/realesrgan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/realesrgan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/xinntao/Real-ESRGAN&#34; alt=&#34;Open issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN&#34; alt=&#34;Closed issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/xinntao/Real-ESRGAN.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;AnimeVideo-v3 model (Âä®Êº´ËßÜÈ¢ëÂ∞èÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;&lt;em&gt;anime video models&lt;/em&gt;&lt;/a&gt;] and [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;&lt;em&gt;comparisons&lt;/em&gt;&lt;/a&gt;]&lt;br&gt; üî• &lt;strong&gt;RealESRGAN_x4plus_anime_6B&lt;/strong&gt; for anime images &lt;strong&gt;(Âä®Êº´ÊèíÂõæÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;em&gt;anime_model&lt;/em&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can try in our website: &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;ARC Demo&lt;/a&gt; (now only support RealESRGAN_x4plus_anime_6B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Portable &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;. You can find more information &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;here&lt;/a&gt;. The ncnn implementation is in &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can watch enhanced animations in &lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;Tencent Video&lt;/a&gt;. Ê¨¢ËøéËßÇÁúã&lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;ËÖæËÆØËßÜÈ¢ëÂä®Êº´‰øÆÂ§ç&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Real-ESRGAN aims at developing &lt;strong&gt;Practical Algorithms for General Image/Video Restoration&lt;/strong&gt;.&lt;br&gt; We extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data.&lt;/p&gt; &#xA;&lt;p&gt;üåå Thanks for your valuable feedbacks/suggestions. All the feedbacks are updated in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/feedback.md&#34;&gt;feedback.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If Real-ESRGAN is helpful, please help to ‚≠ê this repo or recommend it to your friends üòä &lt;br&gt; Other recommended projects:&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;: A practical algorithm for real-world face restoration &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;BasicSR&lt;/a&gt;: An open-source image and video restoration toolbox&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions.&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyFigure&#34;&gt;HandyFigure&lt;/a&gt;: Open source of paper figures &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üìñ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2107.10833&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.youtube.com/watch?v=fxHWoDSSvSc&#34;&gt;YouTube Video&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.bilibili.com/video/BV1H34y1m7sS/&#34;&gt;BÁ´ôËÆ≤Ëß£&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://xinntao.github.io/projects/RealESRGAN_src/RealESRGAN_poster.pdf&#34;&gt;Poster&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://docs.google.com/presentation/d/1QtW6Iy8rm8rGLsJ0Ldti6kP-7Qyzy6XL/edit?usp=sharing&amp;amp;ouid=109799856763657548160&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;PPT slides&lt;/a&gt;]&lt;br&gt; &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, Liangbin Xie, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&#34;&gt;Chao Dong&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;Tencent ARC Lab&lt;/a&gt;; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/teaser.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Updates ---------------------------&gt; &#xA;&lt;h2&gt;üö© Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ Update the &lt;strong&gt;RealESRGAN AnimeVideo-v3&lt;/strong&gt; model. Please see &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;comparisons&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add small models for anime videos. More details are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add the ncnn implementation &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;&lt;em&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/em&gt;&lt;/a&gt;, which is optimized for &lt;strong&gt;anime&lt;/strong&gt; images with much smaller model size. More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support finetuning on your own data or paired data (&lt;em&gt;i.e.&lt;/em&gt;, finetuning ESRGAN). See &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md#Finetune-Real-ESRGAN-on-your-own-dataset&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrate &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; to support &lt;strong&gt;face enhancement&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support arbitrary scale with &lt;code&gt;--outscale&lt;/code&gt; (It actually further resizes outputs with &lt;code&gt;LANCZOS4&lt;/code&gt;). Add &lt;em&gt;RealESRGAN_x2plus.pth&lt;/em&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/inference_realesrgan.py&#34;&gt;The inference code&lt;/a&gt; supports: 1) &lt;strong&gt;tile&lt;/strong&gt; options; 2) images with &lt;strong&gt;alpha channel&lt;/strong&gt;; 3) &lt;strong&gt;gray&lt;/strong&gt; images; 4) &lt;strong&gt;16-bit&lt;/strong&gt; images.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ The training codes have been released. A detailed guide can be found in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Training.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Demo videos ---------------------------&gt; &#xA;&lt;h2&gt;üëÄ Demos Videos&lt;/h2&gt; &#xA;&lt;h4&gt;Bilibili&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1ja41117zb&#34;&gt;Â§ßÈóπÂ§©ÂÆ´ÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wY4y1L7hT/&#34;&gt;Anime dance cut Âä®Êº´È≠îÊÄßËàûËπà&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1i3411L7Gy/&#34;&gt;Êµ∑Ë¥ºÁéãÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;YouTube&lt;/h4&gt; &#xA;&lt;h2&gt;üîß Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 1.7&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/xinntao/Real-ESRGAN.git&#xA;cd Real-ESRGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install basicsr - https://github.com/xinntao/BasicSR&#xA;# We use BasicSR for both training and inference&#xA;pip install basicsr&#xA;# facexlib and gfpgan are for face enhancement&#xA;pip install facexlib&#xA;pip install gfpgan&#xA;pip install -r requirements.txt&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö° Quick Inference&lt;/h2&gt; &#xA;&lt;p&gt;There are usually three ways to inference Real-ESRGAN.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#online-inference&#34;&gt;Online inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;Portable executable files (NCNN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#python-script&#34;&gt;Python script&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Online inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can try in our website: &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;ARC Demo&lt;/a&gt; (now only support RealESRGAN_x4plus_anime_6B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Portable executable files (NCNN)&lt;/h3&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This executable file is &lt;strong&gt;portable&lt;/strong&gt; and includes all the binaries and models required. No CUDA or PyTorch environment is needed.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can simply run the following command (the Windows example, more information is in the README.md of each executable files):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n model_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have provided five models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;realesrgan-x4plus (default)&lt;/li&gt; &#xA; &lt;li&gt;realesrnet-x4plus&lt;/li&gt; &#xA; &lt;li&gt;realesrgan-x4plus-anime (optimized for anime images, small model size)&lt;/li&gt; &#xA; &lt;li&gt;realesr-animevideov3 (animation video)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;-n&lt;/code&gt; argument for other models, for example, &lt;code&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n realesrnet-x4plus&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Usage of portable executable files&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan#computer-usages&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Note that it does not support all the functions (such as &lt;code&gt;outscale&lt;/code&gt;) as the python script &lt;code&gt;inference_realesrgan.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: realesrgan-ncnn-vulkan.exe -i infile -o outfile [options]...&#xA;&#xA;  -h                   show this help&#xA;  -i input-path        input image path (jpg/png/webp) or directory&#xA;  -o output-path       output image path (jpg/png/webp) or directory&#xA;  -s scale             upscale ratio (can be 2, 3, 4. default=4)&#xA;  -t tile-size         tile size (&amp;gt;=32/0=auto, default=0) can be 0,0,0 for multi-gpu&#xA;  -m model-path        folder path to the pre-trained models. default=models&#xA;  -n model-name        model name (default=realesr-animevideov3, can be realesr-animevideov3 | realesrgan-x4plus | realesrgan-x4plus-anime | realesrnet-x4plus)&#xA;  -g gpu-id            gpu device to use (default=auto) can be 0,1,2 for multi-gpu&#xA;  -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2,2,2:2 for multi-gpu&#xA;  -x                   enable tta mode&#34;&#xA;  -f format            output image format (jpg/png/webp, default=ext/png)&#xA;  -v                   verbose output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that it may introduce block inconsistency (and also generate slightly different results from the PyTorch implementation), because this executable file first crops the input image into several tiles, and then processes them separately, finally stitches together.&lt;/p&gt; &#xA;&lt;h3&gt;Python script&lt;/h3&gt; &#xA;&lt;h4&gt;Usage of python script&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can use X4 model for &lt;strong&gt;arbitrary output size&lt;/strong&gt; with the argument &lt;code&gt;outscale&lt;/code&gt;. The program will further perform cheap resize operation after the Real-ESRGAN output.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile -o outfile [options]...&#xA;&#xA;A common command: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile --outscale 3.5 --face_enhance&#xA;&#xA;  -h                   show this help&#xA;  -i --input           Input image or folder. Default: inputs&#xA;  -o --output          Output folder. Default: results&#xA;  -n --model_name      Model name. Default: RealESRGAN_x4plus&#xA;  -s, --outscale       The final upsampling scale of the image. Default: 4&#xA;  --suffix             Suffix of the restored image. Default: out&#xA;  -t, --tile           Tile size, 0 for no tile during testing. Default: 0&#xA;  --face_enhance       Whether to use GFPGAN to enhance face. Default: False&#xA;  --fp32               Use fp32 precision during inference. Default: fp16 (half precision).&#xA;  --ext                Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inference general images&lt;/h4&gt; &#xA;&lt;p&gt;Download pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h4&gt;Inference anime images&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B&lt;/a&gt;&lt;br&gt; More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download model&#xA;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models&#xA;# inference&#xA;python inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i inputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{wang2021realesrgan,&#xA;    author    = {Xintao Wang and Liangbin Xie and Chao Dong and Ying Shan},&#xA;    title     = {Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},&#xA;    booktitle = {International Conference on Computer Vision Workshops (ICCVW)},&#xA;    date      = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt; or &lt;code&gt;xintaowang@tencent.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- Projects that use Real-ESRGAN ---------------------------&gt; &#xA;&lt;h2&gt;üß© Projects that use Real-ESRGAN&lt;/h2&gt; &#xA;&lt;p&gt;If you develop/use Real-ESRGAN in your projects, welcome to let me know.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NCNN-Android: &lt;a href=&#34;https://github.com/tumuyan/RealSR-NCNN-Android&#34;&gt;RealSR-NCNN-Android&lt;/a&gt; by &lt;a href=&#34;https://github.com/tumuyan&#34;&gt;tumuyan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VapourSynth: &lt;a href=&#34;https://github.com/HolyWu/vs-realesrgan&#34;&gt;vs-realesrgan&lt;/a&gt; by &lt;a href=&#34;https://github.com/HolyWu&#34;&gt;HolyWu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NCNN: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;GUI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AaronFeng753/Waifu2x-Extension-GUI&#34;&gt;Waifu2x-Extension-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/AaronFeng753&#34;&gt;AaronFeng753&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Justin62628/Squirrel-RIFE&#34;&gt;Squirrel-RIFE&lt;/a&gt; by &lt;a href=&#34;https://github.com/Justin62628&#34;&gt;Justin62628&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scifx/Real-GUI&#34;&gt;Real-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/scifx&#34;&gt;scifx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/net2cn/Real-ESRGAN_GUI&#34;&gt;Real-ESRGAN_GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/net2cn&#34;&gt;net2cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WGzeyu/Real-ESRGAN-EGUI&#34;&gt;Real-ESRGAN-EGUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/WGzeyu&#34;&gt;WGzeyu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shangar21/anime_upscaler&#34;&gt;anime_upscaler&lt;/a&gt; by &lt;a href=&#34;https://github.com/shangar21&#34;&gt;shangar21&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for all the contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AK391&#34;&gt;AK391&lt;/a&gt;: Integrate RealESRGAN to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Asiimoviet&#34;&gt;Asiimoviet&lt;/a&gt;: Translate the README.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/2ji3150&#34;&gt;2ji3150&lt;/a&gt;: Thanks for the &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues/131&#34;&gt;detailed and valuable feedbacks/suggestions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jared-02&#34;&gt;Jared-02&lt;/a&gt;: Translate the Training.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>TencentARC/GFPGAN</title>
    <updated>2022-09-01T01:54:02Z</updated>
    <id>tag:github.com,2022-09-01:/TencentARC/GFPGAN</id>
    <link href="https://github.com/TencentARC/GFPGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/assets/gfpgan_logo.png&#34; height=&#34;130&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://twitter.com/_Xintao_&#34; style=&#34;text-decoration:none;&#34;&gt;&#xA;    &lt;img src=&#34;https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png&#34; width=&#34;4%&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gfpgan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gfpgan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/TencentARC/GFPGAN&#34; alt=&#34;Open issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/TencentARC/GFPGAN&#34; alt=&#34;Closed issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;strong&gt;Updated&lt;/strong&gt; online demo: &lt;a href=&#34;https://replicate.com/tencentarc/gfpgan&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Replicate&amp;amp;color=blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;. Here is the &lt;a href=&#34;https://replicate.com/xinntao/gfpgan&#34;&gt;backup&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;strong&gt;Updated&lt;/strong&gt; online demo: &lt;a href=&#34;https://huggingface.co/spaces/Xintao/GFPGAN&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Huggingface%20Gradio&amp;amp;color=orange&#34; alt=&#34;Huggingface Gradio&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo&#34;&gt;Colab Demo&lt;/a&gt; for GFPGAN &lt;a href=&#34;https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;; (Another &lt;a href=&#34;https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for the original paper model)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)&#xA;4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)&#xA;5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üöÄ&lt;/span&gt; &lt;strong&gt;Thanks for your interest in our work. You may also want to check our new updates on the &lt;em&gt;tiny models&lt;/em&gt; for &lt;em&gt;anime images and videos&lt;/em&gt; in &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/docs/anime_video_model.md&#34;&gt;Real-ESRGAN&lt;/a&gt;&lt;/strong&gt; &lt;span&gt;üòä&lt;/span&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;GFPGAN aims at developing a &lt;strong&gt;Practical Algorithm for Real-world Face Restoration&lt;/strong&gt;.&lt;br&gt; It leverages rich and diverse priors encapsulated in a pretrained face GAN (&lt;em&gt;e.g.&lt;/em&gt;, StyleGAN2) for blind face restoration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ùì&lt;/span&gt; Frequently Asked Questions can be found in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/FAQ.md&#34;&gt;FAQ.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Add &lt;strong&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;V1.3 model&lt;/a&gt;&lt;/strong&gt;, which produces &lt;strong&gt;more natural&lt;/strong&gt; restoration results, and better results on &lt;em&gt;very low-quality&lt;/em&gt; / &lt;em&gt;high-quality&lt;/em&gt; inputs. See more in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/#european_castle-model-zoo&#34;&gt;Model zoo&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/Comparisons.md&#34;&gt;Comparisons.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/GFPGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Support enhancing non-face regions (background) with &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; We provide a &lt;em&gt;clean&lt;/em&gt; version of GFPGAN, which does not require CUDA extensions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; We provide an updated model without colorizing faces.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If GFPGAN is helpful in your photos/projects, please help to &lt;span&gt;‚≠ê&lt;/span&gt; this repo or recommend it to your friends. Thanks&lt;span&gt;üòä&lt;/span&gt; Other recommended projects:&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;: A practical algorithm for general image restoration&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;BasicSR&lt;/a&gt;: An open-source image and video restoration toolbox&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üìñ&lt;/span&gt; GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://xinntao.github.io/projects/gfpgan&#34;&gt;Project Page&lt;/a&gt;] ‚ÄÉ [Demo] &lt;br&gt; &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt; &lt;br&gt; Applied Research Center (ARC), Tencent PCG&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 1.7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option: NVIDIA GPU + &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option: Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;We now provide a &lt;em&gt;clean&lt;/em&gt; version of GFPGAN, which does not require customized CUDA extensions. &lt;br&gt; If you want to use the original model in our paper, please see &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/PaperModel.md&#34;&gt;PaperModel.md&lt;/a&gt; for installation.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/TencentARC/GFPGAN.git&#xA;cd GFPGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install basicsr - https://github.com/xinntao/BasicSR&#xA;# We use BasicSR for both training and inference&#xA;pip install basicsr&#xA;&#xA;# Install facexlib - https://github.com/xinntao/facexlib&#xA;# We use face detection and face restoration helper in the facexlib package&#xA;pip install facexlib&#xA;&#xA;pip install -r requirements.txt&#xA;python setup.py develop&#xA;&#xA;# If you want to enhance the background (non-face) regions with Real-ESRGAN,&#xA;# you also need to install the realesrgan package&#xA;pip install realesrgan&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ö°&lt;/span&gt; Quick Inference&lt;/h2&gt; &#xA;&lt;p&gt;We take the v1.3 version for an example. More models can be found &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/#european_castle-model-zoo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Download pre-trained models: &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inference!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...&#xA;&#xA;  -h                   show this help&#xA;  -i input             Input image or folder. Default: inputs/whole_imgs&#xA;  -o output            Output folder. Default: results&#xA;  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3&#xA;  -s upscale           The final upsampling scale of the image. Default: 2&#xA;  -bg_upsampler        background upsampler. Default: realesrgan&#xA;  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400&#xA;  -suffix              Suffix of the restored faces&#xA;  -only_center_face    Only restore the center face&#xA;  -aligned             Input are aligned faces&#xA;  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use the original model in our paper, please see &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/PaperModel.md&#34;&gt;PaperModel.md&lt;/a&gt; for installation and inference.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üè∞&lt;/span&gt; Model Zoo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Based on V1.2; &lt;strong&gt;more natural&lt;/strong&gt; restoration results; better results on very low-quality / high-quality inputs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth&#34;&gt;GFPGANCleanv1-NoCE-C2.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No colorization; no CUDA extensions are required. Trained with more data with pre-processing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth&#34;&gt;GFPGANv1.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The paper model, with colorization.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The comparisons are in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/Comparisons.md&#34;&gt;Comparisons.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Strengths&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weaknesses&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì natural outputs&lt;br&gt; ‚úìbetter results on very low-quality inputs &lt;br&gt; ‚úì work on relatively high-quality inputs &lt;br&gt;‚úì can have repeated (twice) restorations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó not very sharp &lt;br&gt; ‚úó have a slight change on identity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì sharper output &lt;br&gt; ‚úì with beauty makeup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó some outputs are unnatural&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can find &lt;strong&gt;more models (such as the discriminators)&lt;/strong&gt; here: [&lt;a href=&#34;https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;], OR [&lt;a href=&#34;https://share.weiyun.com/ShYoCCoc&#34;&gt;Tencent Cloud ËÖæËÆØÂæÆ‰∫ë&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Training&lt;/h2&gt; &#xA;&lt;p&gt;We provide the training codes for GFPGAN (used in our paper). &lt;br&gt; You could improve it according to your own needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;More high quality faces can improve the restoration quality.&lt;/li&gt; &#xA; &lt;li&gt;You may need to perform some pre-processing, such as beauty makeup.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Procedures&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;(You can try a simple version ( &lt;code&gt;options/train_gfpgan_v1_simple.yml&lt;/code&gt;) that does not require face component landmarks.)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Dataset preparation: &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pre-trained models and other data. Put them in the &lt;code&gt;experiments/pretrained_models&lt;/code&gt; folder.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth&#34;&gt;Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth&#34;&gt;Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth&#34;&gt;A simple ArcFace model: arcface_resnet18.pth&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Modify the configuration file &lt;code&gt;options/train_gfpgan_v1.yml&lt;/code&gt; accordingly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Training&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìú&lt;/span&gt; License and Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;GFPGAN is released under Apache License Version 2.0.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{wang2021gfpgan,&#xA;    author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},&#xA;    title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},&#xA;    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìß&lt;/span&gt; Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt; or &lt;code&gt;xintaowang@tencent.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>