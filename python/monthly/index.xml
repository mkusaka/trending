<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-01T02:00:31Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>harry0703/MoneyPrinterTurbo</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/harry0703/MoneyPrinterTurbo</id>
    <link href="https://github.com/harry0703/MoneyPrinterTurbo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;利用AI大模型，一键生成高清短视频 Generate short videos with one click using AI LLM.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;MoneyPrinterTurbo 💸&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Stargazers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;h3&gt;简体中文 | &lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README-en.md&#34;&gt;English&lt;/a&gt;&lt;/h3&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://trendshift.io/repositories/8731&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/8731&#34; alt=&#34;harry0703%2FMoneyPrinterTurbo | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; 只需提供一个视频 &#xA; &lt;b&gt;主题&lt;/b&gt; 或 &#xA; &lt;b&gt;关键词&lt;/b&gt; ，就可以全自动生成视频文案、视频素材、视频字幕、视频背景音乐，然后合成一个高清的短视频。 &#xA; &lt;br&gt; &#xA; &lt;h4&gt;Web界面&lt;/h4&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h4&gt;API界面&lt;/h4&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;特别感谢 🙏&lt;/h2&gt; &#xA;&lt;p&gt;由于该项目的 &lt;strong&gt;部署&lt;/strong&gt; 和 &lt;strong&gt;使用&lt;/strong&gt;，对于一些小白用户来说，还是 &lt;strong&gt;有一定的门槛&lt;/strong&gt;，在此特别感谢 &lt;strong&gt;录咖（AI智能 多媒体服务平台）&lt;/strong&gt; 网站基于该项目，提供的免费&lt;code&gt;AI视频生成器&lt;/code&gt;服务，可以不用部署，直接在线使用，非常方便。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;中文版：&lt;a href=&#34;https://reccloud.cn&#34;&gt;https://reccloud.cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;英文版：&lt;a href=&#34;https://reccloud.com&#34;&gt;https://reccloud.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/reccloud.cn.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;感谢赞助 🙏&lt;/h2&gt; &#xA;&lt;p&gt;感谢佐糖 &lt;a href=&#34;https://picwish.cn&#34;&gt;https://picwish.cn&lt;/a&gt; 对该项目的支持和赞助，使得该项目能够持续的更新和维护。&lt;/p&gt; &#xA;&lt;p&gt;佐糖专注于&lt;strong&gt;图像处理领域&lt;/strong&gt;，提供丰富的&lt;strong&gt;图像处理工具&lt;/strong&gt;，将复杂操作极致简化，真正实现让图像处理更简单。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/picwish.jpg&#34; alt=&#34;picwish.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;功能特性 🎯&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 完整的 &lt;strong&gt;MVC架构&lt;/strong&gt;，代码 &lt;strong&gt;结构清晰&lt;/strong&gt;，易于维护，支持 &lt;code&gt;API&lt;/code&gt; 和 &lt;code&gt;Web界面&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持视频文案 &lt;strong&gt;AI自动生成&lt;/strong&gt;，也可以&lt;strong&gt;自定义文案&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持多种 &lt;strong&gt;高清视频&lt;/strong&gt; 尺寸 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 竖屏 9:16，&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 横屏 16:9，&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;批量视频生成&lt;/strong&gt;，可以一次生成多个视频，然后选择一个最满意的&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;视频片段时长&lt;/strong&gt; 设置，方便调节素材切换频率&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;中文&lt;/strong&gt; 和 &lt;strong&gt;英文&lt;/strong&gt; 视频文案&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;多种语音&lt;/strong&gt; 合成，可 &lt;strong&gt;实时试听&lt;/strong&gt; 效果&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;字幕生成&lt;/strong&gt;，可以调整 &lt;code&gt;字体&lt;/code&gt;、&lt;code&gt;位置&lt;/code&gt;、&lt;code&gt;颜色&lt;/code&gt;、&lt;code&gt;大小&lt;/code&gt;，同时支持&lt;code&gt;字幕描边&lt;/code&gt;设置&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;背景音乐&lt;/strong&gt;，随机或者指定音乐文件，可设置&lt;code&gt;背景音乐音量&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 视频素材来源 &lt;strong&gt;高清&lt;/strong&gt;，而且 &lt;strong&gt;无版权&lt;/strong&gt;，也可以使用自己的 &lt;strong&gt;本地素材&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;OpenAI&lt;/strong&gt;、&lt;strong&gt;Moonshot&lt;/strong&gt;、&lt;strong&gt;Azure&lt;/strong&gt;、&lt;strong&gt;gpt4free&lt;/strong&gt;、&lt;strong&gt;one-api&lt;/strong&gt;、&lt;strong&gt;通义千问&lt;/strong&gt;、&lt;strong&gt;Google Gemini&lt;/strong&gt;、&lt;strong&gt;Ollama&lt;/strong&gt;、 &lt;strong&gt;DeepSeek&lt;/strong&gt;、 &lt;strong&gt;文心一言&lt;/strong&gt; 等多种模型接入 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;中国用户建议使用 &lt;strong&gt;DeepSeek&lt;/strong&gt; 或 &lt;strong&gt;Moonshot&lt;/strong&gt; 作为大模型提供商（国内可直接访问，不需要VPN。注册就送额度，基本够用）&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;后期计划 📅&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GPT-SoVITS 配音支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 优化语音合成，利用大模型，使其合成的声音，更加自然，情绪更加丰富&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加视频转场效果，使其看起来更加的流畅&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加更多视频素材来源，优化视频素材和文案的匹配度&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加视频长度选项：短、中、长&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持更多的语音合成服务商，比如 OpenAI TTS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 自动上传到YouTube平台&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;视频演示 📺&lt;/h2&gt; &#xA;&lt;h3&gt;竖屏 9:16&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     ▶️&#xA;    &lt;/g-emoji&gt; 《如何增加生活的乐趣》&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     ▶️&#xA;    &lt;/g-emoji&gt; 《金钱的作用》&lt;br&gt;更真实的合成声音&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     ▶️&#xA;    &lt;/g-emoji&gt; 《生命的意义是什么》&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;横屏 16:9&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     ▶️&#xA;    &lt;/g-emoji&gt;《生命的意义是什么》&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     ▶️&#xA;    &lt;/g-emoji&gt;《为什么要运动》&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;配置要求 📦&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;建议最低 CPU 4核或以上，内存 8G 或以上，显卡非必须&lt;/li&gt; &#xA; &lt;li&gt;Windows 10 或 MacOS 11.0 以上系统&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速开始 🚀&lt;/h2&gt; &#xA;&lt;p&gt;下载一键启动包，解压直接使用（路径不要有 &lt;strong&gt;中文&lt;/strong&gt;、&lt;strong&gt;特殊字符&lt;/strong&gt;、&lt;strong&gt;空格&lt;/strong&gt;）&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;百度网盘（1.2.1 最新版本）: &lt;a href=&#34;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&#34;&gt;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&lt;/a&gt; 提取码: g36q&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;下载后，建议先&lt;strong&gt;双击执行&lt;/strong&gt; &lt;code&gt;update.bat&lt;/code&gt; 更新到&lt;strong&gt;最新代码&lt;/strong&gt;，然后双击 &lt;code&gt;start.bat&lt;/code&gt; 启动&lt;/p&gt; &#xA;&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; &#xA;&lt;h3&gt;其他系统&lt;/h3&gt; &#xA;&lt;p&gt;还没有制作一键启动包，看下面的 &lt;strong&gt;安装部署&lt;/strong&gt; 部分，建议使用 &lt;strong&gt;docker&lt;/strong&gt; 部署，更加方便。&lt;/p&gt; &#xA;&lt;h2&gt;安装部署 📥&lt;/h2&gt; &#xA;&lt;h3&gt;前提条件&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;尽量不要使用 &lt;strong&gt;中文路径&lt;/strong&gt;，避免出现一些无法预料的问题&lt;/li&gt; &#xA; &lt;li&gt;请确保你的 &lt;strong&gt;网络&lt;/strong&gt; 是正常的，VPN需要打开&lt;code&gt;全局流量&lt;/code&gt;模式&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;① 克隆代码&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;② 修改配置文件&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;将 &lt;code&gt;config.example.toml&lt;/code&gt; 文件复制一份，命名为 &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;按照 &lt;code&gt;config.toml&lt;/code&gt; 文件中的说明，配置好 &lt;code&gt;pexels_api_keys&lt;/code&gt; 和 &lt;code&gt;llm_provider&lt;/code&gt;，并根据 llm_provider 对应的服务商，配置相关的 API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker部署 🐳&lt;/h3&gt; &#xA;&lt;h4&gt;① 启动Docker&lt;/h4&gt; &#xA;&lt;p&gt;如果未安装 Docker，请先安装 &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果是Windows系统，请参考微软的文档：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/wsl/install&#34;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&#34;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd MoneyPrinterTurbo&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注意：最新版的docker安装时会自动以插件的形式安装docker compose，启动命令调整为docker compose up&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;② 访问Web界面&lt;/h4&gt; &#xA;&lt;p&gt;打开浏览器，访问 &lt;a href=&#34;http://0.0.0.0:8501&#34;&gt;http://0.0.0.0:8501&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;③ 访问API文档&lt;/h4&gt; &#xA;&lt;p&gt;打开浏览器，访问 &lt;a href=&#34;http://0.0.0.0:8080/docs&#34;&gt;http://0.0.0.0:8080/docs&lt;/a&gt; 或者 &lt;a href=&#34;http://0.0.0.0:8080/redoc&#34;&gt;http://0.0.0.0:8080/redoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;手动部署 📦&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;视频教程&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;完整的使用演示：&lt;a href=&#34;https://v.douyin.com/iFhnwsKY/&#34;&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;如何在Windows上部署：&lt;a href=&#34;https://v.douyin.com/iFyjoW3M&#34;&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;① 创建虚拟环境&lt;/h4&gt; &#xA;&lt;p&gt;建议使用 &lt;a href=&#34;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&#34;&gt;conda&lt;/a&gt; 创建 python 虚拟环境&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git&#xA;cd MoneyPrinterTurbo&#xA;conda create -n MoneyPrinterTurbo python=3.11&#xA;conda activate MoneyPrinterTurbo&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;② 安装好 ImageMagick&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;下载 &lt;a href=&#34;https://imagemagick.org/script/download.php&#34;&gt;https://imagemagick.org/script/download.php&lt;/a&gt; 选择Windows版本，切记一定要选择 &lt;strong&gt;静态库&lt;/strong&gt; 版本，比如 ImageMagick-7.1.1-32-Q16-x64-&lt;strong&gt;static&lt;/strong&gt;.exe&lt;/li&gt; &#xA;   &lt;li&gt;安装下载好的 ImageMagick，&lt;strong&gt;注意不要修改安装路径&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;修改 &lt;code&gt;配置文件 config.toml&lt;/code&gt; 中的 &lt;code&gt;imagemagick_path&lt;/code&gt; 为你的 &lt;strong&gt;实际安装路径&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CentOS&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum install ImageMagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;③ 启动Web界面 🌐&lt;/h4&gt; &#xA;&lt;p&gt;注意需要到 MoneyPrinterTurbo 项目 &lt;code&gt;根目录&lt;/code&gt; 下执行以下命令&lt;/p&gt; &#xA;&lt;h6&gt;Windows&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;conda activate MoneyPrinterTurbo&#xA;webui.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;MacOS or Linux&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda activate MoneyPrinterTurbo&#xA;sh webui.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; &#xA;&lt;h4&gt;④ 启动API服务 🚀&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动后，可以查看 &lt;code&gt;API文档&lt;/code&gt; &lt;a href=&#34;http://127.0.0.1:8080/docs&#34;&gt;http://127.0.0.1:8080/docs&lt;/a&gt; 或者 &lt;a href=&#34;http://127.0.0.1:8080/redoc&#34;&gt;http://127.0.0.1:8080/redoc&lt;/a&gt; 直接在线调试接口，快速体验。&lt;/p&gt; &#xA;&lt;h2&gt;语音合成 🗣&lt;/h2&gt; &#xA;&lt;p&gt;所有支持的声音列表，可以查看：&lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt&#34;&gt;声音列表&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2024-04-16 v1.1.2 新增了9种Azure的语音合成声音，需要配置API KEY，该声音合成的更加真实。&lt;/p&gt; &#xA;&lt;h2&gt;字幕生成 📜&lt;/h2&gt; &#xA;&lt;p&gt;当前支持2种字幕生成方式：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;edge&lt;/strong&gt;: 生成&lt;code&gt;速度快&lt;/code&gt;，性能更好，对电脑配置没有要求，但是质量可能不稳定&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;whisper&lt;/strong&gt;: 生成&lt;code&gt;速度慢&lt;/code&gt;，性能较差，对电脑配置有一定要求，但是&lt;code&gt;质量更可靠&lt;/code&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;可以修改 &lt;code&gt;config.toml&lt;/code&gt; 配置文件中的 &lt;code&gt;subtitle_provider&lt;/code&gt; 进行切换&lt;/p&gt; &#xA;&lt;p&gt;建议使用 &lt;code&gt;edge&lt;/code&gt; 模式，如果生成的字幕质量不好，再切换到 &lt;code&gt;whisper&lt;/code&gt; 模式&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注意：&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;whisper 模式下需要到 HuggingFace 下载一个模型文件，大约 3GB 左右，请确保网络通畅&lt;/li&gt; &#xA; &lt;li&gt;如果留空，表示不生成字幕。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;由于国内无法访问 HuggingFace，可以使用以下方法下载 &lt;code&gt;whisper-large-v3&lt;/code&gt; 的模型文件&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;下载地址：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;百度网盘: &lt;a href=&#34;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&#34;&gt;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;夸克网盘：&lt;a href=&#34;https://pan.quark.cn/s/3ee3d991d64b&#34;&gt;https://pan.quark.cn/s/3ee3d991d64b&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;模型下载后解压，整个目录放到 &lt;code&gt;.\MoneyPrinterTurbo\models&lt;/code&gt; 里面， 最终的文件路径应该是这样: &lt;code&gt;.\MoneyPrinterTurbo\models\whisper-large-v3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MoneyPrinterTurbo  &#xA;  ├─models&#xA;  │   └─whisper-large-v3&#xA;  │          config.json&#xA;  │          model.bin&#xA;  │          preprocessor_config.json&#xA;  │          tokenizer.json&#xA;  │          vocabulary.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;背景音乐 🎵&lt;/h2&gt; &#xA;&lt;p&gt;用于视频的背景音乐，位于项目的 &lt;code&gt;resource/songs&lt;/code&gt; 目录下。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;当前项目里面放了一些默认的音乐，来自于 YouTube 视频，如有侵权，请删除。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;字幕字体 🅰&lt;/h2&gt; &#xA;&lt;p&gt;用于视频字幕的渲染，位于项目的 &lt;code&gt;resource/fonts&lt;/code&gt; 目录下，你也可以放进去自己的字体。&lt;/p&gt; &#xA;&lt;h2&gt;常见问题 🤔&lt;/h2&gt; &#xA;&lt;h3&gt;❓如何使用免费的OpenAI GPT-3.5模型?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/start-using-chatgpt-instantly&#34;&gt;OpenAI宣布ChatGPT里面3.5已经免费了&lt;/a&gt;，有开发者将其封装成了API，可以直接调用&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;确保你安装和启动了docker服务&lt;/strong&gt;，执行以下命令启动docker服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -p 3040:3040 missuo/freegpt35&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动成功后，修改 &lt;code&gt;config.toml&lt;/code&gt; 中的配置&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_provider&lt;/code&gt; 设置为 &lt;code&gt;openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_api_key&lt;/code&gt; 随便填写一个即可，比如 &#39;123456&#39;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_base_url&lt;/code&gt; 改为 &lt;code&gt;http://localhost:3040/v1/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_model_name&lt;/code&gt; 改为 &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注意：该方式稳定性较差&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;❓AttributeError: &#39;str&#39; object has no attribute &#39;choices&#39;`&lt;/h3&gt; &#xA;&lt;p&gt;这个问题是由于大模型没有返回正确的回复导致的。&lt;/p&gt; &#xA;&lt;p&gt;大概率是网络原因， 使用 &lt;strong&gt;VPN&lt;/strong&gt;，或者设置 &lt;code&gt;openai_base_url&lt;/code&gt; 为你的代理 ，应该就可以解决了。&lt;/p&gt; &#xA;&lt;p&gt;同时建议使用 &lt;strong&gt;Moonshot&lt;/strong&gt; 或 &lt;strong&gt;DeepSeek&lt;/strong&gt; 作为大模型提供商，这两个服务商在国内访问速度更快，更加稳定。&lt;/p&gt; &#xA;&lt;h3&gt;❓RuntimeError: No ffmpeg exe could be found&lt;/h3&gt; &#xA;&lt;p&gt;通常情况下，ffmpeg 会被自动下载，并且会被自动检测到。 但是如果你的环境有问题，无法自动下载，可能会遇到如下错误：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: No ffmpeg exe could be found.&#xA;Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;此时你可以从 &lt;a href=&#34;https://www.gyan.dev/ffmpeg/builds/&#34;&gt;https://www.gyan.dev/ffmpeg/builds/&lt;/a&gt; 下载ffmpeg，解压后，设置 &lt;code&gt;ffmpeg_path&lt;/code&gt; 为你的实际安装路径即可。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[app]&#xA;# 请根据你的实际路径设置，注意 Windows 路径分隔符为 \\&#xA;ffmpeg_path = &#34;C:\\Users\\harry\\Downloads\\ffmpeg.exe&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;❓ImageMagick的安全策略阻止了与临时文件@/tmp/tmpur5hyyto.txt相关的操作&lt;/h3&gt; &#xA;&lt;p&gt;可以在ImageMagick的配置文件policy.xml中找到这些策略。 这个文件通常位于 /etc/ImageMagick-&lt;code&gt;X&lt;/code&gt;/ 或 ImageMagick 安装目录的类似位置。 修改包含&lt;code&gt;pattern=&#34;@&#34;&lt;/code&gt;的条目，将&lt;code&gt;rights=&#34;none&#34;&lt;/code&gt;更改为&lt;code&gt;rights=&#34;read|write&#34;&lt;/code&gt;以允许对文件的读写操作。&lt;/p&gt; &#xA;&lt;h3&gt;❓OSError: [Errno 24] Too many open files&lt;/h3&gt; &#xA;&lt;p&gt;这个问题是由于系统打开文件数限制导致的，可以通过修改系统的文件打开数限制来解决。&lt;/p&gt; &#xA;&lt;p&gt;查看当前限制&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ulimit -n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果过低，可以调高一些，比如&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ulimit -n 10240&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;❓Whisper 模型下载失败，出现如下错误&lt;/h3&gt; &#xA;&lt;p&gt;LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass &#39;local files only=False&#39; as input.&lt;/p&gt; &#xA;&lt;p&gt;或者&lt;/p&gt; &#xA;&lt;p&gt;An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.&lt;/p&gt; &#xA;&lt;p&gt;解决方法：&lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-&#34;&gt;点击查看如何从网盘手动下载模型&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;反馈建议 📢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;可以提交 &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues&#34;&gt;issue&lt;/a&gt; 或者 &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/pulls&#34;&gt;pull request&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参考项目 📚&lt;/h2&gt; &#xA;&lt;p&gt;该项目基于 &lt;a href=&#34;https://github.com/FujiwaraChoki/MoneyPrinter&#34;&gt;https://github.com/FujiwaraChoki/MoneyPrinter&lt;/a&gt; 重构而来，做了大量的优化，增加了更多的功能。 感谢原作者的开源精神。&lt;/p&gt; &#xA;&lt;h2&gt;许可证 📝&lt;/h2&gt; &#xA;&lt;p&gt;点击查看 &lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; 文件&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>assafelovic/gpt-researcher</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/assafelovic/gpt-researcher</id>
    <link href="https://github.com/assafelovic/gpt-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM based autonomous agent that conducts deep local and web research on any topic and generates a long report with citations.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;top&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3&#34; alt=&#34;Logo&#34; width=&#34;80&#34;&gt; &#xA; &lt;h4&gt;&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&amp;amp;logo=world&amp;amp;logoColor=white&amp;amp;color=0891b2&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&amp;amp;theme=clean-inverted&amp;amp;?compact=true&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&amp;amp;logoColor=white&amp;amp;style=flat&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub Release&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=grey&amp;amp;color=yellow&amp;amp;label=%20&amp;amp;style=flat&amp;amp;logoSize=40&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/gptresearcher/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&amp;amp;style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;color=1D63ED&#34; alt=&#34;Docker Image Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/assaf_elovic&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/assaf_elovic?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-zh_CN.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ja_JP.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ko_KR.md&#34;&gt;한국어&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;🔎 GPT Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPT Researcher is an open deep research agent designed for both web and local research on any given task.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Plan-and-Solve&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;&gt;RAG&lt;/a&gt; papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Objective conclusions for manual research can take weeks, requiring vast resources and time.&lt;/li&gt; &#xA; &lt;li&gt;LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.&lt;/li&gt; &#xA; &lt;li&gt;Current LLMs have token limitations, insufficient for generating long research reports.&lt;/li&gt; &#xA; &lt;li&gt;Limited web sources in existing services lead to misinformation and shallow results.&lt;/li&gt; &#xA; &lt;li&gt;Selective web sources can introduce bias into research tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&#34;&gt;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The core idea is to utilize &#39;planner&#39; and &#39;execution&#39; agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; height=&#34;600&#34; src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a task-specific agent based on a research query.&lt;/li&gt; &#xA; &lt;li&gt;Generate questions that collectively form an objective opinion on the task.&lt;/li&gt; &#xA; &lt;li&gt;Use a crawler agent for gathering information for each question.&lt;/li&gt; &#xA; &lt;li&gt;Summarize and source-track each resource.&lt;/li&gt; &#xA; &lt;li&gt;Filter and aggregate summaries into a final research report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.gptr.dev/blog/building-gpt-researcher&#34;&gt;How it Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea&#34;&gt;How to Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8&#34;&gt;Live Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📝 Generate detailed research reports using web and local documents.&lt;/li&gt; &#xA; &lt;li&gt;🖼️ Smart image scraping and filtering for reports.&lt;/li&gt; &#xA; &lt;li&gt;📜 Generate detailed reports exceeding 2,000 words.&lt;/li&gt; &#xA; &lt;li&gt;🌐 Aggregate over 20 sources for objective conclusions.&lt;/li&gt; &#xA; &lt;li&gt;🖥️ Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.&lt;/li&gt; &#xA; &lt;li&gt;🔍 JavaScript-enabled web scraping.&lt;/li&gt; &#xA; &lt;li&gt;📂 Maintains memory and context throughout research.&lt;/li&gt; &#xA; &lt;li&gt;📄 Export reports to PDF, Word, and other formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✨ Deep Research&lt;/h2&gt; &#xA;&lt;p&gt;GPT Researcher now includes Deep Research - an advanced recursive research workflow that explores topics with agentic depth and breadth. This feature employs a tree-like exploration pattern, diving deeper into subtopics while maintaining a comprehensive view of the research subject.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🌳 Tree-like exploration with configurable depth and breadth&lt;/li&gt; &#xA; &lt;li&gt;⚡️ Concurrent processing for faster results&lt;/li&gt; &#xA; &lt;li&gt;🤝 Smart context management across research branches&lt;/li&gt; &#xA; &lt;li&gt;⏱️ Takes ~5 minutes per deep research&lt;/li&gt; &#xA; &lt;li&gt;💰 Costs ~$0.4 per research (using &lt;code&gt;o3-mini&lt;/code&gt; on &#34;high&#34; reasoning effort)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research&#34;&gt;Learn more about Deep Research&lt;/a&gt; in our documentation.&lt;/p&gt; &#xA;&lt;h2&gt;📖 Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Documentation&lt;/a&gt; for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installation and setup guides&lt;/li&gt; &#xA; &lt;li&gt;Configuration and customization options&lt;/li&gt; &#xA; &lt;li&gt;How-To examples&lt;/li&gt; &#xA; &lt;li&gt;Full API references&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⚙️ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python 3.11 or later. &lt;a href=&#34;https://www.tutorialsteacher.com/python/install-python&#34;&gt;Guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the project and navigate to the directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/assafelovic/gpt-researcher.git&#xA;cd gpt-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up API keys by exporting them or storing them in a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY={Your OpenAI API Key here}&#xA;export TAVILY_API_KEY={Your Tavily API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies and start the server:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m uvicorn main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; to start.&lt;/p&gt; &#xA;&lt;p&gt;For other setups (e.g., Poetry or virtual environments), check the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Getting Started page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run as PIP package&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gpt-researcher&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;...&#xA;from gpt_researcher import GPTResearcher&#xA;&#xA;query = &#34;why is Nvidia stock going up?&#34;&#xA;researcher = GPTResearcher(query=query, report_type=&#34;research_report&#34;)&#xA;# Conduct research on the given query&#xA;research_result = await researcher.conduct_research()&#xA;# Write the report&#xA;report = await researcher.write_report()&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more examples and configurations, please refer to the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package&#34;&gt;PIP documentation&lt;/a&gt; page.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run with Docker&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker&#34;&gt;Install Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Clone the &#39;.env.example&#39; file, add your API Keys to the cloned file and save the file as &#39;.env&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Within the docker-compose file comment out services that you don&#39;t want to run with Docker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If that doesn&#39;t work, try running it without the dash:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - By default, if you haven&#39;t uncommented anything in your docker-compose file, this flow will start 2 processes:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the Python server running on localhost:8000&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;the React app running on localhost:3000&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Visit localhost:3000 on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;h2&gt;📄 Research on Local Documents&lt;/h2&gt; &#xA;&lt;p&gt;You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Add the env variable &lt;code&gt;DOC_PATH&lt;/code&gt; pointing to the folder where your documents are located.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DOC_PATH=&#34;./my-docs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you&#39;re running the frontend app on localhost:8000, simply select &#34;My Documents&#34; from the &#34;Report Source&#34; Dropdown Options.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re running GPT Researcher with the &lt;a href=&#34;https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package&#34;&gt;PIP package&lt;/a&gt;, pass the &lt;code&gt;report_source&lt;/code&gt; argument as &#34;local&#34; when you instantiate the &lt;code&gt;GPTResearcher&lt;/code&gt; class &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research&#34;&gt;code sample here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👪 Multi-Agent Assistant&lt;/h2&gt; &#xA;&lt;p&gt;As AI evolves from prompt engineering and RAG to multi-agent systems, we&#39;re excited to introduce our new multi-agent assistant built with &lt;a href=&#34;https://python.langchain.com/v0.1/docs/langgraph/&#34;&gt;LangGraph&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;STORM&lt;/a&gt; paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.&lt;/p&gt; &#xA;&lt;p&gt;An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.&lt;/p&gt; &#xA;&lt;p&gt;Check it out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents&#34;&gt;here&lt;/a&gt; or head over to our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph&#34;&gt;documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;🖥️ Frontend Applications&lt;/h2&gt; &#xA;&lt;p&gt;GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An intuitive interface for inputting research queries&lt;/li&gt; &#xA; &lt;li&gt;Real-time progress tracking of research tasks&lt;/li&gt; &#xA; &lt;li&gt;Interactive display of research findings&lt;/li&gt; &#xA; &lt;li&gt;Customizable settings for tailored research experiences&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Two deployment options are available:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A lightweight static frontend served by FastAPI&lt;/li&gt; &#xA; &lt;li&gt;A feature-rich NextJS application for advanced functionality&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For detailed setup instructions and more information about the frontend features, please visit our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We highly welcome contributions! Please check out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/raw/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; if you&#39;re interested.&lt;/p&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://trello.com/b/3O7KBePw/gpt-researcher-roadmap&#34;&gt;roadmap&lt;/a&gt; page and reach out to us via our &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;Discord community&lt;/a&gt; if you&#39;re interested in joining our mission. &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=assafelovic/gpt-researcher&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;✉️ Support / Contact us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/spBgZmm3Xe&#34;&gt;Community Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Author Email: &lt;a href=&#34;mailto:assaf.elovic@gmail.com&#34;&gt;assaf.elovic@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛡 Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; &#xA;&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.&lt;/li&gt; &#xA; &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#assafelovic/gpt-researcher&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;   &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/#top&#34;&gt;⬆️ Back to Top&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hiyouga/LLaMA-Factory</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/hiyouga/LLaMA-Factory</id>
    <link href="https://github.com/hiyouga/LLaMA-Factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png&#34; alt=&#34;# LLaMA Factory&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub workflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llamafactory/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llamafactory&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://scholar.google.com/scholar?cites=12620864006390196564&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-319-green&#34; alt=&#34;Citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/llamafactory_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/llamafactory_ai&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/rKfvV9r9FK&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitcode.com/zhengyaowei/LLaMA-Factory&#34;&gt;&lt;img src=&#34;https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg?sanitize=true&#34; alt=&#34;GitCode&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&#34;&gt;&lt;img src=&#34;https://gallery.pai-ml.com/assets/open-in-dsw.svg?sanitize=true&#34; alt=&#34;Open in DSW&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue&#34; alt=&#34;Studios&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Easily fine-tune 100+ large language models with zero-code &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&#34;&gt;CLI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&#34;&gt;Web UI&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;Github trend&#34; src=&#34;https://trendshift.io/api/badge/repositories/4535&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p&gt;👋 Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_npu.jpg&#34;&gt;NPU user group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md&#34;&gt;中文&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7c96b465-9df7-45f4-8053-bf03e58386d3&#34;&gt;https://github.com/user-attachments/assets/7c96b465-9df7-45f4-8053-bf03e58386d3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose your path:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documentation (WIP)&lt;/strong&gt;: &lt;a href=&#34;https://llamafactory.readthedocs.io/zh-cn/latest/&#34;&gt;https://llamafactory.readthedocs.io/zh-cn/latest/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colab&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PAI-DSW&lt;/strong&gt;: &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&#34;&gt;Llama3 Example&lt;/a&gt; | &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl&#34;&gt;Qwen2-VL Example&lt;/a&gt; | &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b&#34;&gt;DeepSeek-R1-Distill Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt;: &lt;a href=&#34;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches&#34;&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets&#34;&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement&#34;&gt;Requirement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&#34;&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker&#34;&gt;Build Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm&#34;&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&#34;&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&#34;&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger&#34;&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&#34;&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&#34;&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href=&#34;https://github.com/jiaweizzhao/GaLore&#34;&gt;GaLore&lt;/a&gt;, &lt;a href=&#34;https://github.com/Ledzy/BAdam&#34;&gt;BAdam&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhuhanqing/APOLLO&#34;&gt;APOLLO&lt;/a&gt;, &lt;a href=&#34;https://github.com/zyushun/Adam-mini&#34;&gt;Adam-mini&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;Unsloth&lt;/a&gt;, &lt;a href=&#34;https://github.com/linkedin/Liger-Kernel&#34;&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with vLLM worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Support Date&lt;/th&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Day 0&lt;/td&gt; &#xA;   &lt;td&gt;Qwen2.5 / Qwen2-VL / QwQ / QvQ / InternLM3 / MiniCPM-o-2.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Day 1&lt;/td&gt; &#xA;   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Compared to ChatGLM&#39;s &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning&#34;&gt;P-Tuning&lt;/a&gt;, LLaMA Factory&#39;s LoRA tuning offers up to &lt;strong&gt;3.7 times faster&lt;/strong&gt; training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#39;s QLoRA further improves the efficiency regarding the GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/benchmark.svg?sanitize=true&#34; alt=&#34;benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Definitions&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Training Speed&lt;/strong&gt;: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Rouge Score&lt;/strong&gt;: Rouge-2 score on the development set of the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;advertising text generation&lt;/a&gt; task. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;We adopt &lt;code&gt;pre_seq_len=128&lt;/code&gt; for ChatGLM&#39;s P-Tuning and &lt;code&gt;lora_rank=32&lt;/code&gt; for LLaMA Factory&#39;s LoRA tuning.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href=&#34;https://github.com/hiyouga/EasyR1&#34;&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; &#xA;&lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct&#34;&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; &#xA;&lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-R1&#34;&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Full Changelog&lt;/summary&gt; &#xA; &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.05270&#34;&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6&#34;&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/BUAADreamer&#34;&gt;@BUAADreamer&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/collections/internlm/&#34;&gt;InternLM3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/hhaAndroid&#34;&gt;@hhaAndroid&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-4&#34;&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; &#xA; &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&#34;&gt;this section&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B&#34;&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&#34;&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; &#xA; &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&#34;https://modelers.cn/models&#34;&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&#34;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-vl/&#34;&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/simonJJJ&#34;&gt;@simonJJJ&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/linkedin/Liger-Kernel&#34;&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/zyushun/Adam-mini&#34;&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage. Thank &lt;a href=&#34;https://github.com/relic-yuexi&#34;&gt;@relic-yuexi&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/07/04] We supported &lt;a href=&#34;https://github.com/MeetKai/functionary/tree/main/functionary/train/packing&#34;&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href=&#34;https://github.com/chuan298&#34;&gt;@chuan298&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02948&#34;&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://github.com/THUDM/GLM-4&#34;&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14734&#34;&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.01306&#34;&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&#34;&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href=&#34;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat&#34;&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/zhichen/Llama3-Chinese&#34;&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02258&#34;&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href=&#34;https://github.com/astramind-ai/Mixture-of-depths&#34;&gt;AstraMindAI&#39;s implementation&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/21] Our paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2403.13372&#34;&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;&#34; is available at arXiv!&lt;/p&gt; &#xA; &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.12354&#34;&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/TencentARC/LLaMA-Pro&#34;&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5/&#34;&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&#34;&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&#34;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.05914&#34;&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/dvlab-research/LongLoRA&#34;&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href=&#34;https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat&#34;&gt;LLaMA-2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-13B-sft&#34;&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href=&#34;https://github.com/KanadeSiina&#34;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&#34;https://github.com/codemayq&#34;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-7B-sft&#34;&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/22] We aligned the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model size&lt;/th&gt; &#xA;   &lt;th&gt;Template&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc&#34;&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;baichuan2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience&#34;&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;chatglm3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CohereForAI&#34;&gt;Command R&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;35B/104B&lt;/td&gt; &#xA;   &lt;td&gt;cohere&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;236B/671B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; &#xA;   &lt;td&gt;falcon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google&#34;&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; &#xA;   &lt;td&gt;gemma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;GLM-4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;9B&lt;/td&gt; &#xA;   &lt;td&gt;glm4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai-community&#34;&gt;GPT-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ibm-granite&#34;&gt;Granite 3.0-3.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; &#xA;   &lt;td&gt;granite3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IndexTeam&#34;&gt;Index&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9B&lt;/td&gt; &#xA;   &lt;td&gt;index&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm&#34;&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8B/20B&lt;/td&gt; &#xA;   &lt;td&gt;intern2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/70B&lt;/td&gt; &#xA;   &lt;td&gt;llama2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; &#xA;   &lt;td&gt;llama3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11B/90B&lt;/td&gt; &#xA;   &lt;td&gt;mllama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;llava&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; &#xA;   &lt;td&gt;llava_next&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/34B&lt;/td&gt; &#xA;   &lt;td&gt;llava_next_video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openbmb&#34;&gt;MiniCPM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/2B/4B&lt;/td&gt; &#xA;   &lt;td&gt;cpm/cpm3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openbmb&#34;&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B/12B&lt;/td&gt; &#xA;   &lt;td&gt;ministral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; &#xA;   &lt;td&gt;mistral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;24B&lt;/td&gt; &#xA;   &lt;td&gt;mistral_small&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai&#34;&gt;OLMo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google&#34;&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/10B/28B&lt;/td&gt; &#xA;   &lt;td&gt;paligemma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3B/2.7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4B/14B&lt;/td&gt; &#xA;   &lt;td&gt;phi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;phi_small&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;phi4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Pixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12B&lt;/td&gt; &#xA;   &lt;td&gt;pixtral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen/QwQ (1-2.5) (Code/Math/MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; &#xA;   &lt;td&gt;qwen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;qwen2_audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/3B/7B/72B&lt;/td&gt; &#xA;   &lt;td&gt;qwen2_vl&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Skywork&#34;&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;skywork_o1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode&#34;&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/7B/15B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI&#34;&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; &#xA;   &lt;td&gt;telechat2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xverse&#34;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/65B&lt;/td&gt; &#xA;   &lt;td&gt;xverse&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; &#xA;   &lt;td&gt;yi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B/34B&lt;/td&gt; &#xA;   &lt;td&gt;yi_vl&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IEITYuan&#34;&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/51B/102B&lt;/td&gt; &#xA;   &lt;td&gt;yuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] For the &#34;base&#34; models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the &#34;instruct/chat&#34; models.&lt;/p&gt; &#xA; &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py&#34;&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; &#xA;&lt;p&gt;You also can add a custom chat template to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py&#34;&gt;template.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Approach&lt;/th&gt; &#xA;   &lt;th&gt;Full-tuning&lt;/th&gt; &#xA;   &lt;th&gt;Freeze-tuning&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;QLoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pre-Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reward Modeling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KTO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ORPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href=&#34;https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Provided Datasets&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Pre-training datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt&#34;&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2&#34;&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/olm/olm-wikipedia-20221220&#34;&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&#34;&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/EleutherAI/pile&#34;&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Skywork/SkyPile-150B&#34;&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceFW/fineweb&#34;&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu&#34;&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/the-stack&#34;&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/starcoderdata&#34;&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json&#34;&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3&#34;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2&#34;&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/GAIR/lima&#34;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&#34;&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/OpenOrca&#34;&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/SlimOrca&#34;&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&#34;&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/wiki_qa&#34;&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/zxbsmk/webnovel_cn&#34;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&#34;&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HasturOfficial/adgen&#34;&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k&#34;&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/shibing624/sharegpt_gpt4&#34;&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k&#34;&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/THUDM/AgentInstruct&#34;&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/lmsys/lmsys-chat-1m&#34;&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k&#34;&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceTB/cosmopedia&#34;&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/stem_zh_instruction&#34;&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo&#34;&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/m-a-p/neo_sft_phase2&#34;&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered&#34;&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/argilla/magpie-ultra-v0.1&#34;&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/WebInstructSub&#34;&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&#34;&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&#34;&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/open-r1/OpenR1-Math-220k&#34;&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT&#34;&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k&#34;&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions&#34;&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/oasst_de&#34;&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de&#34;&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de&#34;&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de&#34;&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de&#34;&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolphin_de&#34;&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/booksum_de&#34;&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de&#34;&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de&#34;&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Preference datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k&#34;&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized&#34;&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/openbmb/RLHF-V-Dataset&#34;&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Zhihui/VLFeedback&#34;&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Intel/orca_dpo_pairs&#34;&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de&#34;&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/argilla/kto-mix-15k&#34;&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Mandatory&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;python&lt;/td&gt; &#xA;   &lt;td&gt;3.9&lt;/td&gt; &#xA;   &lt;td&gt;3.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;torch&lt;/td&gt; &#xA;   &lt;td&gt;1.13.1&lt;/td&gt; &#xA;   &lt;td&gt;2.4.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;transformers&lt;/td&gt; &#xA;   &lt;td&gt;4.41.2&lt;/td&gt; &#xA;   &lt;td&gt;4.49.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;datasets&lt;/td&gt; &#xA;   &lt;td&gt;2.16.0&lt;/td&gt; &#xA;   &lt;td&gt;3.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;accelerate&lt;/td&gt; &#xA;   &lt;td&gt;0.34.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;peft&lt;/td&gt; &#xA;   &lt;td&gt;0.11.1&lt;/td&gt; &#xA;   &lt;td&gt;0.12.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trl&lt;/td&gt; &#xA;   &lt;td&gt;0.8.6&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Optional&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;11.6&lt;/td&gt; &#xA;   &lt;td&gt;12.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deepspeed&lt;/td&gt; &#xA;   &lt;td&gt;0.10.0&lt;/td&gt; &#xA;   &lt;td&gt;0.16.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bitsandbytes&lt;/td&gt; &#xA;   &lt;td&gt;0.39.0&lt;/td&gt; &#xA;   &lt;td&gt;0.43.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vllm&lt;/td&gt; &#xA;   &lt;td&gt;0.4.3&lt;/td&gt; &#xA;   &lt;td&gt;0.7.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;flash-attn&lt;/td&gt; &#xA;   &lt;td&gt;2.3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.7.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Hardware Requirement&lt;/h3&gt; &#xA;&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;7B&lt;/th&gt; &#xA;   &lt;th&gt;13B&lt;/th&gt; &#xA;   &lt;th&gt;30B&lt;/th&gt; &#xA;   &lt;th&gt;70B&lt;/th&gt; &#xA;   &lt;th&gt;110B&lt;/th&gt; &#xA;   &lt;th&gt;8x7B&lt;/th&gt; &#xA;   &lt;th&gt;8x22B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;   &lt;td&gt;2000GB&lt;/td&gt; &#xA;   &lt;td&gt;900GB&lt;/td&gt; &#xA;   &lt;td&gt;2400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;300GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;900GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;200GB&lt;/td&gt; &#xA;   &lt;td&gt;360GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;32GB&lt;/td&gt; &#xA;   &lt;td&gt;64GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;320GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;140GB&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;72GB&lt;/td&gt; &#xA;   &lt;td&gt;30GB&lt;/td&gt; &#xA;   &lt;td&gt;96GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;4GB&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;18GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git&#xA;cd LLaMA-Factory&#xA;pip install -e &#34;.[torch,metrics]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, apollo, badam, adam-mini, qwen, minicpm_v, modelscope, openmind, swanlab, quality&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Use &lt;code&gt;pip install --no-deps -e .&lt;/code&gt; to resolve package conflicts.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Create an isolated Python environment with &lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv sync --extra torch --extra metrics --prerelease=allow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Windows users&lt;/summary&gt; &#xA; &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; &#xA; &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href=&#34;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&#34;&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; &#xA; &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href=&#34;https://huggingface.co/lldacing/flash-attention-windows-wheel&#34;&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; &#xA; &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e &#34;.[torch-npu,metrics]&#34;&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href=&#34;https://www.hiascend.com/developer/download/community/result?module=cann&#34;&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href=&#34;https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html&#34;&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# replace the url according to your CANN version and devices&#xA;# install CANN Toolkit&#xA;wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run&#xA;bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install&#xA;&#xA;# install CANN Kernels&#xA;wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run&#xA;bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install&#xA;&#xA;# set env variables&#xA;source /usr/local/Ascend/ascend-toolkit/set_env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Requirement&lt;/th&gt; &#xA;    &lt;th&gt;Minimum&lt;/th&gt; &#xA;    &lt;th&gt;Recommend&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CANN&lt;/td&gt; &#xA;    &lt;td&gt;8.0.RC1&lt;/td&gt; &#xA;    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torch&lt;/td&gt; &#xA;    &lt;td&gt;2.1.0&lt;/td&gt; &#xA;    &lt;td&gt;2.4.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torch-npu&lt;/td&gt; &#xA;    &lt;td&gt;2.1.0&lt;/td&gt; &#xA;    &lt;td&gt;2.4.0.post2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;deepspeed&lt;/td&gt; &#xA;    &lt;td&gt;0.13.2&lt;/td&gt; &#xA;    &lt;td&gt;0.16.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; &#xA; &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; &#xA; &lt;p&gt;Download the pre-built Docker images: &lt;a href=&#34;http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html&#34;&gt;32GB&lt;/a&gt; | &lt;a href=&#34;http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html&#34;&gt;64GB&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; &#xA; &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href=&#34;https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU&#34;&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install bitsandbytes from source&#xA;# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch&#xA;git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git&#xA;cd bitsandbytes/&#xA;&#xA;# Install dependencies&#xA;pip install -r requirements-dev.txt&#xA;&#xA;# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference&#xA;apt-get install -y build-essential cmake&#xA;&#xA;# Compile &amp;amp; install  &#xA;cmake -DCOMPUTE_BACKEND=npu -S .&#xA;make&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b main https://github.com/huggingface/transformers.git&#xA;cd transformers&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml&#34;&gt;example&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope / Modelers hub or load the dataset in local disk.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml&#xA;llamafactory-cli chat examples/inference/llama3_lora_sft.yaml&#xA;llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llamafactory-cli webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build Docker&lt;/h3&gt; &#xA;&lt;p&gt;For CUDA users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-cuda/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Ascend NPU users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-npu/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For AMD ROCm users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-rocm/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; &#xA; &lt;p&gt;For CUDA users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f ./docker/docker-cuda/Dockerfile \&#xA;    --build-arg INSTALL_BNB=false \&#xA;    --build-arg INSTALL_VLLM=false \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg INSTALL_FLASHATTN=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;docker run -dit --gpus=all \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For Ascend NPU users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Choose docker image upon your environment&#xA;docker build -f ./docker/docker-npu/Dockerfile \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;# Change `device` upon your resources&#xA;docker run -dit \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -v /usr/local/dcmi:/usr/local/dcmi \&#xA;    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \&#xA;    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \&#xA;    -v /etc/ascend_install.info:/etc/ascend_install.info \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --device /dev/davinci0 \&#xA;    --device /dev/davinci_manager \&#xA;    --device /dev/devmm_svm \&#xA;    --device /dev/hisi_hdc \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For AMD ROCm users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f ./docker/docker-rocm/Dockerfile \&#xA;    --build-arg INSTALL_BNB=false \&#xA;    --build-arg INSTALL_VLLM=false \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg INSTALL_FLASHATTN=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;docker run -dit \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -v ./saves:/app/saves \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --device /dev/kfd \&#xA;    --device /dev/dri \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Details about volume&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ms_cache&lt;/code&gt;: Similar to Hugging Face cache but for ModelScope users.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;om_cache&lt;/code&gt;: Similar to Hugging Face cache but for Modelers users.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Visit &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; &#xA; &lt;p&gt;Examples: &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py&#34;&gt;Image understanding&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py&#34;&gt;Function calling&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; &#xA;&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; &#xA;&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&#34;https://modelers.cn/models&#34;&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://wandb.ai&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;report_to: wandb&#xA;run_name: test_run # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href=&#34;https://wandb.ai/authorize&#34;&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; &#xA;&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;use_swanlab: true&#xA;swanlab_run_name: test_run # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href=&#34;https://swanlab.cn/settings&#34;&gt;API key&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href=&#34;https://swanlab.cn/settings&#34;&gt;API key&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; &#xA;&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click to show&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.02223&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10092&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10526&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href=&#34;https://arxiv.org/abs/2311.07816&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href=&#34;https://arxiv.org/abs/2312.15710&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.04319&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.07286&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.05904&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.07625&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11176&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11187&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11746&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11801&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href=&#34;https://arxiv.org/abs/2402.11809&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11819&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.12204&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.14714&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.15043&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.02333&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.03419&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.08228&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.09073&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.14541&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.15246&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16008&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16443&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.00604&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.04167&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.04316&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.07084&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.09836&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.11581&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.14215&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.16621&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href=&#34;https://arxiv.org/abs/2404.17140&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.18585&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.04760&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Dammu et al. &#34;They are uncultured&#34;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.05378&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.09055&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.12739&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.13816&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.20215&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href=&#34;https://aclanthology.org/2024.lt4hala-1.30&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.00380&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.02106&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.03136&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.04496&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.05688&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.05955&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.06973&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.07115&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.07815&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.10099&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.10173&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.12074&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.14408&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.14546&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.15695&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.17233&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.18069&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href=&#34;https://aclanthology.org/2024.americasnlp-1.25&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.19949&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.00365&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.01470&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.06129&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.08044&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.09756&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href=&#34;https://scholarcommons.scu.edu/cseng_senior/272/&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.13561&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.16637&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.17535&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.19705&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.00137&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href=&#34;https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.04693&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.04168&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href=&#34;https://aclanthology.org/2024.finnlp-2.1/&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.08072&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3627673.3679611&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Yu-Yang-Li/StarWhisper&#34;&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-LawLLM&#34;&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/X-D-Lab/Sunsimiao&#34;&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/WangRongsheng/CareGPT&#34;&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Machine-Mindset/&#34;&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Nekochu/Luminia-13B-v3&#34;&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href=&#34;https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt&#34;&gt;[demo]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/BUAADreamer/Chinese-LLaVA-Med&#34;&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/THUDM/AutoRE&#34;&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NVIDIA/RTX-AI-Toolkit&#34;&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/LazyAGI/LazyLLM&#34;&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NLPJCL/RAG-Retrieval&#34;&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/987727357&#34;&gt;[blog]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Qihoo360/360-LLaMA-Factory&#34;&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://novasky-ai.github.io/posts/sky-t1/&#34;&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf&#34;&gt;Baichuan 2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigscience/license&#34;&gt;BLOOM&lt;/a&gt; / &lt;a href=&#34;https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE&#34;&gt;ChatGLM3&lt;/a&gt; / &lt;a href=&#34;https://cohere.com/c4ai-cc-by-nc-license&#34;&gt;Command R&lt;/a&gt; / &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;DeepSeek&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt&#34;&gt;Falcon&lt;/a&gt; / &lt;a href=&#34;https://ai.google.dev/gemma/terms&#34;&gt;Gemma&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE&#34;&gt;GLM-4&lt;/a&gt; / &lt;a href=&#34;https://github.com/openai/gpt-2/raw/master/LICENSE&#34;&gt;GPT-2&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Granite&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE&#34;&gt;Index&lt;/a&gt; / &lt;a href=&#34;https://github.com/InternLM/InternLM#license&#34;&gt;InternLM&lt;/a&gt; / &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Llama&lt;/a&gt; / &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama 2 (LLaVA-1.5)&lt;/a&gt; / &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama 3&lt;/a&gt; / &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&#34;&gt;MiniCPM&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;OLMo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx&#34;&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&#34;&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT&#34;&gt;Qwen&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf&#34;&gt;Skywork&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;StarCoder 2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;TeleChat2&lt;/a&gt; / &lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf&#34;&gt;XVERSE&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE&#34;&gt;Yi&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Yi-1.5&lt;/a&gt; / &lt;a href=&#34;https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan&#34;&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{zheng2024llamafactory,&#xA;  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},&#xA;  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},&#xA;  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},&#xA;  address={Bangkok, Thailand},&#xA;  publisher={Association for Computational Linguistics},&#xA;  year={2024},&#xA;  url={http://arxiv.org/abs/2403.13372}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;TRL&lt;/a&gt;, &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>