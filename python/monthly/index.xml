<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-01T02:00:31Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>harry0703/MoneyPrinterTurbo</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/harry0703/MoneyPrinterTurbo</id>
    <link href="https://github.com/harry0703/MoneyPrinterTurbo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Stargazers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README-en.md&#34;&gt;English&lt;/a&gt;&lt;/h3&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://trendshift.io/repositories/8731&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/8731&#34; alt=&#34;harry0703%2FMoneyPrinterTurbo | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ &#xA; &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– &#xA; &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚ &#xA; &lt;br&gt; &#xA; &lt;h4&gt;Webç•Œé¢&lt;/h4&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h4&gt;APIç•Œé¢&lt;/h4&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ç‰¹åˆ«æ„Ÿè°¢ ğŸ™&lt;/h2&gt; &#xA;&lt;p&gt;ç”±äºè¯¥é¡¹ç›®çš„ &lt;strong&gt;éƒ¨ç½²&lt;/strong&gt; å’Œ &lt;strong&gt;ä½¿ç”¨&lt;/strong&gt;ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ &lt;strong&gt;æœ‰ä¸€å®šçš„é—¨æ§›&lt;/strong&gt;ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢ &lt;strong&gt;å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰&lt;/strong&gt; ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹&lt;code&gt;AIè§†é¢‘ç”Ÿæˆå™¨&lt;/code&gt;æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸­æ–‡ç‰ˆï¼š&lt;a href=&#34;https://reccloud.cn&#34;&gt;https://reccloud.cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;è‹±æ–‡ç‰ˆï¼š&lt;a href=&#34;https://reccloud.com&#34;&gt;https://reccloud.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/reccloud.cn.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;æ„Ÿè°¢èµåŠ© ğŸ™&lt;/h2&gt; &#xA;&lt;p&gt;æ„Ÿè°¢ä½ç³– &lt;a href=&#34;https://picwish.cn&#34;&gt;https://picwish.cn&lt;/a&gt; å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä½ç³–ä¸“æ³¨äº&lt;strong&gt;å›¾åƒå¤„ç†é¢†åŸŸ&lt;/strong&gt;ï¼Œæä¾›ä¸°å¯Œçš„&lt;strong&gt;å›¾åƒå¤„ç†å·¥å…·&lt;/strong&gt;ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/picwish.jpg&#34; alt=&#34;picwish.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;åŠŸèƒ½ç‰¹æ€§ ğŸ¯&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å®Œæ•´çš„ &lt;strong&gt;MVCæ¶æ„&lt;/strong&gt;ï¼Œä»£ç  &lt;strong&gt;ç»“æ„æ¸…æ™°&lt;/strong&gt;ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ &lt;code&gt;API&lt;/code&gt; å’Œ &lt;code&gt;Webç•Œé¢&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒè§†é¢‘æ–‡æ¡ˆ &lt;strong&gt;AIè‡ªåŠ¨ç”Ÿæˆ&lt;/strong&gt;ï¼Œä¹Ÿå¯ä»¥&lt;strong&gt;è‡ªå®šä¹‰æ–‡æ¡ˆ&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒå¤šç§ &lt;strong&gt;é«˜æ¸…è§†é¢‘&lt;/strong&gt; å°ºå¯¸ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ç«–å± 9:16ï¼Œ&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ¨ªå± 16:9ï¼Œ&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;æ‰¹é‡è§†é¢‘ç”Ÿæˆ&lt;/strong&gt;ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;è§†é¢‘ç‰‡æ®µæ—¶é•¿&lt;/strong&gt; è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;ä¸­æ–‡&lt;/strong&gt; å’Œ &lt;strong&gt;è‹±æ–‡&lt;/strong&gt; è§†é¢‘æ–‡æ¡ˆ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;å¤šç§è¯­éŸ³&lt;/strong&gt; åˆæˆï¼Œå¯ &lt;strong&gt;å®æ—¶è¯•å¬&lt;/strong&gt; æ•ˆæœ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;å­—å¹•ç”Ÿæˆ&lt;/strong&gt;ï¼Œå¯ä»¥è°ƒæ•´ &lt;code&gt;å­—ä½“&lt;/code&gt;ã€&lt;code&gt;ä½ç½®&lt;/code&gt;ã€&lt;code&gt;é¢œè‰²&lt;/code&gt;ã€&lt;code&gt;å¤§å°&lt;/code&gt;ï¼ŒåŒæ—¶æ”¯æŒ&lt;code&gt;å­—å¹•æè¾¹&lt;/code&gt;è®¾ç½®&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;èƒŒæ™¯éŸ³ä¹&lt;/strong&gt;ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®&lt;code&gt;èƒŒæ™¯éŸ³ä¹éŸ³é‡&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; è§†é¢‘ç´ ææ¥æº &lt;strong&gt;é«˜æ¸…&lt;/strong&gt;ï¼Œè€Œä¸” &lt;strong&gt;æ— ç‰ˆæƒ&lt;/strong&gt;ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ &lt;strong&gt;æœ¬åœ°ç´ æ&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒ &lt;strong&gt;OpenAI&lt;/strong&gt;ã€&lt;strong&gt;Moonshot&lt;/strong&gt;ã€&lt;strong&gt;Azure&lt;/strong&gt;ã€&lt;strong&gt;gpt4free&lt;/strong&gt;ã€&lt;strong&gt;one-api&lt;/strong&gt;ã€&lt;strong&gt;é€šä¹‰åƒé—®&lt;/strong&gt;ã€&lt;strong&gt;Google Gemini&lt;/strong&gt;ã€&lt;strong&gt;Ollama&lt;/strong&gt;ã€ &lt;strong&gt;DeepSeek&lt;/strong&gt;ã€ &lt;strong&gt;æ–‡å¿ƒä¸€è¨€&lt;/strong&gt; ç­‰å¤šç§æ¨¡å‹æ¥å…¥ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ &lt;strong&gt;DeepSeek&lt;/strong&gt; æˆ– &lt;strong&gt;Moonshot&lt;/strong&gt; ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;åæœŸè®¡åˆ’ ğŸ“…&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GPT-SoVITS é…éŸ³æ”¯æŒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;è§†é¢‘æ¼”ç¤º ğŸ“º&lt;/h2&gt; &#xA;&lt;h3&gt;ç«–å± 9:16&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     â–¶ï¸&#xA;    &lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     â–¶ï¸&#xA;    &lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     â–¶ï¸&#xA;    &lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;æ¨ªå± 16:9&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     â–¶ï¸&#xA;    &lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;g-emoji class=&#34;g-emoji&#34; alias=&#34;arrow_forward&#34;&gt;&#xA;     â–¶ï¸&#xA;    &lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;é…ç½®è¦æ±‚ ğŸ“¦&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å»ºè®®æœ€ä½ CPU 4æ ¸æˆ–ä»¥ä¸Šï¼Œå†…å­˜ 8G æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»&lt;/li&gt; &#xA; &lt;li&gt;Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¿«é€Ÿå¼€å§‹ ğŸš€&lt;/h2&gt; &#xA;&lt;p&gt;ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ &lt;strong&gt;ä¸­æ–‡&lt;/strong&gt;ã€&lt;strong&gt;ç‰¹æ®Šå­—ç¬¦&lt;/strong&gt;ã€&lt;strong&gt;ç©ºæ ¼&lt;/strong&gt;ï¼‰&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç™¾åº¦ç½‘ç›˜ï¼ˆ1.2.1 æœ€æ–°ç‰ˆæœ¬ï¼‰: &lt;a href=&#34;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&#34;&gt;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&lt;/a&gt; æå–ç : g36q&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä¸‹è½½åï¼Œå»ºè®®å…ˆ&lt;strong&gt;åŒå‡»æ‰§è¡Œ&lt;/strong&gt; &lt;code&gt;update.bat&lt;/code&gt; æ›´æ–°åˆ°&lt;strong&gt;æœ€æ–°ä»£ç &lt;/strong&gt;ï¼Œç„¶ååŒå‡» &lt;code&gt;start.bat&lt;/code&gt; å¯åŠ¨&lt;/p&gt; &#xA;&lt;p&gt;å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ &lt;strong&gt;Chrome&lt;/strong&gt; æˆ–è€… &lt;strong&gt;Edge&lt;/strong&gt; æ‰“å¼€ï¼‰&lt;/p&gt; &#xA;&lt;h3&gt;å…¶ä»–ç³»ç»Ÿ&lt;/h3&gt; &#xA;&lt;p&gt;è¿˜æ²¡æœ‰åˆ¶ä½œä¸€é”®å¯åŠ¨åŒ…ï¼Œçœ‹ä¸‹é¢çš„ &lt;strong&gt;å®‰è£…éƒ¨ç½²&lt;/strong&gt; éƒ¨åˆ†ï¼Œå»ºè®®ä½¿ç”¨ &lt;strong&gt;docker&lt;/strong&gt; éƒ¨ç½²ï¼Œæ›´åŠ æ–¹ä¾¿ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å®‰è£…éƒ¨ç½² ğŸ“¥&lt;/h2&gt; &#xA;&lt;h3&gt;å‰ææ¡ä»¶&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å°½é‡ä¸è¦ä½¿ç”¨ &lt;strong&gt;ä¸­æ–‡è·¯å¾„&lt;/strong&gt;ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜&lt;/li&gt; &#xA; &lt;li&gt;è¯·ç¡®ä¿ä½ çš„ &lt;strong&gt;ç½‘ç»œ&lt;/strong&gt; æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€&lt;code&gt;å…¨å±€æµé‡&lt;/code&gt;æ¨¡å¼&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;â‘  å…‹éš†ä»£ç &lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å°† &lt;code&gt;config.example.toml&lt;/code&gt; æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;æŒ‰ç…§ &lt;code&gt;config.toml&lt;/code&gt; æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ &lt;code&gt;pexels_api_keys&lt;/code&gt; å’Œ &lt;code&gt;llm_provider&lt;/code&gt;ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„ API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dockeréƒ¨ç½² ğŸ³&lt;/h3&gt; &#xA;&lt;h4&gt;â‘  å¯åŠ¨Docker&lt;/h4&gt; &#xA;&lt;p&gt;å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/wsl/install&#34;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&#34;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd MoneyPrinterTurbo&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;â‘¡ è®¿é—®Webç•Œé¢&lt;/h4&gt; &#xA;&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® &lt;a href=&#34;http://0.0.0.0:8501&#34;&gt;http://0.0.0.0:8501&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;â‘¢ è®¿é—®APIæ–‡æ¡£&lt;/h4&gt; &#xA;&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® &lt;a href=&#34;http://0.0.0.0:8080/docs&#34;&gt;http://0.0.0.0:8080/docs&lt;/a&gt; æˆ–è€… &lt;a href=&#34;http://0.0.0.0:8080/redoc&#34;&gt;http://0.0.0.0:8080/redoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;è§†é¢‘æ•™ç¨‹&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼š&lt;a href=&#34;https://v.douyin.com/iFhnwsKY/&#34;&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼š&lt;a href=&#34;https://v.douyin.com/iFyjoW3M&#34;&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ&lt;/h4&gt; &#xA;&lt;p&gt;å»ºè®®ä½¿ç”¨ &lt;a href=&#34;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&#34;&gt;conda&lt;/a&gt; åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git&#xA;cd MoneyPrinterTurbo&#xA;conda create -n MoneyPrinterTurbo python=3.11&#xA;conda activate MoneyPrinterTurbo&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;â‘¡ å®‰è£…å¥½ ImageMagick&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¸‹è½½ &lt;a href=&#34;https://imagemagick.org/script/download.php&#34;&gt;https://imagemagick.org/script/download.php&lt;/a&gt; é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© &lt;strong&gt;é™æ€åº“&lt;/strong&gt; ç‰ˆæœ¬ï¼Œæ¯”å¦‚ ImageMagick-7.1.1-32-Q16-x64-&lt;strong&gt;static&lt;/strong&gt;.exe&lt;/li&gt; &#xA;   &lt;li&gt;å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ&lt;strong&gt;æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ä¿®æ”¹ &lt;code&gt;é…ç½®æ–‡ä»¶ config.toml&lt;/code&gt; ä¸­çš„ &lt;code&gt;imagemagick_path&lt;/code&gt; ä¸ºä½ çš„ &lt;strong&gt;å®é™…å®‰è£…è·¯å¾„&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CentOS&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo yum install ImageMagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ&lt;/h4&gt; &#xA;&lt;p&gt;æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® &lt;code&gt;æ ¹ç›®å½•&lt;/code&gt; ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤&lt;/p&gt; &#xA;&lt;h6&gt;Windows&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;conda activate MoneyPrinterTurbo&#xA;webui.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;MacOS or Linux&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda activate MoneyPrinterTurbo&#xA;sh webui.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ &lt;strong&gt;Chrome&lt;/strong&gt; æˆ–è€… &lt;strong&gt;Edge&lt;/strong&gt; æ‰“å¼€ï¼‰&lt;/p&gt; &#xA;&lt;h4&gt;â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ &lt;code&gt;APIæ–‡æ¡£&lt;/code&gt; &lt;a href=&#34;http://127.0.0.1:8080/docs&#34;&gt;http://127.0.0.1:8080/docs&lt;/a&gt; æˆ–è€… &lt;a href=&#34;http://127.0.0.1:8080/redoc&#34;&gt;http://127.0.0.1:8080/redoc&lt;/a&gt; ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚&lt;/p&gt; &#xA;&lt;h2&gt;è¯­éŸ³åˆæˆ ğŸ—£&lt;/h2&gt; &#xA;&lt;p&gt;æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š&lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt&#34;&gt;å£°éŸ³åˆ—è¡¨&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å­—å¹•ç”Ÿæˆ ğŸ“œ&lt;/h2&gt; &#xA;&lt;p&gt;å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;edge&lt;/strong&gt;: ç”Ÿæˆ&lt;code&gt;é€Ÿåº¦å¿«&lt;/code&gt;ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;whisper&lt;/strong&gt;: ç”Ÿæˆ&lt;code&gt;é€Ÿåº¦æ…¢&lt;/code&gt;ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯&lt;code&gt;è´¨é‡æ›´å¯é &lt;/code&gt;ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¯ä»¥ä¿®æ”¹ &lt;code&gt;config.toml&lt;/code&gt; é…ç½®æ–‡ä»¶ä¸­çš„ &lt;code&gt;subtitle_provider&lt;/code&gt; è¿›è¡Œåˆ‡æ¢&lt;/p&gt; &#xA;&lt;p&gt;å»ºè®®ä½¿ç”¨ &lt;code&gt;edge&lt;/code&gt; æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° &lt;code&gt;whisper&lt;/code&gt; æ¨¡å¼&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æ³¨æ„ï¼š&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ &lt;code&gt;whisper-large-v3&lt;/code&gt; çš„æ¨¡å‹æ–‡ä»¶&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;ä¸‹è½½åœ°å€ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç™¾åº¦ç½‘ç›˜: &lt;a href=&#34;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&#34;&gt;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å¤¸å…‹ç½‘ç›˜ï¼š&lt;a href=&#34;https://pan.quark.cn/s/3ee3d991d64b&#34;&gt;https://pan.quark.cn/s/3ee3d991d64b&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° &lt;code&gt;.\MoneyPrinterTurbo\models&lt;/code&gt; é‡Œé¢ï¼Œ æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: &lt;code&gt;.\MoneyPrinterTurbo\models\whisper-large-v3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MoneyPrinterTurbo  &#xA;  â”œâ”€models&#xA;  â”‚   â””â”€whisper-large-v3&#xA;  â”‚          config.json&#xA;  â”‚          model.bin&#xA;  â”‚          preprocessor_config.json&#xA;  â”‚          tokenizer.json&#xA;  â”‚          vocabulary.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;èƒŒæ™¯éŸ³ä¹ ğŸµ&lt;/h2&gt; &#xA;&lt;p&gt;ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ &lt;code&gt;resource/songs&lt;/code&gt; ç›®å½•ä¸‹ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;å­—å¹•å­—ä½“ ğŸ…°&lt;/h2&gt; &#xA;&lt;p&gt;ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ &lt;code&gt;resource/fonts&lt;/code&gt; ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å¸¸è§é—®é¢˜ ğŸ¤”&lt;/h2&gt; &#xA;&lt;h3&gt;â“å¦‚ä½•ä½¿ç”¨å…è´¹çš„OpenAI GPT-3.5æ¨¡å‹?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/start-using-chatgpt-instantly&#34;&gt;OpenAIå®£å¸ƒChatGPTé‡Œé¢3.5å·²ç»å…è´¹äº†&lt;/a&gt;ï¼Œæœ‰å¼€å‘è€…å°†å…¶å°è£…æˆäº†APIï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¡®ä¿ä½ å®‰è£…å’Œå¯åŠ¨äº†dockeræœåŠ¡&lt;/strong&gt;ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨dockeræœåŠ¡&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -p 3040:3040 missuo/freegpt35&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯åŠ¨æˆåŠŸåï¼Œä¿®æ”¹ &lt;code&gt;config.toml&lt;/code&gt; ä¸­çš„é…ç½®&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_provider&lt;/code&gt; è®¾ç½®ä¸º &lt;code&gt;openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_api_key&lt;/code&gt; éšä¾¿å¡«å†™ä¸€ä¸ªå³å¯ï¼Œæ¯”å¦‚ &#39;123456&#39;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_base_url&lt;/code&gt; æ”¹ä¸º &lt;code&gt;http://localhost:3040/v1/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_model_name&lt;/code&gt; æ”¹ä¸º &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æ³¨æ„ï¼šè¯¥æ–¹å¼ç¨³å®šæ€§è¾ƒå·®&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;â“AttributeError: &#39;str&#39; object has no attribute &#39;choices&#39;`&lt;/h3&gt; &#xA;&lt;p&gt;è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºå¤§æ¨¡å‹æ²¡æœ‰è¿”å›æ­£ç¡®çš„å›å¤å¯¼è‡´çš„ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¤§æ¦‚ç‡æ˜¯ç½‘ç»œåŸå› ï¼Œ ä½¿ç”¨ &lt;strong&gt;VPN&lt;/strong&gt;ï¼Œæˆ–è€…è®¾ç½® &lt;code&gt;openai_base_url&lt;/code&gt; ä¸ºä½ çš„ä»£ç† ï¼Œåº”è¯¥å°±å¯ä»¥è§£å†³äº†ã€‚&lt;/p&gt; &#xA;&lt;p&gt;åŒæ—¶å»ºè®®ä½¿ç”¨ &lt;strong&gt;Moonshot&lt;/strong&gt; æˆ– &lt;strong&gt;DeepSeek&lt;/strong&gt; ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼Œè¿™ä¸¤ä¸ªæœåŠ¡å•†åœ¨å›½å†…è®¿é—®é€Ÿåº¦æ›´å¿«ï¼Œæ›´åŠ ç¨³å®šã€‚&lt;/p&gt; &#xA;&lt;h3&gt;â“RuntimeError: No ffmpeg exe could be found&lt;/h3&gt; &#xA;&lt;p&gt;é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚ ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: No ffmpeg exe could be found.&#xA;Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ­¤æ—¶ä½ å¯ä»¥ä» &lt;a href=&#34;https://www.gyan.dev/ffmpeg/builds/&#34;&gt;https://www.gyan.dev/ffmpeg/builds/&lt;/a&gt; ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® &lt;code&gt;ffmpeg_path&lt;/code&gt; ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[app]&#xA;# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\&#xA;ffmpeg_path = &#34;C:\\Users\\harry\\Downloads\\ffmpeg.exe&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ&lt;/h3&gt; &#xA;&lt;p&gt;å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚ è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-&lt;code&gt;X&lt;/code&gt;/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚ ä¿®æ”¹åŒ…å«&lt;code&gt;pattern=&#34;@&#34;&lt;/code&gt;çš„æ¡ç›®ï¼Œå°†&lt;code&gt;rights=&#34;none&#34;&lt;/code&gt;æ›´æ”¹ä¸º&lt;code&gt;rights=&#34;read|write&#34;&lt;/code&gt;ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚&lt;/p&gt; &#xA;&lt;h3&gt;â“OSError: [Errno 24] Too many open files&lt;/h3&gt; &#xA;&lt;p&gt;è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æŸ¥çœ‹å½“å‰é™åˆ¶&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ulimit -n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ulimit -n 10240&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯&lt;/h3&gt; &#xA;&lt;p&gt;LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass &#39;local files only=False&#39; as input.&lt;/p&gt; &#xA;&lt;p&gt;æˆ–è€…&lt;/p&gt; &#xA;&lt;p&gt;An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.&lt;/p&gt; &#xA;&lt;p&gt;è§£å†³æ–¹æ³•ï¼š&lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-&#34;&gt;ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;åé¦ˆå»ºè®® ğŸ“¢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¯ä»¥æäº¤ &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues&#34;&gt;issue&lt;/a&gt; æˆ–è€… &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/pulls&#34;&gt;pull request&lt;/a&gt;ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å‚è€ƒé¡¹ç›® ğŸ“š&lt;/h2&gt; &#xA;&lt;p&gt;è¯¥é¡¹ç›®åŸºäº &lt;a href=&#34;https://github.com/FujiwaraChoki/MoneyPrinter&#34;&gt;https://github.com/FujiwaraChoki/MoneyPrinter&lt;/a&gt; é‡æ„è€Œæ¥ï¼Œåšäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œå¢åŠ äº†æ›´å¤šçš„åŠŸèƒ½ã€‚ æ„Ÿè°¢åŸä½œè€…çš„å¼€æºç²¾ç¥ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;è®¸å¯è¯ ğŸ“&lt;/h2&gt; &#xA;&lt;p&gt;ç‚¹å‡»æŸ¥çœ‹ &lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; æ–‡ä»¶&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>assafelovic/gpt-researcher</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/assafelovic/gpt-researcher</id>
    <link href="https://github.com/assafelovic/gpt-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM based autonomous agent that conducts deep local and web research on any topic and generates a long report with citations.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;top&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3&#34; alt=&#34;Logo&#34; width=&#34;80&#34;&gt; &#xA; &lt;h4&gt;&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&amp;amp;logo=world&amp;amp;logoColor=white&amp;amp;color=0891b2&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&amp;amp;theme=clean-inverted&amp;amp;?compact=true&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&amp;amp;logoColor=white&amp;amp;style=flat&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub Release&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=grey&amp;amp;color=yellow&amp;amp;label=%20&amp;amp;style=flat&amp;amp;logoSize=40&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/gptresearcher/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&amp;amp;style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;color=1D63ED&#34; alt=&#34;Docker Image Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/assaf_elovic&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/assaf_elovic?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-zh_CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ja_JP.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ko_KR.md&#34;&gt;í•œêµ­ì–´&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;ğŸ” GPT Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPT Researcher is an open deep research agent designed for both web and local research on any given task.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Plan-and-Solve&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;&gt;RAG&lt;/a&gt; papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Objective conclusions for manual research can take weeks, requiring vast resources and time.&lt;/li&gt; &#xA; &lt;li&gt;LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.&lt;/li&gt; &#xA; &lt;li&gt;Current LLMs have token limitations, insufficient for generating long research reports.&lt;/li&gt; &#xA; &lt;li&gt;Limited web sources in existing services lead to misinformation and shallow results.&lt;/li&gt; &#xA; &lt;li&gt;Selective web sources can introduce bias into research tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&#34;&gt;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The core idea is to utilize &#39;planner&#39; and &#39;execution&#39; agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; height=&#34;600&#34; src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a task-specific agent based on a research query.&lt;/li&gt; &#xA; &lt;li&gt;Generate questions that collectively form an objective opinion on the task.&lt;/li&gt; &#xA; &lt;li&gt;Use a crawler agent for gathering information for each question.&lt;/li&gt; &#xA; &lt;li&gt;Summarize and source-track each resource.&lt;/li&gt; &#xA; &lt;li&gt;Filter and aggregate summaries into a final research report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.gptr.dev/blog/building-gpt-researcher&#34;&gt;How it Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea&#34;&gt;How to Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8&#34;&gt;Live Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“ Generate detailed research reports using web and local documents.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ–¼ï¸ Smart image scraping and filtering for reports.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“œ Generate detailed reports exceeding 2,000 words.&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒ Aggregate over 20 sources for objective conclusions.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ–¥ï¸ Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ” JavaScript-enabled web scraping.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“‚ Maintains memory and context throughout research.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“„ Export reports to PDF, Word, and other formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;âœ¨ Deep Research&lt;/h2&gt; &#xA;&lt;p&gt;GPT Researcher now includes Deep Research - an advanced recursive research workflow that explores topics with agentic depth and breadth. This feature employs a tree-like exploration pattern, diving deeper into subtopics while maintaining a comprehensive view of the research subject.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸŒ³ Tree-like exploration with configurable depth and breadth&lt;/li&gt; &#xA; &lt;li&gt;âš¡ï¸ Concurrent processing for faster results&lt;/li&gt; &#xA; &lt;li&gt;ğŸ¤ Smart context management across research branches&lt;/li&gt; &#xA; &lt;li&gt;â±ï¸ Takes ~5 minutes per deep research&lt;/li&gt; &#xA; &lt;li&gt;ğŸ’° Costs ~$0.4 per research (using &lt;code&gt;o3-mini&lt;/code&gt; on &#34;high&#34; reasoning effort)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research&#34;&gt;Learn more about Deep Research&lt;/a&gt; in our documentation.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Documentation&lt;/a&gt; for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installation and setup guides&lt;/li&gt; &#xA; &lt;li&gt;Configuration and customization options&lt;/li&gt; &#xA; &lt;li&gt;How-To examples&lt;/li&gt; &#xA; &lt;li&gt;Full API references&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;âš™ï¸ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python 3.11 or later. &lt;a href=&#34;https://www.tutorialsteacher.com/python/install-python&#34;&gt;Guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the project and navigate to the directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/assafelovic/gpt-researcher.git&#xA;cd gpt-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up API keys by exporting them or storing them in a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY={Your OpenAI API Key here}&#xA;export TAVILY_API_KEY={Your Tavily API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies and start the server:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m uvicorn main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; to start.&lt;/p&gt; &#xA;&lt;p&gt;For other setups (e.g., Poetry or virtual environments), check the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Getting Started page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run as PIP package&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gpt-researcher&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;...&#xA;from gpt_researcher import GPTResearcher&#xA;&#xA;query = &#34;why is Nvidia stock going up?&#34;&#xA;researcher = GPTResearcher(query=query, report_type=&#34;research_report&#34;)&#xA;# Conduct research on the given query&#xA;research_result = await researcher.conduct_research()&#xA;# Write the report&#xA;report = await researcher.write_report()&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more examples and configurations, please refer to the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package&#34;&gt;PIP documentation&lt;/a&gt; page.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run with Docker&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker&#34;&gt;Install Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Clone the &#39;.env.example&#39; file, add your API Keys to the cloned file and save the file as &#39;.env&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Within the docker-compose file comment out services that you don&#39;t want to run with Docker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If that doesn&#39;t work, try running it without the dash:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - By default, if you haven&#39;t uncommented anything in your docker-compose file, this flow will start 2 processes:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the Python server running on localhost:8000&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;the React app running on localhost:3000&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Visit localhost:3000 on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“„ Research on Local Documents&lt;/h2&gt; &#xA;&lt;p&gt;You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Add the env variable &lt;code&gt;DOC_PATH&lt;/code&gt; pointing to the folder where your documents are located.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DOC_PATH=&#34;./my-docs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you&#39;re running the frontend app on localhost:8000, simply select &#34;My Documents&#34; from the &#34;Report Source&#34; Dropdown Options.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re running GPT Researcher with the &lt;a href=&#34;https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package&#34;&gt;PIP package&lt;/a&gt;, pass the &lt;code&gt;report_source&lt;/code&gt; argument as &#34;local&#34; when you instantiate the &lt;code&gt;GPTResearcher&lt;/code&gt; class &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research&#34;&gt;code sample here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ‘ª Multi-Agent Assistant&lt;/h2&gt; &#xA;&lt;p&gt;As AI evolves from prompt engineering and RAG to multi-agent systems, we&#39;re excited to introduce our new multi-agent assistant built with &lt;a href=&#34;https://python.langchain.com/v0.1/docs/langgraph/&#34;&gt;LangGraph&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;STORM&lt;/a&gt; paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.&lt;/p&gt; &#xA;&lt;p&gt;An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.&lt;/p&gt; &#xA;&lt;p&gt;Check it out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents&#34;&gt;here&lt;/a&gt; or head over to our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph&#34;&gt;documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ–¥ï¸ Frontend Applications&lt;/h2&gt; &#xA;&lt;p&gt;GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An intuitive interface for inputting research queries&lt;/li&gt; &#xA; &lt;li&gt;Real-time progress tracking of research tasks&lt;/li&gt; &#xA; &lt;li&gt;Interactive display of research findings&lt;/li&gt; &#xA; &lt;li&gt;Customizable settings for tailored research experiences&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Two deployment options are available:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A lightweight static frontend served by FastAPI&lt;/li&gt; &#xA; &lt;li&gt;A feature-rich NextJS application for advanced functionality&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For detailed setup instructions and more information about the frontend features, please visit our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We highly welcome contributions! Please check out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/raw/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; if you&#39;re interested.&lt;/p&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://trello.com/b/3O7KBePw/gpt-researcher-roadmap&#34;&gt;roadmap&lt;/a&gt; page and reach out to us via our &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;Discord community&lt;/a&gt; if you&#39;re interested in joining our mission. &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=assafelovic/gpt-researcher&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;âœ‰ï¸ Support / Contact us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/spBgZmm3Xe&#34;&gt;Community Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Author Email: &lt;a href=&#34;mailto:assaf.elovic@gmail.com&#34;&gt;assaf.elovic@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ›¡ Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; &#xA;&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.&lt;/li&gt; &#xA; &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#assafelovic/gpt-researcher&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;   &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/#top&#34;&gt;â¬†ï¸ Back to Top&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hiyouga/LLaMA-Factory</title>
    <updated>2025-03-01T02:00:31Z</updated>
    <id>tag:github.com,2025-03-01:/hiyouga/LLaMA-Factory</id>
    <link href="https://github.com/hiyouga/LLaMA-Factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png&#34; alt=&#34;# LLaMA Factory&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub workflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llamafactory/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llamafactory&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://scholar.google.com/scholar?cites=12620864006390196564&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-319-green&#34; alt=&#34;Citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/llamafactory_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/llamafactory_ai&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/rKfvV9r9FK&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitcode.com/zhengyaowei/LLaMA-Factory&#34;&gt;&lt;img src=&#34;https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg?sanitize=true&#34; alt=&#34;GitCode&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&#34;&gt;&lt;img src=&#34;https://gallery.pai-ml.com/assets/open-in-dsw.svg?sanitize=true&#34; alt=&#34;Open in DSW&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue&#34; alt=&#34;Studios&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Easily fine-tune 100+ large language models with zero-code &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&#34;&gt;CLI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&#34;&gt;Web UI&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;Github trend&#34; src=&#34;https://trendshift.io/api/badge/repositories/4535&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p&gt;ğŸ‘‹ Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_npu.jpg&#34;&gt;NPU user group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7c96b465-9df7-45f4-8053-bf03e58386d3&#34;&gt;https://github.com/user-attachments/assets/7c96b465-9df7-45f4-8053-bf03e58386d3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose your path:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documentation (WIP)&lt;/strong&gt;: &lt;a href=&#34;https://llamafactory.readthedocs.io/zh-cn/latest/&#34;&gt;https://llamafactory.readthedocs.io/zh-cn/latest/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colab&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PAI-DSW&lt;/strong&gt;: &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&#34;&gt;Llama3 Example&lt;/a&gt; | &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl&#34;&gt;Qwen2-VL Example&lt;/a&gt; | &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b&#34;&gt;DeepSeek-R1-Distill Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt;: &lt;a href=&#34;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches&#34;&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets&#34;&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement&#34;&gt;Requirement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&#34;&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker&#34;&gt;Build Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm&#34;&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&#34;&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&#34;&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger&#34;&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&#34;&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&#34;&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href=&#34;https://github.com/jiaweizzhao/GaLore&#34;&gt;GaLore&lt;/a&gt;, &lt;a href=&#34;https://github.com/Ledzy/BAdam&#34;&gt;BAdam&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhuhanqing/APOLLO&#34;&gt;APOLLO&lt;/a&gt;, &lt;a href=&#34;https://github.com/zyushun/Adam-mini&#34;&gt;Adam-mini&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;Unsloth&lt;/a&gt;, &lt;a href=&#34;https://github.com/linkedin/Liger-Kernel&#34;&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with vLLM worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Support Date&lt;/th&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Day 0&lt;/td&gt; &#xA;   &lt;td&gt;Qwen2.5 / Qwen2-VL / QwQ / QvQ / InternLM3 / MiniCPM-o-2.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Day 1&lt;/td&gt; &#xA;   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Compared to ChatGLM&#39;s &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning&#34;&gt;P-Tuning&lt;/a&gt;, LLaMA Factory&#39;s LoRA tuning offers up to &lt;strong&gt;3.7 times faster&lt;/strong&gt; training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#39;s QLoRA further improves the efficiency regarding the GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/benchmark.svg?sanitize=true&#34; alt=&#34;benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Definitions&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Training Speed&lt;/strong&gt;: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Rouge Score&lt;/strong&gt;: Rouge-2 score on the development set of the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;advertising text generation&lt;/a&gt; task. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;We adopt &lt;code&gt;pre_seq_len=128&lt;/code&gt; for ChatGLM&#39;s P-Tuning and &lt;code&gt;lora_rank=32&lt;/code&gt; for LLaMA Factory&#39;s LoRA tuning.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href=&#34;https://github.com/hiyouga/EasyR1&#34;&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; &#xA;&lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct&#34;&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; &#xA;&lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-R1&#34;&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Full Changelog&lt;/summary&gt; &#xA; &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.05270&#34;&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6&#34;&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/BUAADreamer&#34;&gt;@BUAADreamer&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/collections/internlm/&#34;&gt;InternLM3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/hhaAndroid&#34;&gt;@hhaAndroid&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-4&#34;&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; &#xA; &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&#34;&gt;this section&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B&#34;&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&#34;&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; &#xA; &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&#34;https://modelers.cn/models&#34;&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&#34;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-vl/&#34;&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&#34;https://github.com/simonJJJ&#34;&gt;@simonJJJ&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/linkedin/Liger-Kernel&#34;&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; &#xA; &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/zyushun/Adam-mini&#34;&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage. Thank &lt;a href=&#34;https://github.com/relic-yuexi&#34;&gt;@relic-yuexi&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/07/04] We supported &lt;a href=&#34;https://github.com/MeetKai/functionary/tree/main/functionary/train/packing&#34;&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href=&#34;https://github.com/chuan298&#34;&gt;@chuan298&lt;/a&gt;&#39;s PR.&lt;/p&gt; &#xA; &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02948&#34;&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://github.com/THUDM/GLM-4&#34;&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14734&#34;&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.01306&#34;&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&#34;&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href=&#34;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat&#34;&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/zhichen/Llama3-Chinese&#34;&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02258&#34;&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href=&#34;https://github.com/astramind-ai/Mixture-of-depths&#34;&gt;AstraMindAI&#39;s implementation&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/21] Our paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2403.13372&#34;&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;&#34; is available at arXiv!&lt;/p&gt; &#xA; &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.12354&#34;&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/TencentARC/LLaMA-Pro&#34;&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5/&#34;&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&#34;&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&#34;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.05914&#34;&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/dvlab-research/LongLoRA&#34;&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href=&#34;https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat&#34;&gt;LLaMA-2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-13B-sft&#34;&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href=&#34;https://github.com/KanadeSiina&#34;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&#34;https://github.com/codemayq&#34;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; âš¡ğŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-7B-sft&#34;&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/22] We aligned the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model size&lt;/th&gt; &#xA;   &lt;th&gt;Template&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc&#34;&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;baichuan2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience&#34;&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;chatglm3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CohereForAI&#34;&gt;Command R&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;35B/104B&lt;/td&gt; &#xA;   &lt;td&gt;cohere&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;236B/671B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; &#xA;   &lt;td&gt;deepseek3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; &#xA;   &lt;td&gt;falcon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google&#34;&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; &#xA;   &lt;td&gt;gemma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;GLM-4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;9B&lt;/td&gt; &#xA;   &lt;td&gt;glm4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai-community&#34;&gt;GPT-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ibm-granite&#34;&gt;Granite 3.0-3.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; &#xA;   &lt;td&gt;granite3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IndexTeam&#34;&gt;Index&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9B&lt;/td&gt; &#xA;   &lt;td&gt;index&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm&#34;&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8B/20B&lt;/td&gt; &#xA;   &lt;td&gt;intern2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/70B&lt;/td&gt; &#xA;   &lt;td&gt;llama2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; &#xA;   &lt;td&gt;llama3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11B/90B&lt;/td&gt; &#xA;   &lt;td&gt;mllama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;llava&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; &#xA;   &lt;td&gt;llava_next&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/34B&lt;/td&gt; &#xA;   &lt;td&gt;llava_next_video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openbmb&#34;&gt;MiniCPM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/2B/4B&lt;/td&gt; &#xA;   &lt;td&gt;cpm/cpm3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openbmb&#34;&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B/12B&lt;/td&gt; &#xA;   &lt;td&gt;ministral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; &#xA;   &lt;td&gt;mistral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;24B&lt;/td&gt; &#xA;   &lt;td&gt;mistral_small&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai&#34;&gt;OLMo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google&#34;&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/10B/28B&lt;/td&gt; &#xA;   &lt;td&gt;paligemma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3B/2.7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4B/14B&lt;/td&gt; &#xA;   &lt;td&gt;phi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;phi_small&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;phi4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Pixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12B&lt;/td&gt; &#xA;   &lt;td&gt;pixtral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen/QwQ (1-2.5) (Code/Math/MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; &#xA;   &lt;td&gt;qwen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;qwen2_audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/3B/7B/72B&lt;/td&gt; &#xA;   &lt;td&gt;qwen2_vl&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Skywork&#34;&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;skywork_o1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode&#34;&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/7B/15B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI&#34;&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; &#xA;   &lt;td&gt;telechat2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xverse&#34;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/65B&lt;/td&gt; &#xA;   &lt;td&gt;xverse&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; &#xA;   &lt;td&gt;yi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B/34B&lt;/td&gt; &#xA;   &lt;td&gt;yi_vl&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IEITYuan&#34;&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/51B/102B&lt;/td&gt; &#xA;   &lt;td&gt;yuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] For the &#34;base&#34; models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the &#34;instruct/chat&#34; models.&lt;/p&gt; &#xA; &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py&#34;&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; &#xA;&lt;p&gt;You also can add a custom chat template to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py&#34;&gt;template.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Approach&lt;/th&gt; &#xA;   &lt;th&gt;Full-tuning&lt;/th&gt; &#xA;   &lt;th&gt;Freeze-tuning&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;QLoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pre-Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reward Modeling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KTO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ORPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href=&#34;https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Provided Datasets&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Pre-training datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt&#34;&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2&#34;&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/olm/olm-wikipedia-20221220&#34;&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&#34;&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/EleutherAI/pile&#34;&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Skywork/SkyPile-150B&#34;&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceFW/fineweb&#34;&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu&#34;&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/the-stack&#34;&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/starcoderdata&#34;&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json&#34;&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3&#34;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2&#34;&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/GAIR/lima&#34;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&#34;&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/OpenOrca&#34;&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/SlimOrca&#34;&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&#34;&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/wiki_qa&#34;&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/zxbsmk/webnovel_cn&#34;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&#34;&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HasturOfficial/adgen&#34;&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k&#34;&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/shibing624/sharegpt_gpt4&#34;&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k&#34;&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/THUDM/AgentInstruct&#34;&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/lmsys/lmsys-chat-1m&#34;&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k&#34;&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceTB/cosmopedia&#34;&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/stem_zh_instruction&#34;&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo&#34;&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/m-a-p/neo_sft_phase2&#34;&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered&#34;&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/argilla/magpie-ultra-v0.1&#34;&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/WebInstructSub&#34;&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&#34;&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&#34;&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/open-r1/OpenR1-Math-220k&#34;&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT&#34;&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k&#34;&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions&#34;&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/oasst_de&#34;&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de&#34;&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de&#34;&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de&#34;&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de&#34;&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolphin_de&#34;&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/booksum_de&#34;&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de&#34;&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de&#34;&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Preference datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k&#34;&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized&#34;&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/openbmb/RLHF-V-Dataset&#34;&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Zhihui/VLFeedback&#34;&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Intel/orca_dpo_pairs&#34;&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de&#34;&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/argilla/kto-mix-15k&#34;&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Mandatory&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;python&lt;/td&gt; &#xA;   &lt;td&gt;3.9&lt;/td&gt; &#xA;   &lt;td&gt;3.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;torch&lt;/td&gt; &#xA;   &lt;td&gt;1.13.1&lt;/td&gt; &#xA;   &lt;td&gt;2.4.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;transformers&lt;/td&gt; &#xA;   &lt;td&gt;4.41.2&lt;/td&gt; &#xA;   &lt;td&gt;4.49.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;datasets&lt;/td&gt; &#xA;   &lt;td&gt;2.16.0&lt;/td&gt; &#xA;   &lt;td&gt;3.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;accelerate&lt;/td&gt; &#xA;   &lt;td&gt;0.34.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;peft&lt;/td&gt; &#xA;   &lt;td&gt;0.11.1&lt;/td&gt; &#xA;   &lt;td&gt;0.12.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trl&lt;/td&gt; &#xA;   &lt;td&gt;0.8.6&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Optional&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;11.6&lt;/td&gt; &#xA;   &lt;td&gt;12.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deepspeed&lt;/td&gt; &#xA;   &lt;td&gt;0.10.0&lt;/td&gt; &#xA;   &lt;td&gt;0.16.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bitsandbytes&lt;/td&gt; &#xA;   &lt;td&gt;0.39.0&lt;/td&gt; &#xA;   &lt;td&gt;0.43.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vllm&lt;/td&gt; &#xA;   &lt;td&gt;0.4.3&lt;/td&gt; &#xA;   &lt;td&gt;0.7.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;flash-attn&lt;/td&gt; &#xA;   &lt;td&gt;2.3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.7.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Hardware Requirement&lt;/h3&gt; &#xA;&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;7B&lt;/th&gt; &#xA;   &lt;th&gt;13B&lt;/th&gt; &#xA;   &lt;th&gt;30B&lt;/th&gt; &#xA;   &lt;th&gt;70B&lt;/th&gt; &#xA;   &lt;th&gt;110B&lt;/th&gt; &#xA;   &lt;th&gt;8x7B&lt;/th&gt; &#xA;   &lt;th&gt;8x22B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;   &lt;td&gt;2000GB&lt;/td&gt; &#xA;   &lt;td&gt;900GB&lt;/td&gt; &#xA;   &lt;td&gt;2400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;300GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;900GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;200GB&lt;/td&gt; &#xA;   &lt;td&gt;360GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;32GB&lt;/td&gt; &#xA;   &lt;td&gt;64GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;320GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;140GB&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;72GB&lt;/td&gt; &#xA;   &lt;td&gt;30GB&lt;/td&gt; &#xA;   &lt;td&gt;96GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;4GB&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;18GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git&#xA;cd LLaMA-Factory&#xA;pip install -e &#34;.[torch,metrics]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, apollo, badam, adam-mini, qwen, minicpm_v, modelscope, openmind, swanlab, quality&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Use &lt;code&gt;pip install --no-deps -e .&lt;/code&gt; to resolve package conflicts.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Create an isolated Python environment with &lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv sync --extra torch --extra metrics --prerelease=allow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Windows users&lt;/summary&gt; &#xA; &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; &#xA; &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href=&#34;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&#34;&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; &#xA; &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href=&#34;https://huggingface.co/lldacing/flash-attention-windows-wheel&#34;&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; &#xA; &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e &#34;.[torch-npu,metrics]&#34;&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href=&#34;https://www.hiascend.com/developer/download/community/result?module=cann&#34;&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href=&#34;https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html&#34;&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# replace the url according to your CANN version and devices&#xA;# install CANN Toolkit&#xA;wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run&#xA;bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install&#xA;&#xA;# install CANN Kernels&#xA;wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run&#xA;bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install&#xA;&#xA;# set env variables&#xA;source /usr/local/Ascend/ascend-toolkit/set_env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Requirement&lt;/th&gt; &#xA;    &lt;th&gt;Minimum&lt;/th&gt; &#xA;    &lt;th&gt;Recommend&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CANN&lt;/td&gt; &#xA;    &lt;td&gt;8.0.RC1&lt;/td&gt; &#xA;    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torch&lt;/td&gt; &#xA;    &lt;td&gt;2.1.0&lt;/td&gt; &#xA;    &lt;td&gt;2.4.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torch-npu&lt;/td&gt; &#xA;    &lt;td&gt;2.1.0&lt;/td&gt; &#xA;    &lt;td&gt;2.4.0.post2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;deepspeed&lt;/td&gt; &#xA;    &lt;td&gt;0.13.2&lt;/td&gt; &#xA;    &lt;td&gt;0.16.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; &#xA; &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; &#xA; &lt;p&gt;Download the pre-built Docker images: &lt;a href=&#34;http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html&#34;&gt;32GB&lt;/a&gt; | &lt;a href=&#34;http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html&#34;&gt;64GB&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; &#xA; &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href=&#34;https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU&#34;&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install bitsandbytes from source&#xA;# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch&#xA;git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git&#xA;cd bitsandbytes/&#xA;&#xA;# Install dependencies&#xA;pip install -r requirements-dev.txt&#xA;&#xA;# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference&#xA;apt-get install -y build-essential cmake&#xA;&#xA;# Compile &amp;amp; install  &#xA;cmake -DCOMPUTE_BACKEND=npu -S .&#xA;make&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b main https://github.com/huggingface/transformers.git&#xA;cd transformers&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml&#34;&gt;example&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope / Modelers hub or load the dataset in local disk.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml&#xA;llamafactory-cli chat examples/inference/llama3_lora_sft.yaml&#xA;llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llamafactory-cli webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build Docker&lt;/h3&gt; &#xA;&lt;p&gt;For CUDA users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-cuda/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Ascend NPU users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-npu/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For AMD ROCm users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker/docker-rocm/&#xA;docker compose up -d&#xA;docker compose exec llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; &#xA; &lt;p&gt;For CUDA users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f ./docker/docker-cuda/Dockerfile \&#xA;    --build-arg INSTALL_BNB=false \&#xA;    --build-arg INSTALL_VLLM=false \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg INSTALL_FLASHATTN=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;docker run -dit --gpus=all \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For Ascend NPU users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Choose docker image upon your environment&#xA;docker build -f ./docker/docker-npu/Dockerfile \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;# Change `device` upon your resources&#xA;docker run -dit \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -v /usr/local/dcmi:/usr/local/dcmi \&#xA;    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \&#xA;    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \&#xA;    -v /etc/ascend_install.info:/etc/ascend_install.info \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --device /dev/davinci0 \&#xA;    --device /dev/davinci_manager \&#xA;    --device /dev/devmm_svm \&#xA;    --device /dev/hisi_hdc \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For AMD ROCm users:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f ./docker/docker-rocm/Dockerfile \&#xA;    --build-arg INSTALL_BNB=false \&#xA;    --build-arg INSTALL_VLLM=false \&#xA;    --build-arg INSTALL_DEEPSPEED=false \&#xA;    --build-arg INSTALL_FLASHATTN=false \&#xA;    --build-arg PIP_INDEX=https://pypi.org/simple \&#xA;    -t llamafactory:latest .&#xA;&#xA;docker run -dit \&#xA;    -v ./hf_cache:/root/.cache/huggingface \&#xA;    -v ./ms_cache:/root/.cache/modelscope \&#xA;    -v ./om_cache:/root/.cache/openmind \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -v ./saves:/app/saves \&#xA;    -p 7860:7860 \&#xA;    -p 8000:8000 \&#xA;    --device /dev/kfd \&#xA;    --device /dev/dri \&#xA;    --shm-size 16G \&#xA;    --name llamafactory \&#xA;    llamafactory:latest&#xA;&#xA;docker exec -it llamafactory bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Details about volume&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ms_cache&lt;/code&gt;: Similar to Hugging Face cache but for ModelScope users.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;om_cache&lt;/code&gt;: Similar to Hugging Face cache but for Modelers users.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Visit &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; &#xA; &lt;p&gt;Examples: &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py&#34;&gt;Image understanding&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py&#34;&gt;Function calling&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; &#xA;&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; &#xA;&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&#34;https://modelers.cn/models&#34;&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://wandb.ai&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;report_to: wandb&#xA;run_name: test_run # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href=&#34;https://wandb.ai/authorize&#34;&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; &#xA;&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;use_swanlab: true&#xA;swanlab_run_name: test_run # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href=&#34;https://swanlab.cn/settings&#34;&gt;API key&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href=&#34;https://swanlab.cn/settings&#34;&gt;API key&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; &#xA;&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click to show&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.02223&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10092&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10526&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href=&#34;https://arxiv.org/abs/2311.07816&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href=&#34;https://arxiv.org/abs/2312.15710&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.04319&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.07286&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.05904&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.07625&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11176&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11187&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11746&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11801&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href=&#34;https://arxiv.org/abs/2402.11809&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11819&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.12204&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.14714&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.15043&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.02333&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.03419&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.08228&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.09073&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.14541&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.15246&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16008&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16443&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.00604&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.04167&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.04316&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.07084&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.09836&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.11581&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.14215&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.16621&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href=&#34;https://arxiv.org/abs/2404.17140&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.18585&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.04760&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Dammu et al. &#34;They are uncultured&#34;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.05378&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.09055&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.12739&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.13816&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2405.20215&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href=&#34;https://aclanthology.org/2024.lt4hala-1.30&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.00380&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.02106&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.03136&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.04496&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.05688&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.05955&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.06973&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.07115&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.07815&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.10099&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.10173&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.12074&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.14408&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.14546&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.15695&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.17233&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.18069&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href=&#34;https://aclanthology.org/2024.americasnlp-1.25&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href=&#34;https://arxiv.org/abs/2406.19949&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.00365&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.01470&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.06129&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.08044&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.09756&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href=&#34;https://scholarcommons.scu.edu/cseng_senior/272/&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.13561&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.16637&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.17535&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href=&#34;https://arxiv.org/abs/2407.19705&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.00137&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href=&#34;https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.04693&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.04168&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href=&#34;https://aclanthology.org/2024.finnlp-2.1/&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href=&#34;https://arxiv.org/abs/2408.08072&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3627673.3679611&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Yu-Yang-Li/StarWhisper&#34;&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-LawLLM&#34;&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/X-D-Lab/Sunsimiao&#34;&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/WangRongsheng/CareGPT&#34;&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Machine-Mindset/&#34;&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Nekochu/Luminia-13B-v3&#34;&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href=&#34;https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt&#34;&gt;[demo]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/BUAADreamer/Chinese-LLaVA-Med&#34;&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/THUDM/AutoRE&#34;&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NVIDIA/RTX-AI-Toolkit&#34;&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/LazyAGI/LazyLLM&#34;&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NLPJCL/RAG-Retrieval&#34;&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/987727357&#34;&gt;[blog]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Qihoo360/360-LLaMA-Factory&#34;&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://novasky-ai.github.io/posts/sky-t1/&#34;&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf&#34;&gt;Baichuan 2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigscience/license&#34;&gt;BLOOM&lt;/a&gt; / &lt;a href=&#34;https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE&#34;&gt;ChatGLM3&lt;/a&gt; / &lt;a href=&#34;https://cohere.com/c4ai-cc-by-nc-license&#34;&gt;Command R&lt;/a&gt; / &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;DeepSeek&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt&#34;&gt;Falcon&lt;/a&gt; / &lt;a href=&#34;https://ai.google.dev/gemma/terms&#34;&gt;Gemma&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE&#34;&gt;GLM-4&lt;/a&gt; / &lt;a href=&#34;https://github.com/openai/gpt-2/raw/master/LICENSE&#34;&gt;GPT-2&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Granite&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE&#34;&gt;Index&lt;/a&gt; / &lt;a href=&#34;https://github.com/InternLM/InternLM#license&#34;&gt;InternLM&lt;/a&gt; / &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Llama&lt;/a&gt; / &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama 2 (LLaVA-1.5)&lt;/a&gt; / &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama 3&lt;/a&gt; / &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&#34;&gt;MiniCPM&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;OLMo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx&#34;&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&#34;&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT&#34;&gt;Qwen&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf&#34;&gt;Skywork&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;StarCoder 2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;TeleChat2&lt;/a&gt; / &lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf&#34;&gt;XVERSE&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE&#34;&gt;Yi&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Yi-1.5&lt;/a&gt; / &lt;a href=&#34;https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan&#34;&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{zheng2024llamafactory,&#xA;  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},&#xA;  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},&#xA;  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},&#xA;  address={Bangkok, Thailand},&#xA;  publisher={Association for Computational Linguistics},&#xA;  year={2024},&#xA;  url={http://arxiv.org/abs/2403.13372}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;TRL&lt;/a&gt;, &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>