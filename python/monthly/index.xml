<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-01T01:50:57Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zeyi-Lin/HivisionIDPhotos</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/Zeyi-Lin/HivisionIDPhotos</id>
    <link href="https://github.com/Zeyi-Lin/HivisionIDPhotos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;âš¡ï¸HivisionIDPhotos: a lightweight and efficient AI ID photos tools. ä¸€ä¸ªè½»é‡çº§çš„AIè¯ä»¶ç…§åˆ¶ä½œç®—æ³•ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;hivision_logo&#34; src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/hivision_logo.png&#34; width=&#34;120&#34; height=&#34;120&#34;&gt; &#xA; &lt;h1&gt;HivisionIDPhoto&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_EN.md&#34;&gt;English&lt;/a&gt; / ä¸­æ–‡ / &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_JP.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_KO.md&#34;&gt;í•œêµ­ì–´&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&amp;amp;labelColor=black&amp;amp;logo=github&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/linzeyi/hivision_idphotos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&amp;amp;label=docker&amp;amp;labelColor=black&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://docs.qq.com/doc/DUkpBdk90eWZFS2JW&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-%E5%BE%AE%E4%BF%A1-4cb55e&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo&#34;&gt;&lt;img src=&#34;https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&amp;amp;path=swanhub.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelers.cn/spaces/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;amp;ytag=HG_GPU_HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/11622&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/11622&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6&#34;&gt;&lt;img src=&#34;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&amp;amp;claim_uid=Oh5UaGjfrblg0yZ&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/demoImage.jpg&#34; width=&#34;900&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;ç›¸å…³é¡¹ç›®&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt;ï¼šä¸€ä¸ªå¼€æºã€ç°ä»£åŒ–è®¾è®¡çš„æ·±åº¦å­¦ä¹ è®­ç»ƒè·Ÿè¸ªä¸å¯è§†åŒ–å·¥å…·ï¼ŒåŒæ—¶æ”¯æŒäº‘ç«¯/ç¦»çº¿ä½¿ç”¨ï¼Œå›½å†…å¥½ç”¨çš„Wandbå¹³æ›¿ï¼›é€‚é…30+ä¸»æµæ¡†æ¶ï¼ˆPyTorchã€HuggingFace Transformersã€LLaMA Factoryã€Lightningç­‰ï¼‰ï¼Œæ¬¢è¿ä½¿ç”¨ï¼&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ç›®å½•&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E6%9C%80%E8%BF%91%E6%9B%B4%E6%96%B0&#34;&gt;æœ€è¿‘æ›´æ–°&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B&#34;&gt;é¡¹ç›®ç®€ä»‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E7%A4%BE%E5%8C%BA&#34;&gt;ç¤¾åŒº&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C&#34;&gt;å‡†å¤‡å·¥ä½œ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E8%BF%90%E8%A1%8C-gradio-demo&#34;&gt;Demoå¯åŠ¨&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-python-%E6%8E%A8%E7%90%86&#34;&gt;Pythonæ¨ç†&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#%EF%B8%8F-%E9%83%A8%E7%BD%B2-api-%E6%9C%8D%E5%8A%A1&#34;&gt;APIæœåŠ¡éƒ¨ç½²&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-docker-%E9%83%A8%E7%BD%B2&#34;&gt;Dockeréƒ¨ç½²&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC&#34;&gt;è”ç³»æˆ‘ä»¬&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E6%84%9F%E8%B0%A2%E6%94%AF%E6%8C%81&#34;&gt;æ„Ÿè°¢æ”¯æŒ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-lincese&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E5%BC%95%E7%94%A8&#34;&gt;å¼•ç”¨&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ¤© æœ€è¿‘æ›´æ–°&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;åœ¨çº¿ä½“éªŒï¼š &lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=SwanHub%20Demo&amp;amp;color=blue&#34; alt=&#34;SwanHub Demo&#34;&gt;&lt;/a&gt;ã€&lt;a href=&#34;https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Spaces&#34;&gt;&lt;/a&gt;ã€&lt;a href=&#34;https://modelscope.cn/studios/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt;ã€&lt;a href=&#34;https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;amp;ytag=HG_GPU_HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.11.20: Gradio Demoå¢åŠ &lt;strong&gt;æ‰“å°æ’ç‰ˆ&lt;/strong&gt;é€‰é¡¹å¡ï¼Œæ”¯æŒå…­å¯¸ã€äº”å¯¸ã€A4ã€3Rã€4Räº”ç§æ’ç‰ˆå°ºå¯¸&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.11.16: APIæ¥å£å¢åŠ ç¾é¢œå‚æ•°&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.25: å¢åŠ &lt;strong&gt;äº”å¯¸ç›¸çº¸&lt;/strong&gt;å’Œ&lt;strong&gt;JPEGä¸‹è½½&lt;/strong&gt;é€‰é¡¹ï½œé»˜è®¤ç…§ç‰‡ä¸‹è½½æ”¯æŒ300DPI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.24: APIæ¥å£å¢åŠ base64å›¾åƒä¼ å…¥é€‰é¡¹ | Gradio Demoå¢åŠ &lt;strong&gt;æ’ç‰ˆç…§è£å‰ªçº¿&lt;/strong&gt;åŠŸèƒ½&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.22: Gradio Demoå¢åŠ &lt;strong&gt;é‡å…½æ¨¡å¼&lt;/strong&gt;ï¼Œå¯è®¾ç½®å†…å­˜åŠ è½½ç­–ç•¥ | APIæ¥å£å¢åŠ &lt;strong&gt;dpiã€face_alignment&lt;/strong&gt;å‚æ•°&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.18: Gradio Demoå¢åŠ &lt;strong&gt;åˆ†äº«æ¨¡ç‰ˆç…§&lt;/strong&gt;åŠŸèƒ½ã€å¢åŠ &lt;strong&gt;ç¾å¼è¯ä»¶ç…§&lt;/strong&gt;èƒŒæ™¯é€‰é¡¹&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.17: Gradio Demoå¢åŠ &lt;strong&gt;è‡ªå®šä¹‰åº•è‰²-HEXè¾“å…¥&lt;/strong&gt;åŠŸèƒ½ | &lt;strong&gt;ï¼ˆç¤¾åŒºè´¡çŒ®ï¼‰C++ç‰ˆæœ¬&lt;/strong&gt; - &lt;a href=&#34;https://github.com/zjkhahah/HivisionIDPhotos-cpp&#34;&gt;HivisionIDPhotos-cpp&lt;/a&gt; è´¡çŒ® by &lt;a href=&#34;https://github.com/zjkhahah&#34;&gt;zjkhahah&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.16: Gradio Demoå¢åŠ &lt;strong&gt;äººè„¸æ—‹è½¬å¯¹é½&lt;/strong&gt;åŠŸèƒ½ï¼Œè‡ªå®šä¹‰å°ºå¯¸è¾“å…¥æ”¯æŒ&lt;strong&gt;æ¯«ç±³&lt;/strong&gt;å•ä½&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;é¡¹ç›®ç®€ä»‹&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ğŸš€ è°¢è°¢ä½ å¯¹æˆ‘ä»¬çš„å·¥ä½œæ„Ÿå…´è¶£ã€‚æ‚¨å¯èƒ½è¿˜æƒ³æŸ¥çœ‹æˆ‘ä»¬åœ¨å›¾åƒé¢†åŸŸçš„å…¶ä»–æˆæœï¼Œæ¬¢è¿æ¥ä¿¡:&lt;a href=&#34;mailto:zeyi.lin@swanhub.co&#34;&gt;zeyi.lin@swanhub.co&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;HivisionIDPhoto æ—¨åœ¨å¼€å‘ä¸€ç§å®ç”¨ã€ç³»ç»Ÿæ€§çš„è¯ä»¶ç…§æ™ºèƒ½åˆ¶ä½œç®—æ³•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å®ƒåˆ©ç”¨ä¸€å¥—å®Œå–„çš„AIæ¨¡å‹å·¥ä½œæµç¨‹ï¼Œå®ç°å¯¹å¤šç§ç”¨æˆ·æ‹ç…§åœºæ™¯çš„è¯†åˆ«ã€æŠ å›¾ä¸è¯ä»¶ç…§ç”Ÿæˆã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HivisionIDPhoto å¯ä»¥åšåˆ°ï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;è½»é‡çº§æŠ å›¾ï¼ˆçº¯ç¦»çº¿ï¼Œä»…éœ€ &lt;strong&gt;CPU&lt;/strong&gt; å³å¯å¿«é€Ÿæ¨ç†ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;æ ¹æ®ä¸åŒå°ºå¯¸è§„æ ¼ç”Ÿæˆä¸åŒçš„æ ‡å‡†è¯ä»¶ç…§ã€å…­å¯¸æ’ç‰ˆç…§&lt;/li&gt; &#xA; &lt;li&gt;æ”¯æŒ çº¯ç¦»çº¿ æˆ– ç«¯äº‘ æ¨ç†&lt;/li&gt; &#xA; &lt;li&gt;ç¾é¢œ&lt;/li&gt; &#xA; &lt;li&gt;æ™ºèƒ½æ¢æ­£è£…ï¼ˆwaitingï¼‰&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/demo.png&#34; width=&#34;900&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;å¦‚æœ HivisionIDPhoto å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯· star è¿™ä¸ª repo æˆ–æ¨èç»™ä½ çš„æœ‹å‹ï¼Œè§£å†³è¯ä»¶ç…§åº”æ€¥åˆ¶ä½œé—®é¢˜ï¼&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ  ç¤¾åŒº&lt;/h1&gt; &#xA;&lt;p&gt;æˆ‘ä»¬åˆ†äº«äº†ä¸€äº›ç”±ç¤¾åŒºæ„å»ºçš„HivisionIDPhotosçš„æœ‰è¶£åº”ç”¨å’Œæ‰©å±•ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&#34;&gt;HivisionIDPhotos-ComfyUI&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt;HivisionIDPhotos-wechat-weapp&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/comfyui.png&#34; width=&#34;900&#34; alt=&#34;ComfyUI workflow&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-wechat-miniprogram.png&#34; width=&#34;900&#34; alt=&#34;ComfyUI workflow&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ComfyUIè¯ä»¶ç…§å¤„ç†å·¥ä½œæµ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼ˆJAVAåç«¯+åŸç”Ÿå‰ç«¯ï¼‰&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/soulerror/HivisionIDPhotos-Uniapp&#34;&gt;HivisionIDPhotos-Uniapp&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-web&#34;&gt;HivisionIDPhotos-web&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/soulerror/HivisionIDPhotos-Uniapp&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-uniapp-wechat-miniprogram.png&#34; width=&#34;900&#34; alt=&#34;HivisionIDPhotos-uniapp&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-web&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-web.png&#34; width=&#34;900&#34; alt=&#34;HivisionIDPhotos-uniapp&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼ˆuniappï¼‰&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯ä»¶ç…§åº”ç”¨ç½‘é¡µç‰ˆ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zjkhahah/HivisionIDPhotos-cpp&#34;&gt;HivisionIDPhotos-cpp&lt;/a&gt;: HivisionIDphotos C++ç‰ˆæœ¬ï¼Œç”± &lt;a href=&#34;https://github.com/zjkhahah&#34;&gt;zjkhahah&lt;/a&gt; æ„å»º&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wmlcjj/ai-idphoto&#34;&gt;ai-idphoto&lt;/a&gt;: &lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt;HivisionIDPhotos-wechat-weapp&lt;/a&gt; çš„uniappå¤šç«¯å…¼å®¹ç‰ˆï¼Œç”± &lt;a href=&#34;https://github.com/wmlcjj&#34;&gt;wmlcjj&lt;/a&gt; è´¡çŒ®&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/&#34;&gt;HivisionIDPhotos-uniapp-WeChat-gpto1&lt;/a&gt;: ç”±gpt-o1è¾…åŠ©å®Œæˆå¼€å‘çš„è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼Œç”± &lt;a href=&#34;https://github.com/jkm199&#34;&gt;jkm199&lt;/a&gt; è´¡çŒ®&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI&#34;&gt;HivisionIDPhotos-windows-GUI&lt;/a&gt;ï¼šWindowså®¢æˆ·ç«¯åº”ç”¨ï¼Œç”± &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;zhaoyun0071&lt;/a&gt; æ„å»º&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ONG-Leo/HivisionIDPhotos-NAS&#34;&gt;HivisionIDPhotos-NAS&lt;/a&gt;: ç¾¤æ™–NASéƒ¨ç½²ä¸­æ–‡æ•™ç¨‹ï¼Œç”± &lt;a href=&#34;https://github.com/ONG-Leo&#34;&gt;ONG-Leo&lt;/a&gt; è´¡çŒ®&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ”§ å‡†å¤‡å·¥ä½œ&lt;/h1&gt; &#xA;&lt;p&gt;ç¯å¢ƒå®‰è£…ä¸ä¾èµ–ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7ï¼ˆé¡¹ç›®ä¸»è¦æµ‹è¯•åœ¨ python 3.10ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;OS: Linux, Windows, MacOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. å…‹éš†é¡¹ç›®&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git&#xA;cd  HivisionIDPhotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. å®‰è£…ä¾èµ–ç¯å¢ƒ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å»ºè®® conda åˆ›å»ºä¸€ä¸ª python3.10 è™šæ‹Ÿç¯å¢ƒåï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;pip install -r requirements-app.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. ä¸‹è½½äººåƒæŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼ä¸€ï¼šè„šæœ¬ä¸‹è½½&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/download_model.py --models all&#xA;# å¦‚éœ€æŒ‡å®šä¸‹è½½æŸä¸ªæ¨¡å‹&#xA;# python scripts/download_model.py --models modnet_photographic_portrait_matting&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼äºŒï¼šç›´æ¥ä¸‹è½½&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ¨¡å‹å‡å­˜åˆ°é¡¹ç›®çš„&lt;code&gt;hivision/creator/weights&lt;/code&gt;ç›®å½•ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;äººåƒæŠ å›¾æ¨¡å‹&lt;/th&gt; &#xA;   &lt;th&gt;ä»‹ç»&lt;/th&gt; &#xA;   &lt;th&gt;ä¸‹è½½&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;å®˜æ–¹æƒé‡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx&#34;&gt;ä¸‹è½½&lt;/a&gt;(24.7MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hivision_modnet&lt;/td&gt; &#xA;   &lt;td&gt;å¯¹çº¯è‰²æ¢åº•é€‚é…æ€§æ›´å¥½çš„æŠ å›¾æ¨¡å‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx&#34;&gt;ä¸‹è½½&lt;/a&gt;(24.7MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rmbg-1.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/briaai/RMBG-1.4&#34;&gt;BRIA AI&lt;/a&gt; å¼€æºçš„æŠ å›¾æ¨¡å‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true&#34;&gt;ä¸‹è½½&lt;/a&gt;(176.2MB)åé‡å‘½åä¸º&lt;code&gt;rmbg-1.4.onnx&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;birefnet-v1-lite&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet&#34;&gt;ZhengPeng7&lt;/a&gt; å¼€æºçš„æŠ å›¾æ¨¡å‹ï¼Œæ‹¥æœ‰æœ€å¥½çš„åˆ†å‰²ç²¾åº¦&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx&#34;&gt;ä¸‹è½½&lt;/a&gt;(224MB)åé‡å‘½åä¸º&lt;code&gt;birefnet-v1-lite.onnx&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å¦‚æœä¸‹è½½ç½‘é€Ÿä¸é¡ºåˆ©ï¼šå‰å¾€&lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main&#34;&gt;SwanHub&lt;/a&gt;ä¸‹è½½ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;4. äººè„¸æ£€æµ‹æ¨¡å‹é…ç½®ï¼ˆå¯é€‰ï¼‰&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ‹“å±•äººè„¸æ£€æµ‹æ¨¡å‹&lt;/th&gt; &#xA;   &lt;th&gt;ä»‹ç»&lt;/th&gt; &#xA;   &lt;th&gt;ä½¿ç”¨æ–‡æ¡£&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MTCNN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ç¦»çº¿&lt;/strong&gt;äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œé«˜æ€§èƒ½CPUæ¨ç†ï¼ˆæ¯«ç§’çº§ï¼‰ï¼Œä¸ºé»˜è®¤æ¨¡å‹ï¼Œæ£€æµ‹ç²¾åº¦è¾ƒä½&lt;/td&gt; &#xA;   &lt;td&gt;Cloneæ­¤é¡¹ç›®åç›´æ¥ä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ç¦»çº¿&lt;/strong&gt;äººè„¸æ£€æµ‹æ¨¡å‹ï¼ŒCPUæ¨ç†é€Ÿåº¦ä¸­ç­‰ï¼ˆç§’çº§ï¼‰ï¼Œç²¾åº¦è¾ƒé«˜&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx&#34;&gt;ä¸‹è½½&lt;/a&gt;åæ”¾åˆ°&lt;code&gt;hivision/creator/retinaface/weights&lt;/code&gt;ç›®å½•ä¸‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Face++&lt;/td&gt; &#xA;   &lt;td&gt;æ—·è§†æ¨å‡ºçš„åœ¨çº¿äººè„¸æ£€æµ‹APIï¼Œæ£€æµ‹ç²¾åº¦è¾ƒé«˜ï¼Œ&lt;a href=&#34;https://console.faceplusplus.com.cn/documents/4888373&#34;&gt;å®˜æ–¹æ–‡æ¡£&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/face++_CN.md&#34;&gt;ä½¿ç”¨æ–‡æ¡£&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;5. æ€§èƒ½å‚è€ƒ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æµ‹è¯•ç¯å¢ƒä¸ºMac M1 Max 64GBï¼ŒéGPUåŠ é€Ÿï¼Œæµ‹è¯•å›¾ç‰‡åˆ†è¾¨ç‡ä¸º 512x715(1) ä¸ 764Ã—1146(2)ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹ç»„åˆ&lt;/th&gt; &#xA;   &lt;th&gt;å†…å­˜å ç”¨&lt;/th&gt; &#xA;   &lt;th&gt;æ¨ç†æ—¶é•¿(1)&lt;/th&gt; &#xA;   &lt;th&gt;æ¨ç†æ—¶é•¿(2)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet + mtcnn&lt;/td&gt; &#xA;   &lt;td&gt;410MB&lt;/td&gt; &#xA;   &lt;td&gt;0.207s&lt;/td&gt; &#xA;   &lt;td&gt;0.246s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet + retinaface&lt;/td&gt; &#xA;   &lt;td&gt;405MB&lt;/td&gt; &#xA;   &lt;td&gt;0.571s&lt;/td&gt; &#xA;   &lt;td&gt;0.971s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;birefnet-v1-lite + retinaface&lt;/td&gt; &#xA;   &lt;td&gt;6.20GB&lt;/td&gt; &#xA;   &lt;td&gt;7.063s&lt;/td&gt; &#xA;   &lt;td&gt;7.128s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;6. GPUæ¨ç†åŠ é€Ÿï¼ˆå¯é€‰ï¼‰&lt;/h2&gt; &#xA;&lt;p&gt;åœ¨å½“å‰ç‰ˆæœ¬ï¼Œå¯è¢«è‹±ä¼Ÿè¾¾GPUåŠ é€Ÿçš„æ¨¡å‹ä¸º&lt;code&gt;birefnet-v1-lite&lt;/code&gt;ï¼Œå¹¶è¯·ç¡®ä¿ä½ æœ‰16GBå·¦å³çš„æ˜¾å­˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚éœ€ä½¿ç”¨è‹±ä¼Ÿè¾¾GPUåŠ é€Ÿæ¨ç†ï¼Œåœ¨ç¡®ä¿ä½ å·²ç»å®‰è£…&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;ä¸&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;åï¼Œæ ¹æ®&lt;a href=&#34;https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x&#34;&gt;onnxruntime-gpuæ–‡æ¡£&lt;/a&gt;æ‰¾åˆ°å¯¹åº”çš„&lt;code&gt;onnxruntime-gpu&lt;/code&gt;ç‰ˆæœ¬å®‰è£…ï¼Œä»¥åŠæ ¹æ®&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;pytorchå®˜ç½‘&lt;/a&gt;æ‰¾åˆ°å¯¹åº”çš„&lt;code&gt;torch&lt;/code&gt;ç‰ˆæœ¬å®‰è£…ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# å‡å¦‚ä½ çš„ç”µè„‘å®‰è£…çš„æ˜¯CUDA 12.x, cuDNN 8&#xA;# å®‰è£…torchæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä½ å§‹ç»ˆé…ç½®ä¸å¥½cuDNNï¼Œé‚£ä¹ˆè¯•è¯•å®‰è£…torch&#xA;pip install onnxruntime-gpu==1.18.0&#xA;pip install torch --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å®Œæˆå®‰è£…åï¼Œè°ƒç”¨&lt;code&gt;birefnet-v1-lite&lt;/code&gt;æ¨¡å‹å³å¯åˆ©ç”¨GPUåŠ é€Ÿæ¨ç†ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;TIPS: CUDA æ”¯æŒå‘ä¸‹å…¼å®¹ã€‚æ¯”å¦‚ä½ çš„ CUDA ç‰ˆæœ¬ä¸º 12.6ï¼Œ&lt;code&gt;torch&lt;/code&gt; å®˜æ–¹ç›®å‰æ”¯æŒçš„æœ€é«˜ç‰ˆæœ¬ä¸º 12.4ï¼ˆ&amp;lt;12.6ï¼‰ï¼Œ&lt;code&gt;torch&lt;/code&gt;ä»å¯ä»¥æ­£å¸¸ä½¿ç”¨CUDAã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;âš¡ï¸ è¿è¡Œ Gradio Demo&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¿è¡Œç¨‹åºå°†ç”Ÿæˆä¸€ä¸ªæœ¬åœ° Web é¡µé¢ï¼Œåœ¨é¡µé¢ä¸­å¯å®Œæˆè¯ä»¶ç…§çš„æ“ä½œä¸äº¤äº’ã€‚&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/harry.png&#34; width=&#34;900&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸš€ Python æ¨ç†&lt;/h1&gt; &#xA;&lt;p&gt;æ ¸å¿ƒå‚æ•°ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-i&lt;/code&gt;: è¾“å…¥å›¾åƒè·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-o&lt;/code&gt;: ä¿å­˜å›¾åƒè·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;: æ¨ç†ç±»å‹ï¼Œæœ‰idphotoã€human_mattingã€add_backgroundã€generate_layout_photoså¯é€‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--matting_model&lt;/code&gt;: äººåƒæŠ å›¾æ¨¡å‹æƒé‡é€‰æ‹©&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--face_detect_model&lt;/code&gt;: äººè„¸æ£€æµ‹æ¨¡å‹é€‰æ‹©&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ›´å¤šå‚æ•°å¯é€šè¿‡&lt;code&gt;python inference.py --help&lt;/code&gt;æŸ¥çœ‹&lt;/p&gt; &#xA;&lt;h2&gt;1. è¯ä»¶ç…§åˆ¶ä½œ&lt;/h2&gt; &#xA;&lt;p&gt;è¾“å…¥ 1 å¼ ç…§ç‰‡ï¼Œè·å¾— 1 å¼ æ ‡å‡†è¯ä»¶ç…§å’Œ 1 å¼ é«˜æ¸…è¯ä»¶ç…§çš„ 4 é€šé“é€æ˜ png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. äººåƒæŠ å›¾&lt;/h2&gt; &#xA;&lt;p&gt;è¾“å…¥ 1 å¼ ç…§ç‰‡ï¼Œè·å¾— 1å¼  4 é€šé“é€æ˜ png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. é€æ˜å›¾å¢åŠ åº•è‰²&lt;/h2&gt; &#xA;&lt;p&gt;è¾“å…¥ 1 å¼  4 é€šé“é€æ˜ pngï¼Œè·å¾— 1 å¼ å¢åŠ äº†åº•è‰²çš„ 3é€šé“å›¾åƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. å¾—åˆ°å…­å¯¸æ’ç‰ˆç…§&lt;/h2&gt; &#xA;&lt;p&gt;è¾“å…¥ 1 å¼  3 é€šé“ç…§ç‰‡ï¼Œè·å¾— 1 å¼ å…­å¯¸æ’ç‰ˆç…§&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. è¯ä»¶ç…§è£å‰ª&lt;/h2&gt; &#xA;&lt;p&gt;è¾“å…¥ 1 å¼  4 é€šé“ç…§ç‰‡ï¼ˆæŠ å›¾å¥½çš„å›¾åƒï¼‰ï¼Œè·å¾— 1 å¼ æ ‡å‡†è¯ä»¶ç…§å’Œ 1 å¼ é«˜æ¸…è¯ä»¶ç…§çš„ 4 é€šé“é€æ˜ png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;âš¡ï¸ éƒ¨ç½² API æœåŠ¡&lt;/h1&gt; &#xA;&lt;h2&gt;å¯åŠ¨åç«¯&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python deploy_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;è¯·æ±‚ API æœåŠ¡&lt;/h2&gt; &#xA;&lt;p&gt;è¯¦ç»†è¯·æ±‚æ–¹å¼è¯·å‚è€ƒ &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md&#34;&gt;API æ–‡æ¡£&lt;/a&gt;ï¼ŒåŒ…å«ä»¥ä¸‹è¯·æ±‚ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md#curl-%E8%AF%B7%E6%B1%82%E7%A4%BA%E4%BE%8B&#34;&gt;cURL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md#python-%E8%AF%B7%E6%B1%82%E7%A4%BA%E4%BE%8B&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ³ Docker éƒ¨ç½²&lt;/h1&gt; &#xA;&lt;h2&gt;1. æ‹‰å–æˆ–æ„å»ºé•œåƒ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ä»¥ä¸‹æ–¹å¼ä¸‰é€‰ä¸€&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼ä¸€ï¼šæ‹‰å–æœ€æ–°é•œåƒï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull linzeyi/hivision_idphotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼äºŒï¼šDockrfile ç›´æ¥æ„å»ºé•œåƒï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;åœ¨ç¡®ä¿å°†è‡³å°‘ä¸€ä¸ª&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#3-%E4%B8%8B%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6&#34;&gt;æŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶&lt;/a&gt;æ”¾åˆ°&lt;code&gt;hivision/creator/weights&lt;/code&gt;ä¸‹åï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t linzeyi/hivision_idphotos .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼ä¸‰ï¼šDocker compose æ„å»ºï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;åœ¨ç¡®ä¿å°†è‡³å°‘ä¸€ä¸ª&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#3-%E4%B8%8B%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6&#34;&gt;æŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶&lt;/a&gt;æ”¾åˆ°&lt;code&gt;hivision/creator/weights&lt;/code&gt;ä¸‹åï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. è¿è¡ŒæœåŠ¡&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;å¯åŠ¨ Gradio Demo æœåŠ¡&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;è¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œåœ¨ä½ çš„æœ¬åœ°è®¿é—® &lt;a href=&#34;http://127.0.0.1:7860/&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; å³å¯ä½¿ç”¨ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 7860:7860 linzeyi/hivision_idphotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;å¯åŠ¨ API åç«¯æœåŠ¡&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;ä¸¤ä¸ªæœåŠ¡åŒæ—¶å¯åŠ¨&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ç¯å¢ƒå˜é‡&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æä¾›äº†ä¸€äº›é¢å¤–çš„é…ç½®é¡¹ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡è¿›è¡Œè®¾ç½®ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ç¯å¢ƒå˜é‡&lt;/th&gt; &#xA;   &lt;th&gt;ç±»å‹&lt;/th&gt; &#xA;   &lt;th&gt;æè¿°&lt;/th&gt; &#xA;   &lt;th&gt;ç¤ºä¾‹&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FACE_PLUS_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;å¯é€‰&lt;/td&gt; &#xA;   &lt;td&gt;è¿™æ˜¯ä½ åœ¨ Face++ æ§åˆ¶å°ç”³è¯·çš„ API å¯†é’¥&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;7-fZStDJÂ·Â·Â·Â·&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FACE_PLUS_API_SECRET&lt;/td&gt; &#xA;   &lt;td&gt;å¯é€‰&lt;/td&gt; &#xA;   &lt;td&gt;Face++ APIå¯†é’¥å¯¹åº”çš„Secret&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VTee824EÂ·Â·Â·Â·&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RUN_MODE&lt;/td&gt; &#xA;   &lt;td&gt;å¯é€‰&lt;/td&gt; &#xA;   &lt;td&gt;è¿è¡Œæ¨¡å¼ï¼Œå¯é€‰å€¼ä¸º&lt;code&gt;beast&lt;/code&gt;(é‡å…½æ¨¡å¼)ã€‚é‡å…½æ¨¡å¼ä¸‹äººè„¸æ£€æµ‹å’ŒæŠ å›¾æ¨¡å‹å°†ä¸é‡Šæ”¾å†…å­˜ï¼Œä»è€Œè·å¾—æ›´å¿«çš„äºŒæ¬¡æ¨ç†é€Ÿåº¦ã€‚å»ºè®®å†…å­˜16GBä»¥ä¸Šå°è¯•ã€‚&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;beast&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DEFAULT_LANG&lt;/td&gt; &#xA;   &lt;td&gt;å¯é€‰&lt;/td&gt; &#xA;   &lt;td&gt;Gradio Demoå¯åŠ¨æ—¶çš„é»˜è®¤è¯­è¨€&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;dockerä½¿ç”¨ç¯å¢ƒå˜é‡ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run  -d -p 7860:7860 \&#xA;    -e FACE_PLUS_API_KEY=7-fZStDJÂ·Â·Â·Â· \&#xA;    -e FACE_PLUS_API_SECRET=VTee824EÂ·Â·Â·Â· \&#xA;    -e RUN_MODE=beast \&#xA;    -e DEFAULT_LANG=en \&#xA;    linzeyi/hivision_idphotos  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;1. å¦‚ä½•ä¿®æ”¹é¢„è®¾å°ºå¯¸å’Œé¢œè‰²ï¼Ÿ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å°ºå¯¸ï¼šä¿®æ”¹&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/demo/assets/size_list_CN.csv&#34;&gt;size_list_CN.csv&lt;/a&gt;åå†æ¬¡è¿è¡Œ &lt;code&gt;app.py&lt;/code&gt; å³å¯ï¼Œå…¶ä¸­ç¬¬ä¸€åˆ—ä¸ºå°ºå¯¸åï¼Œç¬¬äºŒåˆ—ä¸ºé«˜åº¦ï¼Œç¬¬ä¸‰åˆ—ä¸ºå®½åº¦ã€‚&lt;/li&gt; &#xA; &lt;li&gt;é¢œè‰²ï¼šä¿®æ”¹&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/demo/assets/color_list_CN.csv&#34;&gt;color_list_CN.csv&lt;/a&gt;åå†æ¬¡è¿è¡Œ &lt;code&gt;app.py&lt;/code&gt; å³å¯ï¼Œå…¶ä¸­ç¬¬ä¸€åˆ—ä¸ºé¢œè‰²åï¼Œç¬¬äºŒåˆ—ä¸ºHexå€¼ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. å¦‚ä½•ä¿®æ”¹æ°´å°å­—ä½“ï¼Ÿ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å°†å­—ä½“æ–‡ä»¶æ”¾åˆ°&lt;code&gt;hivision/plugin/font&lt;/code&gt;æ–‡ä»¶å¤¹ä¸‹&lt;/li&gt; &#xA; &lt;li&gt;ä¿®æ”¹&lt;code&gt;hivision/plugin/watermark.py&lt;/code&gt;çš„&lt;code&gt;font_file&lt;/code&gt;å‚æ•°å€¼ä¸ºå­—ä½“æ–‡ä»¶å&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;3. å¦‚ä½•æ·»åŠ ç¤¾äº¤åª’ä½“æ¨¡æ¿ç…§ï¼Ÿ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å°†æ¨¡æ¿å›¾ç‰‡æ”¾åˆ°&lt;code&gt;hivision/plugin/template/assets&lt;/code&gt;æ–‡ä»¶å¤¹ä¸‹ã€‚æ¨¡æ¿å›¾ç‰‡æ˜¯ä¸€ä¸ª4é€šé“çš„é€æ˜pngã€‚&lt;/li&gt; &#xA; &lt;li&gt;åœ¨&lt;code&gt;hivision/plugin/template/assets/template_config.json&lt;/code&gt;æ–‡ä»¶ä¸­æ·»åŠ æœ€æ–°çš„æ¨¡æ¿ä¿¡æ¯ï¼Œå…¶ä¸­&lt;code&gt;width&lt;/code&gt;ä¸ºæ¨¡æ¿å›¾å®½åº¦(px)ï¼Œ&lt;code&gt;height&lt;/code&gt;ä¸ºæ¨¡æ¿å›¾é«˜åº¦(px)ï¼Œ&lt;code&gt;anchor_points&lt;/code&gt;ä¸ºæ¨¡æ¿ä¸­é€æ˜åŒºåŸŸçš„å››ä¸ªè§’çš„åæ ‡(px)ï¼›&lt;code&gt;rotation&lt;/code&gt;ä¸ºé€æ˜åŒºåŸŸç›¸å¯¹äºå‚ç›´æ–¹å‘çš„æ—‹è½¬è§’åº¦ï¼Œ&amp;gt;0ä¸ºé€†æ—¶é’ˆï¼Œ&amp;lt;0ä¸ºé¡ºæ—¶é’ˆã€‚&lt;/li&gt; &#xA; &lt;li&gt;åœ¨&lt;code&gt;demo/processor.py&lt;/code&gt;çš„&lt;code&gt;_generate_image_template&lt;/code&gt;å‡½æ•°ä¸­çš„&lt;code&gt;TEMPLATE_NAME_LIST&lt;/code&gt;å˜é‡æ·»åŠ æœ€æ–°çš„æ¨¡æ¿å&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/social_template.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;h2&gt;4. å¦‚ä½•ä¿®æ”¹Gradio Demoçš„é¡¶éƒ¨å¯¼èˆªæ ï¼Ÿ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¿®æ”¹&lt;code&gt;demo/assets/title.md&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. å¦‚ä½•æ·»åŠ /ä¿®æ”¹ã€Œæ‰“å°æ’ç‰ˆã€ä¸­çš„å°ºå¯¸ï¼Ÿ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¿®æ”¹&lt;code&gt;demo/locales.py&lt;/code&gt;ä¸­çš„&lt;code&gt;print_switch&lt;/code&gt;å­—å…¸ï¼Œæ·»åŠ /ä¿®æ”¹æ–°çš„å°ºå¯¸åç§°å’Œå°ºå¯¸å‚æ•°ï¼Œç„¶åé‡æ–°è¿è¡Œ&lt;code&gt;python app.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ“§ è”ç³»æˆ‘ä»¬&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‘é‚®ä»¶è‡³ &lt;a href=&#34;mailto:zeyi.lin@swanhub.co&#34;&gt;zeyi.lin@swanhub.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ™ æ„Ÿè°¢æ”¯æŒ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos&#34; alt=&#34;Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members&#34;&gt;&lt;img src=&#34;https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos&#34; alt=&#34;Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;è´¡çŒ®è€…ä»¬ï¼š&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin&#34;&gt;Zeyi-Lin&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/SAKURA-CAT&#34;&gt;SAKURA-CAT&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/Feudalman&#34;&gt;Feudalman&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/swpfY&#34;&gt;swpfY&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/Kaikaikaifang&#34;&gt;Kaikaikaifang&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/ShaohonChen&#34;&gt;ShaohonChen&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/KashiwaByte&#34;&gt;KashiwaByte&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ“œ Lincese&lt;/h1&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;ğŸ“š å¼•ç”¨&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–é¡¹ç›®ä¸­ä½¿ç”¨äº†HivisionIDPhotosï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹BibTeXæ¡ç›®ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hivisionidphotos,&#xA;      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},&#xA;      author={Zeyi Lin and SwanLab Team},&#xA;      year={2024},&#xA;      publisher={GitHub},&#xA;      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- å¾®ä¿¡ç¾¤é“¾æ¥ --&gt; &#xA;&lt;!-- Github Release --&gt; &#xA;&lt;!-- ç¤¾åŒºé¡¹ç›®é“¾æ¥ --&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/qlib</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/microsoft/qlib</id>
    <link href="https://github.com/microsoft/qlib" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pyqlib/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/#history&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyqlib&#34; alt=&#34;PypI Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true&#34; alt=&#34;Upload Python Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/actions&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main&#34; alt=&#34;Github Actions Test Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/qlib/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/pyqlib&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/Microsoft/qlib&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ“°&lt;/span&gt; &lt;strong&gt;What&#39;s NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;ğŸ’–&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Recent released features&lt;/p&gt; &#xA;&lt;h3&gt;Introducing &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png&#34; alt=&#34;RD_Agent&#34; style=&#34;height: 2em&#34;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; &#xA;&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;ğŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; &#xA;&lt;p&gt;RD-Agent is now available on &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;GitHub&lt;/a&gt;, and we welcome your starğŸŒŸ!&lt;/p&gt; &#xA;&lt;p&gt;To learn more, please visit our &lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;â™¾ï¸Demo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; &#xA;&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scenario&lt;/th&gt; &#xA;   &lt;th&gt;Demo video (English)&lt;/th&gt; &#xA;   &lt;th&gt;Demo video (ä¸­æ–‡)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Factor Mining&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Model Optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“ƒ&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2505.15155&#34;&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘¾&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/microsoft/RD-Agent/&#34;&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{li2025rdagentquant,&#xA;    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},&#xA;    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},&#xA;    year={2025},&#xA;    eprint={2505.15155},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2505.15155&#34;&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; &#xA;   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ“ˆComing soon!(&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1863&#34;&gt;Under review&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ğŸ”¥LLM-driven Auto Quant FactoryğŸ”¥&lt;/td&gt; &#xA;   &lt;td&gt;ğŸš€ Released in &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;â™¾ï¸RD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1414/&#34;&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.9.0&#34;&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL Learning Framework&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;span&gt;ğŸ“ˆ&lt;/span&gt; Released on Nov 10, 2022. &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1332&#34;&gt;#1332&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1322&#34;&gt;#1322&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1316&#34;&gt;#1316&lt;/a&gt;,&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1299&#34;&gt;#1299&lt;/a&gt;,&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1263&#34;&gt;#1263&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1244&#34;&gt;#1244&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1169&#34;&gt;#1169&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1125&#34;&gt;#1125&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1076&#34;&gt;#1076&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1040&#34;&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qlib &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/examples/tutorial&#34;&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ“– &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1037&#34;&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ibovespa index data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸš&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/990&#34;&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Point-in-Time database&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/343&#34;&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/744&#34;&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/743&#34;&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/754&#34;&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.8.0&#34;&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ADD model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/704&#34;&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ADARNN model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/689&#34;&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TCN model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/668&#34;&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nested Decision Framework&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/438&#34;&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href=&#34;https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py&#34;&gt;Example&lt;/a&gt; and &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/highfreq.html&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/531&#34;&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/508&#34;&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.7.0&#34;&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TCTS Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/491&#34;&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/290&#34;&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/286&#34;&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency data processing example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/257&#34;&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency trading example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/227&#34;&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸš&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/221&#34;&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tabnet Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/205&#34;&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#39;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; &#xA;&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href=&#34;https://arxiv.org/abs/2009.11189&#34;&gt;&#34;Qlib: An AI-oriented Quantitative Investment Platform&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; &#xA;   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#plans&#34;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib&#34;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;ul dir=&#34;auto&#34;&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#installation&#34;&gt;Installation&lt;/a&gt; &lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow&#34;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code&#34;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo&#34;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework&#34;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib&#34;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode&#34;&gt;Offline Mode and Online Mode&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server&#34;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports&#34;&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us&#34;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; &#xA;   &lt;td valign=&#34;baseline&#34;&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research&#34;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns&#34;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; &#xA;       &lt;ul&gt; &#xA;        &lt;li type=&#34;disc&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo&#34;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; &#xA;         &lt;ul&gt; &#xA;          &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model&#34;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; &#xA;          &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models&#34;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; &#xA;         &lt;/ul&gt; &lt;/li&gt; &#xA;       &lt;/ul&gt; &lt;/li&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics&#34;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions&#34;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Plans&lt;/h1&gt; &#xA;&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; &#xA;&lt;!-- | Feature                        | Status      | --&gt; &#xA;&lt;!-- | --                      | ------    | --&gt; &#xA;&lt;h1&gt;Framework of Qlib&lt;/h1&gt; &#xA;&lt;div style=&#34;align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework&#34;&gt;detailed framework&lt;/a&gt; of Qlib&#39;s design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; &#xA;&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html&#34;&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl.html&#34;&gt;reinforcement learning&lt;/a&gt;, &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section&#34;&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/meta.html&#34;&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/strategy.html&#34;&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/highfreq.html&#34;&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/report.html&#34;&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/online.html&#34;&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It&#39;s very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href=&#34;https://terminalizer.com/view/3f24561a4470&#34;&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&#34;&gt;instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;install with pip&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;install from source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;plot&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; &#xA; &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;&#39;s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pip install pyqlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; &#xA;&lt;h3&gt;Install from source&lt;/h3&gt; &#xA;&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install numpy&#xA;pip install --upgrade cython&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib&#xA;pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml&#34;&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;â— Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href=&#34;https://github.com/chenditc/investment_data/releases&#34;&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz&#xA;mkdir -p ~/.qlib/qlib_data/cn_data&#xA;tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1&#xA;rm -f qlib_bin.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; &#xA;&lt;h3&gt;Get with module&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get 1d data&#xA;python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn&#xA;&#xA;# get 1min data&#xA;python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get from source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get 1d data&#xA;python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn&#xA;&#xA;# get 1min data&#xA;python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This dataset is created by public data collected by &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/&#34;&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset&#34;&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href=&#34;https://finance.yahoo.com/lookup&#34;&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format&#34;&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; &#xA; &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can&#39;t incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&#34;&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; &#xA; &lt;p&gt;For more information, please refer to: &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&#34;&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Automatic update of data to the &#34;qlib&#34; directory each trading day(Linux)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Checking the health of the data&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &#xA;- Run the initialization code and get stock data:&#xA;&#xA;  ```python&#xA;  import qlib&#xA;  from qlib.data import D&#xA;  from qlib.constant import REG_CN&#xA;&#xA;  # Initialization&#xA;  mount_path = &#34;~/.qlib/qlib_data/cn_data&#34;  # target_dir&#xA;  qlib.init(mount_path=mount_path, region=REG_CN)&#xA;&#xA;  # Get stock data by Qlib&#xA;  # Load trading calendar with the given time range and frequency&#xA;  print(D.calendar(start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;)[:2])&#xA;&#xA;  # Parse a given market name into a stockpool config&#xA;  instruments = D.instruments(&#39;csi500&#39;)&#xA;  print(D.list_instruments(instruments=instruments, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, as_list=True)[:6])&#xA;&#xA;  # Load features of certain instruments in given time range&#xA;  instruments = [&#39;SH600000&#39;]&#xA;  fields = [&#39;$close&#39;, &#39;$volume&#39;, &#39;Ref($close, 1)&#39;, &#39;Mean($close, 3)&#39;, &#39;$high-$low&#39;]&#xA;  print(D.features(instruments, fields, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;).head())&#xA;  ```&#xA; --&gt; &#xA;&lt;h2&gt;Docker images&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull pyqlib/qlib_image_stable:stable&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn&#xA;&amp;gt;&amp;gt;&amp;gt; python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; exit&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start -i -a &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker stop &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker rm &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want to know more information, please refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; &#xA;&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#34;&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  cd examples  # Avoid running program under the directory contains `qlib`&#xA;  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/strategy.html#result&#34;&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#39;The following are analysis results of the excess return without cost.&#39;&#xA;                       risk&#xA;mean               0.000708&#xA;std                0.005626&#xA;annualized_return  0.178316&#xA;information_ratio  1.996555&#xA;max_drawdown      -0.081806&#xA;&#39;The following are analysis results of the excess return with cost.&#39;&#xA;                       risk&#xA;mean               0.000512&#xA;std                0.005626&#xA;annualized_return  0.128982&#xA;information_ratio  1.444287&#xA;max_drawdown      -0.091078&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/workflow.html&#34;&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Cumulative Return of groups &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png&#34; alt=&#34;Cumulative Return&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Return distribution &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png&#34; alt=&#34;long_short&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Information Coefficient (IC) &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png&#34; alt=&#34;Information Coefficient&#34;&gt; &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png&#34; alt=&#34;Monthly IC&#34;&gt; &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png&#34; alt=&#34;IC&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png&#34; alt=&#34;Auto Correlation&#34;&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Backtest return &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png&#34; alt=&#34;Report&#34;&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;!-- &#xA;- Score IC&#xA;![Score IC](docs/_static/img/score_ic.png)&#xA;- Cumulative Return&#xA;![Cumulative Return](docs/_static/img/cumulative_return.png)&#xA;- Risk Analysis&#xA;![Risk Analysis](docs/_static/img/risk_analysis.png)&#xA;- Rank Label&#xA;![Rank Label](docs/_static/img/rank_label.png)&#xA;--&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/report.html&#34;&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; &#xA;&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb&#34;&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; &#xA;&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; &#xA;&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; &#xA;&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; &#xA;&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; &#xA;&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&#34;&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/&#34;&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/&#34;&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/&#34;&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/&#34;&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/&#34;&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/&#34;&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM&#34;&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/&#34;&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/&#34;&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/&#34;&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/&#34;&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/&#34;&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/&#34;&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/&#34;&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/&#34;&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/&#34;&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/&#34;&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/&#34;&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/&#34;&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/&#34;&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/&#34;&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/&#34;&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/&#34;&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; &#xA;&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run a single model&lt;/h3&gt; &#xA;&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&#34;&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model&#39;s workflow based from a config file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py&#34;&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/&#34;&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run multiple models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn&#39;t support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; &#xA;&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python run_all_model.py run 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Break change&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn&#39;t guarantee that some programmes will run correctly, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; &#xA; &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; &#xA; &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic&#34;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies&#39; performance.&lt;/p&gt; &#xA;&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/&#34;&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/&#34;&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; &#xA;&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; &#xA;&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&#34;&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution&#34;&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml&#34;&gt;TWAP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml&#34;&gt;PPO: &#34;An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization&#34;, IJCAL 2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml&#34;&gt;OPDS: &#34;Universal Trading for Order Execution with Oracle Policy Distillation&#34;, AAAI 2021&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; &#xA;&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;US Market&lt;/th&gt; &#xA;   &lt;th&gt;China Market&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&#34;&gt;Alpha360&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âˆš&lt;/td&gt; &#xA;   &lt;td&gt;âˆš&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&#34;&gt;Alpha158&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âˆš&lt;/td&gt; &#xA;   &lt;td&gt;âˆš&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/advanced/alpha.html&#34;&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; &#xA;&lt;h1&gt;Learning Framework&lt;/h1&gt; &#xA;&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/model.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl.html&#34;&gt;here&lt;/a&gt;. Qlib&#39;s RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It&#39;s worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;More About Qlib&lt;/h1&gt; &#xA;&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The detailed documents are organized in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/&#34;&gt;docs&lt;/a&gt;. &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;conda install sphinx sphinx_rtd_theme -y&#xA;# Otherwise, you can install them with pip&#xA;# pip install sphinx sphinx_rtd_theme&#xA;make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also view the &lt;a href=&#34;http://qlib.readthedocs.io/&#34;&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; &#xA;&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href=&#34;https://github.com/microsoft/qlib/projects/1&#34;&gt;github project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; &#xA;&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; &#xA;&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; &#xA;&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href=&#34;https://qlib-server.readthedocs.io/&#34;&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href=&#34;https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure&#34;&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href=&#34;https://github.com/microsoft/qlib-server&#34;&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; &#xA;&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; &#xA;&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;HDF5&lt;/th&gt; &#xA;   &lt;th&gt;MySQL&lt;/th&gt; &#xA;   &lt;th&gt;MongoDB&lt;/th&gt; &#xA;   &lt;th&gt;InfluxDB&lt;/th&gt; &#xA;   &lt;th&gt;Qlib -E -D&lt;/th&gt; &#xA;   &lt;th&gt;Qlib +E -D&lt;/th&gt; &#xA;   &lt;th&gt;Qlib +E +D&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; &#xA;   &lt;td&gt;184.4Â±3.7&lt;/td&gt; &#xA;   &lt;td&gt;365.3Â±7.5&lt;/td&gt; &#xA;   &lt;td&gt;253.6Â±6.7&lt;/td&gt; &#xA;   &lt;td&gt;368.2Â±3.6&lt;/td&gt; &#xA;   &lt;td&gt;147.0Â±8.8&lt;/td&gt; &#xA;   &lt;td&gt;47.6Â±1.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.4Â±0.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8.8Â±0.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.2Â±0.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; &#xA;&lt;h1&gt;Related Reports&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://analyticsindiamag.com/qlib/&#34;&gt;Guide To Qlib: Microsoftâ€™s AI Investment Platform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ&#34;&gt;å¾®è½¯ä¹ŸæAIé‡åŒ–å¹³å°ï¼Ÿè¿˜æ˜¯å¼€æºçš„ï¼&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ&#34;&gt;å¾®çŸ¿Qlibï¼šä¸šå†…é¦–ä¸ªAIé‡åŒ–æŠ•èµ„å¼€æºå¹³å°&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contact Us&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have any issues, please create issue &lt;a href=&#34;https://github.com/microsoft/qlib/issues/new/choose&#34;&gt;here&lt;/a&gt; or send messages in &lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;gitter&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href=&#34;https://github.com/microsoft/qlib/compare&#34;&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href=&#34;mailto:qlib@microsoft.com&#34;&gt;qlib@microsoft.com&lt;/a&gt;). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Join IM discussion groups:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;Gitter&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href=&#34;https://github.com/microsoft/qlib/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/qlib&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href=&#34;https://github.com/evanzd/evanzd&#34;&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href=&#34;https://github.com/evanzd/evanzd&#34;&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; &#xA;&lt;h2&gt;Guidance&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions.&lt;br&gt; &lt;strong&gt;Here are some &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst&#34;&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href=&#34;https://github.com/microsoft/qlib/issues&#34;&gt;issues list&lt;/a&gt; or &lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to contribute to Qlib&#39;s document/code, you can follow the steps in the figure below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t know how to start to contribute, you can refer to the following examples.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Examples&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Solving issues&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/issues/749&#34;&gt;Answer a question&lt;/a&gt;; &lt;a href=&#34;https://github.com/microsoft/qlib/issues/765&#34;&gt;issuing&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/qlib/pull/792&#34;&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/797/files&#34;&gt;Improve docs quality&lt;/a&gt; ; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/774&#34;&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Feature&lt;/td&gt; &#xA;   &lt;td&gt;Implement a &lt;a href=&#34;https://github.com/microsoft/qlib/projects&#34;&gt;requested feature&lt;/a&gt; like &lt;a href=&#34;https://github.com/microsoft/qlib/pull/754&#34;&gt;this&lt;/a&gt;; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/539/files&#34;&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataset&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/733&#34;&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/689&#34;&gt;Implement a new model&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing&#34;&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/labels/good%20first%20issue&#34;&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; &#xA;&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg &#39;TODO|FIXME&#39; qlib&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to become one of Qlib&#39;s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href=&#34;mailto:qlib@microsoft.com&#34;&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lightricks/LTX-Video</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/Lightricks/LTX-Video</id>
    <link href="https://github.com/Lightricks/LTX-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;LTX-Video&lt;/h1&gt; &#xA; &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.lightricks.com/ltxv&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer&#34;&gt;Trainer&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/Mn8BRgUKKy&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news&#34;&gt;What&#39;s new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models--workflows&#34;&gt;Models &amp;amp; Workflows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide&#34;&gt;Quick Start Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference&#34;&gt;Use online&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally&#34;&gt;Run locally&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration&#34;&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide&#34;&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution&#34;&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#%E2%9A%A1%EF%B8%8F-training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#%F0%9F%9A%80-join-us&#34;&gt;Join Us!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &#xA;&lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &#xA;&lt;h3&gt;Image to video examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif&#34; alt=&#34;example2&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif&#34; alt=&#34;example3&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif&#34; alt=&#34;example4&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif&#34; alt=&#34;example8&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif&#34; alt=&#34;example9&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text to video examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#39;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00010.gif&#34; alt=&#34;example10&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00015.gif&#34; alt=&#34;example3&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#39; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00014.gif&#34; alt=&#34;example2&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00013.gif&#34; alt=&#34;example13&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00011.gif&#34; alt=&#34;example11&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; &#xA;   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; &#xA;     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official ComfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors&#34;&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRAM (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official ComfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new upscalers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors&#34;&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors&#34;&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; &#xA; &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Release a new distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; &#xA; &lt;li&gt;New default resolution and FPS: 1216 Ã— 704 pixels at 30 FPS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New license for commercial use (&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt&#34;&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support keyframes and video extension&lt;/li&gt; &#xA; &lt;li&gt;Support higher resolutions&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt understanding&lt;/li&gt; &#xA; &lt;li&gt;Improved VAE&lt;/li&gt; &#xA; &lt;li&gt;New online web app in &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; &#xA; &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; &#xA; &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release the &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;research paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support for STG / PAG&lt;/li&gt; &#xA; &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; &#xA; &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; &#xA; &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; &#xA; &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; &#xA; &lt;li&gt;Relax transformers dependency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;th&gt;inference.py config&lt;/th&gt; &#xA;   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-dev&lt;/td&gt; &#xA;   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json&#34;&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;ltxv-13b-0.9.7-mix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json&#34;&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv&#34;&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json&#34;&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA to make ltxv-13b-dev behave like the distilled model&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-fp8&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json&#34;&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json&#34;&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml&#34;&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json&#34;&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; &#xA;   &lt;td&gt;15Ã— faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml&#34;&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json&#34;&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Quick Start Guide&lt;/h1&gt; &#xA;&lt;h2&gt;Online inference&lt;/h2&gt; &#xA;&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv&#34;&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video&#34;&gt;Fal.ai text-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video/image-to-video&#34;&gt;Fal.ai image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/lightricks/ltx-video&#34;&gt;Replicate text-to-video and image-to-video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run locally&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightricks/LTX-Video.git&#xA;cd LTX-Video&#xA;&#xA;# create env&#xA;python -m venv env&#xA;source env/bin/activate&#xA;python -m pip install -e .\[inference-script\]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;ğŸ“ &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI&lt;/a&gt; workflow. Weâ€™re working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; &#xA;&lt;p&gt;To use our model, please follow the inference code in &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py&#34;&gt;inference.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h4&gt;For text-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extending a video:&lt;/h4&gt; &#xA;&lt;p&gt;ğŸ“ &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; &#xA;&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8&#34;&gt;see details below&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model User Guide&lt;/h1&gt; &#xA;&lt;h2&gt;ğŸ“ Prompt Engineering&lt;/h2&gt; &#xA;&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; &#xA; &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; &#xA; &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; &#xA; &lt;li&gt;Include background and environment details&lt;/li&gt; &#xA; &lt;li&gt;Specify camera angles and movements&lt;/li&gt; &#xA; &lt;li&gt;Describe lighting and colors&lt;/li&gt; &#xA; &lt;li&gt;Note any changes or sudden events&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;When using &lt;code&gt;inference.py&lt;/code&gt;, shorts prompts (below &lt;code&gt;prompt_enhancement_words_threshold&lt;/code&gt; words) are automatically enhanced by a language model. This is supported with text-to-video and image-to-video (first-frame conditioning).&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ® Parameter Guide&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; &#xA; &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; &#xA; &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; &#xA; &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸ“ For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contribution&lt;/h2&gt; &#xA;&lt;h3&gt;ComfyUI-LTXTricks ğŸ› ï¸&lt;/h3&gt; &#xA;&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸ”„ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href=&#34;https://rf-inversion.github.io/&#34;&gt;RF-Inversion&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;âœ‚ï¸ &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/wangjiangshan0725/RF-Solver-Edit&#34;&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸŒŠ &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/fallenshock/FlowEdit&#34;&gt;FlowEdit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ¥ &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;âœ¨ &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href=&#34;https://junhahyung.github.io/STGuidance/&#34;&gt;STGuidance&lt;/a&gt;. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ–¼ï¸ &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LTX-VideoQ8 ğŸ± &lt;a id=&#34;ltx-videoq8&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href=&#34;https://github.com/Lightricks/LTX-Video&#34;&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/KONAKONA666/LTX-Video&#34;&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸš€ Up to 3X speed-up with no accuracy loss&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ¥ Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ› ï¸ Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/&#34;&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href=&#34;https://github.com/sayakpaul/q8-ltx-video&#34;&gt;Details here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TeaCache for LTX-Video ğŸµ &lt;a id=&#34;TeaCache&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video&#34;&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸš€ Speeds up LTX-Video inference.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ“Š Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ› ï¸ No retraining required: Works directly with existing models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Your Contribution&lt;/h3&gt; &#xA;&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; &#xA;&lt;h1&gt;âš¡ï¸ Training&lt;/h1&gt; &#xA;&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer&#34;&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training.&lt;/p&gt; &#xA;&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md&#34;&gt;README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;ğŸš€ Join Us&lt;/h1&gt; &#xA;&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; &#xA;&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we&#39;re revolutionizing how visual content is created.&lt;/p&gt; &#xA;&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D&#34;&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ“„ Our tech report is out! If you find our work helpful, please â­ï¸ star the repository and cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,&#xA;  title={LTX-Video: Realtime Video Latent Diffusion},&#xA;  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},&#xA;  journal={arXiv preprint arXiv:2501.00103},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>