<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-01T02:30:42Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iperov/DeepFaceLive</title>
    <updated>2022-07-01T02:30:42Z</updated>
    <id>tag:github.com,2022-07-01:/iperov/DeepFaceLive</id>
    <link href="https://github.com/iperov/DeepFaceLive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-time face swap for PC streaming or video calls&lt;/p&gt;&lt;hr&gt;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/deepfacelive_intro.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_onnx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_directx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_python.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Swapper&lt;/h2&gt; &lt;p&gt;You can swap your face from a webcam or the face in the video using trained face models.&lt;/p&gt; &lt;p&gt;Here is a list of available ready-to-use public face models.&lt;/p&gt; &lt;p&gt;These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he&#39;s breathtaking!&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Keanu Reeves &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/Keanu_Reeves.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ava de Addario &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/Ava_de_Addario.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Dilraba Dilmurat &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;examples&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Matilda Bobbie &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Yohanna Coralson &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Amber Song &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Amber_Song/Amber_Song.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;examples&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Kim Jarrey &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/Kim_Jarrey.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; David Kovalniy &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/David_Kovalniy.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ewon Spice &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/Ewon_Spice.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Bryan Greynolds &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Nicola Badge &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/Nicola_Badge.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td&gt; Dean Wiesel &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dean_Wiesel/Dean_Wiesel.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dean_Wiesel/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Silwan Stillwone &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Tim Chrys &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/Tim_Chrys.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Zahar Lupin &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/Zahar_Lupin.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Tim Norland &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/Tim_Norland.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; If you want a higher quality or better face match, you can train your own face model using &lt;a href=&#34;https://github.com/iperov/DeepFaceLab&#34;&gt;DeepFaceLab&lt;/a&gt; &lt;p&gt;Here is an &lt;a href=&#34;https://www.tiktok.com/@arnoldschwarzneggar/video/6995538782204300545&#34;&gt;example&lt;/a&gt; of Arnold Schwarzneggar trained on a particular face and used in a video call. Read the FAQ for more information.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Animator&lt;/h2&gt; &lt;p&gt;There is also a Face Animator module in DeepFaceLive app. You can control a static face picture using video or your own face from the camera. The quality is not the best, and requires fine face matching and tuning parameters for every face pair, but enough for funny videos and memes or real-time streaming at 25 fps using 35 TFLOPS GPU.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/face_animator_example.gif&#34;&gt;&lt;/p&gt; &lt;p&gt;Here is a &lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/FaceAnimator_tutor.webm?raw=true&#34;&gt;mini video&lt;/a&gt; showing the process of setting up the Face Animator for Obama controlling Kim Chen&#39;s face.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;System requirements&lt;/h2&gt; &lt;p&gt;any DirectX12 compatible graphics card&lt;/p&gt; &lt;p&gt;(Recommended RTX 2070+ / Radeon RX 5700 XT+ )&lt;/p&gt; &lt;p&gt;Modern CPU with AVX instructions&lt;/p&gt; &lt;p&gt;4GB RAM, 32GB+ paging file&lt;/p&gt; &lt;p&gt;Windows 10&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Documentation&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; Windows &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/windows/main_setup.md&#34;&gt;Main setup&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/windows/for_streaming.md&#34;&gt;additional setup for streaming&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/windows/for_video_calls.md&#34;&gt;additional setup for video calls&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/windows/using_android_phone_camera.md&#34;&gt;Using Android phone camera&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; Linux &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/build/linux&#34;&gt;Build info&lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; Frequently asked questions &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/user_faq/user_faq.md&#34;&gt;for User&lt;/a&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/developer_faq/developer_faq.md&#34;&gt;for Developer&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://disk.yandex.ru/d/7i5XTKIKVg5UUg&#34;&gt;Windows 10 x64 (yandex.ru)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://mega.nz/folder/m10iELBK#Y0H6BflF9C4k_clYofC7yA&#34;&gt;Windows 10 x64 (mega.nz)&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; Contains stand-alone zero-dependency all-in-one ready-to-use portable self-extracting folder! You don&#39;t need to install anything other than video drivers. &lt;br&gt;&lt;br&gt; DirectX12 build : NVIDIA, AMD, Intel videocards. &lt;br&gt;&lt;br&gt; NVIDIA build : NVIDIA cards only, GT730 and higher. Works faster than DX12. FaceMerger can work also on AMD/Intel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Communication groups&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://discord.gg/S2h7kPySQp&#34;&gt;Discord&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Official discord channel. English / Russian.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://mrdeepfakes.com/forums/&#34;&gt;mrdeepfakes&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;the biggest NSFW English deepfake community&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://www.dfldata.xyz&#34;&gt;dfldata.xyz&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;‰∏≠Êñá‰∫§ÊµÅËÆ∫ÂùõÔºåÂÖçË¥πËΩØ‰ª∂ÊïôÁ®ã„ÄÅÊ®°Âûã„ÄÅ‰∫∫ËÑ∏Êï∞ÊçÆ&lt;/td&gt;&#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;How can I help the project?&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; I need the computing power to train models. &lt;br&gt; If you have a free computer with 2080TI or better card with 12GB+ VRAM, you can give me remote access to it. I will train 1 model in a month. Contact me(iperov#6528) in Discord channel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; Register github account and push &#34;Star&#34; button. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;!--&lt;tr&gt;&lt;td colspan=2 align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;https://www.paypal.com/paypalme/DeepFaceLab&#34;&gt;Donate via Paypal&lt;/a&gt;&#xA;&lt;/td&gt;&lt;/tr&gt;--&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;a href=&#34;https://money.yandex.ru/to/41001142318065&#34;&gt;Donate via Yandex.Money&lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; bitcoin:bc1qewl062v70rszulml3f0mjdjrys8uxdydw3v6rq &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;!--&#xA;    &lt;a href=&#34;https://br-stone.online&#34;&gt;&lt;img src=&#34;doc/logo_barclay_stone.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&lt;a href=&#34;https://exmo.com&#34;&gt;&lt;img src=&#34;doc/logo_exmo.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&#xA;    presents&#xA;&#xA;    &lt;tr&gt;&lt;td align=&#34;right&#34;&gt;&#xA;&#xA;&#xA;    &lt;a href=&#34;&#34;&gt;Windows (magnet link)&lt;/a&gt;&#xA;    &lt;/td&gt;&lt;td align=&#34;center&#34;&gt;Latest release. Use torrent client to download.&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;/tr&gt;&#xA;--&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>elebumm/RedditVideoMakerBot</title>
    <updated>2022-07-01T02:30:42Z</updated>
    <id>tag:github.com,2022-07-01:/elebumm/RedditVideoMakerBot</id>
    <link href="https://github.com/elebumm/RedditVideoMakerBot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create Reddit Videos with just‚ú® one command ‚ú®&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reddit Video Maker Bot üé•&lt;/h1&gt; &#xA;&lt;p&gt;All done WITHOUT video editing or asset compiling. Just pure ‚ú®programming magic‚ú®.&lt;/p&gt; &#xA;&lt;p&gt;Created by Lewis Menelaws &amp;amp; &lt;a href=&#34;https://tmrrwinc.ca&#34;&gt;TMRRW&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://tmrrwinc.ca&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/6053155/170528535-e274dc0b-7972-4b27-af22-637f8c370133.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png&#34;&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png&#34; width=&#34;350&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Video Explainer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3gjcY_00U1w&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6053155/173631669-1d1b14ad-c478-4010-b57d-d79592a789f2.png&#34; alt=&#34;lewisthumbnail&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Motivation ü§î&lt;/h2&gt; &#xA;&lt;p&gt;These videos on TikTok, YouTube and Instagram get MILLIONS of views across all platforms and require very little effort. The only original thing being done is the editing and gathering of all materials...&lt;/p&gt; &#xA;&lt;p&gt;... but what if we can automate that process? ü§î&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimers üö®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;At the moment&lt;/strong&gt;, this repository won&#39;t attempt to upload this content through this bot. It will give you a file that you will then have to upload manually. This is for the sake of avoiding any sort of community guideline issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.6+&lt;/li&gt; &#xA; &lt;li&gt;Playwright (this should install automatically in installation)&lt;/li&gt; &#xA; &lt;li&gt;Sox&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation üë©‚Äçüíª&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2a &lt;strong&gt;Automatic Install&lt;/strong&gt;: Run &lt;code&gt;python main.py&lt;/code&gt; and type &#39;yes&#39; to activate the setup assistant.&lt;/p&gt; &lt;p&gt;2b &lt;strong&gt;Manual Install&lt;/strong&gt;: Rename &lt;code&gt;.env.template&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and replace all values with the appropriate fields. To get Reddit keys (&lt;strong&gt;required&lt;/strong&gt;), visit &lt;a href=&#34;https://www.reddit.com/prefs/apps&#34;&gt;the Reddit Apps page.&lt;/a&gt; TL;DR set up an app that is a &#34;script&#34;. Copy your keys into the &lt;code&gt;.env&lt;/code&gt; file, along with whether your account uses two-factor authentication.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://sourceforge.net/projects/sox/files/sox/&#34;&gt;SoX&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;playwright install&lt;/code&gt; and &lt;code&gt;playwright install-deps&lt;/code&gt;. (if this fails try adding python -m to the front of the command)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;python main.py&lt;/code&gt; (unless you chose automatic install, then the installer will automatically run main.py) required**), visit &lt;a href=&#34;https://www.reddit.com/prefs/apps&#34;&gt;the Reddit Apps page.&lt;/a&gt; TL;DR set up an app that is a &#34;script&#34;. Copy your keys into the &lt;code&gt;.env&lt;/code&gt; file, along with whether your account uses two-factor authentication.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enjoy üòé&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;(Note if you got an error installing or running the bot try first rerunning the command with a three after the name e.g. python3 or pip3)&lt;/p&gt; &#xA;&lt;h2&gt;Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/66544866/173453972-6526e4e6-c6ef-41c5-ab40-5d275e724e7c.mp4&#34;&gt;https://user-images.githubusercontent.com/66544866/173453972-6526e4e6-c6ef-41c5-ab40-5d275e724e7c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing &amp;amp; Ways to improve üìà&lt;/h2&gt; &#xA;&lt;p&gt;In its current state, this bot does exactly what it needs to do. However, lots of improvements can be made.&lt;/p&gt; &#xA;&lt;p&gt;I have tried to simplify the code so anyone can read it and start contributing at any skill level. Don&#39;t be shy :) contribute!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Creating better documentation and adding a command line interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allowing users to choose a reddit thread instead of being randomized.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allowing users to choose a background that is picked instead of the Minecraft one.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allowing users to choose between any subreddit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allowing users to change voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Checks if a video has already been created&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Light and Dark modes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; NSFW post filter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please read our &lt;a href=&#34;https://raw.githubusercontent.com/elebumm/RedditVideoMakerBot/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more detailed information.&lt;/p&gt; &#xA;&lt;h2&gt;Developers and maintainers.&lt;/h2&gt; &#xA;&lt;p&gt;Elebumm (Lewis#6305) - &lt;a href=&#34;https://github.com/elebumm&#34;&gt;https://github.com/elebumm&lt;/a&gt; (Founder)&lt;/p&gt; &#xA;&lt;p&gt;Jason (JasonLovesDoggo#1904) - &lt;a href=&#34;https://github.com/JasonLovesDoggo&#34;&gt;https://github.com/JasonLovesDoggo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CallumIO (c.#6837) - &lt;a href=&#34;https://github.com/CallumIO&#34;&gt;https://github.com/CallumIO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;HarryDaDev (hrvyy#9677) - &lt;a href=&#34;https://github.com/ImmaHarry&#34;&gt;https://github.com/ImmaHarry&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;LukaHietala (Pix.#0001) - &lt;a href=&#34;https://github.com/LukaHietala&#34;&gt;https://github.com/LukaHietala&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Freebiell (Freebie#6429) - &lt;a href=&#34;https://github.com/FreebieII&#34;&gt;https://github.com/FreebieII&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>borisdayma/dalle-mini</title>
    <updated>2022-07-01T02:30:42Z</updated>
    <id>tag:github.com,2022-07-01:/borisdayma/dalle-mini</id>
    <link href="https://github.com/borisdayma/dalle-mini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DALL¬∑E Mini - Generate images from a text prompt&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DALL¬∑E Mini&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Join us on Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Generate images from a text prompt&lt;/em&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/borisdayma/dalle-mini/raw/main/img/logo.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;Our logo was generated with DALL¬∑E mini using the prompt &#34;logo of an armchair in the shape of an avocado&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use it?&lt;/h2&gt; &#xA;&lt;p&gt;There are several ways to use DALL¬∑E mini to create your own images:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;use &lt;a href=&#34;https://www.craiyon.com/&#34;&gt;the app at üñçÔ∏è craiyon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;experiment with the pipeline step by step through our &lt;a href=&#34;https://raw.githubusercontent.com/borisdayma/dalle-mini/main/tools/inference/inference_pipeline.ipynb&#34;&gt;&lt;code&gt;inference pipeline notebook&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also use these great projects from the community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;spin off your own app with &lt;a href=&#34;https://github.com/saharmor/dalle-playground&#34;&gt;DALL-E Playground repository&lt;/a&gt; (thanks &lt;a href=&#34;https://twitter.com/theaievangelist&#34;&gt;Sahar&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;try &lt;a href=&#34;https://github.com/jina-ai/dalle-flow&#34;&gt;DALL¬∑E Flow&lt;/a&gt; project for generating, diffusion, and upscaling in a Human-in-the-Loop workflow (thanks &lt;a href=&#34;https://github.com/hanxiao&#34;&gt;Han Xiao&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;run on &lt;a href=&#34;https://replicate.com/borisdayma/dalle-mini&#34;&gt;Replicate&lt;/a&gt;, in the browser or via API&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;Refer to our reports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy&#34;&gt;DALL¬∑E mini - Generate Images from Any Text Prompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA&#34;&gt;DALL¬∑E mini - Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2&#34;&gt;DALL¬∑E mega - Training Journal&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Join the community on the &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;LAION Discord&lt;/a&gt;. Any contribution is welcome, from reporting issues to proposing fixes/improvements or testing the model with cool prompts!&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Dependencies Installation&lt;/h3&gt; &#xA;&lt;p&gt;For inference only, use &lt;code&gt;pip install git+https://github.com/borisdayma/dalle-mini.git&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For development, clone the repo and use &lt;code&gt;pip install -e &#34;.[dev]&#34;&lt;/code&gt;. Before making a PR, check style with &lt;code&gt;make style&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training of DALL¬∑E mini&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/borisdayma/dalle-mini/main/tools/train/train.py&#34;&gt;&lt;code&gt;tools/train/train.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also adjust the &lt;a href=&#34;https://docs.wandb.ai/guides/sweeps&#34;&gt;sweep configuration file&lt;/a&gt; if you need to perform a hyperparameter search.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Where to find the latest models?&lt;/h3&gt; &#xA;&lt;p&gt;Trained models are on ü§ó Model Hub:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/dalle-mini/vqgan_imagenet_f16_16384&#34;&gt;VQGAN-f16-16384&lt;/a&gt; for encoding/decoding images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/flax-community/dalle-mini&#34;&gt;DALL¬∑E mini&lt;/a&gt; for generating images from a text prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Where does the logo come from?&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;armchair in the shape of an avocado&#34; was used by OpenAI when releasing DALL¬∑E to illustrate the model&#39;s capabilities. Having successful predictions on this prompt represents a big milestone for us.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ó Hugging Face for organizing &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects&#34;&gt;the FLAX/JAX community week&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google &lt;a href=&#34;https://sites.research.google/trc/&#34;&gt;TPU Research Cloud (TRC) program&lt;/a&gt; for providing computing resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.com/&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; for providing the infrastructure for experiment tracking and model management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Authors &amp;amp; Contributors&lt;/h2&gt; &#xA;&lt;p&gt;DALL¬∑E mini was initially developed by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/borisdayma&#34;&gt;Boris Dayma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/patil-suraj&#34;&gt;Suraj Patil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pcuenca&#34;&gt;Pedro Cuenca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khalidsaifullaah&#34;&gt;Khalid Saifullah&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tmabraham&#34;&gt;Tanishq Abraham&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lkhphuc&#34;&gt;Ph√∫c L√™ Kh·∫Øc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lukemelas&#34;&gt;Luke Melas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ghosh-r&#34;&gt;Ritobrata Ghosh&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Many thanks to the people who helped make it better:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;DALLE-Pytorch&lt;/a&gt; and &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt; communities for testing and exchanging cool ideas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rohan-anil&#34;&gt;Rohan Anil&lt;/a&gt; for adding Distributed Shampoo optimizer and always giving great suggestions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;Phil Wang&lt;/a&gt; has provided a lot of cool implementations of transformer variants and gives interesting insights with &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt; for &lt;a href=&#34;https://twitter.com/RiversHaveWings/status/1478093658716966912&#34;&gt;super conditioning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://gradio.app/&#34;&gt;Gradio team&lt;/a&gt; made an amazing UI for our app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing DALL¬∑E mini&lt;/h2&gt; &#xA;&lt;p&gt;If you find DALL¬∑E mini useful in your research or wish to refer, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{Dayma_DALL¬∑E_Mini_2021,&#xA;      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and L√™ Kh·∫Øc, Ph√∫c and Melas, Luke and Ghosh, Ritobrata},&#xA;      doi = {10.5281/zenodo.5146400},&#xA;      month = {7},&#xA;      title = {DALL¬∑E Mini},&#xA;      url = {https://github.com/borisdayma/dalle-mini},&#xA;      year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;Original DALL¬∑E from &#34;&lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34;&gt;Zero-Shot Text-to-Image Generation&lt;/a&gt;&#34; with image quantization from &#34;&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Image encoder from &#34;&lt;a href=&#34;https://arxiv.org/abs/2012.09841v2&#34;&gt;Taming Transformers for High-Resolution Image Synthesis&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Sequence to sequence model based on &#34;&lt;a href=&#34;https://arxiv.org/abs/1910.13461v1&#34;&gt;BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension&lt;/a&gt;&#34; with implementation of a few variants:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2002.05202&#34;&gt;GLU Variants Improve Transformer&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2203.00555&#34;&gt;Deepnet: Scaling Transformers to 1,000 Layers&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2110.09456&#34;&gt;NormFormer: Improved Transformer Pretraining with Extra Normalization&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2105.13290v2&#34;&gt;CogView: Mastering Text-to-Image Generation via Transformers&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/1910.07467&#34;&gt;Root Mean Square Layer Normalization&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;&lt;a href=&#34;https://arxiv.org/abs/2110.11773&#34;&gt;Sinkformers: Transformers with Doubly Stochastic Attention&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main optimizer (Distributed Shampoo) from &#34;&lt;a href=&#34;https://arxiv.org/abs/2002.09018&#34;&gt;Scalable Second Order Optimization for Deep Learning&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Citations&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={Zero-Shot Text-to-Image Generation}, &#xA;  author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},&#xA;  year={2021},&#xA;  eprint={2102.12092},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={Learning Transferable Visual Models From Natural Language Supervision}, &#xA;  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},&#xA;  year={2021},&#xA;  eprint={2103.00020},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={Taming Transformers for High-Resolution Image Synthesis}, &#xA;  author={Patrick Esser and Robin Rombach and Bj√∂rn Ommer},&#xA;  year={2021},&#xA;  eprint={2012.09841},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, &#xA;  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},&#xA;  year={2019},&#xA;  eprint={1910.13461},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={Scalable Second Order Optimization for Deep Learning},&#xA;  author={Rohan Anil and Vineet Gupta and Tomer Koren and Kevin Regan and Yoram Singer},&#xA;  year={2021},&#xA;  eprint={2002.09018},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={GLU Variants Improve Transformer},&#xA;  author={Noam Shazeer},&#xA;  year={2020},&#xA;  url={https://arxiv.org/abs/2002.05202}    &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt; @misc{&#xA;  title={DeepNet: Scaling transformers to 1,000 layers},&#xA;  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},&#xA;  year={2022},&#xA;  eprint={2203.00555}&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.LG}&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title={NormFormer: Improved Transformer Pretraining with Extra Normalization},&#xA;  author={Sam Shleifer and Jason Weston and Myle Ott},&#xA;  year={2021},&#xA;  eprint={2110.09456},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@inproceedings{&#xA;  title={Swin Transformer V2: Scaling Up Capacity and Resolution}, &#xA;  author={Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},&#xA;  booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title = {CogView: Mastering Text-to-Image Generation via Transformers},&#xA;  author = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},&#xA;  year = {2021},&#xA;  eprint = {2105.13290},&#xA;  archivePrefix = {arXiv},&#xA;  primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title = {Root Mean Square Layer Normalization},&#xA;  author = {Biao Zhang and Rico Sennrich},&#xA;  year = {2019},&#xA;  eprint = {1910.07467},&#xA;  archivePrefix = {arXiv},&#xA;  primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title = {Sinkformers: Transformers with Doubly Stochastic Attention},&#xA;  url = {https://arxiv.org/abs/2110.11773},&#xA;  author = {Sander, Michael E. and Ablin, Pierre and Blondel, Mathieu and Peyr√©, Gabriel},&#xA;  publisher = {arXiv},&#xA;  year = {2021},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{&#xA;  title = {Smooth activations and reproducibility in deep networks},&#xA;  url = {https://arxiv.org/abs/2010.09931},&#xA;  author = {Shamir, Gil I. and Lin, Dong and Coviello, Lorenzo},&#xA;  publisher = {arXiv},&#xA;  year = {2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>