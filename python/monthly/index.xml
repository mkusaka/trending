<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-01T01:49:11Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dreammis/social-auto-upload</title>
    <updated>2025-05-01T01:49:11Z</updated>
    <id>tag:github.com,2025-05-01:/dreammis/social-auto-upload</id>
    <link href="https://github.com/dreammis/social-auto-upload" rel="alternate"></link>
    <summary type="html">&lt;p&gt;自动化上传视频到社交媒体：抖音、小红书、视频号、tiktok、youtube、bilibili&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;social-auto-upload&lt;/h1&gt; &#xA;&lt;p&gt;social-auto-upload 该项目旨在自动化发布视频到各个社交媒体平台&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/show/tkupload.gif&#34; alt=&#34;tiktok show&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;💡Feature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;中国主流社交媒体平台：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 抖音&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 视频号&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; bilibili&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 小红书&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 快手&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 百家号&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; qq視頻&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;部分国外社交媒体：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; tiktok&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; youtube&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 易用版本(支持非开发人员使用)：Gui or Cli&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; API 封装 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Docker 部署&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 自动化上传(schedule)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 定时上传(cron)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; cookie 管理&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 国外平台proxy 设置&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 多线程上传&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; slack 推送&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;💾Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;playwright install chromium firefox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;非程序员，&lt;a href=&#34;https://juejin.cn/post/7372114027840208911&#34;&gt;新手级教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;🐇 About&lt;/h1&gt; &#xA;&lt;p&gt;该项目为我自用项目抽离出来，我的发布策略是定时发布（提前一天发布），故发布部分采用的事件均为第二天的时间&lt;/p&gt; &#xA;&lt;p&gt;如果你有需求立即发布，可自行研究源码或者向我提问&lt;/p&gt; &#xA;&lt;h1&gt;📃详细文档&lt;/h1&gt; &#xA;&lt;p&gt;请查看&lt;a href=&#34;https://sap-doc.nasdaddy.com/&#34;&gt;详细文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🐾Communicate&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/hysn2001m&#34;&gt;Donate as u like&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你也是&lt;code&gt;一个人&lt;/code&gt;，喜欢&lt;code&gt;折腾&lt;/code&gt;， 想要在如此恶劣的大环境寻找突破&lt;/p&gt; &#xA;&lt;p&gt;希望探索 #技术变现 #AI创业 #跨境航海 #自动化上传 #自动化视频 #技术探讨&lt;/p&gt; &#xA;&lt;p&gt;可以来群里和大家交流&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/mp.jpg&#34; alt=&#34;Nas&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/QR.png&#34; alt=&#34;赞赏&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;后台回复 &lt;code&gt;上传&lt;/code&gt; 加群交流&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;如果你觉得有用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果这个项目对你有帮助，⭐以表示支持&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dreammis/social-auto-upload&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jlowin/fastmcp</title>
    <updated>2025-05-01T01:49:11Z</updated>
    <id>tag:github.com,2025-05-01:/jlowin/fastmcp</id>
    <link href="https://github.com/jlowin/fastmcp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀 The fast, Pythonic way to build MCP servers and clients&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- omit in toc --&gt; &#xA; &lt;h1&gt;FastMCP v2 🚀&lt;/h1&gt; &#xA; &lt;p&gt;&lt;strong&gt;The fast, Pythonic way to build MCP servers and clients.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gofastmcp.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-gofastmcp.com-blue&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/fastmcp&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastmcp.svg?sanitize=true&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jlowin/fastmcp/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/jlowin/fastmcp.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/13266&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13266&#34; alt=&#34;jlowin%2Ffastmcp | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://modelcontextprotocol.io&#34;&gt;Model Context Protocol (MCP)&lt;/a&gt; is a new, standardized way to provide context and tools to your LLMs, and FastMCP makes building MCP servers and clients simple and intuitive. Create tools, expose resources, define prompts, and connect components with clean, Pythonic code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# server.py&#xA;from fastmcp import FastMCP&#xA;&#xA;mcp = FastMCP(&#34;Demo 🚀&#34;)&#xA;&#xA;@mcp.tool()&#xA;def add(a: int, b: int) -&amp;gt; int:&#xA;    &#34;&#34;&#34;Add two numbers&#34;&#34;&#34;&#xA;    return a + b&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    mcp.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the server locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastmcp run server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;FastMCP handles the complex protocol details and server management, letting you focus on building great tools and applications. It&#39;s designed to feel natural to Python developers.&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#what-is-mcp&#34;&gt;What is MCP?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#why-fastmcp&#34;&gt;Why FastMCP?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#key-features&#34;&gt;Key Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#servers&#34;&gt;Servers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#clients&#34;&gt;Clients&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#whats-new-in-v2&#34;&gt;What&#39;s New in v2?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#documentation&#34;&gt;Documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#core-concepts&#34;&gt;Core Concepts&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#the-fastmcp-server&#34;&gt;The &lt;code&gt;FastMCP&lt;/code&gt; Server&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#tools&#34;&gt;Tools&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#prompts&#34;&gt;Prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#context&#34;&gt;Context&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#images&#34;&gt;Images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#mcp-clients&#34;&gt;MCP Clients&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#client-methods&#34;&gt;Client Methods&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#transport-options&#34;&gt;Transport Options&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#llm-sampling&#34;&gt;LLM Sampling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#roots-access&#34;&gt;Roots Access&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#advanced-features&#34;&gt;Advanced Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#proxy-servers&#34;&gt;Proxy Servers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#composing-mcp-servers&#34;&gt;Composing MCP Servers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#openapi--fastapi-generation&#34;&gt;OpenAPI &amp;amp; FastAPI Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#handling-stderr&#34;&gt;Handling &lt;code&gt;stderr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#running-your-server&#34;&gt;Running Your Server&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#development-mode-recommended-for-building--testing&#34;&gt;Development Mode (Recommended for Building &amp;amp; Testing)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#claude-desktop-integration-for-regular-use&#34;&gt;Claude Desktop Integration (For Regular Use)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#direct-execution-for-advanced-use-cases&#34;&gt;Direct Execution (For Advanced Use Cases)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#server-object-names&#34;&gt;Server Object Names&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#contributing&#34;&gt;Contributing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#formatting--linting&#34;&gt;Formatting &amp;amp; Linting&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/#pull-requests&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is MCP?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://modelcontextprotocol.io&#34;&gt;Model Context Protocol (MCP)&lt;/a&gt; lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expose data through &lt;strong&gt;Resources&lt;/strong&gt; (think GET endpoints; load info into context)&lt;/li&gt; &#xA; &lt;li&gt;Provide functionality through &lt;strong&gt;Tools&lt;/strong&gt; (think POST/PUT endpoints; execute actions)&lt;/li&gt; &#xA; &lt;li&gt;Define interaction patterns through &lt;strong&gt;Prompts&lt;/strong&gt; (reusable templates)&lt;/li&gt; &#xA; &lt;li&gt;And more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FastMCP provides a high-level, Pythonic interface for building and interacting with these servers.&lt;/p&gt; &#xA;&lt;h2&gt;Why FastMCP?&lt;/h2&gt; &#xA;&lt;p&gt;The MCP protocol is powerful but implementing it involves a lot of boilerplate - server setup, protocol handlers, content types, error management. FastMCP handles all the complex protocol details and server management, so you can focus on building great tools. It&#39;s designed to be high-level and Pythonic; in most cases, decorating a function is all you need.&lt;/p&gt; &#xA;&lt;p&gt;FastMCP aims to be:&lt;/p&gt; &#xA;&lt;p&gt;🚀 &lt;strong&gt;Fast:&lt;/strong&gt; High-level interface means less code and faster development&lt;/p&gt; &#xA;&lt;p&gt;🍀 &lt;strong&gt;Simple:&lt;/strong&gt; Build MCP servers with minimal boilerplate&lt;/p&gt; &#xA;&lt;p&gt;🐍 &lt;strong&gt;Pythonic:&lt;/strong&gt; Feels natural to Python developers&lt;/p&gt; &#xA;&lt;p&gt;🔍 &lt;strong&gt;Complete:&lt;/strong&gt; FastMCP aims to provide a full implementation of the core MCP specification for both servers and clients&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;Servers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; servers with minimal boilerplate using intuitive decorators&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Proxy&lt;/strong&gt; existing servers to modify configuration or transport&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compose&lt;/strong&gt; servers into complex applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt; servers from OpenAPI specs or FastAPI objects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Clients&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interact&lt;/strong&gt; with MCP servers programmatically&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; to any MCP server using any transport&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test&lt;/strong&gt; your servers without manual intervention&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Innovate&lt;/strong&gt; with core MCP capabilities like LLM sampling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s New in v2?&lt;/h2&gt; &#xA;&lt;p&gt;FastMCP 1.0 made it so easy to build MCP servers that it&#39;s now part of the &lt;a href=&#34;https://github.com/modelcontextprotocol/python-sdk&#34;&gt;official Model Context Protocol Python SDK&lt;/a&gt;! For basic use cases, you can use the upstream version by importing &lt;code&gt;mcp.server.fastmcp.FastMCP&lt;/code&gt; (or installing &lt;code&gt;fastmcp=1.0&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Based on how the MCP ecosystem is evolving, FastMCP 2.0 builds on that foundation to introduce a variety of new features (and more experimental ideas). It adds advanced features like proxying and composing MCP servers, as well as automatically generating them from OpenAPI specs or FastAPI objects. FastMCP 2.0 also introduces new client-side functionality like LLM sampling.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;📚 FastMCP&#39;s documentation is available at &lt;a href=&#34;https://gofastmcp.com&#34;&gt;gofastmcp.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;We strongly recommend installing FastMCP with &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;, as it is required for deploying servers via the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv pip install fastmcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: on macOS, uv may need to be installed with Homebrew (&lt;code&gt;brew install uv&lt;/code&gt;) in order to make it available to the Claude Desktop app.&lt;/p&gt; &#xA;&lt;p&gt;For development, install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repo first&#xA;git clone https://github.com/jlowin/fastmcp.git&#xA;cd fastmcp&#xA;# Install with dev dependencies&#xA;uv sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s create a simple MCP server that exposes a calculator tool and some data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# server.py&#xA;from fastmcp import FastMCP&#xA;&#xA;# Create an MCP server&#xA;mcp = FastMCP(&#34;Demo&#34;)&#xA;&#xA;# Add an addition tool&#xA;@mcp.tool()&#xA;def add(a: int, b: int) -&amp;gt; int:&#xA;    &#34;&#34;&#34;Add two numbers&#34;&#34;&#34;&#xA;    return a + b&#xA;&#xA;# Add a dynamic greeting resource&#xA;@mcp.resource(&#34;greeting://{name}&#34;)&#xA;def get_greeting(name: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Get a personalized greeting&#34;&#34;&#34;&#xA;    return f&#34;Hello, {name}!&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can install this server in &lt;a href=&#34;https://claude.ai/download&#34;&gt;Claude Desktop&lt;/a&gt; and interact with it right away by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastmcp install server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jlowin/fastmcp/main/docs/assets/demo-inspector.png&#34; alt=&#34;MCP Inspector&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Core Concepts&lt;/h2&gt; &#xA;&lt;p&gt;These are the building blocks for creating MCP servers, using the familiar decorator-based approach.&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;FastMCP&lt;/code&gt; Server&lt;/h3&gt; &#xA;&lt;p&gt;The central object representing your MCP application. It handles connections, protocol details, and routing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import FastMCP&#xA;&#xA;# Create a named server&#xA;mcp = FastMCP(&#34;My App&#34;)&#xA;&#xA;# Specify dependencies needed when deployed via `fastmcp install`&#xA;mcp = FastMCP(&#34;My App&#34;, dependencies=[&#34;pandas&#34;, &#34;numpy&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;Tools allow LLMs to perform actions by executing your Python functions. They are ideal for tasks that involve computation, external API calls, or side effects.&lt;/p&gt; &#xA;&lt;p&gt;Decorate synchronous or asynchronous functions with &lt;code&gt;@mcp.tool()&lt;/code&gt;. FastMCP automatically generates the necessary MCP schema based on type hints and docstrings. Pydantic models can be used for complex inputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import httpx&#xA;from pydantic import BaseModel&#xA;&#xA;class UserInfo(BaseModel):&#xA;    user_id: int&#xA;    notify: bool = False&#xA;&#xA;@mcp.tool()&#xA;async def send_notification(user: UserInfo, message: str) -&amp;gt; dict:&#xA;    &#34;&#34;&#34;Sends a notification to a user if requested.&#34;&#34;&#34;&#xA;    if user.notify:&#xA;        # Simulate sending notification&#xA;        print(f&#34;Notifying user {user.user_id}: {message}&#34;)&#xA;        return {&#34;status&#34;: &#34;sent&#34;, &#34;user_id&#34;: user.user_id}&#xA;    return {&#34;status&#34;: &#34;skipped&#34;, &#34;user_id&#34;: user.user_id}&#xA;&#xA;@mcp.tool()&#xA;def get_stock_price(ticker: str) -&amp;gt; float:&#xA;    &#34;&#34;&#34;Gets the current price for a stock ticker.&#34;&#34;&#34;&#xA;    # Replace with actual API call&#xA;    prices = {&#34;AAPL&#34;: 180.50, &#34;GOOG&#34;: 140.20}&#xA;    return prices.get(ticker.upper(), 0.0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;Resources expose data to LLMs. They should primarily provide information without significant computation or side effects (like GET requests).&lt;/p&gt; &#xA;&lt;p&gt;Decorate functions with &lt;code&gt;@mcp.resource(&#34;your://uri&#34;)&lt;/code&gt;. Use curly braces &lt;code&gt;{}&lt;/code&gt; in the URI to define dynamic resources (templates) where parts of the URI become function parameters.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Static resource returning simple text&#xA;@mcp.resource(&#34;config://app-version&#34;)&#xA;def get_app_version() -&amp;gt; str:&#xA;    &#34;&#34;&#34;Returns the application version.&#34;&#34;&#34;&#xA;    return &#34;v2.1.0&#34;&#xA;&#xA;# Dynamic resource template expecting a &#39;user_id&#39; from the URI&#xA;@mcp.resource(&#34;db://users/{user_id}/email&#34;)&#xA;async def get_user_email(user_id: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Retrieves the email address for a given user ID.&#34;&#34;&#34;&#xA;    # Replace with actual database lookup&#xA;    emails = {&#34;123&#34;: &#34;alice@example.com&#34;, &#34;456&#34;: &#34;bob@example.com&#34;}&#xA;    return emails.get(user_id, &#34;not_found@example.com&#34;)&#xA;&#xA;# Resource returning JSON data&#xA;@mcp.resource(&#34;data://product-categories&#34;)&#xA;def get_categories() -&amp;gt; list[str]:&#xA;    &#34;&#34;&#34;Returns a list of available product categories.&#34;&#34;&#34;&#xA;    return [&#34;Electronics&#34;, &#34;Books&#34;, &#34;Home Goods&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prompts&lt;/h3&gt; &#xA;&lt;p&gt;Prompts define reusable templates or interaction patterns for the LLM. They help guide the LLM on how to use your server&#39;s capabilities effectively.&lt;/p&gt; &#xA;&lt;p&gt;Decorate functions with &lt;code&gt;@mcp.prompt()&lt;/code&gt;. The function should return the desired prompt content, which can be a simple string, a &lt;code&gt;Message&lt;/code&gt; object (like &lt;code&gt;UserMessage&lt;/code&gt; or &lt;code&gt;AssistantMessage&lt;/code&gt;), or a list of these.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp.prompts.base import UserMessage, AssistantMessage&#xA;&#xA;@mcp.prompt()&#xA;def ask_review(code_snippet: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Generates a standard code review request.&#34;&#34;&#34;&#xA;    return f&#34;Please review the following code snippet for potential bugs and style issues:\n```python\n{code_snippet}\n```&#34;&#xA;&#xA;@mcp.prompt()&#xA;def debug_session_start(error_message: str) -&amp;gt; list[Message]:&#xA;    &#34;&#34;&#34;Initiates a debugging help session.&#34;&#34;&#34;&#xA;    return [&#xA;        UserMessage(f&#34;I encountered an error:\n{error_message}&#34;),&#xA;        AssistantMessage(&#34;Okay, I can help with that. Can you provide the full traceback and tell me what you were trying to do?&#34;)&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Context&lt;/h3&gt; &#xA;&lt;p&gt;Gain access to MCP server capabilities &lt;em&gt;within&lt;/em&gt; your tool or resource functions by adding a parameter type-hinted with &lt;code&gt;fastmcp.Context&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import Context, FastMCP&#xA;&#xA;mcp = FastMCP(&#34;Context Demo&#34;)&#xA;&#xA;@mcp.resource(&#34;system://status&#34;)&#xA;async def get_system_status(ctx: Context) -&amp;gt; dict:&#xA;    &#34;&#34;&#34;Checks system status and logs information.&#34;&#34;&#34;&#xA;    await ctx.info(&#34;Checking system status...&#34;)&#xA;    # Perform checks&#xA;    await ctx.report_progress(1, 1) # Report completion&#xA;    return {&#34;status&#34;: &#34;OK&#34;, &#34;load&#34;: 0.5, &#34;client&#34;: ctx.client_id}&#xA;&#xA;@mcp.tool()&#xA;async def process_large_file(file_uri: str, ctx: Context) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Processes a large file, reporting progress and reading resources.&#34;&#34;&#34;&#xA;    await ctx.info(f&#34;Starting processing for {file_uri}&#34;)&#xA;    # Read the resource using the context&#xA;    file_content_resource = await ctx.read_resource(file_uri)&#xA;    file_content = file_content_resource[0].content # Assuming single text content&#xA;    lines = file_content.splitlines()&#xA;    total_lines = len(lines)&#xA;&#xA;    for i, line in enumerate(lines):&#xA;        # Process line...&#xA;        if (i + 1) % 100 == 0: # Report progress every 100 lines&#xA;            await ctx.report_progress(i + 1, total_lines)&#xA;&#xA;    await ctx.info(f&#34;Finished processing {file_uri}&#34;)&#xA;    return f&#34;Processed {total_lines} lines.&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;Context&lt;/code&gt; object provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logging: &lt;code&gt;ctx.debug()&lt;/code&gt;, &lt;code&gt;ctx.info()&lt;/code&gt;, &lt;code&gt;ctx.warning()&lt;/code&gt;, &lt;code&gt;ctx.error()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Progress Reporting: &lt;code&gt;ctx.report_progress(current, total)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Resource Access: &lt;code&gt;await ctx.read_resource(uri)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Request Info: &lt;code&gt;ctx.request_id&lt;/code&gt;, &lt;code&gt;ctx.client_id&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sampling (Advanced): &lt;code&gt;await ctx.sample(...)&lt;/code&gt; to ask the connected LLM client for completions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Images&lt;/h3&gt; &#xA;&lt;p&gt;Easily handle image outputs using the &lt;code&gt;fastmcp.Image&lt;/code&gt; helper class.&lt;/p&gt; &#xA;&lt;tip&gt;&#xA;  The below code requires the `pillow` library to be installed. &#xA;&lt;/tip&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mcp.server.fastmcp import FastMCP, Image&#xA;from io import BytesIO&#xA;try:&#xA;    from PIL import Image as PILImage&#xA;except ImportError:&#xA;    raise ImportError(&#34;Please install the `pillow` library to run this example.&#34;)&#xA;&#xA;mcp = FastMCP(&#34;My App&#34;)&#xA;&#xA;@mcp.tool()&#xA;def create_thumbnail(image_path: str) -&amp;gt; Image:&#xA;    &#34;&#34;&#34;Create a thumbnail from an image&#34;&#34;&#34;&#xA;    img = PILImage.open(image_path)&#xA;    img.thumbnail((100, 100))    &#xA;    buffer = BytesIO()&#xA;    img.save(buffer, format=&#34;PNG&#34;)&#xA;    return Image(data=buffer.getvalue(), format=&#34;png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Return the &lt;code&gt;Image&lt;/code&gt; helper class from your tool to send an image to the client. The &lt;code&gt;Image&lt;/code&gt; helper class handles the conversion to/from the base64-encoded format required by the MCP protocol. It works with either a path to an image file, or a bytes object.&lt;/p&gt; &#xA;&lt;h3&gt;MCP Clients&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;Client&lt;/code&gt; class lets you interact with any MCP server (not just FastMCP ones) from Python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import Client&#xA;&#xA;async with Client(&#34;path/to/server&#34;) as client:&#xA;    # Call a tool&#xA;    result = await client.call_tool(&#34;weather&#34;, {&#34;location&#34;: &#34;San Francisco&#34;})&#xA;    print(result)&#xA;    &#xA;    # Read a resource&#xA;    res = await client.read_resource(&#34;db://users/123/profile&#34;)&#xA;    print(res)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can connect to servers using any supported transport protocol (Stdio, SSE, FastMCP, etc.). If you don&#39;t specify a transport, the &lt;code&gt;Client&lt;/code&gt; class automatically attempts to detect an appropriate one from your connection string or server object.&lt;/p&gt; &#xA;&lt;h4&gt;Client Methods&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;Client&lt;/code&gt; class exposes several methods for interacting with MCP servers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async with Client(&#34;path/to/server&#34;) as client:&#xA;    # List available tools&#xA;    tools = await client.list_tools()&#xA;    &#xA;    # List available resources&#xA;    resources = await client.list_resources()&#xA;    &#xA;    # Call a tool with arguments&#xA;    result = await client.call_tool(&#34;generate_report&#34;, {&#34;user_id&#34;: 123})&#xA;    &#xA;    # Read a resource&#xA;    user_data = await client.read_resource(&#34;db://users/123/profile&#34;)&#xA;        &#xA;    # Get a prompt&#xA;    greeting = await client.get_prompt(&#34;welcome&#34;, {&#34;name&#34;: &#34;Alice&#34;})&#xA;    &#xA;    # Send progress updates&#xA;    await client.progress(&#34;task-123&#34;, 50, 100)  # 50% complete&#xA;    &#xA;    # Basic connectivity testing&#xA;    await client.ping()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These methods correspond directly to MCP protocol operations, making it easy to interact with any MCP-compatible server (not just FastMCP ones).&lt;/p&gt; &#xA;&lt;h4&gt;Transport Options&lt;/h4&gt; &#xA;&lt;p&gt;FastMCP supports various transport protocols for connecting to MCP servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import Client&#xA;from fastmcp.client.transports import (&#xA;    SSETransport, &#xA;    PythonStdioTransport, &#xA;    FastMCPTransport&#xA;)&#xA;&#xA;# Connect to a server over SSE (common for web-based MCP servers)&#xA;async with Client(SSETransport(&#34;http://localhost:8000/mcp&#34;)) as client:&#xA;    # Use client here...&#xA;&#xA;# Connect to a Python script using stdio (useful for local tools)&#xA;async with Client(PythonStdioTransport(&#34;path/to/script.py&#34;)) as client:&#xA;    # Use client here...&#xA;&#xA;# Connect directly to a FastMCP server object in the same process&#xA;from your_app import mcp_server&#xA;async with Client(FastMCPTransport(mcp_server)) as client:&#xA;    # Use client here...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Common transport options include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SSETransport&lt;/code&gt;: Connect to a server via Server-Sent Events (HTTP)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PythonStdioTransport&lt;/code&gt;: Run a Python script and communicate via stdio&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FastMCPTransport&lt;/code&gt;: Connect directly to a FastMCP server object&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WSTransport&lt;/code&gt;: Connect via WebSockets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition, if you pass a connection string or &lt;code&gt;FastMCP&lt;/code&gt; server object to the &lt;code&gt;Client&lt;/code&gt; constructor, it will try to automatically detect the appropriate transport.&lt;/p&gt; &#xA;&lt;h4&gt;LLM Sampling&lt;/h4&gt; &#xA;&lt;p&gt;Sampling is an MCP feature that allows a server to request a completion from the client LLM, enabling sophisticated use cases while maintaining security and privacy on the server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import marvin  # Or any other LLM client&#xA;from fastmcp import Client, Context, FastMCP&#xA;from fastmcp.client.sampling import RequestContext, SamplingMessage, SamplingParams&#xA;&#xA;# -- SERVER SIDE --&#xA;# Create a server that requests LLM completions from the client&#xA;&#xA;mcp = FastMCP(&#34;Sampling Example&#34;)&#xA;&#xA;@mcp.tool()&#xA;async def generate_poem(topic: str, context: Context) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Generate a short poem about the given topic.&#34;&#34;&#34;&#xA;    # The server requests a completion from the client LLM&#xA;    response = await context.sample(&#xA;        f&#34;Write a short poem about {topic}&#34;,&#xA;        system_prompt=&#34;You are a talented poet who writes concise, evocative verses.&#34;&#xA;    )&#xA;    return response.text&#xA;&#xA;@mcp.tool()&#xA;async def summarize_document(document_uri: str, context: Context) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Summarize a document using client-side LLM capabilities.&#34;&#34;&#34;&#xA;    # First read the document as a resource&#xA;    doc_resource = await context.read_resource(document_uri)&#xA;    doc_content = doc_resource[0].content  # Assuming single text content&#xA;    &#xA;    # Then ask the client LLM to summarize it&#xA;    response = await context.sample(&#xA;        f&#34;Summarize the following document:\n\n{doc_content}&#34;,&#xA;        system_prompt=&#34;You are an expert summarizer. Create a concise summary.&#34;&#xA;    )&#xA;    return response.text&#xA;&#xA;# -- CLIENT SIDE --&#xA;# Create a client that handles the sampling requests&#xA;&#xA;async def sampling_handler(&#xA;    messages: list[SamplingMessage],&#xA;    params: SamplingParams,&#xA;    ctx: RequestContext,&#xA;) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Handle sampling requests from the server using your preferred LLM.&#34;&#34;&#34;&#xA;    # Extract the messages and system prompt&#xA;    prompt = [m.content.text for m in messages if m.content.type == &#34;text&#34;]&#xA;    system_instruction = params.systemPrompt or &#34;You are a helpful assistant.&#34;&#xA;    &#xA;    # Use your preferred LLM client to generate completions&#xA;    return await marvin.say_async(&#xA;        message=prompt,&#xA;        instructions=system_instruction,&#xA;    )&#xA;&#xA;# Connect them together&#xA;async with Client(mcp, sampling_handler=sampling_handler) as client:&#xA;    result = await client.call_tool(&#34;generate_poem&#34;, {&#34;topic&#34;: &#34;autumn leaves&#34;})&#xA;    print(result.content[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This pattern is powerful because:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The server can delegate text generation to the client LLM&lt;/li&gt; &#xA; &lt;li&gt;The server remains focused on business logic and data handling&lt;/li&gt; &#xA; &lt;li&gt;The client maintains control over which LLM is used and how requests are handled&lt;/li&gt; &#xA; &lt;li&gt;No sensitive data needs to be sent to external APIs&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Roots Access&lt;/h4&gt; &#xA;&lt;p&gt;FastMCP exposes the MCP roots functionality, allowing clients to specify which file system roots they can access. This creates a secure boundary for tools that need to work with files. Note that the server must account for client roots explicitly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import Client, RootsList&#xA;&#xA;# Specify file roots that the client can access&#xA;roots = [&#34;file:///path/to/allowed/directory&#34;]&#xA;&#xA;async with Client(mcp_server, roots=roots) as client:&#xA;    # Now tools in the MCP server can access files in the specified roots&#xA;    await client.call_tool(&#34;process_file&#34;, {&#34;filename&#34;: &#34;data.csv&#34;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;p&gt;Building on the core concepts, FastMCP v2 introduces powerful features for more complex scenarios:&lt;/p&gt; &#xA;&lt;h3&gt;Proxy Servers&lt;/h3&gt; &#xA;&lt;p&gt;Create a FastMCP server that acts as an intermediary, proxying requests to another MCP endpoint (which could be a server or another client connection).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Use Cases:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transport Conversion:&lt;/strong&gt; Expose a server running on Stdio (like many local tools) over SSE or WebSockets, making it accessible to web clients or Claude Desktop.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adding Functionality:&lt;/strong&gt; Wrap an existing server to add authentication, request logging, or modified tool behavior.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aggregating Servers:&lt;/strong&gt; Combine multiple backend MCP servers behind a single proxy interface (though &lt;code&gt;mount&lt;/code&gt; might be simpler for this).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from fastmcp import FastMCP, Client&#xA;from fastmcp.client.transports import PythonStdioTransport&#xA;&#xA;# Create a client that connects to the original server&#xA;proxy_client = Client(&#xA;    transport=PythonStdioTransport(&#39;path/to/original_stdio_server.py&#39;),&#xA;)&#xA;&#xA;# Create a proxy server that connects to the client and exposes its capabilities&#xA;proxy = FastMCP.from_client(proxy_client, name=&#34;Stdio-to-SSE Proxy&#34;)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    proxy.run(transport=&#39;sse&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;FastMCP.from_client&lt;/code&gt; is a class method that connects to the target, discovers its capabilities, and dynamically builds the proxy server instance.&lt;/p&gt; &#xA;&lt;h3&gt;Composing MCP Servers&lt;/h3&gt; &#xA;&lt;p&gt;Structure larger MCP applications by creating modular FastMCP servers and &#34;mounting&#34; them onto a parent server. This automatically handles prefixing for tool names and resource URIs, preventing conflicts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastmcp import FastMCP&#xA;&#xA;# --- Weather MCP ---&#xA;weather_mcp = FastMCP(&#34;Weather Service&#34;)&#xA;&#xA;@weather_mcp.tool()&#xA;def get_forecast(city: str): &#xA;    return f&#34;Sunny in {city}&#34;&#xA;&#xA;@weather_mcp.resource(&#34;data://temp/{city}&#34;)&#xA;def get_temp(city: str): &#xA;    return 25.0&#xA;&#xA;# --- News MCP ---&#xA;news_mcp = FastMCP(&#34;News Service&#34;)&#xA;&#xA;@news_mcp.tool()&#xA;def fetch_headlines():&#xA;    return [&#34;Big news!&#34;, &#34;Other news&#34;]&#xA;&#xA;@news_mcp.resource(&#34;data://latest_story&#34;)&#xA;def get_story():&#xA;    return &#34;A story happened.&#34;&#xA;&#xA;# --- Composite MCP ---&#xA;&#xA;mcp = FastMCP(&#34;Composite&#34;)&#xA;&#xA;# Mount sub-apps with prefixes&#xA;mcp.mount(&#34;weather&#34;, weather_mcp) # Tools prefixed &#34;weather/&#34;, resources prefixed &#34;weather+&#34;&#xA;mcp.mount(&#34;news&#34;, news_mcp)       # Tools prefixed &#34;news/&#34;, resources prefixed &#34;news+&#34;&#xA;&#xA;@mcp.tool()&#xA;def ping(): &#xA;    return &#34;Composite OK&#34;&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    mcp.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This promotes code organization and reusability for complex MCP systems.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAPI &amp;amp; FastAPI Generation&lt;/h3&gt; &#xA;&lt;p&gt;Leverage your existing web APIs by automatically generating FastMCP servers from them.&lt;/p&gt; &#xA;&lt;p&gt;By default, the following rules are applied:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GET&lt;/code&gt; requests -&amp;gt; MCP resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GET&lt;/code&gt; requests with path parameters -&amp;gt; MCP resource templates&lt;/li&gt; &#xA; &lt;li&gt;All other HTTP methods -&amp;gt; MCP tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can override these rules to customize or even ignore certain endpoints.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From FastAPI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI&#xA;from fastmcp import FastMCP&#xA;&#xA;# Your existing FastAPI application&#xA;fastapi_app = FastAPI(title=&#34;My Existing API&#34;)&#xA;&#xA;@fastapi_app.get(&#34;/status&#34;)&#xA;def get_status(): &#xA;    return {&#34;status&#34;: &#34;running&#34;}&#xA;&#xA;@fastapi_app.post(&#34;/items&#34;)&#xA;def create_item(name: str, price: float): &#xA;    return {&#34;id&#34;: 1, &#34;name&#34;: name, &#34;price&#34;: price}&#xA;&#xA;# Generate an MCP server directly from the FastAPI app&#xA;mcp_server = FastMCP.from_fastapi(fastapi_app)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    mcp_server.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;From an OpenAPI Specification:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import httpx&#xA;import json&#xA;from fastmcp import FastMCP&#xA;&#xA;# Load the OpenAPI spec (dict)&#xA;# with open(&#34;my_api_spec.json&#34;, &#34;r&#34;) as f:&#xA;#     openapi_spec = json.load(f)&#xA;openapi_spec = { ... } # Your spec dict&#xA;&#xA;# Create an HTTP client to make requests to the actual API endpoint&#xA;http_client = httpx.AsyncClient(base_url=&#34;https://api.yourservice.com&#34;)&#xA;&#xA;# Generate the MCP server&#xA;mcp_server = FastMCP.from_openapi(openapi_spec, client=http_client)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    mcp_server.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Handling &lt;code&gt;stderr&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The MCP spec allows for the server to write anything it wants to &lt;code&gt;stderr&lt;/code&gt;, and it doesn&#39;t specify the format in any way. FastMCP will forward the server&#39;s &lt;code&gt;stderr&lt;/code&gt; to the client&#39;s &lt;code&gt;stderr&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running Your Server&lt;/h2&gt; &#xA;&lt;p&gt;Choose the method that best suits your needs:&lt;/p&gt; &#xA;&lt;h3&gt;Development Mode (Recommended for Building &amp;amp; Testing)&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;fastmcp dev&lt;/code&gt; for an interactive testing environment with the MCP Inspector.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastmcp dev your_server_file.py&#xA;# With temporary dependencies&#xA;fastmcp dev your_server_file.py --with pandas --with numpy&#xA;# With local package in editable mode&#xA;fastmcp dev your_server_file.py --with-editable .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Claude Desktop Integration (For Regular Use)&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;fastmcp install&lt;/code&gt; to set up your server for persistent use within the Claude Desktop app. It handles creating an isolated environment using &lt;code&gt;uv&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastmcp install your_server_file.py&#xA;# With a custom name in Claude&#xA;fastmcp install your_server_file.py --name &#34;My Analysis Tool&#34;&#xA;# With extra packages and environment variables&#xA;fastmcp install server.py --with requests -v API_KEY=123 -f .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Direct Execution (For Advanced Use Cases)&lt;/h3&gt; &#xA;&lt;p&gt;Run your server script directly for custom deployments or integrations outside of Claude. You manage the environment and dependencies yourself.&lt;/p&gt; &#xA;&lt;p&gt;Add to your &lt;code&gt;your_server_file.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if __name__ == &#34;__main__&#34;:&#xA;    mcp.run() # Assuming &#39;mcp&#39; is your FastMCP instance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python your_server_file.py&#xA;# or&#xA;uv run python your_server_file.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Server Object Names&lt;/h3&gt; &#xA;&lt;p&gt;If your &lt;code&gt;FastMCP&lt;/code&gt; instance is not named &lt;code&gt;mcp&lt;/code&gt;, &lt;code&gt;server&lt;/code&gt;, or &lt;code&gt;app&lt;/code&gt;, specify it using &lt;code&gt;file:object&lt;/code&gt; syntax for the &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;install&lt;/code&gt; commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastmcp dev my_module.py:my_mcp_instance&#xA;fastmcp install api.py:api_app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Explore the &lt;code&gt;examples/&lt;/code&gt; directory for code samples demonstrating various features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;simple_echo.py&lt;/code&gt;: Basic tool, resource, and prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;complex_inputs.py&lt;/code&gt;: Using Pydantic models for tool inputs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mount_example.py&lt;/code&gt;: Mounting multiple FastMCP servers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sampling.py&lt;/code&gt;: Using LLM completions within your MCP server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;screenshot.py&lt;/code&gt;: Tool returning an Image object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text_me.py&lt;/code&gt;: Tool interacting with an external API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;memory.py&lt;/code&gt;: More complex example with database interaction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions make the open-source community vibrant! We welcome improvements and features.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Open Developer Guide&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Setup&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Clone: &lt;code&gt;git clone https://github.com/jlowin/fastmcp.git &amp;amp;&amp;amp; cd fastmcp&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install Env &amp;amp; Dependencies: &lt;code&gt;uv venv &amp;amp;&amp;amp; uv sync&lt;/code&gt; (Activate the &lt;code&gt;.venv&lt;/code&gt; after creation)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;Testing&lt;/h4&gt; &#xA; &lt;p&gt;Run the test suite:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv run pytest -vv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Formatting &amp;amp; Linting&lt;/h4&gt; &#xA; &lt;p&gt;We use &lt;code&gt;ruff&lt;/code&gt; via &lt;code&gt;pre-commit&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install hooks: &lt;code&gt;pre-commit install&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run checks: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;Pull Requests&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA;  &lt;li&gt;Create a feature branch.&lt;/li&gt; &#xA;  &lt;li&gt;Make changes, commit, and push to your fork.&lt;/li&gt; &#xA;  &lt;li&gt;Open a pull request against the &lt;code&gt;main&lt;/code&gt; branch of &lt;code&gt;jlowin/fastmcp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Please open an issue or discussion for questions or suggestions!&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>BerriAI/litellm</title>
    <updated>2025-05-01T01:49:11Z</updated>
    <id>tag:github.com,2025-05-01:/BerriAI/litellm</id>
    <link href="https://github.com/BerriAI/litellm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; 🚅 LiteLLM &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://render.com/images/deploy-to-render-button.svg?sanitize=true&#34; alt=&#34;Deploy to Render&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://railway.app/template/HLP0Ub?referralCode=jch2ME&#34;&gt; &lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] &lt;br&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34; target=&#34;_blank&#34;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&#34;https://docs.litellm.ai/docs/hosted&#34; target=&#34;_blank&#34;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&#34;https://docs.litellm.ai/docs/enterprise&#34; target=&#34;_blank&#34;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/litellm/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/litellm.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.ycombinator.com/companies/berriai&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&#34; alt=&#34;Y Combinator W23&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://wa.link/huol9n&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=WhatsApp&amp;amp;color=success&amp;amp;logo=WhatsApp&amp;amp;style=flat-square&#34; alt=&#34;Whatsapp&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Discord&amp;amp;color=blue&amp;amp;logo=Discord&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;LiteLLM manages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Translate inputs to provider&#39;s &lt;code&gt;completion&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, and &lt;code&gt;image_generation&lt;/code&gt; endpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/output&#34;&gt;Consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - &lt;a href=&#34;https://docs.litellm.ai/docs/routing&#34;&gt;Router&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set Budgets &amp;amp; Rate limits per project, api key, model &lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs&#34;&gt;&lt;strong&gt;Jump to LiteLLM Proxy (LLM Gateway) Docs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs&#34;&gt;&lt;strong&gt;Jump to Supported LLM Providers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🚨 &lt;strong&gt;Stable Release:&lt;/strong&gt; Use docker images with the &lt;code&gt;-stable&lt;/code&gt; tag. These have undergone 12 hour load tests, before being published. &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/release_cycle&#34;&gt;More information about the release cycle here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Support for more providers. Missing a provider or LLM Platform, raise a &lt;a href=&#34;https://github.com/BerriAI/litellm/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.yml&amp;amp;title=%5BFeature%5D%3A+&#34;&gt;feature request&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage (&lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] LiteLLM v1.0.0 now requires &lt;code&gt;openai&amp;gt;=1.0.0&lt;/code&gt;. Migration guide &lt;a href=&#34;https://docs.litellm.ai/docs/migration&#34;&gt;here&lt;/a&gt;&lt;br&gt; LiteLLM v1.40.14+ now requires &lt;code&gt;pydantic&amp;gt;=2.0.0&lt;/code&gt;. No changes required.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install litellm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;import os&#xA;&#xA;## set ENV variables&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;your-openai-key&#34;&#xA;os.environ[&#34;ANTHROPIC_API_KEY&#34;] = &#34;your-anthropic-key&#34;&#xA;&#xA;messages = [{ &#34;content&#34;: &#34;Hello, how are you?&#34;,&#34;role&#34;: &#34;user&#34;}]&#xA;&#xA;# openai call&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=messages)&#xA;&#xA;# anthropic call&#xA;response = completion(model=&#34;anthropic/claude-3-sonnet-20240229&#34;, messages=messages)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response (OpenAI Format)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885&#34;,&#xA;    &#34;created&#34;: 1734366691,&#xA;    &#34;model&#34;: &#34;claude-3-sonnet-20240229&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion&#34;,&#xA;    &#34;system_fingerprint&#34;: null,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;finish_reason&#34;: &#34;stop&#34;,&#xA;            &#34;index&#34;: 0,&#xA;            &#34;message&#34;: {&#xA;                &#34;content&#34;: &#34;Hello! As an AI language model, I don&#39;t have feelings, but I&#39;m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?&#34;,&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;tool_calls&#34;: null,&#xA;                &#34;function_call&#34;: null&#xA;            }&#xA;        }&#xA;    ],&#xA;    &#34;usage&#34;: {&#xA;        &#34;completion_tokens&#34;: 43,&#xA;        &#34;prompt_tokens&#34;: 13,&#xA;        &#34;total_tokens&#34;: 56,&#xA;        &#34;completion_tokens_details&#34;: null,&#xA;        &#34;prompt_tokens_details&#34;: {&#xA;            &#34;audio_tokens&#34;: null,&#xA;            &#34;cached_tokens&#34;: 0&#xA;        },&#xA;        &#34;cache_creation_input_tokens&#34;: 0,&#xA;        &#34;cache_read_input_tokens&#34;: 0&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Call any model supported by a provider, with &lt;code&gt;model=&amp;lt;provider_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt;. There might be provider-specific details here, so refer to &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;provider docs for more information&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Async (&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-completion&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import acompletion&#xA;import asyncio&#xA;&#xA;async def test_get_response():&#xA;    user_message = &#34;Hello, how are you?&#34;&#xA;    messages = [{&#34;content&#34;: user_message, &#34;role&#34;: &#34;user&#34;}]&#xA;    response = await acompletion(model=&#34;openai/gpt-4o&#34;, messages=messages)&#xA;    return response&#xA;&#xA;response = asyncio.run(test_get_response())&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming (&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response.&lt;br&gt; Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=messages, stream=True)&#xA;for part in response:&#xA;    print(part.choices[0].delta.content or &#34;&#34;)&#xA;&#xA;# claude 2&#xA;response = completion(&#39;anthropic/claude-3-sonnet-20240229&#39;, messages, stream=True)&#xA;for part in response:&#xA;    print(part)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response chunk (OpenAI Format)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697&#34;,&#xA;    &#34;created&#34;: 1734366925,&#xA;    &#34;model&#34;: &#34;claude-3-sonnet-20240229&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion.chunk&#34;,&#xA;    &#34;system_fingerprint&#34;: null,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;finish_reason&#34;: null,&#xA;            &#34;index&#34;: 0,&#xA;            &#34;delta&#34;: {&#xA;                &#34;content&#34;: &#34;Hello&#34;,&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;function_call&#34;: null,&#xA;                &#34;tool_calls&#34;: null,&#xA;                &#34;audio&#34;: null&#xA;            },&#xA;            &#34;logprobs&#34;: null&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Logging Observability (&lt;a href=&#34;https://docs.litellm.ai/docs/observability/callbacks&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;&#xA;## set env variables for logging tools (when using MLflow, no API key set up is required)&#xA;os.environ[&#34;LUNARY_PUBLIC_KEY&#34;] = &#34;your-lunary-public-key&#34;&#xA;os.environ[&#34;HELICONE_API_KEY&#34;] = &#34;your-helicone-auth-key&#34;&#xA;os.environ[&#34;LANGFUSE_PUBLIC_KEY&#34;] = &#34;&#34;&#xA;os.environ[&#34;LANGFUSE_SECRET_KEY&#34;] = &#34;&#34;&#xA;os.environ[&#34;ATHINA_API_KEY&#34;] = &#34;your-athina-api-key&#34;&#xA;&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;your-openai-key&#34;&#xA;&#xA;# set callbacks&#xA;litellm.success_callback = [&#34;lunary&#34;, &#34;mlflow&#34;, &#34;langfuse&#34;, &#34;athina&#34;, &#34;helicone&#34;] # log input/output to lunary, langfuse, supabase, athina, helicone etc&#xA;&#xA;#openai call&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hi 👋 - i&#39;m openai&#34;}])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LiteLLM Proxy Server (LLM Gateway) - (&lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34;&gt;Docs&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;p&gt;Track spend + Load Balance across multiple projects&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/hosted&#34;&gt;Hosted Proxy (Preview)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The proxy provides:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth&#34;&gt;Hooks for auth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class&#34;&gt;Hooks for logging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend&#34;&gt;Cost tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/users#set-rate-limits&#34;&gt;Rate Limiting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📖 Proxy Endpoints - &lt;a href=&#34;https://litellm-api.up.railway.app/&#34;&gt;Swagger Docs&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Quick Start Proxy - CLI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#39;litellm[proxy]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Start litellm proxy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ litellm --model huggingface/bigcode/starcoder&#xA;&#xA;#INFO: Proxy running on http://0.0.0.0:4000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Make ChatCompletions Request to Proxy&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] 💡 &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/user_keys&#34;&gt;Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai # openai v1.0.0+&#xA;client = openai.OpenAI(api_key=&#34;anything&#34;,base_url=&#34;http://0.0.0.0:4000&#34;) # set proxy to base_url&#xA;# request sent to model set on litellm proxy, `litellm --model`&#xA;response = client.chat.completions.create(model=&#34;gpt-3.5-turbo&#34;, messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;this is a test request, write a short poem&#34;&#xA;    }&#xA;])&#xA;&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Proxy Key Management (&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Connect the proxy with a Postgres DB to create proxy keys&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get the code&#xA;git clone https://github.com/BerriAI/litellm&#xA;&#xA;# Go to folder&#xA;cd litellm&#xA;&#xA;# Add the master key - you can change this after setup&#xA;echo &#39;LITELLM_MASTER_KEY=&#34;sk-1234&#34;&#39; &amp;gt; .env&#xA;&#xA;# Add the litellm salt key - you cannot change this after adding a model&#xA;# It is used to encrypt / decrypt your LLM API Key credentials&#xA;# We recommend - https://1password.com/password-generator/ &#xA;# password generator to get a random hash for litellm salt key&#xA;echo &#39;LITELLM_SALT_KEY=&#34;sk-1234&#34;&#39; &amp;gt; .env&#xA;&#xA;source .env&#xA;&#xA;# Start&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;UI on &lt;code&gt;/ui&lt;/code&gt; on your proxy server &lt;img src=&#34;https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033&#34; alt=&#34;ui_3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Set budgets and rate limits across multiple projects &lt;code&gt;POST /key/generate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Request&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl &#39;http://0.0.0.0:4000/key/generate&#39; \&#xA;--header &#39;Authorization: Bearer sk-1234&#39; \&#xA;--header &#39;Content-Type: application/json&#39; \&#xA;--data-raw &#39;{&#34;models&#34;: [&#34;gpt-3.5-turbo&#34;, &#34;gpt-4&#34;, &#34;claude-2&#34;], &#34;duration&#34;: &#34;20m&#34;,&#34;metadata&#34;: {&#34;user&#34;: &#34;ishaan@berri.ai&#34;, &#34;team&#34;: &#34;core-infra&#34;}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Expected Response&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;{&#xA;    &#34;key&#34;: &#34;sk-kdEXbIqZRwEeEiHwdg7sFA&#34;, # Bearer token&#xA;    &#34;expires&#34;: &#34;2023-11-19T01:38:25.838000+00:00&#34; # datetime object&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported Providers (&lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/#basic-usage&#34;&gt;Completion&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#streaming-responses&#34;&gt;Streaming&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-completion&#34;&gt;Async Completion&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-streaming&#34;&gt;Async Streaming&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/embedding/supported_embedding&#34;&gt;Async Embedding&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/image_generation&#34;&gt;Async Image Generation&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/openai&#34;&gt;openai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/azure&#34;&gt;azure&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aiml&#34;&gt;AI/ML API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aws_sagemaker&#34;&gt;aws - sagemaker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/bedrock&#34;&gt;aws - bedrock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/vertex&#34;&gt;google - vertex_ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/palm&#34;&gt;google - palm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/gemini&#34;&gt;google AI Studio - gemini&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/mistral&#34;&gt;mistral ai api&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/cloudflare_workers&#34;&gt;cloudflare AI Workers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/cohere&#34;&gt;cohere&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/anthropic&#34;&gt;anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/empower&#34;&gt;empower&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/huggingface&#34;&gt;huggingface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/replicate&#34;&gt;replicate&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/togetherai&#34;&gt;together_ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/openrouter&#34;&gt;openrouter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/ai21&#34;&gt;ai21&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/baseten&#34;&gt;baseten&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/vllm&#34;&gt;vllm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/nlp_cloud&#34;&gt;nlp_cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aleph_alpha&#34;&gt;aleph alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/petals&#34;&gt;petals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/ollama&#34;&gt;ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/deepinfra&#34;&gt;deepinfra&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/perplexity&#34;&gt;perplexity-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/groq&#34;&gt;Groq AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/deepseek&#34;&gt;Deepseek&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/anyscale&#34;&gt;anyscale&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/watsonx&#34;&gt;IBM - watsonx.ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/voyage&#34;&gt;voyage ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/xinference&#34;&gt;xinference [Xorbits Inference]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/friendliai&#34;&gt;FriendliAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/galadriel&#34;&gt;Galadriel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;&lt;strong&gt;Read the Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and contributing LLM integrations are both accepted and highly encouraged! &lt;a href=&#34;https://docs.litellm.ai/docs/extras/contributing_code&#34;&gt;See our Contribution Guide for more details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Enterprise&lt;/h1&gt; &#xA;&lt;p&gt;For companies that need better security, user management and professional support&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat&#34;&gt;Talk to founders&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This covers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Features under the &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/enterprise&#34;&gt;LiteLLM Commercial License&lt;/a&gt;:&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Feature Prioritization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Custom Integrations&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Professional Support - Dedicated discord + slack&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Custom SLAs&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;strong&gt;Secure access with Single Sign-On&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Code Quality / Linting&lt;/h1&gt; &#xA;&lt;p&gt;LiteLLM follows the &lt;a href=&#34;https://google.github.io/styleguide/pyguide.html&#34;&gt;Google Python Style Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ruff for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320&#34;&gt;formatting and linting checks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mypy + Pyright for typing &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4&#34;&gt;2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Black for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79&#34;&gt;formatting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;isort for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10&#34;&gt;import sorting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have suggestions on how to improve the code quality feel free to open an issue or a PR.&lt;/p&gt; &#xA;&lt;h1&gt;Support / talk with founders&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version&#34;&gt;Schedule Demo 👋&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt;Community Discord 💭&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬&lt;/li&gt; &#xA; &lt;li&gt;Our emails ✉️ &lt;a href=&#34;mailto:ishaan@berri.ai&#34;&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href=&#34;mailto:krrish@berri.ai&#34;&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Why did we build this&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI and Cohere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;a href=&#34;https://github.com/BerriAI/litellm/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=BerriAI/litellm&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Run in Developer mode&lt;/h2&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup .env file in root&lt;/li&gt; &#xA; &lt;li&gt;Run dependant services &lt;code&gt;docker-compose up db prometheus&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Backend&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(In root) create virtual environment &lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate virtual environment &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;pip install -e &#34;.[all]&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start proxy backend &lt;code&gt;uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;code&gt;ui/litellm-dashboard&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;npm run dev&lt;/code&gt; to start the dashboard&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>