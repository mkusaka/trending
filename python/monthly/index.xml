<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-01T01:52:17Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>coqui-ai/TTS</title>
    <updated>2024-10-01T01:52:17Z</updated>
    <id>tag:github.com,2024-10-01:/coqui-ai/TTS</id>
    <link href="https://github.com/coqui-ai/TTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üê∏üí¨ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üê∏Coqui.ai News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTSv2 is here with 16 languages and better performance across the board.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS fine-tuning code is out. Check the &lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech&#34;&gt;example recipes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS can now stream with &amp;lt;200ms latency.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS, our production TTS model that can speak 13 languages, is released &lt;a href=&#34;https://coqui.ai/blog/tts/open_xtts&#34;&gt;Blog Post&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/coqui/xtts&#34;&gt;Demo&lt;/a&gt;, &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/xtts.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;üê∂Bark&lt;/a&gt; is now available for inference with unconstrained voice cloning. &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/bark.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ You can use &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;~1100 Fairseq models&lt;/a&gt; with üê∏TTS.&lt;/li&gt; &#xA; &lt;li&gt;üì£ üê∏TTS now supports üê¢Tortoise with faster inference. &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/tortoise.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2&#34;&gt; &#xA; &lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png&#34; height=&#34;56&#34;&gt;&lt;/h2&gt; &#xA; &lt;p&gt;&lt;strong&gt;üê∏TTS is a library for advanced Text-to-Speech generation.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;üöÄ Pretrained models in +1100 languages.&lt;/p&gt; &#xA; &lt;p&gt;üõ†Ô∏è Tools for training new models and fine-tuning existing models in any language.&lt;/p&gt; &#xA; &lt;p&gt;üìö Utilities for dataset analysis and curation.&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/5eXr5seRrv&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1037326658807533628?color=%239B59B6&amp;amp;label=chat%20on%20discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MPL-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/TTS&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/TTS.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667&#34; alt=&#34;Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/tts&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/tts&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/265612440&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/265612440.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/tts/badge/?version=latest&amp;amp;style=plastic&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí¨ Where to ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Platforms&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üö® &lt;strong&gt;Bug Reports&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üéÅ &lt;strong&gt;Feature Requests &amp;amp; Ideas&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Usage Questions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üóØ &lt;strong&gt;General Discussion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; or &lt;a href=&#34;https://discord.gg/5eXr5seRrv&#34;&gt;Discord&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíº &lt;strong&gt;Documentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;ReadTheDocs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev#installation&#34;&gt;TTS/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Contributing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìå &lt;strong&gt;Road Map&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/issues/378&#34;&gt;Main Development Plans&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üöÄ &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/releases&#34;&gt;TTS Releases&lt;/a&gt; and &lt;a href=&#34;https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models&#34;&gt;Experimental Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üì∞ &lt;strong&gt;Papers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/erogol/TTS-papers&#34;&gt;TTS Papers&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ü•á TTS Performance&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are &lt;strong&gt;internal&lt;/strong&gt; üê∏TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-performance Deep Learning models for Text2Speech tasks. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).&lt;/li&gt; &#xA;   &lt;li&gt;Speaker Encoder to compute speaker embeddings efficiently.&lt;/li&gt; &#xA;   &lt;li&gt;Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast and efficient model training.&lt;/li&gt; &#xA; &lt;li&gt;Detailed training logs on the terminal and Tensorboard.&lt;/li&gt; &#xA; &lt;li&gt;Support for Multi-speaker TTS.&lt;/li&gt; &#xA; &lt;li&gt;Efficient, flexible, lightweight but feature complete &lt;code&gt;Trainer API&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Released and ready-to-use models.&lt;/li&gt; &#xA; &lt;li&gt;Tools to curate Text2Speech datasets under&lt;code&gt;dataset_analysis&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Utilities to use and test your models.&lt;/li&gt; &#xA; &lt;li&gt;Modular (but not too much) code base enabling easy implementation of new ideas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Implementations&lt;/h2&gt; &#xA;&lt;h3&gt;Spectrogram models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tacotron: &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tacotron2: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Glow-TTS: &lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Speedy-Speech: &lt;a href=&#34;https://arxiv.org/abs/2008.03802&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Align-TTS: &lt;a href=&#34;https://arxiv.org/abs/2003.01950&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastPitch: &lt;a href=&#34;https://arxiv.org/pdf/2006.06873.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech: &lt;a href=&#34;https://arxiv.org/abs/1905.09263&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech2: &lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SC-GlowTTS: &lt;a href=&#34;https://arxiv.org/abs/2104.05557&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Capacitron: &lt;a href=&#34;https://arxiv.org/abs/1906.03402&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OverFlow: &lt;a href=&#34;https://arxiv.org/abs/2211.06892&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural HMM TTS: &lt;a href=&#34;https://arxiv.org/abs/2108.13320&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Delightful TTS: &lt;a href=&#34;https://arxiv.org/abs/2110.12612&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;End-to-End Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ìçTTS: &lt;a href=&#34;https://coqui.ai/blog/tts/open_xtts&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VITS: &lt;a href=&#34;https://arxiv.org/pdf/2106.06103&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê∏ YourTTS: &lt;a href=&#34;https://arxiv.org/abs/2112.02418&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê¢ Tortoise: &lt;a href=&#34;https://github.com/neonbjb/tortoise-tts&#34;&gt;orig. repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê∂ Bark: &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;orig. repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Attention Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Guided Attention: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forward Backward Decoding: &lt;a href=&#34;https://arxiv.org/abs/1907.09006&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Graves Attention: &lt;a href=&#34;https://arxiv.org/abs/1910.10288&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Double Decoder Consistency: &lt;a href=&#34;https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Convolutional Attention: &lt;a href=&#34;https://arxiv.org/pdf/1910.10288.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alignment Network: &lt;a href=&#34;https://arxiv.org/abs/2108.10447&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Speaker Encoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GE2E: &lt;a href=&#34;https://arxiv.org/abs/1710.10467&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Angular Loss: &lt;a href=&#34;https://arxiv.org/pdf/2003.11982.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MelGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MultiBandMelGAN: &lt;a href=&#34;https://arxiv.org/abs/2005.05106&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ParallelWaveGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GAN-TTS discriminators: &lt;a href=&#34;https://arxiv.org/abs/1909.11646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveRNN: &lt;a href=&#34;https://github.com/fatchord/WaveRNN/&#34;&gt;origin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveGrad: &lt;a href=&#34;https://arxiv.org/abs/2009.00713&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HiFiGAN: &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;UnivNet: &lt;a href=&#34;https://arxiv.org/abs/2106.07889&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Voice Conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FreeVC: &lt;a href=&#34;https://arxiv.org/abs/2210.15418&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also help us implement more models.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;üê∏TTS is tested on Ubuntu 18.04 with &lt;strong&gt;python &amp;gt;= 3.9, &amp;lt; 3.12.&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are only interested in &lt;a href=&#34;https://tts.readthedocs.io/en/latest/inference.html&#34;&gt;synthesizing speech&lt;/a&gt; with the released üê∏TTS models, installing from PyPI is the easiest option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install TTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you plan to code or train models, clone üê∏TTS and install it locally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coqui-ai/TTS&#xA;pip install -e .[all,dev,notebooks]  # Select the relevant extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Ubuntu (Debian), you can also run following commands for installation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.&#xA;$ make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Windows, üëë@GuyPaddock wrote installation instructions &lt;a href=&#34;https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Docker Image&lt;/h2&gt; &#xA;&lt;p&gt;You can also try TTS without install with the docker image. Simply run the following command and you will be able to run TTS without installing it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu&#xA;python3 TTS/server/server.py --list_models #To get the list of available models&#xA;python3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then enjoy the TTS server &lt;a href=&#34;http://%5B::1%5D:5002/&#34;&gt;here&lt;/a&gt; More details about the docker images (like GPU support) can be found &lt;a href=&#34;https://tts.readthedocs.io/en/latest/docker_images.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Synthesizing speech by üê∏TTS&lt;/h2&gt; &#xA;&lt;h3&gt;üêç Python API&lt;/h3&gt; &#xA;&lt;h4&gt;Running a multi-speaker and multi-lingual model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from TTS.api import TTS&#xA;&#xA;# Get device&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;# List available üê∏TTS models&#xA;print(TTS().list_models())&#xA;&#xA;# Init TTS&#xA;tts = TTS(&#34;tts_models/multilingual/multi-dataset/xtts_v2&#34;).to(device)&#xA;&#xA;# Run TTS&#xA;# ‚ùó Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language&#xA;# Text to speech list of amplitude values as output&#xA;wav = tts.tts(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;)&#xA;# Text to speech to a file&#xA;tts.tts_to_file(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running a single speaker model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init TTS with the target model name&#xA;tts = TTS(model_name=&#34;tts_models/de/thorsten/tacotron2-DDC&#34;, progress_bar=False).to(device)&#xA;&#xA;# Run TTS&#xA;tts.tts_to_file(text=&#34;Ich bin eine Testnachricht.&#34;, file_path=OUTPUT_PATH)&#xA;&#xA;# Example voice cloning with YourTTS in English, French and Portuguese&#xA;tts = TTS(model_name=&#34;tts_models/multilingual/multi-dataset/your_tts&#34;, progress_bar=False).to(device)&#xA;tts.tts_to_file(&#34;This is voice cloning.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)&#xA;tts.tts_to_file(&#34;C&#39;est le clonage de la voix.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;fr-fr&#34;, file_path=&#34;output.wav&#34;)&#xA;tts.tts_to_file(&#34;Isso √© clonagem de voz.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;pt-br&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example voice conversion&lt;/h4&gt; &#xA;&lt;p&gt;Converting the voice in &lt;code&gt;source_wav&lt;/code&gt; to the voice of &lt;code&gt;target_wav&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tts = TTS(model_name=&#34;voice_conversion_models/multilingual/vctk/freevc24&#34;, progress_bar=False).to(&#34;cuda&#34;)&#xA;tts.voice_conversion_to_file(source_wav=&#34;my/source.wav&#34;, target_wav=&#34;my/target.wav&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example voice cloning together with the voice conversion model.&lt;/h4&gt; &#xA;&lt;p&gt;This way, you can clone voices by using any model in üê∏TTS.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;tts = TTS(&#34;tts_models/de/thorsten/tacotron2-DDC&#34;)&#xA;tts.tts_with_vc_to_file(&#xA;    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,&#xA;    speaker_wav=&#34;target/speaker.wav&#34;,&#xA;    file_path=&#34;output.wav&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example text to speech using &lt;strong&gt;Fairseq models in ~1100 languages&lt;/strong&gt; ü§Ø.&lt;/h4&gt; &#xA;&lt;p&gt;For Fairseq models, use the following name format: &lt;code&gt;tts_models/&amp;lt;lang-iso_code&amp;gt;/fairseq/vits&lt;/code&gt;. You can find the language ISO codes &lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;here&lt;/a&gt; and learn about the Fairseq models &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TTS with on the fly voice conversion&#xA;api = TTS(&#34;tts_models/deu/fairseq/vits&#34;)&#xA;api.tts_with_vc_to_file(&#xA;    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,&#xA;    speaker_wav=&#34;target/speaker.wav&#34;,&#xA;    file_path=&#34;output.wav&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command-line &lt;code&gt;tts&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;!-- begin-tts-readme --&gt; &#xA;&lt;p&gt;Synthesize speech on command line.&lt;/p&gt; &#xA;&lt;p&gt;You can either use your trained model or choose a model from the provided list.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t specify any models, then it uses LJSpeech based English model.&lt;/p&gt; &#xA;&lt;h4&gt;Single Speaker Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List provided models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --list_models&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get model info (for both tts_models and vocoder_models):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/name: The model_info_by_name uses the name as it from the --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts&#xA;$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/idx: The model_query_idx uses the corresponding idx from --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx &#34;&amp;lt;model_type&amp;gt;/&amp;lt;model_query_idx&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx tts_models/3&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query info for model info by full name:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS with default models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS and pipe out the generated TTS wav file data:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --pipe_out --out_path output/path/speech.wav | aplay&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run a TTS model with its default vocoder model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run with specific TTS and vocoder models from the list:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --vocoder_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS model (Using Griffin-Lim Vocoder):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS and Vocoder models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-speaker Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List the available speakers and choose a &amp;lt;speaker_id&amp;gt; among them:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --list_speaker_idxs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the multi-speaker TTS model with the target speaker ID:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own multi-speaker TTS model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Voice Conversion Models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ tts --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --source_wav &amp;lt;path/to/speaker/wav&amp;gt; --target_wav &amp;lt;path/to/reference/wav&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- end-tts-readme --&gt; &#xA;&lt;h2&gt;Directory Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)&#xA;|- utils/           (common utilities.)&#xA;|- TTS&#xA;    |- bin/             (folder for all the executables.)&#xA;      |- train*.py                  (train your target model.)&#xA;      |- ...&#xA;    |- tts/             (text to speech models)&#xA;        |- layers/          (model layer definitions)&#xA;        |- models/          (model definitions)&#xA;        |- utils/           (model specific utilities.)&#xA;    |- speaker_encoder/ (Speaker Encoder models.)&#xA;        |- (same)&#xA;    |- vocoder/         (Vocoder models.)&#xA;        |- (same)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>odoo/odoo</title>
    <updated>2024-10-01T01:52:17Z</updated>
    <id>tag:github.com,2024-10-01:/odoo/odoo</id>
    <link href="https://github.com/odoo/odoo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Odoo. Open Source Apps To Grow Your Business.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://runbot.odoo.com/runbot&#34;&gt;&lt;img src=&#34;https://runbot.odoo.com/runbot/badge/flat/1/master.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.odoo.com/documentation/17.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F&#34; alt=&#34;Tech Doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.odoo.com/forum/help-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F&#34; alt=&#34;Help&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://nightly.odoo.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F&#34; alt=&#34;Nightly Builds&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Odoo&lt;/h2&gt; &#xA;&lt;p&gt;Odoo is a suite of web based open source business apps.&lt;/p&gt; &#xA;&lt;p&gt;The main Odoo Apps include an &lt;a href=&#34;https://www.odoo.com/page/crm&#34;&gt;Open Source CRM&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/website&#34;&gt;Website Builder&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/ecommerce&#34;&gt;eCommerce&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/inventory&#34;&gt;Warehouse Management&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/project&#34;&gt;Project Management&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/accounting&#34;&gt;Billing &amp;amp; Accounting&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/point-of-sale-shop&#34;&gt;Point of Sale&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/employees&#34;&gt;Human Resources&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/social-marketing&#34;&gt;Marketing&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/app/manufacturing&#34;&gt;Manufacturing&lt;/a&gt;, &lt;a href=&#34;https://www.odoo.com/&#34;&gt;...&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get a full-featured &lt;a href=&#34;https://www.odoo.com&#34;&gt;Open Source ERP&lt;/a&gt; when you install several Apps.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started with Odoo&lt;/h2&gt; &#xA;&lt;p&gt;For a standard installation please follow the &lt;a href=&#34;https://www.odoo.com/documentation/17.0/administration/install/install.html&#34;&gt;Setup instructions&lt;/a&gt; from the documentation.&lt;/p&gt; &#xA;&lt;p&gt;To learn the software, we recommend the &lt;a href=&#34;https://www.odoo.com/slides&#34;&gt;Odoo eLearning&lt;/a&gt;, or &lt;a href=&#34;https://www.odoo.com/page/scale-up-business-game&#34;&gt;Scale-up&lt;/a&gt;, the &lt;a href=&#34;https://www.odoo.com/page/scale-up-business-game&#34;&gt;business game&lt;/a&gt;. Developers can start with &lt;a href=&#34;https://www.odoo.com/documentation/17.0/developer/howtos.html&#34;&gt;the developer tutorials&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>unclecode/crawl4ai</title>
    <updated>2024-10-01T01:52:17Z</updated>
    <id>tag:github.com,2024-10-01:/unclecode/crawl4ai</id>
    <link href="https://github.com/unclecode/crawl4ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî•üï∑Ô∏è Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scrapper&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Crawl4AI (Async Version) üï∑Ô∏èü§ñ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/unclecode/crawl4ai/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/unclecode/crawl4ai?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/unclecode/crawl4ai?style=social&#34; alt=&#34;GitHub Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/unclecode/crawl4ai&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/unclecode/crawl4ai&#34; alt=&#34;GitHub Pull Requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/unclecode/crawl4ai&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Crawl4AI simplifies asynchronous web crawling and data extraction, making it accessible for large language models (LLMs) and AI applications. üÜìüåê&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Looking for the synchronous version? Check out &lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/README.sync.md&#34;&gt;README.sync.md&lt;/a&gt;. You can also access the previous version in the branch &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/v0.2.76&#34;&gt;V0.2.76&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Try it Now!&lt;/h2&gt; &#xA;&lt;p&gt;‚ú® Play around with this &lt;a href=&#34;https://colab.research.google.com/drive/1REChY6fXQf-EaVYLv0eHEWvzlYxGm0pd?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ú® Visit our &lt;a href=&#34;https://crawl4ai.com/mkdocs/&#34;&gt;Documentation Website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üÜì Completely free and open-source&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Blazing fast performance, outperforming many paid services&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ LLM-friendly output formats (JSON, cleaned HTML, markdown)&lt;/li&gt; &#xA; &lt;li&gt;üåç Supports crawling multiple URLs simultaneously&lt;/li&gt; &#xA; &lt;li&gt;üé® Extracts and returns all media tags (Images, Audio, and Video)&lt;/li&gt; &#xA; &lt;li&gt;üîó Extracts all external and internal links&lt;/li&gt; &#xA; &lt;li&gt;üìö Extracts metadata from the page&lt;/li&gt; &#xA; &lt;li&gt;üîÑ Custom hooks for authentication, headers, and page modifications before crawling&lt;/li&gt; &#xA; &lt;li&gt;üïµÔ∏è User-agent customization&lt;/li&gt; &#xA; &lt;li&gt;üñºÔ∏è Takes screenshots of the page&lt;/li&gt; &#xA; &lt;li&gt;üìú Executes multiple custom JavaScripts before crawling&lt;/li&gt; &#xA; &lt;li&gt;üìä Generates structured output without LLM using JsonCssExtractionStrategy&lt;/li&gt; &#xA; &lt;li&gt;üìö Various chunking strategies: topic-based, regex, sentence, and more&lt;/li&gt; &#xA; &lt;li&gt;üß† Advanced extraction strategies: cosine clustering, LLM, and more&lt;/li&gt; &#xA; &lt;li&gt;üéØ CSS selector support for precise data extraction&lt;/li&gt; &#xA; &lt;li&gt;üìù Passes instructions/keywords to refine extraction&lt;/li&gt; &#xA; &lt;li&gt;üîí Proxy support for enhanced privacy and access&lt;/li&gt; &#xA; &lt;li&gt;üîÑ Session management for complex multi-page crawling scenarios&lt;/li&gt; &#xA; &lt;li&gt;üåê Asynchronous architecture for improved performance and scalability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation üõ†Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.&lt;/p&gt; &#xA;&lt;h3&gt;Using pip üêç&lt;/h3&gt; &#xA;&lt;p&gt;Choose the installation option that best fits your needs:&lt;/p&gt; &#xA;&lt;h4&gt;Basic Installation&lt;/h4&gt; &#xA;&lt;p&gt;For basic web crawling and scraping tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install crawl4ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.&lt;/p&gt; &#xA;&lt;p&gt;üëâ Note: When you install Crawl4AI, the setup script should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Through the command line:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If the above doesn&#39;t work, try this more specific command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m playwright install chromium&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This second method has proven to be more reliable in some cases.&lt;/p&gt; &#xA;&lt;h4&gt;Installation with Synchronous Version&lt;/h4&gt; &#xA;&lt;p&gt;If you need the synchronous version using Selenium:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install crawl4ai[sync]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Development Installation&lt;/h4&gt; &#xA;&lt;p&gt;For contributors who plan to modify the source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/unclecode/crawl4ai.git&#xA;cd crawl4ai&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Docker üê≥&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;re in the process of creating Docker images and pushing them to Docker Hub. This will provide an easy way to run Crawl4AI in a containerized environment. Stay tuned for updates!&lt;/p&gt; &#xA;&lt;p&gt;For more detailed installation instructions and options, please refer to our &lt;a href=&#34;https://crawl4ai.com/mkdocs/installation&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start üöÄ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from crawl4ai import AsyncWebCrawler&#xA;&#xA;async def main():&#xA;    async with AsyncWebCrawler(verbose=True) as crawler:&#xA;        result = await crawler.arun(url=&#34;https://www.nbcnews.com/business&#34;)&#xA;        print(result.markdown)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced Usage üî¨&lt;/h2&gt; &#xA;&lt;h3&gt;Executing JavaScript and Using CSS Selectors&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from crawl4ai import AsyncWebCrawler&#xA;&#xA;async def main():&#xA;    async with AsyncWebCrawler(verbose=True) as crawler:&#xA;        js_code = [&#34;const loadMoreButton = Array.from(document.querySelectorAll(&#39;button&#39;)).find(button =&amp;gt; button.textContent.includes(&#39;Load More&#39;)); loadMoreButton &amp;amp;&amp;amp; loadMoreButton.click();&#34;]&#xA;        result = await crawler.arun(&#xA;            url=&#34;https://www.nbcnews.com/business&#34;,&#xA;            js_code=js_code,&#xA;            css_selector=&#34;article.tease-card&#34;,&#xA;            bypass_cache=True&#xA;        )&#xA;        print(result.extracted_content)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using a Proxy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from crawl4ai import AsyncWebCrawler&#xA;&#xA;async def main():&#xA;    async with AsyncWebCrawler(verbose=True, proxy=&#34;http://127.0.0.1:7890&#34;) as crawler:&#xA;        result = await crawler.arun(&#xA;            url=&#34;https://www.nbcnews.com/business&#34;,&#xA;            bypass_cache=True&#xA;        )&#xA;        print(result.markdown)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Extracting Structured Data without LLM&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;JsonCssExtractionStrategy&lt;/code&gt; allows for precise extraction of structured data from web pages using CSS selectors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;import json&#xA;from crawl4ai import AsyncWebCrawler&#xA;from crawl4ai.extraction_strategy import JsonCssExtractionStrategy&#xA;&#xA;async def extract_news_teasers():&#xA;    schema = {&#xA;        &#34;name&#34;: &#34;News Teaser Extractor&#34;,&#xA;        &#34;baseSelector&#34;: &#34;.wide-tease-item__wrapper&#34;,&#xA;        &#34;fields&#34;: [&#xA;            {&#xA;                &#34;name&#34;: &#34;category&#34;,&#xA;                &#34;selector&#34;: &#34;.unibrow span[data-testid=&#39;unibrow-text&#39;]&#34;,&#xA;                &#34;type&#34;: &#34;text&#34;,&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;headline&#34;,&#xA;                &#34;selector&#34;: &#34;.wide-tease-item__headline&#34;,&#xA;                &#34;type&#34;: &#34;text&#34;,&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;summary&#34;,&#xA;                &#34;selector&#34;: &#34;.wide-tease-item__description&#34;,&#xA;                &#34;type&#34;: &#34;text&#34;,&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;time&#34;,&#xA;                &#34;selector&#34;: &#34;[data-testid=&#39;wide-tease-date&#39;]&#34;,&#xA;                &#34;type&#34;: &#34;text&#34;,&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;image&#34;,&#xA;                &#34;type&#34;: &#34;nested&#34;,&#xA;                &#34;selector&#34;: &#34;picture.teasePicture img&#34;,&#xA;                &#34;fields&#34;: [&#xA;                    {&#34;name&#34;: &#34;src&#34;, &#34;type&#34;: &#34;attribute&#34;, &#34;attribute&#34;: &#34;src&#34;},&#xA;                    {&#34;name&#34;: &#34;alt&#34;, &#34;type&#34;: &#34;attribute&#34;, &#34;attribute&#34;: &#34;alt&#34;},&#xA;                ],&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;link&#34;,&#xA;                &#34;selector&#34;: &#34;a[href]&#34;,&#xA;                &#34;type&#34;: &#34;attribute&#34;,&#xA;                &#34;attribute&#34;: &#34;href&#34;,&#xA;            },&#xA;        ],&#xA;    }&#xA;&#xA;    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)&#xA;&#xA;    async with AsyncWebCrawler(verbose=True) as crawler:&#xA;        result = await crawler.arun(&#xA;            url=&#34;https://www.nbcnews.com/business&#34;,&#xA;            extraction_strategy=extraction_strategy,&#xA;            bypass_cache=True,&#xA;        )&#xA;&#xA;        assert result.success, &#34;Failed to crawl the page&#34;&#xA;&#xA;        news_teasers = json.loads(result.extracted_content)&#xA;        print(f&#34;Successfully extracted {len(news_teasers)} news teasers&#34;)&#xA;        print(json.dumps(news_teasers[0], indent=2))&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(extract_news_teasers())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced usage examples, check out our &lt;a href=&#34;https://crawl4ai.com/mkdocs/full_details/advanced_jsoncss_extraction.md&#34;&gt;Examples&lt;/a&gt; section in the documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting Structured Data with OpenAI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import asyncio&#xA;from crawl4ai import AsyncWebCrawler&#xA;from crawl4ai.extraction_strategy import LLMExtractionStrategy&#xA;from pydantic import BaseModel, Field&#xA;&#xA;class OpenAIModelFee(BaseModel):&#xA;    model_name: str = Field(..., description=&#34;Name of the OpenAI model.&#34;)&#xA;    input_fee: str = Field(..., description=&#34;Fee for input token for the OpenAI model.&#34;)&#xA;    output_fee: str = Field(..., description=&#34;Fee for output token for the OpenAI model.&#34;)&#xA;&#xA;async def main():&#xA;    async with AsyncWebCrawler(verbose=True) as crawler:&#xA;        result = await crawler.arun(&#xA;            url=&#39;https://openai.com/api/pricing/&#39;,&#xA;            word_count_threshold=1,&#xA;            extraction_strategy=LLMExtractionStrategy(&#xA;                provider=&#34;openai/gpt-4o&#34;, api_token=os.getenv(&#39;OPENAI_API_KEY&#39;), &#xA;                schema=OpenAIModelFee.schema(),&#xA;                extraction_type=&#34;schema&#34;,&#xA;                instruction=&#34;&#34;&#34;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. &#xA;                Do not miss any models in the entire content. One extracted model JSON format should look like this: &#xA;                {&#34;model_name&#34;: &#34;GPT-4&#34;, &#34;input_fee&#34;: &#34;US$10.00 / 1M tokens&#34;, &#34;output_fee&#34;: &#34;US$30.00 / 1M tokens&#34;}.&#34;&#34;&#34;&#xA;            ),            &#xA;            bypass_cache=True,&#xA;        )&#xA;        print(result.extracted_content)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Session Management and Dynamic Content Crawling&lt;/h3&gt; &#xA;&lt;p&gt;Crawl4AI excels at handling complex scenarios, such as crawling multiple pages with dynamic content loaded via JavaScript. Here&#39;s an example of crawling GitHub commits across multiple pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;import re&#xA;from bs4 import BeautifulSoup&#xA;from crawl4ai import AsyncWebCrawler&#xA;&#xA;async def crawl_typescript_commits():&#xA;    first_commit = &#34;&#34;&#xA;    async def on_execution_started(page):&#xA;        nonlocal first_commit &#xA;        try:&#xA;            while True:&#xA;                await page.wait_for_selector(&#39;li.Box-sc-g0xbh4-0 h4&#39;)&#xA;                commit = await page.query_selector(&#39;li.Box-sc-g0xbh4-0 h4&#39;)&#xA;                commit = await commit.evaluate(&#39;(element) =&amp;gt; element.textContent&#39;)&#xA;                commit = re.sub(r&#39;\s+&#39;, &#39;&#39;, commit)&#xA;                if commit and commit != first_commit:&#xA;                    first_commit = commit&#xA;                    break&#xA;                await asyncio.sleep(0.5)&#xA;        except Exception as e:&#xA;            print(f&#34;Warning: New content didn&#39;t appear after JavaScript execution: {e}&#34;)&#xA;&#xA;    async with AsyncWebCrawler(verbose=True) as crawler:&#xA;        crawler.crawler_strategy.set_hook(&#39;on_execution_started&#39;, on_execution_started)&#xA;&#xA;        url = &#34;https://github.com/microsoft/TypeScript/commits/main&#34;&#xA;        session_id = &#34;typescript_commits_session&#34;&#xA;        all_commits = []&#xA;&#xA;        js_next_page = &#34;&#34;&#34;&#xA;        const button = document.querySelector(&#39;a[data-testid=&#34;pagination-next-button&#34;]&#39;);&#xA;        if (button) button.click();&#xA;        &#34;&#34;&#34;&#xA;&#xA;        for page in range(3):  # Crawl 3 pages&#xA;            result = await crawler.arun(&#xA;                url=url,&#xA;                session_id=session_id,&#xA;                css_selector=&#34;li.Box-sc-g0xbh4-0&#34;,&#xA;                js=js_next_page if page &amp;gt; 0 else None,&#xA;                bypass_cache=True,&#xA;                js_only=page &amp;gt; 0&#xA;            )&#xA;&#xA;            assert result.success, f&#34;Failed to crawl page {page + 1}&#34;&#xA;&#xA;            soup = BeautifulSoup(result.cleaned_html, &#39;html.parser&#39;)&#xA;            commits = soup.select(&#34;li&#34;)&#xA;            all_commits.extend(commits)&#xA;&#xA;            print(f&#34;Page {page + 1}: Found {len(commits)} commits&#34;)&#xA;&#xA;        await crawler.crawler_strategy.kill_session(session_id)&#xA;        print(f&#34;Successfully crawled {len(all_commits)} commits across 3 pages&#34;)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(crawl_typescript_commits())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example demonstrates Crawl4AI&#39;s ability to handle complex scenarios where content is loaded asynchronously. It crawls multiple pages of GitHub commits, executing JavaScript to load new content and using custom hooks to ensure data is loaded before proceeding.&lt;/p&gt; &#xA;&lt;p&gt;For more advanced usage examples, check out our &lt;a href=&#34;https://crawl4ai.com/mkdocs/full_details/session_based_crawling.md&#34;&gt;Examples&lt;/a&gt; section in the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Speed Comparison üöÄ&lt;/h2&gt; &#xA;&lt;p&gt;Crawl4AI is designed with speed as a primary focus. Our goal is to provide the fastest possible response with high-quality data extraction, minimizing abstractions between the data and the user.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve conducted a speed comparison between Crawl4AI and Firecrawl, a paid service. The results demonstrate Crawl4AI&#39;s superior performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Firecrawl:&#xA;Time taken: 7.02 seconds&#xA;Content length: 42074 characters&#xA;Images found: 49&#xA;&#xA;Crawl4AI (simple crawl):&#xA;Time taken: 1.60 seconds&#xA;Content length: 18238 characters&#xA;Images found: 49&#xA;&#xA;Crawl4AI (with JavaScript execution):&#xA;Time taken: 4.64 seconds&#xA;Content length: 40869 characters&#xA;Images found: 89&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you can see, Crawl4AI outperforms Firecrawl significantly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple crawl: Crawl4AI is over 4 times faster than Firecrawl.&lt;/li&gt; &#xA; &lt;li&gt;With JavaScript execution: Even when executing JavaScript to load more content (doubling the number of images found), Crawl4AI is still faster than Firecrawl&#39;s simple crawl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find the full comparison code in our repository at &lt;code&gt;docs/examples/crawl4ai_vs_firecrawl.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation üìö&lt;/h2&gt; &#xA;&lt;p&gt;For detailed documentation, including installation instructions, advanced features, and API reference, visit our &lt;a href=&#34;https://crawl4ai.com/mkdocs/&#34;&gt;Documentation Website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the open-source community. Check out our &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License üìÑ&lt;/h2&gt; &#xA;&lt;p&gt;Crawl4AI is released under the &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact üìß&lt;/h2&gt; &#xA;&lt;p&gt;For questions, suggestions, or feedback, feel free to reach out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/unclecode&#34;&gt;unclecode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/unclecode&#34;&gt;@unclecode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://crawl4ai.com&#34;&gt;crawl4ai.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Happy Crawling! üï∏Ô∏èüöÄ&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#unclecode/crawl4ai&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=unclecode/crawl4ai&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>