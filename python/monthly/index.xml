<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-01T02:07:38Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haotian-liu/LLaVA</title>
    <updated>2023-11-01T02:07:38Z</updated>
    <id>tag:github.com,2023-11-01:/haotian-liu/LLaVA</id>
    <link href="https://github.com/haotian-liu/LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS&#39;23 Oral] Visual Instruction Tuning: LLaVA (Large Language-and-Vision Assistant) built towards GPT-4V level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üåã LLaVA: Large Language and Vision Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;ü§ùCommunity Contributions: [&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt;] [&lt;a href=&#34;https://github.com/camenduru/LLaVA-colab&#34;&gt;Colab&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/badayvedat/LLaVA&#34;&gt;ü§óSpace&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Baselines with Visual Instruction Tuning&lt;/strong&gt; [&lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&#34;https://yuheng-li.github.io/&#34;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; (NeurIPS 2023, &lt;strong&gt;Oral&lt;/strong&gt;) [&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Paper&lt;/a&gt;]&lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu*&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/&#34;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;!--p align=&#34;center&#34;&gt;&#xA;    &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;images/llava_logo.png&#34; width=&#34;50%&#34;&gt;&lt;/a&gt; &lt;br&gt;&#xA;    Generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; via &#34;a cute lava llama with glasses&#34; and box prompt&#xA;&lt;/p--&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[10/26] üî• LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15&#34;&gt;ckpts&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;script&lt;/a&gt;). We also provide a &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/li&gt; &#xA; &lt;li&gt;[10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href=&#34;https://huggingface.co/spaces/etri-vilab/Ko-LLaVA&#34;&gt;ü§ó Demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;[10/12] LLaVA is now supported in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/li&gt; &#xA; &lt;li&gt;[10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[10/5] üî• LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;technical report&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[9/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href=&#34;https://llava-rlhf.github.io/&#34;&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[9/22] &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[9/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href=&#34;https://arxiv.org/abs/2309.09958&#34;&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href=&#34;https://arxiv.org/abs/2309.10020&#34;&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#39;&#39;.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[7/19] üî• We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[6/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA; &lt;li&gt;[6/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[6/1] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[5/6] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] üî• We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[4/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/17] üî• We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#Demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;If you are not using Linux, do &lt;em&gt;NOT&lt;/em&gt; proceed, see instructions for &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/macOS.md&#34;&gt;macOS&lt;/a&gt; and &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Windows.md&#34;&gt;Windows&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/haotian-liu/LLaVA.git&#xA;cd LLaVA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e &#34;.[train]&#34;&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade to latest code base&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLaVA Weights&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt; for all public LLaVA checkpoints, and the instructions of how to use the weights.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;To run our demo, you need to prepare LLaVA checkpoints locally. Please follow the instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;here&lt;/a&gt; to download the checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;To launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server &lt;em&gt;ONCE&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart BT&#xA;    %% Declare Nodes&#xA;    gws(&#34;Gradio (UI Server)&#34;)&#xA;    c(&#34;Controller (API Server):&amp;lt;br/&amp;gt;PORT: 10000&#34;)&#xA;    mw7b(&#34;Model Worker:&amp;lt;br/&amp;gt;llava-v1.5-7b&amp;lt;br/&amp;gt;PORT: 40000&#34;)&#xA;    mw13b(&#34;Model Worker:&amp;lt;br/&amp;gt;llava-v1.5-13b&amp;lt;br/&amp;gt;PORT: 40001&#34;)&#xA;&#xA;    %% Declare Styles&#xA;    classDef data fill:#3af,stroke:#48a,stroke-width:2px,color:#444&#xA;    classDef success fill:#8f8,stroke:#0a0,stroke-width:2px,color:#444&#xA;    classDef failure fill:#f88,stroke:#f00,stroke-width:2px,color:#444&#xA;&#xA;    %% Assign Styles&#xA;    class id,od data;&#xA;    class cimg,cs_s,scsim_s success;&#xA;    class ncimg,cs_f,scsim_f failure;&#xA;&#xA;    subgraph Demo Connections&#xA;        direction BT&#xA;        c&amp;lt;--&amp;gt;gws&#xA;        &#xA;        mw7b&amp;lt;--&amp;gt;c&#xA;        mw13b&amp;lt;--&amp;gt;c&#xA;    end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; &#xA;&lt;p&gt;You can launch as many workers as you want, and compare between different model checkpoints in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path &amp;lt;ckpt2&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using an Apple device with an M1 or M2 chip, you can specify the mps device by using the &lt;code&gt;--device&lt;/code&gt; flag: &lt;code&gt;--device mps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;. Below is an example of running with the first two GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (4-bit, 8-bit inference, quantized)&lt;/h4&gt; &#xA;&lt;p&gt;You can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint, potentially allowing you to run on a GPU with as few as 12GB VRAM. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append &lt;code&gt;--load-4bit&lt;/code&gt; or &lt;code&gt;--load-8bit&lt;/code&gt; to the &lt;strong&gt;model worker&lt;/strong&gt; command that you are executing. Below is an example of running with 4-bit quantization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (LoRA weights, unmerged)&lt;/h4&gt; &#xA;&lt;p&gt;You can launch the model worker with LoRA weights, without merging them with the base checkpoint, to save disk space. There will be additional loading time, while the inference speed is the same as the merged checkpoints. Unmerged LoRA checkpoints do not have &lt;code&gt;lora-merge&lt;/code&gt; in the model name, and are usually much smaller (less than 1GB) than the merged checkpoints (13G for 7B, and 25G for 13B).&lt;/p&gt; &#xA;&lt;p&gt;To load unmerged LoRA weights, you simply need to pass an additional argument &lt;code&gt;--model-base&lt;/code&gt;, which is the base LLM that is used to train the LoRA weights. You can check the base LLM of each LoRA weights in the &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base lmsys/vicuna-13b-v1.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;Chat about images using LLaVA without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization, for our LLaVA-1.5-7B, it uses less than 8GB VRAM on a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.cli \&#xA;    --model-path liuhaotian/llava-v1.5-7b \&#xA;    --image-file &#34;https://llava-vl.github.io/static/images/view.jpg&#34; \&#xA;    --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/demo_cli.gif&#34; width=&#34;70%&#34;&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Below is the latest training configuration for LLaVA v1.5. For legacy models, please refer to README of &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/tree/v1.0.1&#34;&gt;this&lt;/a&gt; version for now. We&#39;ll add them in a separate doc later.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLaVA training consists of two stages: (1) feature alignment stage: use our 558K subset of the LAION-CC-SBU dataset to connect a &lt;em&gt;frozen pretrained&lt;/em&gt; vision encoder to a &lt;em&gt;frozen LLM&lt;/em&gt;; (2) visual instruction tuning stage: use 150K GPT-generated multimodal instruction-following data, plus around 515K VQA data from academic-oriented tasks, to teach the model to follow multimodal instructions.&lt;/p&gt; &#xA;&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly. Always keep the global batch size the same: &lt;code&gt;per_device_train_batch_size&lt;/code&gt; x &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; x &lt;code&gt;num_gpus&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-v1.5-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-v1.5-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Download Vicuna checkpoints (automatically)&lt;/h3&gt; &#xA;&lt;p&gt;Our base model Vicuna v1.5, which is an instruction-tuned chatbot, will be downloaded automatically when you run our provided training scripts. No action is needed.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrain (feature alignment)&lt;/h3&gt; &#xA;&lt;p&gt;Please download the 558K subset of the LAION-CC-SBU dataset with BLIP captions we use in the paper &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pretrain takes around 5.5 hours for LLaVA-v1.5-13B on 8x A100 (80G), due to the increased resolution to 336px. It takes around 3.5 hours for LLaVA-v1.5-7B.&lt;/p&gt; &#xA;&lt;p&gt;Training script with DeepSpeed ZeRO-2: &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/pretrain.sh&#34;&gt;&lt;code&gt;pretrain.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--mm_projector_type mlp2x_gelu&lt;/code&gt;: the two-layer MLP vision-language connector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--vision_tower openai/clip-vit-large-patch14-336&lt;/code&gt;: CLIP ViT-L/14 336px.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visual Instruction Tuning&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please download the annotation of the final mixture our instruction tuning data &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json&#34;&gt;llava_v1_5_mix665k.json&lt;/a&gt;, and download the images from constituting datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;COCO: &lt;a href=&#34;http://images.cocodataset.org/zips/train2017.zip&#34;&gt;train2017&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GQA: &lt;a href=&#34;https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip&#34;&gt;images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OCR-VQA: &lt;a href=&#34;https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing&#34;&gt;download script&lt;/a&gt;, &lt;strong&gt;we save all files as &lt;code&gt;.jpg&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;TextVQA: &lt;a href=&#34;https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip&#34;&gt;train_val_images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VisualGenome: &lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip&#34;&gt;part1&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip&#34;&gt;part2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After downloading all of them, organize the data as follows in &lt;code&gt;./playground/data&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ coco&#xA;‚îÇ   ‚îî‚îÄ‚îÄ train2017&#xA;‚îú‚îÄ‚îÄ gqa&#xA;‚îÇ   ‚îî‚îÄ‚îÄ images&#xA;‚îú‚îÄ‚îÄ ocr_vqa&#xA;‚îÇ   ‚îî‚îÄ‚îÄ images&#xA;‚îú‚îÄ‚îÄ textvqa&#xA;‚îÇ   ‚îî‚îÄ‚îÄ train_images&#xA;‚îî‚îÄ‚îÄ vg&#xA;    ‚îú‚îÄ‚îÄ VG_100K&#xA;    ‚îî‚îÄ‚îÄ VG_100K_2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start training!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You may download our pretrained projectors in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. It is not recommended to use legacy projectors, as they may be trained with a different version of the codebase, and if any option is off, the model will not function/train as we expected.&lt;/p&gt; &#xA;&lt;p&gt;Visual instruction tuning takes around 20 hours for LLaVA-v1.5-13B on 8x A100 (80G), due to the increased resolution to 336px. It takes around 10 hours for LLaVA-v1.5-7B on 8x A100 (40G).&lt;/p&gt; &#xA;&lt;p&gt;Training script with DeepSpeed ZeRO-3: &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/finetune.sh&#34;&gt;&lt;code&gt;finetune.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are do not have enough GPU memory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use LoRA: &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/finetune_lora.sh&#34;&gt;&lt;code&gt;finetune_lora.sh&lt;/code&gt;&lt;/a&gt;. We are able to fit 13B training in 8-A100-40G/8-A6000, and 7B training in 8-RTX3090. Make sure &lt;code&gt;per_device_train_batch_size*gradient_accumulation_steps&lt;/code&gt; is the same as the provided script for best reproducibility.&lt;/li&gt; &#xA; &lt;li&gt;Replace &lt;code&gt;zero3.json&lt;/code&gt; with &lt;code&gt;zero3_offload.json&lt;/code&gt; which offloads some parameters to CPU RAM. This slows down the training speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are interested in finetuning LLaVA model to your own task/data, please check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;&lt;code&gt;Finetune_Custom_Data.md&lt;/code&gt;&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;New options to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--mm_projector_type mlp2x_gelu&lt;/code&gt;: the two-layer MLP vision-language connector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--vision_tower openai/clip-vit-large-patch14-336&lt;/code&gt;: CLIP ViT-L/14 336px.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--image_aspect_ratio pad&lt;/code&gt;: this pads the non-square images to square, instead of cropping them; it slightly reduces hallucination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--group_by_modality_length True&lt;/code&gt;: this should only be used when your instruction tuning dataset contains both language (e.g. ShareGPT) and multimodal (e.g. LLaVA-Instruct). It makes the training sampler only sample a single modality (either image or language) during training, which we observe to speed up training by ~25%, and does not affect the final outcome.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;In LLaVA-1.5, we evaluate models on a diverse set of 12 benchmarks. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;Evaluation.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-path ./checkpoints/LLaVA-13B-v0 \&#xA;    --question-file \&#xA;    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --image-folder \&#xA;    /path/to/coco2014_val \&#xA;    --answers-file \&#xA;    /path/to/answer-file-our.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl&#34;&gt;&lt;code&gt;answer-file-ref.jsonl&lt;/code&gt;&lt;/a&gt; is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;OPENAI_API_KEY=&#34;sk-***********************************&#34; python llava/eval/eval_gpt_review_visual.py \&#xA;    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --context llava/eval/table/caps_boxes_coco2014_val_80.jsonl \&#xA;    --answer-list \&#xA;    /path/to/answer-file-ref.jsonl \&#xA;    /path/to/answer-file-our.jsonl \&#xA;    --rule llava/eval/table/rule.json \&#xA;    --output /path/to/review.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find LLaVA useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;@misc{liu2023improvedllava,&#xA;      title={Improved Baselines with Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},&#xA;      publisher={arXiv:2310.03744},&#xA;      year={2023},&#xA;}&#xA;&#xA;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={arXiv:2304.08485},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Em1tSan/NeuroGPT</title>
    <updated>2023-11-01T02:07:38Z</updated>
    <id>tag:github.com,2023-11-01:/Em1tSan/NeuroGPT</id>
    <link href="https://github.com/Em1tSan/NeuroGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free ChatGPT 3.5 / ChatGPT 4 | Free OpenAI / ChatGPT API&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://t.me/neurogen_news&#34;&gt; &lt;img src=&#34;https://readme-typing-svg.herokuapp.com?font=Jura&amp;amp;weight=700&amp;amp;size=30&amp;amp;duration=4000&amp;amp;pause=1000&amp;amp;color=1BED29&amp;amp;center=true&amp;amp;width=435&amp;amp;lines=NeuroGPT+by+NeurogenAI&#34; alt=&#34;NeuroGPT&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;&lt;strong&gt; &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/raw/main/.github/README_RU.md&#34;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | English &lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Your API access to ChatGPT.&lt;/p&gt; &#xA; &lt;pre&gt;&#xA;Technical work is underway to improve stability&#xA;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;¬∑ &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/wiki#english-language&#34;&gt;Wiki&lt;/a&gt; ¬∑ &lt;a href=&#34;https://chat.neuroapi.host/&#34;&gt;Web Site&lt;/a&gt; ¬∑ Docs ¬∑ Q&amp;amp;A ¬∑ &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/commits/main&#34;&gt;Change Log&lt;/a&gt; ¬∑&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/wiki/PC-client-installation#windows&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Windows-1371c3?logo=windows&#34; alt=&#34;windows&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/wiki/PC-client-installation#linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Linux-F1502F?logo=linux&#34; alt=&#34;linux&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/wiki/PC-client-installation#macos&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-MacOS-C0BFC0?logo=apple&#34; alt=&#34;macos&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT/wiki/PC-client-installation#portable-version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Portable version-8080ff?logo=portable&#34; alt=&#34;portable&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://t.me/neurogen_news&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Telegram channel-0088CC?logo=telegram&#34; alt=&#34;telegram&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://t.me/neurogen_chat&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Telegram chat-0088CC?logo=telegram&#34; alt=&#34;telegram_chat&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;&lt;br&gt; Support the project: &lt;br&gt; &lt;a href=&#34;https://boosty.to/neurogen&#34;&gt; &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Boosty_logo.svg/512px-Boosty_logo.svg.png?20230209172145&#34; alt=&#34;neurogen_boosty&#34; width=&#34;20%&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;About NeuroGPT&lt;/h2&gt; &#xA;&lt;p&gt;PC app configured to use ChatGPT with &lt;a href=&#34;https://github.com/Em1tSan/NeuroGPT#about-neuroapi&#34;&gt;our API&lt;/a&gt;. Based on &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT&#34;&gt;ChuanhuChatGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Web search&lt;/li&gt; &#xA; &lt;li&gt;Dialog context&lt;/li&gt; &#xA; &lt;li&gt;No-logs&lt;/li&gt; &#xA; &lt;li&gt;Dialog history&lt;/li&gt; &#xA; &lt;li&gt;Setting generation parameters for GPT models&lt;/li&gt; &#xA; &lt;li&gt;Built-in prompt templates and jailbreaks for various tasks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/NealBelov/screenshots/raw/main/demo001.gif?raw=true&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;About NeuroAPI&lt;/h2&gt; &#xA;&lt;p&gt;Core of the project. Reverse API server compatible with OpenAI API format. Freemium. Multiple hundreds of providers. Based on a modified version of &lt;a href=&#34;https://github.com/xtekky/gpt4free&#34;&gt;gpt4free&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-3.5 models&lt;/h3&gt; &#xA;&lt;p&gt;15 requests per minute, 2000 per day.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Endpoint:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://neuroapi.host&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPT-4 models&lt;/h3&gt; &#xA;&lt;p&gt;3 requests per minute, 200 per day.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Endpoint:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://neuroapi.host/gpt4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://star-history.com/#Em1tSan/NeuroGPT&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Em1tSan/NeuroGPT&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>Pythagora-io/gpt-pilot</title>
    <updated>2023-11-01T02:07:38Z</updated>
    <id>tag:github.com,2023-11-01:/Pythagora-io/gpt-pilot</id>
    <link href="https://github.com/Pythagora-io/gpt-pilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dev tool that writes scalable apps from scratch while the developer oversees the implementation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üßë‚Äç‚úàÔ∏è GPT PILOT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/466&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/466&#34; alt=&#34;Pythagora-io%2Fgpt-pilot | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GPT Pilot helps developers build apps 20x faster&lt;/h3&gt; &#xA;&lt;p&gt;You specify what kind of app you want to build. Then, GPT Pilot asks clarifying questions, creates the product and technical requirements, sets up the environment, and &lt;strong&gt;starts coding the app step by step, like in real life, while you oversee the development process&lt;/strong&gt;. It asks you to review each task it finishes or to help when it gets stuck. This way, GPT Pilot acts as a coder while you are a lead dev who reviews code and helps when needed.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-requirements&#34;&gt;üîå Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#how-to-start-using-gpt-pilot&#34;&gt;üö¶How to start using gpt-pilot?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-how-to-start-gpt-pilot-in-docker&#34;&gt;üê≥ How to start gpt-pilot in docker?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#%EF%B8%8F-cli-arguments&#34;&gt;üßë‚ÄçüíªÔ∏è CLI arguments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-examples&#34;&gt;üîé Examples&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-real-time-chat-app&#34;&gt;Real-time chat app&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-markdown-editor&#34;&gt;Markdown editor&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-timer-app&#34;&gt;Timer app&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-main-pillars-of-gpt-pilot&#34;&gt;üèõ Main pillars of GPT Pilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-how-gpt-pilot-works&#34;&gt;üèó How GPT Pilot works?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#hows-gpt-pilot-different-from-smol-developer-and-gpt-engineer&#34;&gt;üï¥How&#39;s GPT Pilot different from &lt;em&gt;Smol developer&lt;/em&gt; and &lt;em&gt;GPT engineer&lt;/em&gt;?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-contributing&#34;&gt;üçª Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-connect-with-us&#34;&gt;üîó Connect with us&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-star-history&#34;&gt;üåü Star history&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;GPT Pilot aims to research how much GPT-4 can be utilized to generate fully working, production-ready apps while the developer oversees the implementation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The main idea is that AI can write most of the code for an app (maybe 95%), but for the rest, 5%, a developer is and will be needed until we get full AGI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve broken down the idea behind GPT Pilot and how it works in the following blog posts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://blog.pythagora.ai/2023/08/23/430/&#34;&gt;[Part 1/3] High-level concepts + GPT Pilot workflow until the coding part&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://blog.pythagora.ai/2023/09/04/gpt-pilot-coding-workflow-part-2-3/&#34;&gt;[Part 2/3] GPT Pilot coding workflow&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Part 3/3] Other important concepts and future plans (COMING UP)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-examples&#34;&gt;üëâ Examples of apps written by GPT Pilot üëà&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/0495631b-511e-451b-93d5-8a42acf22d3d&#34;&gt;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/0495631b-511e-451b-93d5-8a42acf22d3d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîå Requirements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python 3.9-3.11&lt;/strong&gt; (3.12 is currently not working due to a &lt;a href=&#34;https://github.com/psycopg/psycopg2/issues/1628&#34;&gt;dependency issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PostgreSQL&lt;/strong&gt; (optional, projects default is SQLite) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DB is needed for multiple reasons like continuing app development. If you have to stop at any point or the app crashes, go back to a specific step so that you can change some later steps in development, and easier debugging, in future we will add functionality to update project (change some things in existing project or add new features to the project and so on)...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üö¶How to start using gpt-pilot?&lt;/h1&gt; &#xA;&lt;p&gt;After you have Python and (optionally) PostgreSQL installed, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/Pythagora-io/gpt-pilot.git&lt;/code&gt; (clone the repo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd gpt-pilot&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m venv pilot-env&lt;/code&gt; (create a virtual environment)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;source pilot-env/bin/activate&lt;/code&gt; (or on Windows &lt;code&gt;pilot-env\Scripts\activate&lt;/code&gt;) (activate the virtual environment)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; (install the dependencies)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd pilot&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mv .env.example .env&lt;/code&gt; (create the .env file)&lt;/li&gt; &#xA; &lt;li&gt;Add your environment to the &lt;code&gt;.env&lt;/code&gt; file: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LLM Provider (OpenAI/Azure/Openrouter)&lt;/li&gt; &#xA;   &lt;li&gt;your API key&lt;/li&gt; &#xA;   &lt;li&gt;database settings: SQLite/PostgreSQL (to change from SQLite to PostgreSQL, just set &lt;code&gt;DATABASE_TYPE=postgres&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;optionally set IGNORE_FOLDERS for the folders which shouldn&#39;t be tracked by GPT Pilot in workspace, useful to ignore folders created by compilers (i.e. &lt;code&gt;IGNORE_FOLDERS=folder1,folder2,folder3&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python db_init.py&lt;/code&gt; (initialize the database)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python main.py&lt;/code&gt; (start GPT Pilot)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After, this, you can just follow the instructions in the terminal.&lt;/p&gt; &#xA;&lt;p&gt;All generated code will be stored in the folder &lt;code&gt;workspace&lt;/code&gt; inside the folder named after the app name you enter upon starting the pilot.&lt;/p&gt; &#xA;&lt;h2&gt;üê≥ How to start gpt-pilot in docker?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/Pythagora-io/gpt-pilot.git&lt;/code&gt; (clone the repo)&lt;/li&gt; &#xA; &lt;li&gt;Update the &lt;code&gt;docker-compose.yml&lt;/code&gt; environment variables, which can be done via &lt;code&gt;docker compose config&lt;/code&gt; . if you use local model, please go to &lt;a href=&#34;https://localai.io/basics/getting_started/&#34;&gt;https://localai.io/basics/getting_started/&lt;/a&gt; start.&lt;/li&gt; &#xA; &lt;li&gt;By default, GPT Pilot will read &amp;amp; write to &lt;code&gt;~/gpt-pilot-workspace&lt;/code&gt; on your machine, you can also edit this in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;docker compose build&lt;/code&gt;. this will build a gpt-pilot container for you.&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;docker compose up&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;access the web terminal on &lt;code&gt;port 7681&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python db_init.py&lt;/code&gt; (initialize the database)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python main.py&lt;/code&gt; (start GPT Pilot)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This will start two containers, one being a new image built by the &lt;code&gt;Dockerfile&lt;/code&gt; and a Postgres database. The new image also has &lt;a href=&#34;https://github.com/tsl0922/ttyd&#34;&gt;ttyd&lt;/a&gt; installed so that you can easily interact with gpt-pilot. Node is also installed on the image and port 3000 is exposed.&lt;/p&gt; &#xA;&lt;h1&gt;üßë‚ÄçüíªÔ∏è CLI arguments&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;code&gt;app_type&lt;/code&gt; and &lt;code&gt;name&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If not provided, the ProductOwner will ask for these values:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;app_type&lt;/code&gt; is used as a hint to the LLM as to what kind of architecture, language options and conventions would apply. If not provided, &lt;code&gt;prompts.prompts.ask_for_app_type()&lt;/code&gt; will ask for it.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;const.common.APP_TYPES&lt;/code&gt;: &#39;Web App&#39;, &#39;Script&#39;, &#39;Mobile App&#39;, &#39;Chrome Extension&#39;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;app_id&lt;/code&gt; and &lt;code&gt;workspace&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Continue working on an existing app using &lt;strong&gt;&lt;code&gt;app_id&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;or&lt;/em&gt; &lt;strong&gt;&lt;code&gt;workspace&lt;/code&gt;&lt;/strong&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py workspace=&amp;lt;PATH_TO_PROJECT_WORKSPACE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each user can have their own workspace path for each App.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;user_id&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;, and &lt;code&gt;password&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;These values will be saved to the User table in the DB.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py user_id=me_at_work&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If not specified, &lt;code&gt;user_id&lt;/code&gt; defaults to the OS username but can be provided explicitly if your OS username differs from your GitHub or work username. This value is used to load the &lt;code&gt;App&lt;/code&gt; config when the &lt;code&gt;workspace&lt;/code&gt; arg is provided.&lt;/p&gt; &#xA;&lt;p&gt;If not specified &lt;code&gt;email&lt;/code&gt; will be parsed from &lt;code&gt;~/.gitconfig&lt;/code&gt; if the file exists.&lt;/p&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot/discussions/55&#34;&gt;What&#39;s the purpose of arguments.password / User.password?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;advanced&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The Architect, by default, favors certain technologies, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.JS&lt;/li&gt; &#xA; &lt;li&gt;MongoDB&lt;/li&gt; &#xA; &lt;li&gt;PeeWee ORM&lt;/li&gt; &#xA; &lt;li&gt;Jest &amp;amp; PyUnit&lt;/li&gt; &#xA; &lt;li&gt;Bootstrap&lt;/li&gt; &#xA; &lt;li&gt;Vanilla JavaScript&lt;/li&gt; &#xA; &lt;li&gt;Socket.io&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have your own preferences, you can have a deeper conversation with the Architect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py advanced=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;code&gt;step&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Continue working on an existing app from a specific &lt;strong&gt;&lt;code&gt;step&lt;/code&gt;&lt;/strong&gt; (eg: &lt;code&gt;user_tasks&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt; step=&amp;lt;STEP_FROM_CONST_COMMON&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;code&gt;skip_until_dev_step&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Continue working on an existing app from a specific &lt;strong&gt;development step&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt; skip_until_dev_step=&amp;lt;DEV_STEP&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is basically the same as &lt;code&gt;step&lt;/code&gt; but during the development process. If you want to play around with gpt-pilot, this is likely the flag you will often use. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Erase all development steps previously done and continue working on an existing app from the start of development.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt; skip_until_dev_step=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;code&gt;theme&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py theme=light&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/138990495/c3d08f21-7e3b-4ee4-981f-281d1c97149e&#34; alt=&#34;Â±èÂπïÊà™Âõæ 2023-10-15 103907&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py theme=dark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dark mode. &lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/138990495/942cd1c9-b774-498e-b72a-677b01be1ac3&#34; alt=&#34;Â±èÂπïÊà™Âõæ 2023-10-15 104120&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;code&gt;delete_unrelated_steps&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;code&gt;update_files_before_start&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;h1&gt;üîé Examples&lt;/h1&gt; &#xA;&lt;p&gt;Here are a couple of example apps GPT Pilot created by itself:&lt;/p&gt; &#xA;&lt;h3&gt;üì± Real-time chat app&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;A simple chat app with real-time communication&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/bUj9DbMRYhA&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-chat-app-demo&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìù Markdown editor&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;Build a simple markdown editor using HTML, CSS, and JavaScript. Allow users to input markdown text and display the formatted output in real-time.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/uZeA1iX9dgg&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚è±Ô∏è Timer app&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;Create a simple timer app using HTML, CSS, and JavaScript that allows users to set a countdown timer and receive an alert when the time is up.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/CMN3W18zfiE&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-timer-app-demo&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üèõ Main pillars of GPT Pilot:&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For AI to create a fully working app, &lt;strong&gt;a developer needs to be involved&lt;/strong&gt; in the app creation process. They need to be able to change the code at any moment, and GPT Pilot needs to continue working with those changes (e.g., add an API key or fix an issue if an AI gets stuck). &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The app needs to be written step by step as a developer would write it&lt;/strong&gt; - Let&#39;s say you want to create a simple app, know everything you need to code, and have the entire architecture in your head. Even then, you won&#39;t code it out entirely, then run it for the first time and debug all the issues simultaneously. Instead, you will implement something simple, like add routes, run it, see how it works, and then move on to the next task. This way, you can debug issues as they arise. The same should be the case when AI codes. It will make mistakes for sure, so in order for it to have an easier time debugging issues and for the developer to understand what is happening, the AI shouldn&#39;t just spit out the entire codebase at once. Instead, the app should be developed step by step just like a developer would code it - e.g. setup routes, add database connection, etc. &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The approach needs to be scalable&lt;/strong&gt; so that AI can create a production-ready app: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Context rewinding&lt;/strong&gt; - for solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50. Because of this, the conversation needs to be rewound to the first message upon each task. &lt;a href=&#34;https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714&#34;&gt;See the diagram here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Recursive conversations&lt;/strong&gt; are LLM conversations set up to be used ‚Äúrecursively‚Äù. For example, if GPT Pilot detects an error, it needs to debug it, but let‚Äôs say that another error happens during the debugging process. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;TDD (Test Driven Development)&lt;/strong&gt; - for GPT Pilot to be able to scale the codebase, it will need to be able to create new code without breaking previously written code. There is no better way to do this than working with TDD methodology. For each code that GPT Pilot writes, it needs to write tests that check if the code works as intended so that all previous tests can be run whenever new changes are made.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The idea is that AI won&#39;t be able to (at least in the near future) create apps from scratch without the developer being involved. That&#39;s why we created an interactive tool that generates code but also requires the developer to check each step so that they can understand what&#39;s going on and so that the AI can have a better overview of the entire codebase.&lt;/p&gt; &#xA;&lt;p&gt;Obviously, it still can&#39;t create any production-ready app but the general concept of how this could work is there.&lt;/p&gt; &#xA;&lt;h1&gt;üèó How GPT Pilot works?&lt;/h1&gt; &#xA;&lt;p&gt;Here are the steps GPT Pilot takes to create an app:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/d89ba1d4-1208-4b7f-b3d4-76e3ccea584e&#34; alt=&#34;GPT Pilot workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You enter the app name and the description.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product Owner agent&lt;/strong&gt; asks a couple of questions to understand the requirements better.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product Owner agent&lt;/strong&gt; writes user stories and asks you if they are all correct (this helps it create code later on).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Architect agent&lt;/strong&gt; writes up technologies that will be used for the app.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DevOps agent&lt;/strong&gt; checks if all technologies are installed on the machine and installs them if not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tech Lead agent&lt;/strong&gt; writes up development tasks that the Developer must implement. This is an important part because, for each step, the Tech Lead needs to specify how the user (real-world developer) can review if the task is done (e.g. open localhost:3000 and do something).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Developer agent&lt;/strong&gt; takes each task and writes up what needs to be done to implement it. The description is in human-readable form.&lt;/li&gt; &#xA; &lt;li&gt;Finally, &lt;strong&gt;Code Monkey agent&lt;/strong&gt; takes the Developer&#39;s description and the existing file and implements the changes. We realized this works much better than giving it to the Developer right away to implement changes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more details on the roles of agents employed by GPT Pilot, please take a look at &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot/raw/main/pilot/helpers/agents/AGENTS.md&#34;&gt;AGENTS.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/53ea246c-cefe-401c-8ba0-8e4dd49c987b&#34; alt=&#34;GPT Pilot Coding Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üï¥How&#39;s GPT Pilot different from &lt;em&gt;Smol developer&lt;/em&gt; and &lt;em&gt;GPT engineer&lt;/em&gt;?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPT Pilot works with the developer to create a fully working production-ready app&lt;/strong&gt; - I don&#39;t think AI can (at least in the near future) create apps without a developer being involved. So, &lt;strong&gt;GPT Pilot codes the app step by step&lt;/strong&gt; just like a developer would in real life. This way, it can debug issues as they arise throughout the development process. If it gets stuck, you, the developer in charge, can review the code and fix the issue. Other similar tools give you the entire codebase at once - this way, bugs are much harder to fix for AI and for you as a developer. &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Works at scale&lt;/strong&gt; - GPT Pilot isn&#39;t meant to create simple apps but rather so it can work at any scale. It has mechanisms that filter out the code, so in each LLM conversation, it doesn&#39;t need to store the entire codebase in context, but it shows the LLM only the relevant code for the current task it&#39;s working on. Once an app is finished, you can continue working on it by writing instructions on what feature you want to add.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üçª Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in contributing to GPT Pilot, I would be more than happy to have you on board and also help you get started. Feel free to ping &lt;a href=&#34;mailto:zvonimir@pythagora.ai&#34;&gt;zvonimir@pythagora.ai&lt;/a&gt;, and I&#39;ll help you get started.&lt;/p&gt; &#xA;&lt;h2&gt;üî¨Ô∏è Research&lt;/h2&gt; &#xA;&lt;p&gt;Since this is a research project, there are many areas that need to be researched on both practical and theoretical levels. We&#39;re happy to hear how the entire GPT Pilot concept can be improved. For example, maybe it would work better if we structured functional requirements differently, or maybe technical requirements need to be specified in a different way.&lt;/p&gt; &#xA;&lt;h2&gt;üñ• Development&lt;/h2&gt; &#xA;&lt;p&gt;Other than the research, GPT Pilot needs to be debugged to work in different scenarios. For example, we realized that the quality of the code generated is very sensitive to the size of the development task. When the task is too broad, the code has too many bugs that are hard to fix, but when the development task is too narrow, GPT also seems to struggle in getting the task implemented into the existing code.&lt;/p&gt; &#xA;&lt;h1&gt;üîó Connect with us&lt;/h1&gt; &#xA;&lt;p&gt;üåü As an open-source tool, it would mean the world to us if you starred the GPT-pilot repo üåü&lt;/p&gt; &#xA;&lt;p&gt;üí¨ Join &lt;a href=&#34;https://discord.gg/HaqXugmxr9&#34;&gt;the Discord server&lt;/a&gt; to get in touch.&lt;/p&gt; &#xA;&lt;h1&gt;üåü Star History&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Pythagora-io/gpt-pilot&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Pythagora-io/gpt-pilot&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>