<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-01T02:02:48Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ocrmypdf/OCRmyPDF</title>
    <updated>2025-04-01T02:02:48Z</updated>
    <id>tag:github.com,2025-04-01:/ocrmypdf/OCRmyPDF</id>
    <link href="https://github.com/ocrmypdf/OCRmyPDF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ocrmypdf/OCRmyPDF/main/docs/images/logo.svg?sanitize=true&#34; width=&#34;240&#34; alt=&#34;OCRmyPDF&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ocrmypdf/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ocrmypdf.svg?sanitize=true&#34; alt=&#34;PyPI version&#34; title=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/homebrew/v/ocrmypdf.svg?sanitize=true&#34; alt=&#34;Homebrew version&#34; title=&#34;Homebrew version&#34;&gt; &lt;img src=&#34;https://readthedocs.org/projects/ocrmypdf/badge/?version=latest&#34; alt=&#34;ReadTheDocs&#34; title=&#34;RTD&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/ocrmypdf&#34; alt=&#34;Python versions&#34; title=&#34;Supported Python versions&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ocrmypdf                      # it&#39;s a scriptable command line program&#xA;   -l eng+fra                 # it supports multiple languages&#xA;   --rotate-pages             # it can fix pages that are misrotated&#xA;   --deskew                   # it can deskew crooked PDFs!&#xA;   --title &#34;My PDF&#34;           # it can change output metadata&#xA;   --jobs 4                   # it uses multiple cores by default&#xA;   --output-type pdfa         # it produces PDF/A by default&#xA;   input_scanned.pdf          # takes PDF input (or images)&#xA;   output_searchable.pdf      # produces validated PDF output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ocrmypdf.readthedocs.io/en/latest/release_notes.html&#34;&gt;See the release notes for details on the latest changes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generates a searchable &lt;a href=&#34;https://en.wikipedia.org/?title=PDF/A&#34;&gt;PDF/A&lt;/a&gt; file from a regular PDF&lt;/li&gt; &#xA; &lt;li&gt;Places OCR text accurately below the image to ease copy / paste&lt;/li&gt; &#xA; &lt;li&gt;Keeps the exact resolution of the original embedded images&lt;/li&gt; &#xA; &lt;li&gt;When possible, inserts OCR information as a &#34;lossless&#34; operation without disrupting any other content&lt;/li&gt; &#xA; &lt;li&gt;Optimizes PDF images, often producing files smaller than the input file&lt;/li&gt; &#xA; &lt;li&gt;If requested, deskews and/or cleans the image before performing OCR&lt;/li&gt; &#xA; &lt;li&gt;Validates input and output files&lt;/li&gt; &#xA; &lt;li&gt;Distributes work across all available CPU cores&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract&#34;&gt;Tesseract OCR&lt;/a&gt; engine to recognize more than &lt;a href=&#34;https://github.com/tesseract-ocr/tessdata&#34;&gt;100 languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Keeps your private data private.&lt;/li&gt; &#xA; &lt;li&gt;Scales properly to handle files with thousands of pages.&lt;/li&gt; &#xA; &lt;li&gt;Battle-tested on millions of PDFs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ocrmypdf/OCRmyPDF/main/misc/screencast/demo.svg?sanitize=true&#34; alt=&#34;Demo of OCRmyPDF in a terminal session&#34;&gt; &#xA;&lt;p&gt;For details: please consult the &lt;a href=&#34;https://ocrmypdf.readthedocs.io/en/latest/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;I searched the web for a free command line tool to OCR PDF files: I found many, but none of them were really satisfying:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Either they produced PDF files with misplaced text under the image (making copy/paste impossible)&lt;/li&gt; &#xA; &lt;li&gt;Or they did not handle accents and multilingual characters&lt;/li&gt; &#xA; &lt;li&gt;Or they changed the resolution of the embedded images&lt;/li&gt; &#xA; &lt;li&gt;Or they generated ridiculously large PDF files&lt;/li&gt; &#xA; &lt;li&gt;Or they crashed when trying to OCR&lt;/li&gt; &#xA; &lt;li&gt;Or they did not produce valid PDF files&lt;/li&gt; &#xA; &lt;li&gt;On top of that none of them produced PDF/A files (format dedicated for long time storage)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;...so I decided to develop my own tool.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Linux, Windows, macOS and FreeBSD are supported. Docker images are also available, for both x64 and ARM.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating system&lt;/th&gt; &#xA;   &lt;th&gt;Install command&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Debian, Ubuntu&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;apt install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Subsystem for Linux&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;apt install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fedora&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;dnf install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS (Homebrew)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;brew install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS (MacPorts)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;port install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS (nix)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nix-env -i ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LinuxBrew&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;brew install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FreeBSD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pkg install py-ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ubuntu Snap&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;snap install ocrmypdf&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For everyone else, &lt;a href=&#34;https://ocrmypdf.readthedocs.io/en/latest/installation.html&#34;&gt;see our documentation&lt;/a&gt; for installation steps.&lt;/p&gt; &#xA;&lt;h2&gt;Languages&lt;/h2&gt; &#xA;&lt;p&gt;OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Display a list of all Tesseract language packs&#xA;apt-cache search tesseract-ocr&#xA;&#xA;# Debian/Ubuntu users&#xA;apt-get install tesseract-ocr-chi-sim  # Example: Install Chinese Simplified language pack&#xA;&#xA;# Arch Linux users&#xA;pacman -S tesseract-data-eng tesseract-data-deu # Example: Install the English and German language packs&#xA;&#xA;# brew macOS users&#xA;brew install tesseract-lang&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then pass the &lt;code&gt;-l LANG&lt;/code&gt; argument to OCRmyPDF to give a hint as to what languages it should search for. Multiple languages can be requested.&lt;/p&gt; &#xA;&lt;p&gt;OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the &lt;code&gt;PATH&lt;/code&gt; environment variable. On Windows, if &lt;code&gt;PATH&lt;/code&gt; does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and support&lt;/h2&gt; &#xA;&lt;p&gt;Once OCRmyPDF is installed, the built-in help which explains the command syntax and options can be accessed via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ocrmypdf --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://ocrmypdf.readthedocs.io/en/latest/index.html&#34;&gt;documentation is served on Read the Docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report issues on our &lt;a href=&#34;https://github.com/ocrmypdf/OCRmyPDF/issues&#34;&gt;GitHub issues&lt;/a&gt; page, and follow the issue template for quick response.&lt;/p&gt; &#xA;&lt;h2&gt;Feature demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add an OCR layer and convert to PDF/A&#xA;ocrmypdf input.pdf output.pdf&#xA;&#xA;# Convert an image to single page PDF&#xA;ocrmypdf input.jpg output.pdf&#xA;&#xA;# Add OCR to a file in place (only modifies file on success)&#xA;ocrmypdf myfile.pdf myfile.pdf&#xA;&#xA;# OCR with non-English languages (look up your language&#39;s ISO 639-3 code)&#xA;ocrmypdf -l fra LeParisien.pdf LeParisien.pdf&#xA;&#xA;# OCR multilingual documents&#xA;ocrmypdf -l eng+fra Bilingual-English-French.pdf Bilingual-English-French.pdf&#xA;&#xA;# Deskew (straighten crooked pages)&#xA;ocrmypdf --deskew input.pdf output.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more features, see the &lt;a href=&#34;https://ocrmypdf.readthedocs.io/en/latest/index.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD.&lt;/p&gt; &#xA;&lt;h2&gt;Press &amp;amp; Media&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@ikirichenko/going-paperless-with-ocrmypdf-e2f36143f46a&#34;&gt;Going paperless with OCRmyPDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@treyharris/converting-a-scanned-document-into-a-compressed-searchable-pdf-with-redactions-63f61c34fe4c&#34;&gt;Converting a scanned document into a compressed searchable PDF with redactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://heise.de/-2279695&#34;&gt;c&#39;t 1-2014, page 59&lt;/a&gt;: Detailed presentation of OCRmyPDF v1.0 in the leading German IT magazine c&#39;t&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://heise.de/-2356670&#34;&gt;heise Open Source, 09/2014: Texterkennung mit OCRmyPDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.heise.de/ratgeber/Durchsuchbare-PDF-Dokumente-mit-OCRmyPDF-erstellen-4607592.html&#34;&gt;heise Durchsuchbare PDF-Dokumente mit OCRmyPDF erstellen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linuxlinks.com/excellent-utilities-ocrmypdf-add-ocr-text-layer-scanned-pdfs/&#34;&gt;Excellent Utilities: OCRmyPDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linux-community.de/ausgaben/linuxuser/2021/06/texterkennung-mit-ocrmypdf-und-scanbd-automatisieren/&#34;&gt;LinuxUser Texterkennung mit OCRmyPDF und Scanbd automatisieren&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=32028752&#34;&gt;Y Combinator discussion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Business enquiries&lt;/h2&gt; &#xA;&lt;p&gt;OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The OCRmyPDF software is licensed under the Mozilla Public License 2.0 (MPL-2.0). This license permits integration of OCRmyPDF with other code, included commercial and closed source, but asks you to publish source-level modifications you make to OCRmyPDF.&lt;/p&gt; &#xA;&lt;p&gt;Some components of OCRmyPDF have other licenses, as indicated by standard SPDX license identifiers or the DEP5 copyright and licensing information file. Generally speaking, non-core code is licensed under MIT, and the documentation and test files are licensed under Creative Commons ShareAlike 4.0 (CC-BY-SA 4.0).&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The software is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/lerobot</title>
    <updated>2025-04-01T02:02:48Z</updated>
    <id>tag:github.com,2025-04-01:/huggingface/lerobot</id>
    <link href="https://github.com/huggingface/lerobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;img alt=&#34;LeRobot, Hugging Face Robotics Library&#34; src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/huggingface/lerobot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lerobot&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/lerobot&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lerobot&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/tree/main/examples&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Examples-green.svg?sanitize=true&#34; alt=&#34;Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/s3KuuzsPFb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&#34;&gt; Build Your Own SO-100 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so100/leader_follower.webp?raw=true&#34; alt=&#34;SO-100 leader and follower arms&#34; title=&#34;SO-100 leader and follower arms&#34; width=&#34;50%&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Meet the SO-100 â€“ Just $110 per arm!&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; &#xA; &lt;p&gt;Then sit back and watch your creation act autonomously! ðŸ¤¯&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&#34;&gt; Get the full SO-100 tutorial here.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Want to take it to the next level? Make your SO-100 mobile by building LeKiwi!&lt;/p&gt; &#xA; &lt;p&gt;Check out the &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/11_use_lekiwi.md&#34;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp?raw=true&#34; alt=&#34;LeKiwi mobile robot&#34; title=&#34;LeKiwi mobile robot&#34; width=&#34;50%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif&#34; width=&#34;100%&#34; alt=&#34;ACT policy on ALOHA env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif&#34; width=&#34;100%&#34; alt=&#34;TDMPC policy on SimXArm env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif&#34; width=&#34;100%&#34; alt=&#34;Diffusion policy on PushT env&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACT policy on ALOHA env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TDMPC policy on SimXArm env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Diffusion policy on PushT env&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Acknowledgment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ALOHA&lt;/a&gt; and &lt;a href=&#34;https://mobile-aloha.github.io&#34;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&#34;https://umi-gripper.github.io&#34;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&#34;https://github.com/nicklashansen/tdmpc&#34;&gt;TDMPC&lt;/a&gt; and &lt;a href=&#34;https://www.yunhaifeng.com/FOWM&#34;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://sjlee.cc/&#34;&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href=&#34;https://mahis.life/&#34;&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href=&#34;https://sjlee.cc/vq-bet/&#34;&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href=&#34;https://github.com/jayLEE0301/vq_bet_official&#34;&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download our source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/lerobot.git&#xA;cd lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n lerobot python=3.10&#xA;conda activate lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using &lt;code&gt;miniconda&lt;/code&gt;, if you don&#39;t have &lt;code&gt;ffmpeg&lt;/code&gt; in your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ðŸ¤— LeRobot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-binary=av -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter build errors, you may need to install additional dependencies (&lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;build-essential&lt;/code&gt;, and &lt;code&gt;ffmpeg libs&lt;/code&gt;). On Linux, run: &lt;code&gt;sudo apt-get install cmake build-essential python-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config&lt;/code&gt;. For other systems, see: &lt;a href=&#34;https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg&#34;&gt;Compiling PyAV&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For simulations, ðŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-aloha&#34;&gt;aloha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-xarm&#34;&gt;xarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-pusht&#34;&gt;pusht&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For instance, to install ðŸ¤— LeRobot with aloha and pusht, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-binary=av -e &#34;.[aloha, pusht]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://docs.wandb.ai/quickstart&#34;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;â”œâ”€â”€ examples             # contains demonstration examples, start here to learn about LeRobot&#xA;|   â””â”€â”€ advanced         # contains even more examples for those who have mastered the basics&#xA;â”œâ”€â”€ lerobot&#xA;|   â”œâ”€â”€ configs          # contains config classes with all options that you can override in the command line&#xA;|   â”œâ”€â”€ common           # contains classes and utilities&#xA;|   |   â”œâ”€â”€ datasets       # various datasets of human demonstrations: aloha, pusht, xarm&#xA;|   |   â”œâ”€â”€ envs           # various sim environments: aloha, pusht, xarm&#xA;|   |   â”œâ”€â”€ policies       # various policies: act, diffusion, tdmpc&#xA;|   |   â”œâ”€â”€ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots&#xA;|   |   â””â”€â”€ utils          # various utilities&#xA;|   â””â”€â”€ scripts          # contains functions to execute via command line&#xA;|       â”œâ”€â”€ eval.py                 # load policy and evaluate it on an environment&#xA;|       â”œâ”€â”€ train.py                # train a policy via imitation learning and/or reinforcement learning&#xA;|       â”œâ”€â”€ control_robot.py        # teleoperate a real robot, record data, run a policy&#xA;|       â”œâ”€â”€ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub&#xA;|       â””â”€â”€ visualize_dataset.py    # load a dataset and render its demonstrations&#xA;â”œâ”€â”€ outputs               # contains results of scripts execution: logs, videos, model checkpoints&#xA;â””â”€â”€ tests                 # contains pytest utilities for continuous integration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize datasets&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--local-files-only&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --root ./my_local_data_dir \&#xA;    --local-files-only 1 \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; &#xA;&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset(&#34;lerobot/aloha_static_coffee&#34;)&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; &#xA;&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {&#34;observation.image&#34;: [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 &#34;previous&#34; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; &#xA;&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset(&#34;lerobot/aloha_static_coffee&#34;)&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset attributes:&#xA;  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:&#xA;  â”‚  â”œ observation.images.cam_high (VideoFrame):&#xA;  â”‚  â”‚   VideoFrame = {&#39;path&#39;: path to a mp4 video, &#39;timestamp&#39; (float32): timestamp in the video}&#xA;  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)&#xA;  â”‚  ... (more observations)&#xA;  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)&#xA;  â”‚  â”œ episode_index (int64): index of the episode for this sample&#xA;  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode&#xA;  â”‚  â”œ timestamp (float32): timestamp in the episode&#xA;  â”‚  â”œ next.done (bool): indicates the end of en episode ; True for the last frame in each episode&#xA;  â”‚  â”” index (int64): general index in the whole dataset&#xA;  â”œ episode_data_index: contains 2 tensors with the start and end indices of each episode&#xA;  â”‚  â”œ from (1D int64 tensor): first frame index for each episode â€” shape (num episodes,) starts with 0&#xA;  â”‚  â”” to: (1D int64 tensor): last frame index for each episode â€” shape (num episodes,)&#xA;  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance&#xA;  â”‚  â”œ observation.images.cam_high: {&#39;max&#39;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}&#xA;  â”‚  ...&#xA;  â”œ info: a dictionary of metadata on the dataset&#xA;  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with&#xA;  â”‚  â”œ fps (float): frame per second the dataset is recorded/synchronized to&#xA;  â”‚  â”œ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files&#xA;  â”‚  â”” encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos&#xA;  â”œ videos_dir (Path): where the mp4 videos or png images are stored/accessed&#xA;  â”” camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&#34;observation.images.cam_high&#34;, ...]`)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; &#xA; &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; &#xA; &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it&#39;s not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&#34;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    --policy.path=lerobot/diffusion_pusht \&#xA;    --env.type=pusht \&#xA;    --eval.batch_size=10 \&#xA;    --eval.n_episodes=10 \&#xA;    --policy.use_amp=false \&#xA;    --policy.device=cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Train your own policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&#34;&gt;example 3&lt;/a&gt; that illustrate how to train a model using our core library in python, and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&#34;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; &#xA;&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding &lt;code&gt;--wandb.enable=true&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md#typical-logs-and-metrics&#34;&gt;here&lt;/a&gt; for the explanation of some commonly used metrics in logs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;--eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; &#xA;&lt;p&gt;We provide some pretrained policies on our &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to ðŸ¤— LeRobot, please check out our &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ### Add a new dataset&#xA;&#xA;To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):&#xA;```bash&#xA;huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential&#xA;```&#xA;&#xA;Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:&#xA;```bash&#xA;python lerobot/scripts/push_dataset_to_hub.py \&#xA;--raw-dir data/aloha_static_pingpong_test_raw \&#xA;--out-dir data \&#xA;--repo-id lerobot/aloha_static_pingpong_test \&#xA;--raw-format aloha_hdf5&#xA;```&#xA;&#xA;See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.&#xA;&#xA;If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt; &#xA;&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&#34;https://huggingface.co/docs/safetensors/index&#34;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameter userd for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. Thisis useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&#34;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; &#xA;&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; &#xA;&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.profiler import profile, record_function, ProfilerActivity&#xA;&#xA;def trace_handler(prof):&#xA;    prof.export_chrome_trace(f&#34;tmp/trace_schedule_{prof.step_num}.json&#34;)&#xA;&#xA;with profile(&#xA;    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],&#xA;    schedule=torch.profiler.schedule(&#xA;        wait=2,&#xA;        warmup=2,&#xA;        active=3,&#xA;    ),&#xA;    on_trace_ready=trace_handler&#xA;) as prof:&#xA;    with record_function(&#34;eval_policy&#34;):&#xA;        for i in range(num_episodes):&#xA;            prof.step()&#xA;            # insert code to profile, potentially whole body of eval_policy function&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{cadene2024lerobot,&#xA;    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},&#xA;    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},&#xA;    howpublished = &#34;\url{https://github.com/huggingface/lerobot}&#34;,&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chi2024diffusionpolicy,&#xA;&#x9;author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},&#xA;&#x9;title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},&#xA;&#x9;journal = {The International Journal of Robotics Research},&#xA;&#x9;year = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ACT or ALOHA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhao2023learning,&#xA;  title={Learning fine-grained bimanual manipulation with low-cost hardware},&#xA;  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},&#xA;  journal={arXiv preprint arXiv:2304.13705},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nicklashansen.com/td-mpc/&#34;&gt;TDMPC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Hansen2022tdmpc,&#xA;&#x9;title={Temporal Difference Learning for Model Predictive Control},&#xA;&#x9;author={Nicklas Hansen and Xiaolong Wang and Hao Su},&#xA;&#x9;booktitle={ICML},&#xA;&#x9;year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sjlee.cc/vq-bet/&#34;&gt;VQ-BeT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{lee2024behavior,&#xA;  title={Behavior generation with latent actions},&#xA;  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},&#xA;  journal={arXiv preprint arXiv:2403.03181},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#huggingface/lerobot&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/DiffSynth-Studio</title>
    <updated>2025-04-01T02:02:48Z</updated>
    <id>tag:github.com,2025-04-01:/modelscope/DiffSynth-Studio</id>
    <link href="https://github.com/modelscope/DiffSynth-Studio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth Studio&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/DiffSynth/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/DiffSynth&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/pull/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio&#34; alt=&#34;GitHub latest commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/10946&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/10946&#34; alt=&#34;modelscope%2FDiffSynth-Studio | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Document: &lt;a href=&#34;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&#34;&gt;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the magic world of Diffusion models!&lt;/p&gt; &#xA;&lt;p&gt;DiffSynth consists of two open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio&#34;&gt;DiffSynth-Studio&lt;/a&gt;: Focused on aggressive technological exploration. Targeted at academia. Provides more cutting-edge technical support and novel inference capabilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Engine&#34;&gt;DiffSynth-Engine&lt;/a&gt;: Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;DiffSynth-Studio is an open-source project aimed at exploring innovations in AIGC technology. We have integrated numerous open-source Diffusion models, including FLUX and Wan, among others. Through this open-source project, we hope to connect models within the open-source community and explore new technologies based on diffusion models.&lt;/p&gt; &#xA;&lt;p&gt;Until now, DiffSynth-Studio has supported the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stepfun-ai/Step-Video-T2V&#34;&gt;StepVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;HunyuanVideo-I2V&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b&#34;&gt;CogVideoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;FLUX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ExVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Kwai-Kolors/Kolors&#34;&gt;Kolors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-3-medium&#34;&gt;Stable Diffusion 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt&#34;&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hzwer/ECCV2022-RIFE&#34;&gt;RIFE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Ip-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 31, 2025&lt;/strong&gt; We support InfiniteYou, an identity preserving method for FLUX. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/InfiniteYou/&#34;&gt;./examples/InfiniteYou/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 25, 2025&lt;/strong&gt; ðŸ”¥ðŸ”¥ðŸ”¥ Our new open-source project, &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Engine&#34;&gt;DiffSynth-Engine&lt;/a&gt;, is now open-sourced! Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 13, 2025&lt;/strong&gt; We support HunyuanVideo-I2V, the image-to-video generation version of HunyuanVideo open-sourced by Tencent. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/&#34;&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/&#34;&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href=&#34;https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary&#34;&gt;StepVideo&lt;/a&gt;! State-of-the-art video synthesis model! See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/&#34;&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/&#34;&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2501.01097&#34;&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/Eligen&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/modelscope/EliGen&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Online Demo: &lt;a href=&#34;https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen&#34;&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Training Dataset: &lt;a href=&#34;https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet&#34;&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/&#34;&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2412.12888&#34;&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Examples: &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Demo: &lt;a href=&#34;https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0&#34;&gt;ModelScope&lt;/a&gt;, HuggingFace (Coming soon)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/&#34;&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024.&lt;/strong&gt; We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;ModelScope&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; CogVideoX-5B is supported in this project. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text to video&lt;/li&gt; &#xA;   &lt;li&gt;Video editing&lt;/li&gt; &#xA;   &lt;li&gt;Self-upscaling&lt;/li&gt; &#xA;   &lt;li&gt;Video interpolation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use it in our &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui&#34;&gt;WebUI&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024.&lt;/strong&gt; FLUX is supported in DiffSynth-Studio.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable CFG and highres-fix to improve visual quality. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LoRA, ControlNet, and additional models will be available soon.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024.&lt;/strong&gt; We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/ExVideoProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Source code is released in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Models are released on &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2406.14130&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can try ExVideo in this &lt;a href=&#34;https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1&#34;&gt;Demo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024.&lt;/strong&gt; DiffSynth Studio is transferred to ModelScope. The developers have transitioned from &#34;I&#34; to &#34;we&#34;. Of course, I will still participate in development and maintenance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jan 29, 2024.&lt;/strong&gt; We propose Diffutoon, a fantastic solution for toon shading.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffutoonProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in this project.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2401.16224&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dec 8, 2023.&lt;/strong&gt; We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nov 15, 2023.&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sd-webui extension is released on &lt;a href=&#34;https://github.com/Artiprocher/sd-webui-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d94y1W7PE&#34;&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Lw411m71p&#34;&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1RB4y1Z7LF&#34;&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2311.09265&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href=&#34;https://github.com/AInseven/ComfyUI-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Oct 1, 2023.&lt;/strong&gt; We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The source codes are released on &lt;a href=&#34;https://github.com/Artiprocher/FastSDXL&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The original repo of OLSS is &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;A demo video is shown on &lt;a href=&#34;https://www.bilibili.com/video/BV1w8411y7uj&#34;&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Since OLSS requires additional training, we don&#39;t implement it in this project.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aug 29, 2023.&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffSynth.github.io/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth&#34;&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install from source code (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git&#xA;cd DiffSynth-Studio&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from pypi (There is a delay in the update. If you want to experience the latest features, please do not use this installation method.):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install diffsynth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter issues during installation, it may be caused by the packages we depend on. Please refer to the documentation of the package that caused the problem.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;torch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org&#34;&gt;cmake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cupy.dev/en/stable/install.html&#34;&gt;cupy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage (in Python code)&lt;/h2&gt; &#xA;&lt;p&gt;The Python examples are in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;. We provide an overview here.&lt;/p&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;Download the pre-set models. Model IDs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/diffsynth/configs/model_config.py&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth import download_models&#xA;&#xA;download_models([&#34;FLUX.1-dev&#34;, &#34;Kolors&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download your own models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth.models.downloader import download_from_huggingface, download_from_modelscope&#xA;&#xA;# From Modelscope (recommended)&#xA;download_from_modelscope(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.bin&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;# From Huggingface&#xA;download_from_huggingface(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.safetensors&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Video Synthesis&lt;/h3&gt; &#xA;&lt;h4&gt;Text-to-video using CogVideoX-5B&lt;/h4&gt; &#xA;&lt;p&gt;CogVideoX-5B is released by ZhiPu. We provide an improved pipeline, supporting text-to-video, video editing, self-upscaling and video interpolation. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;&lt;code&gt;examples/video_synthesis&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video on the left is generated using the original text-to-video pipeline, while the video on the right is the result after editing and frame interpolation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&#34;&gt;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Long Video Synthesis&lt;/h4&gt; &#xA;&lt;p&gt;We trained extended video synthesis models, which can generate 128 frames. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&#34;&gt;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Toon Shading&lt;/h4&gt; &#xA;&lt;p&gt;Render realistic videos in a flatten style and enable video editing features. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/&#34;&gt;&lt;code&gt;examples/Diffutoon&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Video Stylization&lt;/h4&gt; &#xA;&lt;p&gt;Video stylization without video models. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/&#34;&gt;&lt;code&gt;examples/diffsynth&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Generate high-resolution images, by breaking the limitation of diffusion models! &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/&#34;&gt;&lt;code&gt;examples/image_synthesis&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LoRA fine-tuning is supported in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/train/&#34;&gt;&lt;code&gt;examples/train&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;FLUX&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion 3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f&#34; alt=&#34;image_1024_cfg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kolors&lt;/th&gt; &#xA;   &lt;th&gt;Hunyuan-DiT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Stable Diffusion&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion XL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage (in WebUI)&lt;/h2&gt; &#xA;&lt;p&gt;Create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&#34;&gt;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This video is not rendered in real-time.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before launching the WebUI, please download models to the folder &lt;code&gt;./models&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#download-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Gradio&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python apps/gradio/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/59613157-de51-4109-99b3-97cbffd88076&#34; alt=&#34;20240822102002&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Streamlit&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install streamlit streamlit-drawable-canvas&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m streamlit run apps/streamlit/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>