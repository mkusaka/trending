<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-01T02:11:34Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/llama</title>
    <updated>2023-08-01T02:11:34Z</updated>
    <id>tag:github.com,2023-08-01:/facebookresearch/llama</id>
    <link href="https://github.com/facebookresearch/llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for LLaMA models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama 2&lt;/h1&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly.&lt;/p&gt; &#xA;&lt;p&gt;This release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 70B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&#34;&gt;Llama 2&lt;/a&gt; models and run inference. For more detailed examples leveraging HuggingFace, see &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/&#34;&gt;llama-recipes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;7/18: We&#39;re aware of people encountering a number of download issues today. Anyone still encountering issues should remove all local files, re-clone the repository, and &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;request a new download link&lt;/a&gt;. It&#39;s critical to do all of these in case you have local corrupt files. When you receive the email, copy &lt;em&gt;only&lt;/em&gt; the link text - it should begin with &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt; and not with &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, which will give errors.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizer, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta AI website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, &lt;strong&gt;do not use the &#39;Copy link address&#39; option&lt;/strong&gt; when you right click the URL. If the copied URL text starts with: &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt;, you copied it correctly. If the copied URL text starts with: &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, you copied it the wrong way.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then to run the script: &lt;code&gt;./download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h3&gt;Access on Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;We are also providing downloads on &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Hugging Face&lt;/a&gt;. You must first request a download from the Meta AI website using the same email address as your Hugging Face account. After doing so, you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda env with PyTorch / CUDA available, clone the repo and run in the top-level directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models support sequence length up to 4096 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;These models are not finetuned for chat or Q&amp;amp;A. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_text_completion.py&lt;/code&gt; for some examples. To illustrate, see command below to run it with the llama-2-7b model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_text_completion.py \&#xA;    --ckpt_dir llama-2-7b/ \&#xA;    --tokenizer_path tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuned Chat Models&lt;/h3&gt; &#xA;&lt;p&gt;The fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/llama/generation.py#L212&#34;&gt;&lt;code&gt;chat_completion&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and breaklines in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces).&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/inference/inference.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using llama-2-7b-chat:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_chat_completion.py \&#xA;    --ckpt_dir llama-2-7b-chat/ \&#xA;    --tokenizer_path tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research paper as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software “bug,” or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/llama&#34;&gt;github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&#34;&gt;Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama&#34;&gt;Llama 2 technical overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/llama/open-innovation-ai-research-community/&#34;&gt;Open Innovation AI Research Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Original LLaMA&lt;/h2&gt; &#xA;&lt;p&gt;The repo for the original llama release is in the &lt;a href=&#34;https://github.com/facebookresearch/llama/tree/llama_v1&#34;&gt;&lt;code&gt;llama_v1&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hiyouga/ChatGLM-Efficient-Tuning</title>
    <updated>2023-08-01T02:11:34Z</updated>
    <id>tag:github.com,2023-08-01:/hiyouga/ChatGLM-Efficient-Tuning</id>
    <link href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效 ChatGLM 微调&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM Efficient Tuning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/glmtuner/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/glmtuner&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning 🤖&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; model with 🤗&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;👋 Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/README_zh.md&#34;&gt;中文&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[23/07/15] Now we develop an all-in-one Web UI for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune ChatGLM-6B model in your Web browser. Thank &lt;a href=&#34;https://github.com/KanadeSiina&#34;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&#34;https://github.com/codemayq&#34;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; &#xA;&lt;p&gt;[23/07/09] Now we release &lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt;⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/25] Now we align the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/25] Now we support fine-tuning the &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2-6B&lt;/a&gt; model with our framework!&lt;/p&gt; &#xA;&lt;p&gt;[23/06/05] Now we support 4-bit LoRA training (aka &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;). Try &lt;code&gt;--quantization_bit 4&lt;/code&gt; argument to work with 4-bit quantized model. (experimental feature)&lt;/p&gt; &#xA;&lt;p&gt;[23/06/01] We implemented a framework supporting the efficient tuning of LLaMA and BLOOM models. Please follow &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Efficient-Tuning&#34;&gt;LLaMA-Efficient-Tuning&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA;&lt;p&gt;[23/05/19] Now we support using the development set to evaluate the model while training. Try &lt;code&gt;--dev_ratio&lt;/code&gt; argument to specify the size of development set.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/29] Now we support training ChatGLM with &lt;strong&gt;Reinforcement Learning with Human Feedback (RLHF)&lt;/strong&gt; ! We provide several examples to run RLHF training, please refer to the &lt;code&gt;examples&lt;/code&gt; folder for details.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/20] Our repo achieved 100 stars within 12 days! Congratulations!&lt;/p&gt; &#xA;&lt;p&gt;[23/04/19] Now we support &lt;strong&gt;merging the weights&lt;/strong&gt; of fine-tuned models trained by LoRA! Try &lt;code&gt;--checkpoint_dir checkpoint1,checkpoint2&lt;/code&gt; argument for continually fine-tuning the models.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/18] Now we support training the &lt;strong&gt;quantized models&lt;/strong&gt; using three fine-tuning methods! Try &lt;code&gt;quantization_bit&lt;/code&gt; argument for training the model in 4/8 bits.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/12] Now we support &lt;strong&gt;training from checkpoints&lt;/strong&gt;! Use &lt;code&gt;--checkpoint_dir&lt;/code&gt; argument to specify the checkpoint model to fine-tune from.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/11] Now we support training with &lt;strong&gt;combined datasets&lt;/strong&gt;! Try &lt;code&gt;--dataset dataset1,dataset2&lt;/code&gt; argument for training with multiple datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For supervised fine-tuning: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;Open Assistant (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/data/self_cognition.json&#34;&gt;Self-cognition (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection&#34;&gt;ShareGPT (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/sufengniu/RefGPT&#34;&gt;RefGPT (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/GAIR/lima&#34;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/zxbsmk/webnovel_cn&#34;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For reward modelling: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;Open Assistant (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-Tuning Methods&lt;/h2&gt; &#xA;&lt;p&gt;Our script now supports the following fine-tuning methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the low-rank adapters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning V2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the prefix encoder of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.14913&#34;&gt;Freeze&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the MLPs in the last n blocks of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Full Tuning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning all the parameters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+ and PyTorch 1.13.1+&lt;/li&gt; &#xA; &lt;li&gt;🤗Transformers, Datasets, Accelerate, PEFT and TRL&lt;/li&gt; &#xA; &lt;li&gt;fire, protobuf, cpm-kernels and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;jieba, rouge-chinese and nltk (used at evaluation)&lt;/li&gt; &#xA; &lt;li&gt;gradio and matplotlib (used in train_web.py)&lt;/li&gt; &#xA; &lt;li&gt;uvicorn, fastapi and sse-starlette (used in api_demo.py)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And &lt;strong&gt;powerful GPUs&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;data/example_dataset&lt;/code&gt; for checking the details about the format of dataset files. You can either use a single &lt;code&gt;.json&lt;/code&gt; file or a &lt;a href=&#34;https://huggingface.co/docs/datasets/dataset_script&#34;&gt;dataset loading script&lt;/a&gt; with multiple files to create a custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note: please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset. About the format of this file, please refer to &lt;code&gt;data/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dependence Installation (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git&#xA;conda create -n chatglm_etuning python=3.10&#xA;conda activate chatglm_etuning&#xA;cd ChatGLM-Efficient-Tuning&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you will be required to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;All-in-one Web UI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/train_web.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with a Single GPU&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \&#xA;    --stage sft \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_sft_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki&#34;&gt;Wiki&lt;/a&gt; about the details of the arguments.&lt;/p&gt; &#xA;&lt;h3&gt;Distributed Fine-tuning with Multiple GPUs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config # configure the environment&#xA;accelerate launch src/train_bash.py # arguments (same as above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training Reward Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \&#xA;    --stage rm \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --do_train \&#xA;    --dataset comparison_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --resume_lora_training False \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --output_dir path_to_rm_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with RLHF&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \&#xA;    --stage ppo \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --resume_lora_training False \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --reward_model path_to_rm_checkpoint \&#xA;    --output_dir path_to_ppo_checkpoint \&#xA;    --per_device_train_batch_size 2 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --plot_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation (BLEU and ROUGE_CHINESE)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \&#xA;    --stage sft \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --do_eval \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_eval_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Predict&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \&#xA;    --stage sft \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --do_predict \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_predict_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 100 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to predict the samples with empty responses, please kindly fill the &lt;code&gt;response&lt;/code&gt; column with &lt;strong&gt;dummy tokens&lt;/strong&gt; to ensure the sample will not be discarded throughout the preprocessing phase.&lt;/p&gt; &#xA;&lt;h3&gt;API Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/api_demo.py \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit &lt;code&gt;http://localhost:8000/docs&lt;/code&gt; for API documentation.&lt;/p&gt; &#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/cli_demo.py \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/web_demo.py \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/export_model.py \&#xA;    --model_name_or_path path_to_your_chatglm_model \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_export&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Fine-tune method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;28GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze (l=3)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RM method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;22GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;11GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RLHF method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;23GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: &lt;code&gt;r&lt;/code&gt; is the lora rank, &lt;code&gt;p&lt;/code&gt; is the number of prefix tokens, &lt;code&gt;l&lt;/code&gt; is the number of trainable layers, &lt;code&gt;ex/s&lt;/code&gt; is the examples per second at training. The &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; is set to &lt;code&gt;1&lt;/code&gt;. All are evaluated on a single Tesla V100 (32G) GPU, they are approximated values and may vary in different GPUs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Fine-tuning ChatGLM: A Case&lt;/h2&gt; &#xA;&lt;h3&gt;Training Results&lt;/h3&gt; &#xA;&lt;p&gt;We use the whole &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to fine-tune the ChatGLM model with LoRA (r=8) for one epoch, using the default hyper-parameters. The loss curve during training is presented below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/trainer_state.jpg&#34; alt=&#34;training loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation Results&lt;/h3&gt; &#xA;&lt;p&gt;We select 100 instances in the &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to evaluate the fine-tuned ChatGLM model and compute the BLEU and ROUGE scores. The results are presented below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;   &lt;th&gt;Original&lt;/th&gt; &#xA;   &lt;th&gt;FZ (l=2)&lt;/th&gt; &#xA;   &lt;th&gt;PT (p=16)&lt;/th&gt; &#xA;   &lt;th&gt;LoRA (r=8)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLEU-4&lt;/td&gt; &#xA;   &lt;td&gt;15.75&lt;/td&gt; &#xA;   &lt;td&gt;16.85&lt;/td&gt; &#xA;   &lt;td&gt;16.06&lt;/td&gt; &#xA;   &lt;td&gt;17.01 (&lt;strong&gt;+1.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-1&lt;/td&gt; &#xA;   &lt;td&gt;34.51&lt;/td&gt; &#xA;   &lt;td&gt;36.62&lt;/td&gt; &#xA;   &lt;td&gt;34.80&lt;/td&gt; &#xA;   &lt;td&gt;36.77 (&lt;strong&gt;+2.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-2&lt;/td&gt; &#xA;   &lt;td&gt;15.11&lt;/td&gt; &#xA;   &lt;td&gt;17.04&lt;/td&gt; &#xA;   &lt;td&gt;15.32&lt;/td&gt; &#xA;   &lt;td&gt;16.83 (&lt;strong&gt;+1.72&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-l&lt;/td&gt; &#xA;   &lt;td&gt;26.18&lt;/td&gt; &#xA;   &lt;td&gt;28.17&lt;/td&gt; &#xA;   &lt;td&gt;26.35&lt;/td&gt; &#xA;   &lt;td&gt;28.86 (&lt;strong&gt;+2.68&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Params (%)&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;4.35%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;FZ: freeze tuning, PT: P-Tuning V2 (we use &lt;code&gt;pre_seq_len=16&lt;/code&gt; for fair comparison with LoRA), Params: the percentange of trainable parameters.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SupritYoung/RLHF-Label-Tool/tree/master&#34;&gt;SupritYoung/RLHF-Label-Tool&lt;/a&gt;: A tool for ranking the responses of LLMs to generate annotated samples used in RLHF training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compared with Existing Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning&#34;&gt;THUDM/ChatGLM-6B&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Official implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning v2&lt;/a&gt; on the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;ADGEN&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is largely depend on it. We further implement the &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; tuning method. Additionally, we &lt;strong&gt;dynamically&lt;/strong&gt; pad the inputs to the longest sequence in the batch instead of the maximum length, to accelerate the fine-tuning.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;mymusise/ChatGLM-Tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unoffical implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We borrowed some ideas from it. Our fine-tuning script &lt;strong&gt;integrates&lt;/strong&gt; the data pre-processing part into the training procedure, so we need not generate a pre-processed dataset before training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/chatglm_finetuning&#34;&gt;ssbuild/chatglm_finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several PEFT methods on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is implemented &lt;strong&gt;purely&lt;/strong&gt; with &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face transformers&lt;/a&gt; and is independent of the &lt;a href=&#34;https://github.com/ssbuild/deep_training&#34;&gt;deep_training&lt;/a&gt; framework.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lich99/ChatGLM-finetune-LoRA&#34;&gt;lich99/ChatGLM-finetune-LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We use the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;Hugging Face PEFT&lt;/a&gt; to provide the state-of-the-art PEFT methods.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning&#34;&gt;liucongg/ChatGLM-Finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several methods including Freeze, LoRA and P-Tuning on the industrial dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We are aim to incorporate more instruction-following datasets for fine-tuning the ChatGLM model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yanqiangmiffy/InstructGLM&#34;&gt;yanqiangmiffy/InstructGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM that explores the ChatGLM&#39;s ability on the instruction-following datasets.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script integrates the data pre-processing part in to the training procedure.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Employing &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; to easily build applications that are capable of leveraging external knowledge upon fine-tuned ChatGLM models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implementing the alignment algorithms to align human preferrences. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat&#34;&gt;RLHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/GanjinZero/RRHF&#34;&gt;RRHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;RAFT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;Chinese datasets&lt;/a&gt; into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/pCLUE&#34;&gt;pCLUE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;CLUECorpus&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;GuanacoDataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;FireflyDataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4&lt;/a&gt; self-chat data into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/project-baize/baize-chatbot&#34;&gt;Baize&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4-LLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implementing the Freeze-Tuning and P-Tuning method.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supporting Multi-GPUs fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Adding script for evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Loading from checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fine-tuning the quantized model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Writing a guidebook about how to fine-tune ChatGLM with this framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Combining with state-of-the-art model editing algorithms. (&lt;em&gt;e.g. &lt;a href=&#34;https://arxiv.org/abs/2110.11309&#34;&gt;MEND&lt;/a&gt;&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Incorporating the &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant Conversations Dataset&lt;/a&gt; for SFT and alignment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating the high quality Chinese instruction dataset &lt;a href=&#34;https://huggingface.co/datasets/BAAI/COIG&#34;&gt;COIG&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;. Please follow the &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/raw/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt; to use ChatGLM-6B model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{chatglm-efficient-tuning,&#xA;  title = {ChatGLM Efficient Tuning},&#xA;  author = {hiyouga},&#xA;  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;, &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuanzhoulvpi2017/zero_nlp&#34;&gt;yuanzhoulvpi2017/zero_nlp&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>paul-gauthier/aider</title>
    <updated>2023-08-01T02:11:34Z</updated>
    <id>tag:github.com,2023-08-01:/paul-gauthier/aider</id>
    <link href="https://github.com/paul-gauthier/aider" rel="alternate"></link>
    <summary type="html">&lt;p&gt;aider is GPT powered coding in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aider is GPT powered coding in your terminal&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;aider&lt;/code&gt; is a command-line chat tool that allows you to write and edit code with OpenAI&#39;s GPT models. You can ask GPT to help you start a new project, or modify code in your existing git repo. Aider makes it easy to &lt;a href=&#34;https://aider.chat/docs/faq.html#how-does-aider-use-git&#34;&gt;git commit, diff &amp;amp; undo changes&lt;/a&gt; proposed by GPT without copy/pasting. It also has features that &lt;a href=&#34;https://aider.chat/docs/ctags.html&#34;&gt;help GPT-4 understand and modify larger codebases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/assets/screencast.svg?sanitize=true&#34; alt=&#34;aider screencast&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/Tv2uQnR88V&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Join-Discord-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#getting-started&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#example-chat-transcripts&#34;&gt;Example chat transcripts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#in-chat-commands&#34;&gt;In-chat commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#tips&#34;&gt;Tips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/faq.html#gpt-4-vs-gpt-35&#34;&gt;GPT-4 vs GPT-3.5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/faq.html&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;installation instructions&lt;/a&gt; for more details, but you can get started quickly like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install aider-chat&#xA;$ export OPENAI_API_KEY=your-key-goes-here&#xA;$ aider hello.js&#xA;&#xA;Using git repo: .git&#xA;Added hello.js to the chat.&#xA;&#xA;hello.js&amp;gt; write a js app that prints hello world&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example chat transcripts&lt;/h2&gt; &#xA;&lt;p&gt;Here are some example transcripts that show how you can chat with &lt;code&gt;aider&lt;/code&gt; to write and edit code with GPT-4.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/hello-world-flask.html&#34;&gt;&lt;strong&gt;Hello World Flask App&lt;/strong&gt;&lt;/a&gt;: Start from scratch and have GPT create a simple Flask app with various endpoints, such as adding two numbers and calculating the Fibonacci sequence.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/2048-game.html&#34;&gt;&lt;strong&gt;Javascript Game Modification&lt;/strong&gt;&lt;/a&gt;: Dive into an existing open-source repo, and get GPT&#39;s help to understand it and make modifications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/complex-change.html&#34;&gt;&lt;strong&gt;Complex Multi-file Change with Debugging&lt;/strong&gt;&lt;/a&gt;: GPT makes a complex code change that is coordinated across multiple source files, and resolves bugs by reviewing error output and doc snippets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/add-test.html&#34;&gt;&lt;strong&gt;Create a Black Box Test Case&lt;/strong&gt;&lt;/a&gt;: GPT creates a &#34;black box&#34; test case without access to the source of the method being tested, using only a &lt;a href=&#34;https://aider.chat/docs/ctags.html&#34;&gt;high level map of the repository based on ctags&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find more chat transcripts on the &lt;a href=&#34;https://aider.chat/examples/&#34;&gt;examples page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat with GPT about your code by launching &lt;code&gt;aider&lt;/code&gt; from the command line with set of source files to discuss and edit together. Aider lets GPT see and edit the content of those files.&lt;/li&gt; &#xA; &lt;li&gt;GPT can write and edit code in most popular languages: python, javascript, typescript, html, css, etc.&lt;/li&gt; &#xA; &lt;li&gt;Request new features, changes, improvements, or bug fixes to your code. Ask for new test cases, updated documentation or code refactors.&lt;/li&gt; &#xA; &lt;li&gt;Aider will apply the edits suggested by GPT directly to your source files.&lt;/li&gt; &#xA; &lt;li&gt;Aider will &lt;a href=&#34;https://aider.chat/docs/faq.html#how-does-aider-use-git&#34;&gt;automatically commit each changeset to your local git repo&lt;/a&gt; with a descriptive commit message. These frequent, automatic commits provide a safety net. It&#39;s easy to undo changes or use standard git workflows to manage longer sequences of changes.&lt;/li&gt; &#xA; &lt;li&gt;You can use aider with multiple source files at once, so GPT can make coordinated code changes across all of them in a single changeset/commit.&lt;/li&gt; &#xA; &lt;li&gt;Aider can &lt;a href=&#34;https://aider.chat/docs/ctags.html&#34;&gt;give &lt;em&gt;GPT-4&lt;/em&gt; a map of your entire git repo&lt;/a&gt;, which helps it understand and modify large codebases.&lt;/li&gt; &#xA; &lt;li&gt;You can also edit files by hand using your editor while chatting with aider. Aider will notice these out-of-band edits and ask if you&#39;d like to commit them. This lets you bounce back and forth between the aider chat and your editor, to collaboratively code with GPT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;aider&lt;/code&gt; tool by executing the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;aider &amp;lt;file1&amp;gt; &amp;lt;file2&amp;gt; ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your pip install did not place the &lt;code&gt;aider&lt;/code&gt; executable on your path, you can invoke aider like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m aider.main &amp;lt;file1&amp;gt; &amp;lt;file2&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;file1&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;file2&amp;gt;&lt;/code&gt;, etc., with the paths to the source code files you want to work on. These files will be &#34;added to the chat session&#34;, so that GPT can see their contents and edit them according to your instructions.&lt;/p&gt; &#xA;&lt;p&gt;You can also just launch &lt;code&gt;aider&lt;/code&gt; anywhere in a git repo without naming files on the command line. It will discover all the files in the repo. You can then add and remove individual files in the chat session with the &lt;code&gt;/add&lt;/code&gt; and &lt;code&gt;/drop&lt;/code&gt; chat commands described below. If you or GPT mention one of the repo&#39;s filenames in the conversation, aider will ask if you&#39;d like to add it to the chat.&lt;/p&gt; &#xA;&lt;p&gt;Aider will work best if you think about which files need to be edited to make your change and add them to the chat. Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself.&lt;/p&gt; &#xA;&lt;p&gt;Aider also has many additional command-line options, environment variables or configuration file to set many options. See &lt;code&gt;aider --help&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;In-chat commands&lt;/h2&gt; &#xA;&lt;p&gt;Aider supports commands from within the chat, which all start with &lt;code&gt;/&lt;/code&gt;. Here are some of the most useful in-chat commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/add &amp;lt;file&amp;gt;&lt;/code&gt;: Add matching files to the chat session.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/drop &amp;lt;file&amp;gt;&lt;/code&gt;: Remove matching files from the chat session.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/undo&lt;/code&gt;: Undo the last git commit if it was done by aider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/diff&lt;/code&gt;: Display the diff of the last aider commit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/run &amp;lt;command&amp;gt;&lt;/code&gt;: Run a shell command and optionally add the output to the chat.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/help&lt;/code&gt;: Show help about all commands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/commands.html&#34;&gt;full command docs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Think about which files need to be edited to make your change and add them to the chat. Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself.&lt;/li&gt; &#xA; &lt;li&gt;Large changes are best performed as a sequence of thoughtful bite sized steps, where you plan out the approach and overall design. Walk GPT through changes like you might with a junior dev. Ask for a refactor to prepare, then ask for the actual change. Spend the time to ask for code quality/structure improvements.&lt;/li&gt; &#xA; &lt;li&gt;Use Control-C to safely interrupt GPT if it isn&#39;t providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply to GPT with more information or direction.&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;/run&lt;/code&gt; command to run tests, linters, etc and show the output to GPT so it can fix any issues.&lt;/li&gt; &#xA; &lt;li&gt;Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages. Or enter &lt;code&gt;{&lt;/code&gt; alone on the first line to start a multiline message and &lt;code&gt;}&lt;/code&gt; alone on the last line to end it.&lt;/li&gt; &#xA; &lt;li&gt;If your code is throwing an error, share the error output with GPT using &lt;code&gt;/run&lt;/code&gt; or by pasting it into the chat. Let GPT figure out and fix the bug.&lt;/li&gt; &#xA; &lt;li&gt;GPT knows about a lot of standard tools and libraries, but may get some of the fine details wrong about APIs and function arguments. You can paste doc snippets into the chat to resolve these issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/faq.html#how-does-aider-use-git&#34;&gt;Aider will notice if you launch it on a git repo with uncommitted changes and offer to commit them before proceeding&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;GPT can only see the content of the files you specifically &#34;add to the chat&#34;. Aider also sends GPT-4 a &lt;a href=&#34;https://aider.chat/docs/ctags.html&#34;&gt;map of your entire git repo&lt;/a&gt;. So GPT may ask to see additional files if it feels that&#39;s needed for your requests.&lt;/li&gt; &#xA; &lt;li&gt;I also shared some general &lt;a href=&#34;https://news.ycombinator.com/item?id=36211879&#34;&gt;GPT coding tips on Hacker News&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GPT-4 vs GPT-3.5&lt;/h2&gt; &#xA;&lt;p&gt;Aider supports all of OpenAI&#39;s chat models. You can choose a model with the &lt;code&gt;--model&lt;/code&gt; command line argument.&lt;/p&gt; &#xA;&lt;p&gt;You should probably use GPT-4 if you can. For more details see the &lt;a href=&#34;https://aider.chat/docs/faq.html#gpt-4-vs-gpt-35&#34;&gt;FAQ entry that compares GPT-4 vs GPT-3.5&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a discussion of using other non-OpenAI models, see the &lt;a href=&#34;https://aider.chat/docs/faq.html#can-i-use-aider-with-other-llms-local-llms-etc&#34;&gt;FAQ about other LLMs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;For more information, see the &lt;a href=&#34;https://aider.chat/docs/faq.html&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Kind words from users&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;The best AI coding assistant so far.&lt;/em&gt; -- &lt;a href=&#34;https://www.youtube.com/watch?v=df8afeb1FY8&#34;&gt;Matthew Berman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Hands down, this is the best AI coding assistant tool so far.&lt;/em&gt; -- &lt;a href=&#34;https://www.youtube.com/watch?v=MPYFPvxfGZs&#34;&gt;IndyDevDan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Aider ... has easily quadrupled my coding productivity.&lt;/em&gt; -- &lt;a href=&#34;https://news.ycombinator.com/item?id=36212100&#34;&gt;SOLAR_FIELDS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;It&#39;s really like having your senior developer live right in your Git repo - truly amazing!&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/124&#34;&gt;rappster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;What an amazing tool. It&#39;s incredible.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/6#issue-1722897858&#34;&gt;valyagolev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Aider is such an astounding thing!&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/82#issuecomment-1631876700&#34;&gt;cgrothaus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;It was WAY faster than I would be getting off the ground and making the first few working versions.&lt;/em&gt; -- &lt;a href=&#34;https://twitter.com/d_feldman/status/1662295077387923456&#34;&gt;Daniel Feldman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;This project is stellar.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/112#issuecomment-1637429008&#34;&gt;funkytaco&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Amazing project, definitely the best AI coding assistant I&#39;ve used.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/84&#34;&gt;joshuavial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>