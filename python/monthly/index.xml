<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-01T01:55:40Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apple/ml-stable-diffusion</title>
    <updated>2023-01-01T01:55:40Z</updated>
    <id>tag:github.com,2023-01-01:/apple/ml-stable-diffusion</id>
    <link href="https://github.com/apple/ml-stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion with Core ML on Apple Silicon&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Core ML Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;Run Stable Diffusion on Apple Silicon with Core ML&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/readme_reel.png&#34;&gt; &#xA;&lt;p&gt;This repository comprises:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt;, a Python package for converting PyTorch models to Core ML format and performing image generation with Hugging Face &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;StableDiffusion&lt;/code&gt;, a Swift package that developers can add to their Xcode projects as a dependency to deploy image generation capabilities in their apps. The Swift package relies on the Core ML model files generated by &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you run into issues during installation or runtime, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#faq&#34;&gt;FAQ&lt;/a&gt; section. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#system-requirements&#34;&gt;System Requirements&lt;/a&gt; section before getting started.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;example-results&#34;&gt;&lt;/a&gt; Example Results&lt;/h2&gt; &#xA;&lt;p&gt;There are numerous versions of Stable Diffusion available on the &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt;. Here are example results from three of those models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;--model-version&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-base&#34;&gt;stabilityai/stable-diffusion-2-base&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Output&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_11_computeUnit_CPU_AND_GPU_modelVersion_stabilityai_stable-diffusion-2-base.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_13_computeUnit_CPU_AND_NE_modelVersion_CompVis_stable-diffusion-v1-4.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_NE_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M1 iPad Pro 8GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M1 MacBook Pro 16GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M2 MacBook Air 8GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#important-notes-on-performance-benchmarks&#34;&gt;Important Notes on Performance Benchmarks&lt;/a&gt; section for details.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;system-requirements&#34;&gt;&lt;/a&gt; System Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The following is recommended to use all the functionality in this repository:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Python&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;macOS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Xcode&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;iPadOS, iOS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;using-converted-weights&#34;&gt;&lt;/a&gt; Using Ready-made Core ML Models from Hugging Face Hub&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;ðŸ¤— Hugging Face ran the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-coreml&#34;&gt;conversion procedure&lt;/a&gt; on the following models and made the Core ML weights publicly available on the Hub. If you would like to convert a version of Stable Diffusion that is not already available on the Hub, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-core-ml&#34;&gt;Converting Models to Core ML&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/coreml-stable-diffusion-v1-4&#34;&gt;&lt;code&gt;CompVis/stable-diffusion-v1-4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/coreml-stable-diffusion-v1-5&#34;&gt;&lt;code&gt;runwayml/stable-diffusion-v1-5&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/coreml-stable-diffusion-2-base&#34;&gt;&lt;code&gt;stabilityai/stable-diffusion-2-base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;If you want to use any of those models you may download the weights and proceed to &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#image-generation-with-python&#34;&gt;generate images with Python&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#image-generation-with-swift&#34;&gt;Swift&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;There are several variants in each model repository. You may clone the whole repos using &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;git lfs&lt;/code&gt; to download all variants, or selectively download the ones you need.&lt;/p&gt; &#xA; &lt;p&gt;To clone the repos using &lt;code&gt;git&lt;/code&gt;, please follow this process:&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Install the &lt;code&gt;git lfs&lt;/code&gt; extension for your system.&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;git lfs&lt;/code&gt; stores large files outside the main git repo, and it downloads them from the appropriate server after you clone or checkout. It is available in most package managers, check &lt;a href=&#34;https://git-lfs.com&#34;&gt;the installation page&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Enable &lt;code&gt;git lfs&lt;/code&gt; by running this command once:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Use &lt;code&gt;git clone&lt;/code&gt; to download a copy of the repo that includes all model variants. For Stable Diffusion version 1.4, you&#39;d issue the following command in your terminal:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://huggingface.co/apple/coreml-stable-diffusion-v1-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you prefer to download specific variants instead of cloning the repos, you can use the &lt;code&gt;huggingface_hub&lt;/code&gt; Python library. For example, to do generation in Python using the &lt;code&gt;ORIGINAL&lt;/code&gt; attention implementation (read &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-core-ml&#34;&gt;this section&lt;/a&gt; for details), you could use the following helper code:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from huggingface_hub import snapshot_download&#xA;from huggingface_hub.file_download import repo_folder_name&#xA;from pathlib import Path&#xA;import shutil&#xA;&#xA;repo_id = &#34;apple/coreml-stable-diffusion-v1-4&#34;&#xA;variant = &#34;original/packages&#34;&#xA;&#xA;def download_model(repo_id, variant, output_dir):&#xA;    destination = Path(output_dir) / (repo_id.split(&#34;/&#34;)[-1] + &#34;_&#34; + variant.replace(&#34;/&#34;, &#34;_&#34;))&#xA;    if destination.exists():&#xA;        raise Exception(f&#34;Model already exists at {destination}&#34;)&#xA;    &#xA;    # Download and copy without symlinks&#xA;    downloaded = snapshot_download(repo_id, allow_patterns=f&#34;{variant}/*&#34;, cache_dir=output_dir)&#xA;    downloaded_bundle = Path(downloaded) / variant&#xA;    shutil.copytree(downloaded_bundle, destination)&#xA;&#xA;    # Remove all downloaded files&#xA;    cache_folder = Path(output_dir) / repo_folder_name(repo_id=repo_id, repo_type=&#34;model&#34;)&#xA;    shutil.rmtree(cache_folder)&#xA;    return destination&#xA;&#xA;model_path = download_model(repo_id, variant, output_dir=&#34;./models&#34;)&#xA;print(f&#34;Model downloaded at {model_path}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;code&gt;model_path&lt;/code&gt; would be the path in your local filesystem where the checkpoint was saved. Please, refer to &lt;a href=&#34;https://huggingface.co/blog/diffusers-coreml&#34;&gt;this post&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;converting-models-to-coreml&#34;&gt;&lt;/a&gt; Converting Models to Core ML&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Create a Python environment and install dependencies:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n coreml_stable_diffusion python=3.8 -y&#xA;conda activate coreml_stable_diffusion&#xA;cd /path/to/cloned/ml-stable-diffusion/repository&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Log in to or register for your &lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face account&lt;/a&gt;, generate a &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;User Access Token&lt;/a&gt; and use this token to set up Hugging Face API access by running &lt;code&gt;huggingface-cli login&lt;/code&gt; in a Terminal window.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Navigate to the version of Stable Diffusion that you would like to use on &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt; and accept its Terms of Use. The default model version is &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;. The model version may be changed by the user as described in the next step.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Execute the following command from the Terminal to generate Core ML model files (&lt;code&gt;.mlpackage&lt;/code&gt;)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o &amp;lt;output-mlpackages-directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; This command will download several GB worth of PyTorch checkpoints from Hugging Face. Please ensure that you are on Wi-Fi and have enough disk space.&lt;/p&gt; &#xA; &lt;p&gt;This generally takes 15-20 minutes on an M1 MacBook Pro. Upon successful execution, the 4 neural network models that comprise Stable Diffusion will have been converted from PyTorch to Core ML (&lt;code&gt;.mlpackage&lt;/code&gt;) and saved into the specified &lt;code&gt;&amp;lt;output-mlpackages-directory&amp;gt;&lt;/code&gt;. Some additional notable arguments:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--model-version&lt;/code&gt;: The model version defaults to &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;. Developers may specify other versions that are available on &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt;, e.g. &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-base&#34;&gt;stabilityai/stable-diffusion-2-base&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--bundle-resources-for-swift-cli&lt;/code&gt;: Compiles all 4 models and bundles them along with necessary resources for text tokenization into &lt;code&gt;&amp;lt;output-mlpackages-directory&amp;gt;/Resources&lt;/code&gt; which should provided as input to the Swift package. This flag is not necessary for the diffusers-based Python pipeline.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--chunk-unet&lt;/code&gt;: Splits the Unet model in two approximately equal chunks (each with less than 1GB of weights) for mobile-friendly deployment. This is &lt;strong&gt;required&lt;/strong&gt; for Neural Engine deployment on iOS and iPadOS. This is not required for macOS. Swift CLI is able to consume both the chunked and regular versions of the Unet model but prioritizes the former. Note that chunked unet is not compatible with the Python pipeline because Python pipeline is intended for macOS only. Chunking is for on-device deployment with Swift only.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--attention-implementation&lt;/code&gt;: Defaults to &lt;code&gt;SPLIT_EINSUM&lt;/code&gt; which is the implementation described in &lt;a href=&#34;https://machinelearning.apple.com/research/neural-engine-transformers&#34;&gt;Deploying Transformers on the Apple Neural Engine&lt;/a&gt;. &lt;code&gt;--attention-implementation ORIGINAL&lt;/code&gt; will switch to an alternative that should be used for CPU or GPU deployment. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#performance-benchmark&#34;&gt;Performance Benchmark&lt;/a&gt; section for further guidance.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--check-output-correctness&lt;/code&gt;: Compares original PyTorch model&#39;s outputs to final Core ML model&#39;s outputs. This flag increases RAM consumption significantly so it is recommended only for debugging purposes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;image-generation-with-python&#34;&gt;&lt;/a&gt; Image Generation with Python&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;Run text-to-image generation using the example Python pipeline based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m python_coreml_stable_diffusion.pipeline --prompt &#34;a photo of an astronaut riding a horse on mars&#34; -i &amp;lt;output-mlpackages-directory&amp;gt; -o &amp;lt;/path/to/output/image&amp;gt; --compute-unit ALL --seed 93&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please refer to the help menu for all available arguments: &lt;code&gt;python -m python_coreml_stable_diffusion.pipeline -h&lt;/code&gt;. Some notable arguments:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;-i&lt;/code&gt;: Should point to the &lt;code&gt;-o&lt;/code&gt; directory from Step 4 of &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-coreml&#34;&gt;Converting Models to Core ML&lt;/a&gt; section from above.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--model-version&lt;/code&gt;: If you overrode the default model version while converting models to Core ML, you will need to specify the same model version here.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--compute-unit&lt;/code&gt;: Note that the most performant compute unit for this particular implementation may differ across different hardware. &lt;code&gt;CPU_AND_GPU&lt;/code&gt; or &lt;code&gt;CPU_AND_NE&lt;/code&gt; may be faster than &lt;code&gt;ALL&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#performance-benchmark&#34;&gt;Performance Benchmark&lt;/a&gt; section for further guidance.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--scheduler&lt;/code&gt;: If you would like to experiment with different schedulers, you may specify it here. For available options, please see the help menu. You may also specify a custom number of inference steps by &lt;code&gt;--num-inference-steps&lt;/code&gt; which defaults to 50.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;image-gen-swift&#34;&gt;&lt;/a&gt; Image Generation with Swift&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;h3&gt;&lt;a name=&#34;swift-requirements&#34;&gt;&lt;/a&gt; System Requirements&lt;/h3&gt; &#xA; &lt;p&gt;&lt;strong&gt;Building&lt;/strong&gt; (recommended):&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Xcode 14.2&lt;/li&gt; &#xA;  &lt;li&gt;Command Line Tools for Xcode 14.2&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Check &lt;a href=&#34;https://developer.apple.com/download/all/?q=xcode&#34;&gt;developer.apple.com&lt;/a&gt; for the latest versions.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Running&lt;/strong&gt; (minimum):&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Mac&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;iPad*&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;iPhone*&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;macOS 13.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;iPadOS 16.2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;iOS 16.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;M1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;M1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;iPhone 12 Pro&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;You will also need the resources generated by the &lt;code&gt;--bundle-resources-for-swift-cli&lt;/code&gt; option described in &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-coreml&#34;&gt;Converting Models to Core ML&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;* Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#faq&#34;&gt;FAQ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#q-mobile-app&#34;&gt;Q6&lt;/a&gt; regarding deploying on iPad and iPhone.&lt;/p&gt; &#xA; &lt;h3&gt;Example CLI Usage&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;swift run StableDiffusionSample &#34;a photo of an astronaut riding a horse on mars&#34; --resource-path &amp;lt;output-mlpackages-directory&amp;gt;/Resources/ --seed 93 --output-path &amp;lt;/path/to/output/image&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The output will be named based on the prompt and random seed: e.g. &lt;code&gt;&amp;lt;/path/to/output/image&amp;gt;/a_photo_of_an_astronaut_riding_a_horse_on_mars.93.final.png&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;Please use the &lt;code&gt;--help&lt;/code&gt; flag to learn about batched generation and more.&lt;/p&gt; &#xA; &lt;h3&gt;Example Library Usage&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import StableDiffusion&#xA;...&#xA;let pipeline = try StableDiffusionPipeline(resourcesAt: resourceURL)&#xA;pipeline.loadResources()&#xA;let image = try pipeline.generateImages(prompt: prompt, seed: seed).first&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;On iOS, the &lt;code&gt;reduceMemory&lt;/code&gt; option should be set to &lt;code&gt;true&lt;/code&gt; when constructing &lt;code&gt;StableDiffusionPipeline&lt;/code&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Swift Package Details&lt;/h3&gt; &#xA; &lt;p&gt;This Swift package contains two products:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;StableDiffusion&lt;/code&gt; library&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;StableDiffusionSample&lt;/code&gt; command-line tool&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Both of these products require the Core ML models and tokenization resources to be supplied. When specifying resources via a directory path that directory must contain the following:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;TextEncoder.mlmodelc&lt;/code&gt; (text embedding model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;Unet.mlmodelc&lt;/code&gt; or &lt;code&gt;UnetChunk1.mlmodelc&lt;/code&gt; &amp;amp; &lt;code&gt;UnetChunk2.mlmodelc&lt;/code&gt; (denoising autoencoder model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;VAEDecoder.mlmodelc&lt;/code&gt; (image decoder model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;vocab.json&lt;/code&gt; (tokenizer vocabulary file)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;merges.text&lt;/code&gt; (merges for byte pair encoding file)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Optionally, it may also include the safety checker model that some versions of Stable Diffusion include:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;SafetyChecker.mlmodelc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note that the chunked version of Unet is checked for first. Only if it is not present will the full &lt;code&gt;Unet.mlmodelc&lt;/code&gt; be loaded. Chunking is required for iOS and iPadOS and not necessary for macOS.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;swift-app&#34;&gt;&lt;/a&gt; Example Swift App&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;ðŸ¤— Hugging Face created an &lt;a href=&#34;https://github.com/huggingface/swift-coreml-diffusers&#34;&gt;open-source demo app&lt;/a&gt; on top of this library. It&#39;s written in native Swift and Swift UI, and runs on macOS, iOS and iPadOS. You can use the code as a starting point for your app, or to see how to integrate this library in your own projects.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;performance-benchmark&#34;&gt;&lt;/a&gt; Performance Benchmark&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;Standard &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt; Benchmark&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Device&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;--compute-unit&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;--attention-implementation&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;Latency (seconds)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mac Studio (M1 Ultra, 64-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mac Studio (M1 Ultra, 48-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Max, 32-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Max, 24-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Pro, 16-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ALL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M2)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;iPad Pro (5th gen, M1)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#important-notes-on-performance-benchmarks&#34;&gt;Important Notes on Performance Benchmarks&lt;/a&gt; section for details.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;important-notes-on-performance-benchmarks&#34;&gt;&lt;/a&gt; Important Notes on Performance Benchmarks&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;This benchmark was conducted by Apple using public beta versions of iOS 16.2, iPadOS 16.2 and macOS 13.1 in November 2022.&lt;/li&gt; &#xA;  &lt;li&gt;The executed program is &lt;code&gt;python_coreml_stable_diffusion.pipeline&lt;/code&gt; for macOS devices and a minimal Swift test app built on the &lt;code&gt;StableDiffusion&lt;/code&gt; Swift package for iOS and iPadOS devices.&lt;/li&gt; &#xA;  &lt;li&gt;The median value across 3 end-to-end executions is reported.&lt;/li&gt; &#xA;  &lt;li&gt;Performance may materially differ across different versions of Stable Diffusion due to architecture changes in the model itself. Each reported number is specific to the model version mentioned in that context.&lt;/li&gt; &#xA;  &lt;li&gt;The image generation procedure follows the standard configuration: 50 inference steps, 512x512 output image resolution, 77 text token sequence length, classifier-free guidance (batch size of 2 for unet).&lt;/li&gt; &#xA;  &lt;li&gt;The actual prompt length does not impact performance because the Core ML model is converted with a static shape that computes the forward pass for all of the 77 elements (&lt;code&gt;tokenizer.model_max_length&lt;/code&gt;) in the text token sequence regardless of the actual length of the input text.&lt;/li&gt; &#xA;  &lt;li&gt;Pipelining across the 4 models is not optimized and these performance numbers are subject to variance under increased system load from other applications. Given these factors, we do not report sub-second variance in latency.&lt;/li&gt; &#xA;  &lt;li&gt;Weights and activations are in float16 precision for both the GPU and the Neural Engine.&lt;/li&gt; &#xA;  &lt;li&gt;The Swift CLI program consumes a peak memory of approximately 2.6GB (without the safety checker), 2.1GB of which is model weights in float16 precision. We applied &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-affine-quantization&#34;&gt;8-bit weight quantization&lt;/a&gt; to reduce peak memory consumption by approximately 1GB. However, we observed that it had an adverse effect on generated image quality and we rolled it back. We encourage developers to experiment with other advanced weight compression techniques such as &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-a-lookup-table&#34;&gt;palettization&lt;/a&gt; and/or &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-sparse-representation&#34;&gt;pruning&lt;/a&gt; which may yield better results.&lt;/li&gt; &#xA;  &lt;li&gt;In the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/performance-benchmark&#34;&gt;benchmark table&lt;/a&gt;, we report the best performing &lt;code&gt;--compute-unit&lt;/code&gt; and &lt;code&gt;--attention-implementation&lt;/code&gt; values per device. The former does not modify the Core ML model and can be applied during runtime. The latter modifies the Core ML model. Note that the best performing compute unit is model version and hardware-specific.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;results-with-different-compute-units&#34;&gt;&lt;/a&gt; Results with Different Compute Units&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;It is highly probable that there will be slight differences across generated images using different compute units.&lt;/p&gt; &#xA; &lt;p&gt;The following images were generated on an M1 MacBook Pro and macOS 13.1 with the prompt &lt;em&gt;&#34;a photo of an astronaut riding a horse on mars&#34;&lt;/em&gt; using the &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt; model version. The random seed was set to 93:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;CPU_AND_NE&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;CPU_AND_GPU&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;ALL&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_NE_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_GPU_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_ALL_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Differences may be less or more pronounced for different inputs. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#faq&#34;&gt;FAQ&lt;/a&gt; Q8 for a detailed explanation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;faq&#34;&gt;&lt;/a&gt; FAQ&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q1: &lt;/b&gt; &lt;code&gt; ERROR: Failed building wheel for tokenizers or error: can&#39;t find Rust compiler &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A1: &lt;/b&gt; Please review this &lt;a href=&#34;https://github.com/huggingface/transformers/issues/2831#issuecomment-592724471&#34;&gt;potential solution&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q2: &lt;/b&gt; &lt;code&gt; RuntimeError: {NSLocalizedDescription = &#34;Error computing NN outputs.&#34; &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A2: &lt;/b&gt; There are many potential causes for this error. In this context, it is highly likely to be encountered when your system is under increased memory pressure from other applications. Reducing memory utilization of other applications is likely to help alleviate the issue.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; &lt;a name=&#34;low-mem-conversion&#34;&gt;&lt;/a&gt; Q3: &lt;/b&gt; My Mac has 8GB RAM and I am converting models to Core ML using the example command. The process is getting killed because of memory issues. How do I fix this issue? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A3: &lt;/b&gt; In order to minimize the memory impact of the model conversion process, please execute the following command instead:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-vae-decoder -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-text-encoder -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-safety-checker -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;If you need &lt;code&gt;--chunk-unet&lt;/code&gt;, you may do so in yet another independent command which will reuse the previously exported Unet model and simply chunk it in place:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --chunk-unet -o &amp;lt;output-mlpackages-directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q4: &lt;/b&gt; My Mac has 8GB RAM, should image generation work on my machine? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A4: &lt;/b&gt; Yes! Especially the &lt;code&gt;--compute-unit CPU_AND_NE&lt;/code&gt; option should work under reasonable system load from other applications. Note that part of the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#example-results&#34;&gt;Example Results&lt;/a&gt; were generated using an M2 MacBook Air with 8GB RAM.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q5: &lt;/b&gt; Every time I generate an image using the Python pipeline, loading all the Core ML models takes 2-3 minutes. Is this expected? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A5: &lt;/b&gt; Yes and using the Swift library reduces this to just a few seconds. The reason is that &lt;code&gt;coremltools&lt;/code&gt; loads Core ML models (&lt;code&gt;.mlpackage&lt;/code&gt;) and each model is compiled to be run on the requested compute unit during load time. Because of the size and number of operations of the unet model, it takes around 2-3 minutes to compile it for Neural Engine execution. Other models should take at most a few seconds. Note that &lt;code&gt;coremltools&lt;/code&gt; does not cache the compiled model for later loads so each load takes equally long. In order to benefit from compilation caching, &lt;code&gt;StableDiffusion&lt;/code&gt; Swift package by default relies on compiled Core ML models (&lt;code&gt;.mlmodelc&lt;/code&gt;) which will be compiled down for the requested compute unit upon first load but then the cache will be reused on subsequent loads until it is purged due to lack of use.&lt;/p&gt; &#xA;  &lt;p&gt;If you intend to use the Python pipeline in an application, we recommend initializing the pipeline once so that the load time is only incurred once. Afterwards, generating images using different prompts and random seeds will not incur the load time for the current session of your application.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; &lt;a name=&#34;q-mobile-app&#34;&gt;&lt;/a&gt; Q6: &lt;/b&gt; I want to deploy &lt;code&gt;StableDiffusion&lt;/code&gt;, the Swift package, in my mobile app. What should I be aware of? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A6: &lt;/b&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#image-gen-swift&#34;&gt;Image Generation with Swift&lt;/a&gt; section describes the minimum SDK and OS versions as well as the device models supported by this package. We recommend carefully testing the package on the device with the least amount of RAM available among your deployment targets.&lt;/p&gt; &#xA;  &lt;p&gt;The image generation process in &lt;code&gt;StableDiffusion&lt;/code&gt; can yield over 2 GB of peak memory during runtime depending on the compute units selected. On iPadOS, we recommend using &lt;code&gt;.cpuAndNeuralEngine&lt;/code&gt; in your configuration and the &lt;code&gt;reduceMemory&lt;/code&gt; option when constructing a &lt;code&gt;StableDiffusionPipeline&lt;/code&gt; to minimize memory pressure.&lt;/p&gt; &#xA;  &lt;p&gt;If your app crashes during image generation, consider adding the &lt;a href=&#34;https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit&#34;&gt;Increased Memory Limit&lt;/a&gt; capability to inform the system that some of your appâ€™s core features may perform better by exceeding the default app memory limit on supported devices.&lt;/p&gt; &#xA;  &lt;p&gt;On iOS, depending on the iPhone model, Stable Diffusion model versions, selected compute units, system load and design of your app, this may still not be sufficient to keep your apps peak memory under the limit. Please remember, because the device shares memory between apps and iOS processes, one app using too much memory can compromise the user experience across the whole device.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q7: &lt;/b&gt; How do I generate images with different resolutions using the same Core ML models? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A7: &lt;/b&gt; The current version of &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt; does not support single-model multi-resolution out of the box. However, developers may fork this project and leverage the &lt;a href=&#34;https://coremltools.readme.io/docs/flexible-inputs&#34;&gt;flexible shapes&lt;/a&gt; support from coremltools to extend the &lt;code&gt;torch2coreml&lt;/code&gt; script by using &lt;code&gt;coremltools.EnumeratedShapes&lt;/code&gt;. Note that, while the &lt;code&gt;text_encoder&lt;/code&gt; is agnostic to the image resolution, the inputs and outputs of &lt;code&gt;vae_decoder&lt;/code&gt; and &lt;code&gt;unet&lt;/code&gt; models are dependent on the desired image resolution.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q8: &lt;/b&gt; Are the Core ML and PyTorch generated images going to be identical? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A8: &lt;/b&gt; If desired, the generated images across PyTorch and Core ML can be made approximately identical. However, it is not guaranteed by default. There are several factors that might lead to different images across PyTorch and Core ML:&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 1. Random Number Generator Behavior &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The main source of potentially different results across PyTorch and Core ML is the Random Number Generator (&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_number_generation&#34;&gt;RNG&lt;/a&gt;) behavior. PyTorch and Numpy have different sources of randomness. &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt; generally relies on Numpy for RNG (e.g. latents initialization) and &lt;code&gt;StableDiffusion&lt;/code&gt; Swift Library reproduces this RNG behavior. However, PyTorch-based pipelines such as Hugging Face &lt;code&gt;diffusers&lt;/code&gt; relies on PyTorch&#39;s RNG behavior.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 2. PyTorch &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&#34;Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.&#34;&lt;/em&gt; (&lt;a href=&#34;https://pytorch.org/docs/stable/notes/randomness.html#reproducibility&#34;&gt;source&lt;/a&gt;).&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 3. Model Function Drift During Conversion &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The difference in outputs across corresponding PyTorch and Core ML models is a potential cause. The signal integrity is tested during the conversion process (enabled via &lt;code&gt;--check-output-correctness&lt;/code&gt; argument to &lt;code&gt;python_coreml_stable_diffusion.torch2coreml&lt;/code&gt;) and it is verified to be above a minimum &lt;a href=&#34;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&#34;&gt;PSNR&lt;/a&gt; value as tested on random inputs. Note that this is simply a sanity check and does not guarantee this minimum PSNR across all possible inputs. Furthermore, the results are not guaranteed to be identical when executing the same Core ML models across different compute units. This is not expected to be a major source of difference as the sample visual results indicate in &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#results-with-different-compute-units&#34;&gt;this section&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 4. Weights and Activations Data Type &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;When quantizing models from float32 to lower-precision data types such as float16, the generated images are &lt;a href=&#34;https://lambdalabs.com/blog/inference-benchmark-stable-diffusion&#34;&gt;known to vary slightly&lt;/a&gt; in semantics even when using the same PyTorch model. Core ML models generated by coremltools have float16 weights and activations by default &lt;a href=&#34;https://github.com/apple/coremltools/raw/main/coremltools/converters/_converters_entry.py#L256&#34;&gt;unless explicitly overridden&lt;/a&gt;. This is not expected to be a major source of difference.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q9: &lt;/b&gt; The model files are very large, how do I avoid a large binary for my App? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A9: &lt;/b&gt; The recommended option is to prompt the user to download these assets upon first launch of the app. This keeps the app binary size independent of the Core ML models being deployed. Disclosing the size of the download to the user is extremely important as there could be data charges or storage impact that the user might not be comfortable with.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q10: &lt;/b&gt; &lt;code&gt; `Could not initialize NNPACK! Reason: Unsupported hardware` &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A10: &lt;/b&gt; This warning is safe to ignore in the context of this repository.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q11: &lt;/b&gt; &lt;code&gt; TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A11: &lt;/b&gt; This warning is safe to ignore in the context of this repository.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q12: &lt;/b&gt; &lt;code&gt; UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A12: &lt;/b&gt; If this warning is printed right after &lt;code&gt; zsh: killed python -m python_coreml_stable_diffusion.torch2coreml ... &lt;/code&gt;, then it is highly likely that your Mac has run out of memory while converting models to Core ML. Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#low-mem-conversion&#34;&gt;Q3&lt;/a&gt; from above for the solution.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>carson-katri/dream-textures</title>
    <updated>2023-01-01T01:55:40Z</updated>
    <id>tag:github.com,2023-01-01:/carson-katri/dream-textures</id>
    <link href="https://github.com/carson-katri/dream-textures" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion built-in to the Blender shader editor&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/banner.png&#34; alt=&#34;Dream Textures, subtitle: Stable Diffusion built-in to Blender&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/carson-katri/dream-textures/releases/latest&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/release/carson-katri/dream-textures&#34; alt=&#34;Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/EmDJ8CaWZ7&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/badge/icon/discord?icon=discord&amp;amp;label&#34; alt=&#34;Join the Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/carson-katri/dream-textures/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/carson-katri/dream-textures/total?style=flat-square&#34; alt=&#34;Total Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.blendermarket.com/products/dream-textures&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/badge/buy/blender%20market/orange&#34; alt=&#34;Buy on Blender Market&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create textures, concept art, background assets, and more with a simple text prompt&lt;/li&gt; &#xA; &lt;li&gt;Use the &#39;Seamless&#39; option to create textures that tile perfectly with no visible seam&lt;/li&gt; &#xA; &lt;li&gt;Texture entire scenes with &#39;Project Dream Texture&#39; and depth to image&lt;/li&gt; &#xA; &lt;li&gt;Re-style animations with the Cycles render pass&lt;/li&gt; &#xA; &lt;li&gt;Run the models on your machine to iterate without slowdowns from a service&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://github.com/carson-katri/dream-textures/releases/latest&#34;&gt;latest release&lt;/a&gt; and follow the instructions there to get up and running.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;On macOS, it is possible you will run into a quarantine issue with the dependencies. To work around this, run the following command in the app &lt;code&gt;Terminal&lt;/code&gt;: &lt;code&gt;xattr -r -d com.apple.quarantine ~/Library/Application\ Support/Blender/3.3/scripts/addons/dream_textures/.python_dependencies&lt;/code&gt;. This will allow the PyTorch &lt;code&gt;.dylib&lt;/code&gt;s and &lt;code&gt;.so&lt;/code&gt;s to load without having to manually allow each one in System Preferences.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want a visual guide to installation, see this video tutorial from Ashlee Martino-Tarr: &lt;a href=&#34;https://youtu.be/kEcr8cNmqZk&#34;&gt;https://youtu.be/kEcr8cNmqZk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ensure you always install the &lt;a href=&#34;https://github.com/carson-katri/dream-textures/releases/latest&#34;&gt;latest version&lt;/a&gt; of the add-on if any guides become out of date.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Here&#39;s a few quick guides:&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/SETUP.md&#34;&gt;Setting Up&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Setup instructions for various platforms and configurations.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/IMAGE_GENERATION.md&#34;&gt;Image Generation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Create textures, concept art, and more with text prompts. Learn how to use the various configuration options to get exactly what you&#39;re looking for.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/image_generation.png&#34; alt=&#34;A graphic showing each step of the image generation process&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/TEXTURE_PROJECTION.md&#34;&gt;Texture Projection&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Texture entire models and scenes with depth to image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/texture_projection.png&#34; alt=&#34;A graphic showing each step of the texture projection process&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/INPAINT_OUTPAINT.md&#34;&gt;Inpaint/Outpaint&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Inpaint to fix up images and convert existing textures into seamless ones automatically.&lt;/p&gt; &#xA;&lt;p&gt;Outpaint to increase the size of an image by extending it in any direction.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/inpaint_outpaint.png&#34; alt=&#34;A graphic showing each step of the outpainting process&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/RENDER_PASS.md&#34;&gt;Render Pass&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Perform style transfer and create novel animations with Stable Diffusion as a post processing step.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/render_pass.png&#34; alt=&#34;A graphic showing each frame of a render pass, split with the original and generated result&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/AI_UPSCALING.md&#34;&gt;AI Upscaling&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Upscale your low-res generations 4x.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/assets/upscale.png&#34; alt=&#34;A graphic showing each step of the upscaling process&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/carson-katri/dream-textures/main/docs/HISTORY.md&#34;&gt;History&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Recall, export, and import history entries for later use.&lt;/p&gt; &#xA;&lt;h1&gt;Compatibility&lt;/h1&gt; &#xA;&lt;p&gt;Dream Textures has been tested with CUDA and Apple Silicon GPUs. Over 4GB of VRAM is recommended.&lt;/p&gt; &#xA;&lt;p&gt;If you have an issue with a supported GPU, please create an issue.&lt;/p&gt; &#xA;&lt;h3&gt;Cloud Processing&lt;/h3&gt; &#xA;&lt;p&gt;If your hardware is unsupported, you can use DreamStudio to process in the cloud. Follow the instructions in the release notes to setup with DreamStudio.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;After cloning the repository, there a few more steps you need to complete to setup your development environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install submodules:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;I recommend the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=JacquesLucke.blender-development&#34;&gt;Blender Development&lt;/a&gt; extension for VS Code for debugging. If you just want to install manually though, you can put the &lt;code&gt;dream_textures&lt;/code&gt; repo folder in Blender&#39;s addon directory.&lt;/li&gt; &#xA; &lt;li&gt;After running the local add-on in Blender, setup the model weights like normal.&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies locally &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open Blender&#39;s preferences window&lt;/li&gt; &#xA;   &lt;li&gt;Enable &lt;em&gt;Interface&lt;/em&gt; &amp;gt; &lt;em&gt;Display&lt;/em&gt; &amp;gt; &lt;em&gt;Developer Extras&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Then install dependencies for development under &lt;em&gt;Add-ons&lt;/em&gt; &amp;gt; &lt;em&gt;Dream Textures&lt;/em&gt; &amp;gt; &lt;em&gt;Development Tools&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;This will download all pip dependencies for the selected platform into &lt;code&gt;.python_dependencies&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On Apple Silicon, with the &lt;code&gt;requirements-dream-studio.txt&lt;/code&gt; you may run into an error with gRPC using an incompatible binary. If so, please use the following command to install the correct gRPC version:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install --no-binary :all: grpcio --ignore-installed --target .python_dependencies --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>openai/openai-python</title>
    <updated>2023-01-01T01:55:40Z</updated>
    <id>tag:github.com,2023-01-01:/openai/openai-python</id>
    <link href="https://github.com/openai/openai-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python Library&lt;/h1&gt; &#xA;&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/api-reference?lang=python&#34;&gt;OpenAI API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You don&#39;t need this source code unless you want to modify the package. If you just want to use the package, just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install --upgrade openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install from source with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The library needs to be configured with your account&#39;s secret key which is available on the &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;website&lt;/a&gt;. Either set it as the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable before using the library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;sk-...&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or set &lt;code&gt;openai.api_key&lt;/code&gt; to its value:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;&#xA;&#xA;# list engines&#xA;engines = openai.Engine.list()&#xA;&#xA;# print the first engine&#39;s id&#xA;print(engines.data[0].id)&#xA;&#xA;# create a completion&#xA;completion = openai.Completion.create(engine=&#34;ada&#34;, prompt=&#34;Hello world&#34;)&#xA;&#xA;# print the completion&#xA;print(completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Params&lt;/h3&gt; &#xA;&lt;p&gt;All endpoints have a &lt;code&gt;.create&lt;/code&gt; method that support a &lt;code&gt;request_timeout&lt;/code&gt; param. This param takes a &lt;code&gt;Union[float, Tuple[float, float]]&lt;/code&gt; and will raise a &lt;code&gt;openai.error.TimeoutError&lt;/code&gt; error if the request exceeds that time in seconds (See: &lt;a href=&#34;https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts&#34;&gt;https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Microsoft Azure Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;In order to use the library with Microsoft Azure endpoints, you need to set the &lt;code&gt;api_type&lt;/code&gt;, &lt;code&gt;api_base&lt;/code&gt; and &lt;code&gt;api_version&lt;/code&gt; in addition to the &lt;code&gt;api_key&lt;/code&gt;. The &lt;code&gt;api_type&lt;/code&gt; must be set to &#39;azure&#39; and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_type = &#34;azure&#34;&#xA;openai.api_key = &#34;...&#34;&#xA;openai.api_base = &#34;https://example-endpoint.openai.azure.com&#34;&#xA;openai.api_version = &#34;2022-12-01&#34;&#xA;&#xA;# create a completion&#xA;completion = openai.Completion.create(engine=&#34;deployment-name&#34;, prompt=&#34;Hello world&#34;)&#xA;&#xA;# print the completion&#xA;print(completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, embedding, and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples/azure/completions.ipynb&#34;&gt;Using Azure completions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples/azure/finetuning.ipynb&#34;&gt;Using Azure fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/azure/embeddings.ipynb&#34;&gt;Using Azure embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Microsoft Azure Active Directory Authentication&lt;/h3&gt; &#xA;&lt;p&gt;In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the &lt;code&gt;api_type&lt;/code&gt; to &#34;azure_ad&#34; and pass the acquired credential token to &lt;code&gt;api_key&lt;/code&gt;. The rest of the parameters need to be set as specified in the previous section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from azure.identity import DefaultAzureCredential&#xA;import openai&#xA;&#xA;# Request credential&#xA;default_credential = DefaultAzureCredential()&#xA;token = default_credential.get_token(&#34;https://cognitiveservices.azure.com/.default&#34;)&#xA;&#xA;# Setup parameters&#xA;openai.api_type = &#34;azure_ad&#34;&#xA;openai.api_key = token.token&#xA;openai.api_base = &#34;https://example-endpoint.openai.azure.com/&#34;&#xA;openai.api_version = &#34;2022-12-01&#34;&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command-line interface&lt;/h3&gt; &#xA;&lt;p&gt;This library additionally provides an &lt;code&gt;openai&lt;/code&gt; command-line utility which makes it easy to interact with the API from your terminal. Run &lt;code&gt;openai api -h&lt;/code&gt; for usage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# list engines&#xA;openai api engines.list&#xA;&#xA;# create a completion&#xA;openai api completions.create -e ada -p &#34;Hello world&#34;&#xA;&#xA;# generate images via DALLÂ·E API&#xA;openai api image.create -p &#34;two dogs playing chess, cartoon&#34; -n 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example code&lt;/h2&gt; &#xA;&lt;p&gt;Examples of how to use this Python library to accomplish various tasks can be found in the &lt;a href=&#34;https://github.com/openai/openai-cookbook/&#34;&gt;OpenAI Cookbook&lt;/a&gt;. It contains code examples for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Classification using fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Clustering&lt;/li&gt; &#xA; &lt;li&gt;Code search&lt;/li&gt; &#xA; &lt;li&gt;Customizing embeddings&lt;/li&gt; &#xA; &lt;li&gt;Question answering from a corpus of documents&lt;/li&gt; &#xA; &lt;li&gt;Recommendations&lt;/li&gt; &#xA; &lt;li&gt;Visualization of embeddings&lt;/li&gt; &#xA; &lt;li&gt;And more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the &lt;a href=&#34;https://github.com/openai/openai-cookbook/&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.&lt;/p&gt; &#xA;&lt;p&gt;To get an embedding for a text string, you can use the embeddings method as follows in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;# choose text to embed&#xA;text_string = &#34;sample text&#34;&#xA;&#xA;# choose an embedding&#xA;model_id = &#34;text-similarity-davinci-001&#34;&#xA;&#xA;# compute the embedding of the text&#xA;embedding = openai.Embedding.create(input=text_string, engine=model_id)[&#39;data&#39;][0][&#39;embedding&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of how to call the embeddings method is shown in this &lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Get_embeddings.ipynb&#34;&gt;get embeddings notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Examples of how to use embeddings are shared in the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Classification_using_embeddings.ipynb&#34;&gt;Classification using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Clustering.ipynb&#34;&gt;Clustering using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Code_search.ipynb&#34;&gt;Code search using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Semantic_text_search_using_embeddings.ipynb&#34;&gt;Semantic text search using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/User_and_product_embeddings.ipynb&#34;&gt;User and product embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Zero-shot_classification_with_embeddings.ipynb&#34;&gt;Zero-shot classification using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Recommendation_using_embeddings.ipynb&#34;&gt;Recommendation using embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information on embeddings and the types of embeddings OpenAI offers, read the &lt;a href=&#34;https://beta.openai.com/docs/guides/embeddings&#34;&gt;embeddings guide&lt;/a&gt; in the OpenAI documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Fine tuning&lt;/h3&gt; &#xA;&lt;p&gt;Fine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).&lt;/p&gt; &#xA;&lt;p&gt;Examples of fine tuning are shared in the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Fine-tuned_classification.ipynb&#34;&gt;Classification with fine tuning&lt;/a&gt; (a simple notebook that shows the steps required for fine tuning)&lt;/li&gt; &#xA; &lt;li&gt;Fine tuning a model that answers questions about the 2020 Olympics &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb&#34;&gt;Step 1: Collecting data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb&#34;&gt;Step 2: Creating a synthetic Q&amp;amp;A dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb&#34;&gt;Step 3: Train a fine-tuning model specialized for Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sync your fine-tunes to &lt;a href=&#34;https://wandb.me/openai-docs&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; to track experiments, models, and datasets in your central dashboard with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openai wandb sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on fine tuning, read the &lt;a href=&#34;https://beta.openai.com/docs/guides/fine-tuning&#34;&gt;fine-tuning guide&lt;/a&gt; in the OpenAI documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Moderation&lt;/h3&gt; &#xA;&lt;p&gt;OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI &lt;a href=&#34;https://beta.openai.com/docs/usage-policies&#34;&gt;content policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;moderation_resp = openai.Moderation.create(input=&#34;Here is some perfectly innocuous text that follows all OpenAI content policies.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/guides/moderation&#34;&gt;moderation guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Image generation (DALLÂ·E)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;image_resp = openai.Image.create(prompt=&#34;two dogs playing chess, oil painting&#34;, n=4, size=&#34;512x512&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/guides/images&#34;&gt;usage guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7.1+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at &lt;a href=&#34;mailto:support@openai.com&#34;&gt;support@openai.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;This library is forked from the &lt;a href=&#34;https://github.com/stripe/stripe-python&#34;&gt;Stripe Python Library&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>