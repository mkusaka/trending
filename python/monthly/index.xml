<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-01T02:21:14Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CorentinJ/Real-Time-Voice-Cloning</title>
    <updated>2023-10-01T02:21:14Z</updated>
    <id>tag:github.com,2023-10-01:/CorentinJ/Real-Time-Voice-Cloning</id>
    <link href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; &#xA;&lt;p&gt;This repository is an implementation of &lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href=&#34;https://matheo.uliege.be/handle/2268.2/6801&#34;&gt;master&#39;s thesis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-O_hYhToKoA&#34;&gt;&lt;img src=&#34;https://i.imgur.com/8lFUlgz.png&#34; alt=&#34;Toolbox demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Papers implemented&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Heads up&lt;/h2&gt; &#xA;&lt;p&gt;Like everything else in Deep Learning, this repo is quickly getting old. Many other open-source repositories or SaaS apps (often paying) will give you a better audio quality than this repository will. If you care about the fidelity of the voice you&#39;re cloning, and its expressivity, here are some personal recommendations of alternative voice cloning solutions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://github.com/coqui-ai/tts&#34;&gt;CoquiTTS&lt;/a&gt; for an open source repository that is more up-to-date, with a better voice cloning quality and more functionalities.&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://paperswithcode.com/task/speech-synthesis/&#34;&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://www.resemble.ai/&#34;&gt;Resemble.ai&lt;/a&gt; (disclaimer: I work there) for state of the art voice cloning with little hassle.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; &#xA; &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you&#39;ll probably have to tweak the dependencies&#39; versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; &#xA; &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn&#39;t work for you, you can manually download them &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If all tests pass, you&#39;re good to go.&lt;/p&gt; &#xA;&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; &#xA;&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href=&#34;https://www.openslr.org/resources/12/train-clean-100.tar.gz&#34;&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets&#34;&gt;here&lt;/a&gt;. You&#39;re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; &#xA;&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; &#xA;&lt;p&gt;You can then try the toolbox:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt; or&lt;br&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/nougat</title>
    <updated>2023-10-01T02:21:14Z</updated>
    <id>tag:github.com,2023-10-01:/facebookresearch/nougat</id>
    <link href="https://github.com/facebookresearch/nougat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Nougat Neural Optical Understanding for Academic Documents&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Nougat: Neural Optical Understanding for Academic Documents&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.13418&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-arxiv.2308.13418-white&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/nougat&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/facebookresearch/nougat&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/nougat-ocr&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/nougat-ocr?logo=pypi&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ysharma/nougat&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Community%20Space-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is the official repository for Nougat, the academic document PDF parser that understands LaTeX math and tables.&lt;/p&gt; &#xA;&lt;p&gt;Project page: &lt;a href=&#34;https://facebookresearch.github.io/nougat/&#34;&gt;https://facebookresearch.github.io/nougat/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;From pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nougat-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/facebookresearch/nougat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, on Windows: If you want to utilize a GPU, make sure you first install the correct PyTorch version. Follow instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;There are extra dependencies if you want to call the model from an API or generate a dataset. Install via&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install &#34;nougat-ocr[api]&#34;&lt;/code&gt; or &lt;code&gt;pip install &#34;nougat-ocr[dataset]&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Get prediction for a PDF&lt;/h3&gt; &#xA;&lt;h4&gt;CLI&lt;/h4&gt; &#xA;&lt;p&gt;To get predictions for a PDF run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ nougat path/to/file.pdf -o output_directory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A path to a directory or to a file where each line is a path to a PDF can also be passed as a positional argument&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ nougat path/to/directory -o output_directory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: nougat [-h] [--batchsize BATCHSIZE] [--checkpoint CHECKPOINT] [--model MODEL] [--out OUT]&#xA;              [--recompute] [--markdown] [--no-skipping] pdf [pdf ...]&#xA;&#xA;positional arguments:&#xA;  pdf                   PDF(s) to process.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --batchsize BATCHSIZE, -b BATCHSIZE&#xA;                        Batch size to use.&#xA;  --checkpoint CHECKPOINT, -c CHECKPOINT&#xA;                        Path to checkpoint directory.&#xA;  --model MODEL_TAG, -m MODEL_TAG&#xA;                        Model tag to use.&#xA;  --out OUT, -o OUT     Output directory.&#xA;  --recompute           Recompute already computed PDF, discarding previous predictions.&#xA;  --markdown            Add postprocessing step for markdown compatibility.&#xA;  --no-skipping         Don&#39;t apply failure detection heuristic.&#xA;  --pages PAGES, -p PAGES&#xA;                        Provide page numbers like &#39;1-4,7&#39; for pages 1 through 4 and page 7. Only works for single PDFs.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default model tag is &lt;code&gt;0.1.0-small&lt;/code&gt;. If you want to use the base model, use &lt;code&gt;0.1.0-base&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ nougat path/to/file.pdf -o output_directory -m 0.1.0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the output directory every PDF will be saved as a &lt;code&gt;.mmd&lt;/code&gt; file, the lightweight markup language, mostly compatible with &lt;a href=&#34;https://github.com/Mathpix/mathpix-markdown-it&#34;&gt;Mathpix Markdown&lt;/a&gt; (we make use of the LaTeX tables).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: On some devices the failure detection heuristic is not working properly. If you experience a lot of &lt;code&gt;[MISSING_PAGE]&lt;/code&gt; responses, try to run with the &lt;code&gt;--no-skipping&lt;/code&gt; flag. Related: &lt;a href=&#34;https://github.com/facebookresearch/nougat/issues/11&#34;&gt;#11&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/nougat/issues/67&#34;&gt;#67&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;API&lt;/h4&gt; &#xA;&lt;p&gt;With the extra dependencies you use &lt;code&gt;app.py&lt;/code&gt; to start an API. Call&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ nougat_api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get a prediction of a PDF file by making a POST request to &lt;a href=&#34;http://127.0.0.1:8503/predict/&#34;&gt;http://127.0.0.1:8503/predict/&lt;/a&gt;. It also accepts parameters &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;stop&lt;/code&gt; to limit the computation to select page numbers (boundaries are included).&lt;/p&gt; &#xA;&lt;p&gt;The response is a string with the markdown text of the document.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8503/predict/&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;file=@&amp;lt;PDFFILE.pdf&amp;gt;;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the limit the conversion to pages 1 to 5, use the start/stop parameters in the request URL: &lt;a href=&#34;http://127.0.0.1:8503/predict/?start=1&amp;amp;stop=5&#34;&gt;http://127.0.0.1:8503/predict/?start=1&amp;amp;stop=5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;Generate dataset&lt;/h3&gt; &#xA;&lt;p&gt;To generate a dataset you need&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A directory containing the PDFs&lt;/li&gt; &#xA; &lt;li&gt;A directory containing the &lt;code&gt;.html&lt;/code&gt; files (processed &lt;code&gt;.tex&lt;/code&gt; files by &lt;a href=&#34;https://math.nist.gov/~BMiller/LaTeXML/&#34;&gt;LaTeXML&lt;/a&gt;) with the same folder structure&lt;/li&gt; &#xA; &lt;li&gt;A binary file of &lt;a href=&#34;https://github.com/allenai/pdffigures2&#34;&gt;pdffigures2&lt;/a&gt; and a corresponding environment variable &lt;code&gt;export PDFFIGURES_PATH=&#34;/path/to/binary.jar&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Next run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nougat.dataset.split_htmls_to_pages --html path/html/root --pdfs path/pdf/root --out path/paired/output --figure path/pdffigures/outputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional arguments include&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Argument&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--recompute&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;recompute all splits&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--markdown MARKDOWN&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Markdown output dir&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--workers WORKERS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How many processes to use&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--dpi DPI&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What resolution the pages will be saved at&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--timeout TIMEOUT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;max time per paper in seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--tesseract&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tesseract OCR prediction for each page&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Finally create a &lt;code&gt;jsonl&lt;/code&gt; file that contains all the image paths, markdown text and meta information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nougat.dataset.create_index --dir path/paired/output --out index.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For each &lt;code&gt;jsonl&lt;/code&gt; file you also need to generate a seek map for faster data loading:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nougat.dataset.gen_seek file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting directory structure can look as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;root/&#xA;├── images&#xA;├── train.jsonl&#xA;├── train.seek.map&#xA;├── test.jsonl&#xA;├── test.seek.map&#xA;├── validation.jsonl&#xA;└── validation.seek.map&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;.mmd&lt;/code&gt; and &lt;code&gt;.json&lt;/code&gt; files in the &lt;code&gt;path/paired/output&lt;/code&gt; (here &lt;code&gt;images&lt;/code&gt;) are no longer required. This can be useful for pushing to a S3 bucket by halving the amount of files.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train or fine tune a Nougat model, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train.py --config config/train_nougat.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test.py --checkpoint path/to/checkpoint --dataset path/to/test.jsonl --save_path path/to/results.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the results for the different text modalities, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nougat.metrics path/to/results.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Why am I only getting &lt;code&gt;[MISSING_PAGE]&lt;/code&gt;?&lt;/p&gt; &lt;p&gt;Nougat was trained on scientific papers found on arXiv and PMC. Is the document you&#39;re processing similar to that? What language is the document in? Nougat works best with English papers, other Latin-based languages might work. &lt;strong&gt;Chinese, Russian, Japanese etc. will not work&lt;/strong&gt;. If these requirements are fulfilled it might be because of false positives in the failure detection, when computing on CPU or older GPUs (&lt;a href=&#34;https://github.com/facebookresearch/nougat/issues/11&#34;&gt;#11&lt;/a&gt;). Try passing the &lt;code&gt;--no-skipping&lt;/code&gt; flag for now.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Where can I download the model checkpoint from.&lt;/p&gt; &lt;p&gt;They are uploaded here on GitHub in the release section. You can also download them during the first execution of the program. Choose the preferred preferred model by passing &lt;code&gt;--model 0.1.0-{base,small}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{blecher2023nougat,&#xA;      title={Nougat: Neural Optical Understanding for Academic Documents}, &#xA;      author={Lukas Blecher and Guillem Cucurull and Thomas Scialom and Robert Stojnic},&#xA;      year={2023},&#xA;      eprint={2308.13418},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This repository builds on top of the &lt;a href=&#34;https://github.com/clovaai/donut/&#34;&gt;Donut&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Nougat codebase is licensed under MIT.&lt;/p&gt; &#xA;&lt;p&gt;Nougat model weights are licensed under CC-BY-NC.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>isocpp/CppCoreGuidelines</title>
    <updated>2023-10-01T02:21:14Z</updated>
    <id>tag:github.com,2023-10-01:/isocpp/CppCoreGuidelines</id>
    <link href="https://github.com/isocpp/CppCoreGuidelines" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The C++ Core Guidelines are a set of tried-and-true guidelines, rules, and best practices about coding in C++&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/cpp_core_guidelines_logo_text.png&#34; alt=&#34;C++ Core Guidelines&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;Within C++ is a smaller, simpler, safer language struggling to get out.&#34; -- &lt;cite&gt;Bjarne Stroustrup&lt;/cite&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/CppCoreGuidelines.md&#34;&gt;C++ Core Guidelines&lt;/a&gt; are a collaborative effort led by Bjarne Stroustrup, much like the C++ language itself. They are the result of many person-years of discussion and design across a number of organizations. Their design encourages general applicability and broad adoption but they can be freely copied and modified to meet your organization&#39;s needs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The guidelines themselves are found at &lt;a href=&#34;https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/CppCoreGuidelines.md&#34;&gt;CppCoreGuidelines&lt;/a&gt;. The document is in &lt;a href=&#34;https://github.github.com/gfm/&#34;&gt;GH-flavored MarkDown&lt;/a&gt;. It is intentionally kept simple, mostly in ASCII, to allow automatic post-processing such as language translation and reformatting. The editors maintain one &lt;a href=&#34;http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines&#34;&gt;version formatted for browsing&lt;/a&gt;. Note that it is manually integrated and can be slightly older than the version in the master branch.&lt;/p&gt; &#xA;&lt;p&gt;The Guidelines are a constantly evolving document without a strict &#34;release&#34; cadence. Bjarne Stroustrup periodically reviews the document and increments the version number in the introduction. &lt;a href=&#34;https://github.com/isocpp/CppCoreGuidelines/releases&#34;&gt;Checkins that increment the version number&lt;/a&gt; are tagged in git.&lt;/p&gt; &#xA;&lt;p&gt;Many of the guidelines make use of the header-only Guidelines Support Library. One implementation is available at &lt;a href=&#34;https://github.com/Microsoft/GSL&#34;&gt;GSL: Guidelines Support Library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Background and scope&lt;/h2&gt; &#xA;&lt;p&gt;The aim of the guidelines is to help people to use modern C++ effectively. By &#34;modern C++&#34; we mean C++11 and newer. In other words, what would you like your code to look like in 5 years&#39; time, given that you can start now? In 10 years&#39; time?&lt;/p&gt; &#xA;&lt;p&gt;The guidelines are focused on relatively higher-level issues, such as interfaces, resource management, memory management, and concurrency. Such rules affect application architecture and library design. Following the rules will lead to code that is statically type-safe, has no resource leaks, and catches many more programming logic errors than is common in code today. And it will run fast -- you can afford to do things right.&lt;/p&gt; &#xA;&lt;p&gt;We are less concerned with low-level issues, such as naming conventions and indentation style. However, no topic that can help a programmer is out of bounds.&lt;/p&gt; &#xA;&lt;p&gt;Our initial set of rules emphasizes safety (of various forms) and simplicity. They may very well be too strict. We expect to have to introduce more exceptions to better accommodate real-world needs. We also need more rules.&lt;/p&gt; &#xA;&lt;p&gt;You will find some of the rules contrary to your expectations or even contrary to your experience. If we haven&#39;t suggested that you change your coding style in any way, we have failed! Please try to verify or disprove rules! In particular, we&#39;d really like to have some of our rules backed up with measurements or better examples.&lt;/p&gt; &#xA;&lt;p&gt;You will find some of the rules obvious or even trivial. Please remember that one purpose of a guideline is to help someone who is less experienced or coming from a different background or language to get up to speed.&lt;/p&gt; &#xA;&lt;p&gt;The rules are designed to be supported by an analysis tool. Violations of rules will be flagged with references (or links) to the relevant rule. We do not expect you to memorize all the rules before trying to write code.&lt;/p&gt; &#xA;&lt;p&gt;The rules are meant for gradual introduction into a code base. We plan to build tools for that and hope others will too.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;Comments and suggestions for improvements are most welcome. We plan to modify and extend this document as our understanding improves and the language and the set of available libraries improve. More details are found at &lt;a href=&#34;https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.digitalocean.com/?refcode=32f291566cf7&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=CopyPaste&#34;&gt;DigitalOcean&lt;/a&gt; for hosting the Standard C++ Foundation website.&lt;/p&gt;</summary>
  </entry>
</feed>