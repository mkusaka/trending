<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-01T01:50:57Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zeyi-Lin/HivisionIDPhotos</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/Zeyi-Lin/HivisionIDPhotos</id>
    <link href="https://github.com/Zeyi-Lin/HivisionIDPhotos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;⚡️HivisionIDPhotos: a lightweight and efficient AI ID photos tools. 一个轻量级的AI证件照制作算法。&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;hivision_logo&#34; src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/hivision_logo.png&#34; width=&#34;120&#34; height=&#34;120&#34;&gt; &#xA; &lt;h1&gt;HivisionIDPhoto&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_EN.md&#34;&gt;English&lt;/a&gt; / 中文 / &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_JP.md&#34;&gt;日本語&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/README_KO.md&#34;&gt;한국어&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&amp;amp;labelColor=black&amp;amp;logo=github&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/linzeyi/hivision_idphotos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&amp;amp;label=docker&amp;amp;labelColor=black&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zeyi-lin/hivisionidphotos/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://docs.qq.com/doc/DUkpBdk90eWZFS2JW&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-%E5%BE%AE%E4%BF%A1-4cb55e&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo&#34;&gt;&lt;img src=&#34;https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&amp;amp;path=swanhub.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelers.cn/spaces/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;amp;ytag=HG_GPU_HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/11622&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/11622&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6&#34;&gt;&lt;img src=&#34;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&amp;amp;claim_uid=Oh5UaGjfrblg0yZ&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/demoImage.jpg&#34; width=&#34;900&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;相关项目&lt;/strong&gt;：&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/SwanHubX/SwanLab&#34;&gt;SwanLab&lt;/a&gt;：一个开源、现代化设计的深度学习训练跟踪与可视化工具，同时支持云端/离线使用，国内好用的Wandb平替；适配30+主流框架（PyTorch、HuggingFace Transformers、LLaMA Factory、Lightning等），欢迎使用！&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;目录&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E6%9C%80%E8%BF%91%E6%9B%B4%E6%96%B0&#34;&gt;最近更新&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B&#34;&gt;项目简介&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E7%A4%BE%E5%8C%BA&#34;&gt;社区&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C&#34;&gt;准备工作&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E8%BF%90%E8%A1%8C-gradio-demo&#34;&gt;Demo启动&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-python-%E6%8E%A8%E7%90%86&#34;&gt;Python推理&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#%EF%B8%8F-%E9%83%A8%E7%BD%B2-api-%E6%9C%8D%E5%8A%A1&#34;&gt;API服务部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-docker-%E9%83%A8%E7%BD%B2&#34;&gt;Docker部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC&#34;&gt;联系我们&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E6%84%9F%E8%B0%A2%E6%94%AF%E6%8C%81&#34;&gt;感谢支持&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-lincese&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#-%E5%BC%95%E7%94%A8&#34;&gt;引用&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🤩 最近更新&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;在线体验： &lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=SwanHub%20Demo&amp;amp;color=blue&#34; alt=&#34;SwanHub Demo&#34;&gt;&lt;/a&gt;、&lt;a href=&#34;https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Spaces&#34;&gt;&lt;/a&gt;、&lt;a href=&#34;https://modelscope.cn/studios/SwanLab/HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&#34; alt=&#34;&#34;&gt;&lt;/a&gt;、&lt;a href=&#34;https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;amp;ytag=HG_GPU_HivisionIDPhotos&#34;&gt;&lt;img src=&#34;https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.11.20: Gradio Demo增加&lt;strong&gt;打印排版&lt;/strong&gt;选项卡，支持六寸、五寸、A4、3R、4R五种排版尺寸&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.11.16: API接口增加美颜参数&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.25: 增加&lt;strong&gt;五寸相纸&lt;/strong&gt;和&lt;strong&gt;JPEG下载&lt;/strong&gt;选项｜默认照片下载支持300DPI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.24: API接口增加base64图像传入选项 | Gradio Demo增加&lt;strong&gt;排版照裁剪线&lt;/strong&gt;功能&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.22: Gradio Demo增加&lt;strong&gt;野兽模式&lt;/strong&gt;，可设置内存加载策略 | API接口增加&lt;strong&gt;dpi、face_alignment&lt;/strong&gt;参数&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.18: Gradio Demo增加&lt;strong&gt;分享模版照&lt;/strong&gt;功能、增加&lt;strong&gt;美式证件照&lt;/strong&gt;背景选项&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.17: Gradio Demo增加&lt;strong&gt;自定义底色-HEX输入&lt;/strong&gt;功能 | &lt;strong&gt;（社区贡献）C++版本&lt;/strong&gt; - &lt;a href=&#34;https://github.com/zjkhahah/HivisionIDPhotos-cpp&#34;&gt;HivisionIDPhotos-cpp&lt;/a&gt; 贡献 by &lt;a href=&#34;https://github.com/zjkhahah&#34;&gt;zjkhahah&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024.09.16: Gradio Demo增加&lt;strong&gt;人脸旋转对齐&lt;/strong&gt;功能，自定义尺寸输入支持&lt;strong&gt;毫米&lt;/strong&gt;单位&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;项目简介&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🚀 谢谢你对我们的工作感兴趣。您可能还想查看我们在图像领域的其他成果，欢迎来信:&lt;a href=&#34;mailto:zeyi.lin@swanhub.co&#34;&gt;zeyi.lin@swanhub.co&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;HivisionIDPhoto 旨在开发一种实用、系统性的证件照智能制作算法。&lt;/p&gt; &#xA;&lt;p&gt;它利用一套完善的AI模型工作流程，实现对多种用户拍照场景的识别、抠图与证件照生成。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HivisionIDPhoto 可以做到：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;轻量级抠图（纯离线，仅需 &lt;strong&gt;CPU&lt;/strong&gt; 即可快速推理）&lt;/li&gt; &#xA; &lt;li&gt;根据不同尺寸规格生成不同的标准证件照、六寸排版照&lt;/li&gt; &#xA; &lt;li&gt;支持 纯离线 或 端云 推理&lt;/li&gt; &#xA; &lt;li&gt;美颜&lt;/li&gt; &#xA; &lt;li&gt;智能换正装（waiting）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/demo.png&#34; width=&#34;900&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;如果 HivisionIDPhoto 对你有帮助，请 star 这个 repo 或推荐给你的朋友，解决证件照应急制作问题！&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🏠 社区&lt;/h1&gt; &#xA;&lt;p&gt;我们分享了一些由社区构建的HivisionIDPhotos的有趣应用和扩展：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&#34;&gt;HivisionIDPhotos-ComfyUI&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt;HivisionIDPhotos-wechat-weapp&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/comfyui.png&#34; width=&#34;900&#34; alt=&#34;ComfyUI workflow&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-wechat-miniprogram.png&#34; width=&#34;900&#34; alt=&#34;ComfyUI workflow&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ComfyUI证件照处理工作流&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;证件照微信小程序（JAVA后端+原生前端）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/soulerror/HivisionIDPhotos-Uniapp&#34;&gt;HivisionIDPhotos-Uniapp&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-web&#34;&gt;HivisionIDPhotos-web&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/soulerror/HivisionIDPhotos-Uniapp&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-uniapp-wechat-miniprogram.png&#34; width=&#34;900&#34; alt=&#34;HivisionIDPhotos-uniapp&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-web&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/community-web.png&#34; width=&#34;900&#34; alt=&#34;HivisionIDPhotos-uniapp&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;证件照微信小程序（uniapp）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;证件照应用网页版&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zjkhahah/HivisionIDPhotos-cpp&#34;&gt;HivisionIDPhotos-cpp&lt;/a&gt;: HivisionIDphotos C++版本，由 &lt;a href=&#34;https://github.com/zjkhahah&#34;&gt;zjkhahah&lt;/a&gt; 构建&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wmlcjj/ai-idphoto&#34;&gt;ai-idphoto&lt;/a&gt;: &lt;a href=&#34;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&#34;&gt;HivisionIDPhotos-wechat-weapp&lt;/a&gt; 的uniapp多端兼容版，由 &lt;a href=&#34;https://github.com/wmlcjj&#34;&gt;wmlcjj&lt;/a&gt; 贡献&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/&#34;&gt;HivisionIDPhotos-uniapp-WeChat-gpto1&lt;/a&gt;: 由gpt-o1辅助完成开发的证件照微信小程序，由 &lt;a href=&#34;https://github.com/jkm199&#34;&gt;jkm199&lt;/a&gt; 贡献&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI&#34;&gt;HivisionIDPhotos-windows-GUI&lt;/a&gt;：Windows客户端应用，由 &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;zhaoyun0071&lt;/a&gt; 构建&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ONG-Leo/HivisionIDPhotos-NAS&#34;&gt;HivisionIDPhotos-NAS&lt;/a&gt;: 群晖NAS部署中文教程，由 &lt;a href=&#34;https://github.com/ONG-Leo&#34;&gt;ONG-Leo&lt;/a&gt; 贡献&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🔧 准备工作&lt;/h1&gt; &#xA;&lt;p&gt;环境安装与依赖：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7（项目主要测试在 python 3.10）&lt;/li&gt; &#xA; &lt;li&gt;OS: Linux, Windows, MacOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. 克隆项目&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git&#xA;cd  HivisionIDPhotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. 安装依赖环境&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;建议 conda 创建一个 python3.10 虚拟环境后，执行以下命令&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;pip install -r requirements-app.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. 下载人像抠图模型权重文件&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;方式一：脚本下载&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/download_model.py --models all&#xA;# 如需指定下载某个模型&#xA;# python scripts/download_model.py --models modnet_photographic_portrait_matting&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;方式二：直接下载&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;模型均存到项目的&lt;code&gt;hivision/creator/weights&lt;/code&gt;目录下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;人像抠图模型&lt;/th&gt; &#xA;   &lt;th&gt;介绍&lt;/th&gt; &#xA;   &lt;th&gt;下载&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;官方权重&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx&#34;&gt;下载&lt;/a&gt;(24.7MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hivision_modnet&lt;/td&gt; &#xA;   &lt;td&gt;对纯色换底适配性更好的抠图模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx&#34;&gt;下载&lt;/a&gt;(24.7MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rmbg-1.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/briaai/RMBG-1.4&#34;&gt;BRIA AI&lt;/a&gt; 开源的抠图模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true&#34;&gt;下载&lt;/a&gt;(176.2MB)后重命名为&lt;code&gt;rmbg-1.4.onnx&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;birefnet-v1-lite&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet&#34;&gt;ZhengPeng7&lt;/a&gt; 开源的抠图模型，拥有最好的分割精度&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx&#34;&gt;下载&lt;/a&gt;(224MB)后重命名为&lt;code&gt;birefnet-v1-lite.onnx&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果下载网速不顺利：前往&lt;a href=&#34;https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main&#34;&gt;SwanHub&lt;/a&gt;下载。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;4. 人脸检测模型配置（可选）&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;拓展人脸检测模型&lt;/th&gt; &#xA;   &lt;th&gt;介绍&lt;/th&gt; &#xA;   &lt;th&gt;使用文档&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MTCNN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;离线&lt;/strong&gt;人脸检测模型，高性能CPU推理（毫秒级），为默认模型，检测精度较低&lt;/td&gt; &#xA;   &lt;td&gt;Clone此项目后直接使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;离线&lt;/strong&gt;人脸检测模型，CPU推理速度中等（秒级），精度较高&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx&#34;&gt;下载&lt;/a&gt;后放到&lt;code&gt;hivision/creator/retinaface/weights&lt;/code&gt;目录下&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Face++&lt;/td&gt; &#xA;   &lt;td&gt;旷视推出的在线人脸检测API，检测精度较高，&lt;a href=&#34;https://console.faceplusplus.com.cn/documents/4888373&#34;&gt;官方文档&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/face++_CN.md&#34;&gt;使用文档&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;5. 性能参考&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;测试环境为Mac M1 Max 64GB，非GPU加速，测试图片分辨率为 512x715(1) 与 764×1146(2)。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型组合&lt;/th&gt; &#xA;   &lt;th&gt;内存占用&lt;/th&gt; &#xA;   &lt;th&gt;推理时长(1)&lt;/th&gt; &#xA;   &lt;th&gt;推理时长(2)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet + mtcnn&lt;/td&gt; &#xA;   &lt;td&gt;410MB&lt;/td&gt; &#xA;   &lt;td&gt;0.207s&lt;/td&gt; &#xA;   &lt;td&gt;0.246s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MODNet + retinaface&lt;/td&gt; &#xA;   &lt;td&gt;405MB&lt;/td&gt; &#xA;   &lt;td&gt;0.571s&lt;/td&gt; &#xA;   &lt;td&gt;0.971s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;birefnet-v1-lite + retinaface&lt;/td&gt; &#xA;   &lt;td&gt;6.20GB&lt;/td&gt; &#xA;   &lt;td&gt;7.063s&lt;/td&gt; &#xA;   &lt;td&gt;7.128s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;6. GPU推理加速（可选）&lt;/h2&gt; &#xA;&lt;p&gt;在当前版本，可被英伟达GPU加速的模型为&lt;code&gt;birefnet-v1-lite&lt;/code&gt;，并请确保你有16GB左右的显存。&lt;/p&gt; &#xA;&lt;p&gt;如需使用英伟达GPU加速推理，在确保你已经安装&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;与&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;后，根据&lt;a href=&#34;https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x&#34;&gt;onnxruntime-gpu文档&lt;/a&gt;找到对应的&lt;code&gt;onnxruntime-gpu&lt;/code&gt;版本安装，以及根据&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;pytorch官网&lt;/a&gt;找到对应的&lt;code&gt;torch&lt;/code&gt;版本安装。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 假如你的电脑安装的是CUDA 12.x, cuDNN 8&#xA;# 安装torch是可选的，如果你始终配置不好cuDNN，那么试试安装torch&#xA;pip install onnxruntime-gpu==1.18.0&#xA;pip install torch --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;完成安装后，调用&lt;code&gt;birefnet-v1-lite&lt;/code&gt;模型即可利用GPU加速推理。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;TIPS: CUDA 支持向下兼容。比如你的 CUDA 版本为 12.6，&lt;code&gt;torch&lt;/code&gt; 官方目前支持的最高版本为 12.4（&amp;lt;12.6），&lt;code&gt;torch&lt;/code&gt;仍可以正常使用CUDA。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;⚡️ 运行 Gradio Demo&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行程序将生成一个本地 Web 页面，在页面中可完成证件照的操作与交互。&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/harry.png&#34; width=&#34;900&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🚀 Python 推理&lt;/h1&gt; &#xA;&lt;p&gt;核心参数：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-i&lt;/code&gt;: 输入图像路径&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-o&lt;/code&gt;: 保存图像路径&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;: 推理类型，有idphoto、human_matting、add_background、generate_layout_photos可选&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--matting_model&lt;/code&gt;: 人像抠图模型权重选择&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--face_detect_model&lt;/code&gt;: 人脸检测模型选择&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;更多参数可通过&lt;code&gt;python inference.py --help&lt;/code&gt;查看&lt;/p&gt; &#xA;&lt;h2&gt;1. 证件照制作&lt;/h2&gt; &#xA;&lt;p&gt;输入 1 张照片，获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. 人像抠图&lt;/h2&gt; &#xA;&lt;p&gt;输入 1 张照片，获得 1张 4 通道透明 png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. 透明图增加底色&lt;/h2&gt; &#xA;&lt;p&gt;输入 1 张 4 通道透明 png，获得 1 张增加了底色的 3通道图像&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. 得到六寸排版照&lt;/h2&gt; &#xA;&lt;p&gt;输入 1 张 3 通道照片，获得 1 张六寸排版照&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. 证件照裁剪&lt;/h2&gt; &#xA;&lt;p&gt;输入 1 张 4 通道照片（抠图好的图像），获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;⚡️ 部署 API 服务&lt;/h1&gt; &#xA;&lt;h2&gt;启动后端&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python deploy_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;请求 API 服务&lt;/h2&gt; &#xA;&lt;p&gt;详细请求方式请参考 &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md&#34;&gt;API 文档&lt;/a&gt;，包含以下请求示例：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md#curl-%E8%AF%B7%E6%B1%82%E7%A4%BA%E4%BE%8B&#34;&gt;cURL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/docs/api_CN.md#python-%E8%AF%B7%E6%B1%82%E7%A4%BA%E4%BE%8B&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🐳 Docker 部署&lt;/h1&gt; &#xA;&lt;h2&gt;1. 拉取或构建镜像&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;以下方式三选一&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;方式一：拉取最新镜像：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull linzeyi/hivision_idphotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;方式二：Dockrfile 直接构建镜像：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在确保将至少一个&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#3-%E4%B8%8B%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6&#34;&gt;抠图模型权重文件&lt;/a&gt;放到&lt;code&gt;hivision/creator/weights&lt;/code&gt;下后，在项目根目录执行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t linzeyi/hivision_idphotos .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;方式三：Docker compose 构建：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在确保将至少一个&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/#3-%E4%B8%8B%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6&#34;&gt;抠图模型权重文件&lt;/a&gt;放到&lt;code&gt;hivision/creator/weights&lt;/code&gt;下后，在项目根目录下执行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. 运行服务&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;启动 Gradio Demo 服务&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;运行下面的命令，在你的本地访问 &lt;a href=&#34;http://127.0.0.1:7860/&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; 即可使用。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 7860:7860 linzeyi/hivision_idphotos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;启动 API 后端服务&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;两个服务同时启动&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;环境变量&lt;/h2&gt; &#xA;&lt;p&gt;本项目提供了一些额外的配置项，使用环境变量进行设置：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;环境变量&lt;/th&gt; &#xA;   &lt;th&gt;类型&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;th&gt;示例&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FACE_PLUS_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;可选&lt;/td&gt; &#xA;   &lt;td&gt;这是你在 Face++ 控制台申请的 API 密钥&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;7-fZStDJ····&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FACE_PLUS_API_SECRET&lt;/td&gt; &#xA;   &lt;td&gt;可选&lt;/td&gt; &#xA;   &lt;td&gt;Face++ API密钥对应的Secret&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VTee824E····&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RUN_MODE&lt;/td&gt; &#xA;   &lt;td&gt;可选&lt;/td&gt; &#xA;   &lt;td&gt;运行模式，可选值为&lt;code&gt;beast&lt;/code&gt;(野兽模式)。野兽模式下人脸检测和抠图模型将不释放内存，从而获得更快的二次推理速度。建议内存16GB以上尝试。&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;beast&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DEFAULT_LANG&lt;/td&gt; &#xA;   &lt;td&gt;可选&lt;/td&gt; &#xA;   &lt;td&gt;Gradio Demo启动时的默认语言&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;docker使用环境变量示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run  -d -p 7860:7860 \&#xA;    -e FACE_PLUS_API_KEY=7-fZStDJ···· \&#xA;    -e FACE_PLUS_API_SECRET=VTee824E···· \&#xA;    -e RUN_MODE=beast \&#xA;    -e DEFAULT_LANG=en \&#xA;    linzeyi/hivision_idphotos  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;1. 如何修改预设尺寸和颜色？&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;尺寸：修改&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/demo/assets/size_list_CN.csv&#34;&gt;size_list_CN.csv&lt;/a&gt;后再次运行 &lt;code&gt;app.py&lt;/code&gt; 即可，其中第一列为尺寸名，第二列为高度，第三列为宽度。&lt;/li&gt; &#xA; &lt;li&gt;颜色：修改&lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/demo/assets/color_list_CN.csv&#34;&gt;color_list_CN.csv&lt;/a&gt;后再次运行 &lt;code&gt;app.py&lt;/code&gt; 即可，其中第一列为颜色名，第二列为Hex值。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. 如何修改水印字体？&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;将字体文件放到&lt;code&gt;hivision/plugin/font&lt;/code&gt;文件夹下&lt;/li&gt; &#xA; &lt;li&gt;修改&lt;code&gt;hivision/plugin/watermark.py&lt;/code&gt;的&lt;code&gt;font_file&lt;/code&gt;参数值为字体文件名&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;3. 如何添加社交媒体模板照？&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;将模板图片放到&lt;code&gt;hivision/plugin/template/assets&lt;/code&gt;文件夹下。模板图片是一个4通道的透明png。&lt;/li&gt; &#xA; &lt;li&gt;在&lt;code&gt;hivision/plugin/template/assets/template_config.json&lt;/code&gt;文件中添加最新的模板信息，其中&lt;code&gt;width&lt;/code&gt;为模板图宽度(px)，&lt;code&gt;height&lt;/code&gt;为模板图高度(px)，&lt;code&gt;anchor_points&lt;/code&gt;为模板中透明区域的四个角的坐标(px)；&lt;code&gt;rotation&lt;/code&gt;为透明区域相对于垂直方向的旋转角度，&amp;gt;0为逆时针，&amp;lt;0为顺时针。&lt;/li&gt; &#xA; &lt;li&gt;在&lt;code&gt;demo/processor.py&lt;/code&gt;的&lt;code&gt;_generate_image_template&lt;/code&gt;函数中的&lt;code&gt;TEMPLATE_NAME_LIST&lt;/code&gt;变量添加最新的模板名&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/assets/social_template.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;h2&gt;4. 如何修改Gradio Demo的顶部导航栏？&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修改&lt;code&gt;demo/assets/title.md&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. 如何添加/修改「打印排版」中的尺寸？&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修改&lt;code&gt;demo/locales.py&lt;/code&gt;中的&lt;code&gt;print_switch&lt;/code&gt;字典，添加/修改新的尺寸名称和尺寸参数，然后重新运行&lt;code&gt;python app.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;📧 联系我们&lt;/h1&gt; &#xA;&lt;p&gt;如果您有任何问题，请发邮件至 &lt;a href=&#34;mailto:zeyi.lin@swanhub.co&#34;&gt;zeyi.lin@swanhub.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🙏 感谢支持&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos&#34; alt=&#34;Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members&#34;&gt;&lt;img src=&#34;https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos&#34; alt=&#34;Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;贡献者们：&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeyi-Lin&#34;&gt;Zeyi-Lin&lt;/a&gt;、&lt;a href=&#34;https://github.com/SAKURA-CAT&#34;&gt;SAKURA-CAT&lt;/a&gt;、&lt;a href=&#34;https://github.com/Feudalman&#34;&gt;Feudalman&lt;/a&gt;、&lt;a href=&#34;https://github.com/swpfY&#34;&gt;swpfY&lt;/a&gt;、&lt;a href=&#34;https://github.com/Kaikaikaifang&#34;&gt;Kaikaikaifang&lt;/a&gt;、&lt;a href=&#34;https://github.com/ShaohonChen&#34;&gt;ShaohonChen&lt;/a&gt;、&lt;a href=&#34;https://github.com/KashiwaByte&#34;&gt;KashiwaByte&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;📜 Lincese&lt;/h1&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Zeyi-Lin/HivisionIDPhotos/master/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;📚 引用&lt;/h1&gt; &#xA;&lt;p&gt;如果您在研究或项目中使用了HivisionIDPhotos，请考虑引用我们的工作。您可以使用以下BibTeX条目：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hivisionidphotos,&#xA;      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},&#xA;      author={Zeyi Lin and SwanLab Team},&#xA;      year={2024},&#xA;      publisher={GitHub},&#xA;      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- 微信群链接 --&gt; &#xA;&lt;!-- Github Release --&gt; &#xA;&lt;!-- 社区项目链接 --&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/qlib</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/microsoft/qlib</id>
    <link href="https://github.com/microsoft/qlib" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pyqlib/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/#history&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyqlib&#34; alt=&#34;PypI Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyqlib/&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true&#34; alt=&#34;Upload Python Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/actions&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main&#34; alt=&#34;Github Actions Test Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/qlib/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/pyqlib&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/Microsoft/qlib&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📰&lt;/span&gt; &lt;strong&gt;What&#39;s NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;💖&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Recent released features&lt;/p&gt; &#xA;&lt;h3&gt;Introducing &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png&#34; alt=&#34;RD_Agent&#34; style=&#34;height: 2em&#34;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; &#xA;&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; &#xA;&lt;p&gt;RD-Agent is now available on &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;GitHub&lt;/a&gt;, and we welcome your star🌟!&lt;/p&gt; &#xA;&lt;p&gt;To learn more, please visit our &lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;♾️Demo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; &#xA;&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scenario&lt;/th&gt; &#xA;   &lt;th&gt;Demo video (English)&lt;/th&gt; &#xA;   &lt;th&gt;Demo video (中文)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Factor Mining&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quant Model Optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop?lang=en&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop?lang=zh&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📃&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2505.15155&#34;&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;👾&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/microsoft/RD-Agent/&#34;&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{li2025rdagentquant,&#xA;    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},&#xA;    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},&#xA;    year={2025},&#xA;    eprint={2505.15155},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2505.15155&#34;&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; &#xA;   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; &#xA;   &lt;td&gt;📈Coming soon!(&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1863&#34;&gt;Under review&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🔥LLM-driven Auto Quant Factory🔥&lt;/td&gt; &#xA;   &lt;td&gt;🚀 Released in &lt;a href=&#34;https://github.com/microsoft/RD-Agent&#34;&gt;♾️RD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1414/&#34;&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.9.0&#34;&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL Learning Framework&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;span&gt;📈&lt;/span&gt; Released on Nov 10, 2022. &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1332&#34;&gt;#1332&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1322&#34;&gt;#1322&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1316&#34;&gt;#1316&lt;/a&gt;,&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1299&#34;&gt;#1299&lt;/a&gt;,&lt;a href=&#34;https://github.com/microsoft/qlib/pull/1263&#34;&gt;#1263&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1244&#34;&gt;#1244&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1169&#34;&gt;#1169&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1125&#34;&gt;#1125&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1076&#34;&gt;#1076&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1040&#34;&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qlib &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/examples/tutorial&#34;&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;📖 &lt;a href=&#34;https://github.com/microsoft/qlib/pull/1037&#34;&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ibovespa index data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/990&#34;&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Point-in-Time database&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/343&#34;&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/744&#34;&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/743&#34;&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/754&#34;&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.8.0&#34;&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ADD model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/704&#34;&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ADARNN model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/689&#34;&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TCN model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/668&#34;&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nested Decision Framework&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/438&#34;&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href=&#34;https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py&#34;&gt;Example&lt;/a&gt; and &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/highfreq.html&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/531&#34;&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/508&#34;&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/microsoft/qlib/releases/tag/v0.7.0&#34;&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TCTS Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/491&#34;&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/290&#34;&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/286&#34;&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency data processing example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/257&#34;&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency trading example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/227&#34;&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/221&#34;&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tabnet Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/205&#34;&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#39;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; &#xA;&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href=&#34;https://arxiv.org/abs/2009.11189&#34;&gt;&#34;Qlib: An AI-oriented Quantitative Investment Platform&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; &#xA;   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#plans&#34;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib&#34;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;ul dir=&#34;auto&#34;&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#installation&#34;&gt;Installation&lt;/a&gt; &lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow&#34;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code&#34;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo&#34;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework&#34;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib&#34;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode&#34;&gt;Offline Mode and Online Mode&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server&#34;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports&#34;&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us&#34;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; &#xA;   &lt;td valign=&#34;baseline&#34;&gt; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research&#34;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns&#34;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; &#xA;       &lt;ul&gt; &#xA;        &lt;li type=&#34;disc&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo&#34;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; &#xA;         &lt;ul&gt; &#xA;          &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model&#34;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; &#xA;          &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models&#34;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; &#xA;         &lt;/ul&gt; &lt;/li&gt; &#xA;       &lt;/ul&gt; &lt;/li&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics&#34;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li type=&#34;circle&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions&#34;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Plans&lt;/h1&gt; &#xA;&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; &#xA;&lt;!-- | Feature                        | Status      | --&gt; &#xA;&lt;!-- | --                      | ------    | --&gt; &#xA;&lt;h1&gt;Framework of Qlib&lt;/h1&gt; &#xA;&lt;div style=&#34;align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework&#34;&gt;detailed framework&lt;/a&gt; of Qlib&#39;s design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; &#xA;&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html&#34;&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl.html&#34;&gt;reinforcement learning&lt;/a&gt;, &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section&#34;&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/meta.html&#34;&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/strategy.html&#34;&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/highfreq.html&#34;&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/report.html&#34;&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/online.html&#34;&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It&#39;s very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href=&#34;https://terminalizer.com/view/3f24561a4470&#34;&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&#34;&gt;instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;install with pip&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;install from source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;plot&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; &#xA; &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;&#39;s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pip install pyqlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; &#xA;&lt;h3&gt;Install from source&lt;/h3&gt; &#xA;&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install numpy&#xA;pip install --upgrade cython&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib&#xA;pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml&#34;&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href=&#34;https://github.com/chenditc/investment_data/releases&#34;&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz&#xA;mkdir -p ~/.qlib/qlib_data/cn_data&#xA;tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1&#xA;rm -f qlib_bin.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; &#xA;&lt;h3&gt;Get with module&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get 1d data&#xA;python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn&#xA;&#xA;# get 1min data&#xA;python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get from source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get 1d data&#xA;python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn&#xA;&#xA;# get 1min data&#xA;python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This dataset is created by public data collected by &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/&#34;&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset&#34;&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href=&#34;https://finance.yahoo.com/lookup&#34;&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format&#34;&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; &#xA; &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can&#39;t incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&#34;&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; &#xA; &lt;p&gt;For more information, please refer to: &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&#34;&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Automatic update of data to the &#34;qlib&#34; directory each trading day(Linux)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Checking the health of the data&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &#xA;- Run the initialization code and get stock data:&#xA;&#xA;  ```python&#xA;  import qlib&#xA;  from qlib.data import D&#xA;  from qlib.constant import REG_CN&#xA;&#xA;  # Initialization&#xA;  mount_path = &#34;~/.qlib/qlib_data/cn_data&#34;  # target_dir&#xA;  qlib.init(mount_path=mount_path, region=REG_CN)&#xA;&#xA;  # Get stock data by Qlib&#xA;  # Load trading calendar with the given time range and frequency&#xA;  print(D.calendar(start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;)[:2])&#xA;&#xA;  # Parse a given market name into a stockpool config&#xA;  instruments = D.instruments(&#39;csi500&#39;)&#xA;  print(D.list_instruments(instruments=instruments, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, as_list=True)[:6])&#xA;&#xA;  # Load features of certain instruments in given time range&#xA;  instruments = [&#39;SH600000&#39;]&#xA;  fields = [&#39;$close&#39;, &#39;$volume&#39;, &#39;Ref($close, 1)&#39;, &#39;Mean($close, 3)&#39;, &#39;$high-$low&#39;]&#xA;  print(D.features(instruments, fields, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;).head())&#xA;  ```&#xA; --&gt; &#xA;&lt;h2&gt;Docker images&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull pyqlib/qlib_image_stable:stable&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn&#xA;&amp;gt;&amp;gt;&amp;gt; python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; exit&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start -i -a &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker stop &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker rm &amp;lt;container name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want to know more information, please refer to the &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; &#xA;&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#34;&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  cd examples  # Avoid running program under the directory contains `qlib`&#xA;  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/strategy.html#result&#34;&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#39;The following are analysis results of the excess return without cost.&#39;&#xA;                       risk&#xA;mean               0.000708&#xA;std                0.005626&#xA;annualized_return  0.178316&#xA;information_ratio  1.996555&#xA;max_drawdown      -0.081806&#xA;&#39;The following are analysis results of the excess return with cost.&#39;&#xA;                       risk&#xA;mean               0.000512&#xA;std                0.005626&#xA;annualized_return  0.128982&#xA;information_ratio  1.444287&#xA;max_drawdown      -0.091078&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/workflow.html&#34;&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Cumulative Return of groups &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png&#34; alt=&#34;Cumulative Return&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Return distribution &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png&#34; alt=&#34;long_short&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Information Coefficient (IC) &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png&#34; alt=&#34;Information Coefficient&#34;&gt; &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png&#34; alt=&#34;Monthly IC&#34;&gt; &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png&#34; alt=&#34;IC&#34;&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png&#34; alt=&#34;Auto Correlation&#34;&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Backtest return &lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png&#34; alt=&#34;Report&#34;&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;!-- &#xA;- Score IC&#xA;![Score IC](docs/_static/img/score_ic.png)&#xA;- Cumulative Return&#xA;![Cumulative Return](docs/_static/img/cumulative_return.png)&#xA;- Risk Analysis&#xA;![Risk Analysis](docs/_static/img/risk_analysis.png)&#xA;- Rank Label&#xA;![Rank Label](docs/_static/img/rank_label.png)&#xA;--&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/report.html&#34;&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; &#xA;&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb&#34;&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; &#xA;&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; &#xA;&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; &#xA;&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; &#xA;&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; &#xA;&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&#34;&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/&#34;&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/&#34;&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/&#34;&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/&#34;&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/&#34;&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/&#34;&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM&#34;&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/&#34;&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/&#34;&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/&#34;&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/&#34;&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/&#34;&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/&#34;&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/&#34;&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/&#34;&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/&#34;&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/&#34;&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/&#34;&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/&#34;&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/&#34;&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/&#34;&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/&#34;&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/&#34;&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; &#xA;&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run a single model&lt;/h3&gt; &#xA;&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&#34;&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model&#39;s workflow based from a config file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py&#34;&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/&#34;&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run multiple models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn&#39;t support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; &#xA;&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python run_all_model.py run 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&#34;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Break change&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn&#39;t guarantee that some programmes will run correctly, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; &#xA; &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; &#xA; &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic&#34;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies&#39; performance.&lt;/p&gt; &#xA;&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/&#34;&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/&#34;&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; &#xA;&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; &#xA;&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&#34;&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution&#34;&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml&#34;&gt;TWAP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml&#34;&gt;PPO: &#34;An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization&#34;, IJCAL 2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml&#34;&gt;OPDS: &#34;Universal Trading for Order Execution with Oracle Policy Distillation&#34;, AAAI 2021&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; &#xA;&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;US Market&lt;/th&gt; &#xA;   &lt;th&gt;China Market&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&#34;&gt;Alpha360&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;√&lt;/td&gt; &#xA;   &lt;td&gt;√&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&#34;&gt;Alpha158&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;√&lt;/td&gt; &#xA;   &lt;td&gt;√&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qlib.readthedocs.io/en/latest/advanced/alpha.html&#34;&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; &#xA;&lt;h1&gt;Learning Framework&lt;/h1&gt; &#xA;&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/model.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href=&#34;https://qlib.readthedocs.io/en/latest/component/rl.html&#34;&gt;here&lt;/a&gt;. Qlib&#39;s RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It&#39;s worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;More About Qlib&lt;/h1&gt; &#xA;&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The detailed documents are organized in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/&#34;&gt;docs&lt;/a&gt;. &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;conda install sphinx sphinx_rtd_theme -y&#xA;# Otherwise, you can install them with pip&#xA;# pip install sphinx sphinx_rtd_theme&#xA;make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also view the &lt;a href=&#34;http://qlib.readthedocs.io/&#34;&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; &#xA;&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href=&#34;https://github.com/microsoft/qlib/projects/1&#34;&gt;github project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; &#xA;&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; &#xA;&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; &#xA;&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href=&#34;https://qlib-server.readthedocs.io/&#34;&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href=&#34;https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure&#34;&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href=&#34;https://github.com/microsoft/qlib-server&#34;&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; &#xA;&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; &#xA;&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;HDF5&lt;/th&gt; &#xA;   &lt;th&gt;MySQL&lt;/th&gt; &#xA;   &lt;th&gt;MongoDB&lt;/th&gt; &#xA;   &lt;th&gt;InfluxDB&lt;/th&gt; &#xA;   &lt;th&gt;Qlib -E -D&lt;/th&gt; &#xA;   &lt;th&gt;Qlib +E -D&lt;/th&gt; &#xA;   &lt;th&gt;Qlib +E +D&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; &#xA;   &lt;td&gt;184.4±3.7&lt;/td&gt; &#xA;   &lt;td&gt;365.3±7.5&lt;/td&gt; &#xA;   &lt;td&gt;253.6±6.7&lt;/td&gt; &#xA;   &lt;td&gt;368.2±3.6&lt;/td&gt; &#xA;   &lt;td&gt;147.0±8.8&lt;/td&gt; &#xA;   &lt;td&gt;47.6±1.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.4±0.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8.8±0.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.2±0.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; &#xA;&lt;h1&gt;Related Reports&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://analyticsindiamag.com/qlib/&#34;&gt;Guide To Qlib: Microsoft’s AI Investment Platform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ&#34;&gt;微软也搞AI量化平台？还是开源的！&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ&#34;&gt;微矿Qlib：业内首个AI量化投资开源平台&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contact Us&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have any issues, please create issue &lt;a href=&#34;https://github.com/microsoft/qlib/issues/new/choose&#34;&gt;here&lt;/a&gt; or send messages in &lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;gitter&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href=&#34;https://github.com/microsoft/qlib/compare&#34;&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href=&#34;mailto:qlib@microsoft.com&#34;&gt;qlib@microsoft.com&lt;/a&gt;). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Join IM discussion groups:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;Gitter&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href=&#34;https://github.com/microsoft/qlib/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/qlib&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href=&#34;https://github.com/evanzd/evanzd&#34;&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href=&#34;https://github.com/evanzd/evanzd&#34;&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; &#xA;&lt;h2&gt;Guidance&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions.&lt;br&gt; &lt;strong&gt;Here are some &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst&#34;&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href=&#34;https://github.com/microsoft/qlib/issues&#34;&gt;issues list&lt;/a&gt; or &lt;a href=&#34;https://gitter.im/Microsoft/qlib&#34;&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to contribute to Qlib&#39;s document/code, you can follow the steps in the figure below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t know how to start to contribute, you can refer to the following examples.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Examples&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Solving issues&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/issues/749&#34;&gt;Answer a question&lt;/a&gt;; &lt;a href=&#34;https://github.com/microsoft/qlib/issues/765&#34;&gt;issuing&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/qlib/pull/792&#34;&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/797/files&#34;&gt;Improve docs quality&lt;/a&gt; ; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/774&#34;&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Feature&lt;/td&gt; &#xA;   &lt;td&gt;Implement a &lt;a href=&#34;https://github.com/microsoft/qlib/projects&#34;&gt;requested feature&lt;/a&gt; like &lt;a href=&#34;https://github.com/microsoft/qlib/pull/754&#34;&gt;this&lt;/a&gt;; &lt;a href=&#34;https://github.com/microsoft/qlib/pull/539/files&#34;&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataset&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/733&#34;&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/pull/689&#34;&gt;Implement a new model&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing&#34;&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/qlib/labels/good%20first%20issue&#34;&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; &#xA;&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg &#39;TODO|FIXME&#39; qlib&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to become one of Qlib&#39;s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href=&#34;mailto:qlib@microsoft.com&#34;&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lightricks/LTX-Video</title>
    <updated>2025-06-01T01:50:57Z</updated>
    <id>tag:github.com,2025-06-01:/Lightricks/LTX-Video</id>
    <link href="https://github.com/Lightricks/LTX-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;LTX-Video&lt;/h1&gt; &#xA; &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.lightricks.com/ltxv&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer&#34;&gt;Trainer&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/Mn8BRgUKKy&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news&#34;&gt;What&#39;s new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models--workflows&#34;&gt;Models &amp;amp; Workflows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide&#34;&gt;Quick Start Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference&#34;&gt;Use online&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally&#34;&gt;Run locally&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration&#34;&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide&#34;&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution&#34;&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#%E2%9A%A1%EF%B8%8F-training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#%F0%9F%9A%80-join-us&#34;&gt;Join Us!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216×704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &#xA;&lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &#xA;&lt;h3&gt;Image to video examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif&#34; alt=&#34;example2&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif&#34; alt=&#34;example3&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif&#34; alt=&#34;example4&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif&#34; alt=&#34;example8&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif&#34; alt=&#34;example9&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text to video examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#39;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00010.gif&#34; alt=&#34;example10&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00015.gif&#34; alt=&#34;example3&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#39; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00014.gif&#34; alt=&#34;example2&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00013.gif&#34; alt=&#34;example13&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00011.gif&#34; alt=&#34;example11&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; &#xA;   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; &#xA;     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official ComfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors&#34;&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRAM (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official ComfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new upscalers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors&#34;&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors&#34;&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; &#xA; &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Release a new distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; &#xA; &lt;li&gt;New default resolution and FPS: 1216 × 704 pixels at 30 FPS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New license for commercial use (&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt&#34;&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support keyframes and video extension&lt;/li&gt; &#xA; &lt;li&gt;Support higher resolutions&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt understanding&lt;/li&gt; &#xA; &lt;li&gt;Improved VAE&lt;/li&gt; &#xA; &lt;li&gt;New online web app in &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; &#xA; &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; &#xA; &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release the &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;research paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support for STG / PAG&lt;/li&gt; &#xA; &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; &#xA; &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; &#xA; &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; &#xA; &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; &#xA; &lt;li&gt;Relax transformers dependency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;th&gt;inference.py config&lt;/th&gt; &#xA;   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-dev&lt;/td&gt; &#xA;   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json&#34;&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;ltxv-13b-0.9.7-mix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json&#34;&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv&#34;&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json&#34;&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors&#34;&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA to make ltxv-13b-dev behave like the distilled model&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-fp8&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json&#34;&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json&#34;&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml&#34;&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json&#34;&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; &#xA;   &lt;td&gt;15× faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml&#34;&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json&#34;&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Quick Start Guide&lt;/h1&gt; &#xA;&lt;h2&gt;Online inference&lt;/h2&gt; &#xA;&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b&#34;&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/motion-workspace?videoModel=ltxv&#34;&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video&#34;&gt;Fal.ai text-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video/image-to-video&#34;&gt;Fal.ai image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/lightricks/ltx-video&#34;&gt;Replicate text-to-video and image-to-video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run locally&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightricks/LTX-Video.git&#xA;cd LTX-Video&#xA;&#xA;# create env&#xA;python -m venv env&#xA;source env/bin/activate&#xA;python -m pip install -e .\[inference-script\]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI&lt;/a&gt; workflow. We’re working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; &#xA;&lt;p&gt;To use our model, please follow the inference code in &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py&#34;&gt;inference.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h4&gt;For text-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extending a video:&lt;/h4&gt; &#xA;&lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; &#xA;&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8&#34;&gt;see details below&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model User Guide&lt;/h1&gt; &#xA;&lt;h2&gt;📝 Prompt Engineering&lt;/h2&gt; &#xA;&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; &#xA; &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; &#xA; &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; &#xA; &lt;li&gt;Include background and environment details&lt;/li&gt; &#xA; &lt;li&gt;Specify camera angles and movements&lt;/li&gt; &#xA; &lt;li&gt;Describe lighting and colors&lt;/li&gt; &#xA; &lt;li&gt;Note any changes or sudden events&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;When using &lt;code&gt;inference.py&lt;/code&gt;, shorts prompts (below &lt;code&gt;prompt_enhancement_words_threshold&lt;/code&gt; words) are automatically enhanced by a language model. This is supported with text-to-video and image-to-video (first-frame conditioning).&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🎮 Parameter Guide&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; &#xA; &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; &#xA; &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; &#xA; &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;📝 For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contribution&lt;/h2&gt; &#xA;&lt;h3&gt;ComfyUI-LTXTricks 🛠️&lt;/h3&gt; &#xA;&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🔄 &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href=&#34;https://rf-inversion.github.io/&#34;&gt;RF-Inversion&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;✂️ &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/wangjiangshan0725/RF-Solver-Edit&#34;&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;🌊 &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/fallenshock/FlowEdit&#34;&gt;FlowEdit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;🎥 &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;✨ &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href=&#34;https://junhahyung.github.io/STGuidance/&#34;&gt;STGuidance&lt;/a&gt;. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;🖼️ &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LTX-VideoQ8 🎱 &lt;a id=&#34;ltx-videoq8&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href=&#34;https://github.com/Lightricks/LTX-Video&#34;&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/KONAKONA666/LTX-Video&#34;&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🚀 Up to 3X speed-up with no accuracy loss&lt;/li&gt; &#xA;   &lt;li&gt;🎥 Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; &#xA;   &lt;li&gt;🛠️ Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/&#34;&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href=&#34;https://github.com/sayakpaul/q8-ltx-video&#34;&gt;Details here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TeaCache for LTX-Video 🍵 &lt;a id=&#34;TeaCache&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video&#34;&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🚀 Speeds up LTX-Video inference.&lt;/li&gt; &#xA;   &lt;li&gt;📊 Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; &#xA;   &lt;li&gt;🛠️ No retraining required: Works directly with existing models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Your Contribution&lt;/h3&gt; &#xA;&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; &#xA;&lt;h1&gt;⚡️ Training&lt;/h1&gt; &#xA;&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer&#34;&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training.&lt;/p&gt; &#xA;&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href=&#34;https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md&#34;&gt;README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;🚀 Join Us&lt;/h1&gt; &#xA;&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; &#xA;&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we&#39;re revolutionizing how visual content is created.&lt;/p&gt; &#xA;&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D&#34;&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;📄 Our tech report is out! If you find our work helpful, please ⭐️ star the repository and cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,&#xA;  title={LTX-Video: Realtime Video Latent Diffusion},&#xA;  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},&#xA;  journal={arXiv preprint arXiv:2501.00103},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>