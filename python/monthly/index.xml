<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-01T02:10:11Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>danielmiessler/fabric</title>
    <updated>2024-03-01T02:10:11Z</updated>
    <id>tag:github.com,2024-03-01:/danielmiessler/fabric</id>
    <link href="https://github.com/danielmiessler/fabric" rel="alternate"></link>
    <summary type="html">&lt;p&gt;fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/images/fabric-logo-gif.gif&#34; alt=&#34;fabriclogo&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA; &lt;h1&gt;&lt;code&gt;fabric&lt;/code&gt;&lt;/h1&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple&#34; alt=&#34;Static Badge&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/danielmiessler/fabric&#34; alt=&#34;GitHub top language&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/danielmiessler/fabric&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p class=&#34;align center&#34;&gt; &lt;/p&gt;&#xA; &lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#introduction-video&#34;&gt;Introduction Video&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#whatandwhy&#34;&gt;What and Why&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy&#34;&gt;Philosophy&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#structure&#34;&gt;Structure&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta&#34;&gt;Meta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Navigation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#introduction-video&#34;&gt;Introduction Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#what-and-why&#34;&gt;What and Why&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy&#34;&gt;Philosophy&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#breaking-problems-into-components&#34;&gt;Breaking problems into components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#too-many-prompts&#34;&gt;Too many prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#our-approach-to-prompting&#34;&gt;The Fabric approach to prompting&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#setting-up-the-fabric-commands&#34;&gt;Setting up the fabric commands&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#using-the-fabric-client&#34;&gt;Using the fabric client&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#just-use-the-patterns&#34;&gt;Just use the Patterns&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#create-your-own-fabric-mill&#34;&gt;Create your own Fabric Mill&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#structure&#34;&gt;Structure&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#cli-native&#34;&gt;CLI-native&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#directly-calling-patterns&#34;&gt;Directly calling Patterns&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta&#34;&gt;Meta&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#primary-contributors&#34;&gt;Primary contributors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Introduction video&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://youtu.be/wPEyyigh10g&#34;&gt; &lt;img width=&#34;972&#34; alt=&#34;fabric_intro_video&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/1eb1b9be-0bab-4c77-8ed2-ed265e8a3435&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What and why&lt;/h2&gt; &#xA;&lt;p&gt;Since the start of 2023 and GenAI we&#39;ve seen a massive number of AI applications for accomplishing tasks. It&#39;s powerful, but &lt;em&gt;it&#39;s not easy to integrate this functionality into our lives.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;In other words, AI doesn&#39;t have a capabilities problem—it has an &lt;em&gt;integration&lt;/em&gt; problem.&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Fabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.&lt;/p&gt; &#xA;&lt;h2&gt;Philosophy&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;AI isn&#39;t a thing; it&#39;s a &lt;em&gt;magnifier&lt;/em&gt; of a thing. And that thing is &lt;strong&gt;human creativity&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the &lt;strong&gt;human&lt;/strong&gt; problems we want to solve.&lt;/p&gt; &#xA;&lt;h3&gt;Breaking problems into components&lt;/h3&gt; &#xA;&lt;p&gt;Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.&lt;/p&gt; &#xA;&lt;img width=&#34;2078&#34; alt=&#34;augmented_challenges&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06&#34;&gt; &#xA;&lt;h3&gt;Too many prompts&lt;/h3&gt; &#xA;&lt;p&gt;Prompts are good for this, but the biggest challenge I faced in 2023——which still exists today—is &lt;strong&gt;the sheer number of AI prompts out there&lt;/strong&gt;. We all have prompts that are useful, but it&#39;s hard to discover new ones, know if they are good or not, &lt;em&gt;and manage different versions of the ones we like&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;One of &lt;code&gt;fabric&lt;/code&gt;&#39;s primary features is helping people collect and integrate prompts, which we call &lt;em&gt;Patterns&lt;/em&gt;, into various parts of their lives.&lt;/p&gt; &#xA;&lt;p&gt;Fabric has Patterns for all sorts of life and work activities, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extracting the most interesting parts of YouTube videos and podcasts&lt;/li&gt; &#xA; &lt;li&gt;Writing an essay in your own voice with just an idea as an input&lt;/li&gt; &#xA; &lt;li&gt;Summarizing opaque academic papers&lt;/li&gt; &#xA; &lt;li&gt;Creating perfectly matched AI art prompts for a piece of writing&lt;/li&gt; &#xA; &lt;li&gt;Rating the quality of content to see if you want to read/watch the whole thing&lt;/li&gt; &#xA; &lt;li&gt;Getting summaries of long, boring content&lt;/li&gt; &#xA; &lt;li&gt;Explaining code to you&lt;/li&gt; &#xA; &lt;li&gt;Turning bad documentation into usable documentation&lt;/li&gt; &#xA; &lt;li&gt;Creating social media posts from any content input&lt;/li&gt; &#xA; &lt;li&gt;And a million more…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Our approach to prompting&lt;/h3&gt; &#xA;&lt;p&gt;Fabric &lt;em&gt;Patterns&lt;/em&gt; are different than most prompts you&#39;ll see.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;First, we use &lt;code&gt;Markdown&lt;/code&gt; to help ensure maximum readability and editability&lt;/strong&gt;. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. &lt;em&gt;Importantly, this also includes the AI you&#39;re sending it to!&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s an example of a Fabric Pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;1461&#34; alt=&#34;pattern-example&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next, we are extremely clear in our instructions&lt;/strong&gt;, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;And finally, we tend to use the System section of the prompt almost exclusively&lt;/strong&gt;. In over a year of being heads-down with this stuff, we&#39;ve just seen more efficacy from doing that. If that changes, or we&#39;re shown data that says otherwise, we will adjust.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The most feature-rich way to use Fabric is to use the &lt;code&gt;fabric&lt;/code&gt; client, which can be found under &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/client&#34;&gt;&lt;code&gt;/client&lt;/code&gt;&lt;/a&gt; directory in this repository.&lt;/p&gt; &#xA;&lt;h3&gt;Setting up the fabric commands&lt;/h3&gt; &#xA;&lt;p&gt;Follow these steps to get all fabric related apps installed and configured.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to where you want the Fabric project to live on your system in a semi-permanent place on your computer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Find a home for Fabric&#xA;cd /where/you/keep/code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone the project to your computer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone Fabric to your computer&#xA;git clone https://github.com/danielmiessler/fabric.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Enter Fabric&#39;s main directory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Enter the project folder (where you cloned it)&#xA;cd fabric&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Ensure the &lt;code&gt;setup.sh&lt;/code&gt; script is executable. If you&#39;re not sure, you can make it executable by running the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Install poetry&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ref.: &lt;a href=&#34;https://python-poetry.org/docs/#installing-with-the-official-installer&#34;&gt;https://python-poetry.org/docs/#installing-with-the-official-installer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSL https://install.python-poetry.org | python3 -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;setup.sh&lt;/code&gt;, which will do the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installs python dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Creates aliases in your OS. It should update &lt;code&gt;~/.bashrc&lt;/code&gt;, &lt;code&gt;/.zshrc&lt;/code&gt;, and &lt;code&gt;~/.bash_profile&lt;/code&gt; if they are present in your file system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Restart your shell to reload everything.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fabric --setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll be asked to enter your OpenAI API key, which will be written to &lt;code&gt;~/.config/fabric/.env&lt;/code&gt;. Patterns will then be downloaded from Github, which will take a few moments.&lt;/p&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Now you are up and running! You can test by pulling the help.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Making sure the paths are set up correctly&#xA;fabric --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; If you&#39;re using the &lt;code&gt;server&lt;/code&gt; functions, &lt;code&gt;fabric-api&lt;/code&gt; and &lt;code&gt;fabric-webui&lt;/code&gt; need to be run in distinct terminal windows.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using the &lt;code&gt;fabric&lt;/code&gt; client&lt;/h3&gt; &#xA;&lt;p&gt;Once you have it all set up, here&#39;s how to use it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check out the options &lt;code&gt;fabric -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fabric [-h] [--text TEXT] [--copy] [--output [OUTPUT]] [--stream] [--list]&#xA;              [--update] [--pattern PATTERN] [--setup]&#xA;&#xA;An open-source framework for augmenting humans using AI.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --text TEXT, -t TEXT  Text to extract summary from&#xA;  --copy, -c            Copy the response to the clipboard&#xA;  --output [OUTPUT], -o [OUTPUT]&#xA;                        Save the response to a file&#xA;  --stream, -s          Use this option if you want to see the results in realtime.&#xA;                        NOTE: You will not be able to pipe the output into another&#xA;                        command.&#xA;  --list, -l            List available patterns&#xA;  --update, -u          Update patterns&#xA;  --pattern PATTERN, -p PATTERN&#xA;                        The pattern (prompt) to use&#xA;  --setup               Set up your fabric instance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example commands&lt;/h4&gt; &#xA;&lt;p&gt;The client, by default, runs Fabric patterns without needing a server (the Patterns were downloaded during setup). This means the client connects directly to OpenAI using the input given and the Fabric pattern used.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;summarize&lt;/code&gt; Pattern based on input from &lt;code&gt;stdin&lt;/code&gt;. In this case, the body of an article.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | fabric --pattern summarize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;analyze_claims&lt;/code&gt; Pattern with the &lt;code&gt;--stream&lt;/code&gt; option to get immediate and streaming results.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | fabric --stream --pattern analyze_claims&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;new&lt;/strong&gt; All of the patterns have been added as aliases to your bash (or zsh) config file&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | analyze_claims --stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; More examples coming in the next few days, including a demo video!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Just use the Patterns&lt;/h3&gt; &#xA;&lt;img width=&#34;1173&#34; alt=&#34;fabric-patterns-screenshot&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;If you&#39;re not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/patterns&#34;&gt;&lt;code&gt;/patterns&lt;/code&gt;&lt;/a&gt; directory and start exploring!&lt;/p&gt; &#xA;&lt;p&gt;We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.&lt;/p&gt; &#xA;&lt;p&gt;You can use any of the Patterns you see there in any AI application that you have, whether that&#39;s ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we&#39;ve published, and they will be way better than ours.&lt;/p&gt; &#xA;&lt;p&gt;The wisdom of crowds for the win.&lt;/p&gt; &#xA;&lt;h3&gt;Create your own Fabric Mill&lt;/h3&gt; &#xA;&lt;img width=&#34;2070&#34; alt=&#34;fabric_mill_architecture&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/ec3bd9b5-d285-483d-9003-7a8e6d842584&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;But we go beyond just providing Patterns. We provide code for you to build your very own Fabric server and personal AI infrastructure!&lt;/p&gt; &#xA;&lt;p&gt;To get started, just run the &lt;code&gt;./setup.sh&lt;/code&gt; file and it&#39;ll set up the client, the API server, and the API server web interface. The output of the setup command will also tell you how to run the commands to start them.&lt;/p&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;p&gt;Fabric is themed off of, well… &lt;em&gt;fabric&lt;/em&gt;—as in…woven materials. So, think blankets, quilts, patterns, etc. Here&#39;s the concept and structure:&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;p&gt;The Fabric ecosystem has three primary components, all named within this textile theme.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Mill&lt;/strong&gt; is the (optional) server that makes &lt;strong&gt;Patterns&lt;/strong&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patterns&lt;/strong&gt; are the actual granular AI use cases (prompts).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stitches&lt;/strong&gt; are chained together &lt;em&gt;Patterns&lt;/em&gt; that create advanced functionality (see below).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Looms&lt;/strong&gt; are the client-side apps that call a specific &lt;strong&gt;Pattern&lt;/strong&gt; hosted by a &lt;strong&gt;Mill&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CLI-native&lt;/h3&gt; &#xA;&lt;p&gt;One of the coolest parts of the project is that it&#39;s &lt;strong&gt;command-line native&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Each Pattern you see in the &lt;code&gt;/patterns&lt;/code&gt; directory can be used in any AI application you use, but you can also set up your own server using the &lt;code&gt;/server&lt;/code&gt; code and then call APIs directly!&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re set up, you can do things like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Take any idea from `stdin` and send it to the `/write_essay` API!&#xA;cat &#34;An idea that coding is like speaking with rules.&#34; | write_essay&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Directly calling Patterns&lt;/h3&gt; &#xA;&lt;p&gt;One key feature of &lt;code&gt;fabric&lt;/code&gt; and its Markdown-based format is the ability to _ directly reference_ (and edit) individual &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;patterns&lt;/a&gt; directly—on their own—without surrounding code.&lt;/p&gt; &#xA;&lt;p&gt;As an example, here&#39;s how to call &lt;em&gt;the direct location&lt;/em&gt; of the &lt;code&gt;extract_wisdom&lt;/code&gt; pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This means you can cleanly, and directly reference any pattern for use in a web-based AI app, your own code, or wherever!&lt;/p&gt; &#xA;&lt;p&gt;Even better, you can also have your &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Mill&lt;/a&gt; functionality directly call &lt;em&gt;system&lt;/em&gt; and &lt;em&gt;user&lt;/em&gt; prompts from &lt;code&gt;fabric&lt;/code&gt;, meaning you can have your personal AI ecosystem automatically kept up to date with the latest version of your favorite &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Patterns&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s what that looks like in code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/server/fabric_api_server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# /extwis&#xA;@app.route(&#34;/extwis&#34;, methods=[&#34;POST&#34;])&#xA;@auth_required  # Require authentication&#xA;def extwis():&#xA;    data = request.get_json()&#xA;&#xA;    # Warn if there&#39;s no input&#xA;    if &#34;input&#34; not in data:&#xA;        return jsonify({&#34;error&#34;: &#34;Missing input parameter&#34;}), 400&#xA;&#xA;    # Get data from client&#xA;    input_data = data[&#34;input&#34;]&#xA;&#xA;    # Set the system and user URLs&#xA;    system_url = &#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/system.md&#34;&#xA;    user_url = &#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/user.md&#34;&#xA;&#xA;    # Fetch the prompt content&#xA;    system_content = fetch_content_from_url(system_url)&#xA;    user_file_content = fetch_content_from_url(user_url)&#xA;&#xA;    # Build the API call&#xA;    system_message = {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: system_content}&#xA;    user_message = {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_file_content + &#34;\n&#34; + input_data}&#xA;    messages = [system_message, user_message]&#xA;    try:&#xA;        response = openai.chat.completions.create(&#xA;            model=&#34;gpt-4-1106-preview&#34;,&#xA;            messages=messages,&#xA;            temperature=0.0,&#xA;            top_p=1,&#xA;            frequency_penalty=0.1,&#xA;            presence_penalty=0.1,&#xA;        )&#xA;        assistant_message = response.choices[0].message.content&#xA;        return jsonify({&#34;response&#34;: assistant_message})&#xA;    except Exception as e:&#xA;        return jsonify({&#34;error&#34;: str(e)}), 500&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s an abridged output example from the &lt;a href=&#34;https://github.com/danielmiessler/fabric/raw/main/patterns/extract_wisdom/system.md&#34;&gt;&lt;code&gt;extract_wisdom&lt;/code&gt;&lt;/a&gt; pattern (limited to only 10 items per section).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Paste in the transcript of a YouTube video of Riva Tez on David Perrel&#39;s podcast&#xA;pbpaste | extract_wisdom&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## SUMMARY:&#xA;&#xA;The content features a conversation between two individuals discussing various topics, including the decline of Western culture, the importance of beauty and subtlety in life, the impact of technology and AI, the resonance of Rilke&#39;s poetry, the value of deep reading and revisiting texts, the captivating nature of Ayn Rand&#39;s writing, the role of philosophy in understanding the world, and the influence of drugs on society. They also touch upon creativity, attention spans, and the importance of introspection.&#xA;&#xA;## IDEAS:&#xA;&#xA;1. Western culture is perceived to be declining due to a loss of values and an embrace of mediocrity.&#xA;2. Mass media and technology have contributed to shorter attention spans and a need for constant stimulation.&#xA;3. Rilke&#39;s poetry resonates due to its focus on beauty and ecstasy in everyday objects.&#xA;4. Subtlety is often overlooked in modern society due to sensory overload.&#xA;5. The role of technology in shaping music and performance art is significant.&#xA;6. Reading habits have shifted from deep, repetitive reading to consuming large quantities of new material.&#xA;7. Revisiting influential books as one ages can lead to new insights based on accumulated wisdom and experiences.&#xA;8. Fiction can vividly illustrate philosophical concepts through characters and narratives.&#xA;9. Many influential thinkers have backgrounds in philosophy, highlighting its importance in shaping reasoning skills.&#xA;10. Philosophy is seen as a bridge between theology and science, asking questions that both fields seek to answer.&#xA;&#xA;## QUOTES:&#xA;&#xA;1. &#34;You can&#39;t necessarily think yourself into the answers. You have to create space for the answers to come to you.&#34;&#xA;2. &#34;The West is dying and we are killing her.&#34;&#xA;3. &#34;The American Dream has been replaced by mass packaged mediocrity porn, encouraging us to revel like happy pigs in our own meekness.&#34;&#xA;4. &#34;There&#39;s just not that many people who have the courage to reach beyond consensus and go explore new ideas.&#34;&#xA;5. &#34;I&#39;ll start watching Netflix when I&#39;ve read the whole of human history.&#34;&#xA;6. &#34;Rilke saw beauty in everything... He sees it&#39;s in one little thing, a representation of all things that are beautiful.&#34;&#xA;7. &#34;Vanilla is a very subtle flavor... it speaks to sort of the sensory overload of the modern age.&#34;&#xA;8. &#34;When you memorize chapters [of the Bible], it takes a few months, but you really understand how things are structured.&#34;&#xA;9. &#34;As you get older, if there&#39;s books that moved you when you were younger, it&#39;s worth going back and rereading them.&#34;&#xA;10. &#34;She [Ayn Rand] took complicated philosophy and embodied it in a way that anybody could resonate with.&#34;&#xA;&#xA;## HABITS:&#xA;&#xA;1. Avoiding mainstream media consumption for deeper engagement with historical texts and personal research.&#xA;2. Regularly revisiting influential books from youth to gain new insights with age.&#xA;3. Engaging in deep reading practices rather than skimming or speed-reading material.&#xA;4. Memorizing entire chapters or passages from significant texts for better understanding.&#xA;5. Disengaging from social media and fast-paced news cycles for more focused thought processes.&#xA;6. Walking long distances as a form of meditation and reflection.&#xA;7. Creating space for thoughts to solidify through introspection and stillness.&#xA;8. Embracing emotions such as grief or anger fully rather than suppressing them.&#xA;9. Seeking out varied experiences across different careers and lifestyles.&#xA;10. Prioritizing curiosity-driven research without specific goals or constraints.&#xA;&#xA;## FACTS:&#xA;&#xA;1. The West is perceived as declining due to cultural shifts away from traditional values.&#xA;2. Attention spans have shortened due to technological advancements and media consumption habits.&#xA;3. Rilke&#39;s poetry emphasizes finding beauty in everyday objects through detailed observation.&#xA;4. Modern society often overlooks subtlety due to sensory overload from various stimuli.&#xA;5. Reading habits have evolved from deep engagement with texts to consuming large quantities quickly.&#xA;6. Revisiting influential books can lead to new insights based on accumulated life experiences.&#xA;7. Fiction can effectively illustrate philosophical concepts through character development and narrative arcs.&#xA;8. Philosophy plays a significant role in shaping reasoning skills and understanding complex ideas.&#xA;9. Creativity may be stifled by cultural nihilism and protectionist attitudes within society.&#xA;10. Short-term thinking undermines efforts to create lasting works of beauty or significance.&#xA;&#xA;## REFERENCES:&#xA;&#xA;1. Rainer Maria Rilke&#39;s poetry&#xA;2. Netflix&#xA;3. Underworld concert&#xA;4. Katy Perry&#39;s theatrical performances&#xA;5. Taylor Swift&#39;s performances&#xA;6. Bible study&#xA;7. Atlas Shrugged by Ayn Rand&#xA;8. Robert Pirsig&#39;s writings&#xA;9. Bertrand Russell&#39;s definition of philosophy&#xA;10. Nietzsche&#39;s walks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Meta&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; Special thanks to the following people for their inspiration and contributions!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Caleb Sima&lt;/em&gt; for pushing me over the edge of whether to make this a public project or not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Joel Parish&lt;/em&gt; for super useful input on the project&#39;s Github directory structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Jonathan Dunn&lt;/em&gt; for spectacular work on the soon-to-be-released universal client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Joseph Thacker&lt;/em&gt; for the idea of a &lt;code&gt;-c&lt;/code&gt; context flag that adds pre-created context in the &lt;code&gt;./config/fabric/&lt;/code&gt; directory to all Pattern queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Jason Haddix&lt;/em&gt; for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using &lt;code&gt;llama2&lt;/code&gt; before sending on to &lt;code&gt;gpt-4&lt;/code&gt; for analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Dani Goland&lt;/em&gt; for enhancing the Fabric Server (Mill) infrastructure by migrating to FastAPI, breaking the server into discrete pieces, and Dockerizing the entire thing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Andre Guerra&lt;/em&gt; for simplifying installation by getting us onto Poetry for virtual environment and dependency management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Primary contributors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danielmiessler&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/50654?v=4&#34; title=&#34;Daniel Miessler&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xssdoctor&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/9218431?v=4&#34; title=&#34;Jonathan Dunn&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sbehrens&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/688589?v=4&#34; title=&#34;Scott Behrens&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agu3rra&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10410523?v=4&#34; title=&#34;Andre Guerra&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;fabric&lt;/code&gt; was created by &lt;a href=&#34;https://danielmiessler.com/subscribe&#34; target=&#34;_blank&#34;&gt;Daniel Miessler&lt;/a&gt; in January of 2024. &lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/intent/user?screen_name=danielmiessler&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/danielmiessler&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stanfordnlp/dspy</title>
    <updated>2024-03-01T02:10:11Z</updated>
    <id>tag:github.com,2024-03-01:/stanfordnlp/dspy</id>
    <link href="https://github.com/stanfordnlp/dspy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DSPy: The framework for programming—not prompting—foundation models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/images/DSPy8.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;—not prompting—Foundation Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Oct&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; [Jan&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2401.12178&#34;&gt;In-Context Learning for Extreme Multi-Label Classification&lt;/a&gt;&lt;br&gt; [Dec&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2312.13382&#34;&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/a&gt;&lt;br&gt; [Dec&#39;22] &lt;a href=&#34;https://arxiv.org/abs/2212.14024.pdf&#34;&gt;Demonstrate-Search-Predict: Composing Retrieval &amp;amp; Language Models for Knowledge-Intensive NLP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Getting Started:&lt;/strong&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;https://dspy-docs.vercel.app/&#34;&gt;DSPy Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy is a framework for algorithmically optimizing LM prompts and weights&lt;/strong&gt;, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system &lt;em&gt;without&lt;/em&gt; DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.&lt;/p&gt; &#xA;&lt;p&gt;To make this more systematic and much more powerful, &lt;strong&gt;DSPy&lt;/strong&gt; does two things. First, it separates the flow of your program (&lt;code&gt;modules&lt;/code&gt;) from the parameters (LM prompts and weights) of each step. Second, &lt;strong&gt;DSPy&lt;/strong&gt; introduces new &lt;code&gt;optimizers&lt;/code&gt;, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a &lt;code&gt;metric&lt;/code&gt; you want to maximize.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; can routinely teach powerful models like &lt;code&gt;GPT-3.5&lt;/code&gt; or &lt;code&gt;GPT-4&lt;/code&gt; and local models like &lt;code&gt;T5-base&lt;/code&gt; or &lt;code&gt;Llama2-13b&lt;/code&gt; to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. &lt;strong&gt;DSPy&lt;/strong&gt; optimizers will &#34;compile&#34; the &lt;em&gt;same&lt;/em&gt; program into &lt;em&gt;different&lt;/em&gt; instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. &lt;strong&gt;tldr;&lt;/strong&gt; less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;p&gt;If you need help thinking about your task, we recently created a &lt;a href=&#34;https://discord.gg/VzS6RHHK6F&#34;&gt;Discord server&lt;/a&gt; for the community.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#1-installation&#34;&gt;Installation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#2-documentation&#34;&gt;Tutorials &amp;amp; Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#3-syntax-youre-in-charge-of-the-workflowits-free-form-python-code&#34;&gt;Framework Syntax&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#4-two-powerful-concepts-signatures--teleprompters&#34;&gt;Compiling: Two Powerful Concepts&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#5-faq-is-dspy-right-for-me&#34;&gt;FAQ: Is DSPy right for me?&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Analogy to Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;When we build neural networks, we don&#39;t write manual &lt;em&gt;for-loops&lt;/em&gt; over lists of &lt;em&gt;hand-tuned&lt;/em&gt; floats. Instead, you might use a framework like &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; to compose declarative layers (e.g., &lt;code&gt;Convolution&lt;/code&gt; or &lt;code&gt;Dropout&lt;/code&gt;) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network.&lt;/p&gt; &#xA;&lt;p&gt;Ditto! &lt;strong&gt;DSPy&lt;/strong&gt; gives you the right general-purpose modules (e.g., &lt;code&gt;ChainOfThought&lt;/code&gt;, &lt;code&gt;ReAct&lt;/code&gt;, etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, &lt;strong&gt;DSPy&lt;/strong&gt; also gives you general optimizers (&lt;code&gt;BootstrapFewShotWithRandomSearch&lt;/code&gt; or &lt;a href=&#34;https://github.com/stanfordnlp/dspy/raw/main/dspy/teleprompt/signature_opt_bayesian.py&#34;&gt;&lt;code&gt;BayesianSignatureOptimizer&lt;/code&gt;&lt;/a&gt;), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can &lt;em&gt;compile&lt;/em&gt; your program again and &lt;strong&gt;DSPy&lt;/strong&gt; will create new effective prompts that fit your changes.&lt;/p&gt; &#xA;&lt;h3&gt;Mini-FAQs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;What do DSPy optimizers tune?&lt;/strong&gt; Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy &lt;code&gt;optimizers&lt;/code&gt; can inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We&#39;d love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, &#34;synthetic data&#34; generation, or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How should I use DSPy for my task?&lt;/strong&gt; Using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (&lt;code&gt;modules&lt;/code&gt;) to use, giving each layer a &lt;code&gt;signature&lt;/code&gt; (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy &lt;code&gt;optimizer&lt;/code&gt; to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What if I have a better idea for prompting or synthetic data generation?&lt;/strong&gt; Perfect. We encourage you to think if it&#39;s best expressed as a module or an optimizer, and we&#39;d love to merge it in DSPy so everyone can use it. DSPy is not a complete project; it&#39;s an ongoing effort to create structure (modules and optimizers) in place of hacky prompt and pipeline engineering tricks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What does DSPy stand for?&lt;/strong&gt; It&#39;s a long story but the backronym now is &lt;strong&gt;D&lt;/strong&gt;eclarative &lt;strong&gt;S&lt;/strong&gt;elf-improving Language &lt;strong&gt;P&lt;/strong&gt;rograms, p&lt;strong&gt;y&lt;/strong&gt;thonically.&lt;/p&gt; &#xA;&lt;h2&gt;1) Installation&lt;/h2&gt; &#xA;&lt;p&gt;All you need is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install dspy-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or open our intro notebook in Google Colab: &lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default, DSPy installs the latest &lt;code&gt;openai&lt;/code&gt; from pip. However, if you install old version before OpenAI changed their API &lt;code&gt;openai~=0.28.1&lt;/code&gt;, the library will use that just fine. Both are supported.&lt;/p&gt; &#xA;&lt;p&gt;For the optional (alphabetically sorted) &lt;a href=&#34;https://github.com/chroma-core/chroma&#34;&gt;Chromadb&lt;/a&gt;, &lt;a href=&#34;https://github.com/qdrant/qdrant&#34;&gt;Qdrant&lt;/a&gt;, &lt;a href=&#34;https://github.com/marqo-ai/marqo&#34;&gt;Marqo&lt;/a&gt;, Pinecone, or &lt;a href=&#34;https://github.com/weaviate/weaviate&#34;&gt;Weaviate&lt;/a&gt; retrieval integration(s), include the extra(s) below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install dspy-ai[chromadb]  # or [qdrant] or [marqo] or [mongodb] or [pinecone] or [weaviate]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2) Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The DSPy documentation is divided into &lt;strong&gt;tutorials&lt;/strong&gt; (step-by-step illustration of solving a task in DSPy), &lt;strong&gt;guides&lt;/strong&gt; (how to use specific parts of the API), and &lt;strong&gt;examples&lt;/strong&gt; (self-contained programs that illustrate usage).&lt;/p&gt; &#xA;&lt;h3&gt;A) Tutorials&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Level&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Run in Colab&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Beginner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/intro.ipynb&#34;&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduces the basic building blocks in DSPy. Tackles the task of complex question answering with HotPotQA.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Beginner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/quick-start/minimal-example&#34;&gt;&lt;strong&gt;Minimal Working Example&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;Builds and optimizes a very simple chain-of-thought program in DSPy for math question answering. Very short.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Beginner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/examples/nli/scone/scone.ipynb&#34;&gt;&lt;strong&gt;Compiling for Tricky Tasks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;Teaches LMs to reason about logical statements and negation. Uses GPT-4 to bootstrap few-shot CoT demonstations for GPT-3.5. Establishes a state-of-the-art result on &lt;a href=&#34;https://arxiv.org/abs/2305.19426&#34;&gt;ScoNe&lt;/a&gt;. Contributed by &lt;a href=&#34;https://twitter.com/ChrisGPotts/status/1740033519446057077&#34;&gt;Chris Potts&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Beginner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/skycamp2023.ipynb&#34;&gt;&lt;strong&gt;Local Models &amp;amp; Custom Datasets&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/skycamp2023.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Illustrates two different things together: how to use local models (Llama-2-13B in particular) and how to use your own data examples for training and development.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intermediate&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;&lt;strong&gt;The DSPy Paper&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intermediate&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.13382&#34;&gt;&lt;strong&gt;DSPy Assertions&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_assertions.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduces example of applying DSPy Assertions while generating long-form responses to questions with citations. Presents comparative evaluation in both zero-shot and compiled settings.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intermediate&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/lateinteraction/status/1712135660797317577&#34;&gt;&lt;strong&gt;Finetuning for Complex Programs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/qa/hotpot/multihop_finetune.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Teaches a local T5 model (770M) to do exceptionally well on HotPotQA. Uses only 200 labeled answers. Uses no hand-written prompts, no calls to OpenAI, and no labels for retrieval or reasoning.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Advanced&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/KarelDoostrlnck/status/1724991014207930696&#34;&gt;&lt;strong&gt;Information Extraction&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1CpsOiLiLYKeGrhmq579_FmtGsD5uZ3Qe&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tackles extracting information from long articles (biomedical research papers). Combines in-context learning and retrieval to set SOTA on BioDEX. Contributed by &lt;a href=&#34;https://twitter.com/KarelDoostrlnck/status/1724991014207930696&#34;&gt;Karel D’Oosterlinck&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other resources people find useful&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Dt3H2ninoeY&#34;&gt;DSPy talk at ScaleByTheBay Nov 2023&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=im7bCLW2aM4&#34;&gt;DSPy webinar with MLOps Learners&lt;/a&gt;, a bit longer with Q&amp;amp;A.&lt;/li&gt; &#xA; &lt;li&gt;Hands-on Overviews of DSPy by the community: &lt;a href=&#34;https://www.youtube.com/watch?v=41EfOY0Ldkc&#34;&gt;DSPy Explained! by Connor Shorten&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=ycfnKPxBMck&#34;&gt;DSPy explained by code_your_own_ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Interviews: &lt;a href=&#34;https://www.youtube.com/watch?v=CDung1LnLbY&#34;&gt;Weaviate Podcast in-person&lt;/a&gt;, and you can find 6-7 other remote podcasts on YouTube from a few different perspectives/audiences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tracing in DSPy&lt;/strong&gt; with Arize Phoenix: &lt;a href=&#34;https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb&#34;&gt;Tutorial for tracing your prompts and the steps of your DSPy programs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;B) Guides&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re new to DSPy, it&#39;s probably best to go in sequential order. You will probably refer to these guides frequently after that, e.g. to copy/paste snippets that you can edit for your own DSPy programs.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/language_models&#34;&gt;Language Models&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/signatures&#34;&gt;Signatures&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/modules&#34;&gt;Modules&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/data&#34;&gt;Data&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/metrics&#34;&gt;Metrics&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/optimizers&#34;&gt;Optimizers (formerly Teleprompters)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/assertions.md&#34;&gt;DSPy Assertions&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;C) Examples&lt;/h3&gt; &#xA;&lt;p&gt;The DSPy team believes complexity has to be justified. We take this seriously: we never release a complex tutorial (above) or example (below) &lt;em&gt;unless we can demonstrate empirically that this complexity has generally led to improved quality or cost.&lt;/em&gt; This kind of rule is rarely enforced by other frameworks or docs, but you can count on it in DSPy examples.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a bunch of examples in the &lt;code&gt;examples/&lt;/code&gt; directory and in the top-level directory. We welcome contributions!&lt;/p&gt; &#xA;&lt;p&gt;You can find other examples tweeted by &lt;a href=&#34;https://twitter.com/lateinteraction&#34;&gt;@lateinteraction&lt;/a&gt; on Twitter/X.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some other examples (not exhaustive, feel free to add more via PR):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Applying DSPy Assertions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_assertions.ipynb&#34;&gt;Long-form Answer Generation with Citations, by Arnav Singhvi&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/quiz/quiz_assertions.ipynb&#34;&gt;Generating Answer Choices for Quiz Questions, by Arnav Singhvi&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/tweets/tweets_assertions.ipynb&#34;&gt;Generating Tweets for QA, by Arnav Singhvi&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stanfordnlp/dspy/raw/main/examples/tweets/compiling_langchain.ipynb&#34;&gt;Compiling LCEL runnables from LangChain in DSPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stanfordnlp/dspy/raw/main/examples/tweets/tweet_metric.py&#34;&gt;AI feedback, or writing LM-based metrics in DSPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stanfordnlp/dspy/tree/main/testing/tasks&#34;&gt;DSPy Optimizers Benchmark on a bunch of different tasks, by Michael Ryan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saifulhaq95/DSPy-Indic/raw/main/indicxlni.ipynb&#34;&gt;Indian Languages NLI with gains due to compiling by Saiful Haq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KarelDO/xmc.dspy&#34;&gt;Sophisticated Extreme Multi-Class Classification, IReRa, by Karel D’Oosterlinck&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drchrislevy.github.io/posts/dspy/dspy.html&#34;&gt;DSPy on BIG-Bench Hard Example, by Chris Levy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/jrknox1977/78c17e492b5a75ee5bbaf9673aee4641&#34;&gt;Using Ollama with DSPy for Mistral (quantized) by @jrknox1977&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.theregister.com/2024/02/22/prompt_engineering_ai_models/&#34;&gt;Using DSPy, &#34;The Unreasonable Effectiveness of Eccentric Automatic Prompts&#34; (paper) by VMware&#39;s Rick Battle &amp;amp; Teja Gollapudi, and interview at TheRegister&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are also recent cool examples at &lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations/dspy&#34;&gt;Weaviate&#39;s DSPy cookbook&lt;/a&gt; by Connor Shorten. &lt;a href=&#34;https://www.youtube.com/watch?v=CEuUG4Umfxs&#34;&gt;See tutorial on YouTube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3) Syntax: You&#39;re in charge of the workflow—it&#39;s free-form Python code!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; hides tedious prompt engineering, but it cleanly exposes the important decisions you need to make: &lt;strong&gt;[1]&lt;/strong&gt; what&#39;s your system design going to look like? &lt;strong&gt;[2]&lt;/strong&gt; what are the important constraints on the behavior of your program?&lt;/p&gt; &#xA;&lt;p&gt;You express your system as free-form Pythonic modules. &lt;strong&gt;DSPy&lt;/strong&gt; will tune the quality of your program &lt;em&gt;in whatever way&lt;/em&gt; you use foundation models: you can code with loops, &lt;code&gt;if&lt;/code&gt; statements, or exceptions, and use &lt;strong&gt;DSPy&lt;/strong&gt; modules within any Python control flow you think works for your task.&lt;/p&gt; &#xA;&lt;p&gt;Suppose you want to build a simple retrieval-augmented generation (RAG) system for question answering. You can define your own &lt;code&gt;RAG&lt;/code&gt; program like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RAG(dspy.Module):&#xA;    def __init__(self, num_passages=3):&#xA;        super().__init__()&#xA;        self.retrieve = dspy.Retrieve(k=num_passages)&#xA;        self.generate_answer = dspy.ChainOfThought(&#34;context, question -&amp;gt; answer&#34;)&#xA;    &#xA;    def forward(self, question):&#xA;        context = self.retrieve(question).passages&#xA;        answer = self.generate_answer(context=context, question=question)&#xA;        return answer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A program has two key methods, which you can edit to fit your needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your &lt;code&gt;__init__&lt;/code&gt; method&lt;/strong&gt; declares the modules you will use. Here, &lt;code&gt;RAG&lt;/code&gt; will use the built-in &lt;code&gt;Retrieve&lt;/code&gt; for retrieval and &lt;code&gt;ChainOfThought&lt;/code&gt; for generating answers. &lt;strong&gt;DSPy&lt;/strong&gt; offers general-purpose modules that take the shape of &lt;em&gt;your own&lt;/em&gt; sub-tasks — and not pre-built functions for specific applications.&lt;/p&gt; &#xA;&lt;p&gt;Modules that use the LM, like &lt;code&gt;ChainOfThought&lt;/code&gt;, require a &lt;em&gt;signature&lt;/em&gt;. That is a declarative spec that tells the module what it&#39;s expected to do. In this example, we use the short-hand signature notation &lt;code&gt;context, question -&amp;gt; answer&lt;/code&gt; to tell &lt;code&gt;ChainOfThought&lt;/code&gt; it will be given some &lt;code&gt;context&lt;/code&gt; and a &lt;code&gt;question&lt;/code&gt; and must produce an &lt;code&gt;answer&lt;/code&gt;. We will discuss more advanced &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#3a-declaring-the-inputoutput-behavior-of-lms-with-dspysignature&#34;&gt;signatures&lt;/a&gt;&lt;/strong&gt; below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your &lt;code&gt;forward&lt;/code&gt; method&lt;/strong&gt; expresses any computation you want to do with your modules. In this case, we use the module &lt;code&gt;self.retrieve&lt;/code&gt; to search for some &lt;code&gt;context&lt;/code&gt; and then use the module &lt;code&gt;self.generate_answer&lt;/code&gt;, which uses the &lt;code&gt;context&lt;/code&gt; and &lt;code&gt;question&lt;/code&gt; to generate the &lt;code&gt;answer&lt;/code&gt;!&lt;/p&gt; &#xA;&lt;p&gt;You can now either use this &lt;code&gt;RAG&lt;/code&gt; program in &lt;strong&gt;zero-shot mode&lt;/strong&gt;. Or &lt;strong&gt;compile&lt;/strong&gt; it to obtain higher quality. Zero-shot usage is simple. Just define an instance of your program and then call it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rag = RAG()  # zero-shot, uncompiled version of RAG&#xA;rag(&#34;what is the capital of France?&#34;).answer  # -&amp;gt; &#34;Paris&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The next section will discuss how to compile our simple &lt;code&gt;RAG&lt;/code&gt; program. When we compile it, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will annotate &lt;em&gt;demonstrations&lt;/em&gt; of its steps: (1) retrieval, (2) using context, and (3) using &lt;em&gt;chain-of-thought&lt;/em&gt; to answer questions. From these demonstrations, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will make sure it produces an effective few-shot prompt that works well with your LM, retrieval model, and data. If you&#39;re working with small models, it&#39;ll finetune your model (instead of prompting) to do this task.&lt;/p&gt; &#xA;&lt;p&gt;If you later decide you need another step in your pipeline, just add another module and compile again. Maybe add a module that takes the chat history into account during search?&lt;/p&gt; &#xA;&lt;h2&gt;4) Two Powerful Concepts: Signatures &amp;amp; Teleprompters&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will soon rename teleprompters to optimizers. This will not affect their functionality, but will simplify the terms used.&lt;/p&gt; &#xA;&lt;p&gt;To make it possible to compile any program you write, &lt;strong&gt;DSPy&lt;/strong&gt; introduces two simple concepts: Signatures and Teleprompters.&lt;/p&gt; &#xA;&lt;h4&gt;4.a) Declaring the input/output behavior of LMs with &lt;code&gt;dspy.Signature&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;When we assign tasks to LMs in &lt;strong&gt;DSPy&lt;/strong&gt;, we specify the behavior we need as a &lt;strong&gt;Signature&lt;/strong&gt;. A signature is a declarative specification of input/output behavior of a &lt;strong&gt;DSPy module&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instead of investing effort into &lt;em&gt;how&lt;/em&gt; to get your LM to do a sub-task, signatures enable you to inform &lt;strong&gt;DSPy&lt;/strong&gt; &lt;em&gt;what&lt;/em&gt; the sub-task is. Later, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will figure out how to build a complex prompt for your large LM (or finetune your small LM) specifically for your signature, on your data, and within your pipeline.&lt;/p&gt; &#xA;&lt;p&gt;A signature consists of three simple elements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A minimal description of the sub-task the LM is supposed to solve.&lt;/li&gt; &#xA; &lt;li&gt;A description of one or more input fields (e.g., input question) that will we will give to the LM.&lt;/li&gt; &#xA; &lt;li&gt;A description of one or more output fields (e.g., the question&#39;s answer) that we will expect from the LM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We support two notations for expressing signatures. The &lt;strong&gt;short-hand signature notation&lt;/strong&gt; is for quick development. You just provide your module (e.g., &lt;code&gt;dspy.ChainOfThought&lt;/code&gt;) with a string with &lt;code&gt;input_field_name_1, ... -&amp;gt; output_field_name_1, ...&lt;/code&gt; with the fields separated by commas.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;RAG&lt;/code&gt; class earlier, we saw:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.generate_answer = dspy.ChainOfThought(&#34;context, question -&amp;gt; answer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In many cases, this barebones signature is sufficient. However, sometimes you need more control. In these cases, we can use the full notation to express a more fully-fledged signature below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class GenerateSearchQuery(dspy.Signature):&#xA;    &#34;&#34;&#34;Write a simple search query that will help answer a complex question.&#34;&#34;&#34;&#xA;&#xA;    context = dspy.InputField(desc=&#34;may contain relevant facts&#34;)&#xA;    question = dspy.InputField()&#xA;    query = dspy.OutputField()&#xA;&#xA;### inside your program&#39;s __init__ function&#xA;self.generate_answer = dspy.ChainOfThought(GenerateSearchQuery)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally provide a &lt;code&gt;prefix&lt;/code&gt; and/or &lt;code&gt;desc&lt;/code&gt; key for each input or output field to refine or constraint the behavior of modules using your signature. The description of the sub-task itself is specified as the docstring (i.e., &lt;code&gt;&#34;&#34;&#34;Write a simple...&#34;&#34;&#34;&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;4.b) Asking &lt;strong&gt;DSPy&lt;/strong&gt; to automatically optimize your program with &lt;code&gt;dspy.teleprompt.*&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;After defining the &lt;code&gt;RAG&lt;/code&gt; program, we can &lt;strong&gt;compile&lt;/strong&gt; it. Compiling a program will update the parameters stored in each module. For large LMs, this is primarily in the form of creating and validating good demonstrations for inclusion in your prompt(s).&lt;/p&gt; &#xA;&lt;p&gt;Compiling depends on three things: a (potentially tiny) training set, a metric for validation, and your choice of teleprompter from &lt;strong&gt;DSPy&lt;/strong&gt;. &lt;strong&gt;Teleprompters&lt;/strong&gt; are powerful optimizers (included in &lt;strong&gt;DSPy&lt;/strong&gt;) that can learn to bootstrap and select effective prompts for the modules of any program. (The &#34;tele-&#34; in the name means &#34;at a distance&#34;, i.e., automatic prompting at a distance.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; typically requires very minimal labeling. For example, our &lt;code&gt;RAG&lt;/code&gt; pipeline may work well with just a handful of examples that contain a &lt;strong&gt;question&lt;/strong&gt; and its (human-annotated) &lt;strong&gt;answer&lt;/strong&gt;. Your pipeline may involve multiple complex steps: our basic &lt;code&gt;RAG&lt;/code&gt; example includes a retrieved context, a chain of thought, and the answer. However, you only need labels for the initial question and the final answer. &lt;strong&gt;DSPy&lt;/strong&gt; will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_rag_trainset = [&#xA;  dspy.Example(&#xA;    question=&#34;Which award did Gary Zukav&#39;s first book receive?&#34;,&#xA;    answer=&#34;National Book Award&#34;&#xA;  ),&#xA;  ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second, define your validation logic, which will express some constraints on the behavior of your program or individual modules. For &lt;code&gt;RAG&lt;/code&gt;, we might express a simple check like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def validate_context_and_answer(example, pred, trace=None):&#xA;    # check the gold label and the predicted answer are the same&#xA;    answer_match = example.answer.lower() == pred.answer.lower()&#xA;&#xA;    # check the predicted answer comes from one of the retrieved contexts&#xA;    context_match = any((pred.answer.lower() in c) for c in pred.context)&#xA;&#xA;    return answer_match and context_match&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Different teleprompters offer various tradeoffs in terms of how much they optimize cost versus quality, etc. For &lt;code&gt;RAG&lt;/code&gt;, we might use the simple teleprompter called &lt;code&gt;BootstrapFewShot&lt;/code&gt;. To do so, we instantiate the teleprompter itself with a validation function &lt;code&gt;my_rag_validation_logic&lt;/code&gt; and then compile against some training set &lt;code&gt;my_rag_trainset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dspy.teleprompt import BootstrapFewShot&#xA;&#xA;teleprompter = BootstrapFewShot(metric=my_rag_validation_logic)&#xA;compiled_rag = teleprompter.compile(RAG(), trainset=my_rag_trainset)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we now use &lt;code&gt;compiled_rag&lt;/code&gt;, it will invoke our LM with rich prompts with few-shot demonstrations of chain-of-thought retrieval-augmented question answering on our data.&lt;/p&gt; &#xA;&lt;h2&gt;5) FAQ: Is DSPy right for me?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; philosophy and abstraction differ significantly from other libraries and frameworks, so it&#39;s usually straightforward to decide when &lt;strong&gt;DSPy&lt;/strong&gt; is (or isn&#39;t) the right framework for your usecase.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable &lt;strong&gt;yes&lt;/strong&gt;. If you&#39;re a practitioner doing other things, please read on.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.a] DSPy vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating)&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;p&gt;In other words: &lt;em&gt;Why can&#39;t I just write my prompts directly as string templates?&lt;/em&gt; Well, for extremely simple settings, this &lt;em&gt;might&lt;/em&gt; work just fine. (If you&#39;re familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.)&lt;/p&gt; &#xA; &lt;p&gt;However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; cleanly abstracts away (&lt;em&gt;and&lt;/em&gt; powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the &lt;em&gt;same program&lt;/em&gt; expressed in 10 or 20 lines of &lt;strong&gt;DSPy&lt;/strong&gt; can easily be compiled into multi-stage instructions for &lt;code&gt;GPT-4&lt;/code&gt;, detailed prompts for &lt;code&gt;Llama2-13b&lt;/code&gt;, or finetunes for &lt;code&gt;T5-base&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Oh, and you wouldn&#39;t need to maintain long, brittle, model-specific strings at the core of your project anymore.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.b] DSPy vs. application development libraries like LangChain, LlamaIndex&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;em&gt;Note: If you use LangChain as a thin wrapper around your own prompt strings, refer to answer [5.a] instead.&lt;/em&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;LangChain and LlamaIndex are popular libraries that target high-level application development with LMs. They offer many &lt;em&gt;batteries-included&lt;/em&gt;, pre-built application modules that plug in with your data or configuration. In practice, indeed, many usecases genuinely &lt;em&gt;don&#39;t need&lt;/em&gt; any special components. If you&#39;d be happy to use someone&#39;s generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL as long as it&#39;s easy to set up on your data, then you will probably find a very rich ecosystem in these libraries.&lt;/p&gt; &#xA; &lt;p&gt;Unlike these libraries, &lt;strong&gt;DSPy&lt;/strong&gt; doesn&#39;t internally contain hand-crafted prompts that target specific applications you can build. Instead, &lt;strong&gt;DSPy&lt;/strong&gt; introduces a very small set of much more powerful and general-purpose modules &lt;em&gt;that can learn to prompt (or finetune) your LM within your pipeline on your data&lt;/em&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; offers a whole different degree of modularity: when you change your data, make tweaks to your program&#39;s control flow, or change your target LM, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that &lt;strong&gt;DSPy&lt;/strong&gt; obtains the highest quality for your task, with the least effort, provided you&#39;re willing to implement (or extend) your own short program. In short, &lt;strong&gt;DSPy&lt;/strong&gt; is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations.&lt;/p&gt; &#xA; &lt;p&gt;If you&#39;re familiar with neural networks:&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;This is like the difference between PyTorch (i.e., representing &lt;strong&gt;DSPy&lt;/strong&gt;) and HuggingFace Transformers (i.e., representing the higher-level libraries). If you simply want to use off-the-shelf &lt;code&gt;BERT-base-uncased&lt;/code&gt; or &lt;code&gt;GPT2-large&lt;/code&gt; or apply minimal finetuning to them, HF Transformers makes it very straightforward. If, however, you&#39;re looking to build your own architecture (or extend an existing one significantly), you have to quickly drop down into something much more modular like PyTorch. Luckily, HF Transformers &lt;em&gt;is&lt;/em&gt; implemented in backends like PyTorch. We are similarly excited about high-level wrapper around &lt;strong&gt;DSPy&lt;/strong&gt; for common applications. If this is implemented using &lt;strong&gt;DSPy&lt;/strong&gt;, your high-level application can also adapt significantly to your data in a way that static prompt chains won&#39;t. Please &lt;a href=&#34;https://github.com/stanfordnlp/dspy/issues/new&#34;&gt;open an issue&lt;/a&gt; if this is something you want to help with.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.c] DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Guidance, LMQL, RELM, and Outlines are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression.&lt;/p&gt; &#xA; &lt;p&gt;This is very useful in many settings, but it&#39;s generally focused on low-level, structured control of a single LM call. It doesn&#39;t help ensure the JSON (or structured output) you get is going to be correct or useful for your task.&lt;/p&gt; &#xA; &lt;p&gt;In contrast, &lt;strong&gt;DSPy&lt;/strong&gt; automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured ouputs. That said, we are considering allowing &lt;strong&gt;Signatures&lt;/strong&gt; in &lt;strong&gt;DSPy&lt;/strong&gt; to express regex-like constraints that are implemented by these libraries.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests, you need to first clone the repository.&lt;/p&gt; &#xA;&lt;p&gt;Then install the package through poetry: Note - You may need to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install --with test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the all tests, or a specific test suite, with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run pytest&#xA;poetry run pytest tests/PATH_TO_TEST_SUITE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for a quickstart guide to contributing to DSPy.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; is led by &lt;strong&gt;Omar Khattab&lt;/strong&gt; at Stanford NLP with &lt;strong&gt;Chris Potts&lt;/strong&gt; and &lt;strong&gt;Matei Zaharia&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Key contributors and team members include &lt;strong&gt;Arnav Singhvi&lt;/strong&gt;, &lt;strong&gt;Krista Opsahl-Ong&lt;/strong&gt;, &lt;strong&gt;Michael Ryan&lt;/strong&gt;, &lt;strong&gt;Karel D&#39;Oosterlinck&lt;/strong&gt;, &lt;strong&gt;Shangyin Tan&lt;/strong&gt;, &lt;strong&gt;Manish Shetty&lt;/strong&gt;, &lt;strong&gt;Paridhi Maheshwari&lt;/strong&gt;, &lt;strong&gt;Keshav Santhanam&lt;/strong&gt;, &lt;strong&gt;Sri Vardhamanan&lt;/strong&gt;, &lt;strong&gt;Eric Zhang&lt;/strong&gt;, &lt;strong&gt;Hanna Moazam&lt;/strong&gt;, &lt;strong&gt;Thomas Joshi&lt;/strong&gt;, &lt;strong&gt;Saiful Haq&lt;/strong&gt;, &lt;strong&gt;Ashutosh Sharma&lt;/strong&gt;, and &lt;strong&gt;Herumb Shandilya&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; includes important contributions from &lt;strong&gt;Rick Battle&lt;/strong&gt; and &lt;strong&gt;Igor Kotenkov&lt;/strong&gt;. It reflects discussions with &lt;strong&gt;Peter Zhong&lt;/strong&gt;, &lt;strong&gt;Haoze He&lt;/strong&gt;, &lt;strong&gt;Lisa Li&lt;/strong&gt;, &lt;strong&gt;David Hall&lt;/strong&gt;, &lt;strong&gt;Ashwin Paranjape&lt;/strong&gt;, &lt;strong&gt;Heather Miller&lt;/strong&gt;, &lt;strong&gt;Chris Manning&lt;/strong&gt;, &lt;strong&gt;Percy Liang&lt;/strong&gt;, and many others.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; logo is designed by &lt;strong&gt;Chuyi Zhang&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📜 Citation &amp;amp; Reading More&lt;/h2&gt; &#xA;&lt;p&gt;To stay up to date or learn more, follow &lt;a href=&#34;https://twitter.com/lateinteraction&#34;&gt;@lateinteraction&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;p&gt;If you use DSPy or DSP in a research paper, please cite our work as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{khattab2023dspy,&#xA;  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},&#xA;  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},&#xA;  journal={arXiv preprint arXiv:2310.03714},&#xA;  year={2023}&#xA;}&#xA;@article{khattab2022demonstrate,&#xA;  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},&#xA;  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},&#xA;  journal={arXiv preprint arXiv:2212.14024},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.13382&#34;&gt;&lt;strong&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/strong&gt;&lt;/a&gt; (Academic Paper, Dec 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;&lt;strong&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/strong&gt;&lt;/a&gt; (Academic Paper, Oct 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/lateinteraction/status/1694748401374490946&#34;&gt;&lt;strong&gt;Releasing DSPy, the latest iteration of the framework&lt;/strong&gt;&lt;/a&gt; (Twitter Thread, Aug 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/lateinteraction/status/1625231662849073160&#34;&gt;&lt;strong&gt;Releasing the DSP Compiler (v0.1)&lt;/strong&gt;&lt;/a&gt; (Twitter Thread, Feb 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/lateinteraction/status/1617953413576425472&#34;&gt;&lt;strong&gt;Introducing DSP&lt;/strong&gt;&lt;/a&gt; (Twitter Thread, Jan 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14024.pdf&#34;&gt;&lt;strong&gt;Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP&lt;/strong&gt;&lt;/a&gt; (Academic Paper, Dec 2022)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: If you&#39;re looking for Demonstrate-Search-Predict (DSP), which is the previous version of DSPy, you can find it on the &lt;a href=&#34;https://github.com/stanfordnlp/dspy/tree/v1&#34;&gt;v1&lt;/a&gt; branch of this repo.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>sherlock-project/sherlock</title>
    <updated>2024-03-01T02:10:11Z</updated>
    <id>tag:github.com,2024-03-01:/sherlock-project/sherlock</id>
    <link href="https://github.com/sherlock-project/sherlock" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🔎 Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://sherlock-project.github.io/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href=&#34;https://github.com/sherlock-project/sherlock/raw/master/sites.md&#34;&gt;social networks&lt;/a&gt;&lt;/span&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/sherlock-project/sherlock/master/#installation&#34;&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/sherlock-project/sherlock/master/#usage&#34;&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/sherlock-project/sherlock/master/#docker-notes&#34;&gt;Docker Notes&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/sherlock-project/sherlock/master/#contributing&#34;&gt;Contributing&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;70%&#34; height=&#34;70%&#34; src=&#34;https://user-images.githubusercontent.com/27065646/219638267-a5e11090-aa6e-4e77-87f7-0e95f6ad5978.png&#34;&gt;  &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# clone the repo&#xA;$ git clone https://github.com/sherlock-project/sherlock.git&#xA;&#xA;# change the working directory to sherlock&#xA;$ cd sherlock&#xA;&#xA;# install the requirements&#xA;$ python3 -m pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python3 sherlock --help&#xA;usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]&#xA;                [--output OUTPUT] [--tor] [--unique-tor] [--csv]&#xA;                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]&#xA;                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]&#xA;                [--browse] [--local] [--nsfw]&#xA;                USERNAMES [USERNAMES ...]&#xA;&#xA;Sherlock: Find Usernames Across Social Networks (Version 0.14.3)&#xA;&#xA;positional arguments:&#xA;  USERNAMES             One or more usernames to check with social networks.&#xA;                        Check similar usernames using {?} (replace to &#39;_&#39;, &#39;-&#39;, &#39;.&#39;).&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --version             Display version information and dependencies.&#xA;  --verbose, -v, -d, --debug&#xA;                        Display extra debugging information and metrics.&#xA;  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT&#xA;                        If using multiple usernames, the output of the results will be&#xA;                        saved to this folder.&#xA;  --output OUTPUT, -o OUTPUT&#xA;                        If using single username, the output of the result will be saved&#xA;                        to this file.&#xA;  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be&#xA;                        installed and in system path.&#xA;  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;&#xA;                        increases runtime; requires Tor to be installed and in system&#xA;                        path.&#xA;  --csv                 Create Comma-Separated Values (CSV) File.&#xA;  --xlsx                Create the standard file for the modern Microsoft Excel&#xA;                        spreadsheet (xslx).&#xA;  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to&#xA;                        specify more than one site.&#xA;  --proxy PROXY_URL, -p PROXY_URL&#xA;                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080&#xA;  --json JSON_FILE, -j JSON_FILE&#xA;                        Load data from a JSON file or an online, valid, JSON file.&#xA;  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)&#xA;  --print-all           Output sites where the username was not found.&#xA;  --print-found         Output sites where the username was found.&#xA;  --no-color            Don&#39;t color terminal output&#xA;  --browse, -b          Browse to all results on default browser.&#xA;  --local, -l           Force the use of the local data.json file.&#xA;  --nsfw                Include checking of NSFW sites from default list.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To search for only one user:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 sherlock user123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To search for more than one user:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 sherlock user1 user2 user3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Anaconda (Windows) Notes&lt;/h2&gt; &#xA;&lt;p&gt;If you are using Anaconda in Windows, using &lt;code&gt;python3&lt;/code&gt; might not work. Use &lt;code&gt;python&lt;/code&gt; instead.&lt;/p&gt; &#xA;&lt;h2&gt;Docker Notes&lt;/h2&gt; &#xA;&lt;p&gt;If docker is installed you can build an image and run this as a container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t mysherlock-image .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the image is built, sherlock can be invoked by running the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -t mysherlock-image user123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the following command to access the saved results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -t -v &#34;$PWD/results:/opt/sherlock/results&#34; mysherlock-image -o /opt/sherlock/results/text.txt user123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker is instructed to create (or use) the folder &lt;code&gt;results&lt;/code&gt; in the current working directory and to mount it at &lt;code&gt;/opt/sherlock/results&lt;/code&gt; on the docker container by using the &lt;code&gt;-v &#34;$PWD/results:/opt/sherlock/results&#34;&lt;/code&gt; options. &lt;code&gt;Sherlock&lt;/code&gt; is instructed to export the result using the &lt;code&gt;-o /opt/sherlock/results/text.txt&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose run sherlock -o /opt/sherlock/results/text.txt user123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued!&lt;/p&gt; &#xA;&lt;p&gt;Here are some things we would appreciate your help on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Addition of new site support ¹&lt;/li&gt; &#xA; &lt;li&gt;Bringing back site support of &lt;a href=&#34;https://raw.githubusercontent.com/sherlock-project/sherlock/master/removed_sites.md&#34;&gt;sites that have been removed&lt;/a&gt; in the past due to false positives&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[1] Please look at the Wiki entry on &lt;a href=&#34;https://github.com/sherlock-project/sherlock/wiki/Adding-Sites-To-Sherlock&#34;&gt;adding new sites&lt;/a&gt; to understand the issues.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for contributing to Sherlock!&lt;/p&gt; &#xA;&lt;p&gt;Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software.&lt;/p&gt; &#xA;&lt;p&gt;The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ cd sherlock/sherlock&#xA;$ python3 -m unittest tests.all --verbose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors.&lt;/p&gt; &#xA;&lt;p&gt;If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a &lt;code&gt;tests/.excluded_sites&lt;/code&gt; file with a list of sites to ignore (one site name per line).&lt;/p&gt; &#xA;&lt;h2&gt;Stargazers over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/sherlock-project/sherlock&#34;&gt;&lt;img src=&#34;https://starchart.cc/sherlock-project/sherlock.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT © Sherlock Project&lt;br&gt; Original Creator - &lt;a href=&#34;https://github.com/sdushantha&#34;&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>