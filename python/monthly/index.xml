<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-01T02:05:04Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Sanster/lama-cleaner</title>
    <updated>2022-12-01T02:05:04Z</updated>
    <id>tag:github.com,2022-12-01:/Sanster/lama-cleaner</id>
    <link href="https://github.com/Sanster/lama-cleaner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Lama Cleaner&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;A free and open-source inpainting tool powered by SOTA AI model.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt; &lt;img alt=&#34;total download&#34; src=&#34;https://pepy.tech/badge/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lama-cleaner/&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/pypi/v/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1e3ZkAJxvkK3uzaTGu91N9TvI_Mahs0Wb?usp=sharing&#34;&gt; &lt;img alt=&#34;Open in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Sanster/Lama-Cleaner-lama&#34;&gt; &lt;img alt=&#34;Hugging Face Spaces&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt; &lt;img alt=&#34;python version&#34; src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/cwq1913/lama-cleaner&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/docker/pulls/cwq1913/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3998421/196976498-ba1ad3ab-fa18-4c55-965f-5c6683141375.mp4&#34;&gt;https://user-images.githubusercontent.com/3998421/196976498-ba1ad3ab-fa18-4c55-965f-5c6683141375.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Completely free and open-source&lt;/li&gt; &#xA; &lt;li&gt;Fully self-hosted&lt;/li&gt; &#xA; &lt;li&gt;Classical image inpainting algorithm powered by &lt;a href=&#34;https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html&#34;&gt;cv2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple SOTA AI models &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/saic-mdal/lama&#34;&gt;LaMa&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;LDM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/DQiaole/ZITS_inpainting&#34;&gt;ZITS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/fenglinglwb/MAT&#34;&gt;MAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/SHI-Labs/FcF-Inpainting&#34;&gt;FcF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/runwayml/stable-diffusion&#34;&gt;SD1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/msxie92/MangaInpainting&#34;&gt;Manga&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support CPU &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;Various inpainting &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#inpainting-strategy&#34;&gt;strategy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run as a desktop APP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;1. Remove any unwanted things on the image&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove unwanted things&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_object.jpg&#34; alt=&#34;unwant_object2&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_object_clean.jpg&#34; alt=&#34;unwant_object2&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove unwanted person&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_person.jpg&#34; alt=&#34;unwant_person&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_person_clean.jpg&#34; alt=&#34;unwant_person&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove text&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_text.jpg&#34; alt=&#34;text&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_text_clean.jpg&#34; alt=&#34;text&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove watermark&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/watermark.jpg&#34; alt=&#34;watermark&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/watermark_cleanup.jpg&#34; alt=&#34;watermark_clean&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove text balloons on manga&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/manga.png&#34; alt=&#34;manga&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/manga_clean.png&#34; alt=&#34;manga_clean&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;2. Fix old photo&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Fix old photo&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/old_photo.jpg&#34; alt=&#34;oldphoto&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/old_photo_clean.jpg&#34; alt=&#34;oldphoto_clean&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;3. Replace something on the image &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Text Driven Inpainting&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/dog.jpg&#34; alt=&#34;dog&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Prompt: a fox sitting on a bench&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/fox.jpg&#34; alt=&#34;fox&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to use Lama Cleaner is to install it using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lama-cleaner&#xA;&#xA;# Models will be downloaded at first time used&#xA;lama-cleaner --model=lama --device=cpu --port=8080&#xA;# Lama Cleaner is now running at http://localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For stable-diffusion model, you need to &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-inpainting&#34;&gt;accepting the terms to access&lt;/a&gt;, and get an access token from here &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;huggingface access token&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to use docker, you can check out &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#docker&#34;&gt;docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you hava no idea what is docker or pip, please check &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/scripts/README.md&#34;&gt;One Click Installer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Available command line arguments:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--model&lt;/td&gt; &#xA;   &lt;td&gt;lama/ldm/zits/mat/fcf/sd1.5/manga See details in &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#inpainting-model&#34;&gt;Inpaint Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;lama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--hf_access_token&lt;/td&gt; &#xA;   &lt;td&gt;stable-diffusion need &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;huggingface access token&lt;/a&gt; to download model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-run-local&lt;/td&gt; &#xA;   &lt;td&gt;Once the model as downloaded, you can pass this arg and remove &lt;code&gt;--hf_access_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-disable-nsfw&lt;/td&gt; &#xA;   &lt;td&gt;Disable stable-diffusion NSFW checker.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-cpu-textencoder&lt;/td&gt; &#xA;   &lt;td&gt;Always run stable-diffusion TextEncoder model on CPU.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-enable-xformers&lt;/td&gt; &#xA;   &lt;td&gt;Enable xFormers optimizations. See: &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;facebookresearch/xformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--device&lt;/td&gt; &#xA;   &lt;td&gt;cuda or cpu&lt;/td&gt; &#xA;   &lt;td&gt;cuda&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--port&lt;/td&gt; &#xA;   &lt;td&gt;Port for backend flask web server&lt;/td&gt; &#xA;   &lt;td&gt;8080&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--gui&lt;/td&gt; &#xA;   &lt;td&gt;Launch lama-cleaner as a desktop application&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--gui_size&lt;/td&gt; &#xA;   &lt;td&gt;Set the window size for the application&lt;/td&gt; &#xA;   &lt;td&gt;1200 900&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--input&lt;/td&gt; &#xA;   &lt;td&gt;Path to image you want to load by default&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--debug&lt;/td&gt; &#xA;   &lt;td&gt;Enable debug mode for flask web server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Inpainting Model&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cv2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; No GPU is required, and for simple backgrounds, the results may even be better than AI models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LaMa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Generalizes well on high resolutions(~2k)&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LDM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Possible to get better and more detail result &lt;br&gt; &lt;span&gt;üëç&lt;/span&gt; The balance of time and quality can be achieved by adjusting &lt;code&gt;steps&lt;/code&gt; &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Slower than GAN model&lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Need more GPU memory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Steps&lt;/code&gt;: You can get better result with large steps, but it will be more time-consuming &lt;br&gt; &lt;code&gt;Sampler&lt;/code&gt;: ddim or &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;plms&lt;/a&gt;. In general plms can get &lt;a href=&#34;https://github.com/Sanster/lama-cleaner/releases/tag/0.13.0&#34;&gt;better results&lt;/a&gt; with fewer steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZITS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Better holistic structures compared with previous methods &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Wireframe module is &lt;strong&gt;very&lt;/strong&gt; slow on CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Wireframe&lt;/code&gt;: Enable edge and line detect&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAT&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FcF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Better structure and texture generation &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Only support fixed size (512x512) input&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD1.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; SOTA text-to-image diffusion model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; See model comparison detail&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs LDM&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;LaMa&lt;/th&gt; &#xA;    &lt;th&gt;LDM&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923525-d6afdec3-7b98-403f-ad20-88ebc6eb8d6d.jpg&#34; alt=&#34;photo-1583445095369-9c651e7e5d34&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923620-a40cc066-fd4a-4d85-a29f-6458711d1247.png&#34; alt=&#34;photo-1583445095369-9c651e7e5d34_cleanup_lama&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923652-0d06c8c8-33ad-4a42-a717-9c99f3268933.png&#34; alt=&#34;photo-1583445095369-9c651e7e5d34_cleanup_ldm&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs ZITS&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;ZITS&lt;/th&gt; &#xA;    &lt;th&gt;LaMa&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464918-eb13ebfb-8718-461c-9e8b-7f6d8bb7a84f.png&#34; alt=&#34;zits_original&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464914-4db722c9-047f-48fe-9bb4-916ba09eb5c6.png&#34; alt=&#34;zits_compare_zits&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464903-ffb5f770-4372-4488-ba76-4b4a8c3323f5.png&#34; alt=&#34;zits_compare_lama&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Image is from &lt;a href=&#34;https://github.com/DQiaole/ZITS_inpainting&#34;&gt;ZITS&lt;/a&gt; paper. I didn&#39;t find a good example to show the advantages of ZITS and let me know if you have a good example. There can also be possible problems with my code, if you find them, please let me know too!&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs FcF&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;LaMa&lt;/th&gt; &#xA;    &lt;th&gt;FcF&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305027-a4260545-c24e-4df7-9739-ac5dc3cae879.jpeg&#34; alt=&#34;texture&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305024-2064ed3e-5af4-4843-ac10-7f9da71e15f8.jpeg&#34; alt=&#34;texture_lama&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305006-a08d2896-a65f-43d5-b9a5-ef62c3129f0c.jpeg&#34; alt=&#34;texture_fcf&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs Manga&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Manga model works better on high-quality manga image then LaMa model.&lt;/p&gt; &#xA; &lt;p&gt;Original Image &lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/manga.png&#34; alt=&#34;manga&#34;&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;1080x740&lt;/th&gt; &#xA;    &lt;th&gt;1470x1010&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Manga&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/202676629-54f40f20-c55b-4e6d-bcc7-0a4e81fbb27d.png&#34; alt=&#34;manga_1080x740&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/202675839-4f8012d5-1c10-47ea-9628-20512e86f192.png&#34; alt=&#34;manga_1470x1010&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/saic-mdal/lama&#34;&gt;LaMa&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/202675704-53fa7a3d-ec74-4044-a19c-c673d74bdd28.png&#34; alt=&#34;lama_1080x740&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/202675746-1e642367-f5d0-4b48-aa8b-5d82f2e29082.png&#34; alt=&#34;lama_1470x1010&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Inpainting Strategy&lt;/h2&gt; &#xA;&lt;p&gt;Lama Cleaner provides three ways to run inpainting model on images, you can change it in the settings dialog.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Strategy&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Original&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the resolution of the original image&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Resize&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Resize the image to a smaller size before inpainting. The area outside the mask will not loss quality.&lt;/td&gt; &#xA;   &lt;td&gt;Midium&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Crop&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Crop masking area from the original image to do inpainting&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Download Model Manually&lt;/h2&gt; &#xA;&lt;p&gt;If you have problems downloading the model automatically when lama-cleaner start, you can download it manually. By default lama-cleaner will load model from &lt;code&gt;TORCH_HOME=~/.cache/torch/hub/checkpoints/&lt;/code&gt;, you can set &lt;code&gt;TORCH_HOME&lt;/code&gt; to other folder and put the models there.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_big_lama&#34;&gt;LaMa&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_ldm&#34;&gt;LDM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_zits&#34;&gt;ZITS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_mat&#34;&gt;MAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_fcf&#34;&gt;FcF&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Baidu: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pan.baidu.com/s/1vUd3BVqIpK6e8N_EA_ZJfw&#34;&gt;https://pan.baidu.com/s/1vUd3BVqIpK6e8N_EA_ZJfw&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;passward: flsu&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Only needed if you plan to modify the frontend and recompile yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;p&gt;Frontend code are modified from &lt;a href=&#34;https://github.com/initml/cleanup.pictures&#34;&gt;cleanup.pictures&lt;/a&gt;, You can experience their great online services &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install dependencies:&lt;code&gt;cd lama_cleaner/app/ &amp;amp;&amp;amp; yarn&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start development server: &lt;code&gt;yarn start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build: &lt;code&gt;yarn build&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://github.com/Sanster/lama-cleaner#run-docker-cpu&#34;&gt;pre-build docker image&lt;/a&gt; to run Lama Cleaner. The model will be downloaded to the cache directory when first time used. You can mount existing cache directory to start the container, so you don&#39;t have to download the model every time you start the container.&lt;/p&gt; &#xA;&lt;p&gt;The cache directories for different models correspond as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;lama/ldm/zits/mat/fcf: /root/.cache/torch&lt;/li&gt; &#xA; &lt;li&gt;sd1.5: /root/.cache/huggingface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run Docker (cpu)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 8080:8080 \&#xA;-v /path/to/torch_cache:/root/.cache/torch \&#xA;-v /path/to/huggingface_cache:/root/.cache/huggingface \&#xA;--rm cwq1913/lama-cleaner:cpu-0.26.1 \&#xA;lama-cleaner --device=cpu --port=8080 --host=0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Docker (gpu)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cuda11.6&lt;/li&gt; &#xA; &lt;li&gt;pytorch1.12.1&lt;/li&gt; &#xA; &lt;li&gt;minimum nvidia driver 510.39.01+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --gpus all -p 8080:8080 \&#xA;-v /path/to/torch_cache:/root/.cache/torch \&#xA;-v /path/to/huggingface_cache:/root/.cache/huggingface \&#xA;--rm cwq1913/lama-cleaner:gpu-0.26.1 \&#xA;lama-cleaner --device=cuda --port=8080 --host=0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build Docker image&lt;/h3&gt; &#xA;&lt;p&gt;cpu only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -f --build-arg version=0.x.0 ./docker/CPUDockerfile -t lamacleaner .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;gpu &amp;amp; cpu&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -f --build-arg version=0.x.0 ./docker/GPUDockerfile -t lamacleaner .&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>hpcaitech/ColossalAI</title>
    <updated>2022-12-01T02:05:04Z</updated>
    <id>tag:github.com,2022-12-01:/hpcaitech/ColossalAI</id>
    <link href="https://github.com/hpcaitech/ColossalAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Colossal-AI: A Unified Deep Learning System for Big Model Era&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Colossal-AI&lt;/h1&gt; &#xA;&lt;div id=&#34;top&#34; align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.colossalai.org/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Colossal-AI_logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Colossal-AI: A Unified Deep Learning System for Big Model Era&lt;/p&gt; &#xA; &lt;h3&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.14883&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://www.colossalai.org/&#34;&gt; Documentation &lt;/a&gt; | &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI-Examples&#34;&gt; Examples &lt;/a&gt; | &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/discussions&#34;&gt; Forum &lt;/a&gt; | &lt;a href=&#34;https://medium.com/@hpcaitech&#34;&gt; Blog &lt;/a&gt;&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/hpcaitech/ColossalAI/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colossalai.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/colossalai/badge/?version=latest&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/hpcaitech/colossalai&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/hpcaitech/colossalai/badge&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/hpcai-tech&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow&#34; alt=&#34;HuggingFace badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%8A%A0%E5%85%A5-green?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/README-zh-Hans.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2022/11] &lt;a href=&#34;https://medium.com/@yangyou_berkeley/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper-85e970fe207b&#34;&gt;Diffusion Pretraining and Hardware Fine-Tuning Can Be Almost 7X Cheaper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/10] &lt;a href=&#34;https://medium.com/@yangyou_berkeley/use-a-laptop-to-analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding-10-000-4c8f0a389cd&#34;&gt;Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference Sequence Exceeding 10,000&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/10] &lt;a href=&#34;https://medium.com/@yangyou_berkeley/embedding-training-with-1-gpu-memory-and-10-times-less-budget-an-open-source-solution-for-6b4c3aba07a8&#34;&gt;Embedding Training With 1% GPU Memory and 100 Times Less Budget for Super-Large Recommendation Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/09] &lt;a href=&#34;https://medium.com/@hpcaitech/hpc-ai-tech-completes-6-million-seed-and-angel-round-fundraising-led-by-bluerun-ventures-in-the-892468cc2b02&#34;&gt;HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/07] &lt;a href=&#34;https://medium.com/@yangyou_berkeley/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face-4d1a887e500d&#34;&gt;Colossal-AI Seamlessly Accelerates Large Models at Low Costs with Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Why-Colossal-AI&#34;&gt;Why Colossal-AI&lt;/a&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Features&#34;&gt;Features&lt;/a&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Parallel-Training-Demo&#34;&gt;Parallel Training Demo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#ViT&#34;&gt;ViT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#GPT-3&#34;&gt;GPT-3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#GPT-2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#BERT&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#PaLM&#34;&gt;PaLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#OPT&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Recommendation-System-Models&#34;&gt;Recommendation System Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Single-GPU-Training-Demo&#34;&gt;Single GPU Training Demo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#GPT-2-Single&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#PaLM-Single&#34;&gt;PaLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Inference-Energon-AI-Demo&#34;&gt;Inference (Energon-AI) Demo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#GPT-3-Inference&#34;&gt;GPT-3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#OPT-Serving&#34;&gt;OPT-175B Online Serving for Text Generation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Colossal-AI-in-the-Real-World&#34;&gt;Colossal-AI for Real World Applications&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#AIGC&#34;&gt;AIGC: Acceleration of Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Biomedicine&#34;&gt;Biomedicine: Acceleration of AlphaFold Protein Structure&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#PyPI&#34;&gt;PyPI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Install-From-Source&#34;&gt;Install From Source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Use-Docker&#34;&gt;Use Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#Cite-Us&#34;&gt;Cite Us&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Colossal-AI&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://youtu.be/KnXSfjqkKN0&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png&#34; width=&#34;600&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;Prof. James Demmel (UC Berkeley): Colossal-AI makes training AI models efficient, easy, and scalable.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Parallelism strategies&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;Pipeline Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;1D, &lt;a href=&#34;https://arxiv.org/abs/2104.05343&#34;&gt;2D&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2105.14500&#34;&gt;2.5D&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2105.14450&#34;&gt;3D&lt;/a&gt; Tensor Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.13120&#34;&gt;Sequence Parallelism&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.02054&#34;&gt;Zero Redundancy Optimizer (ZeRO)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Heterogeneous Memory Management&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.05818&#34;&gt;PatrickStar&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Friendly Usage&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parallelism based on configuration file&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inference&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/EnergonAI&#34;&gt;Energon-AI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Colossal-AI in the Real World&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Biomedicine: &lt;a href=&#34;https://github.com/hpcaitech/FastFold&#34;&gt;FastFold&lt;/a&gt; accelerates training and inference of AlphaFold protein structure&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Parallel Training Demo&lt;/h2&gt; &#xA;&lt;h3&gt;ViT&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;14x larger batch size, and 5x faster training for Tensor Parallelism = 64&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GPT-3&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png&#34; width=&#34;700/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Save 50% GPU resources, and 10.7% acceleration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GPT-2&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png&#34; width=&#34;800/&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;24x larger model size on the same hardware&lt;/li&gt; &#xA; &lt;li&gt;over 3x acceleration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;BERT&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png&#34; width=&#34;800/&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2x faster training, or 50% longer sequence length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PaLM&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/PaLM-colossalai&#34;&gt;PaLM-colossalai&lt;/a&gt;: Scalable implementation of Google&#39;s Pathways Language Model (&lt;a href=&#34;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;&gt;PaLM&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OPT&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png&#34; width=&#34;800/&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/metaseq&#34;&gt;Open Pretrained Transformer (OPT)&lt;/a&gt;, a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because public pretrained model weights.&lt;/li&gt; &#xA; &lt;li&gt;45% speedup fine-tuning OPT at low cost in lines. &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/opt&#34;&gt;[Example]&lt;/a&gt; &lt;a href=&#34;https://service.colossalai.org/opt&#34;&gt;[Online Serving]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://www.colossalai.org/&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI-Examples&#34;&gt;examples&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Recommendation System Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/CachedEmbedding&#34;&gt;Cached Embedding&lt;/a&gt;, utilize software cache to train larger embedding tables with a smaller GPU memory budget.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Single GPU Training Demo&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-2&lt;/h3&gt; &#xA;&lt;p id=&#34;GPT-2-Single&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-GPU1.png&#34; width=&#34;450/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;20x larger model size on the same hardware&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p id=&#34;GPT-2-NVME&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-NVME.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;120x larger model size on the same hardware (RTX 3080)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PaLM&lt;/h3&gt; &#xA;&lt;p id=&#34;PaLM-Single&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/PaLM-GPU1.png&#34; width=&#34;450/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;34x larger model size on the same hardware&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Inference (Energon-AI) Demo&lt;/h2&gt; &#xA;&lt;p id=&#34;GPT-3-Inference&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/EnergonAI&#34;&gt;Energon-AI&lt;/a&gt;: 50% inference acceleration on the same hardware&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p id=&#34;OPT-Serving&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_serving.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://service.colossalai.org/opt&#34;&gt;OPT Serving&lt;/a&gt;: Try 175-billion-parameter OPT online services for free, without any registration whatsoever.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Colossal-AI in the Real World&lt;/h2&gt; &#xA;&lt;h3&gt;AIGC&lt;/h3&gt; &#xA;&lt;p&gt;Acceleration of AIGC (AI-Generated Content) models such as &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p id=&#34;diffusion_train&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/diffusion_train.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion&#34;&gt;Stable Diffusion with Colossal-AI&lt;/a&gt;: 6.5x faster training and pretraining cost saving, the hardware cost of fine-tuning can be almost 7X cheaper (from RTX3090/4090 to RTX3050/2070)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p id=&#34;diffusion_demo&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/diffusion_demo.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;Biomedicine&lt;/h3&gt; &#xA;&lt;p&gt;Acceleration of &lt;a href=&#34;https://alphafold.ebi.ac.uk/&#34;&gt;AlphaFold Protein Structure&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p id=&#34;FastFold&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/FastFold&#34;&gt;FastFold&lt;/a&gt;: accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p id=&#34;xTrimoMultimer&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/biomap-research/xTrimoMultimer&#34;&gt;xTrimoMultimer&lt;/a&gt;: accelerating structure prediction of protein monomers and multimer by 11x.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Download From Official Releases&lt;/h3&gt; &#xA;&lt;p&gt;You can visit the &lt;a href=&#34;https://www.colossalai.org/download&#34;&gt;Download&lt;/a&gt; page to download Colossal-AI with pre-built CUDA extensions.&lt;/p&gt; &#xA;&lt;h3&gt;Download From Source&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problem. :)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/hpcaitech/ColossalAI.git&#xA;cd ColossalAI&#xA;&#xA;# install dependency&#xA;pip install -r requirements/requirements.txt&#xA;&#xA;# install colossalai&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;NO_CUDA_EXT=1 pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Use Docker&lt;/h2&gt; &#xA;&lt;h3&gt;Pull from DockerHub&lt;/h3&gt; &#xA;&lt;p&gt;You can directly pull the docker image from our &lt;a href=&#34;https://hub.docker.com/r/hpcaitech/colossalai&#34;&gt;DockerHub page&lt;/a&gt;. The image is automatically uploaded upon release.&lt;/p&gt; &#xA;&lt;h3&gt;Build On Your Own&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command to build a docker image from Dockerfile provided.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Building Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker Runtime as the default when doing &lt;code&gt;docker build&lt;/code&gt;. More details can be found &lt;a href=&#34;https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime&#34;&gt;here&lt;/a&gt;. We recommend you install Colossal-AI from our &lt;a href=&#34;https://www.colossalai.org&#34;&gt;project page&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ColossalAI&#xA;docker build -t colossalai ./docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following command to start the docker container in interactive mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti --gpus all --rm --ipc=host colossalai bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join the Colossal-AI community on &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/discussions&#34;&gt;Forum&lt;/a&gt;, &lt;a href=&#34;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w&#34;&gt;Slack&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&#34; title=&#34;qrcode&#34;&gt;WeChat&lt;/a&gt; to share your suggestions, feedback, and questions with our engineering team.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to contribute to this project, please follow the guideline in &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks so much to all of our amazing contributors!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/graphs/contributors&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/contributor_avatar.png&#34; width=&#34;800px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;The order of contributor avatars is randomly shuffled.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Cite Us&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{bian2021colossal,&#xA;  title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},&#xA;  author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},&#xA;  journal={arXiv preprint arXiv:2110.14883},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlfoundations/open_clip</title>
    <updated>2022-12-01T02:05:04Z</updated>
    <id>tag:github.com,2022-12-01:/mlfoundations/open_clip</id>
    <link href="https://github.com/mlfoundations/open_clip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source implementation of CLIP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenCLIP&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.01903&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb&#34;&gt;[Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to an open source implementation of OpenAI&#39;s &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;CLIP&lt;/a&gt; (Contrastive Language-Image Pre-training).&lt;/p&gt; &#xA;&lt;p&gt;The goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. Our starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. Specifically, a ResNet-50 model trained with our codebase on OpenAI&#39;s &lt;a href=&#34;https://github.com/openai/CLIP/raw/main/data/yfcc100m.md&#34;&gt;15 million image subset of YFCC&lt;/a&gt; achieves &lt;strong&gt;32.7%&lt;/strong&gt; top-1 accuracy on ImageNet. OpenAI&#39;s CLIP model reaches &lt;strong&gt;31.3%&lt;/strong&gt; when trained on the same subset of YFCC. For ease of experimentation, we also provide code for training on the 3 million images in the &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/download&#34;&gt;Conceptual Captions&lt;/a&gt; dataset, where a ResNet-50x4 trained with our codebase reaches 22.2% top-1 ImageNet accuracy.&lt;/p&gt; &#xA;&lt;p&gt;We further this with a replication study on a dataset of comparable size to OpenAI&#39;s, &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt;, and with the larger &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-2B&lt;/a&gt; superset.&lt;/p&gt; &#xA;&lt;p&gt;We have trained:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ViT-B/32 on LAION-400M with a accuracy of &lt;strong&gt;62.9%&lt;/strong&gt;, comparable to OpenAI&#39;s &lt;strong&gt;63.2%&lt;/strong&gt;, zero-shot top-1 on ImageNet1k&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/32 on LAION-2B with a accuracy of &lt;strong&gt;66.6%&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/16 on LAION-400M achieving an accuracy of &lt;strong&gt;67.1%&lt;/strong&gt;, lower than OpenAI&#39;s &lt;strong&gt;68.3%&lt;/strong&gt; (as measured here, 68.6% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/16+ 240x240 (~50% more FLOPS than B/16 224x224) on LAION-400M achieving an accuracy of &lt;strong&gt;69.2%&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;ViT-L/14 on LAION-400M with an accuracy of &lt;strong&gt;72.77%&lt;/strong&gt;, vs OpenAI&#39;s &lt;strong&gt;75.5%&lt;/strong&gt; (as measured here, 75.3% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-L/14 on LAION-2B with an accuracy of &lt;strong&gt;75.3%&lt;/strong&gt;, vs OpenAI&#39;s &lt;strong&gt;75.5%&lt;/strong&gt; (as measured here, 75.3% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-H/14 on LAION-2B with an accuracy of &lt;strong&gt;78.0&lt;/strong&gt;. The best in1k zero-shot for released, open-source weights thus far.&lt;/li&gt; &#xA; &lt;li&gt;ViT-g/14 on LAION-2B with an accuracy of &lt;strong&gt;76.6&lt;/strong&gt;. This was trained on reduced schedule, same samples seen as 400M models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As we describe in more detail &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/#why-are-low-accuracy-clip-models-interesting&#34;&gt;below&lt;/a&gt;, CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow &lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;reliable scaling laws&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This codebase is work in progress, and we invite all to contribute in making it more accessible and useful. In the future, we plan to add support for TPU training and release larger models. We hope this codebase facilitates and promotes further research in contrastive image-text learning. Please submit an issue or send an email if you have any other requests or suggestions.&lt;/p&gt; &#xA;&lt;p&gt;Note that portions of &lt;code&gt;src/open_clip/&lt;/code&gt; modelling and tokenizer code are adaptations of OpenAI&#39;s official &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png&#34; alt=&#34;CLIP&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Credit: &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install open_clip_torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;import open_clip&#xA;&#xA;model, _, preprocess = open_clip.create_model_and_transforms(&#39;ViT-B-32-quickgelu&#39;, pretrained=&#39;laion400m_e32&#39;)&#xA;tokenizer = open_clip.get_tokenizer(&#39;ViT-B-32-quickgelu&#39;)&#xA;&#xA;image = preprocess(Image.open(&#34;CLIP.png&#34;)).unsqueeze(0)&#xA;text = tokenizer([&#34;a diagram&#34;, &#34;a dog&#34;, &#34;a cat&#34;])&#xA;&#xA;with torch.no_grad(), torch.cuda.amp.autocast():&#xA;    image_features = model.encode_image(image)&#xA;    text_features = model.encode_text(text)&#xA;    image_features /= image_features.norm(dim=-1, keepdim=True)&#xA;    text_features /= text_features.norm(dim=-1, keepdim=True)&#xA;&#xA;    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)&#xA;&#xA;print(&#34;Label probs:&#34;, text_probs)  # prints: [[1., 0., 0.]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compute billions of embeddings efficiently, you can use &lt;a href=&#34;https://github.com/rom1504/clip-retrieval&#34;&gt;clip-retrieval&lt;/a&gt; which has openclip support.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning on classification tasks&lt;/h2&gt; &#xA;&lt;p&gt;This repository is focused on training CLIP models. To fine-tune a &lt;em&gt;trained&lt;/em&gt; zero-shot model on a downstream classification task such as ImageNet, please see &lt;a href=&#34;https://github.com/mlfoundations/wise-ft&#34;&gt;our other repository: WiSE-FT&lt;/a&gt;. The &lt;a href=&#34;https://github.com/mlfoundations/wise-ft&#34;&gt;WiSE-FT repository&lt;/a&gt; contains code for our paper on &lt;a href=&#34;https://arxiv.org/abs/2109.01903&#34;&gt;Robust Fine-tuning of Zero-shot Models&lt;/a&gt;, in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;To download datasets as webdataset, we recommend &lt;a href=&#34;https://github.com/rom1504/img2dataset&#34;&gt;img2dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Conceptual Captions&lt;/h3&gt; &#xA;&lt;p&gt;OpenCLIP reads a CSV file with two columns: a path to an image, and a text caption. The names of the columns are passed as an argument to &lt;code&gt;main.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script &lt;code&gt;src/data/gather_cc.py&lt;/code&gt; will collect the Conceptual Captions images. First, download the &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/download&#34;&gt;Conceptual Captions URLs&lt;/a&gt; and then run the script from our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 src/data/gather_cc.py path/to/Train_GCC-training.tsv path/to/Validation_GCC-1.1.0-Validation.tsv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our training set contains 2.89M images, and our validation set contains 13K images.&lt;/p&gt; &#xA;&lt;h3&gt;YFCC and other datasets&lt;/h3&gt; &#xA;&lt;p&gt;In addition to specifying the training data via CSV files as mentioned above, our codebase also supports &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset&lt;/a&gt;, which is recommended for larger scale datasets. The expected format is a series of &lt;code&gt;.tar&lt;/code&gt; files. Each of these &lt;code&gt;.tar&lt;/code&gt; files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, &lt;code&gt;shard_001.tar&lt;/code&gt; could contain files such as &lt;code&gt;abc.jpg&lt;/code&gt; and &lt;code&gt;abc.txt&lt;/code&gt;. You can learn more about &lt;code&gt;webdataset&lt;/code&gt; at &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;https://github.com/webdataset/webdataset&lt;/a&gt;. We use &lt;code&gt;.tar&lt;/code&gt; files with 1,000 data points each, which we create using &lt;a href=&#34;https://github.com/webdataset/tarp&#34;&gt;tarp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download the YFCC dataset from &lt;a href=&#34;http://mmcommons.org/&#34;&gt;Multimedia Commons&lt;/a&gt;. Similar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers. The indices of images in this subset are in &lt;a href=&#34;https://github.com/openai/CLIP/raw/main/data/yfcc100m.md&#34;&gt;OpenAI&#39;s CLIP repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training CLIP&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;p&gt;We advise you first create a virtual environment with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .env&#xA;source .env/bin/activate&#xA;pip install -U pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then install openclip for training with &lt;code&gt;pip install &#39;open_clip_torch[training]&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Development&lt;/h4&gt; &#xA;&lt;p&gt;If you want to make changes to contribute code, you can close openclip then run &lt;code&gt;make install&lt;/code&gt; in openclip folder (after creating a virtualenv)&lt;/p&gt; &#xA;&lt;p&gt;Install pip PyTorch as per &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Test can be run with &lt;code&gt;make install-test&lt;/code&gt; then &lt;code&gt;make test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m pytest -x -s -v tests -k &#34;training&#34;&lt;/code&gt; to run a specific test&lt;/p&gt; &#xA;&lt;p&gt;When introducing new models, &lt;code&gt;python tests/util_test.py --model=xlm-roberta-large-ViT-H-14&lt;/code&gt; can generate new output expected data.&lt;/p&gt; &#xA;&lt;p&gt;You may run &lt;code&gt;make install-training&lt;/code&gt; to install training deps&lt;/p&gt; &#xA;&lt;h3&gt;Sample single-process running code:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --save-frequency 1 \&#xA;    --zeroshot-frequency 1 \&#xA;    --report-to tensorboard \&#xA;    --train-data=&#34;/path/to/train_data.csv&#34;  \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --csv-img-key filepath \&#xA;    --csv-caption-key title \&#xA;    --imagenet-val=/path/to/imagenet/root/val/ \&#xA;    --warmup 10000 \&#xA;    --batch-size=128 \&#xA;    --lr=1e-3 \&#xA;    --wd=0.1 \&#xA;    --epochs=30 \&#xA;    --workers=8 \&#xA;    --model RN50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;imagenet-val&lt;/code&gt; is the path to the &lt;em&gt;validation&lt;/em&gt; set of ImageNet for zero-shot evaluation, not the training set! You can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the &lt;code&gt;val&lt;/code&gt; folder should contain subfolders. If it doest not, please use &lt;a href=&#34;https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh&#34;&gt;this script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-GPU and Beyond&lt;/h3&gt; &#xA;&lt;p&gt;This code has been battle tested up to 1024 A100s and offers a variety of solutions for distributed training. We include native support for SLURM clusters.&lt;/p&gt; &#xA;&lt;p&gt;As the number of devices used to train increases, so does the space complexity of the the logit matrix. Using a na√Øve all-gather scheme, space complexity will be &lt;code&gt;O(n^2)&lt;/code&gt;. Instead, complexity may become effectively linear if the flags &lt;code&gt;--gather-with-grad&lt;/code&gt; and &lt;code&gt;--local-loss&lt;/code&gt; are used. This alteration results in one-to-one numerical results as the na√Øve method.&lt;/p&gt; &#xA;&lt;h4&gt;Epochs&lt;/h4&gt; &#xA;&lt;p&gt;For larger datasets (eg Laion2B), we recommend setting --train-num-samples to a lower value than the full epoch, for example &lt;code&gt;--train-num-samples 135646078&lt;/code&gt; to 1/16 of an epoch in conjunction with --dataset-resampled to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.&lt;/p&gt; &#xA;&lt;h4&gt;Single-Node&lt;/h4&gt; &#xA;&lt;p&gt;We make use of &lt;code&gt;torchrun&lt;/code&gt; to launch distributed jobs. The following launches a a job on a node of 4 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd open_clip/src&#xA;torchrun --nproc_per_node 4 -m training.main \&#xA;    --train-data &#39;/data/cc12m/cc12m-train-{0000..2175}.tar&#39; \&#xA;    --train-num-samples 10968539 \&#xA;    --dataset-type webdataset \&#xA;    --batch-size 320 \&#xA;    --precision amp \&#xA;    --workers 4 \&#xA;    --imagenet-val /data/imagenet/validation/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-Node&lt;/h4&gt; &#xA;&lt;p&gt;The same script above works, so long as users include information about the number of nodes and host node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd open_clip/src&#xA;torchrun --nproc_per_node=4 \&#xA;    --rdzv_endpoint=$HOSTE_NODE_ADDR \&#xA;    -m training.main \&#xA;    --train-data &#39;/data/cc12m/cc12m-train-{0000..2175}.tar&#39; \&#xA;    --train-num-samples 10968539 \&#xA;    --dataset-type webdataset \&#xA;    --batch-size 320 \&#xA;    --precision amp \&#xA;    --workers 4 \&#xA;    --imagenet-val /data/imagenet/validation/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SLURM&lt;/h4&gt; &#xA;&lt;p&gt;This is likely the easiest solution to utilize. The following script was used to train our largest models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash -x&#xA;#SBATCH --nodes=32&#xA;#SBATCH --gres=gpu:4&#xA;#SBATCH --ntasks-per-node=4&#xA;#SBATCH --cpus-per-task=6&#xA;#SBATCH --wait-all-nodes=1&#xA;#SBATCH --job-name=open_clip&#xA;#SBATCH --account=ACCOUNT_NAME&#xA;#SBATCH --partition PARTITION_NAME&#xA;&#xA;eval &#34;$(/path/to/conda/bin/conda shell.bash hook)&#34; # init conda&#xA;conda activate open_clip&#xA;export CUDA_VISIBLE_DEVICES=0,1,2,3&#xA;export MASTER_PORT=12802&#xA;&#xA;master_addr=$(scontrol show hostnames &#34;$SLURM_JOB_NODELIST&#34; | head -n 1)&#xA;export MASTER_ADDR=$master_addr&#xA;&#xA;cd /shared/open_clip&#xA;export PYTHONPATH=&#34;$PYTHONPATH:$PWD/src&#34;&#xA;srun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \&#xA;    --save-frequency 1 \&#xA;    --report-to tensorboard \&#xA;    --train-data=&#34;/data/LAION-400M/{00000..41455}.tar&#34; \&#xA;    --warmup 2000 \&#xA;    --batch-size=256 \&#xA;    --epochs=32 \&#xA;    --workers=8 \&#xA;    --model ViT-B-32 \&#xA;    --name &#34;ViT-B-32-Vanilla&#34; \&#xA;    --seed 0 \&#xA;    --local-loss \&#xA;    --gather-with-grad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resuming from a checkpoint:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --train-data=&#34;/path/to/train_data.csv&#34; \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --resume /path/to/checkpoints/epoch_K.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with pre-trained language models as text encoder:&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to use different language models as the text encoder for CLIP you can do so by using one of the huggingface model configs in &lt;code&gt;src/open_clip/model_configs&lt;/code&gt; and passing in it&#39;s tokenizer as the &lt;code&gt;--model&lt;/code&gt; and &lt;code&gt;--hf-tokenizer-name&lt;/code&gt; parameters respectively. Currently we only support RoBERTa (&#34;test-roberta&#34; config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the &lt;code&gt;--lock-text-unlocked-layers&lt;/code&gt; parameter. Here&#39;s an example command to train CLIP with the RoBERTa LM that has it&#39;s last 10 layers unfrozen:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;         --train-data=&#34;pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -&#34; \&#xA;         --train-num-samples 3000000 \&#xA;         --val-data=&#34;pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -&#34; \&#xA;         --val-num-samples 10000 \&#xA;         --dataset-type webdataset \&#xA;         --batch-size 256 \&#xA;         --warmup 2000 \&#xA;         --epochs 10 \&#xA;         --lr 5e-4 \&#xA;         --precision amp \&#xA;         --workers 6 \&#xA;         --model &#34;roberta-ViT-B-32&#34; \&#xA;         --lock-text \&#xA;         --lock-text-unlocked-layers 10 \&#xA;         --name &#34;10_unfrozen&#34; \&#xA;         --report-to &#34;tensorboard&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loss Curves&lt;/h3&gt; &#xA;&lt;p&gt;When run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png&#34; alt=&#34;CLIP zero shot training curve&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More detailed curves for Conceptual Captions are given at &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_conceptual_captions.md&#34;&gt;/docs/clip_conceptual_captions.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When training a RN50 on YFCC the same hyperparameters as above are used, with the exception of &lt;code&gt;lr=5e-4&lt;/code&gt; and &lt;code&gt;epochs=32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that to use another model, like &lt;code&gt;ViT-B/32&lt;/code&gt; or &lt;code&gt;RN50x4&lt;/code&gt; or &lt;code&gt;RN50x16&lt;/code&gt; or &lt;code&gt;ViT-B/16&lt;/code&gt;, specify with &lt;code&gt;--model RN50x4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Launch tensorboard:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tensorboard --logdir=logs/tensorboard/ --port=7777&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation / Zero-Shot&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluating local checkpoint:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --model RN101 \&#xA;    --pretrained /path/to/checkpoints/epoch_K.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --imagenet-val /path/to/imagenet/validation \&#xA;    --model ViT-B-32-quickgelu \&#xA;    --pretrained laion400m_e32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrained model details&lt;/h2&gt; &#xA;&lt;h3&gt;LAION-400M - &lt;a href=&#34;https://laion.ai/laion-400-open-dataset&#34;&gt;https://laion.ai/laion-400-open-dataset&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We are working on reproducing OpenAI&#39;s ViT results with the comparably sized (and open) LAION-400M dataset. Trained weights may be found in release &lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/tag/v0.2-weights&#34;&gt;v0.2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The LAION400M weights have been trained on the JUWELS supercomputer (see acknowledgements section below).&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 224x224&lt;/h4&gt; &#xA;&lt;p&gt;We replicate OpenAI&#39;s results on ViT-B/32, reaching a top-1 ImageNet-1k zero-shot accuracy of 62.96%.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot comparison (courtesy of Andreas F√ºrst)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_openai_compare_b32.jpg&#34; width=&#34;700&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/32 was trained with 128 A100 (40 GB) GPUs for ~36 hours, 4600 GPU-hours. The per-GPU batch size was 256 for a global batch size of 32768. 256 is much lower than it could have been (~320-384) due to being sized initially before moving to &#39;local&#39; contrastive loss.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/16 224x224&lt;/h4&gt; &#xA;&lt;p&gt;The B/16 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 67.07.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;This was the first major train session using the updated webdataset 0.2.x code. A bug was found that prevented shards from being shuffled properly between nodes/workers each epoch. This was fixed part way through training (epoch 26) but likely had an impact.&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/16 was trained with 176 A100 (40 GB) GPUS for ~61 hours, 10700 GPU-hours. Batch size per GPU was 192 for a global batch size of 33792.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/16+ 240x240&lt;/h4&gt; &#xA;&lt;p&gt;The B/16+ 240x240 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 69.21.&lt;/p&gt; &#xA;&lt;p&gt;This model is the same depth as the B/16, but increases the&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vision width from 768 -&amp;gt; 896&lt;/li&gt; &#xA; &lt;li&gt;text width from 512 -&amp;gt; 640&lt;/li&gt; &#xA; &lt;li&gt;the resolution 224x224 -&amp;gt; 240x240 (196 -&amp;gt; 225 tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16_plus_240.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;Unlike the B/16 run above, this model was a clean run with no dataset shuffling issues.&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/16+ was trained with 224 A100 (40 GB) GPUS for ~61 hours, 13620 GPU-hours. Batch size per GPU was 160 for a global batch size of 35840.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-L/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;The L/14 LAION-400M training reached a top-1 ImageNet-1k zero-shot validation score of 72.77.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_l14.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;ViT-L/14 was trained with 400 A100 (40 GB) GPUS for ~127 hours, 50800 GPU-hours. Batch size per GPU was 96 for a global batch size of 38400. Grad checkpointing was enabled.&lt;/p&gt; &#xA;&lt;h3&gt;LAION-2B (en) - &lt;a href=&#34;https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/&#34;&gt;https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A ~2B sample subset of LAION-5B with english captions (&lt;a href=&#34;https://huggingface.co/datasets/laion/laion2B-en&#34;&gt;https://huggingface.co/datasets/laion/laion2B-en&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-B/32 trained on LAION-2B, reaching a top-1 ImageNet-1k zero-shot accuracy of 65.62%.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion2b_clip_zeroshot_b32.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;ViT-B/32 was trained with 112 A100 (40 GB) GPUs. The per-GPU batch size was 416 for a global batch size of 46592. Compute generously provided by &lt;a href=&#34;https://stability.ai/&#34;&gt;stability.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A second iteration of B/32 was trained on stability.ai cluster with a larger global batch size and learning rate, hitting 66.6% top-1. See &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ViT-L/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-L/14 with a 75.3% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These weights use a different dataset mean and std than others. Instead of using the OpenAI mean &amp;amp; std, inception style normalization &lt;code&gt;[-1, 1]&lt;/code&gt; is used via a mean and std of &lt;code&gt;[0.5, 0.5, 0.5]&lt;/code&gt;. This is handled automatically if using &lt;code&gt;open_clip.create_model_and_transforms&lt;/code&gt; from pretrained weights.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-H/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-H/14 with a 78.0% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ViT-g/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-g/14 with a 76.6% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This model was trained with a shorted schedule than other LAION-2B models with 12B samples seen instead of 32+B. It matches LAION-400M training in samples seen. Many zero-shot results are lower as a result, but despite this it performs very well in some OOD zero-shot and retrieval tasks.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 roberta base&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-B/32 with roberta base encoder with a 61.7% top-1 ImageNet-1k zero-shot was trained on stability. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k&#34;&gt;https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k&lt;/a&gt; This is the first openclip model using a HF text tower. It has better performance on a range of tasks compared to the standard text encoder, see &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k/blob/main/unknown.png&#34;&gt;metrics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 xlm roberta base&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-B/32 with xlm roberta base encoder with a 62.33% top-1 ImageNet-1k zero-shot was trained on stability. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k&#34;&gt;https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k&lt;/a&gt; This is the first openclip model trained on the full laion5B dataset; hence the first multilingual clip trained with openclip. It has better performance on a range of tasks compared to the standard text encoder, see &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k/blob/main/metrics.png&#34;&gt;metrics&lt;/a&gt; A preliminary multilingual evaluation was run: 43% on imagenet1k italian (vs 21% for english B/32), 37% for imagenet1k japanese (vs 1% for english B/32 and 50% for B/16 clip japanese). It shows the multilingual property is indeed there as expected. Larger models will get even better performance.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-H/14 xlm roberta large&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-H/14 with xlm roberta large encoder with a 77.0% (vs 78% for the english equivalent) top-1 ImageNet-1k zero-shot was trained on stability. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k&#34;&gt;https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This model was trained following the &lt;a href=&#34;https://arxiv.org/abs/2111.07991&#34;&gt;LiT&lt;/a&gt; methodology: the image tower was frozen (initialized from english openclip ViT-H/14), the text tower was initialized from &lt;a href=&#34;https://huggingface.co/xlm-roberta-large&#34;&gt;xlm roberta large&lt;/a&gt; and unfrozen. This reduced training cost by a 3x factor.&lt;/p&gt; &#xA;&lt;p&gt;See full english &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/resolve/main/results_xlm_roberta_large.png&#34;&gt;metrics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On zero shot classification on imagenet with translated prompts this model reaches:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;56% in italian (vs 21% for &lt;a href=&#34;https://github.com/clip-italian/clip-italian&#34;&gt;https://github.com/clip-italian/clip-italian&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;53% in japanese (vs 54.6% for &lt;a href=&#34;https://github.com/rinnakk/japanese-clip&#34;&gt;https://github.com/rinnakk/japanese-clip&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;55.7% in chinese (to be compared with &lt;a href=&#34;https://github.com/OFA-Sys/Chinese-CLIP&#34;&gt;https://github.com/OFA-Sys/Chinese-CLIP&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;YFCC-15M&lt;/h4&gt; &#xA;&lt;p&gt;Below are checkpoints of models trained on YFCC-15M, along with their zero-shot top-1 accuracies on ImageNet and ImageNetV2. These models were trained using 8 GPUs and the same hyperparameters described in the &#34;Sample running code&#34; section, with the exception of &lt;code&gt;lr=5e-4&lt;/code&gt; and &lt;code&gt;epochs=32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt&#34;&gt;ResNet-50&lt;/a&gt; (32.7% / 27.9%)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt&#34;&gt;ResNet-101&lt;/a&gt; (34.8% / 30.0%)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CC12M - &lt;a href=&#34;https://github.com/google-research-datasets/conceptual-12m&#34;&gt;https://github.com/google-research-datasets/conceptual-12m&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt&#34;&gt;ResNet-50&lt;/a&gt; (36.45%)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pretrained Model Interface&lt;/h3&gt; &#xA;&lt;p&gt;We offer a simple model interface to instantiate both pre-trained and untrained models.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with &lt;code&gt;-quickgelu&lt;/code&gt; postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non &lt;code&gt;-quickgelu&lt;/code&gt; model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.&lt;/p&gt; &#xA;&lt;p&gt;Future trained models will use nn.GELU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import open_clip&#xA;&amp;gt;&amp;gt;&amp;gt; open_clip.list_pretrained()&#xA;[(&#39;RN50&#39;, &#39;openai&#39;),&#xA; (&#39;RN50&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50&#39;, &#39;cc12m&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;cc12m&#39;),&#xA; (&#39;RN101&#39;, &#39;openai&#39;),&#xA; (&#39;RN101&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN101-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;RN101-quickgelu&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50x4&#39;, &#39;openai&#39;),&#xA; (&#39;RN50x16&#39;, &#39;openai&#39;),&#xA; (&#39;RN50x64&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion2b_e16&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion2b_s34b_b79k&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-16-plus-240&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-16-plus-240&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion2b_s32b_b82k&#39;),&#xA; (&#39;ViT-L-14-336&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-H-14&#39;, &#39;laion2b_s32b_b79k&#39;),&#xA; (&#39;ViT-g-14&#39;, &#39;laion2b_s12b_b42k&#39;),&#xA; (&#39;roberta-ViT-B-32&#39;, &#39;laion2b_s12b_b32k&#39;),&#xA; (&#39;xlm-roberta-base-ViT-B-32&#39;, &#39;laion5b_s13b_b90k&#39;),&#xA; (&#39;xlm-roberta-large-ViT-H-14&#39;, &#39;frozen_laion5b_s13b_b90k&#39;),]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; model, train_transform, eval_transform = open_clip.create_model_and_transforms(&#39;ViT-B-32&#39;, pretrained=&#39;laion2b_s34b_b79k&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scaling trends&lt;/h2&gt; &#xA;&lt;p&gt;The plot below shows how zero-shot performance of CLIP models varies as we scale the number of samples used for training. Zero-shot performance increases steadily for both ImageNet and &lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;ImageNetV2&lt;/a&gt;, and is far from saturated at ~15M samples.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/scaling.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;h2&gt;Why are low-accuracy CLIP models interesting?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; CLIP models have high effective robustness, even at small scales.&lt;/p&gt; &#xA;&lt;p&gt;CLIP models are particularly intriguing because they are more robust to natural distribution shifts (see Section 3.3 in the &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;CLIP paper&lt;/a&gt;). This phenomena is illustrated by the figure below, with ImageNet accuracy on the x-axis and &lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;ImageNetV2&lt;/a&gt; (a reproduction of the ImageNet validation set with distribution shift) accuracy on the y-axis. Standard training denotes training on the ImageNet train set and the CLIP zero-shot models are shown as stars.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/effective_robustness.png&#34; alt=&#34;CLIP scatter plot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As observed by &lt;a href=&#34;https://arxiv.org/abs/2007.00644&#34;&gt;Taori et al., 2020&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;Miller et al., 2021&lt;/a&gt;, the in-distribution and out-of-distribution accuracies of models trained on ImageNet follow a predictable linear trend (the red line in the above plot). &lt;em&gt;Effective robustness&lt;/em&gt; quantifies robustness as accuracy beyond this baseline, i.e., how far a model lies above the red line. Ideally a model would not suffer from distribution shift and fall on the y = x line (&lt;a href=&#34;http://proceedings.mlr.press/v119/shankar20c.html&#34;&gt;trained human labelers are within a percentage point of the y = x line&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Even though the CLIP models trained with this codebase achieve much lower accuracy than those trained by OpenAI, our models still lie on the same trend of improved effective robustness (the purple line). Therefore, we can study what makes CLIP robust without requiring industrial-scale compute.&lt;/p&gt; &#xA;&lt;p&gt;For more information on effective robustness, please see:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;Recht et al., 2019&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00644&#34;&gt;Taori et al., 2020&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;Miller et al., 2021&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To know more about the factors that contribute to CLIP&#39;s robustness refer to &lt;a href=&#34;https://arxiv.org/abs/2205.01397&#34;&gt;Fang et al., 2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (&lt;a href=&#34;http://www.gauss-centre.eu&#34;&gt;www.gauss-centre.eu&lt;/a&gt;) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J√ºlich Supercomputing Centre (JSC).&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;Current development of this repository is led by &lt;a href=&#34;https://rwightman.com/&#34;&gt;Ross Wightman&lt;/a&gt;, &lt;a href=&#34;http://cadegordon.io/&#34;&gt;Cade Gordon&lt;/a&gt;, and &lt;a href=&#34;http://vaishaal.com/&#34;&gt;Vaishaal Shankar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://gabrielilharco.com/&#34;&gt;Gabriel Ilharco*&lt;/a&gt;, &lt;a href=&#34;https://mitchellnw.github.io/&#34;&gt;Mitchell Wortsman*&lt;/a&gt;, &lt;a href=&#34;https://nicholas.carlini.com/&#34;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&#34;http://www.achaldave.com/&#34;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&#34;http://vaishaal.com/&#34;&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~miller_john/&#34;&gt;John Miller&lt;/a&gt;, &lt;a href=&#34;https://hsnamkoong.github.io/&#34;&gt;Hongseok Namkoong&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~hannaneh/&#34;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~ali/&#34;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/ludwigs/&#34;&gt;Ludwig Schmidt&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://jongwook.kim/&#34;&gt;Jong Wook Kim&lt;/a&gt; and &lt;a href=&#34;https://github.com/Newmu&#34;&gt;Alec Radford&lt;/a&gt; for help with reproducing CLIP!&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{ilharco_gabriel_2021_5143773,&#xA;  author       = {Ilharco, Gabriel and&#xA;                  Wortsman, Mitchell and&#xA;                  Wightman, Ross and&#xA;                  Gordon, Cade and&#xA;                  Carlini, Nicholas and&#xA;                  Taori, Rohan and&#xA;                  Dave, Achal and&#xA;                  Shankar, Vaishaal and&#xA;                  Namkoong, Hongseok and&#xA;                  Miller, John and&#xA;                  Hajishirzi, Hannaneh and&#xA;                  Farhadi, Ali and&#xA;                  Schmidt, Ludwig},&#xA;  title        = {OpenCLIP},&#xA;  month        = jul,&#xA;  year         = 2021,&#xA;  note         = {If you use this software, please cite it as below.},&#xA;  publisher    = {Zenodo},&#xA;  version      = {0.1},&#xA;  doi          = {10.5281/zenodo.5143773},&#xA;  url          = {https://doi.org/10.5281/zenodo.5143773}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Radford2021LearningTV,&#xA;  title={Learning Transferable Visual Models From Natural Language Supervision},&#xA;  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},&#xA;  booktitle={ICML},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/390536799&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/390536799.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>