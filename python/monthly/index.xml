<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-01T01:48:31Z</updated>
  <subtitle>Monthly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vvbbnn00/WARP-Clash-API</title>
    <updated>2024-06-01T01:48:31Z</updated>
    <id>tag:github.com,2024-06-01:/vvbbnn00/WARP-Clash-API</id>
    <link href="https://github.com/vvbbnn00/WARP-Clash-API" rel="alternate"></link>
    <summary type="html">&lt;p&gt;该项目可以让你通过订阅的方式使用Cloudflare WARP+，自动获取流量。This project enables you to use Cloudflare WARP+ through subscription, automatically acquiring traffic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WARP Clash API&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/vvbbnn00/WARP-Clash-API&#34; alt=&#34;GitHub License&#34;&gt; &lt;a href=&#34;https://app.codacy.com/gh/vvbbnn00/WARP-Clash-API/dashboard?utm_source=gh&amp;amp;utm_medium=referral&amp;amp;utm_content=&amp;amp;utm_campaign=Badge_grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/67ca8d105fb947eca6204230ba3ac09b&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/stars/vvbbnn00/WARP-Clash-API?style=flat&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;中文 | &lt;a href=&#34;https://raw.githubusercontent.com/vvbbnn00/WARP-Clash-API/master/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;本项目是完全非商业项目，仅供学习交流使用，请勿用于非法用途，否则后果自负。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🤔 这是什么？&lt;/h2&gt; &#xA;&lt;p&gt;该项目可以让你通过订阅的方式使用&lt;code&gt;WARP+&lt;/code&gt;，支持&lt;code&gt;Clash&lt;/code&gt;、&lt;code&gt;Shadowrocket&lt;/code&gt;等客户端。项目内置了 刷取&lt;code&gt;WARP+&lt;/code&gt;流量的功能，可以让你的&lt;code&gt;WARP+&lt;/code&gt;流量不再受限制（每&lt;code&gt;18&lt;/code&gt;秒可获得&lt;code&gt;1GB&lt;/code&gt;流量），同时， 配备了&lt;code&gt;IP&lt;/code&gt;选优功能。支持&lt;code&gt;Docker compose&lt;/code&gt; 一键部署，无需额外操作，即可享受你自己的&lt;code&gt;WARP+&lt;/code&gt;私 有高速节点！&lt;/p&gt; &#xA;&lt;h2&gt;💡 特色功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;💻 支持&lt;code&gt;Clash&lt;/code&gt;、&lt;code&gt;Surge&lt;/code&gt;、&lt;code&gt;Shadowrocket&lt;/code&gt;等客户端&lt;/li&gt; &#xA; &lt;li&gt;🔑 支持设置您自己的&lt;code&gt;LicenseKey&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;🌏 支持&lt;code&gt;IP&lt;/code&gt;选优&lt;/li&gt; &#xA; &lt;li&gt;🐋 支持&lt;code&gt;Docker compose&lt;/code&gt;一键部署&lt;/li&gt; &#xA; &lt;li&gt;📕 全自动刷取&lt;code&gt;WARP+&lt;/code&gt;流量，请求经过代理，防封&lt;code&gt;IP&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;❓ 每次更新订阅随机节点，让你体验抽卡的乐趣&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 快速上手&lt;/h2&gt; &#xA;&lt;h3&gt;1. 安装&lt;code&gt;Docker&lt;/code&gt;和&lt;code&gt;Docker compose&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Docker&lt;/code&gt; 安装教程：&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;https://docs.docker.com/engine/install/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Docker compose&lt;/code&gt; 安装教程：&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;https://docs.docker.com/compose/install/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. 下载项目&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/vvbbnn00/WARP-Clash-API.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. [可选] 配置&lt;code&gt;SECRET_KEY&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;若您需要在公网上部署该项目，建议您配置&lt;code&gt;SECRET_KEY&lt;/code&gt;与&lt;code&gt;PUBLIC_URL&lt;/code&gt;。在项目目录下创建 &lt;code&gt;.env.local&lt;/code&gt;文件，写入如下内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SECRET_KEY=your_secret_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;关于环境变量的更多信息，请参考&lt;a href=&#34;https://raw.githubusercontent.com/vvbbnn00/WARP-Clash-API/master/#-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F&#34;&gt;环境变量&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;4. 编译并运行&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. 获取订阅链接&lt;/h3&gt; &#xA;&lt;p&gt;访问&lt;code&gt;http://你的IP:21001&lt;/code&gt;，输入&lt;code&gt;SECRET_KEY&lt;/code&gt;（若没有配置，则可以留空），即可获取订阅链接。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🎉 大功告成&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🌏 手动IP选优&lt;/h2&gt; &#xA;&lt;p&gt;项目本身包含了一个选优过的&lt;code&gt;IP&lt;/code&gt;列表，但是由于&lt;code&gt;WARP&lt;/code&gt;的&lt;code&gt;IP&lt;/code&gt; 是动态的，所以可能会出现&lt;code&gt;IP&lt;/code&gt;不可用的 情况。若您需要手动选优，可以遵循以下步骤：&lt;/p&gt; &#xA;&lt;p&gt;若您通过&lt;code&gt;docker-compose&lt;/code&gt;部署，可以在项目目录下通过以下命令手动执行&lt;code&gt;IP&lt;/code&gt;选优：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose exec warp-clash python3 app.py optimize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;否则，可以在项目目录下执行以下命令：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 app.py optimize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔧 环境变量&lt;/h2&gt; &#xA;&lt;p&gt;没错，您可以通过环境变量来配置该项目，在配置时，只需新建一个&lt;code&gt;.env.local&lt;/code&gt;文件，写入您需要的环境 变量即可。&lt;/p&gt; &#xA;&lt;p&gt;以下是可用的环境变量：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;变量名&lt;/th&gt; &#xA;   &lt;th&gt;默认值&lt;/th&gt; &#xA;   &lt;th&gt;说明&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DELAY_THRESHOLD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;500&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;延迟阈值，超过该阈值的&lt;code&gt;IP&lt;/code&gt;将被剔除&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DO_GET_WARP_DATA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;是否刷取&lt;code&gt;WARP+&lt;/code&gt;流量，若不需要刷取流量，则设置为&lt;code&gt;False&lt;/code&gt;即可&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GET_WARP_DATA_INTERVAL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;18&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;刷取&lt;code&gt;WARP+&lt;/code&gt;流量的时间间隔，单位为秒，每隔该时间间隔会刷取一次&lt;code&gt;WARP+&lt;/code&gt;流量，不建议间隔设置过短。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LOSS_THRESHOLD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;丢包率阈值，超过该阈值的&lt;code&gt;IP&lt;/code&gt;将被剔除&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PROXY_POOL_URL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;https://getproxy.bzpl.tech/get/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IP代理池地址，用于刷取&lt;code&gt;WARP+&lt;/code&gt;流量，您可以自行搭建，参照&lt;a href=&#34;https://github.com/jhao104/proxy_pool&#34;&gt;proxy_pool&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PUBLIC_URL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;无&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;部署在公网上时，填写公网&lt;code&gt;IP&lt;/code&gt;或域名，用于生成订阅链接，比如 &lt;code&gt;https://subs.zeabur.app&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RANDOM_COUNT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;每次更新订阅随机节点的数量&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REOPTIMIZE_INTERVAL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;重新选优的时间间隔，单位为秒，若小于等于0，则不会重新选优，否则每隔该时间间隔会重新选优一次，不建议间隔设置过短。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REQUEST_RATE_LIMIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;限制X秒一次请求，该功能不太稳定，建议不要开启&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SECRET_KEY&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;无&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;用于保护订阅链接，若不配置，则不需要输入&lt;code&gt;SECRET_KEY&lt;/code&gt;即可获取订阅链接&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SHARE_SUBSCRIPTION&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;若您的站点想要向社区分享订阅，但不想让自己的账户信息被公开或修改，可以设置为&lt;code&gt;True&lt;/code&gt;，此时，访问订阅链接时，不需要输入&lt;code&gt;SECRET_KEY&lt;/code&gt;即可获取，而对于其他的操作，仍然需要输入&lt;code&gt;SECRET_KEY&lt;/code&gt;。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;📝 配置示例&lt;/h3&gt; &#xA;&lt;p&gt;例如，您设置&lt;code&gt;SECRET_KEY&lt;/code&gt;为&lt;code&gt;123456&lt;/code&gt;，并打算将订阅分享给社区，那么您的&lt;code&gt;.env.local&lt;/code&gt; 文件应该 如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;SECRET_KEY=123456&#xA;SHARE_SUBSCRIPTION=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🧰 进阶操作&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;请注意，如果您设置了&lt;code&gt;SECRET_KEY&lt;/code&gt;，需要在URL的末尾添加&lt;code&gt;key&lt;/code&gt;参数&lt;/strong&gt;，例如：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;http://your_IP:21001/some/api/actions?key=your_secret_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;重置账户的&lt;code&gt;PublicKey&lt;/code&gt;和&lt;code&gt;PrivateKey&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;项目支持您通过请求以下接口来重置&lt;code&gt;PublicKey&lt;/code&gt;和&lt;code&gt;PrivateKey&lt;/code&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://host:port/api/account/reset_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;重置过后，需要重新获取订阅内容，否则可能无法使用。&lt;/p&gt; &#xA;&lt;h3&gt;设置自己的&lt;code&gt;LicenseKey&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;若您已经拥有了&lt;code&gt;WARP+&lt;/code&gt;的&lt;code&gt;LicenseKey&lt;/code&gt;，可以通过以下接口来设置：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://host:port/api/account/update_license -H &#34;Content-Type: application/json&#34; -d &#34;{\&#34;license_key\&#34;: \&#34;your_license_key\&#34;}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;请注意，当您设置了&lt;code&gt;LicenseKey&lt;/code&gt;后，其&lt;code&gt;PublicKey&lt;/code&gt;和&lt;code&gt;PrivateKey&lt;/code&gt;将会被重置，需要重新获取订阅 内容。&lt;/p&gt; &#xA;&lt;h3&gt;使用 IPv6 优选&lt;/h3&gt; &#xA;&lt;p&gt;从原理上来看，本服务通过读取&lt;code&gt;config/result_v6.csv&lt;/code&gt;文件获取&lt;code&gt;IPv6&lt;/code&gt;地址，因此您可以在支持 &lt;code&gt;IPv6&lt;/code&gt;的服务器上运行&lt;code&gt;IP&lt;/code&gt;选优功能来获得&lt;code&gt;IPv6&lt;/code&gt;地址。获取到的接入地址列表只需写入 &lt;code&gt;config/result_v6.csv&lt;/code&gt;文件中，相比在&lt;code&gt;Docker&lt;/code&gt;容器中运行&lt;code&gt;IP&lt;/code&gt;选优功能，此方法更为简便。&lt;/p&gt; &#xA;&lt;p&gt;如果您需要在&lt;code&gt;Docker&lt;/code&gt;容器中运行&lt;code&gt;IP&lt;/code&gt;选优功能，可以使用&lt;code&gt;docker-compose_ipv6.yaml&lt;/code&gt; 文件来让&lt;code&gt;Docker&lt;/code&gt;镜像支持&lt;code&gt;IPv6&lt;/code&gt;。在运行前，请确保您的服务器支持&lt;code&gt;IPv6&lt;/code&gt;，并在&lt;code&gt;Docker&lt;/code&gt;服务的 &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt;中添加以下内容，并重启&lt;code&gt;Docker&lt;/code&gt;服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;experimental&#34;: true,&#xA;  &#34;ip6tables&#34;: true,&#xA;  &#34;ipv6&#34;: true,&#xA;  &#34;fixed-cidr-v6&#34;: &#34;2001:db8:1::/64&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;若您在先前已经运行过&lt;code&gt;Docker&lt;/code&gt;服务，在运行之前，请先停止之前的服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，您可以通过以下命令来运行&lt;code&gt;Docker&lt;/code&gt;服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f docker-compose_ipv6.yaml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗂️ 引用项目&lt;/h2&gt; &#xA;&lt;p&gt;本项目的开发参照了以下项目，感谢这些开源项目的作者：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitlab.com/Misaka-blog/warp-script&#34;&gt;warp-script&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replit.com/@aliilapro/warp&#34;&gt;warp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ViRb3/wgcf&#34;&gt;wgcf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jhao104/proxy_pool&#34;&gt;proxy_pool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dev.maxmind.com/geoip/geolite2-free-geolocation-data&#34;&gt;geolite2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>khoj-ai/khoj</title>
    <updated>2024-06-01T01:48:31Z</updated>
    <id>tag:github.com,2024-06-01:/khoj-ai/khoj</id>
    <link href="https://github.com/khoj-ai/khoj" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Your AI second brain. Get answers to your questions, whether they be online or in your own notes. Use online AI models (e.g gpt4) or private, local LLMs (e.g llama3). Self-host locally or use our cloud instance. Access from Obsidian, Emacs, Desktop app, Web or Whatsapp.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khoj-ai/khoj/master/src/khoj/interface/web/assets/icons/khoj-logo-sideways-500.png&#34; width=&#34;230&#34; alt=&#34;Khoj Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/khoj-ai/khoj/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/khoj-ai/khoj/pkgs/container/khoj&#34;&gt;&lt;img src=&#34;https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg?sanitize=true&#34; alt=&#34;dockerize&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/khoj-assistant/&#34;&gt;&lt;img src=&#34;https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/discord/1112065956647284756?style=plastic&amp;amp;label=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;The open-source, personal AI for your digital brain&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.khoj.dev&#34;&gt;🤖 Read Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://khoj.dev&#34;&gt;🏮 Khoj Cloud&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://discord.gg/BDgyabRM6e&#34;&gt;💬 Get Involved&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://blog.khoj.dev&#34;&gt;📚 Read Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;Khoj is an application that creates always-available, personal AI agents for you to extend your capabilities.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can share your notes and documents to extend your digital brain.&lt;/li&gt; &#xA;  &lt;li&gt;Your AI agents have access to the internet, allowing you to incorporate realtime information.&lt;/li&gt; &#xA;  &lt;li&gt;Khoj is accessible on Desktop, Emacs, Obsidian, Web and Whatsapp.&lt;/li&gt; &#xA;  &lt;li&gt;You can share pdf, markdown, org-mode, notion files and github repositories.&lt;/li&gt; &#xA;  &lt;li&gt;You&#39;ll get fast, accurate semantic search on top of your docs.&lt;/li&gt; &#xA;  &lt;li&gt;Your agents can create deeply personal images and understand your speech.&lt;/li&gt; &#xA;  &lt;li&gt;Khoj is open-source, self-hostable. Always.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;hr&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;See it in action&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/khoj-ai/khoj/raw/master/documentation/assets/img/using_khoj_for_studying.gif?raw=true&#34; alt=&#34;Khoj Demo&#34;&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://app.khoj.dev&#34;&gt;https://app.khoj.dev&lt;/a&gt; to see Khoj live.&lt;/p&gt; &#xA;&lt;h2&gt;Full feature list&lt;/h2&gt; &#xA;&lt;p&gt;You can see the full feature list &lt;a href=&#34;https://docs.khoj.dev/category/features&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Self-Host&lt;/h2&gt; &#xA;&lt;p&gt;To get started with self-hosting Khoj, &lt;a href=&#34;https://docs.khoj.dev/get-started/setup&#34;&gt;read the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Cheers to our awesome contributors! 🎉&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/khoj-ai/khoj/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=khoj-ai/khoj&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Interested in Contributing?&lt;/h3&gt; &#xA;&lt;p&gt;We are always looking for contributors to help us build new features, improve the project documentation, or fix bugs. If you&#39;re interested, please see our &lt;a href=&#34;https://docs.khoj.dev/contributing/development&#34;&gt;Contributing Guidelines&lt;/a&gt; and check out our &lt;a href=&#34;https://github.com/orgs/khoj-ai/projects/4&#34;&gt;Contributors Project Board&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/sponsors/khoj-ai&#34;&gt;Sponsors&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Shout out to our brilliant sponsors! 🌈&lt;/p&gt; &#xA;&lt;a href=&#34;http://github.com/beekeeb&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/beekeeb/piantor/main/docs/beekeeb.png&#34; width=&#34;250/&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>dnhkng/GlaDOS</title>
    <updated>2024-06-01T01:48:31Z</updated>
    <id>tag:github.com,2024-06-01:/dnhkng/GlaDOS</id>
    <link href="https://github.com/dnhkng/GlaDOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the Personality Core for GLaDOS, the first steps towards a real-life implementation of the AI from the Portal series by Valve.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLaDOS Personality Core&lt;/h1&gt; &#xA;&lt;p&gt;This is a project dedicated to building a real-life version of GLaDOS.&lt;/p&gt; &#xA;&lt;p&gt;NEW: If you want to chat or join the community, &lt;a href=&#34;https://discord.gg/9ZRrGj4s&#34;&gt;Join our discord!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KbUfWpykBGg&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/KbUfWpykBGg/0.jpg&#34; alt=&#34;localGLaDOS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This is a hardware and software project that will create an aware, interactive, and embodied GLaDOS.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will entail:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train GLaDOS voice generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generate a prompt that leads to a realistic &#34;Personality Core&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate a &lt;a href=&#34;https://github.com/cpacker/MemGPT&#34;&gt;MemGPT&lt;/a&gt; medium- and long-term memory for GLaDOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Give GLaDOS vision via &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create 3D-printable parts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Design the animatronics system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The initial goals are to develop a low-latency platform, where GLaDOS can respond to voice interactions within 600ms.&lt;/p&gt; &#xA;&lt;p&gt;To do this, the system constantly records data to a circular buffer, waiting for &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;voice to be detected&lt;/a&gt;. When it&#39;s determined that the voice has stopped (including detection of normal pauses), it will be &lt;a href=&#34;https://github.com/huggingface/distil-whisper&#34;&gt;transcribed quickly&lt;/a&gt;. This is then passed to streaming &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;local Large Language Model&lt;/a&gt;, where the streamed text is broken by sentence, and passed to a &lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;text-to-speech system&lt;/a&gt;. This means further sentences can be generated while the current is playing, reducing latency substantially.&lt;/p&gt; &#xA;&lt;h3&gt;Subgoals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The other aim of the project is to minimize dependencies, so this can run on constrained hardware. That means no PyTorch or other large packages.&lt;/li&gt; &#xA; &lt;li&gt;As I want to fully understand the system, I have removed a large amount of redirection: which means extracting and rewriting code. i.e. as GLaDOS only speaks English, I have rewritten the wrapper around &lt;a href=&#34;https://espeak.sourceforge.net/&#34;&gt;espeak&lt;/a&gt; and the entire Text-to-Speech subsystem is about 500 LOC and has only 3 dependencies: numpy, onnxruntime, and sounddevice.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hardware System&lt;/h2&gt; &#xA;&lt;p&gt;This will be based on servo- and stepper-motors. 3D printable STL will be provided to create GlaDOS&#39;s body, and she will be given a set of animations to express herself. The vision system will allow her to track and turn toward people and things of interest.&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instruction&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;em&gt;New Simplified Windows Installation Process&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Don&#39;t want to compile anything? Try this simplified process, but be aware it&#39;s still in the experimental stage!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the Microsoft Store, search for &lt;code&gt;python&lt;/code&gt; and install Python 3.12. a. To use Python 3.10, install &lt;code&gt;typing_extensions&lt;/code&gt; and replace &lt;code&gt;import typing&lt;/code&gt; in &lt;code&gt;glados/llama.py&lt;/code&gt; with &lt;code&gt;import typing_extensions&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download and unzip this repository somewhere in your home folder.&lt;/li&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;install_windows.bat&lt;/code&gt;. During the process, you will be prompted to install eSpeak-ng, which is necessary for GLaDOS&#39;s speech capabilities. This step also downloads the Whisper voice recognition model and the Llama-3 8B model.&lt;/li&gt; &#xA; &lt;li&gt;Once this is all done, you can initiate GLaDOS with the &lt;code&gt;start_windows.bat&lt;/code&gt; script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Regular installation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to install the TTS Engine on your machine, please follow the steps below. This has only been tested on Linux, but I think it will work on Windows with small tweaks. If you are on Windows, I would recommend WSL with an Ubuntu image. Proper Windows and Mac support is in development.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng&#34;&gt;&lt;code&gt;espeak&lt;/code&gt;&lt;/a&gt; synthesizer according to the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng/raw/master/docs/guide.md&#34;&gt;installation instructions&lt;/a&gt; for your operating system.&lt;/li&gt; &#xA; &lt;li&gt;Install the required Python packages, e.g., by running &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; on Mac or Linux systems without an Nvidia GPU, and &lt;code&gt;pip install -r requirements_cuda.txt&lt;/code&gt; if you have a modern Nvidia GPU.&lt;/li&gt; &#xA; &lt;li&gt;Download the models: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/ggml-medium-32-2.en.bin?download=true&#34;&gt;voice recognition model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf?download=true&#34;&gt;Llama-3 8B&lt;/a&gt; or&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bartowski/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-IQ4_XS.gguf?download=true&#34;&gt;Llama-3 70B&lt;/a&gt; and put them in the &#34;.models&#34; directory.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;For voice recognition, we use &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;Whisper.cpp&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;You can either download the compiled &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/releases&#34;&gt;whisper.cpp DLLs&lt;/a&gt; (recommended for Windows), and copy the dll to the ./submodules/whisper.cpp directory&lt;/li&gt; &#xA;   &lt;li&gt;Or compile them yourself. &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;To pull the code, from the GLaDOS directory use: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Move to the right subdirectory: &lt;code&gt;cd submodules/whisper.cpp&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Compile for your system &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;(see the Documentation)&lt;/a&gt;, e.g. &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt;Linux with &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#nvidia-gpu-support&#34;&gt;CUDA&lt;/a&gt;: &lt;code&gt;WHISPER_CUDA=1 make libwhisper.so -j&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;Mac with &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#core-ml-support&#34;&gt;CoreML&lt;/a&gt;: &lt;code&gt;WHISPER_COREML=1 make libwhisper.so -j&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;For the LLM, you have two option: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Compile llama.cpp: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Use: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt; to pull the llama.cpp repo&lt;/li&gt; &#xA;     &lt;li&gt;Move to the right subdirectory: &lt;code&gt;cd submodules/llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Compile llama.cpp, &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;(see the Documentation)&lt;/a&gt; &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt;Linux with &lt;a href=&#34;https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#cuda&#34;&gt;CUDA&lt;/a&gt; &lt;code&gt;make server LLAMA_CUDA=1&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;MacOS with &lt;a href=&#34;https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#metal-build&#34;&gt;Metal&lt;/a&gt; &lt;code&gt;make server&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Use a commercial API or install an inference backend yourself, such as Ollama or Llamafile: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Find and install a backend with an OpenAI compatible API (most of them)&lt;/li&gt; &#xA;     &lt;li&gt;Edit the glados_config.yaml &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt;update &lt;code&gt;completion_url&lt;/code&gt; to the URL of your local server&lt;/li&gt; &#xA;       &lt;li&gt;for commercial APIs, add the &lt;code&gt;api_key&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;remove the LlamaServer configurations (make them null)&lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Help Section&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have an error about packages or files not being found, make sure you have the whisper and llama binaries in the respective submodules folders! They are empy by default, and you manually have to add the binaries as described above!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you are using the right Llama-3 Model! I have made Llama-3 8B, with the quantization Q6_K the default. You might need to redownload the model if you don&#39;t have &lt;code&gt;Meta-Llama-3-8B-Instruct-Q6_K.gguf&lt;/code&gt; in your models folder!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have limited VRAM, you can save 3Gb by using downloading a &lt;a href=&#34;https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-IQ3_XS.gguf?download=true&#34;&gt;highly quantised IQ3_XS model&lt;/a&gt; and moving it to the models folder. If you do this, modifiy the &lt;code&gt;glados_config.yaml&lt;/code&gt; to modify the model used: &lt;code&gt;model_path: &#34;./models/Meta-Llama-3-8B-Instruct-IQ3_XS.gguf&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you find you are getting stuck in loops, as GLaDOS is hearing herself speak, you have two options:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Solve this by upgrading your hardware. You need to you either headphone, so she can&#39;t physically hear herself speak, or a conference-style room microphone/speaker. These have hardware sound cancellation, and prevent these loops.&lt;/li&gt; &#xA;   &lt;li&gt;Disable voice interruption. This means neither you nor GLaDOS can interrupt when GLaDOS is speaking. To accomplish this, edit the &lt;code&gt;glados_config.yaml&lt;/code&gt;, and change &lt;code&gt;interruptible:&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Windows Run&lt;/h2&gt; &#xA;&lt;p&gt;Prerequisite WSL2 with fresh drivers, here is guide &lt;a href=&#34;https://docs.docker.com/desktop/gpu/&#34;&gt;https://docs.docker.com/desktop/gpu/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;put models in models dir or mount that dir into a docker container&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker build -t glados .&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker run -e &#34;PULSE_SERVER=/mnt/wslg/PulseServer&#34; -v &#34;/mnt/wslg/:/mnt/wslg/&#34; --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 glados&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It works in ubuntu terminal started with WSL2&lt;/p&gt; &#xA;&lt;h2&gt;Running GLaDOS&lt;/h2&gt; &#xA;&lt;p&gt;To start GLaDOS, use: &lt;code&gt;python glados.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can stop with &#34;Ctrl-c&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;You can test the systems by exploring the &#39;demo.ipynb&#39;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/9828&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/9828&#34; alt=&#34;dnhkng%2FGlaDOS | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dnhkng/GlaDOS&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dnhkng/GlaDOS&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>