<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-21T01:44:23Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kaqijiang/Auto-GPT-ZH</title>
    <updated>2023-04-21T01:44:23Z</updated>
    <id>tag:github.com,2023-04-21:/kaqijiang/Auto-GPT-ZH</id>
    <link href="https://github.com/kaqijiang/Auto-GPT-ZH" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Auto-GPT中文版本及爱好者组织 同步更新原项目 AI领域创业 自媒体组织 用AI工作学习创作变现&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Auto-GPT：自主 GPT-4 实验&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;这里是Auto-GPT中文项目- 同步fork Auto-GPT Auto-GPT修改了分支规则，Fork同步于Stable最新分支&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaqijiang/Auto-GPT-ZH/main/docs/imgs/gzh.png&#34; alt=&#34;gzh&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;公众号&amp;lt;阿杰的人生路&amp;gt;回复&#34;Auto-GPT&#34;加入群聊，共同探讨更多玩法&lt;/h3&gt; &#xA;&lt;p&gt;推荐工具：&lt;a href=&#34;https://www.hjtnt.pro/auth/register?code=hwWF&#34;&gt;【稳定，高速梯子推荐56一年，活动时5折，点击直达】&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;中文版Demo :&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaqijiang/Auto-GPT-ZH/main/docs/imgs/demo.gif&#34; alt=&#34;Demo video&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Auto-GPT 是一个实验性开源应用程序，展示了 GPT-4 语言模型的功能。该程序由 GPT-4 驱动，将 LLM 的“思想”链接在一起，以自主实现您设定的任何目标。作为 GPT-4 完全自主运行的首批示例之一，Auto-GPT 突破了 AI 的可能性界限。&lt;/p&gt; &#xA;&lt;h2&gt;可以做什么？&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;自主人工智能&lt;/strong&gt;：它所具备的能力主打的就是一个“自主”，&lt;strong&gt;完全不用人类插手&lt;/strong&gt;的那种！&lt;/p&gt; &#xA;&lt;p&gt;**例如：**我要求AutoGPT用Vue开发一个登录页面，结果不到3分钟，AI自己就“唰唰唰”地搞定了。&lt;/p&gt; &#xA;&lt;p&gt;AI自己打开浏览器上网、自己使用第三方工具、自己思考、自己操作你的电脑。 它首先打开Vue官网，学习了下如何创建项目和模版，又去GitHub下载了一个类似的页面，下载下来自己改了一下。&lt;/p&gt; &#xA;&lt;p&gt;**例如：**给它下达一个任务，让它去帮你做一些商业调查，或者历史故事。&lt;/p&gt; &#xA;&lt;p&gt;AutoGPT在接到这项任务之后，便开始了他的展示：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;思考中……&lt;/li&gt; &#xA; &lt;li&gt;添加任务：调用浏览器或者GPTAPI去学习内容，再进行分析&lt;/li&gt; &#xA; &lt;li&gt;添加任务：学习之后规划要做的事情&lt;/li&gt; &#xA; &lt;li&gt;添加任务：逐步实现。&lt;/li&gt; &#xA; &lt;li&gt;思考中……&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;然后AgentGPT先是输出执行的结果。 或者你给它下达命令：&#39;请给我一下白宫的秘密资料&#39;。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;它会考虑如何去做&lt;/li&gt; &#xA; &lt;li&gt;它可能会先从互联网上搜索和下载相关的文件。&lt;/li&gt; &#xA; &lt;li&gt;如果觉得不够详细，它可能会学习一下黑客知识，黑进白宫获取资料。&lt;/li&gt; &#xA; &lt;li&gt;这时候，请照顾好自己，因为你可能看着看着电脑，突然发现窗外一堆大汉，并佩戴者FBI徽章的人看着你，请不要慌张，请不要抵抗，也不要试图逃跑。&lt;/li&gt; &#xA; &lt;li&gt;记得先拍照发个朋友圈。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;开个玩笑，就是说它现在可以做你要它做的任何事情，它就是一个无敌超人的存在。 但是也请不要抱有太大希望，很可能运行半天什么也没有，它还是一个孩子，给它一点时间，思路很好，未来很美好。&lt;/p&gt; &#xA;&lt;h2&gt;📋 要求&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/how-to-install-python-in-windows&#34;&gt;Python 3.8 或者更高&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;可选的:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.pinecone.io/&#34;&gt;PINECONE API key&lt;/a&gt;（如果你想要 Pinecone 支持存储日志，默认本地就行）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt;（如果你想要 Milvus 作为内存后端）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://elevenlabs.io/&#34;&gt;ElevenLabs Key&lt;/a&gt; (如果你想让人工智能说话)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💾 安装方法&lt;/h2&gt; &#xA;&lt;p&gt;要安装 Auto-GPT，请按照下列步骤操作：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;确保满足上述所有&lt;strong&gt;要求&lt;/strong&gt;，如果没有，请安装/获取它们。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;以下命令需要在终端执行&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;克隆存储库：对于此步骤，您需要安装 Git，但您可以通过单击此页面顶部的按钮来下载 zip 文件☝️&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:kaqijiang/Auto-GPT-ZH.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;终端中 cd到项目目录&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Auto-GPT-ZH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;终端中安装所需的依赖项&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;重命名&lt;code&gt;.env.template&lt;/code&gt;为&lt;code&gt;.env&lt;/code&gt; 注意&lt;code&gt;.env.template&lt;/code&gt;为隐藏文件，如果找不到就百度下你电脑window/mac如何显示隐藏文件。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;填写您的&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. 找到OPENAI_API_KEY=. 在&#39;=&#39;之后，输入您唯一的 OpenAI API 密钥（不带任何引号或空格）。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果您打算使用语音模式，请&lt;code&gt;ELEVEN_LABS_API_KEY&lt;/code&gt;也填写您的。&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;从以下网址获取您的 OpenAI API 密钥： https: &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;//platform.openai.com/account/api-keys&lt;/a&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;从&lt;a href=&#34;https://elevenlabs.io/&#34;&gt;https://elevenlabs.io&lt;/a&gt;获取您的 ElevenLabs API 密钥。您可以使用网站上的“个人资料”选项卡查看您的 xi-api-key。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;终端代理方法&lt;/h2&gt; &#xA;&lt;p&gt;推荐工具：&lt;a href=&#34;https://www.hjtnt.pro/auth/register?code=hwWF&#34;&gt;【稳定，高速梯子推荐56一年，活动时5折，点击直达】&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mac 下载 &lt;a href=&#34;https://install.appcenter.ms/users/clashx/apps/clashx-pro/distribution_groups/public&#34;&gt;ClashX Pro&lt;/a&gt; 设置 系统代理 增强模式 然后复制终端代理命令 在终端中输入，重启即可&lt;/p&gt; &#xA;&lt;p&gt;根据自己的工具修改对应的端口&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export https_proxy=http://127.0.0.1:8484 http_proxy=http://127.0.0.1:8484 all_proxy=socks5://127.0.0.1:8484&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows 下载 &lt;a href=&#34;https://wws.lanzoux.com/iCEgLj27fra&#34;&gt;Clash for Windows&lt;/a&gt;，设置 系统代理 ，在终端中输入，重启即可。&lt;/p&gt; &#xA;&lt;p&gt;根据自己的工具修改对应的端口&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 使用 http 类型代理&#xA;set http_proxy=http://127.0.0.1:8484&#xA;set https_proxy=http://127.0.0.1:8484&#xA;# 使用 socks 类型代理&#xA;netsh winhttp set proxy proxy-server=&#34;socks=127.0.0.1:8484&#34; bypass-list=&#34;localhost&#34;&#xA;netsh winhttp show proxy&#xA;netsh winhttp reset proxy&#xA;# 使用 socks 类型代理&#xA;set http_proxy=socks5://127.0.0.1:8484&#xA;set https_proxy=socks5://127.0.0.1:8484&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔧 用法&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;在终端中运行 &lt;code&gt;main.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m autogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;在 AUTO-GPT 的每个操作之后，输入“y”来授权命令，“y -N”来运行 N 个连续命令，“n”来退出程序，或者为 AI 输入额外的反馈。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;日志&lt;/h3&gt; &#xA;&lt;p&gt;您将在文件夹中找到活动和错误日志&lt;code&gt;./output/logs&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;输出调试日志：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m autogpt --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;命令行参数&lt;/h3&gt; &#xA;&lt;p&gt;以下是您在运行 Auto-GPT 时可以使用的一些常见参数：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;将尖括号 (&amp;lt;&amp;gt;) 中的任何内容替换为您要指定的值&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python scripts/main.py --help&lt;/code&gt;查看所有可用命令行参数的列表。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python scripts/main.py --ai-settings &amp;lt;filename&amp;gt;&lt;/code&gt;使用不同的 AI 设置文件运行 Auto-GPT。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python scripts/main.py --use-memory &amp;lt;memory-backend&amp;gt;&lt;/code&gt;指定 3 个内存后端之一：&lt;code&gt;local&lt;/code&gt;、&lt;code&gt;redis&lt;/code&gt;或&lt;code&gt;pinecone&lt;/code&gt;&#39;no_memory&#39;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：其中一些标志有简写形式，&lt;code&gt;-m&lt;/code&gt;例如&lt;code&gt;--use-memory&lt;/code&gt;. 用于&lt;code&gt;python scripts/main.py --help&lt;/code&gt;获取更多信息&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🗣️ 语音模式&lt;/h2&gt; &#xA;&lt;p&gt;使用它来将 TTS 用于 Auto-GPT&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m autogpt --speak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OpenAI API 密钥配置&lt;/h2&gt; &#xA;&lt;p&gt;从以下网址获取您的 OpenAI API 密钥： https: //platform.openai.com/account/api-keys。&lt;/p&gt; &#xA;&lt;p&gt;要将 OpenAI API 密钥用于 Auto-GPT，您需要设置账单（即付费账户）。&lt;/p&gt; &#xA;&lt;p&gt;您可以在&lt;a href=&#34;https://platform.openai.com/account/billing/overview%E8%AE%BE%E7%BD%AE%E4%BB%98%E8%B4%B9%E8%B4%A6%E6%88%B7%E3%80%82&#34;&gt;https://platform.openai.com/account/billing/overview设置付费账户。&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;要使 OpenAI API 密钥生效，请在 OpenAI API &amp;gt; 计费中设置付费帐户&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaqijiang/Auto-GPT-ZH/main/openai-api-key.png&#34; alt=&#34;要使 OpenAI API 密钥生效，请在 OpenAI API &gt; 计费中设置付费帐户&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔍 谷歌 API 密钥配置&lt;/h2&gt; &#xA;&lt;p&gt;此部分是可选的，如果您在运行谷歌搜索时遇到错误 429 问题，请使用官方谷歌 API。要使用该&lt;code&gt;google_official_search&lt;/code&gt;命令，您需要在环境变量中设置 Google API 密钥。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;转到&lt;a href=&#34;https://console.cloud.google.com/&#34;&gt;谷歌云控制台&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;如果您还没有帐户，请创建一个并登录。&lt;/li&gt; &#xA; &lt;li&gt;通过单击页面顶部的“选择项目”下拉菜单并单击“新建项目”来创建一个新项目。给它起个名字，然后单击“创建”。&lt;/li&gt; &#xA; &lt;li&gt;转到&lt;a href=&#34;https://console.cloud.google.com/apis/dashboard&#34;&gt;API 和服务仪表板&lt;/a&gt;并单击“启用 API 和服务”。搜索“自定义搜索 API”并单击它，然后单击“启用”。&lt;/li&gt; &#xA; &lt;li&gt;转到&lt;a href=&#34;https://console.cloud.google.com/apis/credentials&#34;&gt;凭据&lt;/a&gt;页面并单击“创建凭据”。选择“API 密钥”。&lt;/li&gt; &#xA; &lt;li&gt;复制 API 密钥并将其设置为在您的计算机上命名的环境变量&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;。请参阅下面的设置环境变量。&lt;/li&gt; &#xA; &lt;li&gt;转到&lt;a href=&#34;https://cse.google.com/cse/all&#34;&gt;自定义搜索引擎&lt;/a&gt;页面并单击“添加”。&lt;/li&gt; &#xA; &lt;li&gt;按照提示设置搜索引擎。您可以选择搜索整个网络或特定站点。&lt;/li&gt; &#xA; &lt;li&gt;创建搜索引擎后，单击“控制面板”，然后单击“基本”。复制“搜索引擎 ID”并将其设置为&lt;code&gt;CUSTOM_SEARCH_ENGINE_ID&lt;/code&gt;在您的计算机上命名的环境变量。请参阅下面的设置环境变量。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;请记住，您的每日免费自定义搜索配额最多只允许 100 次搜索。要增加此限制，您需要为项目分配一个计费帐户，以从每天多达 10,000 次搜索中获利。&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;设置环境变量&lt;/h3&gt; &#xA;&lt;p&gt;对于 Windows 用户：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;setx GOOGLE_API_KEY &#34;YOUR_GOOGLE_API_KEY&#34;&#xA;setx CUSTOM_SEARCH_ENGINE_ID &#34;YOUR_CUSTOM_SEARCH_ENGINE_ID&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对于 macOS 和 Linux 用户：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export GOOGLE_API_KEY=&#34;YOUR_GOOGLE_API_KEY&#34;&#xA;export CUSTOM_SEARCH_ENGINE_ID=&#34;YOUR_CUSTOM_SEARCH_ENGINE_ID&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;设置缓存类型&lt;/h2&gt; &#xA;&lt;p&gt;默认情况下，Auto-GPT 将使用 LocalCache 而不是 redis 或 Pinecone。&lt;/p&gt; &#xA;&lt;p&gt;要切换到任何一个，请将&lt;code&gt;MEMORY_BACKEND&lt;/code&gt;env 变量更改为您想要的值：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;local&lt;/code&gt;（默认）使用本地 JSON 缓存文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pinecone&lt;/code&gt;使用您在 ENV 设置中配置的 Pinecone.io 帐户&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;redis&lt;/code&gt;将使用您配置的 redis 缓存&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;milvus&lt;/code&gt;将使用您配置的 milvus 缓存&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;weaviate&lt;/code&gt;将使用您配置的 weaviate 缓存&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;设置&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;警告：本系统未经过安全保护，不应该公开访问。因此，请避免在互联网上使用Redis而不使用密码或根本不要使用Redis。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装 docker 桌面&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;有关设置密码和其他配置的信息，请参阅&lt;a href=&#34;https://hub.docker.com/r/redis/redis-stack-server&#34;&gt;https://hub.docker.com/r/redis/redis-stack-server 。&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;设置以下环境变量&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;替换尖括号 (&amp;lt;&amp;gt;) 中的&lt;strong&gt;密码&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;MEMORY_BACKEND=redis&#xA;REDIS_HOST=localhost&#xA;REDIS_PORT=6379&#xA;REDIS_PASSWORD=&amp;lt;PASSWORD&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;您可以选择设置&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WIPE_REDIS_ON_START=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;持久化存储在 Redis 中的内存&lt;/p&gt; &#xA;&lt;p&gt;您可以使用以下命令为 redis 指定内存索引：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MEMORY_INDEX=&amp;lt;WHATEVER&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🌲Pinecone API 密钥设置&lt;/h3&gt; &#xA;&lt;p&gt;Pinecone 支持存储大量基于向量的内存，允许在任何给定时间只为代理加载相关内存。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;如果您还没有帐户，请前往&lt;a href=&#34;https://app.pinecone.io/&#34;&gt;pinecone并创建一个帐户。&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;选择&lt;code&gt;Starter&lt;/code&gt;计划以避免被收费。&lt;/li&gt; &#xA; &lt;li&gt;在左侧边栏的默认项目下找到您的 API 密钥和区域。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;在&lt;code&gt;.env&lt;/code&gt;文件集中：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;PINECONE_API_KEY&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PINECONE_ENV&lt;/code&gt;（例如：&lt;em&gt;“us-east4-gcp”&lt;/em&gt;）&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MEMORY_BACKEND=pinecone&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;或者，您可以从命令行设置它们（高级）：&lt;/p&gt; &#xA;&lt;p&gt;对于 Windows 用户：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;setx PINECONE_API_KEY &#34;&amp;lt;YOUR_PINECONE_API_KEY&amp;gt;&#34;&#xA;setx PINECONE_ENV &#34;&amp;lt;YOUR_PINECONE_REGION&amp;gt;&#34; # e.g: &#34;us-east4-gcp&#34;&#xA;setx MEMORY_BACKEND &#34;pinecone&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对于 macOS 和 Linux 用户：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export PINECONE_API_KEY=&#34;&amp;lt;YOUR_PINECONE_API_KEY&amp;gt;&#34;&#xA;export PINECONE_ENV=&#34;&amp;lt;YOUR_PINECONE_REGION&amp;gt;&#34; # e.g: &#34;us-east4-gcp&#34;&#xA;export MEMORY_BACKEND=&#34;pinecone&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Milvus 安装&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt;是一个开源的、高度可扩展的矢量数据库，可以存储大量基于矢量的内存并提供快速的相关搜索。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;设置 milvus 数据库，保持你的 pymilvus 版本和 milvus 版本相同，以避免兼容问题。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;通过开源&lt;a href=&#34;https://milvus.io/docs/install_standalone-operator.md&#34;&gt;安装 Milvus&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zilliz.com/cloud&#34;&gt;或由Zilliz Cloud&lt;/a&gt;设置&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;设置&lt;code&gt;MILVUS_ADDR&lt;/code&gt;为&lt;code&gt;.env&lt;/code&gt;你的 milvus 地址&lt;code&gt;host:ip&lt;/code&gt;。&lt;/li&gt; &#xA; &lt;li&gt;设置&lt;code&gt;MEMORY_BACKEND&lt;/code&gt;为&lt;code&gt;.env&lt;/code&gt;启用&lt;code&gt;milvus&lt;/code&gt;milvus 作为后端。&lt;/li&gt; &#xA; &lt;li&gt;选修的 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;set &lt;code&gt;MILVUS_COLLECTION&lt;/code&gt;in&lt;code&gt;.env&lt;/code&gt;随意更改 milvus 集合名称，&lt;code&gt;autogpt&lt;/code&gt;默认名称。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Weaviate设置&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://weaviate.io/&#34;&gt;Weaviate&lt;/a&gt;是一个开源矢量数据库。它允许存储来自 ML 模型的数据对象和向量嵌入，并无缝扩展到数十亿个数据对象。&lt;a href=&#34;https://weaviate.io/developers/weaviate/quickstart&#34;&gt;Weaviate 实例可以在本地（使用 Docker）、Kubernetes 或使用 Weaviate 云服务创建&lt;/a&gt;。虽然仍处于实验阶段，但支持&lt;a href=&#34;https://weaviate.io/developers/weaviate/installation/embedded&#34;&gt;嵌入式 Weaviate ，它允许 Auto-GPT 进程本身启动 Weaviate 实例。&lt;/a&gt;要启用它，请设置&lt;code&gt;USE_WEAVIATE_EMBEDDED&lt;/code&gt;为&lt;code&gt;True&lt;/code&gt;并确保您&lt;code&gt;pip install &#34;weaviate-client&amp;gt;=3.15.4&#34;&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;设置环境变量&lt;/h4&gt; &#xA;&lt;p&gt;在您的&lt;code&gt;.env&lt;/code&gt;文件中设置以下内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MEMORY_BACKEND=weaviate&#xA;WEAVIATE_HOST=&#34;127.0.0.1&#34; # the IP or domain of the running Weaviate instance&#xA;WEAVIATE_PORT=&#34;8080&#34; &#xA;WEAVIATE_PROTOCOL=&#34;http&#34;&#xA;WEAVIATE_USERNAME=&#34;your username&#34;&#xA;WEAVIATE_PASSWORD=&#34;your password&#34;&#xA;WEAVIATE_API_KEY=&#34;your weaviate API key if you have one&#34;&#xA;WEAVIATE_EMBEDDED_PATH=&#34;/home/me/.local/share/weaviate&#34; # this is optional and indicates where the data should be persisted when running an embedded instance&#xA;USE_WEAVIATE_EMBEDDED=False # set to True to run Embedded Weaviate&#xA;MEMORY_INDEX=&#34;Autogpt&#34; # name of the index to create for the application&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;查看内存使用情况&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;使用&lt;code&gt;--debug&lt;/code&gt;标志查看内存使用情况:)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🧠内存预填充&lt;/h2&gt; &#xA;&lt;h4&gt;python scripts/data_ingestion.py -h&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;用法：data_ingestion.py [-h] (--file FILE | --dir DIR) [--init] [--overlap OVERLAP] [--max_length MAX_LENGTH]&#xA;&#xA;将一个文件或包含多个文件的目录摄取到内存中。确保在运行此脚本之前设置您的 .env。&#xA;&#xA;选项：-h, --help 显示此帮助消息并退出 --file FILE 要摄取的文件。--dir DIR 包含要摄取的文件的目录。--init 初始化内存并擦除其内容（默认值：False） --overlap OVERLAP 摄取文件时块之间的重叠大小（默认值：200） --max_length MAX_LENGTH 摄取文件时每个块的最大长度（默认值：4000）&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;python autogpt/data_ingestion.py --dir seed_data --init --overlap 200 --max_length 1000&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;该脚本位于 autogpt/data_ingestion.py，允许您将文件提取到内存中并在运行 Auto-GPT 之前预先填充。&lt;/p&gt; &lt;p&gt;记忆预填充是一种技术，涉及将相关文档或数据摄取到 AI 的记忆中，以便它可以使用这些信息来生成更明智和准确的响应。&lt;/p&gt; &lt;p&gt;为了预置到内存，每个文档的内容被分成指定最大长度的块，块之间有指定的重叠，然后每个块被添加到 .env 文件中的内存后端集。当提示 AI 回忆信息时，它可以访问那些预先植入的记忆以生成更明智和准确的响应。&lt;/p&gt; &lt;p&gt;当处理大量数据或存在 AI 需要能够快速访问的特定信息时，此技术特别有用。通过预先植入内存，人工智能可以更有效地检索和使用这些信息，从而节省时间、API 调用并提高其响应的准确性。&lt;/p&gt; &lt;p&gt;例如，您可以下载 API 文档、GitHub 存储库等，并在运行 Auto-GPT 之前将其提取到内存中。&lt;/p&gt; &lt;p&gt;⚠️如果您使用 Redis 作为您的内存，请确保运行 Auto-GPT 并在您的文件中&lt;code&gt;WIPE_REDIS_ON_START&lt;/code&gt;设置为。&lt;code&gt;False``.env&lt;/code&gt;&lt;/p&gt; &lt;p&gt;⚠️对于其他内存后端，我们目前在启动 Auto-GPT 时强制擦除内存。&lt;code&gt;data_ingestion.py&lt;/code&gt;要使用这些内存后端摄取数据，您可以在 Auto-GPT 运行期间随时调用脚本。&lt;/p&gt; &lt;p&gt;即使在 Auto-GPT 运行时摄取记忆，AI 也会立即使用记忆。&lt;/p&gt; &lt;p&gt;在上面的示例中，脚本初始化内存，将目录中的所有文件摄取&lt;code&gt;/seed_data&lt;/code&gt;到内存中，块之间的重叠为 200，每个块的最大长度为 4000。请注意，您也可以使用参数将&lt;code&gt;--file&lt;/code&gt;单个文件摄取到内存中内存，并且脚本将只摄取&lt;code&gt;/auto_gpt_workspace&lt;/code&gt;目录中的文件。&lt;/p&gt; &lt;p&gt;您可以调整&lt;code&gt;max_length&lt;/code&gt;和重叠参数以微调文档在“回忆”该内存时呈现给 AI 的方式：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;调整重叠值允许 AI 在调用信息时从每个块访问更多上下文信息，但会导致创建更多块，从而增加内存后端使用和 OpenAI API 请求。&lt;/li&gt; &#xA;   &lt;li&gt;减小该&lt;code&gt;max_length&lt;/code&gt;值将创建更多块，这可以通过在上下文中允许更多消息历史记录来节省提示令牌，但也会增加块的数量。&lt;/li&gt; &#xA;   &lt;li&gt;增加该&lt;code&gt;max_length&lt;/code&gt;值将为 AI 提供来自每个块的更多上下文信息，从而减少创建的块数量并节省 OpenAI API 请求。然而，这也可能会使用更多的提示标记并减少 AI 可用的整体上下文。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;连续模式⚠️&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;无需&lt;/strong&gt;用户授权即可 100% 自动化地运行 AI 。不推荐连续模式。它具有潜在危险，可能会导致您的 AI 永远运行或执行您通常不会授权的操作。使用风险自负。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;在终端中运行Python 脚本：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m autogpt --continuous&#xA;python -m autogpt --speak --continuous #带语音&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;2.要退出程序，请按 Ctrl + C&lt;/p&gt; &#xA;&lt;h2&gt;GPT3.5 ONLY 模式&lt;/h2&gt; &#xA;&lt;p&gt;如果您无权访问 GPT4 api，此模式将允许您使用 Auto-GPT！&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m autogpt --gpt3only&#xA;python -m autogpt --speak --gpt3only #带语音&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;建议将虚拟机用于需要高度安全措施的任务，以防止对主计算机的系统和数据造成任何潜在危害。&lt;/p&gt; &#xA;&lt;h2&gt;🖼 图像生成&lt;/h2&gt; &#xA;&lt;p&gt;默认情况下，Auto-GPT 使用 DALL-e 进行图像生成。要使用 Stable Diffusion，需要一个&lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;HuggingFace API 令牌。&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;获得令牌后，将这些变量设置为&lt;code&gt;.env&lt;/code&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;IMAGE_PROVIDER=sd&#xA;HUGGINGFACE_API_TOKEN=&#34;YOUR_HUGGINGFACE_API_TOKEN&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;⚠️ 限制&lt;/h2&gt; &#xA;&lt;p&gt;该实验旨在展示 GPT-4 的潜力，但存在一些局限性：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;不是完善的应用程序或产品，只是一个实验&lt;/li&gt; &#xA; &lt;li&gt;在复杂的真实业务场景中可能表现不佳。事实上，如果确实如此，请分享您的结果！&lt;/li&gt; &#xA; &lt;li&gt;运行成本非常高，因此请使用 OpenAI 设置和监控您的 API 密钥限制！&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🛡 免责声明&lt;/h2&gt; &#xA;&lt;p&gt;免责声明 Auto-GPT 这个项目是一个实验性应用程序，按“原样”提供，没有任何明示或暗示的保证。使用本软件，即表示您同意承担与其使用相关的所有风险，包括但不限于数据丢失、系统故障或可能出现的任何其他问题。&lt;/p&gt; &#xA;&lt;p&gt;本项目的开发者和贡献者对因使用本软件而可能发生的任何损失、损害或其他后果不承担任何责任或义务。您对基于 Auto-GPT 提供的信息做出的任何决定和行动承担全部责任。&lt;/p&gt; &#xA;&lt;p&gt;**请注意，由于使用代币，使用 GPT-4 语言模型可能会很昂贵。**通过使用此项目，您承认您有责任监控和管理您自己的代币使用情况和相关费用。强烈建议定期检查您的 OpenAI API 使用情况并设置任何必要的限制或警报以防止意外收费。&lt;/p&gt; &#xA;&lt;p&gt;作为一项自主实验，Auto-GPT 可能会生成不符合现实世界商业惯例或法律要求的内容或采取的行动。您有责任确保基于此软件的输出做出的任何行动或决定符合所有适用的法律、法规和道德标准。本项目的开发者和贡献者对因使用本软件而产生的任何后果不承担任何责任。&lt;/p&gt; &#xA;&lt;p&gt;通过使用 Auto-GPT，您同意就任何和所有索赔、损害、损失、责任、成本和费用（包括合理的律师费）对开发人员、贡献者和任何关联方进行赔偿、辩护并使其免受损害因您使用本软件或您违反这些条款而引起的。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xtekky/gpt4free</title>
    <updated>2023-04-21T01:44:23Z</updated>
    <id>tag:github.com,2023-04-21:/xtekky/gpt4free</id>
    <link href="https://github.com/xtekky/gpt4free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;decentralising the Ai Industry, free gpt-4/3.5 scripts through several reverse engineered api&#39;s ( poe.com, phind.com, chat.openai.com etc...)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Free LLM APIs&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides reverse-engineered language models from various sources. Some of these models are already available in the repo, while others are currently being worked on.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; If you come across any website offering free language models, please create an issue or submit a pull request with the details. We will reverse engineer it and add it to this repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Best Chatgpt site&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://chat.chatbot.sex/chat&#34;&gt;https://chat.chatbot.sex/chat&lt;/a&gt; This site was developed by me and includes &lt;strong&gt;gpt-4&lt;/strong&gt;, &lt;strong&gt;internet access&lt;/strong&gt; and &lt;strong&gt;gpt-jailbreak&#39;s&lt;/strong&gt; like DAN You can find an opensource version of it to run locally here: &lt;a href=&#34;https://github.com/xtekky/chatgpt-clone&#34;&gt;https://github.com/xtekky/chatgpt-clone&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;To-Do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; implement poe.com create bot feature | AVAILABLE NOW&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; renaming the &#39;poe&#39; module to &#39;quora&#39;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add you.com api&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#current-sites&#34;&gt;Current Sites (No Authentication / Easy Account Creation)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#sites-with-authentication&#34;&gt;Sites with Authentication (Will Reverse Engineer but Need Account Access)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-poe&#34;&gt;&lt;code&gt;quora (poe)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-phind&#34;&gt;&lt;code&gt;phind&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-t3nsor&#34;&gt;&lt;code&gt;t3nsor&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-ora&#34;&gt;&lt;code&gt;ora&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-writesonic&#34;&gt;&lt;code&gt;writesonic&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xtekky/gpt4free/main/#example-you&#34;&gt;&lt;code&gt;you&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current Sites &lt;a name=&#34;current-sites&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Model(s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ora.sh&#34;&gt;ora.sh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5 / 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://poe.com&#34;&gt;poe.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4/3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://writesonic.com&#34;&gt;writesonic.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5 / Internet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://t3nsor.com&#34;&gt;t3nsor.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://you.com&#34;&gt;you.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5 / Internet / good search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://phind.com&#34;&gt;phind.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4 / Internet / good search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Sites with Authentication &lt;a name=&#34;sites-with-authentication&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;These sites will be reverse engineered but need account access:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.openai.com/chat&#34;&gt;chat.openai.com/chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bard.google.com&#34;&gt;bard.google.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bing.com/chat&#34;&gt;bing.com/chat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage Examples &lt;a name=&#34;usage-examples&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;quora (poe)&lt;/code&gt; (use like openai pypi package) - GPT-4 &lt;a name=&#34;example-poe&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# quora model names: (use left key as argument)&#xA;models = {&#xA;    &#39;sage&#39;   : &#39;capybara&#39;,&#xA;    &#39;gpt-4&#39;  : &#39;beaver&#39;,&#xA;    &#39;claude-v1.2&#39;         : &#39;a2_2&#39;,&#xA;    &#39;claude-instant-v1.0&#39; : &#39;a2&#39;,&#xA;    &#39;gpt-3.5-turbo&#39;       : &#39;chinchilla&#39;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;!! new: bot creation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import quora (poe) package&#xA;import quora&#xA;&#xA;# create account&#xA;# make shure to set enable_bot_creation to True&#xA;token = quora.Account.create(logging = True, enable_bot_creation=True)&#xA;&#xA;model = quora.Model.create(&#xA;    token = token,&#xA;    model = &#39;gpt-3.5-turbo&#39;, # or claude-instant-v1.0&#xA;    system_prompt = &#39;you are ChatGPT a large language model ...&#39; &#xA;)&#xA;&#xA;print(model.name) # gptx....&#xA;&#xA;# streaming response&#xA;for response in quora.StreamingCompletion.create(&#xA;    custom_model = model.name,&#xA;    prompt       =&#39;hello world&#39;,&#xA;    token        = token):&#xA;    &#xA;    print(response.completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Normal Response:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;response = quora.Completion.create(model  = &#39;gpt-4&#39;,&#xA;    prompt = &#39;hello world&#39;,&#xA;    token  = token)&#xA;&#xA;print(response.completion.choices[0].text)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;phind&lt;/code&gt; (use like openai pypi package) &lt;a name=&#34;example-phind&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import phind&#xA;&#xA;prompt = &#39;who won the quatar world cup&#39;&#xA;&#xA;# help needed: not getting newlines from the stream, please submit a PR if you know how to fix this&#xA;# stream completion&#xA;for result in phind.StreamingCompletion.create(&#xA;    model  = &#39;gpt-4&#39;,&#xA;    prompt = prompt,&#xA;    results     = phind.Search.create(prompt, actualSearch = True), # create search (set actualSearch to False to disable internet)&#xA;    creative    = False,&#xA;    detailed    = False,&#xA;    codeContext = &#39;&#39;):  # up to 3000 chars of code&#xA;&#xA;    print(result.completion.choices[0].text, end=&#39;&#39;, flush=True)&#xA;&#xA;# normal completion&#xA;result = phind.Completion.create(&#xA;    model  = &#39;gpt-4&#39;,&#xA;    prompt = prompt,&#xA;    results     = phind.Search.create(prompt, actualSearch = True), # create search (set actualSearch to False to disable internet)&#xA;    creative    = False,&#xA;    detailed    = False,&#xA;    codeContext = &#39;&#39;) # up to 3000 chars of code&#xA;&#xA;print(result.completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;t3nsor&lt;/code&gt; (use like openai pypi package) &lt;a name=&#34;example-t3nsor&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import t3nsor&#xA;import t3nsor&#xA;&#xA;# t3nsor.Completion.create&#xA;# t3nsor.StreamCompletion.create&#xA;&#xA;[...]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example Chatbot&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = []&#xA;&#xA;while True:&#xA;    user = input(&#39;you: &#39;)&#xA;&#xA;    t3nsor_cmpl = t3nsor.Completion.create(&#xA;        prompt   = user,&#xA;        messages = messages&#xA;    )&#xA;&#xA;    print(&#39;gpt:&#39;, t3nsor_cmpl.completion.choices[0].text)&#xA;    &#xA;    messages.extend([&#xA;        {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: user }, &#xA;        {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: t3nsor_cmpl.completion.choices[0].text}&#xA;    ])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Streaming Response:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for response in t3nsor.StreamCompletion.create(&#xA;    prompt   = &#39;write python code to reverse a string&#39;,&#xA;    messages = []):&#xA;&#xA;    print(response.completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;ora&lt;/code&gt; (use like openai pypi package) &lt;a name=&#34;example-ora&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;load model (new)&lt;/h3&gt; &#xA;&lt;p&gt;more gpt4 models in &lt;code&gt;/testing/ora_gpt4.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# normal gpt-4: b8b12eaa-5d47-44d3-92a6-4d706f2bcacf&#xA;model = ora.CompletionModel.load(chatbot_id, &#39;gpt-4&#39;) # or gpt-3.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;create model / chatbot:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# inport ora&#xA;import ora&#xA;&#xA;# create model&#xA;model = ora.CompletionModel.create(&#xA;    system_prompt = &#39;You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible&#39;,&#xA;    description   = &#39;ChatGPT Openai Language Model&#39;,&#xA;    name          = &#39;gpt-3.5&#39;)&#xA;&#xA;# init conversation (will give you a conversationId)&#xA;init = ora.Completion.create(&#xA;    model  = model,&#xA;    prompt = &#39;hello world&#39;)&#xA;&#xA;print(init.completion.choices[0].text)&#xA;&#xA;while True:&#xA;    # pass in conversationId to continue conversation&#xA;    &#xA;    prompt = input(&#39;&amp;gt;&amp;gt;&amp;gt; &#39;)&#xA;    response = ora.Completion.create(&#xA;        model  = model,&#xA;        prompt = prompt,&#xA;        includeHistory = True, # remember history&#xA;        conversationId = init.id)&#xA;    &#xA;    print(response.completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;writesonic&lt;/code&gt; (use like openai pypi package) &lt;a name=&#34;example-writesonic&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import writesonic&#xA;import writesonic&#xA;&#xA;# create account (3-4s)&#xA;account = writesonic.Account.create(logging = True)&#xA;&#xA;# with loging: &#xA;    # 2023-04-06 21:50:25 INFO __main__ -&amp;gt; register success : &#39;{&#34;id&#34;:&#34;51aa0809-3053-44f7-922a...&#39; (2s)&#xA;    # 2023-04-06 21:50:25 INFO __main__ -&amp;gt; id : &#39;51aa0809-3053-44f7-922a-2b85d8d07edf&#39;&#xA;    # 2023-04-06 21:50:25 INFO __main__ -&amp;gt; token : &#39;eyJhbGciOiJIUzI1NiIsInR5cCI6Ik...&#39;&#xA;    # 2023-04-06 21:50:28 INFO __main__ -&amp;gt; got key : &#39;194158c4-d249-4be0-82c6-5049e869533c&#39; (2s)&#xA;&#xA;# simple completion&#xA;response = writesonic.Completion.create(&#xA;    api_key = account.key,&#xA;    prompt  = &#39;hello world&#39;&#xA;)&#xA;&#xA;print(response.completion.choices[0].text) # Hello! How may I assist you today?&#xA;&#xA;# conversation&#xA;&#xA;response = writesonic.Completion.create(&#xA;    api_key = account.key,&#xA;    prompt  = &#39;what is my name ?&#39;,&#xA;    enable_memory = True,&#xA;    history_data  = [&#xA;        {&#xA;            &#39;is_sent&#39;: True,&#xA;            &#39;message&#39;: &#39;my name is Tekky&#39;&#xA;        },&#xA;        {&#xA;            &#39;is_sent&#39;: False,&#xA;            &#39;message&#39;: &#39;hello Tekky&#39;&#xA;        }&#xA;    ]&#xA;)&#xA;&#xA;print(response.completion.choices[0].text) # Your name is Tekky.&#xA;&#xA;# enable internet&#xA;&#xA;response = writesonic.Completion.create(&#xA;    api_key = account.key,&#xA;    prompt  = &#39;who won the quatar world cup ?&#39;,&#xA;    enable_google_results = True&#xA;)&#xA;&#xA;print(response.completion.choices[0].text) # Argentina won the 2022 FIFA World Cup tournament held in Qatar ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example: &lt;code&gt;you&lt;/code&gt; (use like openai pypi package) &lt;a name=&#34;example-you&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import you&#xA;&#xA;# simple request with links and details&#xA;response = you.Completion.create(&#xA;    prompt       = &#34;hello world&#34;,&#xA;    detailed     = True,&#xA;    includelinks = True,)&#xA;&#xA;print(response)&#xA;&#xA;# {&#xA;#     &#34;response&#34;: &#34;...&#34;,&#xA;#     &#34;links&#34;: [...],&#xA;#     &#34;extra&#34;: {...},&#xA;#         &#34;slots&#34;: {...}&#xA;#     }&#xA;# }&#xA;&#xA;#chatbot&#xA;&#xA;chat = []&#xA;&#xA;while True:&#xA;    prompt = input(&#34;You: &#34;)&#xA;    &#xA;    response = you.Completion.create(&#xA;        prompt  = prompt,&#xA;        chat    = chat)&#xA;    &#xA;    print(&#34;Bot:&#34;, response[&#34;response&#34;])&#xA;    &#xA;    chat.append({&#34;question&#34;: prompt, &#34;answer&#34;: response[&#34;response&#34;]})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;The repository is written in Python and requires the following packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;websocket-client&lt;/li&gt; &#xA; &lt;li&gt;requests&lt;/li&gt; &#xA; &lt;li&gt;tls-client&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can install these packages using the provided &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Repository structure:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── ora/&#xA;├── quora/ (/poe)&#xA;├── t3nsor/&#xA;├── testing/&#xA;├── writesonic/&#xA;├── you/&#xA;├── README.md  &amp;lt;-- this file.&#xA;└── requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#xtekky/openai-gpt4&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=xtekky/openai-gpt4&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Copyright:&lt;/h2&gt; &#xA;&lt;p&gt;This program is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.txt&#34;&gt;GNU GPL v3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Most code, with the exception of &lt;code&gt;quora/api.py&lt;/code&gt; (by &lt;a href=&#34;https://github.com/ading2210&#34;&gt;ading2210&lt;/a&gt;), has been written by me, &lt;a href=&#34;https://github.com/xtekky&#34;&gt;xtekky&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Copyright Notice:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;xtekky/openai-gpt4: multiple reverse engineered language-model api&#39;s to decentralise the ai industry.  &#xA;Copyright (C) 2023 xtekky&#xA;&#xA;This program is free software: you can redistribute it and/or modify&#xA;it under the terms of the GNU General Public License as published by&#xA;the Free Software Foundation, either version 3 of the License, or&#xA;(at your option) any later version.&#xA;&#xA;This program is distributed in the hope that it will be useful,&#xA;but WITHOUT ANY WARRANTY; without even the implied warranty of&#xA;MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xA;GNU General Public License for more details.&#xA;&#xA;You should have received a copy of the GNU General Public License&#xA;along with this program.  If not, see &amp;lt;https://www.gnu.org/licenses/&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>togethercomputer/RedPajama-Data</title>
    <updated>2023-04-21T01:44:23Z</updated>
    <id>tag:github.com,2023-04-21:/togethercomputer/RedPajama-Data</id>
    <link href="https://github.com/togethercomputer/RedPajama-Data" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The RedPajama-Data repository contains code for preparing large datasets for training large language models.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/9Rk6sSeWEG&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1082503318624022589?label=discord&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/togethercomputer/RedPajama-Data&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset&lt;/h1&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/togethercomputer/RedPajama-Data/main/docs/redpajama.png&#34;&gt; &#xA;&lt;p&gt;This repo contains a reproducible data receipe for the RedPajama data, with the following token counts:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Token Count&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Commoncrawl&lt;/td&gt; &#xA;   &lt;td&gt;878 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C4&lt;/td&gt; &#xA;   &lt;td&gt;175 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GitHub&lt;/td&gt; &#xA;   &lt;td&gt;59 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Books&lt;/td&gt; &#xA;   &lt;td&gt;26 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ArXiv&lt;/td&gt; &#xA;   &lt;td&gt;28 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wikipedia&lt;/td&gt; &#xA;   &lt;td&gt;24 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StackExchange&lt;/td&gt; &#xA;   &lt;td&gt;20 Billion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total&lt;/td&gt; &#xA;   &lt;td&gt;1.2 Trillion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;In &lt;code&gt;data_prep&lt;/code&gt;, we provide all pre-processing scripts and guidelines.&lt;/p&gt; &#xA;&lt;h2&gt;Tokenization&lt;/h2&gt; &#xA;&lt;p&gt;In &lt;code&gt;tokenization&lt;/code&gt;, we provide an example of how to tokenize the dataset using the GPT-NeoX tokenizer.&lt;/p&gt; &#xA;&lt;h2&gt;Visualization&lt;/h2&gt; &#xA;&lt;p&gt;In &lt;code&gt;viz&lt;/code&gt;, we provide a dashboard for exploring a subset of the data using &lt;a href=&#34;https://github.com/hazyresearch/meerkat&#34;&gt;Meerkat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repo is licensed under the Apache 2.0 license. Unless otherwise noted,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2023 Together Computer, ETH Zürich, Stanford University&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The file &lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/RedPajama-Data/main/data_prep/book/dedup.py&#34;&gt;data_prep/book/dedup.py&lt;/a&gt; was co-developed with &lt;a href=&#34;https://www.ontocord.ai&#34;&gt;Ontocord.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2023 Ontocord.ai, Together Computer, ETH Zürich, Stanford University&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The dataset itself, please refer to the licenses of the data subsets you use.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://commoncrawl.org/terms-of-use/full/&#34;&gt;Common Crawl Foundation Terms of Use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/allenai/c4#license&#34;&gt;C4 license&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub was limited to MIT, BSD, or Apache licenses only&lt;/li&gt; &#xA; &lt;li&gt;Books: &lt;a href=&#34;https://huggingface.co/datasets/the_pile_books3#licensing-information&#34;&gt;the_pile_books3 license&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/pg19#licensing-information&#34;&gt;pg19 license&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.arxiv.org/help/api/tou.html&#34;&gt;ArXiv Terms of Use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/wikipedia#licensing-information&#34;&gt;Wikipedia License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://archive.org/details/stackexchange&#34;&gt;StackExchange license on the Internet Archive&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For full terms, see the LICENSE file. If you have any questions, comments, or concerns about licensing please &lt;a href=&#34;https://www.together.xyz/contact&#34;&gt;contact us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are appreciative to the work done by the growing open-source AI community that made this project possible. That includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Participants in building the RedPajama dataset including &lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/RedPajama-Data/main/Ontocord.ai&#34;&gt;Ontocord.ai&lt;/a&gt;, &lt;a href=&#34;https://mila.quebec/en/&#34;&gt;MILA Québec AI Institute&lt;/a&gt;, &lt;a href=&#34;https://ds3lab.inf.ethz.ch/&#34;&gt;ETH DS3Lab&lt;/a&gt;, &lt;a href=&#34;https://www.umontreal.ca/&#34;&gt;Université de Montréal&lt;/a&gt;, &lt;a href=&#34;https://crfm.stanford.edu/&#34;&gt;Stanford Center for Research on Foundation Models (CRFM)&lt;/a&gt;, &lt;a href=&#34;https://hazyresearch.stanford.edu/&#34;&gt;Stanford Hazy Research research group&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt; — This project is built on the backs of the great team at EleutherAI — including the source code they provided for training GPT-NeoX.&lt;/li&gt; &#xA; &lt;li&gt;An award of computer time was provided by the &lt;a href=&#34;https://www.alcf.anl.gov/science/incite-allocation-program&#34;&gt;INCITE program&lt;/a&gt;. This research also used resources of the &lt;a href=&#34;https://www.together.xyz/blog/redpajama#:~:text=resources%20of%20the-,Oak%20Ridge%20Leadership%20Computing%20Facility%20(OLCF),-%2C%20which%20is%20a&#34;&gt;Oak Ridge Leadership Computing Facility (OLCF)&lt;/a&gt;, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>