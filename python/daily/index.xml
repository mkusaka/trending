<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-12T01:40:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HarborYuan/ovsam</title>
    <updated>2024-01-12T01:40:58Z</updated>
    <id>tag:github.com,2024-01-12:/HarborYuan/ovsam</id>
    <link href="https://github.com/HarborYuan/ovsam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-Vocabulary SAM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://yuanhaobo.me&#34;&gt;Haobo Yuan&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://lxtgh.github.io&#34;&gt;Xiangtai Li&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://chongzhou96.github.io&#34;&gt;Chong Zhou&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=y_cp1sUAAAAJ&#34;&gt;Yining Li&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://chenkai.site&#34;&gt;Kai Chen&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.mmlab-ntu.com/&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;S-Lab, Nanyang Technological University&lt;/a&gt;, &lt;a href=&#34;https://www.shlab.org.cn/&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;Shanghai Artificial Intelligence Laboratory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;http://arxiv.org/abs/2401.02955&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/ovsam&#34;&gt;&lt;code&gt;Project Page&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/HarborYuan/ovsam&#34;&gt;&lt;code&gt;Hugging Face Demo&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;👀 Overview&lt;/h2&gt; &#xA;&lt;p&gt;We introduce the Open-Vocabulary SAM, a SAM-inspired model designed for simultaneous interactive segmentation and recognition, leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The former adapts SAM&#39;s knowledge into the CLIP via distillation and learnable transformer adapters, while the latter transfers CLIP knowledge into SAM, enhancing its recognition capabilities.&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://www.mmlab-ntu.com/project/ovsam/img/ovsam_teaser.jpg&#34; alt=&#34;OVSAM overview&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🔧Usage&lt;/h2&gt; &#xA;&lt;p&gt;To play with Open-Vocabulary SAM, you can:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Try the online demo on the &lt;a href=&#34;https://huggingface.co/spaces/HarborYuan/ovsam&#34;&gt;🤗Hugging Face Space&lt;/a&gt;. Thanks for the generous support of the Hugging Face team.&lt;/li&gt; &#xA; &lt;li&gt;Run the gradio demo locally by cloning and running the &lt;a href=&#34;https://huggingface.co/spaces/HarborYuan/ovsam/tree/main&#34;&gt;repo&lt;/a&gt; on 🤗Hugging Face: &lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;git lfs install&#xA;git clone https://huggingface.co/spaces/HarborYuan/ovsam ovsam_demo&#xA;cd ovsam_demo&#xA;conda create -n ovsam_demo python=3.10  &amp;amp;&amp;amp; conda activate ovsam_demo&#xA;python -m pip install gradio==4.7.1&#xA;python -m pip install -r requirements.txt&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Try to train or evaluate in this repo following the instructions below.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;⚙️ Installation&lt;/h2&gt; &#xA;&lt;p&gt;We use conda to manage the environment.&lt;/p&gt; &#xA;&lt;p&gt;Pytorch installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;conda install pytorch torchvision torchaudio cuda-toolkit pytorch-cuda==12.1 -c pytorch -c &#34;nvidia/label/cuda-12.1.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;mmengine installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python -m pip install https://github.com/open-mmlab/mmengine/archive/refs/tags/v0.8.5.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;mmcv installation (note that older version mmcv before this commit may cause bugs):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;TORCH_CUDA_ARCH_LIST=&#34;{COMCAP}&#34; TORCH_NVCC_FLAGS=&#34;-Xfatbin -compress-all&#34; CUDA_HOME=$(dirname $(dirname $(which nvcc))) LD_LIBRARY_PATH=$(dirname $(dirname $(which nvcc)))/lib MMCV_WITH_OPS=1 FORCE_CUDA=1 python -m pip install git+https://github.com/open-mmlab/mmcv.git@4f65f91db6502d990ce2ee5de0337441fb69dd10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ask ChatGPT to get &lt;code&gt;COMCAP&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;What is the `Compute Capability` of NVIDIA {YOUR GPU MODEL}? Please only output the number, without text.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other OpenMMLab packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python -m pip install \&#xA;https://github.com/open-mmlab/mmdetection/archive/refs/tags/v3.1.0.zip \&#xA;https://github.com/open-mmlab/mmsegmentation/archive/refs/tags/v1.1.1.zip \&#xA;https://github.com/open-mmlab/mmpretrain/archive/refs/tags/v1.0.1.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extra packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python -m pip install git+https://github.com/cocodataset/panopticapi.git \&#xA;git+https://github.com/HarborYuan/lvis-api.git \&#xA;tqdm terminaltables pycocotools scipy tqdm ftfy regex timm scikit-image kornia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📈 Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Datasets should be put in the &lt;code&gt;data/&lt;/code&gt; folder of this project similar to &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/user_guides/tracking_dataset_prepare.html&#34;&gt;mmdet&lt;/a&gt;. Please prepare dataset in the following format.&lt;/p&gt; &#xA;&lt;h3&gt;COCO dataset&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;├── coco&#xA;│   ├── annotations&#xA;│   │   ├── panoptic_{train,val}2017.json&#xA;│   │   ├── instance_{train,val}2017.json&#xA;│   ├── train2017&#xA;│   ├── val2017&#xA;│   ├── panoptic_{train,val}2017/  # png annotations&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SAM dataset&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;├── sam&#xA;│   ├── train.txt&#xA;│   ├── val.txt&#xA;│   ├── sa_000020&#xA;│   │   ├── sa_223750.jpg&#xA;│   │   ├── sa_223750.json&#xA;│   │   ├── ...&#xA;│   ├── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;train.txt&lt;/code&gt; and &lt;code&gt;val.txt&lt;/code&gt; should contain all the folders you need:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;sa_000020&#xA;sa_000021&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 Training&lt;/h2&gt; &#xA;&lt;p&gt;Please extract the language embeddings first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;bash tools/dist.sh gen_cls seg/configs/ovsam/ovsam_coco_rn50x16_point.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SAM2CLIP&lt;/h3&gt; &#xA;&lt;p&gt;SAM feature extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;bash tools/dist.sh test seg/configs/sam2clip/sam_vith_dump.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SAM2CLIP training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;bash tools/dist.sh train seg/configs/sam2clip/sam2clip_vith_rn50x16.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLIP2SAM&lt;/h3&gt; &#xA;&lt;p&gt;CLIP2SAM training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;bash tools/dist.sh train seg/configs/clip2sam/clip2sam_coco_rn50x16.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🏃‍♀️Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;bash tools/dist.sh test seg/configs/ovsam/ovsam_coco_rn50x16_point.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://huggingface.co/HarborYuan/ovsam_models&#34;&gt;🤗Hugging Face&lt;/a&gt; to get the pre-trained weights:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;git clone https://huggingface.co/HarborYuan/ovsam_models models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📚 Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yuan2024ovsam,&#xA;    title={Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively},&#xA;    author={Yuan, Haobo and Li, Xiangtai and Zhou, Chong and Li, Yining and Chen, Kai and Loy, Chen Change},&#xA;    journal={arXiv preprint},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License &lt;a name=&#34;license&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://github.com/HarborYuan/ovsam/raw/master/LICENSE&#34;&gt;NTU S-Lab License 1.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ProjectNUWA/DragNUWA</title>
    <updated>2024-01-12T01:40:58Z</updated>
    <id>tag:github.com,2024-01-12:/ProjectNUWA/DragNUWA</id>
    <link href="https://github.com/ProjectNUWA/DragNUWA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DragNUWA&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragNUWA&lt;/strong&gt; enables users to manipulate backgrounds or objects within images directly, and the model seamlessly translates these actions into &lt;strong&gt;camera movements&lt;/strong&gt; or &lt;strong&gt;object motions&lt;/strong&gt;, generating the corresponding video.&lt;/p&gt; &#xA;&lt;p&gt;See our paper: &lt;a href=&#34;https://arxiv.org/abs/2308.08089&#34;&gt;DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; href=&#34;https://huggingface.co/spaces/yinsming/DragNUWA&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Open in Spaces&#34;&gt; &lt;/a&gt; &#xA;&lt;a src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; href=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/TOBEDONE&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;DragNUWA 1.5 (Updated on Jan 8, 2024)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragNUWA 1.5&lt;/strong&gt; uses Stable Video Diffusion as a backbone to animate an image according to specific path.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;assets/DragNUWA1.5/figure_raw&lt;/code&gt; for raw gifs.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.5/Figure1.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.5/Figure2.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.5/Figure3.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.5/Figure4.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;DragNUWA 1.0 (Original Paper)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.08089&#34;&gt;&lt;strong&gt;DragNUWA 1.0&lt;/strong&gt;&lt;/a&gt; utilizes text, images, and trajectory as three essential control factors to facilitate highly controllable video generation from semantic, spatial, and temporal aspects.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.0/Figure1.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.0/Figure2.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ProjectNUWA/DragNUWA/main/assets/DragNUWA1.0/Figure3.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Start&lt;/h2&gt; &#xA;&lt;h3&gt;Setting Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git clone https://github.com/ProjectNUWA/DragNUWA.git&#xA;cd DragNUWA&#xA;&#xA;conda create -n DragNUWA python=3.8&#xA;conda activate DragNUWA&#xA;pip install -r environment.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Pretrained Weights&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/file/d/1Z4JOley0SJCb35kFF4PCc6N6P1ftfX4i/view&#34;&gt;Pretrained Weights&lt;/a&gt; to &lt;code&gt;models/&lt;/code&gt; directory or directly run &lt;code&gt;bash models/Download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Drag and Animate!&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python DragNUWA_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will launch a gradio demo, and you can drag an image and animate it!&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;p&gt;We appreciate the open source of the following projects: &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;Stable Video Diffusion&lt;/a&gt;   &lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt;   &lt;a href=&#34;https://github.com/autonomousvision/unimatch&#34;&gt;UniMatch&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yin2023dragnuwa,&#xA;  title={Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory},&#xA;  author={Yin, Shengming and Wu, Chenfei and Liang, Jian and Shi, Jie and Li, Houqiang and Ming, Gong and Duan, Nan},&#xA;  journal={arXiv preprint arXiv:2308.08089},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mikel-brostrom/yolo_tracking</title>
    <updated>2024-01-12T01:40:58Z</updated>
    <id>tag:github.com,2024-01-12:/mikel-brostrom/yolo_tracking</id>
    <link href="https://github.com/mikel-brostrom/yolo_tracking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mikel-brostrom/yolo_tracking/master/assets/images/track_all_seg_1280_025conf.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI CPU testing&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://pepy.tech/project/boxmot&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/boxmot&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://doi.org/10.5281/zenodo.8132989&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.8132989.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains a collections of pluggable state-of-the-art multi-object trackers for segmentation, object detection and pose estimation models. For the methods using appearance description, both heavy (&lt;a href=&#34;https://arxiv.org/pdf/2211.13977.pdf&#34;&gt;CLIPReID&lt;/a&gt;) and lightweight state-of-the-art ReID models (&lt;a href=&#34;https://arxiv.org/pdf/2101.10774.pdf&#34;&gt;LightMBN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1905.00953.pdf&#34;&gt;OSNet&lt;/a&gt; and more) are available for automatic download. We provide examples on how to use this package together with popular object detection models such as: &lt;a href=&#34;https://github.com/ultralytics&#34;&gt;Yolov8&lt;/a&gt;, &lt;a href=&#34;https://github.com/Deci-AI/super-gradients&#34;&gt;Yolo-NAS&lt;/a&gt; and &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Tracker&lt;/th&gt; &#xA;    &lt;th&gt;HOTA↑&lt;/th&gt; &#xA;    &lt;th&gt;MOTA↑&lt;/th&gt; &#xA;    &lt;th&gt;IDF1↑&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2206.14651.pdf&#34;&gt;BoTSORT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;77.8&lt;/td&gt; &#xA;    &lt;td&gt;78.9&lt;/td&gt; &#xA;    &lt;td&gt;88.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.11813.pdf&#34;&gt;DeepOCSORT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;77.4&lt;/td&gt; &#xA;    &lt;td&gt;78.4&lt;/td&gt; &#xA;    &lt;td&gt;89.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14360.pdf&#34;&gt;OCSORT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;77.4&lt;/td&gt; &#xA;    &lt;td&gt;78.4&lt;/td&gt; &#xA;    &lt;td&gt;89.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2308.00783.pdf&#34;&gt;HybridSORT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;77.3&lt;/td&gt; &#xA;    &lt;td&gt;77.9&lt;/td&gt; &#xA;    &lt;td&gt;88.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2110.06864.pdf&#34;&gt;ByteTrack&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;75.6&lt;/td&gt; &#xA;    &lt;td&gt;74.6&lt;/td&gt; &#xA;    &lt;td&gt;86.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2202.13514.pdf&#34;&gt;StrongSORT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img width=&#34;200/&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img width=&#34;100/&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img width=&#34;100/&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img width=&#34;100/&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;sub&gt; NOTES: performed on the 10 first frames of each MOT17 sequence. The detector used is ByteTrack&#39;s YoloXm, trained on: CrowdHuman, MOT17, Cityperson and ETHZ. Each tracker is configured with its original parameters found in their respective official repository.&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;/div&gt;  &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tutorials&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.ultralytics.com/modes/train/&#34;&gt;Yolov8 training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/user_guide.html&#34;&gt;Deep appearance descriptor training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/yolo_tracking/wiki/ReID-multi-framework-model-export&#34;&gt;ReID model export to ONNX, OpenVINO, TensorRT and TorchScript&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/yolo_tracking/wiki/How-to-evaluate-on-custom-tracking-dataset&#34;&gt;Evaluation on custom tracking dataset&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1APUZ1ijCiQFBR9xD0gUvFUOC8yOJIvHm?usp=sharing&#34;&gt;ReID inference acceleration with Nebullvm&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Experiments&lt;/summary&gt; &#xA; &lt;p&gt;In inverse chronological order:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Evaluation-of-the-params-evolved-for-first-half-of-MOT17-on-the-complete-MOT17&#34;&gt;Evaluation of the params evolved for first half of MOT17 on the complete MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Segmentation-model-vs-object-detetion-model-on-MOT-metrics&#34;&gt;Segmentation model vs object detetion model on MOT metrics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Masked-detection-crops-vs-regular-detection-crops-for-ReID-feature-extraction&#34;&gt;Effect of masking objects before feature extraction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/conf-thres-vs-MOT-metrics&#34;&gt;conf-thres vs HOTA, MOTA and IDF1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-KF-updates-ahead-for-tracks-with-no-associations,-on-MOT17&#34;&gt;Effect of KF updates ahead for tracks with no associations on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-passing-full-image-input-vs-1280-re-scaled-to-StrongSORT-on-MOT17&#34;&gt;Effect of full images vs 1280 input to StrongSORT on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/OSNet-architecture-performances-on-MOT16&#34;&gt;Effect of different OSNet architectures on MOT16&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/StrongSORT-vs-BoTSORT-vs-OCSORT&#34;&gt;Yolov5 StrongSORT vs BoTSORT vs OCSORT&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Yolov5 &lt;a href=&#34;https://arxiv.org/abs/2206.14651&#34;&gt;BoTSORT&lt;/a&gt; branch: &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-17-evaluation-(private-detector)&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT17&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Yolov5DeepSORTwithOSNet-vs-Yolov5StrongSORTwithOSNet-ablation-study-on-MOT16&#34;&gt;StrongSORT MOT16 ablation study&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-16-evaluation&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT16 (deprecated)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;h4&gt;News&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom Ultralytics pacakge updated from 8.0.124 to 8.0.224 (December 2023)&lt;/li&gt; &#xA; &lt;li&gt;HybridSORT available (August 2023)&lt;/li&gt; &#xA; &lt;li&gt;SOTA CLIP-ReID people and vehicle models available (August 2023)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why BOXMOT?&lt;/h2&gt; &#xA;&lt;p&gt;Today&#39;s multi-object tracking options are heavily dependant on the computation capabilities of the underlaying hardware. BOXMOT provides a great variety of setup options that meet different hardware limitations: CPU only, low memory GPUs... Everything is designed with simplicity and flexibility in mind. If you don&#39;t get good tracking results on your custom dataset with the out-of-the-box tracker configurations, use the &lt;code&gt;examples/evolve.py&lt;/code&gt; script for tracker hyperparameter tuning.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Start with &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run the YOLOv8, YOLO-NAS or YOLOX examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mikel-brostrom/yolo_tracking.git&#xA;cd yolo_tracking&#xA;pip install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;but if you only want to import the tracking modules you can simply:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install boxmot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;YOLOv8 | YOLO-NAS | YOLOX examples&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracking&lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Yolo models&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python examples/track.py --yolo-model yolov8n       # bboxes only&#xA;  python examples/track.py --yolo-model yolo_nas_s    # bboxes only&#xA;  python examples/track.py --yolo-model yolox_n       # bboxes only&#xA;                                        yolov8n-seg   # bboxes + segmentation masks&#xA;                                        yolov8n-pose  # bboxes + pose estimation&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Tracking methods&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python examples/track.py --tracking-method deepocsort&#xA;                                             strongsort&#xA;                                             ocsort&#xA;                                             bytetrack&#xA;                                             botsort&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Tracking sources&lt;/summary&gt; &#xA;  &lt;p&gt;Tracking can be run on most video formats&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python examples/track.py --source 0                               # webcam&#xA;                                    img.jpg                         # image&#xA;                                    vid.mp4                         # video&#xA;                                    path/                           # directory&#xA;                                    path/*.jpg                      # glob&#xA;                                    &#39;https://youtu.be/Zgi9g1ksQHc&#39;  # YouTube&#xA;                                    &#39;rtsp://example.com/media.mp4&#39;  # RTSP, RTMP, HTTP stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Select ReID model&lt;/summary&gt; &#xA;  &lt;p&gt;Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this &lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO&#34;&gt;ReID model zoo&lt;/a&gt;. These model can be further optimized for you needs by the &lt;a href=&#34;https://github.com/mikel-brostrom/yolo_tracking/raw/master/boxmot/deep/reid_export.py&#34;&gt;reid_export.py&lt;/a&gt; script&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python examples/track.py --source 0 --reid-model lmbn_n_cuhk03_d.pt               # lightweight&#xA;                                                   osnet_x0_25_market1501.pt&#xA;                                                   mobilenetv2_x1_4_msmt17.engine&#xA;                                                   resnet50_msmt17.onnx&#xA;                                                   osnet_x1_0_msmt17.pt&#xA;                                                   clip_market1501.pt               # heavy&#xA;                                                   clip_vehicleid.pt&#xA;                                                   ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Filter tracked classes&lt;/summary&gt; &#xA;  &lt;p&gt;By default the tracker tracks all MS COCO classes.&lt;/p&gt; &#xA;  &lt;p&gt;If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/track.py --source 0 --yolo-model yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/&#34;&gt;Here&lt;/a&gt; is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;MOT compliant results&lt;/summary&gt; &#xA;  &lt;p&gt;Can be saved to your experiment folder &lt;code&gt;runs/track/exp*/&lt;/code&gt; by&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/track.py --source ... --save-mot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Evaluation&lt;/summary&gt; &#xA; &lt;p&gt;Evaluate a combination of detector, tracking method and ReID model on standard MOT dataset or you custom one by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python3 examples/val.py --yolo-model yolo_nas_s.pt --reid-model osnetx1_0_dukemtcereid.pt --tracking-method deepocsort --benchmark MOT16&#xA;                          --yolo-model yolox_n.pt    --reid-model osnet_ain_x1_0_msmt17.pt  --tracking-method ocsort     --benchmark MOT17&#xA;                          --yolo-model yolov8s.pt    --reid-model lmbn_n_market.pt          --tracking-method strongsort --benchmark &amp;lt;your-custom-dataset&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Evolution&lt;/summary&gt; &#xA; &lt;p&gt;We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python examples/evolve.py --tracking-method strongsort --benchmark MOT17 --n-trials 100  # tune strongsort for MOT17&#xA;                            --tracking-method ocsort     --benchmark &amp;lt;your-custom-dataset&amp;gt; --objective HOTA # tune ocsort for maximizing HOTA on your custom tracking dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The set of hyperparameters leading to the best HOTA result are written to the tracker&#39;s config file.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Custom object detection model tracking example&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Minimalistic&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import numpy as np&#xA;from pathlib import Path&#xA;&#xA;from boxmot import DeepOCSORT&#xA;&#xA;&#xA;tracker = DeepOCSORT(&#xA;    model_weights=Path(&#39;osnet_x0_25_msmt17.pt&#39;), # which ReID model to use&#xA;    device=&#39;cuda:0&#39;,&#xA;    fp16=False,&#xA;)&#xA;&#xA;vid = cv2.VideoCapture(0)&#xA;&#xA;while True:&#xA;    ret, im = vid.read()&#xA;&#xA;    # substitute by your object detector, output has to be N X (x, y, x, y, conf, cls)&#xA;    dets = np.array([[144, 212, 578, 480, 0.82, 0],&#xA;                    [425, 281, 576, 472, 0.56, 65]])&#xA;&#xA;    tracks = tracker.update(dets, im) # --&amp;gt; (x, y, x, y, id, conf, cls, ind)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Complete&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import numpy as np&#xA;from pathlib import Path&#xA;&#xA;from boxmot import DeepOCSORT&#xA;&#xA;&#xA;tracker = DeepOCSORT(&#xA;    model_weights=Path(&#39;osnet_x0_25_msmt17.pt&#39;), # which ReID model to use&#xA;    device=&#39;cuda:0&#39;,&#xA;    fp16=True,&#xA;)&#xA;&#xA;vid = cv2.VideoCapture(0)&#xA;color = (0, 0, 255)  # BGR&#xA;thickness = 2&#xA;fontscale = 0.5&#xA;&#xA;while True:&#xA;    ret, im = vid.read()&#xA;&#xA;    # substitute by your object detector, input to tracker has to be N X (x, y, x, y, conf, cls)&#xA;    dets = np.array([[144, 212, 578, 480, 0.82, 0],&#xA;                    [425, 281, 576, 472, 0.56, 65]])&#xA;&#xA;    tracks = tracker.update(dets, im) # --&amp;gt; (x, y, x, y, id, conf, cls, ind)&#xA;&#xA;    xyxys = tracks[:, 0:4].astype(&#39;int&#39;) # float64 to int&#xA;    ids = tracks[:, 4].astype(&#39;int&#39;) # float64 to int&#xA;    confs = tracks[:, 5]&#xA;    clss = tracks[:, 6].astype(&#39;int&#39;) # float64 to int&#xA;    inds = tracks[:, 7].astype(&#39;int&#39;) # float64 to int&#xA;&#xA;    # in case you have segmentations or poses alongside with your detections you can use&#xA;    # the ind variable in order to identify which track is associated to each seg or pose by:&#xA;    # segs = segs[inds]&#xA;    # poses = poses[inds]&#xA;    # you can then zip them together: zip(tracks, poses)&#xA;&#xA;    # print bboxes with their associated id, cls and conf&#xA;    if tracks.shape[0] != 0:&#xA;        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):&#xA;            im = cv2.rectangle(&#xA;                im,&#xA;                (xyxy[0], xyxy[1]),&#xA;                (xyxy[2], xyxy[3]),&#xA;                color,&#xA;                thickness&#xA;            )&#xA;            cv2.putText(&#xA;                im,&#xA;                f&#39;id: {id}, conf: {conf}, c: {cls}&#39;,&#xA;                (xyxy[0], xyxy[1]-10),&#xA;                cv2.FONT_HERSHEY_SIMPLEX,&#xA;                fontscale,&#xA;                color,&#xA;                thickness&#xA;            )&#xA;&#xA;    # show image with bboxes, ids, classes and confidences&#xA;    cv2.imshow(&#39;frame&#39;, im)&#xA;&#xA;    # break on pressing q&#xA;    if cv2.waitKey(1) &amp;amp; 0xFF == ord(&#39;q&#39;):&#xA;        break&#xA;&#xA;vid.release()&#xA;cv2.destroyAllWindows()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tiled inference&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from sahi import AutoDetectionModel&#xA;from sahi.predict import get_sliced_prediction&#xA;import cv2&#xA;import numpy as np&#xA;from pathlib import Path&#xA;from boxmot import DeepOCSORT&#xA;&#xA;&#xA;tracker = DeepOCSORT(&#xA;    model_weights=Path(&#39;osnet_x0_25_msmt17.pt&#39;), # which ReID model to use&#xA;    device=&#39;cpu&#39;,&#xA;    fp16=False,&#xA;)&#xA;&#xA;detection_model = AutoDetectionModel.from_pretrained(&#xA;    model_type=&#39;yolov8&#39;,&#xA;    model_path=&#39;yolov8n.pt&#39;,&#xA;    confidence_threshold=0.5,&#xA;    device=&#34;cpu&#34;,  # or &#39;cuda:0&#39;&#xA;)&#xA;&#xA;vid = cv2.VideoCapture(0)&#xA;color = (0, 0, 255)  # BGR&#xA;thickness = 2&#xA;fontscale = 0.5&#xA;&#xA;while True:&#xA;    ret, im = vid.read()&#xA;&#xA;    # get sliced predictions&#xA;    result = get_sliced_prediction(&#xA;        im,&#xA;        detection_model,&#xA;        slice_height=256,&#xA;        slice_width=256,&#xA;        overlap_height_ratio=0.2,&#xA;        overlap_width_ratio=0.2&#xA;    )&#xA;    num_predictions = len(result.object_prediction_list)&#xA;    dets = np.zeros([num_predictions, 6], dtype=np.float32)&#xA;    for ind, object_prediction in enumerate(result.object_prediction_list):&#xA;        dets[ind, :4] = np.array(object_prediction.bbox.to_xyxy(), dtype=np.float32)&#xA;        dets[ind, 4] = object_prediction.score.value&#xA;        dets[ind, 5] = object_prediction.category.id&#xA;&#xA;    tracks = tracker.update(dets, im) # --&amp;gt; (x, y, x, y, id, conf, cls, ind)&#xA;&#xA;    if tracks.shape[0] != 0:&#xA;&#xA;        xyxys = tracks[:, 0:4].astype(&#39;int&#39;) # float64 to int&#xA;        ids = tracks[:, 4].astype(&#39;int&#39;) # float64 to int&#xA;        confs = tracks[:, 5].round(decimals=2)&#xA;        clss = tracks[:, 6].astype(&#39;int&#39;) # float64 to int&#xA;        inds = tracks[:, 7].astype(&#39;int&#39;) # float64 to int&#xA;&#xA;        # print bboxes with their associated id, cls and conf&#xA;        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):&#xA;            im = cv2.rectangle(&#xA;                im,&#xA;                (xyxy[0], xyxy[1]),&#xA;                (xyxy[2], xyxy[3]),&#xA;                color,&#xA;                thickness&#xA;            )&#xA;            cv2.putText(&#xA;                im,&#xA;                f&#39;id: {id}, conf: {conf}, c: {cls}&#39;,&#xA;                (xyxy[0], xyxy[1]-10),&#xA;                cv2.FONT_HERSHEY_SIMPLEX,&#xA;                fontscale,&#xA;                color,&#xA;                thickness&#xA;            )&#xA;&#xA;    # show image with bboxes, ids, classes and confidences&#xA;    cv2.imshow(&#39;frame&#39;, im)&#xA;&#xA;    # break on pressing q&#xA;    if cv2.waitKey(1) &amp;amp; 0xFF == ord(&#39;q&#39;):&#xA;        break&#xA;&#xA;vid.release()&#xA;cv2.destroyAllWindows()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/mikel-brostrom/yolo_tracking/graphs/contributors &#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=mikel-brostrom/yolo_tracking&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For Yolo tracking bugs and feature requests please visit &lt;a href=&#34;https://github.com/mikel-brostrom/yolo_tracking/issues&#34;&gt;GitHub Issues&lt;/a&gt;. For business inquiries or professional support requests please send an email to: &lt;a href=&#34;mailto:yolov5.deepsort.pytorch@gmail.com&#34;&gt;yolov5.deepsort.pytorch@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>