<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-09T01:30:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen-Agent</title>
    <updated>2024-03-09T01:30:15Z</updated>
    <id>tag:github.com,2024-03-09:/QwenLM/Qwen-Agent</id>
    <link href="https://github.com/QwenLM/Qwen-Agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent framework and applications built upon Qwen1.5, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; ÔΩú English&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/logo-qwen-agent.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;Qwen-Agent is a framework for developing LLM applications based on the instruction following, tool usage, planning, and memory capabilities of Qwen. It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies.&#xA;git clone https://github.com/QwenLM/Qwen-Agent.git&#xA;cd Qwen-Agent&#xA;pip install -e ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Preparation: Model Service&lt;/h2&gt; &#xA;&lt;p&gt;You can either use the model service provided by Alibaba Cloud&#39;s &lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/quick-start&#34;&gt;DashScope&lt;/a&gt;, or deploy and use your own model service using the open-source Qwen models.&lt;/p&gt; &#xA;&lt;p&gt;If you choose to use the model service offered by DashScope, please ensure that you set the environment variable &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt; to your unique DashScope API key.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, if you prefer to deploy and utilize your own model service, kindly follow the instructions outlined in the &lt;a href=&#34;https://github.com/QwenLM/Qwen1.5?tab=readme-ov-file#deployment&#34;&gt;Deployment&lt;/a&gt; section of Qwen1.5&#39;s README to start an OpenAI-compatible API service.&lt;/p&gt; &#xA;&lt;h2&gt;Developing Your Own Agent&lt;/h2&gt; &#xA;&lt;p&gt;Qwen-Agent provides atomic components such as LLMs and prompts, as well as high-level components such as Agents. The example below uses the Assistant component as an illustration, demonstrating how to add custom tools and quickly develop an agent that uses tools.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import json&#xA;import os&#xA;&#xA;import json5&#xA;import urllib.parse&#xA;from qwen_agent.agents import Assistant&#xA;from qwen_agent.tools.base import BaseTool, register_tool&#xA;&#xA;llm_cfg = {&#xA;    # Use the model service provided by DashScope:&#xA;    &#39;model&#39;: &#39;qwen-max&#39;,&#xA;    &#39;model_server&#39;: &#39;dashscope&#39;,&#xA;    # &#39;api_key&#39;: &#39;YOUR_DASHSCOPE_API_KEY&#39;,&#xA;    # It will use the `DASHSCOPE_API_KEY&#39; environment variable if &#39;api_key&#39; is not set here.&#xA;&#xA;    # Use your own model service compatible with OpenAI API:&#xA;    # &#39;model&#39;: &#39;Qwen/Qwen1.5-72B-Chat&#39;,&#xA;    # &#39;model_server&#39;: &#39;http://localhost:8000/v1&#39;,  # api_base&#xA;    # &#39;api_key&#39;: &#39;EMPTY&#39;,&#xA;&#xA;    # (Optional) LLM hyperparameters for generation:&#xA;    &#39;generate_cfg&#39;: {&#xA;        &#39;top_p&#39;: 0.8&#xA;    }&#xA;}&#xA;system = &#39;According to the user\&#39;s request, you first draw a picture and then automatically run code to download the picture &#39; + \&#xA;          &#39;and select an image operation from the given document to process the image&#39;&#xA;&#xA;# Add a custom tool named my_image_genÔºö&#xA;@register_tool(&#39;my_image_gen&#39;)&#xA;class MyImageGen(BaseTool):&#xA;    description = &#39;AI painting (image generation) service, input text description, and return the image URL drawn based on text information.&#39;&#xA;    parameters = [{&#xA;        &#39;name&#39;: &#39;prompt&#39;,&#xA;        &#39;type&#39;: &#39;string&#39;,&#xA;        &#39;description&#39;: &#39;Detailed description of the desired image content, in English&#39;,&#xA;        &#39;required&#39;: True&#xA;    }]&#xA;&#xA;    def call(self, params: str, **kwargs) -&amp;gt; str:&#xA;        prompt = json5.loads(params)[&#39;prompt&#39;]&#xA;        prompt = urllib.parse.quote(prompt)&#xA;        return json.dumps(&#xA;            {&#39;image_url&#39;: f&#39;https://image.pollinations.ai/prompt/{prompt}&#39;},&#xA;            ensure_ascii=False)&#xA;&#xA;&#xA;tools = [&#39;my_image_gen&#39;, &#39;code_interpreter&#39;]  # code_interpreter is a built-in tool in Qwen-Agent&#xA;bot = Assistant(llm=llm_cfg,&#xA;                system_message=system,&#xA;                function_list=tools,&#xA;                files=[os.path.abspath(&#39;doc.pdf&#39;)])&#xA;&#xA;messages = []&#xA;while True:&#xA;    query = input(&#39;user question: &#39;)&#xA;    messages.append({&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: query})&#xA;    response = []&#xA;    for response in bot.run(messages=messages):&#xA;        print(&#39;bot response:&#39;, response)&#xA;    messages.extend(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The framework also provides more atomic components for developers to combine. For additional showcases, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h1&gt;Example Application: BrowserQwen&lt;/h1&gt; &#xA;&lt;p&gt;We have also developed an example application based on Qwen-Agent: a &lt;strong&gt;Chrome browser extension&lt;/strong&gt; called BrowserQwen, which has key features such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can discuss with Qwen regarding the current webpage or PDF document.&lt;/li&gt; &#xA; &lt;li&gt;It records the web pages and PDF/Word/PowerPoint materials that you have browsed. It helps you understand multiple pages, summarize your browsing content, and automate writing tasks.&lt;/li&gt; &#xA; &lt;li&gt;It comes with plugin integration, including &lt;strong&gt;Code Interpreter&lt;/strong&gt; for math problem solving and data visualization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BrowserQwen Demonstration&lt;/h2&gt; &#xA;&lt;p&gt;You can watch the following showcase videos to learn about the basic operations of BrowserQwen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Long-form writing based on visited webpages and PDFs. &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/showcase_write_article_based_on_webpages_and_pdfs.mp4&#34;&gt;video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Drawing a plot using code interpreter based on the given information. &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/showcase_chat_with_docs_and_code_interpreter.mp4&#34;&gt;video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Uploading files, multi-turn conversation, and data analysis using code interpreter. &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/showcase_code_interpreter_multi_turn_chat.mp4&#34;&gt;video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Workstation - Editor Mode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This mode is designed for creating long articles based on browsed web pages and PDFs.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-writing.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;p&gt;&lt;strong&gt;It allows you to call plugins to assist in rich text creation.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-editor-movie.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;h3&gt;Workstation - Chat Mode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;In this mode, you can engage in multi-webpage QA.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-multi-web-qa.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;p&gt;&lt;strong&gt;Create data charts using the code interpreter.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-ci.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;h3&gt;Browser Assistant&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Web page QA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-web-qa.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDF document QA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-pdf-qa.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;h2&gt;BrowserQwen User Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Deploy Local Database Service&lt;/h3&gt; &#xA;&lt;p&gt;On your local machine (the machine where you can open the Chrome browser), you will need to deploy a database service to manage your browsing history and conversation history.&lt;/p&gt; &#xA;&lt;p&gt;If you are using DashScope&#39;s model service, then please execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start the database service, specifying the model on DashScope by using the --llm flag.&#xA;# The value of --llm can be one of the following, in increasing order of resource consumption:&#xA;#   - qwen1.5-7b/14b/72b-chat (the same as the open-sourced Qwen1.5 7B/14B/72B Chat model)&#xA;#   - qwen-turbo, qwen-plus, qwen-max (qwen-max is recommended)&#xA;# &#34;YOUR_DASHSCOPE_API_KEY&#34; is a placeholder. The user should replace it with their actual key.&#xA;python run_server.py --llm qwen-max --model_server dashscope --workstation_port 7864 --api_key YOUR_DASHSCOPE_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using your own model service instead of DashScope, then please execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Specify the model service, and start the database service.&#xA;# Example: Assuming Qwen/Qwen1.5-72B-Chat is deployed at http://localhost:8000 using vLLM, you can specify the model service as:&#xA;#   --llm Qwen/Qwen1.5-72B-Chat --model_server http://localhost:8000/v1 --api_key EMPTY&#xA;python run_server.py --llm {MODEL} --model_server {API_BASE} --workstation_port 7864 --api_key {API_KEY}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can access &lt;a href=&#34;http://127.0.0.1:7864/&#34;&gt;http://127.0.0.1:7864/&lt;/a&gt; to use the Workstation&#39;s Editor mode and Chat mode.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2. Install Browser Assistant&lt;/h3&gt; &#xA;&lt;p&gt;Install the BrowserQwen Chrome extension:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the Chrome browser and enter &lt;code&gt;chrome://extensions/&lt;/code&gt; in the address bar, then press Enter.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that the &lt;code&gt;Developer mode&lt;/code&gt; in the top right corner is turned on, then click on &lt;code&gt;Load unpacked&lt;/code&gt; to upload the &lt;code&gt;browser_qwen&lt;/code&gt; directory from this project and enable it.&lt;/li&gt; &#xA; &lt;li&gt;Click the extension icon in the top right corner of the Chrome browser to pin BrowserQwen to the toolbar.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that after installing the Chrome extension, you need to refresh the page for the extension to take effect.&lt;/p&gt; &#xA;&lt;p&gt;When you want Qwen to read the content of the current webpage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Add to Qwen&#39;s Reading List&lt;/code&gt; button on the screen to authorize Qwen to analyze the page in the background.&lt;/li&gt; &#xA; &lt;li&gt;Click the Qwen icon in the browser&#39;s top right corner to start interacting with Qwen about the current page&#39;s content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project is currently under active development, and backward compatibility may occasionally be broken.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Important: The code interpreter is not sandboxed, and it executes code in your own environment. Please do not ask Qwen to perform dangerous tasks, and do not directly use the code interpreter for production purposes.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>VAST-AI-Research/TripoSR</title>
    <updated>2024-03-09T01:30:15Z</updated>
    <id>tag:github.com,2024-03-09:/VAST-AI-Research/TripoSR</id>
    <link href="https://github.com/VAST-AI-Research/TripoSR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TripoSR &lt;a href=&#34;https://huggingface.co/stabilityai/TripoSR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Model_Card-Huggingface-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/stabilityai/TripoSR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.02151&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2403.02151-B31B1B.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/teaser800.gif&#34; alt=&#34;Teaser Video&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;TripoSR&lt;/strong&gt;, a state-of-the-art open-source model for &lt;strong&gt;fast&lt;/strong&gt; feedforward 3D reconstruction from a single image, collaboratively developed by &lt;a href=&#34;https://www.tripo3d.ai/&#34;&gt;Tripo AI&lt;/a&gt; and &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt;. &lt;br&gt;&lt;br&gt; Leveraging the principles of the &lt;a href=&#34;https://yiconghong.me/LRM/&#34;&gt;Large Reconstruction Model (LRM)&lt;/a&gt;, TripoSR brings to the table key advancements that significantly boost both the speed and quality of 3D reconstruction. Our model is distinguished by its ability to rapidly process inputs, generating high-quality 3D models in less than 0.5 seconds on an NVIDIA A100 GPU. TripoSR has exhibited superior performance in both qualitative and quantitative evaluations, outperforming other open-source alternatives across multiple public datasets. The figures below illustrate visual comparisons and metrics showcasing TripoSR&#39;s performance relative to other leading models. Details about the model architecture, training process, and comparisons can be found in this &lt;a href=&#34;https://arxiv.org/abs/2403.02151&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!--&#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;figures/comparison800.gif&#34; alt=&#34;Teaser Video&#34;&gt;&#xA;&lt;/div&gt;&#xA;--&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/visual_comparisons.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;450&#34; src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/scatter-comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The model is released under the MIT license, which includes the source code, pretrained models, and an interactive online demo. Our goal is to empower researchers, developers, and creatives to push the boundaries of what&#39;s possible in 3D generative AI and 3D content creation.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA if available&lt;/li&gt; &#xA; &lt;li&gt;Install PyTorch according to your platform: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt; &lt;strong&gt;[Please make sure that the locally-installed CUDA major version matches the PyTorch-shipped CUDA major version. For example if you have CUDA 11.x installed, make sure to install PyTorch compiled with CUDA 11.x.]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update setuptools by &lt;code&gt;pip install --upgrade setuptools&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install other dependencies by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Manual Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python run.py examples/chair.png --output-dir output/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save the reconstructed 3D model to &lt;code&gt;output/&lt;/code&gt;. You can also specify more than one image path separated by spaces. The default options takes about &lt;strong&gt;6GB VRAM&lt;/strong&gt; for a single image input.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage of this script, use &lt;code&gt;python run.py --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local Gradio App&lt;/h3&gt; &#xA;&lt;p&gt;Install Gradio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the Gradio App:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;AttributeError: module &#39;torchmcubes_module&#39; has no attribute &#39;mcubes_cuda&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;torchmcubes was not compiled with CUDA support, use CPU version instead.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This is because &lt;code&gt;torchmcubes&lt;/code&gt; is compiled without CUDA support. Please make sure that&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The locally-installed CUDA major version matches the PyTorch-shipped CUDA major version. For example if you have CUDA 11.x installed, make sure to install PyTorch compiled with CUDA 11.x.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setuptools&amp;gt;=49.6.0&lt;/code&gt;. If not, upgrade by &lt;code&gt;pip install --upgrade setuptools&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then re-install &lt;code&gt;torchmcubes&lt;/code&gt; by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip uninstall torchmcubes&#xA;pip install git+https://github.com/tatsy/torchmcubes.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{TripoSR2024,&#xA;  title={TripoSR: Fast 3D Object Reconstruction from a Single Image},&#xA;  author={Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei},&#xA;  journal={arXiv preprint arXiv:2403.02151},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>abi/screenshot-to-code</title>
    <updated>2024-03-09T01:30:15Z</updated>
    <id>tag:github.com,2024-03-09:/abi/screenshot-to-code</id>
    <link href="https://github.com/abi/screenshot-to-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Drop in a screenshot and convert it to clean code (HTML/Tailwind/React/Vue)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;screenshot-to-code&lt;/h1&gt; &#xA;&lt;p&gt;This simple app converts a screenshot to code (HTML/Tailwind CSS, or React or Bootstrap or Vue). It uses GPT-4 Vision (or Claude 3) to generate the code and DALL-E 3 to generate similar-looking images. You can now also enter a URL to clone a live website.&lt;/p&gt; &#xA;&lt;p&gt;üÜï Now, supporting Claude 3!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abi/screenshot-to-code/assets/23818/6cebadae-2fe3-4986-ac6a-8fb9db030045&#34;&gt;https://github.com/abi/screenshot-to-code/assets/23818/6cebadae-2fe3-4986-ac6a-8fb9db030045&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/abi/screenshot-to-code/main/#-examples&#34;&gt;Examples&lt;/a&gt; section below for more demos.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/_abi_&#34;&gt;Follow me on Twitter for updates&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Try It Out!&lt;/h2&gt; &#xA;&lt;p&gt;üÜï &lt;a href=&#34;https://screenshottocode.com&#34;&gt;Try it here&lt;/a&gt; (bring your own OpenAI key - &lt;strong&gt;your key must have access to GPT-4 Vision. See &lt;a href=&#34;https://raw.githubusercontent.com/abi/screenshot-to-code/main/#%EF%B8%8F-faqs&#34;&gt;FAQ&lt;/a&gt; section below for details&lt;/strong&gt;). Or see &lt;a href=&#34;https://raw.githubusercontent.com/abi/screenshot-to-code/main/#-getting-started&#34;&gt;Getting Started&lt;/a&gt; below for local install instructions.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Recent Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mar 8 - üî•üéâüéÅ Video-to-app: turn videos/screen recordings into functional apps&lt;/li&gt; &#xA; &lt;li&gt;Mar 5 - Added support for Claude Sonnet 3 (as capable as or better than GPT-4 Vision, and faster!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ† Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The app has a React/Vite frontend and a FastAPI backend. You will need an OpenAI API key with access to the GPT-4 Vision API.&lt;/p&gt; &#xA;&lt;p&gt;Run the backend (I use Poetry for package management - &lt;code&gt;pip install poetry&lt;/code&gt; if you don&#39;t have it):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd backend&#xA;echo &#34;OPENAI_API_KEY=sk-your-key&#34; &amp;gt; .env&#xA;poetry install&#xA;poetry shell&#xA;poetry run uvicorn main:app --reload --port 7001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the frontend:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;yarn&#xA;yarn dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://localhost:5173&#34;&gt;http://localhost:5173&lt;/a&gt; to use the app.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to run the backend on a different port, update VITE_WS_BACKEND_URL in &lt;code&gt;frontend/.env.local&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For debugging purposes, if you don&#39;t want to waste GPT4-Vision credits, you can run the backend in mock mode (which streams a pre-recorded response):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MOCK=true poetry run uvicorn main:app --reload --port 7001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Video to app (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abi/screenshot-to-code/assets/23818/1468bef4-164f-4046-a6c8-4cfc40a5cdff&#34;&gt;https://github.com/abi/screenshot-to-code/assets/23818/1468bef4-164f-4046-a6c8-4cfc40a5cdff&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Record yourself using any website or app or even a Figma prototype, drag &amp;amp; drop in a video and in a few minutes, get a functional, similar-looking app.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abi/screenshot-to-code/raw/main/blog/video-to-app.md&#34;&gt;You need an Anthropic API key for this functionality. Follow instructions here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can configure the OpenAI base URL if you need to use a proxy: Set OPENAI_BASE_URL in the &lt;code&gt;backend/.env&lt;/code&gt; or directly in the UI in the settings dialog&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using Claude 3&lt;/h2&gt; &#xA;&lt;p&gt;We recently added support for Claude 3 Sonnet. It performs well, on par or better than GPT-4 vision for many inputs, and it tends to be faster.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add an env var &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; to &lt;code&gt;backend/.env&lt;/code&gt; with your API key from Anthropic&lt;/li&gt; &#xA; &lt;li&gt;When using the front-end, select &#34;Claude 3 Sonnet&#34; from the model dropdown&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;If you have Docker installed on your system, in the root directory, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#34;OPENAI_API_KEY=sk-your-key&#34; &amp;gt; .env&#xA;docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The app will be up and running at &lt;a href=&#34;http://localhost:5173&#34;&gt;http://localhost:5173&lt;/a&gt;. Note that you can&#39;t develop the application with this setup as the file changes won&#39;t trigger a rebuild.&lt;/p&gt; &#xA;&lt;h2&gt;üôã‚Äç‚ôÇÔ∏è FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;I&#39;m running into an error when setting up the backend. How can I fix it?&lt;/strong&gt; &lt;a href=&#34;https://github.com/abi/screenshot-to-code/issues/3#issuecomment-1814777959&#34;&gt;Try this&lt;/a&gt;. If that still doesn&#39;t work, open an issue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How do I get an OpenAI API key?&lt;/strong&gt; See &lt;a href=&#34;https://github.com/abi/screenshot-to-code/raw/main/Troubleshooting.md&#34;&gt;https://github.com/abi/screenshot-to-code/blob/main/Troubleshooting.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How can I provide feedback?&lt;/strong&gt; For feedback, feature requests and bug reports, open an issue or ping me on &lt;a href=&#34;https://twitter.com/_abi_&#34;&gt;Twitter&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NYTimes&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Original&lt;/th&gt; &#xA;   &lt;th&gt;Replica&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;1238&#34; alt=&#34;Screenshot 2023-11-20 at 12 54 03 PM&#34; src=&#34;https://github.com/abi/screenshot-to-code/assets/23818/3b644dfa-9ca6-4148-84a7-3405b6671922&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;1414&#34; alt=&#34;Screenshot 2023-11-20 at 12 59 56 PM&#34; src=&#34;https://github.com/abi/screenshot-to-code/assets/23818/26201c9f-1a28-4f35-a3b1-1f04e2b8ce2a&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instagram page (with not Taylor Swift pics)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abi/screenshot-to-code/assets/23818/503eb86a-356e-4dfc-926a-dabdb1ac7ba1&#34;&gt;https://github.com/abi/screenshot-to-code/assets/23818/503eb86a-356e-4dfc-926a-dabdb1ac7ba1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hacker News&lt;/strong&gt; but it gets the colors wrong at first so we nudge it&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abi/screenshot-to-code/assets/23818/3fec0f77-44e8-4fb3-a769-ac7410315e5d&#34;&gt;https://github.com/abi/screenshot-to-code/assets/23818/3fec0f77-44e8-4fb3-a769-ac7410315e5d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üåç Hosted Version&lt;/h2&gt; &#xA;&lt;p&gt;üÜï &lt;a href=&#34;https://screenshottocode.com&#34;&gt;Try it here&lt;/a&gt; (bring your own OpenAI key - &lt;strong&gt;your key must have access to GPT-4 Vision. See &lt;a href=&#34;https://raw.githubusercontent.com/abi/screenshot-to-code/main/#%EF%B8%8F-faqs&#34;&gt;FAQ&lt;/a&gt; section for details&lt;/strong&gt;). Or see &lt;a href=&#34;https://raw.githubusercontent.com/abi/screenshot-to-code/main/#-getting-started&#34;&gt;Getting Started&lt;/a&gt; for local install instructions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/abiraja&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;&amp;quot;Buy Me A Coffee&amp;quot;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>