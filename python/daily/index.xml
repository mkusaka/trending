<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-02T01:44:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/Bringing-Old-Photos-Back-to-Life</title>
    <updated>2023-02-02T01:44:36Z</updated>
    <id>tag:github.com,2023-02-02:/microsoft/Bringing-Old-Photos-Back-to-Life</id>
    <link href="https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing Old Photo Back to Life (CVPR 2020 oral)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Old Photo Restoration (Official PyTorch Implementation)&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/0001.jpg&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://raywzy.com/Old_Photo/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2004.09484&#34;&gt;Paper (CVPR version)&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2009.07047v1.pdf&#34;&gt;Paper (Journal version)&lt;/a&gt; | &lt;a href=&#34;https://hkustconnect-my.sharepoint.com/:f:/g/personal/bzhangai_connect_ust_hk/Em0KnYOeSSxFtp4g_dhWdf0BdeT3tY12jIYJ6qvSf300cA?e=nXkJH2&#34;&gt;Pretrained Model&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; | &lt;a href=&#34;https://replicate.ai/zhangmozhe/bringing-old-photos-back-to-life&#34;&gt;Replicate Demo &amp;amp; Docker Image&lt;/a&gt; &lt;span&gt;ðŸ”¥&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bringing Old Photos Back to Life, CVPR2020 (Oral)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Old Photo Restoration via Deep Latent Space Translation, TPAMI 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://raywzy.com/&#34;&gt;Ziyu Wan&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/zhanbo/&#34;&gt;Bo Zhang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://www.dongdongchen.bid/&#34;&gt;Dongdong Chen&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://panzhang0212.github.io/&#34;&gt;Pan Zhang&lt;/a&gt;&lt;sup&gt;4&lt;/sup&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/doch/&#34;&gt;Dong Chen&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://liaojing.github.io/html/&#34;&gt;Jing Liao&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/fangwen/&#34;&gt;Fang Wen&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;City University of Hong Kong, &lt;sup&gt;2&lt;/sup&gt;Microsoft Research Asia, &lt;sup&gt;3&lt;/sup&gt;Microsoft Cloud AI, &lt;sup&gt;4&lt;/sup&gt;USTC&lt;/p&gt; &#xA;&lt;!-- ## Notes of this project&#xA;The code originates from our research project and the aim is to demonstrate the research idea, so we have not optimized it from a product perspective. And we will spend time to address some common issues, such as out of memory issue, limited resolution, but will not involve too much in engineering problems, such as speedup of the inference, fastapi deployment and so on. **We welcome volunteers to contribute to this project to make it more usable for practical application.** --&gt; &#xA;&lt;h2&gt;&lt;span&gt;âœ¨&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2022.3.31&lt;/strong&gt;: Our new work regarding old film restoration will be published in CVPR 2022. For more details, please refer to the &lt;a href=&#34;http://raywzy.com/Old_Film/&#34;&gt;project website&lt;/a&gt; and &lt;a href=&#34;https://github.com/raywzy/Bringing-Old-Films-Back-to-Life&#34;&gt;github repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The framework now supports the restoration of high-resolution input.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/HR_result.png&#34;&gt; &#xA;&lt;p&gt;Training code is available and welcome to have a try and learn the training details.&lt;/p&gt; &#xA;&lt;p&gt;You can now play with our &lt;a href=&#34;https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing&#34;&gt;Colab&lt;/a&gt; and try it on your photos.&lt;/p&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;p&gt;The code is tested on Ubuntu with Nvidia GPUs and CUDA installed. Python&amp;gt;=3.6 is required to run the code.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Clone the Synchronized-BatchNorm-PyTorch repository for&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Face_Enhancement/models/networks/&#xA;git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch&#xA;cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .&#xA;cd ../../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Global/detection_models&#xA;git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch&#xA;cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .&#xA;cd ../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the landmark detection pretrained model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Face_Detection/&#xA;wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2&#xA;bzip2 -d shape_predictor_68_face_landmarks.dat.bz2&#xA;cd ../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained model, put the file &lt;code&gt;Face_Enhancement/checkpoints.zip&lt;/code&gt; under &lt;code&gt;./Face_Enhancement&lt;/code&gt;, and put the file &lt;code&gt;Global/checkpoints.zip&lt;/code&gt; under &lt;code&gt;./Global&lt;/code&gt;. Then unzip them respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Face_Enhancement/&#xA;wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/face_checkpoints.zip&#xA;unzip face_checkpoints.zip&#xA;cd ../&#xA;cd Global/&#xA;wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/global_checkpoints.zip&#xA;unzip global_checkpoints.zip&#xA;cd ../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸš€&lt;/span&gt; How to use?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: GPU can be set 0 or 0,1,2 or 0,2; use -1 for CPU&lt;/p&gt; &#xA;&lt;h3&gt;1) Full Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;You could easily restore the old photos with one simple command after installation and downloading the pretrained model.&lt;/p&gt; &#xA;&lt;p&gt;For images without scratches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --input_folder [test_image_folder_path] \&#xA;              --output_folder [output_path] \&#xA;              --GPU 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For scratched images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --input_folder [test_image_folder_path] \&#xA;              --output_folder [output_path] \&#xA;              --GPU 0 \&#xA;              --with_scratch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For high-resolution images with scratches&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --input_folder [test_image_folder_path] \&#xA;              --output_folder [output_path] \&#xA;              --GPU 0 \&#xA;              --with_scratch \&#xA;              --HR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Please try to use the absolute path. The final results will be saved in &lt;code&gt;./output_path/final_output/&lt;/code&gt;. You could also check the produced results of different steps in &lt;code&gt;output_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2) Scratch Detection&lt;/h3&gt; &#xA;&lt;p&gt;Currently we don&#39;t plan to release the scratched old photos dataset with labels directly. If you want to get the paired data, you could use our pretrained model to test the collected images to obtain the labels.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Global/&#xA;python detection.py --test_path [test_image_folder_path] \&#xA;                    --output_dir [output_path] \&#xA;                    --input_size [resize_256|full_size|scale_256]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/scratch_detection.png&#34;&gt; &#xA;&lt;h3&gt;3) Global Restoration&lt;/h3&gt; &#xA;&lt;p&gt;A triplet domain translation network is proposed to solve both structured degradation and unstructured degradation of old photos.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/pipeline.PNG&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Global/&#xA;python test.py --Scratch_and_Quality_restore \&#xA;               --test_input [test_image_folder_path] \&#xA;               --test_mask [corresponding mask] \&#xA;               --outputs_dir [output_path]&#xA;&#xA;python test.py --Quality_restore \&#xA;               --test_input [test_image_folder_path] \&#xA;               --outputs_dir [output_path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/global.png&#34;&gt; &#xA;&lt;h3&gt;4) Face Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;We use a progressive generator to refine the face regions of old photos. More details could be found in our journal submission and &lt;code&gt;./Face_Enhancement&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/face_pipeline.jpg&#34; width=&#34;60%&#34; height=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/face.png&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: This repo is mainly for research purpose and we have not yet optimized the running performance.&lt;/p&gt; &#xA; &lt;p&gt;Since the model is pretrained with 256*256 images, the model may not work ideally for arbitrary resolution.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;5) GUI&lt;/h3&gt; &#xA;&lt;p&gt;A user-friendly GUI which takes input of image by user and shows result in respective window.&lt;/p&gt; &#xA;&lt;h4&gt;How it works:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run GUI.py file.&lt;/li&gt; &#xA; &lt;li&gt;Click browse and select your image from test_images/old_w_scratch folder to remove scratches.&lt;/li&gt; &#xA; &lt;li&gt;Click Modify Photo button.&lt;/li&gt; &#xA; &lt;li&gt;Wait for a while and see results on GUI window.&lt;/li&gt; &#xA; &lt;li&gt;Exit window by clicking Exit Window and get your result image in output folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Bringing-Old-Photos-Back-to-Life/master/imgs/gui.PNG&#34;&gt; &#xA;&lt;h2&gt;How to train?&lt;/h2&gt; &#xA;&lt;h3&gt;1) Create Training File&lt;/h3&gt; &#xA;&lt;p&gt;Put the folders of VOC dataset, collected old photos (e.g., Real_L_old and Real_RGB_old) into one shared folder. Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Global/data/&#xA;python Create_Bigfile.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Remember to modify the code based on your own environment.&lt;/p&gt; &#xA;&lt;h3&gt;2) Train the VAEs of domain A and domain B respectively&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ..&#xA;python train_domain_A.py --use_v2_degradation --continue_train --training_dataset domain_A --name domainA_SR_old_photos --label_nc 0 --loadSize 256 --fineSize 256 --dataroot [your_data_folder] --no_instance --resize_or_crop crop_only --batchSize 100 --no_html --gpu_ids 0,1,2,3 --self_gen --nThreads 4 --n_downsample_global 3 --k_size 4 --use_v2 --mc 64 --start_r 1 --kl 1 --no_cgan --outputs_dir [your_output_folder] --checkpoints_dir [your_ckpt_folder]&#xA;&#xA;python train_domain_B.py --continue_train --training_dataset domain_B --name domainB_old_photos --label_nc 0 --loadSize 256 --fineSize 256 --dataroot [your_data_folder]  --no_instance --resize_or_crop crop_only --batchSize 120 --no_html --gpu_ids 0,1,2,3 --self_gen --nThreads 4 --n_downsample_global 3 --k_size 4 --use_v2 --mc 64 --start_r 1 --kl 1 --no_cgan --outputs_dir [your_output_folder]  --checkpoints_dir [your_ckpt_folder]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: For the --name option, please ensure your experiment name contains &#34;domainA&#34; or &#34;domainB&#34;, which will be used to select different dataset.&lt;/p&gt; &#xA;&lt;h3&gt;3) Train the mapping network between domains&lt;/h3&gt; &#xA;&lt;p&gt;Train the mapping without scratches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_mapping.py --use_v2_degradation --training_dataset mapping --use_vae_which_epoch 200 --continue_train --name mapping_quality --label_nc 0 --loadSize 256 --fineSize 256 --dataroot [your_data_folder] --no_instance --resize_or_crop crop_only --batchSize 80 --no_html --gpu_ids 0,1,2,3 --nThreads 8 --load_pretrainA [ckpt_of_domainA_SR_old_photos] --load_pretrainB [ckpt_of_domainB_old_photos] --l2_feat 60 --n_downsample_global 3 --mc 64 --k_size 4 --start_r 1 --mapping_n_block 6 --map_mc 512 --use_l1_feat --niter 150 --niter_decay 100 --outputs_dir [your_output_folder] --checkpoints_dir [your_ckpt_folder]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Traing the mapping with scraches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_mapping.py --no_TTUR --NL_res --random_hole --use_SN --correlation_renormalize --training_dataset mapping --NL_use_mask --NL_fusion_method combine --non_local Setting_42 --use_v2_degradation --use_vae_which_epoch 200 --continue_train --name mapping_scratch --label_nc 0 --loadSize 256 --fineSize 256 --dataroot [your_data_folder] --no_instance --resize_or_crop crop_only --batchSize 36 --no_html --gpu_ids 0,1,2,3 --nThreads 8 --load_pretrainA [ckpt_of_domainA_SR_old_photos] --load_pretrainB [ckpt_of_domainB_old_photos] --l2_feat 60 --n_downsample_global 3 --mc 64 --k_size 4 --start_r 1 --mapping_n_block 6 --map_mc 512 --use_l1_feat --niter 150 --niter_decay 100 --outputs_dir [your_output_folder] --checkpoints_dir [your_ckpt_folder] --irregular_mask [absolute_path_of_mask_file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Traing the mapping with scraches (Multi-Scale Patch Attention for HR input):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_mapping.py --no_TTUR --NL_res --random_hole --use_SN --correlation_renormalize --training_dataset mapping --NL_use_mask --NL_fusion_method combine --non_local Setting_42 --use_v2_degradation --use_vae_which_epoch 200 --continue_train --name mapping_Patch_Attention --label_nc 0 --loadSize 256 --fineSize 256 --dataroot [your_data_folder] --no_instance --resize_or_crop crop_only --batchSize 36 --no_html --gpu_ids 0,1,2,3 --nThreads 8 --load_pretrainA [ckpt_of_domainA_SR_old_photos] --load_pretrainB [ckpt_of_domainB_old_photos] --l2_feat 60 --n_downsample_global 3 --mc 64 --k_size 4 --start_r 1 --mapping_n_block 6 --map_mc 512 --use_l1_feat --niter 150 --niter_decay 100 --outputs_dir [your_output_folder] --checkpoints_dir [your_ckpt_folder] --irregular_mask [absolute_path_of_mask_file] --mapping_exp 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the following papers :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{wan2020bringing,&#xA;title={Bringing Old Photos Back to Life},&#xA;author={Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},&#xA;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;pages={2747--2757},&#xA;year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wan2020old,&#xA;  title={Old Photo Restoration via Deep Latent Space Translation},&#xA;  author={Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},&#xA;  journal={arXiv preprint arXiv:2009.07047},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are also interested in the legacy photo/video colorization, please refer to &lt;a href=&#34;https://github.com/zhangmozhe/video-colorization&#34;&gt;this work&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Maintenance&lt;/h2&gt; &#xA;&lt;p&gt;This project is currently maintained by Ziyu Wan and is for academic research use only. If you have any questions, feel free to contact &lt;a href=&#34;mailto:raywzy@gmail.com&#34;&gt;raywzy@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The codes and the pretrained model in this repository are under the MIT license as specified by the LICENSE file. We use our labeled dataset to train the scratch detection model.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tysam-code/hlb-CIFAR10</title>
    <updated>2023-02-02T01:44:36Z</updated>
    <id>tag:github.com,2023-02-02:/tysam-code/hlb-CIFAR10</id>
    <link href="https://github.com/tysam-code/hlb-CIFAR10" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Train to 94% on CIFAR-10 in less than 10 seconds on a single A100, the current world record. Or ~95.77% in ~188 seconds.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/hi_tysam&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/hi_tysam.svg?style=social&amp;amp;label=Follow%20%40TySam_And&#34; alt=&#34;Twitter URL&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CIFAR10 hyperlightspeedbench&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the hyperlightspeedbench CIFAR-10 (HLB-CIFAR10) repo.&lt;/p&gt; &#xA;&lt;h3&gt;How to Run&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/tysam-code/hlb-CIFAR10 &amp;amp;&amp;amp; cd hlb-CIFAR10 &amp;amp;&amp;amp; python -m pip install -r requirements.txt &amp;amp;&amp;amp; python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re curious, this code is generally Colab friendly (in fact -- most of this was developed in Colab!). Just be sure to uncomment the reset block at the top of the code.&lt;/p&gt; &#xA;&lt;h3&gt;Main&lt;/h3&gt; &#xA;&lt;p&gt;Goals:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;minimalistic&lt;/li&gt; &#xA; &lt;li&gt;beginner-friendly&lt;/li&gt; &#xA; &lt;li&gt;torch- and python-idiomatic&lt;/li&gt; &#xA; &lt;li&gt;hackable&lt;/li&gt; &#xA; &lt;li&gt;few external dependencies (currently only torch and torchvision)&lt;/li&gt; &#xA; &lt;li&gt;~world-record single-GPU training time (this repo holds the current world record at ~&amp;lt;10 seconds on an A100, down from ~18.1 seconds originally).&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;2 seconds training time in &amp;lt;2 years (yep!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is a neural network implementation of a very speedily-training network that originally started as a painstaking reproduction of &lt;a href=&#34;https://myrtle.ai/learn/how-to-train-your-resnet/&#34;&gt;David Page&#39;s original ultra-fast CIFAR-10 implementation on a single GPU&lt;/a&gt;, but written nearly from the ground-up to be extremely rapid-experimentation-friendly. Part of the benefit of this is that we now hold the world record for single GPU training speeds on CIFAR10 (under 10 seconds on an A100!!!)&lt;/p&gt; &#xA;&lt;p&gt;What we&#39;ve added:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;squeeze and excite layers&lt;/li&gt; &#xA; &lt;li&gt;way too much hyperparameter tuning&lt;/li&gt; &#xA; &lt;li&gt;miscellaneous architecture trimmings (see the patch notes)&lt;/li&gt; &#xA; &lt;li&gt;memory format changes (and more!) to better use tensor cores/etc&lt;/li&gt; &#xA; &lt;li&gt;and more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This code, in comparison to David&#39;s original code, is in a single file and extremely flat, but is not as durable for long-term production-level bug maintenance. You&#39;re meant to check out a fresh repo whenever you have a new idea. It is excellent for rapid idea exploring -- almost everywhere in the pipeline is exposed and built to be user-friendly. I truly enjoy personally using this code, and hope you do as well! :D Please let me know if you have any feedback. I hope to continue publishing updates to this in the future, so your support is encouraged. Share this repo with someone you know that might like it!&lt;/p&gt; &#xA;&lt;p&gt;Feel free to check out my&lt;a href=&#34;https://www.patreon.com/user/posts?u=83632131&#34;&gt;Patreon&lt;/a&gt; if you like what I&#39;m doing here and want more!. Additionally, if you want me to work up to a part-time amount of hours with you, feel free to reach out to me at &lt;a href=&#34;mailto:hire.tysam@gmail.com&#34;&gt;hire.tysam@gmail.com&lt;/a&gt;. I&#39;d love to hear from you.&lt;/p&gt; &#xA;&lt;h3&gt;Known Bugs&lt;/h3&gt; &#xA;&lt;p&gt;The Colab-specific code is commented out at the top, and the timing/performance table reprints the entire table instead of appropriately updating in-place each epoch.&lt;/p&gt; &#xA;&lt;h3&gt;Why a ConvNet Still? Why CIFAR10? Aren&#39;t Transformers the New Thing Now?&lt;/h3&gt; &#xA;&lt;p&gt;Transformers are indeed the new thing, but I personally believe that the way information condenses from a training set into a neural network will still practically always follow the same underlying set of mathematical principles. The goal for this codebase is to get training in under two (2) seconds within a year or two (2), and under one (1) seconds within 4-5 years. This should allow for some very interesting scaled experiments for different techniques on a different kind of level. I have a rough path planned down to about 2-3 seconds of training or so, all things working out as they should. It will likely get very, very difficult beyond that point.&lt;/p&gt; &#xA;&lt;p&gt;Basically -- the information gained from experimenting with a technique here should translate in some kind of a way. No need to scale up size arbitrarily when looking to suss out the basics of certain applied mathematical concepts for a problem.&lt;/p&gt; &#xA;&lt;h3&gt;Submissions&lt;/h3&gt; &#xA;&lt;p&gt;Currently, submissions to this codebase as a benchmark are closed as we figure out the level of interest, how to track disparate entries, etc. Feel free to open an issue if you have thoughts on this!&lt;/p&gt; &#xA;&lt;h4&gt;Bugs &amp;amp; Etc.&lt;/h4&gt; &#xA;&lt;p&gt;If you find a bug, open an issue! L:D If you have a success story, let me know! It helps me understand what works and doesn&#39;t more than you might expect -- if I know how this is specifically helping people, that can help me further improve as a developer, as I can keep that in mind when developing other software for people in the future. :D :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Orange-Cyberdefense/KeePwn</title>
    <updated>2023-02-02T01:44:36Z</updated>
    <id>tag:github.com,2023-02-02:/Orange-Cyberdefense/KeePwn</id>
    <link href="https://github.com/Orange-Cyberdefense/KeePwn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A python tool to automate KeePass discovery and secret extraction.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Orange-Cyberdefense/KeePwn/main/.github/images/keepwn_banner.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A python script to help red teamers discover KeePass instances and extract secrets. &lt;/p&gt; &#xA;&lt;h2&gt;Features &amp;amp; Roadmap&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;KeePwn is still in early development and not fully tested yet: please use it with caution and always try it in a lab before (legally) attacking real-life targets!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KeePass Discovery &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Accept multiple target sources (IP, range, hostname, file).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically look for KeePass global installation files via SMB C$ share.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatically look for KeePass portable + Windows store installation files via SMB C$ share.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatically check for running KeePass process through Impacket-based command execution.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Multi-thread implementation to avoid bottleneck hosts.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KeePass Trigger Abuse &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add and remove triggers from KeePass configuration file via SMB C$ share.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically poll for cleartext exports on the remote host.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Customize triggers with command line arguments.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; KeePass Cracking &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Convert KDBX to John and Hashcat compatible formats (including KDBX 4).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; KeePass DLL Injection &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate ready-to-inject shellcode to ease DLL injection, see &lt;a href=&#34;https://github.com/d3lb3/KeeFarceReborn&#34;&gt;KeeFarce Reborn&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate ready-to-use Python shellcode injector, Ã  la &lt;a href=&#34;https://github.com/naksyn/Pyramid&#34;&gt;Pyramid&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Authentication &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support LM/NT hash authentication.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Kerberos Authentication.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Miscellaneous &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Write unit tests.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make the project available on PyPI.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Orange-Cyberdefense/KeePwn&#xA;cd KeePwn&#xA;sudo python3 setup.py install&#xA;KeePwn --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you don&#39;t want to install but just run :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Orange-Cyberdefense/KeePwn&#xA;cd KeePwn&#xA;python3 -m pip install -r requirements.txt&#xA;python3 KeePwn.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Search Mode&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;search&lt;/code&gt; module is used to identify hosts that run KeePass on your target environment. It makes use of the built-in C$ share to look for default KeePass-related files locations. For the moment, it only searches for global KeePass.exe binary (in &lt;em&gt;Program Files&lt;/em&gt;) and local KeePass.config.xml (in &lt;em&gt;%APPDATA%&lt;/em&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Orange-Cyberdefense/KeePwn/main/.github/images/keepwn_search_example.png&#34; alt=&#34;&#34;&gt;While enumerating KeePass through SMB shares is quieter against antiviruses protections, it sometimes lack some information like &#34;is KeePass currently being run ?&#34; (useful if you want to extract secrets through DLL injection with &lt;a href=&#34;https://github.com/d3lb3/KeeFarceReborn&#34;&gt;KeeFarceReborn&lt;/a&gt;). I will soon implement the &lt;code&gt;--search-process&lt;/code&gt; flag that will check for live KeePass process execution through Impacket-based remote command execution.&lt;/p&gt; &#xA;&lt;h3&gt;Trigger Mode&lt;/h3&gt; &#xA;&lt;p&gt;As described in @harmj0y&#39;s &lt;a href=&#34;https://blog.harmj0y.net/redteaming/keethief-a-case-study-in-attacking-keepass-part-2/&#34;&gt;blog post (&lt;em&gt;Exfiltration Without Malware&lt;/em&gt; part)&lt;/a&gt;, KeePass trigger system can be abused in order to export the database in cleartext. KeePwn trigger modules allows to :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Check if a malicious trigger named &#34;export&#34; is currently present in KeePass configuration.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Orange-Cyberdefense/KeePwn/main/.github/images/keepwn_trigger_check_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add and remove a malicious trigger named &#34;export&#34; which performs a cleartext export of the database in %APPDATA% on next KeePass launch.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Orange-Cyberdefense/KeePwn/main/.github/images/keepwn_trigger_add_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Poll %APPDATA% for exports and automatically moves it from remote host to the current directory.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Orange-Cyberdefense/KeePwn/main/.github/images/keepwn_trigger_poll_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once again, these actions are made through SMB C$ share access, limiting antiviral detection as no command execution is performed.&lt;/p&gt; &#xA;&lt;p&gt;If no configuration file path is specified, note that KeePwn will try to find it manually by looking in default locations (shown in the &#39;check&#39; example). As KeePass trigger manipulation should always be performed with caution, this mode is limited to 1 host and 1 configuration file at a time. Feel free to let me know if you think of a use case that would need more than that (massive trigger abuse on a whole network?).&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome (see: roadmap above + TODO* in code).&lt;/p&gt; &#xA;&lt;p&gt;Feel free to open an issue or DM &lt;a href=&#34;https://twitter.com/d3lb3_&#34;&gt;@d3lb3_&lt;/a&gt; on Twitter to suggest improvement.&lt;/p&gt;</summary>
  </entry>
</feed>