<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-04T01:34:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jjleng/copilot-more</title>
    <updated>2025-03-04T01:34:12Z</updated>
    <id>tag:github.com,2025-03-04:/jjleng/copilot-more</id>
    <link href="https://github.com/jjleng/copilot-more" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT-4o and Claude-3.7-Sonnet APIs for coding.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Thanks for Your Support&lt;/h1&gt; &#xA;&lt;p&gt;Given that GitHub may enforce stricter policies, I want to minimize risks for users. As a result, I am transitioning the project to maintenance mode. The repository will remain available for reference, but complete removal is also a possibility, with no further notice. No major developments will be made. Please conduct your own due diligence and assess the risks before using this tool.&lt;/p&gt; &#xA;&lt;h1&gt;MUST READ!!!&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Constantly hitting 429 (rate limit) can get your GH Copilot subscription suspended. Knowing this is important to protect yourself. Not just copilot-more, people get banned by using LM API too because of this, see &lt;a href=&#34;https://www.reddit.com/r/RooCode/s/3VXA5FUpA5&#34;&gt;reddit thread&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It seems you can hit 429 quicker with the Claude 3.7 Sonnet models (probably smaller quotas + 3.7 runs faster than 3.5).&lt;/li&gt; &#xA; &lt;li&gt;copilot-more can now display token usage stats for your awareness. &lt;a href=&#34;https://github.com/jjleng/copilot-more/pull/41&#34;&gt;PR 41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Highly recommend leveraging the advanced rate limiter features (both token- and request-based) to prevent excessive token usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to know more about GH Copilot suspension risk, read this &lt;a href=&#34;https://github.com/RooVetGit/Roo-Code/issues/1203#issuecomment-2692865792&#34;&gt;https://github.com/RooVetGit/Roo-Code/issues/1203#issuecomment-2692865792&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;copilot-more&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;copilot-more&lt;/code&gt; maximizes the value of your GitHub Copilot subscription by exposing models like Claude-3.7-Sonnet for use in agentic coding tools such as Cline, or any tool that supports bring-your-own-model setups. Unlike costly pay-as-you-go APIs, this approach lets you leverage these powerful models affordably.&lt;/p&gt; &#xA;&lt;h2&gt;Ethical Use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Respect the GitHub Copilot terms of service.&lt;/li&gt; &#xA; &lt;li&gt;Only use the API for coding tasks.&lt;/li&gt; &#xA; &lt;li&gt;Be mindful of the risk of being banned by GitHub Copilot for misuse.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üèÉ‚Äç‚ôÇÔ∏è How to Run&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the refresh token&lt;/p&gt; &lt;p&gt;A refresh token is used to get the access token. This token should never be shared with anyone :). You can get the refresh token by following the steps below:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run the following command and note down the returned &lt;code&gt;device_code&lt;/code&gt; and &lt;code&gt;user_code&lt;/code&gt;.:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 01ab8ac9400c4e429b23 is the client_id for the VS Code&#xA;curl https://github.com/login/device/code -X POST -d &#39;client_id=01ab8ac9400c4e429b23&amp;amp;scope=user:email&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;https://github.com/login/device/&#34;&gt;https://github.com/login/device/&lt;/a&gt; and enter the &lt;code&gt;user_code&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Replace &lt;code&gt;YOUR_DEVICE_CODE&lt;/code&gt; with the &lt;code&gt;device_code&lt;/code&gt; obtained earlier and run:&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://github.com/login/oauth/access_token -X POST -d &#39;client_id=01ab8ac9400c4e429b23&amp;amp;scope=user:email&amp;amp;device_code=YOUR_DEVICE_CODE&amp;amp;grant_type=urn:ietf:params:oauth:grant-type:device_code&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note down the &lt;code&gt;access_token&lt;/code&gt; starting with &lt;code&gt;gho_&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install and run copilot_more&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Bare metal installation:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/jjleng/copilot-more.git&#xA;cd copilot-more&#xA;# install dependencies&#xA;poetry install&#xA;# run the server. Replace gho_xxxxx with the refresh token you got in the previous step. Note, you can use any port number you want.&#xA;REFRESH_TOKEN=gho_xxxxx poetry run uvicorn copilot_more.server:app --port 15432&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker Compose installation:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/jjleng/copilot-more.git&#xA;cd copilot-more&#xA;# run the server. Ensure you either have the refresh token in the .env file or pass it as an environment variable.&#xA;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Alternatively, use the &lt;code&gt;refresh-token.sh&lt;/code&gt; script to automate the above.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The application allows you to customize behavior through environment variables or a &lt;code&gt;.env&lt;/code&gt; file. Available configuration options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Setting&lt;/th&gt; &#xA;   &lt;th&gt;Environment Variable&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GitHub Refresh Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;REFRESH_TOKEN&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;None (Required)&lt;/td&gt; &#xA;   &lt;td&gt;GitHub Copilot refresh token&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Log Level&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;LOGURU_LEVEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;INFO&lt;/td&gt; &#xA;   &lt;td&gt;Sets the logging level for the application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Editor Version&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;EDITOR_VERSION&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;vscode/1.97.2&lt;/td&gt; &#xA;   &lt;td&gt;Editor version for API requests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max Tokens&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;MAX_TOKENS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;10240&lt;/td&gt; &#xA;   &lt;td&gt;Maximum tokens in responses&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Timeout&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;TIMEOUT_SECONDS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;API request timeout in seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Record Traffic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RECORD_TRAFFIC&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;Whether to record API traffic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sleep Between Calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SLEEP_BETWEEN_CALLS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;Sleep duration in seconds between API calls&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See &lt;code&gt;.env.example&lt;/code&gt; for a template configuration file. You can &lt;code&gt;cp .env.example .env&lt;/code&gt; and modify the values as needed.&lt;/p&gt; &#xA;&lt;p&gt;Once you have set up your &lt;code&gt;.env&lt;/code&gt; file with all your configuration settings, you can simply run the server without specifying environment variables on the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run uvicorn copilot_more.server:app --port 15432&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Rate Limiting Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Rate limiting is optional and only applied to models that you explicitly configure. You can define rate limits for specific models using a &lt;code&gt;rate_limits.json&lt;/code&gt; file in the project root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;claude-3.7-sonnet&#34;: [&#xA;    {&#xA;      &#34;window_minutes&#34;: 1,&#xA;      &#34;total_tokens&#34;: 50000,&#xA;      &#34;input_tokens&#34;: 50000,&#xA;      &#34;output_tokens&#34;: 5000,&#xA;      &#34;requests&#34;: 5,&#xA;      &#34;behavior&#34;: &#34;delay&#34;&#xA;    },&#xA;    {&#xA;      &#34;window_minutes&#34;: 60,&#xA;      &#34;total_tokens&#34;: 500000,&#xA;      &#34;input_tokens&#34;: 500000,&#xA;      &#34;output_tokens&#34;: 50000,&#xA;      &#34;requests&#34;: 100,&#xA;      &#34;behavior&#34;: &#34;error&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configuration options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;window_minutes&lt;/code&gt;: Time window in minutes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;total_tokens&lt;/code&gt;: Max total tokens in window (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input_tokens&lt;/code&gt;: Max input tokens in window (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_tokens&lt;/code&gt;: Max output tokens in window (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;requests&lt;/code&gt;: Max requests in window (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;behavior&lt;/code&gt;: What to do when limit is hit: &#34;delay&#34; or &#34;error&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Warning:&lt;/strong&gt; The default &lt;code&gt;rate_limits.json&lt;/code&gt; is just an example and not necessarily suitable for production use. You should adjust these limits based on your actual usage patterns.&lt;/p&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rate limits are only applied to models listed in the configuration file&lt;/li&gt; &#xA; &lt;li&gt;Models not listed in the file will have no rate limits&lt;/li&gt; &#xA; &lt;li&gt;You must specify at least one of: total_tokens, input_tokens, output_tokens, or requests&lt;/li&gt; &#xA; &lt;li&gt;Changes to rate limits require restarting the server to take effect&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Additional Rate Control&lt;/h3&gt; &#xA;&lt;p&gt;While rate limits help control usage within time windows, sometimes you may need finer control over request spacing. The &lt;code&gt;SLEEP_BETWEEN_CALLS&lt;/code&gt; setting introduces a fixed delay between API calls, which can help prevent burst requests when the API responds very quickly. This is particularly useful when:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You want to ensure a minimum time gap between requests regardless of response speed&lt;/li&gt; &#xA; &lt;li&gt;You need to prevent rapid successive requests that might trigger rate limits&lt;/li&gt; &#xA; &lt;li&gt;You want to maintain a more consistent, predictable request pattern&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example: Setting &lt;code&gt;SLEEP_BETWEEN_CALLS=1.0&lt;/code&gt; ensures at least 1 second between each API call, even if the API responds faster.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Magic Time&lt;/h2&gt; &#xA;&lt;p&gt;Now you can connect Cline or any other AI client to &lt;code&gt;http://localhost:15432&lt;/code&gt; and start coding with the power of GPT-4o and Claude-3.5-Sonnet without worrying about the cost. Note, the copilot-more manages the access token, you can use whatever string as API keys if Cline or the AI tools ask for one.&lt;/p&gt; &#xA;&lt;h3&gt;üöÄ Cline Integration&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Cline &lt;code&gt;code --install-extension saoudrizwan.claude-dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open Cline and go to the settings&lt;/li&gt; &#xA; &lt;li&gt;Set the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;API Provider&lt;/strong&gt;: &lt;code&gt;OpenAI Compatible&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;API URL&lt;/strong&gt;: &lt;code&gt;http://localhost:15432&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt;: &lt;code&gt;anything&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;code&gt;gpt-4o&lt;/code&gt;, &lt;code&gt;claude-3.7-sonnet&lt;/code&gt;, &lt;code&gt;o1&lt;/code&gt;, &lt;code&gt;o3-mini&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üîç Debugging&lt;/h2&gt; &#xA;&lt;p&gt;For troubleshooting integration issues, you can enable traffic logging to inspect the API requests and responses.&lt;/p&gt; &#xA;&lt;h3&gt;Traffic Logging&lt;/h3&gt; &#xA;&lt;p&gt;To enable logging, set the &lt;code&gt;RECORD_TRAFFIC&lt;/code&gt; environment variable to &lt;code&gt;true&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;RECORD_TRAFFIC=true REFRESH_TOKEN=gho_xxxx poetry run uvicorn copilot_more.server:app --port 15432&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can add &lt;code&gt;RECORD_TRAFFIC=true&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;All traffic will be logged to files in the current directory with the naming pattern: copilot_traffic_YYYYMMDD_HHMMSS.mitm&lt;/p&gt; &#xA;&lt;p&gt;Attach this file when reporting issues. Please zip the original file that ends with the &#39;.mitm&#39; extension and upload to the GH issues.&lt;/p&gt; &#xA;&lt;p&gt;Note: the Authorization header has been redacted, so the refresh token won&#39;t be leaked.&lt;/p&gt; &#xA;&lt;h2&gt;ü§î Limitation&lt;/h2&gt; &#xA;&lt;p&gt;The GH Copilot models sit behind an API server that is not fully compatible with the OpenAI API. You cannot pass in a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: [&#xA;        {&#xA;          &#34;type&#34;: &#34;text&#34;,&#xA;          &#34;text&#34;: &#34;&amp;lt;task&amp;gt;\nreview the code\n&amp;lt;/task&amp;gt;&#34;&#xA;        },&#xA;        {&#xA;          &#34;type&#34;: &#34;text&#34;,&#xA;          &#34;text&#34;: &#34;&amp;lt;task&amp;gt;\nreview the code carefully\n&amp;lt;/task&amp;gt;&#34;&#xA;        }&#xA;      ]&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;copilot-more takes care of this limitation by converting the message to a format that the GH Copilot API understands. However, without the &lt;code&gt;type&lt;/code&gt;, we cannot leverage the models&#39; vision capabilities, so that you cannot do screenshot analysis.&lt;/p&gt;</summary>
  </entry>
</feed>