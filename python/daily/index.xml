<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-11T01:43:41Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qwopqwop200/GPTQ-for-LLaMa</title>
    <updated>2023-03-11T01:43:41Z</updated>
    <id>tag:github.com,2023-03-11:/qwopqwop200/GPTQ-for-LLaMa</id>
    <link href="https://github.com/qwopqwop200/GPTQ-for-LLaMa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;4 bits quantization of LLaMa using GPTQ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPTQ-for-LLaMa&lt;/h1&gt; &#xA;&lt;p&gt;4 bits quantization of &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa&lt;/a&gt; using &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPTQ is SOTA one-shot weight quantization method&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This code is based on &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GPTQ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Result&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model(&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt;)&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;group-size&lt;/th&gt; &#xA;   &lt;th&gt;Wikitext2&lt;/th&gt; &#xA;   &lt;th&gt;PTB&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.67&lt;/td&gt; &#xA;   &lt;td&gt;8.79&lt;/td&gt; &#xA;   &lt;td&gt;7.05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTN&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;6.28&lt;/td&gt; &#xA;   &lt;td&gt;9.68&lt;/td&gt; &#xA;   &lt;td&gt;7.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;6.79&lt;/td&gt; &#xA;   &lt;td&gt;10.67&lt;/td&gt; &#xA;   &lt;td&gt;8.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.16&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9.66&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.52&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTN&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;25.66&lt;/td&gt; &#xA;   &lt;td&gt;61.25&lt;/td&gt; &#xA;   &lt;td&gt;28.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;20.86&lt;/td&gt; &#xA;   &lt;td&gt;37.54&lt;/td&gt; &#xA;   &lt;td&gt;22.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;12.24&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;16.77&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9.55&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model(&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt;)&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;group-size&lt;/th&gt; &#xA;   &lt;th&gt;Wikitext2&lt;/th&gt; &#xA;   &lt;th&gt;PTB&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.08&lt;/td&gt; &#xA;   &lt;td&gt;8.06&lt;/td&gt; &#xA;   &lt;td&gt;6.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTN&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.52&lt;/td&gt; &#xA;   &lt;td&gt;8.62&lt;/td&gt; &#xA;   &lt;td&gt;6.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.35&lt;/td&gt; &#xA;   &lt;td&gt;8.40&lt;/td&gt; &#xA;   &lt;td&gt;6.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.18&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.18&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.66&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTN&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;11.41&lt;/td&gt; &#xA;   &lt;td&gt;21.21&lt;/td&gt; &#xA;   &lt;td&gt;13.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;6.80&lt;/td&gt; &#xA;   &lt;td&gt;10.45&lt;/td&gt; &#xA;   &lt;td&gt;8.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.00&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Quantizing the model requires a large amount of CPU memory. For example, quantizing a LLaMa-13b model requires 42gb, and LLaMa-33b requires more memory than 64gb.&lt;/p&gt; &#xA;&lt;p&gt;Depending on the GPUs/drivers, there may be a difference in performance, which decreases as the model size increases.(&lt;a href=&#34;https://github.com/IST-DASLab/gptq/issues/1&#34;&gt;https://github.com/IST-DASLab/gptq/issues/1&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ paper&lt;/a&gt;, As the size of the model increases, the difference in performance between FP16 and GPTQ decreases.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;torch&lt;/code&gt;: tested on v1.12.1+cu113&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;: &lt;a href=&#34;https://github.com/zphang/transformers/tree/llama_push&#34;&gt;tested on v4.27.0.dev0(required)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt;: tested on v2.10.1&lt;/li&gt; &#xA; &lt;li&gt;(to run 4-bit kernels: setup for compiling PyTorch CUDA extensions, see also &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;https://pytorch.org/tutorials/advanced/cpp_extension.html&lt;/a&gt;, tested on CUDA 11.3)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All experiments were run on a single NVIDIA RTX3090.&lt;/p&gt; &#xA;&lt;h2&gt;Language Generation&lt;/h2&gt; &#xA;&lt;h3&gt;LLaMa&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Compute full precision (FP16) results&#xA;CUDA_VISIBLE_DEVICES=0 python llama.py decapoda-research/llama-7b-hf c4&#xA;# Run RTN baseline and compute results&#xA;CUDA_VISIBLE_DEVICES=0 python llama.py decapoda-research/llama-7b-hf c4 --wbits 4 --nearest&#xA;# Run GPTQ and compute results&#xA;CUDA_VISIBLE_DEVICES=0 python llama.py decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run other LLaMa models replace &lt;code&gt;llama-7b-hf&lt;/code&gt; with one of: &lt;code&gt;llama-13b-hf&lt;/code&gt;, &lt;code&gt;llama-30b-hf&lt;/code&gt;, &lt;code&gt;llama-65b-hf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ZeroShot&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;code&gt;zeroShot/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;CUDA Kernels&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Install kernels&#xA;python setup_cuda.py install&#xA;&#xA;# Benchmark performance for FC2 layer of LLaMa-7B&#xA;CUDA_VISIBLE_DEVICES=0 python test_kernel.py&#xA;&#xA;# Benchmark language generation with 4-bit LLaMa-7B:&#xA;&#xA;# Save compressed model&#xA;CUDA_VISIBLE_DEVICES=0 python llama.py decapoda-research/llama-7b-hf c4 --wbits 4 --save llama7b-4bit.pt&#xA;# Benchmark generating a 2048 token sequence with the saved model&#xA;CUDA_VISIBLE_DEVICES=0 python llama.py decapoda-research/llama-7b-hf c4 --wbits 4 --load llama7b-4bit.pt --benchmark 2048 --check&#xA;# Benchmark FP16 baseline, note that the model will be split across all listed GPUs&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3,4 python llama.py decapoda-research/llama-7b-hf c4 --benchmark 2048 --check&#xA;&#xA;# model inference with the saved model&#xA;CUDA_VISIBLE_DEVICES=0 python llama_inference.py decapoda-research/llama-7b-hf --wbits 4 --load llama7b-4bit.pt --text &#34;this is llama&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA Kernels support 2,3,4,8 bits.&lt;/p&gt; &#xA;&lt;p&gt;Basically, 4-bit quantization is recommended.&lt;/p&gt; &#xA;&lt;p&gt;cuda kernel does not support group size.&lt;/p&gt; &#xA;&lt;h2&gt;Memory Usage&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;memory(MiB)&lt;/th&gt; &#xA;   &lt;th&gt;benchmark(ppl)&lt;/th&gt; &#xA;   &lt;th&gt;Wikitext2&lt;/th&gt; &#xA;   &lt;th&gt;PTB&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint size(GB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt; with FP16&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;13940&lt;/td&gt; &#xA;   &lt;td&gt;5.23&lt;/td&gt; &#xA;   &lt;td&gt;5.67&lt;/td&gt; &#xA;   &lt;td&gt;8.79&lt;/td&gt; &#xA;   &lt;td&gt;7.05&lt;/td&gt; &#xA;   &lt;td&gt;12.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt; with FP16&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.08&lt;/td&gt; &#xA;   &lt;td&gt;8.06&lt;/td&gt; &#xA;   &lt;td&gt;6.58&lt;/td&gt; &#xA;   &lt;td&gt;24.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;7748&lt;/td&gt; &#xA;   &lt;td&gt;5.39&lt;/td&gt; &#xA;   &lt;td&gt;5.67&lt;/td&gt; &#xA;   &lt;td&gt;8.81&lt;/td&gt; &#xA;   &lt;td&gt;7.08&lt;/td&gt; &#xA;   &lt;td&gt;6.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;14570&lt;/td&gt; &#xA;   &lt;td&gt;5.00&lt;/td&gt; &#xA;   &lt;td&gt;5.09&lt;/td&gt; &#xA;   &lt;td&gt;8.06&lt;/td&gt; &#xA;   &lt;td&gt;6.61&lt;/td&gt; &#xA;   &lt;td&gt;12.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;4740&lt;/td&gt; &#xA;   &lt;td&gt;6.23&lt;/td&gt; &#xA;   &lt;td&gt;6.79&lt;/td&gt; &#xA;   &lt;td&gt;10.67&lt;/td&gt; &#xA;   &lt;td&gt;8.28&lt;/td&gt; &#xA;   &lt;td&gt;3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;8410&lt;/td&gt; &#xA;   &lt;td&gt;5.14&lt;/td&gt; &#xA;   &lt;td&gt;5.35&lt;/td&gt; &#xA;   &lt;td&gt;8.40&lt;/td&gt; &#xA;   &lt;td&gt;6.82&lt;/td&gt; &#xA;   &lt;td&gt;6.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;3852&lt;/td&gt; &#xA;   &lt;td&gt;11.43&lt;/td&gt; &#xA;   &lt;td&gt;17.94&lt;/td&gt; &#xA;   &lt;td&gt;31.44&lt;/td&gt; &#xA;   &lt;td&gt;19.65&lt;/td&gt; &#xA;   &lt;td&gt;2.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;6870&lt;/td&gt; &#xA;   &lt;td&gt;5.58&lt;/td&gt; &#xA;   &lt;td&gt;6.77&lt;/td&gt; &#xA;   &lt;td&gt;10.29&lt;/td&gt; &#xA;   &lt;td&gt;8.34&lt;/td&gt; &#xA;   &lt;td&gt;5.06&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-7B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;3076&lt;/td&gt; &#xA;   &lt;td&gt;4152&lt;/td&gt; &#xA;   &lt;td&gt;30749&lt;/td&gt; &#xA;   &lt;td&gt;45936&lt;/td&gt; &#xA;   &lt;td&gt;5045&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-13B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;5275&lt;/td&gt; &#xA;   &lt;td&gt;6903&lt;/td&gt; &#xA;   &lt;td&gt;13203&lt;/td&gt; &#xA;   &lt;td&gt;1384&lt;/td&gt; &#xA;   &lt;td&gt;8.34&lt;/td&gt; &#xA;   &lt;td&gt;5.06&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa-33B&lt;/a&gt; with &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;19499&lt;/td&gt; &#xA;   &lt;td&gt;4.59&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;15.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;This code is based on &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GPTQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks to Meta AI for releasing &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMa&lt;/a&gt;, a powerful LLM.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KohakuBlueleaf/LyCORIS</title>
    <updated>2023-03-11T01:43:41Z</updated>
    <id>tag:github.com,2023-03-11:/KohakuBlueleaf/LyCORIS</id>
    <link href="https://github.com/KohakuBlueleaf/LyCORIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LyCORIS - Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion.&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/59680068/224026402-7b779d58-5164-4ecd-a807-f98badae589e.png&#34; alt=&#34;image&#34;&gt; (This image is generated by the model trained in Hadamard product representation)&lt;/p&gt; &#xA;&lt;p&gt;A project for implementing different algorithm to do parameter-efficient finetuning on stable diffusion or more.&lt;/p&gt; &#xA;&lt;p&gt;This project is started from LoCon(see archive branch).&lt;/p&gt; &#xA;&lt;h2&gt;What we have now&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/KohakuBlueleaf/LyCORIS/raw/main/Algo.md&#34;&gt;Algo.md&lt;/a&gt; or &lt;a href=&#34;https://github.com/KohakuBlueleaf/LyCORIS/raw/main/Demo.md&#34;&gt;Demo.md&lt;/a&gt; for more example and explanation&lt;/p&gt; &#xA;&lt;h3&gt;Conventional LoRA&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Include Conv layer implementation from LoCon&lt;/li&gt; &#xA; &lt;li&gt;recommended settings &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;dim &amp;lt;= 64&lt;/li&gt; &#xA;   &lt;li&gt;alpha = 1 (or lower, like 0.3)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LoRA with Hadamard Product representation (LoHa)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ref: &lt;a href=&#34;https://openreview.net/pdf?id=d71n4ftoCBy&#34;&gt;FedPara Low-Rank Hadamard Product For Communication-Efficient Federated Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;designed for federated learning, but has some cool property like rank&amp;lt;=dim^2 so should be good for parameter-efficient finetuning. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Conventional LoRA is rank&amp;lt;=dim&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;recommended settings &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;dim &amp;lt;= 32&lt;/li&gt; &#xA;   &lt;li&gt;alpha = 1 (or lower)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING: You are not supposed to use dim&amp;gt;64 in LoHa, which is over sqrt(original_dim) for almost all layer in SD&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High dim with LoHa may cause unstable loss or just goes to NaN. If you want to use high dim LoHa, please use lower lr&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING-AGAIN: Use parameter-efficient algorithim in parameter-unefficient way is not a good idea&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;usage&lt;/h2&gt; &#xA;&lt;h3&gt;For kohya script&lt;/h3&gt; &#xA;&lt;p&gt;Activate sd-scripts&#39; venv and then install this package&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source PATH_TO_SDSCRIPTS_VENV/Scripts/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PATH_TO_SDSCRIPTS_VENV\Scripts\Activate.ps1 # or .bat for cmd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then you can install this package:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;through pip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lycoris_lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;from source&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/KohakuBlueleaf/LyCORIS&#xA;cd LyCORIS&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally you can use this package&#39;s kohya module to run kohya&#39;s training script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 sd-scripts/train_network.py \&#xA;  --network_module lycoris.kohya \&#xA;  --network_dim &#34;DIM_FOR_LINEAR&#34; --network_alpha &#34;ALPHA_FOR_LINEAR&#34;\&#xA;  --network_args &#34;conv_dim=DIM_FOR_CONV&#34; &#34;conv_alpha=ALPHA_FOR_CONV&#34; \&#xA;  &#34;dropout=DROPOUT_RATE&#34; &#34;algo=lora&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to train lycoris module for SD model&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;algo list:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;lora: Conventional Methods&lt;/li&gt; &#xA;   &lt;li&gt;loha: Hadamard product representation introduced by FedPara&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Tips:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use network_dim=0 or conv_dim=0 to disable linear/conv layer&lt;/li&gt; &#xA;   &lt;li&gt;LoHa doesn&#39;t support dropout yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;For a1111&#39;s sd-webui&lt;/h3&gt; &#xA;&lt;p&gt;download &lt;a href=&#34;https://github.com/KohakuBlueleaf/a1111-sd-webui-locon&#34;&gt;Extension&lt;/a&gt; into sd-webui, and then use your model as how you use lora model. &lt;strong&gt;LoHa Model supported&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Additional Networks&lt;/h3&gt; &#xA;&lt;p&gt;Once you install the extension. You can also use your model in &lt;a href=&#34;https://github.com/kohya-ss/sd-webui-additional-networks/releases&#34;&gt;addnet&lt;/a&gt;&lt;br&gt; just use it as LoRA model. &lt;strong&gt;LoHa Model not supported yet&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extract LoCon&lt;/h3&gt; &#xA;&lt;p&gt;You can extract LoCon from a dreambooth model with its base model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 extract_locon.py &amp;lt;settings&amp;gt; &amp;lt;base_model&amp;gt; &amp;lt;db_model&amp;gt; &amp;lt;output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use --help to get more info&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 extract_locon.py --help&#xA;usage: extract_locon.py [-h] [--is_v2] [--device DEVICE] [--mode MODE] [--safetensors] [--linear_dim LINEAR_DIM] [--conv_dim CONV_DIM]&#xA;                        [--linear_threshold LINEAR_THRESHOLD] [--conv_threshold CONV_THRESHOLD] [--linear_ratio LINEAR_RATIO] [--conv_ratio CONV_RATIO]&#xA;                        [--linear_percentile LINEAR_PERCENTILE] [--conv_percentile CONV_PERCENTILE]&#xA;                        base_model db_model output_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example and Comparing for different algo&lt;/h2&gt; &#xA;&lt;p&gt;see &lt;a href=&#34;https://github.com/KohakuBlueleaf/LyCORIS/raw/lycoris/Demo.md&#34;&gt;Demo.md&lt;/a&gt; and &lt;a href=&#34;https://github.com/KohakuBlueleaf/LyCORIS/raw/lycoris/Algo.md&#34;&gt;Algo.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Todo list&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Module and Document for using LyCORIS in any other model, Not only SD.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Proposition3 in &lt;a href=&#34;https://arxiv.org/abs/2108.06098&#34;&gt;FedPara&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;also need custom backward to save the vram&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Low rank + sparse representation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more operation, not only linear and conv2d.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Configure varying ranks or dimensions for specific modules as needed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatically selecting an algorithm based on the specific rank requirement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Explore other low-rank representations or parameter-efficient methods to fine-tune either the entire model or specific parts of it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More experiments for different task, not only diffusion models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{LyCORIS,&#xA;  author       = &#34;Shih-Ying Yeh (Kohaku-BlueLeaf), Yu-Guan Hsieh, Zhidong Gao&#34;,&#xA;  title        = &#34;LyCORIS - Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion&#34;,&#xA;  howpublished = &#34;\url{https://github.com/KohakuBlueleaf/LyCORIS}&#34;,&#xA;  month        = &#34;March&#34;,&#xA;  year         = &#34;2023&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>guillaumekln/faster-whisper</title>
    <updated>2023-03-11T01:43:41Z</updated>
    <id>tag:github.com,2023-03-11:/guillaumekln/faster-whisper</id>
    <link href="https://github.com/guillaumekln/faster-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; &#xA;&lt;p&gt;This repository demonstrates how to implement the Whisper transcription using &lt;a href=&#34;https://github.com/OpenNMT/CTranslate2/&#34;&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; &#xA;&lt;p&gt;This implementation is up to 4 times faster than &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;For reference, here&#39;s the time and memory usage that are required to transcribe &lt;strong&gt;13 minutes&lt;/strong&gt; of audio using different implementations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/openai/whisper/commit/7858aa9c08d98f75575035ecd6481f462d66ca27&#34;&gt;7858aa9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;@&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/commit/3b010f9bed9a6068609e9faf52383aea792b0362&#34;&gt;3b010f9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper&#34;&gt;faster-whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/commit/cda834c8ea76c2cab9da19031815c1e937a88c7f&#34;&gt;cda834c&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Large model on GPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Max. CPU memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;4m30s&lt;/td&gt; &#xA;   &lt;td&gt;11413MB&lt;/td&gt; &#xA;   &lt;td&gt;9553MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;1m02s&lt;/td&gt; &#xA;   &lt;td&gt;4659MB&lt;/td&gt; &#xA;   &lt;td&gt;3244MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Small model on CPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;10m39s&lt;/td&gt; &#xA;   &lt;td&gt;2850MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;17m42s&lt;/td&gt; &#xA;   &lt;td&gt;1581MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;12m39s&lt;/td&gt; &#xA;   &lt;td&gt;873MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m53s&lt;/td&gt; &#xA;   &lt;td&gt;1482MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m01s&lt;/td&gt; &#xA;   &lt;td&gt;1008MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .[conversion]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model conversion requires the modules &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; which are installed by the &lt;code&gt;[conversion]&lt;/code&gt; requirement. Once a model is converted, these modules are no longer needed and the installation could be simplified to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also possible to install the module without cloning the Git repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install the master branch:&#xA;pip install &#34;faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/refs/heads/master.tar.gz&#34;&#xA;&#xA;# Install a specific commit:&#xA;pip install &#34;faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPU support&lt;/h3&gt; &#xA;&lt;p&gt;GPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the &lt;a href=&#34;https://opennmt.net/CTranslate2/installation.html&#34;&gt;CTranslate2 documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Model conversion&lt;/h3&gt; &#xA;&lt;p&gt;A Whisper model should be first converted into the CTranslate2 format. We provide a script to download and convert models from the &lt;a href=&#34;https://huggingface.co/models?sort=downloads&amp;amp;search=whisper&#34;&gt;Hugging Face model repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example the command below converts the &#34;large-v2&#34; Whisper model and saves the weights in FP16:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ct2-transformers-converter --model openai/whisper-large-v2 --output_dir whisper-large-v2-ct2 \&#xA;    --copy_files tokenizer.json --quantization float16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/p&gt; &#xA;&lt;p&gt;Models can also be converted from the code. See the &lt;a href=&#34;https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html&#34;&gt;conversion API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Transcription&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faster_whisper import WhisperModel&#xA;&#xA;model_path = &#34;whisper-large-v2-ct2/&#34;&#xA;&#xA;# Run on GPU with FP16&#xA;model = WhisperModel(model_path, device=&#34;cuda&#34;, compute_type=&#34;float16&#34;)&#xA;&#xA;# or run on GPU with INT8&#xA;# model = WhisperModel(model_path, device=&#34;cuda&#34;, compute_type=&#34;int8_float16&#34;)&#xA;# or run on CPU with INT8&#xA;# model = WhisperModel(model_path, device=&#34;cpu&#34;, compute_type=&#34;int8&#34;)&#xA;&#xA;segments, info = model.transcribe(&#34;audio.mp3&#34;, beam_size=5)&#xA;&#xA;print(&#34;Detected language &#39;%s&#39; with probability %f&#34; % (info.language, info.language_probability))&#xA;&#xA;for segment in segments:&#xA;    print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (segment.start, segment.end, segment.text))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See more model and transcription options in the &lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/raw/master/faster_whisper/transcribe.py&#34;&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; &#xA;&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; &#xA; &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OMP_NUM_THREADS=4 python3 my_script.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>