<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-06T01:43:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RUB-SysSec/DroneSecurity</title>
    <updated>2023-03-06T01:43:49Z</updated>
    <id>tag:github.com,2023-03-06:/RUB-SysSec/DroneSecurity</id>
    <link href="https://github.com/RUB-SysSec/DroneSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DroneSecurity (NDSS 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Drone-ID Receiver for DJI OcuSync 2.0&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f217_paper.pdf&#34;&gt;&lt;img alt=&#34;Paper thumbnail&#34; align=&#34;right&#34; width=&#34;250&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/paper_thumbnail.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is a receiver for DJI&#39;s Drone-ID protocol. The receiver works either live with an SDR, or offline on pre-recorded captures.&lt;/p&gt; &#xA;&lt;p&gt;Our paper from NDSS&#39;23 explains the protocol and receiver design: &lt;a href=&#34;https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f217_paper.pdf&#34;&gt;Drone Security and the Mysterious Case of DJI&#39;s DroneID&lt;/a&gt; [pdf]&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re looking for the fuzzer, we will upload that shortly :)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The live receiver was tested with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ettus USRP B205-mini&lt;/li&gt; &#xA; &lt;li&gt;DJI mini 2, DJI Mavic Air 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our software is a proof-of-concept receiver that we used to reverse-engineer an unknown protocol. Hence, it is not optimized for bad RF conditions, performance, or range.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Decoded Payload&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/result.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sample Files&lt;/h2&gt; &#xA;&lt;p&gt;We provide sample files in the &lt;code&gt;samples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;The samples were directly dumped from the first stage of the live receiver that &lt;em&gt;detects&lt;/em&gt; candidate frames and performs no other data processing; it usually hands them directly to the rest of the code that you can test offline.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;inspectrum&lt;/code&gt; to visualize the raw sample file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install inspectrum&#xA;inspectrum -r 50e6 samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Inspectrum screenshot of Drone-ID bursts&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/inspectrum.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start (Offline)&lt;/h2&gt; &#xA;&lt;p&gt;Create a virtual environment for Python and install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip3 install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now run the decoder on the sample file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;The script performs detection and decoding just as the live receiver would. It prints the decoded payload for each Drone-ID frame:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;## Drone-ID Payload ##&#xA;{&#xA;    &#34;pkt_len&#34;: 88,&#xA;    &#34;unk&#34;: 16,&#xA;    &#34;version&#34;: 2,&#xA;    &#34;sequence_number&#34;: 878,&#xA;    &#34;state_info&#34;: 8179,&#xA;    &#34;serial_number&#34;: &#34;SecureStorage?&#34;,&#xA;    &#34;longitude&#34;: 7.267960786785307,&#xA;    &#34;latitude&#34;: 51.446866781640146,&#xA;    &#34;altitude&#34;: 39.32,&#xA;    &#34;height&#34;: 5.49,&#xA;    &#34;v_north&#34;: 0,&#xA;    &#34;v_east&#34;: -7,&#xA;    &#34;v_up&#34;: 0,&#xA;    &#34;d_1_angle&#34;: 16900,&#xA;    &#34;gps_time&#34;: 1650894901980,&#xA;    &#34;app_lat&#34;: 43.26826445428658,&#xA;    &#34;app_lon&#34;: 6.640125363111847,&#xA;    &#34;longitude_home&#34;: 7.26794359805882,&#xA;    &#34;latitude_home&#34;: 51.446883970366635,&#xA;    &#34;device_type&#34;: &#34;Mini 2&#34;,&#xA;    &#34;uuid_len&#34;: 0,&#xA;    &#34;uuid&#34;: &#34;&#34;,&#xA;    &#34;crc-packet&#34;: &#34;c935&#34;,&#xA;    &#34;crc-calculated&#34;: &#34;c935&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The summary contains decoding stats and flight path. In the &lt;code&gt;mini2_sm&lt;/code&gt; sample, the drone did not have GPS coordinates locked yet, and only the smartphone&#39;s location is transmitted:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;… … …&#xA;Frame detection: 10 candidates&#xA;Decoder: 9 total, CRC OK: 7 (2 CRC errors)&#xA;Drone Coordinates:&#xA;App Coordinates:&#xA;(51.447176178716916, 7.266528392911369)&#xA;(51.447176178716916, 7.266528392911369)&#xA;…&#xA;(51.447176178716916, 7.266528392911369)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;samples/mavic_air_2&lt;/code&gt; both locations are transmitted:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./src/droneid_receiver_offline.py -i samples/mavic_air_2&#xA;…&#xA;Decoder: 1 total, CRC OK: 1 (0 CRC errors)&#xA;Drone Coordinates:&#xA;(51.44633393111904, 7.26721594197086, 12.8)&#xA;App Coordinates:&#xA;(51.44620788045814, 7.267101350460944)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Live Receiver&lt;/h1&gt; &#xA;&lt;p&gt;The live receiver additionally requires the UHD driver and &lt;strong&gt;quite powerful machines&lt;/strong&gt; (for captures at 50 MHz bandwidth).&lt;/p&gt; &#xA;&lt;p&gt;Environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ettus USRP B205-mini&lt;/li&gt; &#xA; &lt;li&gt;DJI mini 2, DJI Mavic Air 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First, setup the Python environment. Due to the UHD driver, this does not work with a virtual environment. If you previously activated a virtual environment, exit that environment first. Install Python requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install UHD:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install libuhd-dev uhd-host python3-uhd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the receiver:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./src/droneid_receiver_live.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The receiver will hop through a list of frequencies and, if a drone is detected, lock on that band.&lt;/p&gt; &#xA;&lt;h2&gt;Deeper Dive: Script output&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Processing Pipeline&#34; align=&#34;right&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/pipeline.png&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re looking for a deeper dive into the processing steps, we suggest calling the offline decoder with &lt;code&gt;--debug&lt;/code&gt;. This will &lt;strong&gt;enable a GUI&lt;/strong&gt; with step-by-step decoding.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, the &lt;code&gt;SpectrumCapture&lt;/code&gt; class performs &lt;em&gt;packet detection&lt;/em&gt; and splits the capture file into individual frames:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Packet #0, start 0.000076, end 0.000721, length 0.000644, cfo -12207.031250&#xA;Packet #1, start 0.000811, end 0.001456, length 0.000644, cfo 0.000000&#xA;Packet #2, start 0.001546, end 0.002191, length 0.000644, cfo 0.000000&#xA;…&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of these packets are false-positives and we do not expect successful decoding. Start and end are in seconds, so you can use inspectrum to take a look at individual frames.&lt;/p&gt; &#xA;&lt;p&gt;Next, the &lt;code&gt;Packet&lt;/code&gt; class detects the Zadoff-Chu sequences and performs time and frequency offset corrections. It splits the frames into individual OFDM symbols.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FFO: -6546.528614&#xA;Found ZC sequences: 600 147&#xA;ZC Offset: -2.867868&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;Decoder&lt;/code&gt; class gets the OFDM symbols and demodulates the subcarriers using QPSK. We do not know the QPSK orientation here, hence, we simply brute-force the orientation. &lt;code&gt;decoder.magic()&lt;/code&gt; performs the descrambling and turbo-decode.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;DroneIDPacket&lt;/code&gt; unpacks the resulting bitstream into the Drone-ID struct. At this point the message could be decoded, but might be corrupted (CRC check needed).&lt;/p&gt; &#xA;&lt;p&gt;CRC check FAIL is easy to spot by looking at the Serial Number (should read &#39;SecureStorage?&#39;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    &#34;serial_number&#34;: &#34;Sa#upeStore&amp;amp;q?\u0010\b&#34;,&#xA;    …&#xA;    &#34;crc-packet&#34;: &#34;d985&#34;,&#xA;    &#34;crc-calculated&#34;: &#34;9b01&#34;&#xA;}&#xA;CRC Check FAILED!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At the very end, we print some statistics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Successfully decoded 14 / 34 packets&#xA;4 Packets with CRC error&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So in total we decoded 18 packets, 14 with correct CRC. Again, this is &lt;em&gt;expected&lt;/em&gt; as the sample file includes Drone-ID Frames with greatly varying quality.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ - Frequently Asked Questions&lt;/h1&gt; &#xA;&lt;p&gt;Is DJI&#39;s Drone-ID the same as the standardized, Bluetooth or WiFi-based &#34;Remote ID&#34;?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;No. DJI uses a dedicated wireless protocol for its Drone-ID, hence the need to implement an receiver. Check &lt;a href=&#34;https://www.opendroneid.org&#34;&gt;www.opendroneid.org&lt;/a&gt; for an in-depth description of the EU/US-wide standard.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Can I use &lt;em&gt;this software&lt;/em&gt; to locate drones from other manufacturers?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;No. This software decodes DJI-specific protocols. It does not work with WiFi or Bluetooth-based &#34;Remote ID&#34;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Can I locate drones without this software?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Maybe. Since mid-2022, the US or EU started requiring drone manufacturers to implement &#34;Drone Remote ID&#34; - an international standard that works on top of WiFi or Bluetooth. You can use a smartphone app to locate drones that support the standard.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Citing the Paper&lt;/h1&gt; &#xA;&lt;p&gt;If you would like to cite our work, use the following BibTex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{schiller2023drone,&#xA;  title={Drone Security and the Mysterious Case of DJI&#39;s DroneID},&#xA;  author={Schiller, Nico and Chlosta, Merlin and Schloegel, Moritz and Bars, Nils and Eisenhofer, Thorsten and Scharnowski, Tobias and Domke, Felix and Sch{\&#34;o}nherr, Lea and Holz, Thorsten},&#xA;  booktitle={Network and Distributed System Security Symposium (NDSS)},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>KoboldAI/KoboldAI-Client</title>
    <updated>2023-03-06T01:43:49Z</updated>
    <id>tag:github.com,2023-03-06:/KoboldAI/KoboldAI-Client</id>
    <link href="https://github.com/KoboldAI/KoboldAI-Client" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;KoboldAI - Your gateway to GPT writing&lt;/h2&gt; &#xA;&lt;p&gt;This is a browser-based front-end for AI-assisted writing with multiple local &amp;amp; remote AI models. It offers the standard array of tools, including Memory, Author&#39;s Note, World Info, Save &amp;amp; Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed.&lt;/p&gt; &#xA;&lt;h2&gt;Multiple ways to play&lt;/h2&gt; &#xA;&lt;p&gt;Stories can be played like a Novel, a text adventure game or used as a chatbot with an easy toggles to change between the multiple gameplay styles. This makes KoboldAI both a writing assistant, a game and a platform for so much more. The way you play and how good the AI will be depends on the model or service you decide to use. No matter if you want to use the free, fast power of Google Colab, your own high end graphics card, an online service you have an API key for (Like OpenAI or Inferkit) or if you rather just run it slower on your CPU you will be able to find a way to use KoboldAI that works for you.&lt;/p&gt; &#xA;&lt;h3&gt;Adventure mode&lt;/h3&gt; &#xA;&lt;p&gt;By default KoboldAI will run in a generic mode optimized for writing, but with the right model you can play this like AI Dungeon without any issues. You can enable this in the settings and bring your own prompt, try generating a random prompt or download one of the prompts available at &lt;a href=&#34;https://aetherroom.club/&#34;&gt;/aids/ Prompts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The gameplay will be slightly different than the gameplay in AI Dungeon because we adopted the Type of the Unleashed fork, giving you full control over all the characters because we do not automatically adapt your sentences behind the scenes. This means you can more reliably control characters that are not you.&lt;/p&gt; &#xA;&lt;p&gt;As a result of this what you need to type is slightly different, in AI Dungeon you would type &lt;em&gt;&lt;strong&gt;take the sword&lt;/strong&gt;&lt;/em&gt; while in KoboldAI you would type it like a sentence such as &lt;em&gt;&lt;strong&gt;You take the sword&lt;/strong&gt;&lt;/em&gt; and this is best done with the word You instead of I.&lt;/p&gt; &#xA;&lt;p&gt;To speak simply type : &lt;em&gt;You say &#34;We should probably gather some supplies first&#34;&lt;/em&gt;&lt;br&gt; Just typing the quote might work, but the AI is at its best when you specify who does what in your commands.&lt;/p&gt; &#xA;&lt;p&gt;If you want to do this with your friends we advise using the main character as You and using the other characters by their name if you are playing on a model trained for Adventures. These models assume there is a You in the story. This mode does usually not perform well on Novel models because they do not know how to handle the input those are best used with regular story writing where you take turns with the AI.&lt;/p&gt; &#xA;&lt;h3&gt;Writing assistant&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use KoboldAI as a writing assistant this is best done in the regular mode with a model optimized for Novels. These models do not make the assumption that there is a You character and focus on Novel like writing. For writing these will often give you better results than Adventure or Generic models. That said, if you give it a good introduction to the story large generic models like 13B can be used if a more specific model is not available for what you wish to write. You can also try to use models that are not specific to what you wish to do, for example a NSFW Novel model for a SFW story if a SFW model is unavailable. This will mean you will have to correct the model more often because of its bias, but can still produce good enough results if it is familiar enough with your topic.&lt;/p&gt; &#xA;&lt;h3&gt;Chatbot Mode&lt;/h3&gt; &#xA;&lt;p&gt;In chatbot mode you can use a suitable model as a chatbot, this mode automatically adds your name to the beginning of the sentences and prevents the AI from talking as you. To use it properly you must write your story opening as both characters in the following format (You can use your own text) :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;Bot : Hey!&#xA;You : Hey Boyname, how have you been?&#xA;Bot : Been good! How about you?&#xA;You : Been great to, excited to try out KoboldAI&#xA;Bot : KoboldAI is really fun!&#xA;You : For sure! What is your favorite game?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Its recommended to have your own input be the last input, especially in the beginning its possible that the AI mixes up the names. In that case either retry or manually correct the name. This behavior improves as the chat progresses. Some models may swap names if they are more familiar with a different name that is similar to the name you defined for the bot. In that case you can either do the occasional manual correction or choose a name for your chatbot that the AI likes better.&lt;/p&gt; &#xA;&lt;p&gt;This mode works the best on either a Generic model or a chatbot model specifically designed for it, some models like the AvrilAI model are instead designed to be used in Adventure mode and do not conform to the format above. These models typically ship with adventure mode enabled by default and should not be switched over to chatbot mode.&lt;/p&gt; &#xA;&lt;p&gt;Novel or Adventure models are not recommended for this feature but might still work but can derail away from the conversation format quickly.&lt;/p&gt; &#xA;&lt;h2&gt;Play KoboldAI online for free on Google Colab (The easiest way to play)&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to play KoboldAI online for free on a powerful computer you can use Google Colaboraty. We provide two editions, a TPU and a GPU edition with a variety of models available. These run entirely on Google&#39;s Servers and will automatically upload saves to your Google Drive if you choose to save a story (Alternatively, you can choose to download your save instead so that it never gets stored on Google Drive). Detailed instructions on how to use them are at the bottom of the Colab&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;Each edition features different models and requires different hardware to run, this means that if you are unable to obtain a TPU or a GPU you might still be able to use the other version. The models you can use are listed underneath the edition. To open a Colab click the big link featuring the editions name.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://colab.research.google.com/github/KoboldAI/KoboldAI-Client/blob/main/colab/TPU.ipynb&#34;&gt;TPU Edition Model Descriptions&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Style&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-13B-Nerys&#34;&gt;Nerys&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;Novel/Adventure&lt;/td&gt; &#xA;   &lt;td&gt;Nerys is a hybrid model based on Pike (A newer Janeway), on top of the Pike dataset you also get some Light Novels, Adventure mode support and a little bit of Shinen thrown in the mix. The end result is a very diverse model that is heavily biased towards SFW novel writing, but one that can go beyond its novel training and make for an excellent adventure model to. Adventure mode is best played from a second person perspective, but can be played in first or third person as well. Novel writing can be done best from the first or third person.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/OPT-13B-Erebus&#34;&gt;Erebus&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Erebus is our community&#39;s flagship NSFW model, being a combination of multiple large datasets that include Literotica, Shinen and erotic novels from Nerys and featuring thourough tagging support it covers the vast majority of erotic writing styles. This model is capable of replacing both the Lit and Shinen models in terms of content and style and has been well received as (one of) the best NSFW models out there. If you wish to use this model for commercial or non research usage we recommend choosing the 20B version as that one is not subject to the restrictive OPT license.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-13B-Janeway&#34;&gt;Janeway&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;Novel&lt;/td&gt; &#xA;   &lt;td&gt;Janeway is a model created from Picard&#39;s dataset combined with a brand new collection of ebooks. This model is trained on 20% more content than Picard and has been trained on literature from various genres. Although the model is mainly focussed on SFW, romantic scenes might involve a degree of nudity.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-13B-Shinen&#34;&gt;Shinen&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Shinen is an NSFW model trained on a variety of stories from the website Sexstories it contains many different kinks. It has been merged into the larger (and better) Erebus model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-J-6B-Skein&#34;&gt;Skein&lt;/a&gt; by VE_FORBRYDERNE&lt;/td&gt; &#xA;   &lt;td&gt;Adventure&lt;/td&gt; &#xA;   &lt;td&gt;Skein is best used with Adventure mode enabled, it consists of a 4 times larger adventure dataset than the Adventure model making it excellent for text adventure gaming. On top of that it also consists of light novel training further expanding its knowledge and writing capabilities. It can be used with the You filter bias if you wish to write Novels with it, but dedicated Novel models can perform better for this task.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-J-6B-Adventure&#34;&gt;Adventure&lt;/a&gt; by VE_FORBRYDERNE&lt;/td&gt; &#xA;   &lt;td&gt;Adventure&lt;/td&gt; &#xA;   &lt;td&gt;Adventure is a 6B model designed to mimick the behavior of AI Dungeon. It is exclusively for Adventure Mode and can take you on the epic and wackey adventures that AI Dungeon players love. It also features the many tropes of AI Dungeon as it has been trained on very similar data. It must be used in second person (You).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hakurei/lit-6B&#34;&gt;Lit&lt;/a&gt; (&lt;a href=&#34;https://huggingface.co/hakurei/litv2-6B-rev3&#34;&gt;V2&lt;/a&gt;) by Haru&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Lit is a great NSFW model trained by Haru on both a large set of Literotica stories and high quality novels along with tagging support. Creating a high quality model for your NSFW stories. This model is exclusively a novel model and is best used in third person.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-13b&#34;&gt;OPT&lt;/a&gt; by Metaseq&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;OPT is considered one of the best base models as far as content goes, its behavior has the strengths of both GPT-Neo and Fairseq Dense. Compared to Neo duplicate and unnecessary content has been left out, while additional literature was added in similar to the Fairseq Dense model. The Fairseq Dense model however lacks the broader data that OPT does have. The biggest downfall of OPT is its license, which prohibits any commercial usage, or usage beyond research purposes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;Neo(X)&lt;/a&gt; by EleutherAI&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;NeoX is the largest EleutherAI model currently available, being a generic model it is not particularly trained towards anything and can do a variety of writing, Q&amp;amp;A and coding tasks. 20B&#39;s performance is closely compared to the 13B models and it is worth trying both especially if you have a task that does not involve english writing. Its behavior will be similar to the GPT-J-6B model since they are trained on the same dataset but with more sensitivity towards repetition penalty and with more knowledge.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-13B&#34;&gt;Fairseq Dense&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;Trained by Facebook Researchers this model stems from the MOE research project within Fairseq. This particular version has been converted by us for use in KoboldAI. It is known to be on par with the larger 20B model from EleutherAI and considered as better for pop culture and language tasks. Because the model has never seen a new line (enter) it may perform worse on formatting and paragraphing. Compared to other models the dataset focuses primarily on literature and contains little else.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B&#34;&gt;GPT-J-6B&lt;/a&gt; by EleutherAI&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;This model serves as the basis for most other 6B models (Some being based on Fairseq Dense instead). Being trained on the Pile and not biased towards anything in particular it is suitable for a variety of tasks such as writing, Q&amp;amp;A and coding tasks. You will likely get better result with larger generic models or finetuned models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://colab.research.google.com/github/KoboldAI/KoboldAI-Client/blob/main/colab/GPU.ipynb&#34;&gt;GPU Edition Model Descriptions&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Style&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-2.7B-Nerys&#34;&gt;Nerys&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;Novel/Adventure&lt;/td&gt; &#xA;   &lt;td&gt;Nerys is a hybrid model based on Pike (A newer Janeway), on top of the Pike dataset you also get some Light Novels, Adventure mode support and a little bit of Shinen thrown in the mix. The end result is a very diverse model that is heavily biased towards SFW novel writing, but one that can go beyond its novel training and make for an excellent adventure model to. Adventure mode is best played from a second person perspective, but can be played in first or third person as well. Novel writing can be done best from the first or third person.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/OPT-2.7B-Erebus&#34;&gt;Erebus&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Erebus is our community&#39;s flagship NSFW model, being a combination of multiple large datasets that include Literotica, Shinen and erotic novels from Nerys and featuring thourough tagging support it covers the vast majority of erotic writing styles. This model is capable of replacing both the Lit and Shinen models in terms of content and style and has been well received as (one of) the best NSFW models out there. If you wish to use this model for commercial or non research usage we recommend choosing the 20B version as that one is not subject to the restrictive OPT license.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Janeway&#34;&gt;Janeway&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;Novel&lt;/td&gt; &#xA;   &lt;td&gt;Janeway is a model created from Picard&#39;s dataset combined with a brand new collection of ebooks. This model is trained on 20% more content than Picard and has been trained on literature from various genres. Although the model is mainly focussed on SFW, romantic scenes might involve a degree of nudity.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Picard&#34;&gt;Picard&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;Novel&lt;/td&gt; &#xA;   &lt;td&gt;Picard is a model trained for SFW Novels based on Neo 2.7B. It is focused on Novel style writing without the NSFW bias. While the name suggests a sci-fi model this model is designed for Novels of a variety of genre&#39;s. It is meant to be used in KoboldAI&#39;s regular mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-AID&#34;&gt;AID&lt;/a&gt; by melastacho&lt;/td&gt; &#xA;   &lt;td&gt;Adventure&lt;/td&gt; &#xA;   &lt;td&gt;Also know as Adventure 2.7B this is a clone of the AI Dungeon Classic model and is best known for the epic wackey adventures that AI Dungeon Classic players love.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Horni-LN&#34;&gt;Horni LN&lt;/a&gt; by finetune&lt;/td&gt; &#xA;   &lt;td&gt;Novel&lt;/td&gt; &#xA;   &lt;td&gt;This model is based on Horni 2.7B and retains its NSFW knowledge, but was then further biased towards SFW novel stories. If you seek a balance between a SFW Novel model and a NSFW model this model should be a good choice.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Horni&#34;&gt;Horni&lt;/a&gt; by finetune&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;This model is tuned on Literotica to produce a Novel style model biased towards NSFW content. Can still be used for SFW stories but will have a bias towards NSFW content. It is meant to be used in KoboldAI&#39;s regular mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Shinen&#34;&gt;Shinen&lt;/a&gt; by Mr Seeker&lt;/td&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Shinen is an alternative to the Horni model designed to be more explicit. If Horni is to tame for you Shinen might produce better results. While it is a Novel model it is unsuitable for SFW stories due to its heavy NSFW bias. Shinen will not hold back. It is meant to be used in KoboldAI&#39;s regular mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-2.7b&#34;&gt;OPT&lt;/a&gt; by Metaseq&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;OPT is considered one of the best base models as far as content goes, its behavior has the strengths of both GPT-Neo and Fairseq Dense. Compared to Neo duplicate and unnecessary content has been left out, while additional literature was added in similar to the Fairseq Dense model. The Fairseq Dense model however lacks the broader data that OPT does have. The biggest downfall of OPT is its license, which prohibits any commercial usage, or usage beyond research purposes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KoboldAI/fairseq-dense-2.7B&#34;&gt;Fairseq Dense&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;Trained by Facebook Researchers this model stems from the MOE research project within Fairseq. This particular version has been converted by us for use in KoboldAI. It is known to be on par with the larger models from EleutherAI and considered as better for pop culture and language tasks. Because the model has never seen a new line (enter) it may perform worse on formatting and paragraphing. Compared to other models the dataset focuses primarily on literature and contains little else.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neo-2.7B&#34;&gt;Neo&lt;/a&gt; by EleutherAI&lt;/td&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;This is the base model for all the other 2.7B models, it is best used when you have a use case that we have no other models available for, such as writing blog articles or programming. It can also be a good basis for the experience of some of the softprompts if your softprompt is not about a subject the other models cover.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Styles&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Novel&lt;/td&gt; &#xA;   &lt;td&gt;For regular story writing, not compatible with Adventure mode or other specialty modes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NSFW&lt;/td&gt; &#xA;   &lt;td&gt;Indicates that the model is strongly biased towards NSFW content and is not suitable for children, work environments or livestreaming. Most NSFW models are also Novel models in nature.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Adventure&lt;/td&gt; &#xA;   &lt;td&gt;These models are excellent for people willing to play KoboldAI like a Text Adventure game and are meant to be used with Adventure mode enabled. Even if you wish to use it as a Novel Type model you should always have Adventure mode on and set it to story. These models typically have a strong bias towards the use of the word You and without Adventure mode enabled break the story flow and write actions on your behalf.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid models are a blend between different Types, for example they are trained on both Novel stories and Adventure stories. These models are great variety models that you can use for multiple different playTypes and modes, but depending on your usage you may need to enable Adventure Mode or the You bias (in userscripts).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Generic&lt;/td&gt; &#xA;   &lt;td&gt;Generic models are not trained towards anything specific, typically used as a basis for other tasks and models. They can do everything the other models can do, but require much more handholding to work properly. Generic models are an ideal basis for tasks that we have no specific model for, or for experiencing a softprompt in its raw form.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tips to get the most out of Google Colab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google will occationally show a Captcha, typically after it has been open for 30 minutes but it can be more frequent if you often use Colab. Make sure to do these properly, or you risk getting your instance shut down and getting a lower priority towards the TPU&#39;s.&lt;/li&gt; &#xA; &lt;li&gt;KoboldAI uses Google Drive to store your files and settings, if you wish to upload a softprompt or userscript this can be done directly on the Google Drive website. You can also use this to download backups of your KoboldAI related files or upload models of your own.&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t want to save your stories on Google Drive for privacy reasons? Do not use KoboldAI&#39;s save function and instead click Download as .json, this will automatically download the story to your own computer without ever touching Google&#39;s harddrives. You can load this back trough the Load from file option.&lt;/li&gt; &#xA; &lt;li&gt;Google shut your instance down unexpectedly? You can still make use of the Download as .json button to recover your story as long as you did not close the KoboldAI window. You can then load this back up in your next session.&lt;/li&gt; &#xA; &lt;li&gt;Done with KoboldAI? Go to the Runtime menu, click on Manage Sessions and terminate your open sessions that you no longer need. This trick can help you maintain higher priority towards getting a TPU.&lt;/li&gt; &#xA; &lt;li&gt;Models stored on Google Drive typically load faster than models we need to download from the internet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install KoboldAI on your own computer&lt;/h2&gt; &#xA;&lt;p&gt;KoboldAI has a large number of dependencies you will need to install on your computer, unfortunately Python does not make it easy for us to provide instructions that work for everyone. The instructions below will work on most computers, but if you have multiple versions of Python installed conflicts can occur.&lt;/p&gt; &#xA;&lt;h3&gt;Downloading the latest version of KoboldAI&lt;/h3&gt; &#xA;&lt;p&gt;KoboldAI is a rolling release on our github, the code you see is also the game. You can download the software by clicking on the green Code button at the top of the page and clicking Download ZIP, or use the &lt;code&gt;git clone&lt;/code&gt; command instead. Then, on Windows you need to you run install_requirements.bat (using admin mode is recommanded to avoid errors), and once it&#39;s done, or if you&#39;re on Linux, either play.bat/sh or remote-play.bat/sh to run it.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way for Windows users is to use the &lt;a href=&#34;https://sourceforge.net/projects/koboldai/files/latest/download&#34;&gt;offline installer&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;h3&gt;Installing KoboldAI offline bundle on Windows 7 or higher using the KoboldAI Offline Installer (Easiest)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sourceforge.net/projects/koboldai/files/latest/download&#34;&gt;Download the latest offline installer from here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the installer to place KoboldAI on a location of choice, KoboldAI is portable software and is not bound to a specific harddrive. (Because of long paths inside our dependencies you may not be able to extract it many folders deep).&lt;/li&gt; &#xA; &lt;li&gt;Update KoboldAI to the latest version with update-koboldai.bat if desired.&lt;/li&gt; &#xA; &lt;li&gt;Use KoboldAI offline using play.bat or remotely with remote-play.bat&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installing KoboldAI Github release on Windows 10 or higher using the KoboldAI Runtime Installer&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Extract the .zip to a location you wish to install KoboldAI, you will need roughly 20GB of free space for the installation (this does not include the models).&lt;/li&gt; &#xA; &lt;li&gt;Open install_requirements.bat as &lt;strong&gt;administrator&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Choose the regular version of Transformers (Option 1), finetuneanon is depreciated and no longer recommended.&lt;/li&gt; &#xA; &lt;li&gt;You will now be asked to choose the installation mode, we &lt;strong&gt;strongly&lt;/strong&gt; recommend the Temporary B: drive option. This option eliminates most installation issues and also makes KoboldAI portable. The B: drive will be gone after a reboot and will automatically be recreated each time you play KoboldAI.&lt;/li&gt; &#xA; &lt;li&gt;The installation will now automatically install its requirements, some stages may appear to freeze do not close the installer until it asks you to press a key. Before pressing a key to exit the installer please check if errors occurred. Most problems with the game crashing are related to installation/download errors. Disabling your antivirus can help if you get errors.&lt;/li&gt; &#xA; &lt;li&gt;Use play.bat to start KoboldAI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installing KoboldAI on Linux using the KoboldAI Runtime (Easiest)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the URL of this Github repository (For example git clone &lt;a href=&#34;https://github.com/koboldai/koboldai-client&#34;&gt;https://github.com/koboldai/koboldai-client&lt;/a&gt; )&lt;/li&gt; &#xA; &lt;li&gt;AMD user? Make sure ROCm is installed if you want GPU support. Is yours not compatible with ROCm? Follow the usual instructions.&lt;/li&gt; &#xA; &lt;li&gt;Run play.sh or if your AMD GPU supports ROCm use play-rocm.sh&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;KoboldAI will now automatically configure its dependencies and start up, everything is contained in its own conda runtime so we will not clutter your system. The files will be located in the runtime subfolder. If at any point you wish to force a reinstallation of the runtime you can do so with the install_requirements.sh file. While you can run this manually it is not neccesary.&lt;/p&gt; &#xA;&lt;h3&gt;Manual installation / Mac&lt;/h3&gt; &#xA;&lt;p&gt;We can not provide a step by step guide for manual installation due to the vast differences between the existing software configuration and the systems of our users.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to manually install KoboldAI you will need some python/conda package management knowledge to manually do one of the following steps :&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use our bundled environments files to install your own conda environment, this should also automatically install CUDA (Recommended, you can get Miniconda from &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links&#34;&gt;https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links&lt;/a&gt;). The recommended configuration is huggingface.yml for CUDA users and rocm.yml for ROCm users.&lt;/li&gt; &#xA; &lt;li&gt;If conda is proving difficult you could also look inside requirements.txt for the required dependencies and try to install them yourself. This will likely be a mixture of pip and your native package manager, just installing our requirements.txt is not recommended since we assume local users will run conda to get all dependencies. For local installations definitely prioritize conda as that is a better way for us to enforce that you have the compatible versions.&lt;/li&gt; &#xA; &lt;li&gt;Clone our Github or download the zip file.&lt;/li&gt; &#xA; &lt;li&gt;Now start KoboldAI with aiserver.py and not with our play.bat or play.sh files.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;AMD GPU&#39;s (Linux only)&lt;/h3&gt; &#xA;&lt;p&gt;AMD GPU&#39;s have terrible compute support, this will currently not work on Windows and will only work for a select few Linux GPU&#39;s. &lt;a href=&#34;https://github.com/RadeonOpenCompute/ROCm#Hardware-and-Software-Support&#34;&gt;You can find a list of the compatible GPU&#39;s here&lt;/a&gt;. Any GPU that is not listed is guaranteed not to work with KoboldAI and we will not be able to provide proper support on GPU&#39;s that are not compatible with the versions of ROCm we require. Make sure to first install ROCm on your Linux system using a guide for your distribution, after that you can follow the usual linux instructions above.&lt;/p&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;There are multiple things that can go wrong with the way Python handles its dependencies, unfortunately we do not have direct step by step solutions for every scenario but there are a few common solutions you can try.&lt;/p&gt; &#xA;&lt;h4&gt;ModuleNotFoundError&lt;/h4&gt; &#xA;&lt;p&gt;This is ALWAYS either a download/installation failure or a conflict with other versions of Python. This is very common if users chose the subfolder option during the installation while putting KoboldAI in a location that has spaces in the path. When an antivirus sandboxes the installation or otherwise interferes with the downloads, systems with low disk space or when your operating system was not configured for Long FIle Paths (The installer will do this on Windows 10 and higher if you run it as administrator, anything other than Windows 10 is not supported by our installers).&lt;/p&gt; &#xA;&lt;p&gt;Another reason the installation may have failed is if you have conflicting installations of Python on your machine, if you press the Windows Key + R and enter %appdata% in the Run Dialog it will open the folder Python installs dependencies on some systems. If you have a Python folder in this location rename this folder and try to run the installer again. It should now no longer get stuck on existing dependencies. Try the game and see if it works well. If it does you can try renaming the folder back to see if it remains functional.&lt;/p&gt; &#xA;&lt;p&gt;The third reason the installation may have failed is if you have conda/mamba on your system for other reasons, in that case we recommend either removing your existing installations of python/conda if you do not need them and testing our installer again. Or using conda itself with our bundled environment files to let it create its runtime manually. &lt;strong&gt;Keep in mind that if you go the manual route you should NEVER use play.bat but should instead run aiserver.py directly&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In general, the less versions of Python you have on your system the higher your chances of it installing correctly. We are consistently trying to mitigate these installation conflicts in our installers but for some users we can not yet avoid all conflicts.&lt;/p&gt; &#xA;&lt;h4&gt;GPU not found errors&lt;/h4&gt; &#xA;&lt;p&gt;GPU not found errors can be caused by one of two things, either you do not have a suitable Nvidia GPU (It needs Compute Capability 5.0 or higher to be able to play KoboldAI). Your Nvidia GPU is supported by KoboldAI but is not supported by the latest version of CUDA. Your Nvidia GPU is not yet supported by the latest version of CUDA or you have a dependency conflict like the ones mentioned above.&lt;/p&gt; &#xA;&lt;p&gt;Like with Python version conflicts we recommend uninstalling CUDA from your system if you have manually installed it and do not need it for anything else and trying again. If your GPU needs CUDA10 to function open environments\finetuneanon.yml and add a line that says - cudatoolkit=10.2 underneath dependencies: . After this you can run the installer again (Pick the option to delete the existing files) and it will download a CUDA10 compatible version.&lt;/p&gt; &#xA;&lt;p&gt;If you do not have a suitable Nvidia GPU that can run on CUDA10 or Higher and that supports Compute Capabilities 5.0 or higher we can not help you get the game detected on the GPU. Unless you are following our ROCm guide with a compatible AMD GPU.&lt;/p&gt; &#xA;&lt;h4&gt;vocab.json / config.json is not found error&lt;/h4&gt; &#xA;&lt;p&gt;If you get these errors you either did not select the correct folder for your custom model or the model you have downloaded is not (yet) compatible with KoboldAI. There exist a few models out there that are compatible and provide a pytorch_model.bin file but do not ship all the required files. In this case try downloading a compatible model of the same kind (For example another GPT-Neo if you downloaded a GPT-Neo model) and replace the pytorch_model.bin file with the one you are trying to run. Chances are this will work fine.&lt;/p&gt; &#xA;&lt;h2&gt;Softprompts&lt;/h2&gt; &#xA;&lt;p&gt;Softprompts (also known as Modules in other products) are addons that can change the output of existing models. For example you may load a softprompt that biases the AI towards a certain subject and style like transcripts from your favorite TV show.&lt;/p&gt; &#xA;&lt;p&gt;Since these softprompts are often based on existing franchises we currently do not bundle any of them with KoboldAI due to copyright concerns (We do not want to put the entire project at risk). Instead look at community resources like #softprompts on the &lt;a href=&#34;https://discord.gg/XuQWadgU9k&#34;&gt;KoboldAI Discord&lt;/a&gt; or the &lt;a href=&#34;https://storage.henk.tech/KoboldAI/softprompts/&#34;&gt;community hosted mirror&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;That way we are better protected from any DMCA claims as things can be taken down easier than directly on Github. If you have a copyright free softprompt that you made from scratch and is not based on existing IP that you would like to see officially bundled with KoboldAI issue a pull request with your softprompt.&lt;/p&gt; &#xA;&lt;p&gt;Training softprompts can be done for free with the &lt;a href=&#34;https://colab.research.google.com/gist/henk717/281fd57ebd2e88d852ef9dcc3f29bebf/easy-softprompt-tuner.ipynb#sandboxMode=true&#34;&gt;Easy Softprompt Tuner&lt;/a&gt;, in that case you can leave most of the settings default. Your source data needs to be a folder with text files that are UTF-8 formatted and contain Unix line endings.&lt;/p&gt; &#xA;&lt;h2&gt;Userscripts&lt;/h2&gt; &#xA;&lt;p&gt;Userscripts are scripts that can automate tasks in KoboldAI, or modify the AI behavior / input / output.&lt;br&gt; Scripting is done in LUA5.4 (Lua does not need to be separately installed as long as you got all the python requirements) and has sandboxing to help protect you from malicious behavior. Even with these measures in place we strongly advise you only run userscripts from places you trust and/or understand, otherwise consult the community for advice on how safe the script might be.&lt;/p&gt; &#xA;&lt;p&gt;Inside the userscripts folder you will find our kaipreset scripts, these are default scripts that we think will be useful for our users. These scripts are automatically overwritten when you update KoboldAI, if you wish to modify these scripts make sure to first rename them to something else that does not contain kaipreset so your changes are not lost. These scripts range from a You Bias filter that prevents the AI from addressing characters as you. Ways to be able to prevent the AI from using words, word replacements and more.&lt;/p&gt; &#xA;&lt;p&gt;Along with our preset scripts we also ship examples in the examples folder that merely serve as a demonstration and do not enhance your usage of KoboldAI. To use these scripts make sure to move them out of the examples folder before either using or modifying the script.&lt;/p&gt; &#xA;&lt;p&gt;Lastly the all the features of our userscript API are documented inside the API Documentation files inside the userscripts folder.&lt;/p&gt; &#xA;&lt;p&gt;For our TPU versions keep in mind that scripts modifying AI behavior relies on a different way of processing that is slower than if you leave these userscripts disabled even if your script only sporadically uses this modifier. If you want to partially use a script at its full speed than you can enable &#34;No Gen Modifiers&#34; to ensure that the parts that would make the TPU slow are not active.&lt;/p&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;KoboldAI has a REST API that can be accessed by adding /api to the URL that Kobold provides you (For example &lt;a href=&#34;http://127.0.0.1:5000/api&#34;&gt;http://127.0.0.1:5000/api&lt;/a&gt;).&lt;br&gt; When accessing this link in a browser you will be taken to the interactive documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This project contains work from the following contributors :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Gantian - Creator of KoboldAI, has created most features such as the interface, the different AI model / API integrations and in general the largest part of the project.&lt;/li&gt; &#xA; &lt;li&gt;VE FORBRYDERNE - Contributed many features such as the Editing overhaul, Adventure Mode, expansions to the world info section, breakmodel integration, scripting support, API, softpromtps and much more. As well as vastly improving the TPU compatibility and integrating external code into KoboldAI so we could use official versions of Transformers with virtually no downsides.&lt;/li&gt; &#xA; &lt;li&gt;Henk717 - Contributed the installation scripts, this readme, random story generator, the docker scripts, the foundation for the commandline interface and other smaller changes as well as integrating multiple parts of the code of different forks to unite it all. He also optimized the model loading so that downloaded models get converted to efficient offline models and that in future models are more likely to work out of the box. Not all code Github attributes to Henk717 is by Henk717 as some of it has been integrations of other people&#39;s work. We try to clarify this in the contributors list as much as we can.&lt;/li&gt; &#xA; &lt;li&gt;Ebolam - Automatic Saving, back/redo, pinning, web loading of models&lt;/li&gt; &#xA; &lt;li&gt;one-some, Logits Viewer and Token Streaming&lt;/li&gt; &#xA; &lt;li&gt;db0, KoboldAI Horde&lt;/li&gt; &#xA; &lt;li&gt;Frogging101 - top_k / tfs support (Part of this support was later redone by VE to integrate what was originally inside of finetuneanon&#39;s transformers)&lt;/li&gt; &#xA; &lt;li&gt;UWUplus (Ralf) - Contributed storage systems for community colabs, as well as cleaning up and integrating the website dependencies/code better. He is also the maintainer of flask-cloudflared which we use to generate the cloudflare links.&lt;/li&gt; &#xA; &lt;li&gt;Javalar - Initial Performance increases on the story_refresh&lt;/li&gt; &#xA; &lt;li&gt;LexSong - Initial environment file adaptation for conda that served as a basis for the install_requirements.bat overhaul.&lt;/li&gt; &#xA; &lt;li&gt;Arrmansa - Breakmodel support for other projects that served as a basis for VE FORBRYDERNE&#39;s integration.&lt;/li&gt; &#xA; &lt;li&gt;Jojorne - Small improvements to the response selection for gens per action.&lt;/li&gt; &#xA; &lt;li&gt;OccultSage (GooseAI) - Improved support for GooseAI/OpenAI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As well as various Model creators who will be listed near their models, and all the testers who helped make this possible!&lt;/p&gt; &#xA;&lt;p&gt;Did we miss your contribution? Feel free to issue a commit adding your name to this list.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;KoboldAI is licensed with a AGPL license, in short this means that it can be used by anyone for any purpose. However, if you decide to make a publicly available instance your users are entitled to a copy of the source code including all modifications that you have made (which needs to be available trough an interface such as a button on your website), you may also not distribute this project in a form that does not contain the source code (Such as compiling / encrypting the code and distributing this version without also distributing the source code that includes the changes that you made. You are allowed to distribute this in a closed form if you also provide a separate archive with the source code.).&lt;/p&gt; &#xA;&lt;p&gt;umamba.exe is bundled for convenience because we observed that many of our users had trouble with command line download methods, it is not part of our project and does not fall under the AGPL license. It is licensed under the BSD-3-Clause license. Other files with differing licenses will have a reference or embedded version of this license within the file. It has been sourced from &lt;a href=&#34;https://anaconda.org/conda-forge/micromamba/files&#34;&gt;https://anaconda.org/conda-forge/micromamba/files&lt;/a&gt; and its source code can be found here : &lt;a href=&#34;https://github.com/mamba-org/mamba/tree/master/micromamba&#34;&gt;https://github.com/mamba-org/mamba/tree/master/micromamba&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>seratch/ChatGPT-in-Slack</title>
    <updated>2023-03-06T01:43:49Z</updated>
    <id>tag:github.com,2023-03-06:/seratch/ChatGPT-in-Slack</id>
    <link href="https://github.com/seratch/ChatGPT-in-Slack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Swift demonstration of how to build a Slack app that enables end-users to interact with a ChatGPT bot&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT in Slack&lt;/h1&gt; &#xA;&lt;p&gt;Introducing a transformative app for Slack users, specifically designed to enhance your communication with &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt;! With this app, you can seamlessly interact with ChatGPT through Slack channels, streamlining your planning and writing processes with the power of AI.&lt;/p&gt; &#xA;&lt;p&gt;If you want to see how this app works, you can install the live demo app from &lt;a href=&#34;https://bit.ly/chat-gpt-in-slack&#34;&gt;https://bit.ly/chat-gpt-in-slack&lt;/a&gt;. Please note that this live demo app is being hosted personally by @seratch. If you plan to use this app in your corporate Slack workspace, we highly recommend that you run it on your own infrastructure using the instructions provided below.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;You can interact with ChatGPT like you do in the website. In the same thread, the bot remember what you already said.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/19658/222405498-867f5002-c8ba-4dc9-bd86-fddc5192070c.gif&#34; width=&#34;450&#34;&gt; &#xA;&lt;p&gt;Consider this realistic scenario: ask the bot to generate a business email for communication with your manager.&lt;/p&gt; &#xA;&lt;img width=&#34;700&#34; src=&#34;https://user-images.githubusercontent.com/19658/222609940-eb581361-eeea-441a-a300-96ecdbc23d0b.png&#34;&gt; &#xA;&lt;p&gt;With ChatGPT, you don&#39;t need to ask a perfectly formulated question at first. Adjusting the details after receiving the bot&#39;s initial response is a great approach.&lt;/p&gt; &#xA;&lt;img width=&#34;700&#34; src=&#34;https://user-images.githubusercontent.com/19658/222609947-b99ace0d-4c90-4265-940d-3fc373429b80.png&#34;&gt; &#xA;&lt;p&gt;Doesn&#39;t that sound cool? 😎&lt;/p&gt; &#xA;&lt;h2&gt;Running the App on Your Local Machine&lt;/h2&gt; &#xA;&lt;p&gt;To run this app on your local machine, you only need to follow these simple steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new Slack app using the manifest-dev.yml file&lt;/li&gt; &#xA; &lt;li&gt;Install the app into your Slack workspace&lt;/li&gt; &#xA; &lt;li&gt;Retrieve your OpenAI API key at &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start the app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create an app-level token with connections:write scope&#xA;export SLACK_APP_TOKEN=xapp-1-...&#xA;# Install the app into your workspace to grab this token&#xA;export SLACK_BOT_TOKEN=xoxb-...&#xA;# Visit https://platform.openai.com/account/api-keys for this token&#xA;export OPENAI_API_KEY=sk-...&#xA;# Optional: include priming instructions for ChatGPT to fine tune the bot purpose&#xA;export SYSTEM_TEXT=&#34;You proofread text. When you receive a message, you will check&#xA;for mistakes and make suggestion to improve the language of the given text&#34;&#xA;&#xA;python -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -r requirements.txt&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the App for Company Workspaces&lt;/h2&gt; &#xA;&lt;p&gt;Confidentiality of information is top priority for businesses.&lt;/p&gt; &#xA;&lt;p&gt;This app is open-sourced! so please feel free to fork it and deploy the app onto the infrastructure that you manage. After going through the above local development process, you can deploy the app using &lt;code&gt;Dockerfile&lt;/code&gt;, which is placed at the root directory of this project.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is designed to establish a WebSocket connection with Slack via Socket Mode. This means that there&#39;s no need to provide a public URL for communication with Slack.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;re always welcome to contribute! &lt;span&gt;🙌&lt;/span&gt; When you make changes to the code in this project, please keep these points in mind:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When making changes to the app, please avoid anything that could cause breaking behavior. If such changes are absolutely necessary due to critical reasons, like security issues, please start a discussion in GitHub Issues before making significant alterations.&lt;/li&gt; &#xA; &lt;li&gt;When you have the chance, please write some unit tests. Especially when you touch &lt;code&gt;internals.py&lt;/code&gt; and add/edit the code that do not call any web APIs, writing tests should be relatively easy.&lt;/li&gt; &#xA; &lt;li&gt;Before committing your changes, be sure to run &lt;code&gt;./validate.sh&lt;/code&gt;. The script runs black (code formatter), flake8 and pytype (static code analyzers).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The License&lt;/h2&gt; &#xA;&lt;p&gt;The MIT License&lt;/p&gt;</summary>
  </entry>
</feed>