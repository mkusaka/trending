<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-29T01:39:01Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>floodsung/Deep-Learning-Papers-Reading-Roadmap</title>
    <updated>2022-10-29T01:39:01Z</updated>
    <id>tag:github.com,2022-10-29:/floodsung/Deep-Learning-Papers-Reading-Roadmap</id>
    <link href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Papers Reading Roadmap&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you are a newcomer to the Deep Learning area, the first question you may have is &#34;Which paper should I start reading from?&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Here is a reading roadmap of Deep Learning papers!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The roadmap is constructed in accordance with the following four guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;From outline to detail&lt;/li&gt; &#xA; &lt;li&gt;From old to state-of-the-art&lt;/li&gt; &#xA; &lt;li&gt;from generic to specific areas&lt;/li&gt; &#xA; &lt;li&gt;focus on state-of-the-art&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will find many papers that are quite new but really worth reading.&lt;/p&gt; &#xA;&lt;p&gt;I would continue adding papers to this roadmap.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;1 Deep Learning History and Basics&lt;/h1&gt; &#xA;&lt;h2&gt;1.0 Book&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[0]&lt;/strong&gt; Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. &#34;&lt;strong&gt;Deep learning&lt;/strong&gt;.&#34; An MIT Press book. (2015). &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;[html]&lt;/a&gt; &lt;strong&gt;(Deep Learning Bible, you can read this book while reading following papers.)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;1.1 Survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. &#34;&lt;strong&gt;Deep learning&lt;/strong&gt;.&#34; Nature 521.7553 (2015): 436-444. &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Three Giants&#39; Survey)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. &#34;&lt;strong&gt;A fast learning algorithm for deep belief nets&lt;/strong&gt;.&#34; Neural computation 18.7 (2006): 1527-1554. &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;strong&gt;(Deep Learning Eve)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. &#34;&lt;strong&gt;Reducing the dimensionality of data with neural networks&lt;/strong&gt;.&#34; Science 313.5786 (2006): 504-507. &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/science.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Milestone, Show the promise of deep learning)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;1.3 ImageNet Evolution（Deep Learning broke out from here）&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &#34;&lt;strong&gt;Imagenet classification with deep convolutional neural networks&lt;/strong&gt;.&#34; Advances in neural information processing systems. 2012. &lt;a href=&#34;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(AlexNet, Deep Learning Breakthrough)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Simonyan, Karen, and Andrew Zisserman. &#34;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt;.&#34; arXiv preprint arXiv:1409.1556 (2014). &lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(VGGNet,Neural Networks become very deep!)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Szegedy, Christian, et al. &#34;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt;.&#34; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(GoogLeNet)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; He, Kaiming, et al. &#34;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt;.&#34; arXiv preprint arXiv:1512.03385 (2015). &lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(ResNet,Very very deep networks, CVPR best paper)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;1.4 Speech Recognition Evolution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; Hinton, Geoffrey, et al. &#34;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt;.&#34; IEEE Signal Processing Magazine 29.6 (2012): 82-97. &lt;a href=&#34;http://cs224d.stanford.edu/papers/maas_paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Breakthrough in speech recognition)&lt;/strong&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. &#34;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt;.&#34; 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. &lt;a href=&#34;http://arxiv.org/pdf/1303.5778.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(RNN)&lt;/strong&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[10]&lt;/strong&gt; Graves, Alex, and Navdeep Jaitly. &#34;&lt;strong&gt;Towards End-To-End Speech Recognition with Recurrent Neural Networks&lt;/strong&gt;.&#34; ICML. Vol. 14. 2014. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v32/graves14.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[11]&lt;/strong&gt; Sak, Haşim, et al. &#34;&lt;strong&gt;Fast and accurate recurrent neural network acoustic models for speech recognition&lt;/strong&gt;.&#34; arXiv preprint arXiv:1507.06947 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1507.06947&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Google Speech Recognition System)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[12]&lt;/strong&gt; Amodei, Dario, et al. &#34;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in english and mandarin&lt;/strong&gt;.&#34; arXiv preprint arXiv:1512.02595 (2015). &lt;a href=&#34;https://arxiv.org/pdf/1512.02595.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Baidu Speech Recognition System)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[13]&lt;/strong&gt; W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig &#34;&lt;strong&gt;Achieving Human Parity in Conversational Speech Recognition&lt;/strong&gt;.&#34; arXiv preprint arXiv:1610.05256 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1610.05256v1&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(State-of-the-art in speech recognition, Microsoft)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;#2 Deep Learning Method&lt;/p&gt; &#xA;&lt;h2&gt;2.1 Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[14]&lt;/strong&gt; Hinton, Geoffrey E., et al. &#34;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt;.&#34; arXiv preprint arXiv:1207.0580 (2012). &lt;a href=&#34;https://arxiv.org/pdf/1207.0580.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Dropout)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[15]&lt;/strong&gt; Srivastava, Nitish, et al. &#34;&lt;strong&gt;Dropout: a simple way to prevent neural networks from overfitting&lt;/strong&gt;.&#34; Journal of Machine Learning Research 15.1 (2014): 1929-1958. &lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[16]&lt;/strong&gt; Ioffe, Sergey, and Christian Szegedy. &#34;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt;.&#34; arXiv preprint arXiv:1502.03167 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1502.03167&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(An outstanding Work in 2015)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[17]&lt;/strong&gt; Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. &#34;&lt;strong&gt;Layer normalization&lt;/strong&gt;.&#34; arXiv preprint arXiv:1607.06450 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;amp;utm_medium=refer&amp;amp;utm_campaign=promote&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Update of Batch Normalization)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[18]&lt;/strong&gt; Courbariaux, Matthieu, et al. &#34;&lt;strong&gt;Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1&lt;/strong&gt;.&#34; &lt;a href=&#34;https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(New Model,Fast)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[19]&lt;/strong&gt; Jaderberg, Max, et al. &#34;&lt;strong&gt;Decoupled neural interfaces using synthetic gradients&lt;/strong&gt;.&#34; arXiv preprint arXiv:1608.05343 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1608.05343&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Innovation of Training Method,Amazing Work)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[20]&lt;/strong&gt; Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. &#34;Net2net: Accelerating learning via knowledge transfer.&#34; arXiv preprint arXiv:1511.05641 (2015). &lt;a href=&#34;https://arxiv.org/abs/1511.05641&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Modify previously trained network to reduce training epochs)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[21]&lt;/strong&gt; Wei, Tao, et al. &#34;Network Morphism.&#34; arXiv preprint arXiv:1603.01670 (2016). &lt;a href=&#34;https://arxiv.org/abs/1603.01670&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Modify previously trained network to reduce training epochs)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.2 Optimization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[22]&lt;/strong&gt; Sutskever, Ilya, et al. &#34;&lt;strong&gt;On the importance of initialization and momentum in deep learning&lt;/strong&gt;.&#34; ICML (3) 28 (2013): 1139-1147. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Momentum optimizer)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[23]&lt;/strong&gt; Kingma, Diederik, and Jimmy Ba. &#34;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt;.&#34; arXiv preprint arXiv:1412.6980 (2014). &lt;a href=&#34;http://arxiv.org/pdf/1412.6980&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Maybe used most often currently)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[24]&lt;/strong&gt; Andrychowicz, Marcin, et al. &#34;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.04474 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1606.04474&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Neural Optimizer,Amazing Work)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[25]&lt;/strong&gt; Han, Song, Huizi Mao, and William J. Dally. &#34;&lt;strong&gt;Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding&lt;/strong&gt;.&#34; CoRR, abs/1510.00149 2 (2015). &lt;a href=&#34;https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[26]&lt;/strong&gt; Iandola, Forrest N., et al. &#34;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt;.&#34; arXiv preprint arXiv:1602.07360 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1602.07360&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Also a new direction to optimize NN,DeePhi Tech Startup)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[27]&lt;/strong&gt; Glorat Xavier, Bengio Yoshua, et al. &#34;&lt;strong&gt;Understanding the difficulty of training deep forward neural networks&lt;/strong&gt;.&#34; Proceedings of the thirteenth International Conference on Artificial Intelligence and Statistics, PMLR 9:249-256,2010. &lt;a href=&#34;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.3 Unsupervised Learning / Deep Generative Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[28]&lt;/strong&gt; Le, Quoc V. &#34;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt;.&#34; 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. &lt;a href=&#34;http://arxiv.org/pdf/1112.6209.pdf&amp;amp;embed&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Milestone, Andrew Ng, Google Brain Project, Cat)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[29]&lt;/strong&gt; Kingma, Diederik P., and Max Welling. &#34;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;.&#34; arXiv preprint arXiv:1312.6114 (2013). &lt;a href=&#34;http://arxiv.org/pdf/1312.6114&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(VAE)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[30]&lt;/strong&gt; Goodfellow, Ian, et al. &#34;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt;.&#34; Advances in Neural Information Processing Systems. 2014. &lt;a href=&#34;http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(GAN,super cool idea)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[31]&lt;/strong&gt; Radford, Alec, Luke Metz, and Soumith Chintala. &#34;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1511.06434 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1511.06434&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(DCGAN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[32]&lt;/strong&gt; Gregor, Karol, et al. &#34;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1502.04623 (2015). &lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/gregor15.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(VAE with attention, outstanding work)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[33]&lt;/strong&gt; Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. &#34;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1601.06759 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1601.06759&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(PixelRNN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[34]&lt;/strong&gt; Oord, Aaron van den, et al. &#34;Conditional image generation with PixelCNN decoders.&#34; arXiv preprint arXiv:1606.05328 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1606.05328&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(PixelCNN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[34]&lt;/strong&gt; S. Mehri et al., &#34;&lt;strong&gt;SampleRNN: An Unconditional End-to-End Neural Audio Generation Model&lt;/strong&gt;.&#34; arXiv preprint arXiv:1612.07837 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1612.07837.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.4 RNN / Sequence-to-Sequence Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[35]&lt;/strong&gt; Graves, Alex. &#34;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1308.0850 (2013). &lt;a href=&#34;http://arxiv.org/pdf/1308.0850&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(LSTM, very nice generating result, show the power of RNN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[36]&lt;/strong&gt; Cho, Kyunghyun, et al. &#34;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1406.1078 (2014). &lt;a href=&#34;http://arxiv.org/pdf/1406.1078&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(First Seq-to-Seq Paper)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[37]&lt;/strong&gt; Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. &#34;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt;.&#34; Advances in neural information processing systems. 2014. &lt;a href=&#34;https://arxiv.org/pdf/1409.3215.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Outstanding Work)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[38]&lt;/strong&gt; Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. &#34;&lt;strong&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/strong&gt;.&#34; arXiv preprint arXiv:1409.0473 (2014). &lt;a href=&#34;https://arxiv.org/pdf/1409.0473v7.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[39]&lt;/strong&gt; Vinyals, Oriol, and Quoc Le. &#34;&lt;strong&gt;A neural conversational model&lt;/strong&gt;.&#34; arXiv preprint arXiv:1506.05869 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Seq-to-Seq on Chatbot)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.5 Neural Turing Machine&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[40]&lt;/strong&gt; Graves, Alex, Greg Wayne, and Ivo Danihelka. &#34;&lt;strong&gt;Neural turing machines&lt;/strong&gt;.&#34; arXiv preprint arXiv:1410.5401 (2014). &lt;a href=&#34;http://arxiv.org/pdf/1410.5401.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Basic Prototype of Future Computer)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[41]&lt;/strong&gt; Zaremba, Wojciech, and Ilya Sutskever. &#34;&lt;strong&gt;Reinforcement learning neural Turing machines&lt;/strong&gt;.&#34; arXiv preprint arXiv:1505.00521 362 (2015). &lt;a href=&#34;https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[42]&lt;/strong&gt; Weston, Jason, Sumit Chopra, and Antoine Bordes. &#34;&lt;strong&gt;Memory networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1410.3916 (2014). &lt;a href=&#34;http://arxiv.org/pdf/1410.3916&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[43]&lt;/strong&gt; Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. &#34;&lt;strong&gt;End-to-end memory networks&lt;/strong&gt;.&#34; Advances in neural information processing systems. 2015. &lt;a href=&#34;http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[44]&lt;/strong&gt; Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. &#34;&lt;strong&gt;Pointer networks&lt;/strong&gt;.&#34; Advances in Neural Information Processing Systems. 2015. &lt;a href=&#34;http://papers.nips.cc/paper/5866-pointer-networks.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[45]&lt;/strong&gt; Graves, Alex, et al. &#34;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt;.&#34; Nature (2016). &lt;a href=&#34;https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Milestone,combine above papers&#39; ideas)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.6 Deep Reinforcement Learning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[46]&lt;/strong&gt; Mnih, Volodymyr, et al. &#34;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1312.5602 (2013). &lt;a href=&#34;http://arxiv.org/pdf/1312.5602.pdf&#34;&gt;[pdf]&lt;/a&gt;) &lt;strong&gt;(First Paper named deep reinforcement learning)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[47]&lt;/strong&gt; Mnih, Volodymyr, et al. &#34;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt;.&#34; Nature 518.7540 (2015): 529-533. &lt;a href=&#34;https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Milestone)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[48]&lt;/strong&gt; Wang, Ziyu, Nando de Freitas, and Marc Lanctot. &#34;&lt;strong&gt;Dueling network architectures for deep reinforcement learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1511.06581 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1511.06581&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(ICLR best paper,great idea)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[49]&lt;/strong&gt; Mnih, Volodymyr, et al. &#34;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1602.01783 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1602.01783&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(State-of-the-art method)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[50]&lt;/strong&gt; Lillicrap, Timothy P., et al. &#34;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1509.02971 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1509.02971&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(DDPG)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[51]&lt;/strong&gt; Gu, Shixiang, et al. &#34;&lt;strong&gt;Continuous Deep Q-Learning with Model-based Acceleration&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.00748 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1603.00748&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(NAF)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[52]&lt;/strong&gt; Schulman, John, et al. &#34;&lt;strong&gt;Trust region policy optimization&lt;/strong&gt;.&#34; CoRR, abs/1502.05477 (2015). &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(TRPO)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[53]&lt;/strong&gt; Silver, David, et al. &#34;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt;.&#34; Nature 529.7587 (2016): 484-489. &lt;a href=&#34;http://willamette.edu/~levenick/cs448/goNature.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(AlphaGo)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.7 Deep Transfer Learning / Lifelong Learning / especially for RL&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[54]&lt;/strong&gt; Bengio, Yoshua. &#34;&lt;strong&gt;Deep Learning of Representations for Unsupervised and Transfer Learning&lt;/strong&gt;.&#34; ICML Unsupervised and Transfer Learning 27 (2012): 17-36. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(A Tutorial)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[55]&lt;/strong&gt; Silver, Daniel L., Qiang Yang, and Lianghao Li. &#34;&lt;strong&gt;Lifelong Machine Learning Systems: Beyond Learning Algorithms&lt;/strong&gt;.&#34; AAAI Spring Symposium: Lifelong Machine Learning. 2013. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(A brief discussion about lifelong learning)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[56]&lt;/strong&gt; Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. &#34;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt;.&#34; arXiv preprint arXiv:1503.02531 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1503.02531&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Godfather&#39;s Work)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[57]&lt;/strong&gt; Rusu, Andrei A., et al. &#34;&lt;strong&gt;Policy distillation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1511.06295 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1511.06295&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(RL domain)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[58]&lt;/strong&gt; Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. &#34;&lt;strong&gt;Actor-mimic: Deep multitask and transfer reinforcement learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1511.06342 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1511.06342&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(RL domain)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[59]&lt;/strong&gt; Rusu, Andrei A., et al. &#34;&lt;strong&gt;Progressive neural networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.04671 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1606.04671&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Outstanding Work, A novel idea)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2.8 One Shot Deep Learning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[60]&lt;/strong&gt; Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. &#34;&lt;strong&gt;Human-level concept learning through probabilistic program induction&lt;/strong&gt;.&#34; Science 350.6266 (2015): 1332-1338. &lt;a href=&#34;http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(No Deep Learning,but worth reading)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[61]&lt;/strong&gt; Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. &#34;&lt;strong&gt;Siamese Neural Networks for One-shot Image Recognition&lt;/strong&gt;.&#34;(2015) &lt;a href=&#34;http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[62]&lt;/strong&gt; Santoro, Adam, et al. &#34;&lt;strong&gt;One-shot Learning with Memory-Augmented Neural Networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1605.06065 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1605.06065&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(A basic step to one shot learning)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[63]&lt;/strong&gt; Vinyals, Oriol, et al. &#34;&lt;strong&gt;Matching Networks for One Shot Learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.04080 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1606.04080&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[64]&lt;/strong&gt; Hariharan, Bharath, and Ross Girshick. &#34;&lt;strong&gt;Low-shot visual object recognition&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.02819 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1606.02819&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(A step to large data)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h1&gt;3 Applications&lt;/h1&gt; &#xA;&lt;h2&gt;3.1 NLP(Natural Language Processing)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Antoine Bordes, et al. &#34;&lt;strong&gt;Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing&lt;/strong&gt;.&#34; AISTATS(2012) &lt;a href=&#34;https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&amp;amp;cache=cache&amp;amp;media=en:bordes12aistats.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Mikolov, et al. &#34;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt;.&#34; ANIPS(2013): 3111-3119 &lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(word2vec)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Sutskever, et al. &#34;&lt;strong&gt;“Sequence to sequence learning with neural networks&lt;/strong&gt;.&#34; ANIPS(2014) &lt;a href=&#34;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Ankit Kumar, et al. &#34;&lt;strong&gt;“Ask Me Anything: Dynamic Memory Networks for Natural Language Processing&lt;/strong&gt;.&#34; arXiv preprint arXiv:1506.07285(2015) &lt;a href=&#34;https://arxiv.org/abs/1506.07285&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Yoon Kim, et al. &#34;&lt;strong&gt;Character-Aware Neural Language Models&lt;/strong&gt;.&#34; NIPS(2015) arXiv preprint arXiv:1508.06615(2015) &lt;a href=&#34;https://arxiv.org/abs/1508.06615&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Jason Weston, et al. &#34;&lt;strong&gt;Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1502.05698(2015) &lt;a href=&#34;https://arxiv.org/abs/1502.05698&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(bAbI tasks)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Karl Moritz Hermann, et al. &#34;&lt;strong&gt;Teaching Machines to Read and Comprehend&lt;/strong&gt;.&#34; arXiv preprint arXiv:1506.03340(2015) &lt;a href=&#34;https://arxiv.org/abs/1506.03340&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(CNN/DailyMail cloze style questions)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; Alexis Conneau, et al. &#34;&lt;strong&gt;Very Deep Convolutional Networks for Natural Language Processing&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.01781(2016) &lt;a href=&#34;https://arxiv.org/abs/1606.01781&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(state-of-the-art in text classification)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; Armand Joulin, et al. &#34;&lt;strong&gt;Bag of Tricks for Efficient Text Classification&lt;/strong&gt;.&#34; arXiv preprint arXiv:1607.01759(2016) &lt;a href=&#34;https://arxiv.org/abs/1607.01759&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(slightly worse than state-of-the-art, but a lot faster)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.2 Object Detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. &#34;&lt;strong&gt;Deep neural networks for object detection&lt;/strong&gt;.&#34; Advances in Neural Information Processing Systems. 2013. &lt;a href=&#34;http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Girshick, Ross, et al. &#34;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt;.&#34; Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(RCNN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; He, Kaiming, et al. &#34;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt;.&#34; European Conference on Computer Vision. Springer International Publishing, 2014. &lt;a href=&#34;http://arxiv.org/pdf/1406.4729&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(SPPNet)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Girshick, Ross. &#34;&lt;strong&gt;Fast r-cnn&lt;/strong&gt;.&#34; Proceedings of the IEEE International Conference on Computer Vision. 2015. &lt;a href=&#34;https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Ren, Shaoqing, et al. &#34;&lt;strong&gt;Faster R-CNN: Towards real-time object detection with region proposal networks&lt;/strong&gt;.&#34; Advances in neural information processing systems. 2015. &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Redmon, Joseph, et al. &#34;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt;.&#34; arXiv preprint arXiv:1506.02640 (2015). &lt;a href=&#34;http://homes.cs.washington.edu/~ali/papers/YOLO.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(YOLO,Oustanding Work, really practical)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Liu, Wei, et al. &#34;&lt;strong&gt;SSD: Single Shot MultiBox Detector&lt;/strong&gt;.&#34; arXiv preprint arXiv:1512.02325 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1512.02325&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; Dai, Jifeng, et al. &#34;&lt;strong&gt;R-FCN: Object Detection via Region-based Fully Convolutional Networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1605.06409 (2016). &lt;a href=&#34;https://arxiv.org/abs/1605.06409&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; He, Gkioxari, et al. &#34;&lt;strong&gt;Mask R-CNN&lt;/strong&gt;&#34; arXiv preprint arXiv:1703.06870 (2017). &lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[10]&lt;/strong&gt; Bochkovskiy, Alexey, et al. &#34;&lt;strong&gt;YOLOv4: Optimal Speed and Accuracy of Object Detection.&lt;/strong&gt;&#34; arXiv preprint arXiv:2004.10934 (2020). &lt;a href=&#34;https://arxiv.org/pdf/2004.10934&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[11]&lt;/strong&gt; Tan, Mingxing, et al. “&lt;strong&gt;EfficientDet: Scalable and Efficient Object Detection.&lt;/strong&gt;&#34; arXiv preprint arXiv:1911.09070 (2019). &lt;a href=&#34;https://arxiv.org/pdf/1911.09070&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.3 Visual Tracking&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Wang, Naiyan, and Dit-Yan Yeung. &#34;&lt;strong&gt;Learning a deep compact image representation for visual tracking&lt;/strong&gt;.&#34; Advances in neural information processing systems. 2013. &lt;a href=&#34;http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(First Paper to do visual tracking using Deep Learning,DLT Tracker)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Wang, Naiyan, et al. &#34;&lt;strong&gt;Transferring rich feature hierarchies for robust visual tracking&lt;/strong&gt;.&#34; arXiv preprint arXiv:1501.04587 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1501.04587&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(SO-DLT)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Wang, Lijun, et al. &#34;&lt;strong&gt;Visual tracking with fully convolutional networks&lt;/strong&gt;.&#34; Proceedings of the IEEE International Conference on Computer Vision. 2015. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(FCNT)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Held, David, Sebastian Thrun, and Silvio Savarese. &#34;&lt;strong&gt;Learning to Track at 100 FPS with Deep Regression Networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1604.01802 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1604.01802&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Bertinetto, Luca, et al. &#34;&lt;strong&gt;Fully-Convolutional Siamese Networks for Object Tracking&lt;/strong&gt;.&#34; arXiv preprint arXiv:1606.09549 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1606.09549&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(SiameseFC,New state-of-the-art for real-time object tracking)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. &#34;&lt;strong&gt;Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking&lt;/strong&gt;.&#34; ECCV (2016) &lt;a href=&#34;http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(C-COT)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. &#34;&lt;strong&gt;Modeling and Propagating CNNs in a Tree Structure for Visual Tracking&lt;/strong&gt;.&#34; arXiv preprint arXiv:1608.07242 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1608.07242&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(VOT2016 Winner,TCNN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.4 Image Caption&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Farhadi,Ali,etal. &#34;&lt;strong&gt;Every picture tells a story: Generating sentences from images&lt;/strong&gt;&#34;. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. &lt;a href=&#34;https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Kulkarni, Girish, et al. &#34;&lt;strong&gt;Baby talk: Understanding and generating image descriptions&lt;/strong&gt;&#34;. In Proceedings of the 24th CVPR, 2011. &lt;a href=&#34;http://tamaraberg.com/papers/generation_cvpr11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Vinyals, Oriol, et al. &#34;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1411.4555, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.4555.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Donahue, Jeff, et al. &#34;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1411.4389 ,2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.4389.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Karpathy, Andrej, and Li Fei-Fei. &#34;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1412.2306, 2014. &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/cvpr2015.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. &#34;&lt;strong&gt;Deep fragment embeddings for bidirectional image sentence mapping&lt;/strong&gt;&#34;. In Advances in neural information processing systems, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1406.5679v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Fang, Hao, et al. &#34;&lt;strong&gt;From captions to visual concepts and back&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1411.4952, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.4952v3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; Chen, Xinlei, and C. Lawrence Zitnick. &#34;&lt;strong&gt;Learning a recurrent visual representation for image caption generation&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1411.5654, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.5654v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; Mao, Junhua, et al. &#34;&lt;strong&gt;Deep captioning with multimodal recurrent neural networks (m-rnn)&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1412.6632, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1412.6632v5.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[10]&lt;/strong&gt; Xu, Kelvin, et al. &#34;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1502.03044, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1502.03044v3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.5 Machine Translation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Some milestone papers are listed in RNN / Seq-to-Seq topic.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Luong, Minh-Thang, et al. &#34;&lt;strong&gt;Addressing the rare word problem in neural machine translation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1410.8206 (2014). &lt;a href=&#34;http://arxiv.org/pdf/1410.8206&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Sennrich, et al. &#34;&lt;strong&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1508.07909, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1508.07909.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. &#34;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1508.04025 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1508.04025&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Chung, et al. &#34;&lt;strong&gt;A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1603.06147, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1603.06147.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Lee, et al. &#34;&lt;strong&gt;Fully Character-Level Neural Machine Translation without Explicit Segmentation&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1610.03017, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1610.03017.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Wu, Schuster, Chen, Le, et al. &#34;&lt;strong&gt;Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/strong&gt;&#34;. In arXiv preprint arXiv:1609.08144v2, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1609.08144v2.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Milestone)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.6 Robotics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Koutník, Jan, et al. &#34;&lt;strong&gt;Evolving large-scale neural networks for vision-based reinforcement learning&lt;/strong&gt;.&#34; Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. &lt;a href=&#34;http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Levine, Sergey, et al. &#34;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt;.&#34; Journal of Machine Learning Research 17.39 (2016): 1-40. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/15-522/15-522.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Pinto, Lerrel, and Abhinav Gupta. &#34;&lt;strong&gt;Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours&lt;/strong&gt;.&#34; arXiv preprint arXiv:1509.06825 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1509.06825&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Levine, Sergey, et al. &#34;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.02199 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1603.02199&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Zhu, Yuke, et al. &#34;&lt;strong&gt;Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning&lt;/strong&gt;.&#34; arXiv preprint arXiv:1609.05143 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1609.05143&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Yahya, Ali, et al. &#34;&lt;strong&gt;Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search&lt;/strong&gt;.&#34; arXiv preprint arXiv:1610.00673 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1610.00673&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Gu, Shixiang, et al. &#34;&lt;strong&gt;Deep Reinforcement Learning for Robotic Manipulation&lt;/strong&gt;.&#34; arXiv preprint arXiv:1610.00633 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1610.00633&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.&#34;&lt;strong&gt;Sim-to-Real Robot Learning from Pixels with Progressive Nets&lt;/strong&gt;.&#34; arXiv preprint arXiv:1610.04286 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1610.04286.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; Mirowski, Piotr, et al. &#34;&lt;strong&gt;Learning to navigate in complex environments&lt;/strong&gt;.&#34; arXiv preprint arXiv:1611.03673 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1611.03673&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.7 Art&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). &#34;&lt;strong&gt;Inceptionism: Going Deeper into Neural Networks&lt;/strong&gt;&#34;. Google Research. &lt;a href=&#34;https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html&#34;&gt;[html]&lt;/a&gt; &lt;strong&gt;(Deep Dream)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. &#34;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt;.&#34; arXiv preprint arXiv:1508.06576 (2015). &lt;a href=&#34;http://arxiv.org/pdf/1508.06576&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Outstanding Work, most successful method currently)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Zhu, Jun-Yan, et al. &#34;&lt;strong&gt;Generative Visual Manipulation on the Natural Image Manifold&lt;/strong&gt;.&#34; European Conference on Computer Vision. Springer International Publishing, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1609.03552&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(iGAN)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Champandard, Alex J. &#34;&lt;strong&gt;Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.01768 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1603.01768&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Neural Doodle)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Zhang, Richard, Phillip Isola, and Alexei A. Efros. &#34;&lt;strong&gt;Colorful Image Colorization&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.08511 (2016). &lt;a href=&#34;http://arxiv.org/pdf/1603.08511&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[6]&lt;/strong&gt; Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. &#34;&lt;strong&gt;Perceptual losses for real-time style transfer and super-resolution&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.08155 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1603.08155.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[7]&lt;/strong&gt; Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. &#34;&lt;strong&gt;A learned representation for artistic style&lt;/strong&gt;.&#34; arXiv preprint arXiv:1610.07629 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1610.07629v1.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[8]&lt;/strong&gt; Gatys, Leon and Ecker, et al.&#34;&lt;strong&gt;Controlling Perceptual Factors in Neural Style Transfer&lt;/strong&gt;.&#34; arXiv preprint arXiv:1611.07865 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1611.07865.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(control style transfer over spatial location,colour information and across spatial scale)&lt;/strong&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[9]&lt;/strong&gt; Ulyanov, Dmitry and Lebedev, Vadim, et al. &#34;&lt;strong&gt;Texture Networks: Feed-forward Synthesis of Textures and Stylized Images&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.03417(2016). &lt;a href=&#34;http://arxiv.org/abs/1603.03417&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(texture generation and style transfer)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[10]&lt;/strong&gt; Yijun Li, Ming-Yu Liu ,Xueting Li, Ming-Hsuan Yang,Jan Kautz (NVIDIA). &#34;&lt;strong&gt;A Closed-form Solution to Photorealistic Image Stylization&lt;/strong&gt;.&#34; arXiv preprint arXiv:1802.06474(2018). &lt;a href=&#34;https://arxiv.org/pdf/1802.06474.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;strong&gt;(Very fast and ultra realistic style transfer)&lt;/strong&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3.8 Object Segmentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[1]&lt;/strong&gt; J. Long, E. Shelhamer, and T. Darrell, “&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt;.” in CVPR, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1411.4038v2.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2]&lt;/strong&gt; L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. &#34;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected crfs&lt;/strong&gt;.&#34; In ICLR, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1606.00915v1.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[3]&lt;/strong&gt; Pinheiro, P.O., Collobert, R., Dollar, P. &#34;&lt;strong&gt;Learning to segment object candidates.&lt;/strong&gt;&#34; In: NIPS. 2015. &lt;a href=&#34;https://arxiv.org/pdf/1506.06204v2.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[4]&lt;/strong&gt; Dai, J., He, K., Sun, J. &#34;&lt;strong&gt;Instance-aware semantic segmentation via multi-task network cascades&lt;/strong&gt;.&#34; in CVPR. 2016 &lt;a href=&#34;https://arxiv.org/pdf/1512.04412v1.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[5]&lt;/strong&gt; Dai, J., He, K., Sun, J. &#34;&lt;strong&gt;Instance-sensitive Fully Convolutional Networks&lt;/strong&gt;.&#34; arXiv preprint arXiv:1603.08678 (2016). &lt;a href=&#34;https://arxiv.org/pdf/1603.08678v1.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/encodec</title>
    <updated>2022-10-29T01:39:01Z</updated>
    <id>tag:github.com,2022-10-29:/facebookresearch/encodec</id>
    <link href="https://github.com/facebookresearch/encodec" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State-of-the-art deep learning based audio codec supporting both mono 24 kHz audio and stereo 48 kHz audio.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EnCodec: High Fidelity Neural Audio Compression&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/encodec/workflows/linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/encodec/workflows/tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the code for the EnCodec neural codec presented in the &lt;a href=&#34;https://arxiv.org/pdf/2210.13438.pdf&#34;&gt;High Fidelity Neural Audio Compression&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.13438&#34;&gt;[abs]&lt;/a&gt;. paper. We provide our two multi-bandwidth models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A causal model operating at 24 kHz on monophonic audio trained on a variety of audio data.&lt;/li&gt; &#xA; &lt;li&gt;A non-causal model operationg at 48 kHz on stereophonic audio trained on music-only data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 24 kHz model can compress to 1.5, 3, 6, 12 or 24 kbps, while the 48 kHz model support 3, 6, 12 and 24 kbps. We also provide a pre-trained language model for each of the models, that can further compress the representation by up to 40% without any further loss of quality.&lt;/p&gt; &#xA;&lt;p&gt;For reference, we also provide the code for our novel &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/encodec/msstftd.py&#34;&gt;MS-STFT discriminator&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/encodec/balancer.py&#34;&gt;balancer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/architecture.png&#34; alt=&#34;Schema representing the structure of Encodec,&#xA;    with a convolutional+LSTM encoder, a Residual Vector Quantization in the middle,&#xA;    followed by a convolutional+LSTM decoder. A multiscale complex spectrogram discriminator is applied to the output, along with objective reconstruction losses.&#xA;    A small transformer model is trained to predict the RVQ output.&#34; width=&#34;800px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Samples&lt;/h2&gt; &#xA;&lt;p&gt;Samples including baselines are provided on &lt;a href=&#34;https://ai.honu.io/papers/encodec/samples.html&#34;&gt;our sample page&lt;/a&gt;. You can also have a quick demo of what we achieve for 48 kHz music with EnCodec, along with entropy coding, by clicking the thumbnail (original tracks provided by &lt;a href=&#34;https://open.spotify.com/artist/5eLv7rNfrf3IjMnK311ByP?si=X_zD9ackRRGjFP5Y6Q7Zng&#34;&gt;Lucille Crew&lt;/a&gt; and &lt;a href=&#34;https://open.spotify.com/artist/21HymveeIhDcM4KDKeNLz0?si=4zXF8VpeQpeKR9QUIuck9Q&#34;&gt;Voyageur I&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://ai.honu.io/papers/encodec/final.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/thumbnail.png&#34; alt=&#34;Thumbnail for the sample video.&#xA;&#x9;You will first here the ground truth, then ~3kbps, then 12kbps, for two songs.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s up?&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/CHANGELOG.md&#34;&gt;the changelog&lt;/a&gt; for details on releases.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;EnCodec requires Python 3.8, and a reasonably recent version of PyTorch (1.11.0 ideally). To install EnCodec, you can run from this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U encodec  # stable release&#xA;pip install -U git+https://git@github.com/facebookresearch/encodec#egg=encodec  # bleeding edge&#xA;# of if you cloned the repo locally&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can then use the EnCodec command, either as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m encodec [...]&#xA;# or&#xA;encodec [...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to directly use the compression API, checkout &lt;code&gt;encodec.compress&lt;/code&gt; and &lt;code&gt;encodec.model&lt;/code&gt;. See hereafter for instructions on how to extract the discrete representation.&lt;/p&gt; &#xA;&lt;h3&gt;Model storage&lt;/h3&gt; &#xA;&lt;p&gt;The models will be automatically downloaded on first use using Torch Hub. For more information on where those models are stored, or how to customize the storage location, &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved&#34;&gt;checkout their documentation.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compression&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;encodec [-b TARGET_BANDWIDTH] [-f] [--hq] [--lm] INPUT_FILE [OUTPUT_FILE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Given any audio file supported by torchaudio on your platform, compresses it with EnCodec to the target bandwidth (default is 6 kbps, can be either 1.5, 3, 6, 12 or 24). OUTPUT_FILE must end in &lt;code&gt;.ecdc&lt;/code&gt;. If not provided it will be the same as &lt;code&gt;INPUT_FILE&lt;/code&gt;, replacing the extension with &lt;code&gt;.ecdc&lt;/code&gt;. In order to use the model operating at 48 kHz on stereophonic audio, use the &lt;code&gt;--hq&lt;/code&gt; flag. The &lt;code&gt;-f&lt;/code&gt; flag is used to force overwrite an existing output file. Use the &lt;code&gt;--lm&lt;/code&gt; flag to use the pretrained language model with entropy coding (expect it to be much slower).&lt;/p&gt; &#xA;&lt;p&gt;If the sample rate or number of channels of the input doesn&#39;t match that of the model, the command will automatically resample / reduce channels as needed.&lt;/p&gt; &#xA;&lt;h3&gt;Decompression&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;encodec [-f] [-r] ENCODEC_FILE [OUTPUT_WAV_FILE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Given a &lt;code&gt;.ecdc&lt;/code&gt; file previously generated, this will decode it to the given output wav file. If not provided, the output will default to the input with the &lt;code&gt;.wav&lt;/code&gt; extension. Use the &lt;code&gt;-f&lt;/code&gt; file to force overwrite the output file (be carefull if compress then decompress, not to overwrite your original file !). Use the &lt;code&gt;-r&lt;/code&gt; flag if you experience clipping, this will rescale the output file to avoid it.&lt;/p&gt; &#xA;&lt;h3&gt;Compression + Decompression&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;encodec [-r] [-b TARGET_BANDWIDTH] [-f] [--hq] [--lm] INPUT_FILE OUTPUT_WAV_FILE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When &lt;code&gt;OUTPUT_WAV_FILE&lt;/code&gt; has the &lt;code&gt;.wav&lt;/code&gt; extension (as opposed to &lt;code&gt;.ecdc&lt;/code&gt;), the &lt;code&gt;encodec&lt;/code&gt; command will instead compress and immediately decompress without storing the intermediate &lt;code&gt;.ecdc&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting discrete representations&lt;/h3&gt; &#xA;&lt;p&gt;The EnCodec model can also be used to extract discrete representations from the audio waveform.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from encodec import EncodecModel&#xA;from encodec.utils import convert_audio&#xA;&#xA;import torchaudio&#xA;import torch&#xA;&#xA;# Instantiate a pretrained EnCodec model&#xA;model = EncodecModel.encodec_model_24khz()&#xA;model.set_target_bandwidth(6.0)&#xA;&#xA;# Load and pre-process the audio waveform&#xA;wav, sr = torchaudio.load(&#34;&amp;lt;PATH_TO_AUDIO_FILE&amp;gt;&#34;)&#xA;wav = wav.unsqueeze(0)&#xA;wav = convert_audio(wav, sr, model.sample_rate, model.channels)&#xA;&#xA;# Extract discrete codes from EnCodec&#xA;encoded_frames = model.encode(wav)&#xA;codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the 48 kHz model processes the audio by chunks of 1 seconds, with an overlap of 1%, and renormalizes the audio to have unit scale. For this model, the output of &lt;code&gt;model.encode(wav)&lt;/code&gt; would a list (for each frame of 1 second) of a tuple &lt;code&gt;(codes, scale)&lt;/code&gt; with &lt;code&gt;scale&lt;/code&gt; a scalar tensor.&lt;/p&gt; &#xA;&lt;h2&gt;Installation for development&lt;/h2&gt; &#xA;&lt;p&gt;This will install the dependencies and a &lt;code&gt;encodec&lt;/code&gt; in developer mode (changes to the files will directly reflect), along with the dependencies to run unit tests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e &#39;.[dev]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;p&gt;You can run the unit tests with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code or results in your paper, please cite our work as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{defossez2022highfi,&#xA;  title={High Fidelity Neural Audio Compression},&#xA;  author={Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},&#xA;  journal={arXiv preprint arXiv:2210.13438},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is released under the CC-BY-NC 4.0. license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/encodec/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmdetection3d</title>
    <updated>2022-10-29T01:39:01Z</updated>
    <id>tag:github.com,2022-10-29:/open-mmlab/mmdetection3d</id>
    <link href="https://github.com/open-mmlab/mmdetection3d" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/resources/mmdet3d-logo.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mmdetection3d.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmdetection3d/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmdetection3d&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmdetection3d/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmdetection3d.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;News&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;💎 Stable version&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;v1.0.0rc5&lt;/strong&gt; was released in 11/10/2022&lt;/p&gt; &#xA;&lt;h3&gt;🌟 Preview of 1.1.x version&lt;/h3&gt; &#xA;&lt;p&gt;A brand new version of &lt;strong&gt;MMDetection v1.1.0rc0&lt;/strong&gt; was released in 1/9/2022:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Unifies interfaces of all components based on &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/tree/3.x&#34;&gt;MMDet 3.x&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A standard data protocol defines and unifies the common keys across different datasets.&lt;/li&gt; &#xA; &lt;li&gt;Faster training and testing speed with more strong baselines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find more new features in &lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d/tree/1.1&#34;&gt;1.1.x branch&lt;/a&gt;. Issues and PRs are welcome!&lt;/p&gt; &#xA;&lt;p&gt;The compatibilities of models are broken due to the unification and simplification of coordinate systems. For now, most models are benchmarked with similar performance, though few models are still being benchmarked. In this version, we update some of the model checkpoints after the refactor of coordinate systems. See more details in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/changelog.md&#34;&gt;Changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;a href=&#34;https://www.nuscenes.org/object-detection?externalData=all&amp;amp;mapData=all&amp;amp;modalities=Any&#34;&gt;nuScenes 3D detection challenge&lt;/a&gt; of the 5th AI Driving Olympics in NeurIPS 2020, we obtained the best PKL award and the second runner-up by multi-modality entry, and the best vision-only results.&lt;/p&gt; &#xA;&lt;p&gt;Code and models for the best vision-only method, &lt;a href=&#34;https://arxiv.org/abs/2104.10956&#34;&gt;FCOS3D&lt;/a&gt;, have been released. Please stay tuned for &lt;a href=&#34;https://arxiv.org/abs/2012.12741&#34;&gt;MoCa&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MMDeploy has supported some MMDetection3d model deployment.&lt;/p&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://mmdetection3d.readthedocs.io/&#34;&gt;https://mmdetection3d.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.3+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MMDetection3D is an open source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D detection. It is a part of the OpenMMLab project developed by &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/&#34;&gt;MMLab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/resources/mmdet3d_outdoor_demo.gif&#34; alt=&#34;demo image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Major features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support multi-modality/single-modality detectors out of box&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It directly supports multi-modality/single-modality detectors including MVXNet, VoteNet, PointPillars, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support indoor/outdoor 3D detection out of box&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It directly supports popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we also support &lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d/tree/master/configs/nuimages&#34;&gt;nuImages dataset&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Natural integration with 2D detection&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All the about &lt;strong&gt;300+ models, methods of 40+ papers&lt;/strong&gt;, and modules supported in &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/master/docs/en/model_zoo.md&#34;&gt;MMDetection&lt;/a&gt; can be trained or used in this codebase.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;High efficiency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It trains faster than other codebases. The main results are as below. Details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/benchmarks.md&#34;&gt;benchmark.md&lt;/a&gt;. We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by &lt;code&gt;✗&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Methods&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MMDetection3D&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet&#34;&gt;OpenPCDet&lt;/a&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/votenet&#34;&gt;votenet&lt;/a&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/poodarchu/Det3D&#34;&gt;Det3D&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;VoteNet&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;358&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;77&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;PointPillars-car&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;141&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;140&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;PointPillars-3class&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;107&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;44&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;SECOND&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;30&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Part-A2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Like &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;, MMDetection3D can also be used as a library to support different projects on top of it.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;v1.0.0rc5 was released in 11/10/2022.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark and model zoo&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/model_zoo.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Components&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Heads&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Features&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/pointnet2&#34;&gt;PointNet (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/pointnet2&#34;&gt;PointNet++ (NeurIPS&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/regnet&#34;&gt;RegNet (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/dgcnn&#34;&gt;DGCNN (TOG&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;DLA (CVPR&#39;2018)&lt;/li&gt; &#xA;     &lt;li&gt;MinkResNet (CVPR&#39;2019)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/free_anchor&#34;&gt;FreeAnchor (NeurIPS&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/dynamic_voxelization&#34;&gt;Dynamic Voxelization (CoRL&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Architectures&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;middle&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;3D Object Detection&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Monocular 3D Object Detection&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Multi-modal 3D Object Detection&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;3D Semantic Segmentation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;b&gt;Outdoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/second&#34;&gt;SECOND (Sensor&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/pointpillars&#34;&gt;PointPillars (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/ssn&#34;&gt;SSN (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/3dssd&#34;&gt;3DSSD (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/sassd&#34;&gt;SA-SSD (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/point_rcnn&#34;&gt;PointRCNN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/parta2&#34;&gt;Part-A2 (TPAMI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/centerpoint&#34;&gt;CenterPoint (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Indoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/votenet&#34;&gt;VoteNet (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/h3dnet&#34;&gt;H3DNet (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/groupfree3d&#34;&gt;Group-Free-3D (ICCV&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/fcaf3d&#34;&gt;FCAF3D (ECCV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;b&gt;Outdoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/imvoxelnet&#34;&gt;ImVoxelNet (WACV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/smoke&#34;&gt;SMOKE (CVPRW&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/fcos3d&#34;&gt;FCOS3D (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/pgd&#34;&gt;PGD (CoRL&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/monoflex&#34;&gt;MonoFlex (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Indoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/imvoxelnet&#34;&gt;ImVoxelNet (WACV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;b&gt;Outdoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/mvxnet&#34;&gt;MVXNet (ICRA&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Indoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/imvotenet&#34;&gt;ImVoteNet (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;li&gt;&lt;b&gt;Indoor&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/pointnet2&#34;&gt;PointNet++ (NeurIPS&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/paconv&#34;&gt;PAConv (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/configs/dgcnn&#34;&gt;DGCNN (TOG&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt;  &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ResNet&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PointNet++&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;SECOND&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DGCNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RegNetX&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DLA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MinkResNet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SECOND&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PointPillars&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FreeAnchor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VoteNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;H3DNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3DSSD&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Part-A2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MVXNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CenterPoint&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SSN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImVoteNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FCOS3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PointNet++&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Group-Free-3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImVoxelNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PAConv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DGCNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SMOKE&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PGD&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MonoFlex&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SA-SSD&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FCAF3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; All the about &lt;strong&gt;300+ models, methods of 40+ papers&lt;/strong&gt; in 2D detection supported by &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/master/docs/en/model_zoo.md&#34;&gt;MMDetection&lt;/a&gt; can be trained or used in this codebase.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/getting_started.md&#34;&gt;getting_started.md&lt;/a&gt; for installation.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/getting_started.md&#34;&gt;getting_started.md&lt;/a&gt; for the basic usage of MMDetection3D. We provide guidance for quick run &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/1_exist_data_model.md&#34;&gt;with existing dataset&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/2_new_data_model.md&#34;&gt;with customized dataset&lt;/a&gt; for beginners. There are also tutorials for &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/config.md&#34;&gt;learning configuration systems&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/customize_dataset.md&#34;&gt;adding new dataset&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/data_pipeline.md&#34;&gt;designing data pipeline&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/customize_models.md&#34;&gt;customizing models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/customize_runtime.md&#34;&gt;customizing runtime settings&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/datasets/waymo_det.md&#34;&gt;Waymo dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions. When updating the version of MMDetection3D, please also check the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/compatibility.md&#34;&gt;compatibility doc&lt;/a&gt; to be aware of the BC-breaking updates introduced in each version.&lt;/p&gt; &#xA;&lt;h2&gt;Model deployment&lt;/h2&gt; &#xA;&lt;p&gt;Now MMDeploy has supported some MMDetection3D model deployment. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/docs/en/tutorials/model_deployment.md&#34;&gt;model_deployment.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{mmdet3d2020,&#xA;    title={{MMDetection3D: OpenMMLab} next-generation platform for general {3D} object detection},&#xA;    author={MMDetection3D Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMDetection3D. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection3d/master/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMDetection3D is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors as well as users who give valuable feedbacks. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new 3D detectors.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>