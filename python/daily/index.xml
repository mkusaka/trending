<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-08T01:35:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hacksider/Deep-Live-Cam</title>
    <updated>2024-08-08T01:35:46Z</updated>
    <id>tag:github.com,2024-08-08:/hacksider/Deep-Live-Cam</id>
    <link href="https://github.com/hacksider/Deep-Live-Cam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;real time face swap and one-click video deepfake with only a single image (uncensored)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.&lt;/p&gt; &#xA;&lt;p&gt;The developers of this software are aware of its possible unethical applicaitons and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media including but not limited to nudity, graphic content, sensitive material such as war footage etc. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.&lt;/p&gt; &#xA;&lt;p&gt;Users of this software are expected to use this software responsibly while abiding the local law. If face of a real person is being used, users are suggested to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.&lt;/p&gt; &#xA;&lt;h2&gt;How do I install it?&lt;/h2&gt; &#xA;&lt;h3&gt;Basic: It is more likely to work on your computer but it will also be very slow. You can follow instructions for the basic install (This usually runs via &lt;strong&gt;CPU&lt;/strong&gt;)&lt;/h3&gt; &#xA;&lt;h4&gt;1.Setup your platform&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python (3.10 recommended)&lt;/li&gt; &#xA; &lt;li&gt;pip&lt;/li&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OlNWCpFdVMA&#34;&gt;ffmpeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;visual studio 2022 runtimes (windows)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Clone Repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/hacksider/Deep-Live-Cam.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Download Models&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth&#34;&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx&#34;&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Then put those 2 files on the &#34;&lt;strong&gt;models&lt;/strong&gt;&#34; folder&lt;/p&gt; &#xA;&lt;h4&gt;4. Install dependency&lt;/h4&gt; &#xA;&lt;p&gt;We highly recommend to work with a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;DONE!!! If you dont have any GPU, You should be able to run roop using &lt;code&gt;python run.py&lt;/code&gt; command. Keep in mind that while running the program for first time, it will download some models which can take time depending on your network connection.&lt;/h5&gt; &#xA;&lt;h3&gt;*Proceed if you want to use GPU Acceleration&lt;/h3&gt; &#xA;&lt;h3&gt;CUDA Execution Provider (Nvidia)*&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;CUDA Toolkit 11.8&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-gpu&#xA;pip install onnxruntime-gpu==1.16.3&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider cuda&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-silicon&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Silicon)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-silicon&#xA;pip install onnxruntime-silicon==1.13.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-legacy&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Legacy)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-coreml&#xA;pip install onnxruntime-coreml==1.13.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#directml-execution-provider-windows&#34;&gt;&lt;/a&gt;DirectML Execution Provider (Windows)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-directml&#xA;pip install onnxruntime-directml==1.15.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider directml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#openvino-execution-provider-intel&#34;&gt;&lt;/a&gt;OpenVINO™ Execution Provider (Intel)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-openvino&#xA;pip install onnxruntime-openvino==1.15.0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider openvino&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How do I use it?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: When you run this program for the first time, it will download some models ~300MB in size.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Executing &lt;code&gt;python run.py&lt;/code&gt; command will launch this window: &lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/instruction.png&#34; alt=&#34;gui-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on &lt;code&gt;Start&lt;/code&gt;. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named &lt;code&gt;&amp;lt;video_title&amp;gt;&lt;/code&gt; where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;For the webcam mode&lt;/h2&gt; &#xA;&lt;p&gt;Just follow the clicks on the screenshot&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select a face&lt;/li&gt; &#xA; &lt;li&gt;Click live&lt;/li&gt; &#xA; &lt;li&gt;Wait for a few second (it takes a longer time, usually 10 to 30 seconds before the preview shows up)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just use your favorite screencapture to stream like OBS&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: In case you want to change your face, just select another picture, the preview mode will then restart (so just wait a bit).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Additional command line arguments are given below. To learn out what they do, check &lt;a href=&#34;https://github.com/s0md3v/roop/wiki/Advanced-Options&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;options:&#xA;  -h, --help                                               show this help message and exit&#xA;  -s SOURCE_PATH, --source SOURCE_PATH                     select an source image&#xA;  -t TARGET_PATH, --target TARGET_PATH                     select an target image or video&#xA;  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory&#xA;  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)&#xA;  --keep-fps                                               keep original fps&#xA;  --keep-audio                                             keep original audio&#xA;  --keep-frames                                            keep temporary frames&#xA;  --many-faces                                             process every face&#xA;  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder&#xA;  --video-quality [0-51]                                   adjust output video quality&#xA;  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB&#xA;  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)&#xA;  --execution-threads EXECUTION_THREADS                    number of execution threads&#xA;  -v, --version                                            show program&#39;s version number and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/henryruhs&#34;&gt;henryruhs&lt;/a&gt;: for being an irreplaceable contributor to the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;: for making video related operations easy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight&#34;&gt;deepinsight&lt;/a&gt;: for their &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt; project which provided a well-made library and models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/havok2-htwo&#34;&gt;havok2-htwo&lt;/a&gt; : for sharing the code for webcam&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GosuDRM/nsfw-roop&#34;&gt;GosuDRM&lt;/a&gt; : for uncensoring roop&lt;/li&gt; &#xA; &lt;li&gt;and all developers behind libraries used in this project.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>labmlai/annotated_deep_learning_paper_implementations</title>
    <updated>2024-08-08T01:35:46Z</updated>
    <id>tag:github.com,2024-08-08:/labmlai/annotated_deep_learning_paper_implementations</id>
    <link href="https://github.com/labmlai/annotated_deep_learning_paper_implementations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🧑‍🏫 60 Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86&#34; alt=&#34;Sponsor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://nn.labml.ai/dqn-light.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/transformers/index.html&#34;&gt;Transformers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mha.html&#34;&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/models.html&#34;&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/index.html&#34;&gt;Transformer XL&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/relative_mha.html&#34;&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/alibi/index.html&#34;&gt;Attention with Linear Biases (ALiBi)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/retro/index.html&#34;&gt;RETRO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/compressive/index.html&#34;&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gpt/index.html&#34;&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/glu_variants/simple.html&#34;&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/knn&#34;&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/feedback/index.html&#34;&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/switch/index.html&#34;&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fast_weights/index.html&#34;&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fnet/index.html&#34;&gt;FNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/aft/index.html&#34;&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlm/index.html&#34;&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlp_mixer/index.html&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gmlp/index.html&#34;&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/vit/index.html&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/primer_ez/index.html&#34;&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/hour_glass/index.html&#34;&gt;Hourglass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/neox/index.html&#34;&gt;Eleuther GPT-NeoX&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/samples/generate.html&#34;&gt;Generate on a 48GB GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/samples/finetune.html&#34;&gt;Finetune on two 48GB GPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/utils/llm_int8.html&#34;&gt;LLM.int8()&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/diffusion/index.html&#34;&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/ddpm/index.html&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html&#34;&gt;Denoising Diffusion Implicit Models (DDIM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html&#34;&gt;Latent Diffusion Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/index.html&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/gan/index.html&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/original/index.html&#34;&gt;Original GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/dcgan/index.html&#34;&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/cycle_gan/index.html&#34;&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/index.html&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/stylegan/index.html&#34;&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/recurrent_highway_networks/index.html&#34;&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/lstm/index.html&#34;&gt;LSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/hypernetworks/hyper_lstm.html&#34;&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/resnet/index.html&#34;&gt;ResNet&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/conv_mixer/index.html&#34;&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/capsule_networks/index.html&#34;&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/unet/index.html&#34;&gt;U-Net&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/sketch_rnn/index.html&#34;&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ Graph Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gat/index.html&#34;&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gatv2/index.html&#34;&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/cfr/index.html&#34;&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/cfr/kuhn/index.html&#34;&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/rl/index.html&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/ppo/index.html&#34;&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href=&#34;https://nn.labml.ai/rl/ppo/gae.html&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/dqn/index.html&#34;&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href=&#34;https://nn.labml.ai/rl/dqn/model.html&#34;&gt;Dueling Network&lt;/a&gt;, &lt;a href=&#34;https://nn.labml.ai/rl/dqn/replay_buffer.html&#34;&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/optimizers/index.html&#34;&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam.html&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/amsgrad.html&#34;&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam_warmup.html&#34;&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/noam.html&#34;&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/radam.html&#34;&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/ada_belief.html&#34;&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/sophia.html&#34;&gt;Sophia-G Optimizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/normalization/index.html&#34;&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_norm/index.html&#34;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/layer_norm/index.html&#34;&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/instance_norm/index.html&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/group_norm/index.html&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/weight_standardization/index.html&#34;&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_channel_norm/index.html&#34;&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/deep_norm/index.html&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/distillation/index.html&#34;&gt;Distillation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/adaptive_computation/index.html&#34;&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/adaptive_computation/ponder_net/index.html&#34;&gt;PonderNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/uncertainty/index.html&#34;&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/uncertainty/evidence/index.html&#34;&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/activations/index.html&#34;&gt;Activations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/activations/fta/index.html&#34;&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/sampling/index.html&#34;&gt;Langauge Model Sampling Techniques&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/greedy.html&#34;&gt;Greedy Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/temperature.html&#34;&gt;Temperature Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/top_k.html&#34;&gt;Top-k Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/nucleus.html&#34;&gt;Nucleus Sampling&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/scaling/index.html&#34;&gt;Scalable Training/Inference&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/scaling/zero3/index.html&#34;&gt;Zero3 memory optimizations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install labml-nn&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CogVideo</title>
    <updated>2024-08-08T01:35:46Z</updated>
    <id>tag:github.com,2024-08-08:/THUDM/CogVideo</id>
    <link href="https://github.com/THUDM/CogVideo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text-to-video generation: CogVideoX (2024) and CogVideo (ICLR 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogVideo &amp;amp;&amp;amp; CogVideoX&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/README_zh.md&#34;&gt;中文阅读&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/logo.svg?sanitize=true&#34; width=&#34;50%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 Experience on &lt;a href=&#34;https://huggingface.co/spaces/THUDM/CogVideoX&#34; target=&#34;_blank&#34;&gt;CogVideoX Huggingface Space&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 📚 Check here to view &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/CogVideoX.pdf&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 👋 Join our &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/WECHAT.md&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; and &lt;a href=&#34;https://discord.gg/Ewaabk6s&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 📍 Visit &lt;a href=&#34;https://chatglm.cn/video?fr=osm_cogvideox&#34;&gt;清影&lt;/a&gt; and &lt;a href=&#34;https://open.bigmodel.cn/?utm_campaign=open&amp;amp;_channel_track_key=OWTVNma9&#34;&gt;API Platform&lt;/a&gt; to experience larger-scale commercial video generation models. &lt;/p&gt; &#xA;&lt;h2&gt;Update and News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;News&lt;/strong&gt;: &lt;code&gt;2024/8/7&lt;/code&gt;: CogVideoX has been integrated into &lt;code&gt;diffusers&lt;/code&gt; version 0.30.0. Inference can now be performed on a single 3090 GPU. For more details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/cli_demo.py&#34;&gt;code&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;News&lt;/strong&gt;: &lt;code&gt;2024/8/6&lt;/code&gt;: We have also open-sourced &lt;strong&gt;3D Causal VAE&lt;/strong&gt; used in &lt;strong&gt;CogVideoX-2B&lt;/strong&gt;, which can reconstruct the video almost losslessly.&lt;/li&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;News&lt;/strong&gt;: &lt;code&gt;2024/8/6&lt;/code&gt;: We have open-sourced &lt;strong&gt;CogVideoX-2B&lt;/strong&gt;，the first model in the CogVideoX series of video generation models.&lt;/li&gt; &#xA; &lt;li&gt;🌱 &lt;strong&gt;Source&lt;/strong&gt;: &lt;code&gt;2022/5/19&lt;/code&gt;: We have open-sourced &lt;strong&gt;CogVideo&lt;/strong&gt; (now you can see in &lt;code&gt;CogVideo&lt;/code&gt; branch)，the &lt;strong&gt;first&lt;/strong&gt; open-sourced pretrained text-to-video model, and you can check &lt;a href=&#34;https://arxiv.org/abs/2205.15868&#34;&gt;ICLR&#39;23 CogVideo Paper&lt;/a&gt; for technical details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;More powerful models with larger parameter sizes are on the way~ Stay tuned!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;Jump to a specific section:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Quick-Start&#34;&gt;Quick Start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#sat&#34;&gt;SAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Diffusers&#34;&gt;Diffusers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#cogvideox-2b-gallery&#34;&gt;CogVideoX-2B Video Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Model-Introduction&#34;&gt;Introduction to the CogVideoX Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#project-structure&#34;&gt;Full Project Structure&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#sat&#34;&gt;SAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#tools&#34;&gt;Tools&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#cogvideoiclr23&#34;&gt;Introduction to CogVideo(ICLR&#39;23) Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Citation&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Open-Source-Project-Plan&#34;&gt;Open Source Project Plan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/#Model-License&#34;&gt;Model License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Prompt Optimization&lt;/h3&gt; &#xA;&lt;p&gt;Before running the model, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/convert_demo.py&#34;&gt;this guide&lt;/a&gt; to see how we use the GLM-4 model to optimize the prompt. This is crucial because the model is trained with long prompts, and a good prompt directly affects the quality of the generated video.&lt;/p&gt; &#xA;&lt;h3&gt;SAT&lt;/h3&gt; &#xA;&lt;p&gt;Follow instructions in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/sat/README.md&#34;&gt;sat_demo&lt;/a&gt;: Contains the inference code and fine-tuning code of SAT weights. It is recommended to improve based on the CogVideoX model structure. Innovative researchers use this code to better perform rapid stacking and development. (18 GB for inference, 40GB for lora finetune)&lt;/p&gt; &#xA;&lt;h3&gt;Diffusers&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/cli_demo.py&#34;&gt;diffusers_demo&lt;/a&gt;: A more detailed explanation of the inference code, mentioning the significance of common parameters. (24GB for inference,fine-tuned code are under development)&lt;/p&gt; &#xA;&lt;h2&gt;CogVideoX-2B Gallery&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9&#34; width=&#34;80%&#34; controls autoplay&gt;&lt;/video&gt; &#xA; &lt;p&gt;A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship&#39;s hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children&#39;s items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship&#39;s journey symbolizing endless adventures in a whimsical, indoor setting.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/9de41efd-d4d1-4095-aeda-246dd834e91d&#34; width=&#34;80%&#34; controls autoplay&gt;&lt;/video&gt; &#xA; &lt;p&gt;The camera follows behind a white vintage SUV with a black roof rack as it speeds up a steep dirt road surrounded by pine trees on a steep mountain slope, dust kicks up from its tires, the sunlight shines on the SUV as it speeds along the dirt road, casting a warm glow over the scene. The dirt road curves gently into the distance, with no other cars or vehicles in sight. The trees on either side of the road are redwoods, with patches of greenery scattered throughout. The car is seen from the rear following the curve with ease, making it seem as if it is on a rugged drive through the rugged terrain. The dirt road itself is surrounded by steep hills and mountains, with a clear blue sky above with wispy clouds.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/941d6661-6a8d-4a1b-b912-59606f0b2841&#34; width=&#34;80%&#34; controls autoplay&gt;&lt;/video&gt; &#xA; &lt;p&gt;A street artist, clad in a worn-out denim jacket and a colorful bandana, stands before a vast concrete wall in the heart, holding a can of spray paint, spray-painting a colorful bird on a mottled wall.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/938529c4-91ae-4f60-b96b-3c3947fa63cb&#34; width=&#34;80%&#34; controls autoplay&gt;&lt;/video&gt; &#xA; &lt;p&gt;In the haunting backdrop of a war-torn city, where ruins and crumbled walls tell a story of devastation, a poignant close-up frames a young girl. Her face is smudged with ash, a silent testament to the chaos around her. Her eyes glistening with a mix of sorrow and resilience, capturing the raw emotion of a world that has lost its innocence to the ravages of conflict.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Model Introduction&lt;/h2&gt; &#xA;&lt;p&gt;CogVideoX is an open-source version of the video generation model, which is homologous to &lt;a href=&#34;https://chatglm.cn/video?fr=osm_cogvideox&#34;&gt;清影&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below shows the list of video generation models we currently provide, along with related basic information:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;CogVideoX-2B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prompt Language&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single GPU Inference (FP16)&lt;/td&gt; &#xA;   &lt;td&gt;18GB using &lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt; &lt;br&gt; 23.9GB using diffusers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi GPUs Inference (FP16)&lt;/td&gt; &#xA;   &lt;td&gt;20GB minimum per GPU using diffusers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU Memory Required for Fine-tuning(bs=1)&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prompt Max Length&lt;/td&gt; &#xA;   &lt;td&gt;226 Tokens&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Video Length&lt;/td&gt; &#xA;   &lt;td&gt;6 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Frames Per Second&lt;/td&gt; &#xA;   &lt;td&gt;8 frames&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Resolution&lt;/td&gt; &#xA;   &lt;td&gt;720 * 480&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quantized Inference&lt;/td&gt; &#xA;   &lt;td&gt;Not Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Download Link (HF diffusers Model)&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-2B&#34;&gt;Huggingface&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/models/ZhipuAI/CogVideoX-2b&#34;&gt;🤖 ModelScope&lt;/a&gt; &lt;a href=&#34;https://wisemodel.cn/models/ZhipuAI/CogVideoX-2b&#34;&gt;💫 WiseModel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Download Link (SAT Model)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/sat/README.md&#34;&gt;SAT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;This open-source repository will guide developers to quickly get started with the basic usage and fine-tuning examples of the &lt;strong&gt;CogVideoX&lt;/strong&gt; open-source model.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/cli_demo.py&#34;&gt;diffusers_demo&lt;/a&gt;: A more detailed explanation of the inference code, mentioning the significance of common parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/cli_vae_demo.py&#34;&gt;diffusers_vae_demo&lt;/a&gt;: Executing the VAE inference code alone currently requires 71GB of memory, but it will be optimized in the future.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/convert_demo.py&#34;&gt;convert_demo&lt;/a&gt;: How to convert user input into a format suitable for CogVideoX. Because CogVideoX is trained on long caption, we need to convert the input text to be consistent with the training distribution using a LLM. By default, the script uses GLM4, but it can also be replaced with any other LLM such as GPT, Gemini, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/gradio_web_demo.py&#34;&gt;gradio_web_demo&lt;/a&gt;: A simple gradio web UI demonstrating how to use the CogVideoX-2B model to generate videos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div style=&#34;text-align: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/gradio_demo.png&#34; style=&#34;width: 100%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/inference/streamlit_web_demo.py&#34;&gt;streamlit_web_demo&lt;/a&gt;: A simple streamlit web application demonstrating how to use the CogVideoX-2B model to generate videos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div style=&#34;text-align: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/web_demo.png&#34; style=&#34;width: 100%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;sat&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/sat/README.md&#34;&gt;sat_demo&lt;/a&gt;: Contains the inference code and fine-tuning code of SAT weights. It is recommended to improve based on the CogVideoX model structure. Innovative researchers use this code to better perform rapid stacking and development.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;This folder contains some tools for model conversion / caption generation, etc.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/tools/convert_weight_sat2hf.py&#34;&gt;convert_weight_sat2hf&lt;/a&gt;: Convert SAT model weights to Huggingface model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/tools/caption&#34;&gt;caption_demo&lt;/a&gt;: Caption tool, a model that understands videos and outputs them in text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CogVideo(ICLR&#39;23)&lt;/h2&gt; &#xA;&lt;p&gt;The official repo for the paper: &lt;a href=&#34;https://arxiv.org/abs/2205.15868&#34;&gt;CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers&lt;/a&gt; is on the &lt;a href=&#34;https://github.com/THUDM/CogVideo/tree/CogVideo&#34;&gt;CogVideo branch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CogVideo is able to generate relatively high-frame-rate videos.&lt;/strong&gt; A 4-second clip of 32 frames is shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/appendix-sample-highframerate.png&#34; alt=&#34;High-frame-rate sample&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/CogVideo/assets/intro-image.png&#34; alt=&#34;Intro images&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/2fa19651-e925-4a2a-b8d6-b3f216d490ba&#34; width=&#34;80%&#34; controls autoplay&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The demo for CogVideo is at &lt;a href=&#34;https://models.aminer.cn/cogvideo/&#34;&gt;https://models.aminer.cn/cogvideo&lt;/a&gt;, where you can get hands-on practice on text-to-video generation. &lt;em&gt;The original input is in Chinese.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;🌟 If you find our work helpful, please leave us a star and cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yang2024cogvideox,&#xA;      title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}, &#xA;      author={Zhuoyi Yang and Jiayan Teng and Wendi Zheng and Ming Ding and Shiyu Huang and JiaZheng Xu and Yuanming Yang and Xiaohan Zhang and Xiaotao Gu and Guanyu Feng and Da Yin and Wenyi Hong and Weihan Wang and Yean Cheng and Yuxuan Zhang and Ting Liu and Bin Xu and Yuxiao Dong and Jie Tang},&#xA;      year={2024},&#xA;}&#xA;@article{hong2022cogvideo,&#xA;  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},&#xA;  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},&#xA;  journal={arXiv preprint arXiv:2205.15868},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Open Source Project Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source CogVideoX model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source 3D Causal VAE used in CogVideoX.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CogVideoX model inference example (CLI / Web Demo)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CogVideoX online experience demo (Huggingface Space)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CogVideoX open source model API interface example (Huggingface)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CogVideoX model fine-tuning example (SAT)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; CogVideoX model fine-tuning example (Huggingface / SAT)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open source CogVideoX-Pro (adapted for CogVideoX-2B suite)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release CogVideoX technical report&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We welcome your contributions. You can click &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/resources/contribute.md&#34;&gt;here&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Model License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is released under the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The model weights and implementation code are released under the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVideo/main/MODEL_LICENSE&#34;&gt;CogVideoX LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>