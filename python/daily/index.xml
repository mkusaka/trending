<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-03T01:38:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QIN2DIM/hcaptcha-challenger</title>
    <updated>2022-11-03T01:38:05Z</updated>
    <id>tag:github.com,2022-11-03:/QIN2DIM/hcaptcha-challenger</id>
    <link href="https://github.com/QIN2DIM/hcaptcha-challenger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü•Ç Gracefully face hCaptcha challenge with YOLOv6(ONNX) embedded solution.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; hCaptcha Challenger&lt;/h1&gt; &#xA; &lt;p&gt;üöÄ Gracefully face hCaptcha challenge with YOLOv6(ONNX) embedded solution.&lt;/p&gt; &#xA; &lt;img src=&#34;https://img.shields.io/static/v1?message=reference&amp;amp;color=blue&amp;amp;style=for-the-badge&amp;amp;logo=micropython&amp;amp;label=python&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/github/license/QIN2DIM/hcaptcha-challenger?style=for-the-badge&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/QIN2DIM/hcaptcha-challenger/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/qin2dim/hcaptcha-challenger/total?style=for-the-badge&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/QIN2DIM/hcaptcha-challenger/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/QIN2DIM/hcaptcha-challenger?style=social&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://t.me/+tJrSQ0_0ujkwZmZh&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?style=social&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;message=studio&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/QIN2DIM/img_pool/raw/main/img/hcaptcha-challenger3.gif&#34; alt=&#34;hcaptcha-challenger-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Does not rely on any Tampermonkey script.&lt;/p&gt; &#xA;&lt;p&gt;Does not use any third-party anti-captcha services.&lt;/p&gt; &#xA;&lt;p&gt;Just implement some interfaces to make &lt;code&gt;AI vs AI&lt;/code&gt; possible.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;google-chrome&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/beiyuouo/hcaptcha-model-factory&#34;&gt;beiyuouo/hcaptcha-model-factory üèó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;meituan/YOLOv6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;ultralytics/yolov5: YOLOv5 üöÄ &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultrafunkamsterdam/undetected-chromedriver&#34;&gt;ultrafunkamsterdam/undetected-chromedriver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;hCaptcha challenge template site &lt;a href=&#34;https://github.com/maximedrn/hcaptcha-solver-python-selenium&#34;&gt;@maximedrn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/esm</title>
    <updated>2022-11-03T01:38:05Z</updated>
    <id>tag:github.com,2022-11-03:/facebookresearch/esm</id>
    <link href="https://github.com/facebookresearch/esm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Evolutionary Scale Modeling (esm): Pretrained language models for proteins&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evolutionary Scale Modeling&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://esmatlas.com&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3605224/199301187-a9e38b3f-71a7-44be-94f4-db0d66143c53.png&#34; alt=&#34;atlas&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt;: Check out &lt;a href=&#34;https://esmatlas.com&#34;&gt;ESM Metagenomic Atlas&lt;/a&gt; of 600M metagenomic structures, with bulk download available &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#atlas&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains code and pre-trained weights for &lt;strong&gt;Transformer protein language models&lt;/strong&gt; from Facebook AI Research, including our state-of-the-art &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#esmfold&#34;&gt;&lt;strong&gt;ESM-2&lt;/strong&gt; and &lt;strong&gt;ESMFold&lt;/strong&gt;&lt;/a&gt;, as well as &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1&#34;&gt;&lt;strong&gt;MSA Transformer&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#zs_variant&#34;&gt;&lt;strong&gt;ESM-1v&lt;/strong&gt;&lt;/a&gt; for predicting variant effects and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#invf&#34;&gt;&lt;strong&gt;ESM-IF1&lt;/strong&gt;&lt;/a&gt; for inverse folding. Transformer protein language models were introduced in the preprint of the paper &lt;a href=&#34;https://doi.org/10.1101/622803&#34;&gt;&#34;Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences&#34; (Rives et al., 2019)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ESM-2 outperforms all tested single-sequence protein language models across a range of structure prediction tasks. ESMFold harnesses the ESM-2 language model to generate accurate structure predictions end to end directly from the sequence of a protein.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;Citation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{lin2022language,&#xA;  title={Language models of protein sequences at the scale of evolution enable accurate structure prediction},&#xA;  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and others},&#xA;  journal={bioRxiv},&#xA;  year={2022},&#xA;  publisher={Cold Spring Harbor Laboratory}&#xA;}&#xA;@article{rives2021biological,&#xA;  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},&#xA;  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},&#xA;  journal={Proceedings of the National Academy of Sciences},&#xA;  volume={118},&#xA;  number={15},&#xA;  pages={e2016239118},&#xA;  year={2021},&#xA;  publisher={National Acad Sciences}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;&lt;b&gt;Table of contents&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#main-models&#34;&gt;Main models you should use&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#repostart&#34;&gt;Getting Started with this repository&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#esmfold&#34;&gt;ESMFold Structure Prediction&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#bulk_fasta&#34;&gt;Compute embeddings in bulk from FASTA&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#fsdp&#34;&gt;CPU offloading for inference with large models&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#zs_variant&#34;&gt;Zero-shot variant prediction&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#invf&#34;&gt;Inverse folding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#atlas&#34;&gt;ESM Metagenomic Atlas&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#notebooks&#34;&gt;Notebooks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available&#34;&gt;Available Models and Datasets&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available-models&#34;&gt;Pre-trained Models&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available-esmssd&#34;&gt;ESM Structural Split Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available-pretraining-split&#34;&gt;Pre-training Dataset Split&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#perf_related&#34;&gt;Comparison to related works&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#citations&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;What&#39;s New&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;November 2022: ESM Metagenomic Atlas, a repository of 600M+ metagenomics structures released, see &lt;a href=&#34;https://esmatlas.com/&#34;&gt;website&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#atlas&#34;&gt;bulk download details&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;November 2022: ESMFold - new end-to-end structure prediction model released (see &lt;a href=&#34;https://doi.org/10.1101/2022.07.20.500902&#34;&gt;Lin et al. 2022&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;August 2022: ESM-2 - new SOTA Language Models released (see &lt;a href=&#34;https://doi.org/10.1101/2022.07.20.500902&#34;&gt;Lin et al. 2022&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;April 2022: New inverse folding model ESM-IF1 released, trained on CATH and UniRef50 predicted structures.&lt;/li&gt; &#xA;  &lt;li&gt;August 2021: Added flexibility to tokenizer to allow for spaces and special tokens (like &lt;code&gt;&amp;lt;mask&amp;gt;&lt;/code&gt;) in sequence.&lt;/li&gt; &#xA;  &lt;li&gt;July 2021: New pre-trained model ESM-1v released, trained on UniRef90 (see &lt;a href=&#34;https://doi.org/10.1101/2021.07.09.450648&#34;&gt;Meier et al. 2021&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;July 2021: New MSA Transformer released, with a minor fix in the row positional embeddings (&lt;code&gt;ESM-MSA-1b&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;Feb 2021: MSA Transformer added (see &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1&#34;&gt;Rao et al. 2021&lt;/a&gt;). Example usage in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#notebooks&#34;&gt;notebook&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Dec 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#notebooks&#34;&gt;Self-Attention Contacts&lt;/a&gt; for all pre-trained models (see &lt;a href=&#34;https://doi.org/10.1101/2020.12.15.422761&#34;&gt;Rao et al. 2020&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Dec 2020: Added new pre-trained model &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#perf_related&#34;&gt;ESM-1b&lt;/a&gt; (see &lt;a href=&#34;https://doi.org/10.1101/622803&#34;&gt;Rives et al. 2019&lt;/a&gt; Appendix B)&lt;/li&gt; &#xA;  &lt;li&gt;Dec 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available-esmssd&#34;&gt;ESM Structural Split Dataset&lt;/a&gt; (see &lt;a href=&#34;https://doi.org/10.1101/622803&#34;&gt;Rives et al. 2019&lt;/a&gt; Appendix A.10)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Main models you should use &lt;a name=&#34;main-models&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Shorthand&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;esm.pretrained.&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t36_3B_UR50D()&lt;/code&gt; &lt;code&gt;esm2_t48_15B_UR50D()&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UR50 (sample UR90)&lt;/td&gt; &#xA;   &lt;td&gt;SOTA general-purpose protein language model. Can be used to predict structure, function and other protein properties directly from individual sequences. Released with &lt;a href=&#34;https://doi.org/10.1101/2022.07.20.500902&#34;&gt;Lin et al. 2022&lt;/a&gt; (Aug 2022 update).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESMFold&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esmfold_v1()&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PDB + UR50&lt;/td&gt; &#xA;   &lt;td&gt;End-to-end single sequence 3D structure predictor (Nov 2022 update).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-MSA-1b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm_msa1b_t12_100M_UR50S()&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UR50 + MSA&lt;/td&gt; &#xA;   &lt;td&gt;MSA Transformer language model. Can be used to extract embeddings from an MSA. Enables SOTA inference of structure. Released with &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.02.12.430858v2&#34;&gt;Rao et al. 2021&lt;/a&gt; (ICML&#39;21 version, June 2021).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1v&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1v_t33_650M_UR90S_1()&lt;/code&gt; ... &lt;code&gt;esm1v_t33_650M_UR90S_5()&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UR90&lt;/td&gt; &#xA;   &lt;td&gt;Language model specialized for prediction of variant effects. Enables SOTA zero-shot prediction of the functional effects of sequence variations. Same architecture as ESM-1b, but trained on UniRef90. Released with &lt;a href=&#34;https://doi.org/10.1101/2021.07.09.450648&#34;&gt;Meier et al. 2021&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-IF1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm_if1_gvp4_t16_142M_UR50()&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CATH + UR50&lt;/td&gt; &#xA;   &lt;td&gt;Inverse folding model. Can be used to design sequences for given structures, or to predict functional effects of sequence variation for given structures. Enables SOTA fixed backbone sequence design. Released with &lt;a href=&#34;https://doi.org/10.1101/2022.04.10.487779&#34;&gt;Hsu et al. 2022&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For a complete list of available models, with details and release notes, see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/#available-models&#34;&gt;Pre-trained Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage &lt;a name=&#34;usage&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Quick start &lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;An easy way to get started is to load ESM or ESMFold through the &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/esm&#34;&gt;HuggingFace transformers library&lt;/a&gt;, which has simplified the ESMFold dependencies and provides a standardized API and tools to work with state-of-the-art pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, &lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb&#34;&gt;ColabFold&lt;/a&gt; has integrated ESMFold so that you can easily run it directly in the browser on a Google Colab instance.&lt;/p&gt; &#xA;&lt;p&gt;We also provide an API which you can access through curl or on &lt;a href=&#34;https://esmatlas.com/resources?action=fold&#34;&gt;the ESM Metagenomic Atlas web page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST --data &#34;KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCAKKIVSDGNGMNAWVAWRNRCKGTDVQAWIRGCRL&#34; https://api.esmatlas.com/foldSequence/v1/pdb/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ESM-MSA-1b, ESM-IF1, or any of the other models you can use the original implementation from our repo directly via the instructions below.&lt;/p&gt; &#xA;&lt;h3&gt;Getting started with this repo &lt;a name=&#34;repostart&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;As a prerequisite, you must have PyTorch installed to use this repository.&lt;/p&gt; &#xA;&lt;p&gt;You can use this one-liner for installation, using the latest release of esm:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fair-esm  # latest release, OR:&#xA;pip install git+https://github.com/facebookresearch/esm.git  # bleeding edge, current repo main branch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the ESMFold model, make sure you start from an environment with python &amp;lt;= 3.9 and pytorch installed. Then add the &lt;code&gt;[esmfold]&lt;/code&gt; option to your pip install, which will install the dependencies for OpenFold automatically. Openfold installation requires &lt;code&gt;nvcc&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fair-esm[esmfold]&#xA;# OpenFold and its remaining dependency&#xA;pip install &#39;dllogger @ git+https://github.com/NVIDIA/dllogger.git&#39;&#xA;pip install &#39;openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If openfold installation fails, please double check that &lt;code&gt;nvcc&lt;/code&gt; is available and that a cuda-compatable version of PyTorch has been installed.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, we provide the &lt;code&gt;esmfold&lt;/code&gt; conda environment, which can be built via &lt;code&gt;conda env create -f environment.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also support PyTorch Hub, which removes the need to clone and/or install this repository yourself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;model, alphabet = torch.hub.load(&#34;facebookresearch/esm:main&#34;, &#34;esm2_t33_650M_UR50D&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After pip install, you can load and use a pretrained model as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import esm&#xA;&#xA;# Load ESM-2 model&#xA;model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()&#xA;batch_converter = alphabet.get_batch_converter()&#xA;model.eval()  # disables dropout for deterministic results&#xA;&#xA;# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)&#xA;data = [&#xA;    (&#34;protein1&#34;, &#34;MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG&#34;),&#xA;    (&#34;protein2&#34;, &#34;KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&#34;),&#xA;    (&#34;protein2 with mask&#34;,&#34;KALTARQQEVFDLIRD&amp;lt;mask&amp;gt;ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE&#34;),&#xA;    (&#34;protein3&#34;,  &#34;K A &amp;lt;mask&amp;gt; I S Q&#34;),&#xA;]&#xA;batch_labels, batch_strs, batch_tokens = batch_converter(data)&#xA;&#xA;# Extract per-residue representations (on CPU)&#xA;with torch.no_grad():&#xA;    results = model(batch_tokens, repr_layers=[33], return_contacts=True)&#xA;token_representations = results[&#34;representations&#34;][33]&#xA;&#xA;# Generate per-sequence representations via averaging&#xA;# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.&#xA;sequence_representations = []&#xA;for i, (_, seq) in enumerate(data):&#xA;    sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))&#xA;&#xA;# Look at the unsupervised self-attention map contact predictions&#xA;import matplotlib.pyplot as plt&#xA;for (_, seq), attention_contacts in zip(data, results[&#34;contacts&#34;]):&#xA;    plt.matshow(attention_contacts[: len(seq), : len(seq)])&#xA;    plt.title(seq)&#xA;    plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ESMFold Structure Pediction &lt;a name=&#34;esmfold&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;After installing with the &lt;code&gt;[esmfold]&lt;/code&gt; option, you can use the ESMFold structure prediction model as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import esm&#xA;&#xA;model = esm.pretrained.esmfold_v1()&#xA;model = model.eval().cuda()&#xA;&#xA;# Optionally, uncomment to set a chunk size for axial attention. This can help reduce memory.&#xA;# Lower sizes will have lower memory requirements at the cost of increased speed.&#xA;# model.set_chunk_size(128)&#xA;&#xA;sequence = &#34;MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG&#34;&#xA;# Multimer prediction can be done with chains separated by &#39;:&#39;&#xA;&#xA;with torch.no_grad():&#xA;    output = model.infer_pdb(sequence)&#xA;&#xA;with open(&#34;result.pdb&#34;, &#34;w&#34;) as f:&#xA;    f.write(output)&#xA;&#xA;import biotite.structure.io as bsio&#xA;struct = bsio.load_structure(&#34;result.pdb&#34;, extra_fields=[&#34;b_factor&#34;])&#xA;print(struct.b_factor.mean())  # this will be the pLDDT&#xA;# 88.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Besides &lt;code&gt;esm.pretrained.esmfold_v1()&lt;/code&gt; which is the best performing model we recommend using, we also provide &lt;code&gt;esm.pretrained.esmfold_v0()&lt;/code&gt; which was used for the experiments in &lt;a href=&#34;https://doi.org/10.1101/2022.07.20.500902&#34;&gt;Lin et al. 2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a script (&lt;code&gt;scripts/esmfold_inference.py&lt;/code&gt;) that efficiently predicts structures in bulk from a FASTA file using ESMFold. This can be run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/esmfold_inference.py \&#xA;    -i &amp;lt;input file with multiple sequences&amp;gt; \&#xA;    -o &amp;lt;path to output directory&amp;gt; \&#xA;    --max-tokens-per-batch &amp;lt;int, default: 1024&amp;gt; \&#xA;    --num-recycles &amp;lt;int, default: 4&amp;gt; \&#xA;    --cpu-only &amp;lt;boolean flag&amp;gt;&#xA;    --cpu-offload &amp;lt;boolean flag&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will make one prediction for every sequence in the fasta file. Multimers can be predicted and should be entered in the fasta file as a single sequence, with chains seprated by a &#34;:&#34; character.&lt;/p&gt; &#xA;&lt;p&gt;By default, predictions will be batched together so that shorter sequences are predicted simultaneously. This can be disabled by setting &lt;code&gt;--max-tokens-per-batch=0&lt;/code&gt;. Batching can significantly improve prediction speed on shorter sequences.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--cpu-offload&lt;/code&gt; flag can be useful for making predictions on longer sequences. It will attempt to offload some parameters to the CPU RAM, rather than storing on GPU.&lt;/p&gt; &#xA;&lt;h3&gt;Compute embeddings in bulk from FASTA &lt;a name=&#34;bulk_fasta&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We provide a script that efficiently extracts embeddings in bulk from a FASTA file. A cuda device is optional and will be auto-detected. The following command extracts the final-layer embedding for a FASTA file from the ESM-2 model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/extract.py esm2_t33_650M_UR50D examples/data/some_proteins.fasta \&#xA;  examples/data/some_proteins_emb_esm2 --repr_layers 0 32 33 --include mean per_tok&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Directory &lt;code&gt;some_proteins_emb_esm2/&lt;/code&gt; now contains one &lt;code&gt;.pt&lt;/code&gt; file per FASTA sequence; use &lt;code&gt;torch.load()&lt;/code&gt; to load them. &lt;code&gt;scripts/extract.py&lt;/code&gt; has flags that determine what&#39;s included in the &lt;code&gt;.pt&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--repr-layers&lt;/code&gt; (default: final only) selects which layers to include embeddings from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--include&lt;/code&gt; specifies what embeddings to save. You can use the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;per_tok&lt;/code&gt; includes the full sequence, with an embedding per amino acid (seq_len x hidden_dim).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;mean&lt;/code&gt; includes the embeddings averaged over the full sequence, per layer.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;bos&lt;/code&gt; includes the embeddings from the beginning-of-sequence token. (NOTE: Don&#39;t use with the pre-trained models - we trained without bos-token supervision)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CPU offloading for inference with large models &lt;a name=&#34;fsdp&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you want to load very large models like 15B and/or do inference on long sequences on your machine, regular GPU inference may lead to OOM errors. We show how to load the model with Fairscale&#39;s &lt;a href=&#34;https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html&#34;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt; and use its CPU offloading feature. This allows to do inference of large models on a single GPU. Please check out &lt;code&gt;examples/esm2_infer_fairscale_fsdp_cpu_offloading.py&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot variant prediction &lt;a name=&#34;zs_variant&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;See &#34;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/variant-prediction/&#34;&gt;examples/variant-prediction/&lt;/a&gt;&#34; for code and pre-trained weights for the ESM-1v models described in &lt;a href=&#34;https://doi.org/10.1101/2021.07.09.450648&#34;&gt;Language models enable zero-shot prediction of the effects of mutations on protein function. (Meier et al. 2021)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that ESM-2 could be used for variant prediction as well, and is expected to have similar performance to ESM-1v.&lt;/p&gt; &#xA;&lt;h3&gt;Inverse folding &lt;a name=&#34;invf&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;See &#34;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/inverse_folding/&#34;&gt;examples/inverse_folding/&lt;/a&gt;&#34; for detailed user guide. The ESM-IF1 model is described as &lt;code&gt;GVPTransformer&lt;/code&gt; in &lt;a href=&#34;https://doi.org/10.1101/2022.04.10.487779&#34;&gt;Learning inverse folding from millions of predicted structures. (Hsu et al. 2022)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a colab notebook for the sequence design and sequence scoring functionalities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The ESM-IF1 inverse folding model is built for predicting protein sequences from their backbone atom coordinates. We provide scripts here 1) to sample sequence designs for a given structure and 2) to score sequences for a given structure.&lt;/p&gt; &#xA;&lt;p&gt;Trained with 12M protein structures predicted by AlphaFold2, the ESM-IF1 model consists of invariant geometric input processing layers followed by a sequence-to-sequence transformer, and achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues. The model is also trained with span masking to tolerate missing backbone coordinates and therefore can predict sequences for partially masked structures.&lt;/p&gt; &#xA;&lt;h4&gt;Sample sequence designs for a given structure&lt;/h4&gt; &#xA;&lt;p&gt;The environment setup is described in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/inverse_folding#recommended-environment&#34;&gt;this subsection of examples/inverse_folding&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To sample sequences for a given structure in PDB or mmCIF format, use the &lt;code&gt;sample_sequences.py&lt;/code&gt; script. The input file can have either &lt;code&gt;.pdb&lt;/code&gt; or &lt;code&gt;.cif&lt;/code&gt; as suffix.&lt;/p&gt; &#xA;&lt;p&gt;For example, to sample 3 sequence designs for the golgi casein kinase structure (PDB &lt;a href=&#34;https://www.rcsb.org/structure/5yh2&#34;&gt;5YH2&lt;/a&gt;; &lt;a href=&#34;https://pdb101.rcsb.org/motm/265&#34;&gt;PDB Molecule of the Month from January 2022&lt;/a&gt;), we can run the following command from the esm root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/inverse_folding/sample_sequences.py examples/inverse_folding/data/5YH2.pdb \&#xA;  --chain C --temperature 1 --num-samples 3 --outpath examples/inverse_folding/output/sampled_sequences.fasta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The sampled sequences will be saved in a fasta format to the specified output file.&lt;/p&gt; &#xA;&lt;p&gt;The temperature parameter controls the sharpness of the probability distribution for sequence sampling. Higher sampling temperatures yield more diverse sequences but likely with lower native sequence recovery. The default sampling temperature is 1. To optimize for native sequence recovery, we recommend sampling with low temperature such as 1e-6.&lt;/p&gt; &#xA;&lt;h4&gt;Scoring sequences&lt;/h4&gt; &#xA;&lt;p&gt;To score the conditional log-likelihoods for sequences conditioned on a given structure, use the &lt;code&gt;score_log_likelihoods.py&lt;/code&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;For example, to score the sequences in &lt;code&gt;examples/inverse_folding/data/5YH2_mutated_seqs.fasta&lt;/code&gt; according to the structure in &lt;code&gt;examples/inverse_folding/data/5YH2.pdb&lt;/code&gt;, we can run the following command from the esm root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/inverse_folding/score_log_likelihoods.py examples/inverse_folding/data/5YH2.pdb \&#xA;  examples/inverse_folding/data/5YH2_mutated_seqs.fasta --chain C \&#xA;  --outpath examples/inverse_folding/output/5YH2_mutated_seqs_scores.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The conditional log-likelihoods are saved in a csv format in the specified output path. The output values are the average log-likelihoods averaged over all amino acids in a sequence.&lt;/p&gt; &#xA;&lt;p&gt;For more information, see &#34;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/inverse_folding/&#34;&gt;./examples/inverse_folding/&lt;/a&gt;&#34; for detailed user guide.&lt;/p&gt; &#xA;&lt;h2&gt;ESMFold Metagenomic Atlas &lt;a name=&#34;atlas&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please see the companion &lt;a href=&#34;https://esmatlas.com/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Bulk download instructions available at a seperate README &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/scripts/atlas/README.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searching a high quality subset of the ESM Atlas available &lt;a href=&#34;https://esmatlas.com/resources?action=search_structure&#34;&gt;here&lt;/a&gt;, and Foldseek provides an API with no length limitations &lt;a href=&#34;https://search.foldseek.com/search&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notebooks &lt;a name=&#34;notebooks&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Inverse folding - predicting or scoring sequences based on backbone structures&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The ESM-IF1 inverse folding model predicts protein sequences from their backbone atom coordinates, trained with 12M protein structures predicted by AlphaFold2. This notetook guide you through examples of sampling sequences, calculating conditional log-likelihoods, and extracting encoder output as structure representation.&lt;/p&gt; &#xA;&lt;h3&gt;Supervised variant prediction - training a classifier on the embeddings&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To help you get started with using the embeddings, this &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/sup_variant_prediction.ipynb&#34;&gt;jupyter notebook tutorial&lt;/a&gt; shows how to train a supervised variant predictor using embeddings from ESM-1. You can adopt a similar protocol to train a model for any downstream task, even with limited data. First you can obtain the embeddings for &lt;code&gt;examples/data/P62593.fasta&lt;/code&gt; either by &lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/examples/P62593_reprs.tar.gz&#34;&gt;downloading the precomputed&lt;/a&gt; embeddings as instructed in the notebook or by running the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Obtain the embeddings&#xA;python scripts/extract.py esm1v_t33_650M_UR90S_1 examples/data/P62593.fasta \&#xA;  examples/data/P62593_emb_esm1v --repr_layers 33 --include mean&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, follow the remaining instructions in the tutorial. You can also run the tutorial in a &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb&#34;&gt;colab notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note, alternatively use &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/variant-prediction/&#34;&gt;the newer instructions for zero-shot variant prediction&lt;/a&gt;, which predicts mutational effects without any supervised training.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Unsupervised contact prediction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/contact_prediction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/contact_prediction.ipynb&#34;&gt;jupyter notebook tutorial&lt;/a&gt; demonstrates contact prediction with both the ESM-2 and MSA Transformer (ESM-MSA-1) models. Contact prediction is based on a logistic regression over the model&#39;s attention maps. This methodology is based on our ICLR 2021 paper, &lt;a href=&#34;https://doi.org/10.1101/2020.12.15.422761&#34;&gt;Transformer protein language models are unsupervised structure learners. (Rao et al. 2020)&lt;/a&gt; The MSA Transformer (ESM-MSA-1) takes a multiple sequence alignment (MSA) as input, and uses the tied row self-attention maps in the same way. See &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1&#34;&gt;MSA Transformer. (Rao et al. 2021)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get unsupervised attention-based contacts, call &lt;code&gt;model.predict_contacts(tokens)&lt;/code&gt; or &lt;code&gt;model(tokens, return_contacts=True)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ESMStructuralSplitDataset and self-attention contact prediction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/esm_structural_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And this &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/esm_structural_dataset.ipynb&#34;&gt;jupyter notebook tutorial&lt;/a&gt; shows how to load and index the &lt;code&gt;ESMStructuralSplitDataset&lt;/code&gt;, and computes the self-attention map unsupervised contact predictions using ESM-2.&lt;/p&gt; &#xA;&lt;h2&gt;Available Models and Datasets &lt;a name=&#34;available&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-trained Models &lt;a name=&#34;available-models&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Shorthand&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;esm.pretrained.&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;#layers&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Embedding Dim&lt;/th&gt; &#xA;   &lt;th&gt;Model URL (automatically downloaded to &lt;code&gt;~/.cache/torch/hub/checkpoints&lt;/code&gt;)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t48_15B_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t48_15B_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t48_15B_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t36_3B_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t33_650M_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;650M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t30_150M_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;150M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t12_35M_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;35M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;480&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm2_t6_8M_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;8M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESMFold&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esmfold_v1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48 (+36)&lt;/td&gt; &#xA;   &lt;td&gt;690M (+3B)&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esmfold_v0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48 (+36)&lt;/td&gt; &#xA;   &lt;td&gt;690M (+3B)&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2021_04&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v0.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v0.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-IF1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm_if1_gvp4_t16_142M_UR50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;124M&lt;/td&gt; &#xA;   &lt;td&gt;CATH 4.3 + predicted structures for UR50&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm_if1_gvp4_t16_142M_UR50.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm_if1_gvp4_t16_142M_UR50.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1v&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1v_t33_650M_UR90S_[1-5]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;650M&lt;/td&gt; &#xA;   &lt;td&gt;UR90/S 2020_03&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1v_t33_650M_UR90S_1.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1v_t33_650M_UR90S_1.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-MSA-1b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm_msa1b_t12_100M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;100M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S + MSA 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1b_t12_100M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1b_t12_100M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-MSA-1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm_msa1_t12_100M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;100M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S + MSA 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1_t12_100M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1_t12_100M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1b_t33_650M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;650M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1b_t33_650M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1b_t33_650M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1_t34_670M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;670M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1_t34_670M_UR50D&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;670M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/D 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50D.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50D.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1_t34_670M_UR100&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;670M&lt;/td&gt; &#xA;   &lt;td&gt;UR100 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR100.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR100.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1_t12_85M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;85M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t12_85M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t12_85M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;esm1_t6_43M_UR50S&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;43M&lt;/td&gt; &#xA;   &lt;td&gt;UR50/S 2018_03&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t6_43M_UR50S.pt&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t6_43M_UR50S.pt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here is a chronological list of the released models and the paper they were introduced in:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Shorthand&lt;/th&gt; &#xA;   &lt;th&gt;Release Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1&lt;/td&gt; &#xA;   &lt;td&gt;Released with Rives et al. 2019 (Aug 2020 update).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1b&lt;/td&gt; &#xA;   &lt;td&gt;Released with Rives et al. 2019 (Dec 2020 update). See Appendix B.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-MSA-1&lt;/td&gt; &#xA;   &lt;td&gt;Released with Rao et al. 2021 (Preprint v1).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-MSA-1b&lt;/td&gt; &#xA;   &lt;td&gt;Released with Rao et al. 2021 (ICML&#39;21 version, June 2021).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-1v&lt;/td&gt; &#xA;   &lt;td&gt;Released with Meier et al. 2021.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-IF1&lt;/td&gt; &#xA;   &lt;td&gt;Released with Hsu et al. 2022.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM-2&lt;/td&gt; &#xA;   &lt;td&gt;Released with Lin et al. 2022.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ESM Structural Split Dataset &lt;a name=&#34;available-esmssd&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is a five-fold cross validation dataset of protein domain structures that can be used to measure generalization of representations across different levels of structural dissimilarity. The dataset implements structural holdouts at the family, superfamily, and fold level. The SCOPe database is used to classify domains. Independently for each level of structural hold-out, the domains are split into 5 equal sets, i.e. five sets of folds, superfamilies, or families. This ensures that for each of the five partitions, structures having the same classification do not appear in both the train and test sets. For a given classification level each structure appears in a test set once, so that in the cross validation experiment each of the structures will be evaluated exactly once.&lt;/p&gt; &#xA;&lt;p&gt;The dataset provides 3d coordinates, distance maps, and secondary structure labels. For further details on the construction of the dataset see &lt;a href=&#34;https://doi.org/10.1101/622803&#34;&gt;Rives et al. 2019&lt;/a&gt; Appendix A.10.&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/esm_structural_dataset.ipynb&#34;&gt;jupyter notebook tutorial&lt;/a&gt; shows how to load and index the &lt;code&gt;ESMStructuralSplitDataset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ESMStructuralSplitDataset&lt;/code&gt;, upon initializing, will download &lt;code&gt;splits&lt;/code&gt; and &lt;code&gt;pkl&lt;/code&gt;. We also provide &lt;code&gt;msas&lt;/code&gt; for each of the domains. The data can be directly downloaded below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;splits&lt;/td&gt; &#xA;   &lt;td&gt;train/valid splits&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/structural-data/splits.tar.gz&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/structural-data/splits.tar.gz&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pkl&lt;/td&gt; &#xA;   &lt;td&gt;pkl objects containing sequence, SSP labels, distance map, and 3d coordinates&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/structural-data/pkl.tar.gz&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/structural-data/pkl.tar.gz&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;msas&lt;/td&gt; &#xA;   &lt;td&gt;a3m files containing MSA for each domain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/structural-data/msas.tar.gz&#34;&gt;https://dl.fbaipublicfiles.com/fair-esm/structural-data/msas.tar.gz&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pre-training Dataset Split &lt;a name=&#34;available-pretraining-split&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The split files establishing which UniRef50 clusters were used as held-out evaluation set for pre-training in &lt;a href=&#34;https://doi.org/10.1101/622803&#34;&gt;Rives et al. 2019&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1101/2021.02.12.430858&#34;&gt;Rao et al. 2021&lt;/a&gt; can be found here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/pretraining-data/uniref201803_ur50_valid_headers.txt.gz&#34;&gt;UniRef50 IDs of evaluation set&lt;/a&gt;: 3.016 M clusters&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/fair-esm/pretraining-data/uniref201803_ur100_valid_headers.txt.gz&#34;&gt;UniRef100 IDs of evaluation set&lt;/a&gt;: 13.745 M proteins, expanding the same UniRef50 clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These files only contain only the UniRef50 IDs and UniRef100 IDs corresponding to the &lt;a href=&#34;https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2018_03/uniref/&#34;&gt;UniRef database, 2018-03 release&lt;/a&gt; which is released by the UniProt Consortium under a &lt;a href=&#34;https://www.uniprot.org/help/license&#34;&gt;Creative Commons Attribution (CC BY 4.0) License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Comparison to related works &lt;a name=&#34;perf_related&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;!--&#xA;DO NOT EDIT THIS TABLE! This is the source of truth:&#xA;https://docs.google.com/spreadsheets/d/1RPvWF47rIMEr-Jg-SRCoGElHcwCl5d7RyEeSyPgp59A/edit#gid=0&#xA;exported via https://www.tablesgenerator.com/html_tables&#xA;--&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-0thz&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Task&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-j6zm&#34; colspan=&#34;3&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Unsupervised contact prediction&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-j6zm&#34; colspan=&#34;2&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Structure Prediction&lt;/span&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Test set&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Large valid&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;CASP14&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;CAMEO (Apr-Jun 2022)&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;CASP14&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-j6zm&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;CAMEO (Apr-Jun 2022)&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;Gremlin (Potts)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;TAPE&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;11.2&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ProtBert-BFD&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;34.1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;Prot-T5-XL-BFD&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;46.1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;62.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;Prot-T5-XL-Ur50 (3B)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;49.8&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;69.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;33.7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-1b&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;41.1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;39&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-1v&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;35.3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-MSA-1b&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;57.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (8M)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;15.9&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;9.8&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;15.7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;36.7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;48.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (35M)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;16.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;28.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;41.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;56.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (150M)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;42.2&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;26.8&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;40.1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;49.0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;64.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (700M)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;50.1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;51.3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;70.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (3B)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;52.7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;34.0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;49.9&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;52.5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;71.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;ESM-2 (15B)&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;54.5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-7zrl&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;55.4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-2b7s&#34;&gt;72.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Comparison to related protein language models on structure prediction tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All contact numbers are the top-L,LR precision metric, where long range means sequence separation of at least 24 residues&lt;/li&gt; &#xA; &lt;li&gt;For unsupervised contact prediction, a sparse linear combination of the attention heads is used to directly predict protein contacts, fitted with logistic regression on 20 structures. For more details on the method, see &lt;a href=&#34;https://doi.org/10.1101/2020.12.15.422761&#34;&gt;Rao et al. 2020&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For structure prediction, an AlphaFold2 structure module is trained directly from the frozen language model embeddings. For more details on the method, see &lt;a href=&#34;https://doi.org/10.1101/2022.07.20.500902&#34;&gt;Lin et al. 2022&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Direct coupling analysis methods (Gremlin, mfDCA, Psicov) and ESM-MSA-1 use the &lt;a href=&#34;https://yanglab.nankai.edu.cn/trRosetta/benchmark/&#34;&gt;trRosetta MSAs&lt;/a&gt;, while other methods predict from single sequence.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations &lt;a name=&#34;citations&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you find the models useful in your research, we ask that you cite the relevant paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{rives2019biological,&#xA;  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},&#xA;  title={Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},&#xA;  year={2019},&#xA;  doi={10.1101/622803},&#xA;  url={https://www.biorxiv.org/content/10.1101/622803v4},&#xA;  journal={bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the self-attention contact prediction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{rao2020transformer,&#xA;  author = {Rao, Roshan M and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},&#xA;  title={Transformer protein language models are unsupervised structure learners},&#xA;  year={2020},&#xA;  doi={10.1101/2020.12.15.422761},&#xA;  url={https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1},&#xA;  journal={bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the MSA Transformer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{rao2021msa,&#xA;  author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},&#xA;  title={MSA Transformer},&#xA;  year={2021},&#xA;  doi={10.1101/2021.02.12.430858},&#xA;  url={https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1},&#xA;  journal={bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For variant prediction using ESM-1v:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{meier2021language,&#xA;  author = {Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alexander},&#xA;  title = {Language models enable zero-shot prediction of the effects of mutations on protein function},&#xA;  year={2021},&#xA;  doi={10.1101/2021.07.09.450648},&#xA;  url={https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1},&#xA;  journal={bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For inverse folding using ESM-IF1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hsu2022learning,&#xA;&#x9;author = {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},&#xA;&#x9;title = {Learning inverse folding from millions of predicted structures},&#xA;&#x9;year = {2022},&#xA;&#x9;doi = {10.1101/2022.04.10.487779},&#xA;&#x9;url = {https://www.biorxiv.org/content/early/2022/04/10/2022.04.10.487779},&#xA;&#x9;journal = {bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the ESM-2 language model and ESMFold:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{lin2022language,&#xA;  title={Language models of protein sequences at the scale of evolution enable accurate structure prediction},&#xA;  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and others},&#xA;  journal={bioRxiv},&#xA;  year={2022},&#xA;  publisher={Cold Spring Harbor Laboratory}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Much of this code builds on the &lt;a href=&#34;https://github.com/pytorch/fairseq&#34;&gt;fairseq&lt;/a&gt; sequence modeling framework. We use fairseq internally for our protein language modeling research. We highly recommend trying it out if you&#39;d like to pre-train protein language models from scratch.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, if you would like to use the variant prediction benchmark from Meier et al. (2021), we provide a bibtex file with citations for all data in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/esm/main/examples/variant-prediction/mutation_data.bib&#34;&gt;./examples/variant-prediction/mutation_data.bib&lt;/a&gt;. You can cite each paper individually, or add all citations in bulk using the LaTeX command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;\nocite{wrenbeck2017deep,klesmith2015comprehensive,haddox2018mapping,romero2015dissecting,firnberg2014comprehensive,deng2012deep,stiffler2015evolvability,jacquier2013capturing,findlay2018comprehensive,mclaughlin2012spatial,kitzman2015massively,doud2016accurate,pokusaeva2019experimental,mishra2016systematic,kelsic2016rna,melnikov2014comprehensive,brenan2016phenotypic,rockah2015systematic,wu2015functional,aakre2015evolving,qi2014quantitative,matreyek2018multiplex,bandaru2017deconstruction,roscoe2013analyses,roscoe2014systematic,mavor2016determination,chan2017correlation,melamed2013deep,starita2013activity,araya2012fundamental}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License &lt;a name=&#34;license&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This source code is licensed under the MIT license found in the &lt;code&gt;LICENSE&lt;/code&gt; file in the root directory of this source tree.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmclassification</title>
    <updated>2022-11-03T01:38:05Z</updated>
    <id>tag:github.com,2022-11-03:/open-mmlab/mmclassification</id>
    <link href="https://github.com/open-mmlab/mmclassification" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Image Classification Toolbox and Benchmark&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmclassification/master/resources/mmcls-logo.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmcls&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmcls&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmclassification/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmclassification&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmclassification/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmclassification.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmclassification.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmclassification.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/&#34;&gt;üìò Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/install.html&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt; | &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/model_zoo.html&#34;&gt;üëÄ Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/changelog.html&#34;&gt;üÜï Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/issues/new/choose&#34;&gt;ü§î Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;span&gt;üëâ&lt;/span&gt; &lt;strong&gt;MMClassification 1.0 branch is in trial, welcome every to &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/1.x&#34;&gt;try it&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/discussions&#34;&gt;discuss with us&lt;/a&gt;!&lt;/strong&gt; &lt;span&gt;üëà&lt;/span&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmclassification/master/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MMClassification is an open source image classification toolbox based on PyTorch. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.5+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/9102141/87268895-3e0d0780-c4fe-11ea-849e-6140b7e0d4de.gif&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Major features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Various backbones and pretrained models&lt;/li&gt; &#xA; &lt;li&gt;Bag of training tricks&lt;/li&gt; &#xA; &lt;li&gt;Large-scale training configs&lt;/li&gt; &#xA; &lt;li&gt;High efficiency and extensibility&lt;/li&gt; &#xA; &lt;li&gt;Powerful toolkits&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;p&gt;The MMClassification 1.0 has released! It&#39;s still unstable and in release candidate. If you want to try it, go to &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/1.x&#34;&gt;the 1.x branch&lt;/a&gt; and discuss it with us in &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/discussions&#34;&gt;the discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;v0.24.1 was released in 31/10/2022. Highlights of the new version:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support HUAWEI Ascend device.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;v0.24.0 was released in 30/9/2022. Highlights of the new version:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support &lt;strong&gt;HorNet&lt;/strong&gt;, &lt;strong&gt;EfficientFormerm&lt;/strong&gt;, &lt;strong&gt;SwinTransformer V2&lt;/strong&gt; and &lt;strong&gt;MViT&lt;/strong&gt; backbones.&lt;/li&gt; &#xA; &lt;li&gt;Support Standford Cars dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;v0.23.0 was released in 1/5/2022. Highlights of the new version:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support &lt;strong&gt;DenseNet&lt;/strong&gt;, &lt;strong&gt;VAN&lt;/strong&gt; and &lt;strong&gt;PoolFormer&lt;/strong&gt;, and provide pre-trained models.&lt;/li&gt; &#xA; &lt;li&gt;Support training on IPU.&lt;/li&gt; &#xA; &lt;li&gt;New style API docs, welcome &lt;a href=&#34;https://mmclassification.readthedocs.io/en/master/api/models.html&#34;&gt;view it&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmclassification/master/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for more details and other release history.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Below are quick steps for installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n open-mmlab python=3.8 pytorch=1.10 cudatoolkit=11.3 torchvision==0.11.0 -c pytorch -y&#xA;conda activate open-mmlab&#xA;pip3 install openmim&#xA;mim install mmcv-full&#xA;git clone https://github.com/open-mmlab/mmclassification.git&#xA;cd mmclassification&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/install.html&#34;&gt;install.md&lt;/a&gt; for more detailed installation and dataset preparation.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/getting_started.html&#34;&gt;Getting Started&lt;/a&gt; for the basic usage of MMClassification. There are also tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/config.html&#34;&gt;Learn about Configs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/finetune.html&#34;&gt;Fine-tune Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/new_dataset.html&#34;&gt;Add New Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/data_pipeline.html&#34;&gt;Customizie Data Pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/new_modules.html&#34;&gt;Add New Modules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/schedule.html&#34;&gt;Customizie Schedule&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/tutorials/runtime.html&#34;&gt;Customizie Runtime Settings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Colab tutorials are also provided:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Learn about MMClassification &lt;strong&gt;Python API&lt;/strong&gt;: &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/raw/master/docs/en/tutorials/MMClassification_python.ipynb&#34;&gt;Preview the notebook&lt;/a&gt; or directly &lt;a href=&#34;https://colab.research.google.com/github/open-mmlab/mmclassification/blob/master/docs/en/tutorials/MMClassification_python.ipynb&#34;&gt;run on Colab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn about MMClassification &lt;strong&gt;CLI tools&lt;/strong&gt;: &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/raw/master/docs/en/tutorials/MMClassification_tools.ipynb&#34;&gt;Preview the notebook&lt;/a&gt; or directly &lt;a href=&#34;https://colab.research.google.com/github/open-mmlab/mmclassification/blob/master/docs/en/tutorials/MMClassification_tools.ipynb&#34;&gt;run on Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model zoo&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/model_zoo.html&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Supported backbones&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/vgg&#34;&gt;VGG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/resnet&#34;&gt;ResNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/resnext&#34;&gt;ResNeXt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/seresnet&#34;&gt;SE-ResNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/seresnet&#34;&gt;SE-ResNeXt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/regnet&#34;&gt;RegNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/shufflenet_v1&#34;&gt;ShuffleNetV1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/shufflenet_v2&#34;&gt;ShuffleNetV2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/mobilenet_v2&#34;&gt;MobileNetV2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/mobilenet_v3&#34;&gt;MobileNetV3&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/swin_transformer&#34;&gt;Swin-Transformer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/repvgg&#34;&gt;RepVGG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/vision_transformer&#34;&gt;Vision-Transformer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/tnt&#34;&gt;Transformer-in-Transformer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/res2net&#34;&gt;Res2Net&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/mlp_mixer&#34;&gt;MLP-Mixer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/deit&#34;&gt;DeiT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/conformer&#34;&gt;Conformer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/t2t_vit&#34;&gt;T2T-ViT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/twins&#34;&gt;Twins&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/efficientnet&#34;&gt;EfficientNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/convnext&#34;&gt;ConvNeXt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/hrnet&#34;&gt;HRNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/van&#34;&gt;VAN&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/convmixer&#34;&gt;ConvMixer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/cspnet&#34;&gt;CSPNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/poolformer&#34;&gt;PoolFormer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/mvit&#34;&gt;MViT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/efficientformer&#34;&gt;EfficientFormer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmclassification/tree/master/configs/hornet&#34;&gt;HorNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMClassification. Please refer to &lt;a href=&#34;https://mmclassification.readthedocs.io/en/latest/community/CONTRIBUTING.html&#34;&gt;CONTRUBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMClassification is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new classifiers.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{2020mmclassification,&#xA;    title={OpenMMLab&#39;s Image Classification Toolbox and Benchmark},&#xA;    author={MMClassification Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmclassification}},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmclassification/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>