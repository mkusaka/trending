<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-07T01:38:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>magic-research/magic-animate</title>
    <updated>2023-12-07T01:38:50Z</updated>
    <id>tag:github.com,2023-12-07:/magic-research/magic-animate</id>
    <link href="https://github.com/magic-research/magic-animate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt;MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://scholar.google.com/citations?user=-4iADzMAAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Zhongcong Xu&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;http://jeff95.me/&#34;&gt;&lt;strong&gt;Jianfeng Zhang&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Jun Hao Liew&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://hanshuyan.github.io/&#34;&gt;&lt;strong&gt;Hanshu Yan&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://scholar.google.com/citations?user=stQQf7wAAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Jia-Wei Liu&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://zhangchenxu528.github.io/&#34;&gt;&lt;strong&gt;Chenxu Zhang&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://sites.google.com/site/jshfeng/home&#34;&gt;&lt;strong&gt;Jiashi Feng&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href=&#34;https://sites.google.com/view/showlab&#34;&gt;&lt;strong&gt;Mike Zheng Shou&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16498&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-MagicAnimate-red&#34; alt=&#34;Paper PDF&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://showlab.github.io/magicanimate&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-MagicAnimate-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/zcxu-eric/magicanimate&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;b&gt;National University of Singapore &amp;nbsp; | &amp;nbsp; ByteDance&lt;/b&gt; &lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/magic-animate/main/assets/teaser/t1.gif&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/magic-animate/main/assets/teaser/t4.gif&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/magic-animate/main/assets/teaser/t3.gif&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/magic-animate/main/assets/teaser/t2.gif&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ“¢ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.4]&lt;/strong&gt; Release inference code and gradio demo. We are working to improve MagicAnimate, stay tuned!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11.23]&lt;/strong&gt; Release MagicAnimate paper and project page.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸƒâ€â™‚ï¸ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please download the pretrained base models for &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;MSE-finetuned VAE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Download our MagicAnimate &lt;a href=&#34;https://huggingface.co/zcxu-eric/MagicAnimate&#34;&gt;checkpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Place them as follows:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magic-animate&#xA;|----pretrained_models&#xA;  |----MagicAnimate&#xA;    |----appearance_encoder&#xA;      |----diffusion_pytorch_model.safetensors&#xA;      |----config.json&#xA;    |----densepose_controlnet&#xA;      |----diffusion_pytorch_model.safetensors&#xA;      |----config.json&#xA;    |----temporal_attention&#xA;      |----temporal_attention.ckpt&#xA;  |----sd-vae-ft-mse&#xA;    |----...&#xA;  |----stable-diffusion-v1-5&#xA;    |----...&#xA;|----...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;âš’ï¸ Installation&lt;/h2&gt; &#xA;&lt;p&gt;prerequisites: &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;CUDA&amp;gt;=11.3&lt;/code&gt;, and &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install with &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yaml&#xA;conda activate manimate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ’ƒ Inference&lt;/h2&gt; &#xA;&lt;p&gt;Run inference on single GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/animate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run inference with multiple GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/animate_dist.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ¨ Gradio Demo&lt;/h2&gt; &#xA;&lt;h4&gt;Online Gradio Demo:&lt;/h4&gt; &#xA;&lt;p&gt;Try our &lt;a href=&#34;https://huggingface.co/spaces/zcxu-eric/magicanimate&#34;&gt;online gradio demo&lt;/a&gt; quickly.&lt;/p&gt; &#xA;&lt;h4&gt;Local Gradio Demo:&lt;/h4&gt; &#xA;&lt;p&gt;Launch local gradio demo on single GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m demo.gradio_animate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch local gradio demo if you have multiple GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m demo.gradio_animate_dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open gradio demo in local browser.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ™ Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank &lt;a href=&#34;https://twitter.com/_akhaliq?lang=en&#34;&gt;AK(@_akhaliq)&lt;/a&gt; and huggingface team for the help of setting up oneline gradio demo.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this codebase useful for your research, please use the following entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@inproceedings{xu2023magicanimate,&#xA;    author    = {Xu, Zhongcong and Zhang, Jianfeng and Liew, Jun Hao and Yan, Hanshu and Liu, Jia-Wei and Zhang, Chenxu and Feng, Jiashi and Shou, Mike Zheng},&#xA;    title     = {MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model},&#xA;    booktitle = {arXiv},&#xA;    year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/modelscope-agent</title>
    <updated>2023-12-07T01:38:50Z</updated>
    <id>tag:github.com,2023-12-07:/modelscope/modelscope-agent</id>
    <link href="https://github.com/modelscope/modelscope-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ModelScope-Agent: An agent framework connecting models in ModelScope with the world&lt;/p&gt;&lt;hr&gt;&lt;h1&gt; ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://modelscope.cn/home&#34;&gt;Modelscope Hub&lt;/a&gt; ï½œ &lt;a href=&#34;https://arxiv.org/abs/2309.00986&#34;&gt;Paper&lt;/a&gt; ï½œ &lt;a href=&#34;https://modelscope.cn/studios/damo/ModelScopeGPT/summary&#34;&gt;Demo&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/README_CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&amp;nbsp; ï½œ &amp;amp;nbspEnglish &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;ModelScope-Agent&lt;/strong&gt;, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library that are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;customizable and comprehensive framework&lt;/strong&gt;: customizable engine design to spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;opensourced LLMs as controllers&lt;/strong&gt;: support model training on multiple open-source LLMs of ModelScope Community&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diversified and Comprehensive APIs&lt;/strong&gt;: enabling seamless integration with both model APIs and common APIs in a unified way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/modelscope-agent.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nov 26, 2023: &lt;a href=&#34;https://github.com/modelscope/modelscope-agent/tree/master/apps/agentfabric&#34;&gt;AgentFabric&lt;/a&gt; now supports collaborative use in ModelScope&#39;s &lt;a href=&#34;https://modelscope.cn/studios/modelscope/AgentFabric/summary&#34;&gt;Creation Space&lt;/a&gt;, allowing for the sharing of custom applications in the Creation Space. The update also includes the latest &lt;a href=&#34;https://modelscope.cn/models/damo/nlp_gte_sentence-embedding_chinese-base/summary&#34;&gt;GTE&lt;/a&gt; text embedding integration.&lt;/li&gt; &#xA; &lt;li&gt;Nov 17, 2023: &lt;a href=&#34;https://github.com/modelscope/modelscope-agent/tree/master/apps/agentfabric&#34;&gt;AgentFabric&lt;/a&gt; released, which is an interactive framework to facilitate creation of agents tailored to various real-world applications.&lt;/li&gt; &#xA; &lt;li&gt;Oct 30, 2023: &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/facechain_agent_studio/summary&#34;&gt;Facechain Agent&lt;/a&gt; released a local version of the Facechain Agent that can be run locally. For detailed usage instructions, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/#facechain-agent&#34;&gt;Facechain Agent&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Oct 25, 2023: &lt;a href=&#34;https://modelscope.cn/studios/damo/story_agent/summary&#34;&gt;Story Agent&lt;/a&gt; released a local version of the Story Agent for generating storybook illustrations. It can be run locally. For detailed usage instructions, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/#story-agent&#34;&gt;Story Agent&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Sep 20, 2023: &lt;a href=&#34;https://modelscope.cn/studios/damo/ModelScopeGPT/summary&#34;&gt;ModelScope GPT&lt;/a&gt; offers a local version through gradio that can be run locally. You can navigate to the demo/msgpt/ directory and execute &lt;code&gt;bash run_msgpt.sh&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Sep 4, 2023: Three demos, &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/demo_qwen_agent.ipynb&#34;&gt;demo_qwen&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/demo_retrieval_agent.ipynb&#34;&gt;demo_retrieval_agent&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/demo_register_new_tool.ipynb&#34;&gt;demo_register_tool&lt;/a&gt;, have been added, along with detailed tutorials provided.&lt;/li&gt; &#xA; &lt;li&gt;Sep 2, 2023: The &lt;a href=&#34;https://arxiv.org/abs/2309.00986&#34;&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/li&gt; &#xA; &lt;li&gt;Aug 22, 2023: Support accessing various AI model APIs using ModelScope tokens.&lt;/li&gt; &#xA; &lt;li&gt;Aug 7, 2023: The initial version of the modelscope-agent repository was released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;clone repo and install dependencyï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/modelscope/modelscope-agent.git&#xA;cd modelscope-agent &amp;amp;&amp;amp; pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ModelScope notebookã€recommendedã€‘&lt;/h3&gt; &#xA;&lt;p&gt;The ModelScope Notebook offers a free-tier that allows ModelScope user to run the FaceChain application with minimum setup, refer to &lt;a href=&#34;https://modelscope.cn/my/mynotebook/preset&#34;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1: æˆ‘çš„notebook -&amp;gt; PAI-DSW -&amp;gt; GPUç¯å¢ƒ&#xA;&#xA;# Step2: Download the [demo file](https://github.com/modelscope/modelscope-agent/blob/master/demo/demo_qwen_agent.ipynb) and upload it to the GPU.&#xA;&#xA;# Step3:  Execute the demo notebook in order.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To use modelscope-agent, all you need is to instantiate an &lt;code&gt;AgentExecutor&lt;/code&gt; object, and use &lt;code&gt;run()&lt;/code&gt; to execute your task. For faster agent implementation, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/demo_qwen_agent.ipynb&#34;&gt;demo_agent&lt;/a&gt;. Online demo is available on &lt;a href=&#34;https://modelscope.cn/studios/damo/ModelScopeGPT/summary&#34;&gt;ModelScope&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import os&#xA;from modelscope.utils.config import Config&#xA;from modelscope_agent.llm import LLMFactory&#xA;from modelscope_agent.agent import AgentExecutor&#xA;from modelscope_agent.prompt import MSPromptGenerator&#xA;&#xA;# get cfg from file, refer the example in config folder&#xA;model_cfg_file = os.getenv(&#39;MODEL_CONFIG_FILE&#39;, &#39;config/cfg_model_template.json&#39;)&#xA;model_cfg = Config.from_file(model_cfg_file)&#xA;tool_cfg_file = os.getenv(&#39;TOOL_CONFIG_FILE&#39;, &#39;config/cfg_tool_template.json&#39;)&#xA;tool_cfg = Config.from_file(tool_cfg_file)&#xA;&#xA;&#xA;# instantiation LLM&#xA;model_name = &#39;modelscope-agent-7b&#39;&#xA;llm = LLMFactory.build_llm(model_name, model_cfg)&#xA;&#xA;# prompt generator&#xA;prompt_generator = MSPromptGenerator()&#xA;&#xA;# instantiation agent&#xA;agent = AgentExecutor(llm, tool_cfg, prompt_generator=prompt_generator)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single-step &amp;amp; Multi-step tool-use&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Single-step tool-use&#xA;agent.run(&#39;ä½¿ç”¨åœ°å€è¯†åˆ«æ¨¡å‹ï¼Œä»ä¸‹é¢çš„åœ°å€ä¸­æ‰¾åˆ°çœå¸‚åŒºç­‰å…ƒç´ ï¼Œåœ°å€ï¼šæµ™æ±Ÿæ­å·å¸‚æ±Ÿå¹²åŒºä¹å ¡é•‡ä¸‰æ‘æ‘ä¸€åŒº&#39;, remote=True)&#xA;&#xA;# Multi-step tool-use&#xA;agent.reset()&#xA;agent.run(&#39;å†™ä¸€ç¯‡å…³äºVision Pro VRçœ¼é•œçš„20å­—å®£ä¼ æ–‡æ¡ˆï¼Œå¹¶ç”¨å¥³å£°è¯»å‡ºæ¥ï¼ŒåŒæ—¶ç”Ÿæˆä¸ªè§†é¢‘çœ‹çœ‹&#39;, remote=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div style=&#34;display: flex;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/modelscopegpt_case_single-step.png&#34; alt=&#34;Image 1&#34; style=&#34;width: 45%;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/modelscopegpt_case_video-generation.png&#34; alt=&#34;Image 2&#34; style=&#34;width: 45%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-turn tool-use and knowledge-qa&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Multi-turn tool-use&#xA;agent.reset()&#xA;agent.run(&#39;å†™ä¸€ä¸ª20å­—å·¦å³ç®€çŸ­çš„å°æ•…äº‹&#39;, remote=True)&#xA;agent.run(&#39;ç”¨å¥³å£°å¿µå‡ºæ¥&#39;, remote=True)&#xA;agent.run(&#39;ç»™è¿™ä¸ªæ•…äº‹é…ä¸€å¼ å›¾&#39;, remote=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div style=&#34;display: flex;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/modelscopegpt_case_multi-turn.png&#34; alt=&#34;Image 1&#34; style=&#34;width: 45%;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/modelscopegpt_case_knowledge-qa.png&#34; alt=&#34;Image 2&#34; style=&#34;width: 45%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Main components&lt;/h3&gt; &#xA;&lt;p&gt;An &lt;code&gt;AgentExecutor&lt;/code&gt; object consists of the following components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;LLM&lt;/code&gt;: A large language model that is responsible to process your inputs and decide calling tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tool_list&lt;/code&gt;: A list consists of available tools for agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PromptGenerator&lt;/code&gt;: A module integrates &lt;code&gt;prompt_template&lt;/code&gt;, &lt;code&gt;user_input&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt;, &lt;code&gt;tool_list&lt;/code&gt;... into efficient prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OutputParser&lt;/code&gt;: A module to parse llm response into the tools to be invoked and the corresponding parameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide default implement of these components for users, but you can also custom your components according to your requirement.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Some configurations, &lt;code&gt;user_token&lt;/code&gt; etc are not supposed to be public, so we recommend you to use &lt;code&gt;dotenv&lt;/code&gt; package and &lt;code&gt;.env&lt;/code&gt; file to set these configurations.&lt;/p&gt; &#xA;&lt;p&gt;Concretely, We provide an &lt;code&gt;.env.template&lt;/code&gt; file and corresponding config files in our repo. You can easily customize the configuration by referring to the provided example, and utilize your own &lt;code&gt;.env&lt;/code&gt; file to read the configuration settings.&lt;/p&gt; &#xA;&lt;h3&gt;LLM&lt;/h3&gt; &#xA;&lt;p&gt;We offer a plug-and-play LLM for users to easily utilize. The specific model details are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;modelscope-agent-7b: &lt;a href=&#34;https://modelscope.cn/models/damo/ModelScope-Agent-7B/summary&#34;&gt;modelscope-agent-7b&lt;/a&gt; is a core open-source model that drives the ModelScope-Agent framework. It can be directly downloaded for local use.&lt;/li&gt; &#xA; &lt;li&gt;modelscope-agent: A ModelScope-Agent service deployed on &lt;a href=&#34;http://dashscope.aliyun.com&#34;&gt;DashScope&lt;/a&gt;. No local GPU resources are required. Follow the steps below to apply for the use of modelscope-agent: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Apply to activate the DashScope service, go to &lt;code&gt;æ¨¡å‹å¹¿åœº&lt;/code&gt; -&amp;gt; &lt;code&gt;é€šä¹‰åƒé—®å¼€æºç³»åˆ—&lt;/code&gt; -&amp;gt; apply for a trial of &lt;code&gt;é€šä¹‰åƒé—®7B&lt;/code&gt;. The free quota is 100,000 tokens.&lt;/li&gt; &#xA;   &lt;li&gt;Create an API-KEY in &lt;code&gt;API-kEYç®¡ç†&lt;/code&gt;, and configure it in the &lt;code&gt;config/.env&lt;/code&gt; file.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The default LLM is &lt;code&gt;ModelScope GPT&lt;/code&gt;, which is deployed in a remote server and need user token to request.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use other llm, you can inherit base class and implement &lt;code&gt;generate()&lt;/code&gt; or &lt;code&gt;stream_generate()&lt;/code&gt; specifically.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;generate()&lt;/code&gt;: directly return final response&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stream_generate()&lt;/code&gt;: return a generator of step response, it can be used when you deploy your application in gradio.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also use open-source LLM from ModelScope or Huggingface and inference locally by &lt;code&gt;LLMFactory&lt;/code&gt; class. Moreover, you can finetune these models with your datasets or load your custom weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# local llm cfg&#xA;import os&#xA;from modelscope.utils.config import Config&#xA;from modelscope_agent.llm import LLMFactory&#xA;from modelscope_agent.agent import AgentExecutor&#xA;&#xA;model_name = &#39;modelscope-agent-7b&#39;&#xA;model_cfg = {&#xA;    &#39;modelscope-agent-7b&#39;:{&#xA;        &#39;type&#39;: &#39;modelscope&#39;,&#xA;        &#39;model_id&#39;: &#39;damo/ModelScope-Agent-7B&#39;,&#xA;        &#39;model_revision&#39;: &#39;v1.0.0&#39;,&#xA;        &#39;use_raw_generation_config&#39;: True,&#xA;        &#39;custom_chat&#39;: True&#xA;    }&#xA;}&#xA;&#xA;tool_cfg_file = os.getenv(&#39;TOOL_CONFIG_FILE&#39;, &#39;config/cfg_tool_template.json&#39;)&#xA;tool_cfg = Config.from_file(tool_cfg_file)&#xA;&#xA;llm = LLMFactory(model_name, model_cfg)&#xA;agent = AgentExecutor(llm, tool_cfg)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Custom tools&lt;/h3&gt; &#xA;&lt;p&gt;We provide some default pipeline tools of multiple domain that integrates in modelscope.&lt;/p&gt; &#xA;&lt;p&gt;Also, you can custom your tools by inheriting base tool and define names, descriptions, and parameters according to pre-defined schema. And you can implement &lt;code&gt;_local_call()&lt;/code&gt; or &lt;code&gt;_remote_call()&lt;/code&gt; according to your requirement. Examples of supported tool are provided below. For more detailed tool registration, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/docs/modules/tool.md&#34;&gt;tool_doc&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/demo_register_new_tool.ipynb&#34;&gt;too_demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text-to-Speech Tool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modelscope_agent.tools import ModelscopePipelineTool&#xA;from modelscope.utils.constant import Tasks&#xA;from modelscope_agent.output_wrapper import AudioWrapper&#xA;&#xA;class TexttoSpeechTool(ModelscopePipelineTool):&#xA;    default_model = &#39;damo/speech_sambert-hifigan_tts_zh-cn_16k&#39;&#xA;    description = &#39;æ–‡æœ¬è½¬è¯­éŸ³æœåŠ¡ï¼Œå°†æ–‡å­—è½¬æ¢ä¸ºè‡ªç„¶è€Œé€¼çœŸçš„è¯­éŸ³ï¼Œå¯é…ç½®ç”·å£°/å¥³å£°&#39;&#xA;    name = &#39;modelscope_speech-generation&#39;&#xA;    parameters: list = [{&#xA;        &#39;name&#39;: &#39;input&#39;,&#xA;        &#39;description&#39;: &#39;è¦è½¬æˆè¯­éŸ³çš„æ–‡æœ¬&#39;,&#xA;        &#39;required&#39;: True&#xA;    }, {&#xA;        &#39;name&#39;: &#39;gender&#39;,&#xA;        &#39;description&#39;: &#39;ç”¨æˆ·èº«ä»½&#39;,&#xA;        &#39;required&#39;: True&#xA;    }]&#xA;    task = Tasks.text_to_speech&#xA;&#xA;    def _remote_parse_input(self, *args, **kwargs):&#xA;        if &#39;gender&#39; not in kwargs:&#xA;            kwargs[&#39;gender&#39;] = &#39;man&#39;&#xA;        voice = &#39;zhibei_emo&#39; if kwargs[&#39;gender&#39;] == &#39;man&#39; else &#39;zhiyan_emo&#39;&#xA;        kwargs[&#39;parameters&#39;] = voice&#xA;        kwargs.pop(&#39;gender&#39;)&#xA;        return kwargs&#xA;&#xA;    def _parse_output(self, origin_result, remote=True):&#xA;&#xA;        audio = origin_result[&#39;output_wav&#39;]&#xA;        return {&#39;result&#39;: AudioWrapper(audio)}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text-Address Tool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modelscope_agent.tools import ModelscopePipelineTool&#xA;from modelscope.utils.constant import Tasks&#xA;&#xA;class TextAddressTool(ModelscopePipelineTool):&#xA;    default_model = &#39;damo/mgeo_geographic_elements_tagging_chinese_base&#39;&#xA;    description = &#39;åœ°å€è§£ææœåŠ¡ï¼Œé’ˆå¯¹ä¸­æ–‡åœ°å€ä¿¡æ¯ï¼Œè¯†åˆ«å‡ºé‡Œé¢çš„å…ƒç´ ï¼ŒåŒ…æ‹¬çœã€å¸‚ã€åŒºã€é•‡ã€ç¤¾åŒºã€é“è·¯ã€è·¯å·ã€POIã€æ¥¼æ ‹å·ã€æˆ·å®¤å·ç­‰&#39;&#xA;    name = &#39;modelscope_text-address&#39;&#xA;    parameters: list = [{&#xA;        &#39;name&#39;: &#39;input&#39;,&#xA;        &#39;description&#39;: &#39;ç”¨æˆ·è¾“å…¥çš„åœ°å€ä¿¡æ¯&#39;,&#xA;        &#39;required&#39;: True&#xA;    }]&#xA;    task = Tasks.token_classification&#xA;&#xA;    def _parse_output(self, origin_result, *args, **kwargs):&#xA;        final_result = {}&#xA;        for e in origin_result[&#39;output&#39;]:&#xA;            final_result[e[&#39;type&#39;]] = e[&#39;span&#39;]&#xA;        return final_result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Moreover, if the tool is a &lt;code&gt;langchain tool&lt;/code&gt;, you can directly use our &lt;code&gt;LangchainTool&lt;/code&gt; to wrap and adapt with current frameworks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;&#xA;from modelscope_agent.tools import LangchainTool&#xA;from langchain.tools import ShellTool, ReadFileTool&#xA;&#xA;# wrap langchain tools&#xA;shell_tool = LangchainTool(ShellTool())&#xA;&#xA;print(shell_tool(commands=[&#34;echo &#39;Hello World!&#39;&#34;, &#34;ls&#34;]))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Framework&lt;/h2&gt; &#xA;&lt;p&gt;We provide a training framework in the &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/demo/tool_agent_finetune_swift&#34;&gt;demo/tool_agent_finetune_swift&lt;/a&gt;, which mainly integrates the &lt;a href=&#34;https://github.com/modelscope/swift&#34;&gt;SWIFT&lt;/a&gt; training framework from ModelScope. Additionally, we release a large-scale tool instruction fine-tuning dataset MSAgent-Bench.&lt;/p&gt; &#xA;&lt;h3&gt;MSAgent-Bench&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://modelscope.cn/datasets/damo/MSAgent-Bench/summary&#34;&gt;MSAgent-Bench&lt;/a&gt;, which is a comprehensive tool dataset encompassing 598k dialogues, including Common API, Model API, API-Oriented QA, and API-agnostic Instructions. You can directly download it on the dataset &lt;a href=&#34;https://modelscope.cn/datasets/damo/MSAgent-Bench/files&#34;&gt;link&lt;/a&gt; or access it through sdk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modelscope.msdatasets import MsDataset&#xA;&#xA;ds = MsDataset.load(&#39;damo/MSAgent-Bench&#39;, split=&#39;train&#39;)&#xA;one_ds = next(iter(ds))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope-agent/master/resource/MSAgent-Bench.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;The training samples are organized in the following format, the content included between &amp;lt;|startofthink|&amp;gt; and &amp;lt;|endofthink|&amp;gt; represents the generated API request. The content included between &amp;lt;|startofexec|&amp;gt; and &amp;lt;|endofexec|&amp;gt; represents the API execution result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;:&#34;MS_Agent_Bench_20&#34;,&#xA;    &#34;conversations&#34;:[&#xA;        {&#xA;            &#34;from&#34;:&#34;system&#34;,&#xA;            &#34;value&#34;: &#34;ä½ æ˜¯è¾¾æ‘©é™¢xxxx&#34;&#xA;        },&#xA;        {&#xA;            &#34;from&#34;:&#34;user&#34;,&#xA;            &#34;value&#34;:&#34;æŒ‰ç…§ç»™å®šçš„schemaæŠ½å–å‡ºä¸‹é¢æ–‡æœ¬å¯¹åº”çš„ä¿¡æ¯\nschemaï¼š{\&#34;äººç‰©\&#34;: null, \&#34;åœ°ç†ä½ç½®\&#34;: null, \&#34;ç»„ç»‡æœºæ„\&#34;: null}\nè¿‘æ—¥ï¼Œç¾å›½æ”¿åºœå®£å¸ƒå°†å¯¹ä¸­å›½1000å¤šç§å•†å“åŠ å¾å…³ç¨ï¼Œå¹¶å¨èƒè¿›ä¸€æ­¥åŠ å¾å…³ç¨ã€‚&#34;&#xA;        },&#xA;        {&#xA;            &#34;from&#34;:&#34;assistant&#34;,&#xA;            &#34;value&#34;:&#34;&amp;lt;|startofthink|&amp;gt;```JSON\n{\&#34;api_name\&#34;: \&#34;modelscope_text-ie\&#34;, \&#34;url\&#34;: \&#34;http://9.32.64.200:5873/damo/nlp_structbert_siamese-uie_chinese-base\&#34;, \&#34;parameters\&#34;: {\&#34;text\&#34;: \&#34;è¿‘æ—¥ï¼Œç¾å›½æ”¿åºœå®£å¸ƒå°†å¯¹ä¸­å›½1000å¤šç§å•†å“åŠ å¾å…³ç¨ï¼Œå¹¶å¨èƒè¿›ä¸€æ­¥åŠ å¾å…³ç¨ã€‚\&#34;, \&#34;schema\&#34;: \&#34;{\\\&#34;äººç‰©\\\&#34;: null, \\\&#34;åœ°ç†ä½ç½®\\\&#34;: null, \\\&#34;ç»„ç»‡æœºæ„\\\&#34;: null}\&#34;}}\n```&amp;lt;|endofthink|&amp;gt;\n\n&amp;lt;|startofexec|&amp;gt;```JSON\n{\&#34;äººç‰©\&#34;: [], \&#34;åœ°ç†ä½ç½®\&#34;: [\&#34;ä¸­å›½\&#34;, \&#34;ç¾å›½\&#34;], \&#34;ç»„ç»‡æœºæ„\&#34;: []}\n```&amp;lt;|endofexec|&amp;gt;\nä¿¡æ¯æŠ½å–ç»“æœï¼š{\&#34;äººç‰©\&#34;: [], \&#34;åœ°ç†ä½ç½®\&#34;: [\&#34;ä¸­å›½\&#34;, \&#34;ç¾å›½\&#34;], \&#34;ç»„ç»‡æœºæ„\&#34;: []}ã€‚&#34;&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Execute the training script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd demo/tool_agent_finetune_swift&#xA;PYTHONPATH=./ bash scripts/train/run_qwen_ddp.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to learn more about the practical details of Agent, you can refer to our articles and video tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/L3GiV2QHeybhVZSg_g_JRw&#34;&gt;Article Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://b23.tv/AGIzmHM&#34;&gt;Video Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Share Your Agent&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate your enthusiasm in participating in our open-source ModelScope-Agent project. If you encounter any issues, please feel free to report them to us. If you have built a new Agent demo and are ready to share your work with us, please create a pull request at any time! If you need any further assistance, please contact us via email at &lt;a href=&#34;mailto:contact@modelscope.cn&#34;&gt;contact@modelscope.cn&lt;/a&gt; or &lt;a href=&#34;https://modelscope.cn/docs/%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC&#34;&gt;communication group&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h3&gt;Facechain Agent&lt;/h3&gt; &#xA;&lt;p&gt;Facechain is an open-source project for generating personalized portraits in various styles using facial images uploaded by users. By integrating the capabilities of Facechain into the modelscope-agent framework, we have greatly simplified the usage process. The generation of personalized portraits can now be done through dialogue with the Facechain Agent.&lt;/p&gt; &#xA;&lt;p&gt;FaceChainAgent Studio Application Link: &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/facechain_agent_studio/summary&#34;&gt;https://modelscope.cn/studios/CVstudio/facechain_agent_studio/summary&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can run it directly in a notebook/Colab/local environment: &lt;a href=&#34;https://www.modelscope.cn/my/mynotebook&#34;&gt;https://www.modelscope.cn/my/mynotebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;! git clone -b feat/facechain_agent https://github.com/modelscope/modelscope-agent.git&#xA;&#xA;! cd modelscope-agent &amp;amp;&amp;amp; ! pip install -r requirements.txt&#xA;! cd modelscope-agent/demo/facechain_agent/demo/facechain_agent &amp;amp;&amp;amp; ! pip install -r requirements.txt&#xA;! pip install http://dashscope-cn-beijing.oss-cn-beijing.aliyuncs.com/zhicheng/modelscope_agent-0.1.0-py3-none-any.whl&#xA;! PYTHONPATH=/mnt/workspace/modelscope-agent/demo/facechain_agent &amp;amp;&amp;amp; cd modelscope-agent/demo/facechain_agent/demo/facechain_agent &amp;amp;&amp;amp; python app_v1.0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Story Agent&lt;/h3&gt; &#xA;&lt;p&gt;The Story Agent is an open-source intelligent agent for generating storybooks. Users can create a storybook through dialogue with the agent, and the agent will intelligently guide the user through the entire creation process.&lt;/p&gt; &#xA;&lt;p&gt;StoryAgent Studio Application Link: &lt;a href=&#34;https://modelscope.cn/studios/damo/story_agent/summary&#34;&gt;https://modelscope.cn/studios/damo/story_agent/summary&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also run it directly in a notebook: &lt;a href=&#34;https://www.modelscope.cn/my/mynotebook&#34;&gt;https://www.modelscope.cn/my/mynotebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;! git clone -b feat/story_agent_gradio https://github.com/modelscope/modelscope-agent.git&#xA;&#xA;import os&#xA;os.environ[&#39;DASHSCOPE_API_KEY&#39;] = &#39;yours api-key&#39;&#xA;#DASHSCOPE_API_KEYå¯ä»¥ä»dashscopeç½‘ç«™ https://dashscope.console.aliyun.com/apiKeyè·å–&#xA;! cd modelscope-agent &amp;amp;&amp;amp; ! pip install -r requirements.txt&#xA;! cd modelscope-agent/demo/story_agent &amp;amp;&amp;amp; ! pip install -r requirement_gr.txt&#xA;! cd modelscope-agent/demo/story_agent &amp;amp;&amp;amp; ! sh run_story_agent.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this work useful, consider giving this repository a star and citing our paper as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{li2023modelscopeagent,&#xA;      title={ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models},&#xA;      author={Chenliang Li and Hehong Chen and Ming Yan and Weizhou Shen and Haiyang Xu and Zhikai Wu and Zhicheng Zhang and Wenmeng Zhou and Yingda Chen and Chen Cheng and Hongzhu Shi and Ji Zhang and Fei Huang and Jingren Zhou},&#xA;      year={2023},&#xA;      eprint={2309.00986},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>chrisconlan/algorithmic-trading-with-python</title>
    <updated>2023-12-07T01:38:50Z</updated>
    <id>tag:github.com,2023-12-07:/chrisconlan/algorithmic-trading-with-python</id>
    <link href="https://github.com/chrisconlan/algorithmic-trading-with-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Source code for Algorithmic Trading with Python (2020) by Chris Conlan&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Algorithmic Trading with Python&lt;/h1&gt; &#xA;&lt;p&gt;Source code for Algorithmic Trading with Python (2020) by Chris Conlan.&lt;/p&gt; &#xA;&lt;p&gt;Paperback available for purchase &lt;a href=&#34;https://amzn.to/2UZbHuA&#34;&gt;on Amazon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Useful resources&lt;/h4&gt; &#xA;&lt;p&gt;These stand-alone resources can be useful to researchers with or without the accompanying book. The rest of the material in this repository depends on explanation and context given in the book.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance metrics used to evaluate trading strategies: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/metrics.py&#34;&gt;metrics.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Common technical indicators in pure Pandas: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/indicators.py&#34;&gt;indicators.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Converting common technical indicators into ternary signals: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/signals.py&#34;&gt;signals.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Generic grid search wrapper for numeric optimization: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/optimization.py&#34;&gt;optimization.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Object-oriented building blocks for portfolio simulation: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/portfolio.py&#34;&gt;portfolio.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Generic wrapper for multi-core repeated K fold cross-validation: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/src/pypm/ml_model/model.py&#34;&gt;model.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Free-to-use simulated EOD stock data and alternative data streams: &lt;a href=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/data&#34;&gt;data&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chrisconlan/algorithmic-trading-with-python/master/cover.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>