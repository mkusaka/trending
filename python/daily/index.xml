<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-05T01:35:10Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/adk-samples</title>
    <updated>2025-08-05T01:35:10Z</updated>
    <id>tag:github.com,2025-08-05:/google/adk-samples</id>
    <link href="https://github.com/google/adk-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of sample agents built with Agent Development (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/adk-samples/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png&#34; alt=&#34;Agent Development Kit Logo&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href=&#34;https://google.github.io/adk-docs/&#34;&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; &#xA;&lt;h2&gt;✨ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains ADK sample agents for both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/adk-samples/main/python/&#34;&gt;Python&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/adk-samples/main/java/&#34;&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; &#xA;&lt;p&gt;To learn more, check out the &lt;a href=&#34;https://google.github.io/adk-docs/&#34;&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for &lt;a href=&#34;https://github.com/google/adk-python&#34;&gt;ADK Python&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/adk-java&#34;&gt;ADK Java&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🌳 Repository Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;├── java&#xA;│&amp;nbsp;&amp;nbsp; ├── agents&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── software-bug-assistant&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── time-series-forecasting&#xA;│&amp;nbsp;&amp;nbsp; └── README.md&#xA;├── python&#xA;│&amp;nbsp;&amp;nbsp; ├── agents&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── academic-research&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── brand-search-optimization&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── camel&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── customer-service&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── data-science&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── financial-advisor&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── fomc-research&#xA;│   │   ├── gemini-fullstack&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── image-scoring&#xA;│   │   ├── llm-auditor&#xA;│   │   ├── machine-learning-engineering&#xA;│   │   ├── marketing-agency&#xA;│   │   ├── personalized-shopping&#xA;│   │   ├── RAG&#xA;│   │   ├── README.md&#xA;│   │   ├── software-bug-assistant  &#xA;│   │   └── travel-concierge&#xA;│   └── README.md&#xA;└── README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ℹ️ Getting help&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href=&#34;https://github.com/google/adk-samples/issues&#34;&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! Whether it&#39;s bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href=&#34;https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md&#34;&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;📄 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href=&#34;https://github.com/google/adk-samples/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimers&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href=&#34;https://bughunters.google.com/open-source-security&#34;&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>resemble-ai/chatterbox</title>
    <updated>2025-08-05T01:35:10Z</updated>
    <id>tag:github.com,2025-08-05:/resemble-ai/chatterbox</id>
    <link href="https://github.com/resemble-ai/chatterbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;1200&#34; alt=&#34;cb-big2&#34; src=&#34;https://github.com/user-attachments/assets/bd8c5f03-e91d-4ee5-b680-57355da204d1&#34;&gt; &#xA;&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://resemble-ai.github.io/chatterbox_demopage/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/listen-demo_samples-blue&#34; alt=&#34;Alt Text&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ResembleAI/Chatterbox&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Alt Text&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://podonos.com/resembleai/chatterbox&#34;&gt;&lt;img src=&#34;https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true&#34; alt=&#34;Alt Text&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/rJq9cRJBJ6&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;_Made with ♥️ by &lt;a href=&#34;https://resemble.ai&#34; target=&#34;_blank&#34;&gt;&lt;img width=&#34;100&#34; alt=&#34;resemble-logo-horizontal&#34; src=&#34;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re excited to introduce Chatterbox, &lt;a href=&#34;https://resemble.ai&#34;&gt;Resemble AI&#39;s&lt;/a&gt; first production-grade open source TTS model. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; &#xA;&lt;p&gt;Whether you&#39;re working on memes, videos, games, or AI agents, Chatterbox brings your content to life. It&#39;s also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt;, a powerful feature that makes your voices stand out. Try it now on our &lt;a href=&#34;https://huggingface.co/spaces/ResembleAI/Chatterbox&#34;&gt;Hugging Face Gradio app.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&#34;https://resemble.ai&#34;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms—ideal for production use in agents, applications, or interactive media.&lt;/p&gt; &#xA;&lt;h1&gt;Key Details&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SoTA zeroshot TTS&lt;/li&gt; &#xA; &lt;li&gt;0.5B Llama backbone&lt;/li&gt; &#xA; &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; &#xA; &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; &#xA; &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; &#xA; &lt;li&gt;Watermarked outputs&lt;/li&gt; &#xA; &lt;li&gt;Easy voice conversion script&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://podonos.com/resembleai/chatterbox&#34;&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tips&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts.&lt;/li&gt; &#xA;   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; &#xA;   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install chatterbox-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# conda create -yn chatterbox python=3.11&#xA;# conda activate chatterbox&#xA;&#xA;git clone https://github.com/resemble-ai/chatterbox.git&#xA;cd chatterbox&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debain 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torchaudio as ta&#xA;from chatterbox.tts import ChatterboxTTS&#xA;&#xA;model = ChatterboxTTS.from_pretrained(device=&#34;cuda&#34;)&#xA;&#xA;text = &#34;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#39;s Nexus in an epic late-game pentakill.&#34;&#xA;wav = model.generate(text)&#xA;ta.save(&#34;test-1.wav&#34;, wav, model.sr)&#xA;&#xA;# If you want to synthesize with a different voice, specify the audio prompt&#xA;AUDIO_PROMPT_PATH = &#34;YOUR_FILE.wav&#34;&#xA;wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)&#xA;ta.save(&#34;test-2.wav&#34;, wav, model.sr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; &#xA;&lt;h1&gt;Supported Lanugage&lt;/h1&gt; &#xA;&lt;p&gt;Currenlty only English.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning&#34;&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yl4579/HiFTNet&#34;&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama3&#34;&gt;Llama 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xingchensong/S3Tokenizer&#34;&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; &#xA;&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href=&#34;https://github.com/resemble-ai/perth&#34;&gt;Resemble AI&#39;s Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Watermark extraction&lt;/h2&gt; &#xA;&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import perth&#xA;import librosa&#xA;&#xA;AUDIO_PATH = &#34;YOUR_FILE.wav&#34;&#xA;&#xA;# Load the watermarked audio&#xA;watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)&#xA;&#xA;# Initialize watermarker (same as used for embedding)&#xA;watermarker = perth.PerthImplicitWatermarker()&#xA;&#xA;# Extract watermark&#xA;watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)&#xA;print(f&#34;Extracted watermark: {watermark}&#34;)&#xA;# Output: 0.0 (no watermark) or 1.0 (watermarked)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Official Discord&lt;/h1&gt; &#xA;&lt;p&gt;👋 Join us on &lt;a href=&#34;https://discord.gg/rJq9cRJBJ6&#34;&gt;Discord&lt;/a&gt; and let&#39;s build something awesome together!&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,&#xA;  author       = {{Resemble AI}},&#xA;  title        = {{Chatterbox-TTS}},&#xA;  year         = {2025},&#xA;  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},&#xA;  note         = {GitHub repository}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;Don&#39;t use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stanford-cs336/spring2025-lectures</title>
    <updated>2025-08-05T01:35:10Z</updated>
    <id>tag:github.com,2025-08-05:/stanford-cs336/spring2025-lectures</id>
    <link href="https://github.com/stanford-cs336/spring2025-lectures" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spring 2025 CS336 lectures&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the lecture materials for &#34;Stanford CS336: Language modeling from scratch&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Non-executable (ppt/pdf) lectures&lt;/h2&gt; &#xA;&lt;p&gt;Located in &lt;code&gt;nonexecutable/&lt;/code&gt;as PDFs&lt;/p&gt; &#xA;&lt;h2&gt;Executable lectures&lt;/h2&gt; &#xA;&lt;p&gt;Located as &lt;code&gt;lecture_*.py&lt;/code&gt; in the root directory&lt;/p&gt; &#xA;&lt;p&gt;You can compile a lecture by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python execute.py -m lecture_01&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which generates a &lt;code&gt;var/traces/lecture_01.json&lt;/code&gt; and caches any images as appropriate.&lt;/p&gt; &#xA;&lt;p&gt;However, if you want to run it on the cluster, you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    ./remote_execute.sh lecture_01&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which copies the files to our slurm cluster, runs it there, and copies the results back. You have to setup the appropriate environment and tweak some configs to make this work (these instructions are not complete).&lt;/p&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;p&gt;If you need to tweak the Javascript:&lt;/p&gt; &#xA;&lt;p&gt;Install (one-time):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    npm create vite@latest trace-viewer -- --template react&#xA;    cd trace-viewer&#xA;    npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Load a local server to view at &lt;code&gt;http://localhost:5173?trace=var/traces/sample.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deploy to the main website:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    cd trace-viewer&#xA;    npm run build&#xA;    git add dist/assets&#xA;    # then commit to the repo and it should show up on the website&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>