<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-09T01:34:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/picotron</title>
    <updated>2025-01-09T01:34:24Z</updated>
    <id>tag:github.com,2025-01-09:/huggingface/picotron</id>
    <link href="https://github.com/huggingface/picotron" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimalistic 4D-parallelism distributed training framework for education purpose&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;picotron&lt;/h1&gt; &#xA;&lt;p&gt;In the spirit of &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;NanoGPT&lt;/a&gt;, we created Picotron: The minimalist &amp;amp; most-hackable repository for pre-training Llama-like models with &lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;4D Parallelism&lt;/a&gt; (Data, Tensor, Pipeline, Context parallel). It is designed with simplicity and &lt;strong&gt;educational&lt;/strong&gt; purposes in mind, making it an excellent tool for learning and experimentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/picotron/main/assets/bani%C3%A8re.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The code itself is simple and readable: &lt;code&gt;train.py&lt;/code&gt;, &lt;code&gt;model.py&lt;/code&gt; and &lt;code&gt;[data|tensor|pipeline|context]_parallel.py&lt;/code&gt; are all under &lt;strong&gt;300&lt;/strong&gt; lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Performance is not the best but still under active development. We observed 38% MFU on a LLaMA-2-7B model using 64 H100 GPUs and nearly 50% MFU on the SmolLM-1.7B model with 8 H100 GPUs. Benchmarks will come soon&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tutorial videos&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A step by step tutorial on how to build Picotron distributed training framework form scratch: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL-_armZiJvAnhcRr6yTJ0__f3Oi-LLi9S&#34;&gt;Picotron tutorial (playlist)&lt;/a&gt; üé¨&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/picotron_tutorial&#34;&gt;Picotron tutorial (codebase)&lt;/a&gt; üë∑üèª‚Äç‚ôÇÔ∏è&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Get a HF token &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt; to download models from HuggingFace&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GPU&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# To create a config file in json format under tmp by default&#xA;python create_config.py --out_dir tmp --exp_name llama-1B --dp 8 --model_name HuggingFaceTB/SmolLM-1.7B --num_hidden_layers 15  --grad_acc_steps 32 --mbs 4 --seq_len 1024 --hf_token &amp;lt;HF_TOKEN&amp;gt;&#xA;&#xA;# Locally&#xA;torchrun --nproc_per_node 8 train.py --config tmp/llama-1B/config.json &#xA;&#xA;# 3D Parallelism&#xA;python create_config.py --out_dir tmp --dp 4 --tp 2 --pp 2 --pp_engine 1f1b --exp_name llama-7B --model_name meta-llama/Llama-2-7b-hf  --grad_acc_steps 32 --mbs 4 --seq_len 1024 --hf_token &amp;lt;HF_TOKEN&amp;gt;&#xA;&#xA;# Slurm&#xA;python submit_slurm_jobs.py --inp_dir tmp/llama-7B --qos high --hf_token &amp;lt;HF_TOKEN&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CPU (expect it to be slow)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 3D Parallelism on CPU&#xA;python create_config.py --out_dir tmp --exp_name llama-1B-cpu --dp 2 --tp 2 --pp 2 --pp_engine 1f1b --model_name HuggingFaceTB/SmolLM-1.7B --num_hidden_layers 5  --grad_acc_steps 2 --mbs 4 --seq_len 128 --hf_token &amp;lt;HF_TOKEN&amp;gt; --use_cpu&#xA;&#xA;# Locally&#xA;torchrun --nproc_per_node 8 train.py --config tmp/llama-1B-cpu/config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairscale&#34;&gt;FairScale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt&#34;&gt;LitGPT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>