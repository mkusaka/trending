<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-19T01:41:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wtfloris/hestia</title>
    <updated>2023-11-19T01:41:58Z</updated>
    <id>tag:github.com,2023-11-19:/wtfloris/hestia</id>
    <link href="https://github.com/wtfloris/hestia" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hestia scrapes real estate websites for new rental listings&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Hestia&lt;/h2&gt; &#xA;&lt;p&gt;Hestia scrapes real estate websites for new rental listings, and broadcasts the results via Telegram. Check out @hestia_homes_bot on Telegram: &lt;a href=&#34;https://t.me/hestia_homes_bot&#34;&gt;https://t.me/hestia_homes_bot&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>generatebio/chroma</title>
    <updated>2023-11-19T01:41:58Z</updated>
    <id>tag:github.com,2023-11-19:/generatebio/chroma</id>
    <link href="https://github.com/generatebio/chroma" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generative model for programmable protein design&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/assets/chroma_logo_outline.svg?sanitize=true&#34; width=&#34;280&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#get-started&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#sampling&#34;&gt;&lt;strong&gt;Sampling&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#design&#34;&gt;&lt;strong&gt;Design&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioners&#34;&gt;&lt;strong&gt;Conditioners&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chroma is a generative model for designing proteins &lt;strong&gt;programmatically&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Protein space is complex and hard to navigate. With Chroma, protein design problems are represented in terms of &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioners&#34;&gt;composable building blocks&lt;/a&gt; from which diverse, &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#sampling&#34;&gt;all-atom protein structures can be automatically generated&lt;/a&gt;. As a joint model of structure and sequence, Chroma can also be used for common protein modeling tasks such as &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#design&#34;&gt;generating sequences given backbones&lt;/a&gt;, packing side-chains, and scoring designs.&lt;/p&gt; &#xA;&lt;p&gt;We provide protein conditioners for a variety of constraints, including substructure, symmetry, shape, and neural-network predictions of some protein classes and annotations. We also provide an API for &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioners-api&#34;&gt;creating your own conditioners&lt;/a&gt; in a few lines of code.&lt;/p&gt; &#xA;&lt;p&gt;Internally, Chroma uses diffusion modeling, equivariant graph neural networks, and conditional random fields to efficiently sample all-atom structures with a complexity that is sub-quadratic in the number of residues. It can generate large complexes in a few minutes on a commodity GPU. You can read more about Chroma, including biophysical and crystallographic validation of some early designs, in our paper, &lt;a href=&#34;https://doi.org/10.1038/s41586-023-06728-8&#34;&gt;&lt;em&gt;Illuminating protein space with a programmable generative model&lt;/em&gt;. Nature 2023&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/assets/proteins.png&#34; alt=&#34;Generated protein examples&#34; width=&#34;700px&#34; align=&#34;middle&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; An API key is required to download and use the pretrained model weights. It can be obtained &lt;a href=&#34;https://chroma-weights.generatebiomedicines.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Colab Notebooks&lt;/strong&gt;. The quickest way to get started with Chroma is our Colab notebooks, which provide starting points for a variety of use cases in a preconfigured, in-browser environment&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/generatebio/chroma/blob/main/notebooks/ChromaDemo.ipynb&#34;&gt;Chroma Quickstart&lt;/a&gt;: GUI notebook demonstrating unconditional and conditional generation of proteins with Chroma.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/generatebio/chroma/blob/main/notebooks/ChromaAPI.ipynb&#34;&gt;Chroma API Tutorial&lt;/a&gt;: Code notebook demonstrating protein I/O, sampling, and design configuration directly in &lt;code&gt;python&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/generatebio/chroma/blob/main/notebooks/ChromaConditioners.ipynb&#34;&gt;Chroma Conditioner API Tutorial&lt;/a&gt;: A deeper dive under the hood for implementing new Chroma &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioner-api&#34;&gt;Conditioners&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyPi package&lt;/strong&gt;.You can install the latest release of Chroma with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install generate-chroma&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sampling&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unconditional monomer&lt;/strong&gt;. We provide a unified entry point to both unconditional and conditional protein design with the &lt;code&gt;Chroma.sample()&lt;/code&gt; method. When no conditioners are specified, we can sample a simple 200-amino acid monomeric protein with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chroma import Chroma&#xA;&#xA;chroma = Chroma()&#xA;protein = chroma.sample(chain_lengths=[200])&#xA;&#xA;protein.to(&#34;sample.cif&#34;)&#xA;display(protein)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generally, &lt;code&gt;Chroma.sample()&lt;/code&gt; takes as input design hyperparameters and &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioners&#34;&gt;Conditioners&lt;/a&gt; and outputs &lt;code&gt;Protein&lt;/code&gt; objects representing the all-atom structures of protein systems which can be loaded to and from disk in PDB or mmCIF formats.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unconditional complex&lt;/strong&gt;. To sample a complex instead of a monomer, we can simply do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chroma import Chroma&#xA;&#xA;chroma = Chroma()&#xA;protein = chroma.sample(chain_lengths=[100, 200])&#xA;&#xA;protein.to(&#34;sample-complex.cif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional complex&lt;/strong&gt;. We can further customize sampling towards design objectives via &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#conditioners&#34;&gt;Conditioners&lt;/a&gt; and sampling hyperparameters. For example, to sample a C3-symmetric homo-trimer with 100 residues per monomer, we can do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chroma import Chroma, conditioners&#xA;&#xA;chroma = Chroma()&#xA;conditioner = conditioners.SymmetryConditioner(G=&#34;C_3&#34;, num_chain_neighbors=2)&#xA;protein = chroma.sample(&#xA;    chain_lengths=[100],&#xA;    conditioner=conditioner,&#xA;    langevin_factor=8,&#xA;    inverse_temperature=8,&#xA;    sde_func=&#34;langevin&#34;,&#xA;    potts_symmetry_order=conditioner.potts_symmetry_order)&#xA;&#xA;protein.to(&#34;sample-C3.cif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because compositions of conditioners are conditioners, even relatively complex design problems can follow this basic usage pattern. See the &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#get-started&#34;&gt;demo notebooks&lt;/a&gt; and docstrings for more information on hyperparameters, conditioners, and starting points.&lt;/p&gt; &#xA;&lt;h2&gt;Design&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust design&lt;/strong&gt;. Chroma is a joint model of sequence and structure that uses a common graph neural network base architecture to parameterize both backbone generation and conditional sequence and sidechain generation. These sequence and sidechain decoders are &lt;em&gt;diffusion-aware&lt;/em&gt; in the sense that they have been trained to predict sequence and side chain not just for natural structures at diffusion time $t=0$ but also on noisy structures at all diffusion times $t \in [0,1]$. As a result, the $t$ hyperpameter of the design network provides a kind of tunable robustness via &lt;strong&gt;diffusion augmentation&lt;/strong&gt; in we trade off between how much the model attempts to design the backbone &lt;em&gt;exactly&lt;/em&gt; as specified (e.g. $t=0.0$) versus &lt;em&gt;robust&lt;/em&gt; design within a small neighborhood of nearby backbone conformations (e.g. $t=0.5$).&lt;/p&gt; &#xA;&lt;p&gt;While all results presented in the Chroma &lt;a href=&#34;https://doi.org/10.1038/s41586-023-06728-8&#34;&gt;publication&lt;/a&gt; were done with &lt;strong&gt;exact design&lt;/strong&gt; at $t=0.0$, we have found &lt;strong&gt;robust design&lt;/strong&gt; at times near $t=0.5$ frequently improves one-shot refolding while incurring only minor, often √Öngstrom-scale, relaxation adjustments to target backbones. When we compare the performance of these two design modes on our set of 50,000 unconditional backbones that were analyzed in the paper, we see very large improvements in refolding across both &lt;a href=&#34;https://github.com/google-deepmind/alphafold&#34;&gt;AlphaFold&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/esm&#34;&gt;ESMFold&lt;/a&gt; that stratifies well across protein length, percent helicity, or similarity to a known structure (See Chroma &lt;a href=&#34;https://doi.org/10.1038/s41586-023-06728-8&#34;&gt;Supplementary Figure 14&lt;/a&gt; for further context).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/assets/refolding.png&#34; alt=&#34;alt text&#34; width=&#34;700px&#34; align=&#34;middle&#34;&gt; &#xA;&lt;/div&gt;&#xA;&lt;br&gt; &#xA;&lt;p&gt;The value of diffusion time conditioning $t$ can be set via the &lt;code&gt;design_t&lt;/code&gt; parameter in &lt;code&gt;Chroma.sample&lt;/code&gt; and &lt;code&gt;Chroma.design&lt;/code&gt;. We find that for generated structures, $t = 0.5$ produces highly robust refolding results and is, therefore, the default setting. For experimentally-precise structures, $t = 0.0$ may be more appropriate, and values in between may provide a useful tradeoff between these two regimes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Design &lt;em&gt;a la carte&lt;/em&gt;&lt;/strong&gt;. Chroma&#39;s design network can be accessed separately to design, redesign, and pack arbitrary protein systems. Here we load a protein from the PDB and redesign as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Redesign a Protein&#xA;from chroma import Protein, Chroma&#xA;chroma = Chroma()&#xA;&#xA;protein = Protein(&#39;1GFP&#39;)&#xA;protein = chroma.design(protein)&#xA;&#xA;protein.to(&#34;1GFP-redesign.cif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clamped sub-sequence redesign is also available and compatible with a built-in selection algebra, along with position- and mutation-specific mask constraints as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Redesign a Protein&#xA;from chroma import Protein, Chroma&#xA;chroma = Chroma()&#xA;&#xA;protein = Protein(&#39;my_favorite_protein.cif&#39;) # PDB is fine too&#xA;protein = chroma.design(protein, design_selection=&#34;resid 20-50 around 5.0&#34;) #  5 angstrom bubble around indices 20-50&#xA;&#xA;protein.to(&#34;my_favorite_protein_redesign.cif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide more examples of design in the &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#get-started&#34;&gt;demo notebooks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Conditioners&lt;/h2&gt; &#xA;&lt;p&gt;Protein design with Chroma is &lt;strong&gt;programmable&lt;/strong&gt;. Our &lt;code&gt;Conditioner&lt;/code&gt; framework allows for automatic conditional sampling under arbitrary compositions of protein specifications, which can come in the forms of restraints (biasing the distribution of states) or constraints (directly restrict the domain of underlying sampling process); see Supplementary Appendix M in &lt;a href=&#34;https://doi.org/10.1038/s41586-023-06728-8&#34;&gt;our paper&lt;/a&gt;. We have pre-defined multiple conditioners, including for controlling substructure, symmetry, shape, semantics, and natural-language prompts (see &lt;code&gt;chroma.layers.structure.conditioners&lt;/code&gt;), which can be used in arbitrary combinations.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Conditioner&lt;/th&gt; &#xA;    &lt;th&gt;Class(es) in &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/chroma/layers/structure/conditioners.py&#34;&gt;&lt;code&gt;chroma.conditioners&lt;/code&gt;&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th&gt;Example applications&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Symmetry constraint&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SymmetryConditioner&lt;/code&gt;, &lt;code&gt;ScrewConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Large symmetric assemblies&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Substructure constraint&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SubstructureConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Substructure grafting, scaffold enforcement&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Shape restraint&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ShapeConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Molecular shape control&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Secondary structure&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ProClassConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Secondary-structure specification&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Domain classification&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ProClassConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Specification of class, such as Pfam, CATH, or Taxonomy&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Text caption&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ProCapConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Natural language prompting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Sequence&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SubsequenceConditioner&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Subsequence constraints.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;. The central idea of Conditioners is &lt;em&gt;composable state transformations&lt;/em&gt;, where each Conditioner is a function that modifies the state and/or energy of a protein system in a differentiable way (&lt;a href=&#34;https://doi.org/10.1038/s41586-023-06728-8&#34;&gt;Supplementary Appendix M&lt;/a&gt;). For example, to encode symmetry as a &lt;em&gt;constraint&lt;/em&gt; we can take as input the assymetric unit and tesselate it according to the desired symmetry group to output a protein system that is symmetric by construction. To encode something like a neural network restraint, we can adjust the total system energy by the negative log probability of the target condition. For both of these, we add on the diffusion energy to the output of the Conditioner(s) and then backpropagate the total energy through all intermediate transformations to compute the unconstrained forces that are compatible with generic sampling SDE such as annealed Langevin Dynamics.&lt;/p&gt; &#xA;&lt;p&gt;We schematize this overall Conditioners framework below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/assets/conditioners.png&#34; alt=&#34;alt text&#34; width=&#34;600px&#34; align=&#34;middle&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;figcaption&gt;&#xA;  &lt;i&gt;The &lt;code&gt;Conditioner&lt;/code&gt; class is the composable building block of protein design with Chroma.&lt;/i&gt;&#xA; &lt;/figcaption&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Conditioner API&lt;/h4&gt; &#xA;&lt;p&gt;It is simple to develop new conditioners. A &lt;code&gt;Conditioner&lt;/code&gt; is a Pytorch &lt;code&gt;nn.Module&lt;/code&gt; which takes in the system state - i.e. the structure, energy, and diffusion time - and outputs potentially updated structures and energies as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;class Conditioner(torch.nn.Module):&#xA;    &#34;&#34;&#34;A composable function for parameterizing protein design problems.&#xA;    &#34;&#34;&#34;&#xA;    def __init__(self, *args, **kwargs):&#xA;        super().__init__()&#xA;        # Setup your conditioner&#39;s hyperparameters&#xA;&#xA;    def forward(&#xA;        self,&#xA;        X: torch.Tensor,                # Input coordinates&#xA;        C: torch.LongTensor,            # Input chain map (for complexes)&#xA;        O: torch.Tensor,                # Input sequence (one-hot, not used)&#xA;        U: torch.Tensor,                # Input energy (one-hot, not used)&#xA;        t: Union[torch.Tensor, float],  # Diffusion time&#xA;    ):&#xA;        # Update the state, e.g. map from an unconstrained to constrained manifold&#xA;        X_update, C_update  = update_state(X, C, t)&#xA;&#xA;        # Update the energy, e.g. add a restraint potential&#xA;        U_update = U + update_energy(X, C, t)&#xA;        return X_update, C_update, O, U_update, t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Roughly speaking, &lt;code&gt;Conditioner&lt;/code&gt;s are composable by construction because their input and output type signatures are matched (i.e. they are an endomorphism). So we also simply build conditioners from conditioners by &#34;stacking&#34; them much as we would with traditional neural network layer developemnt. With the final &lt;code&gt;Conditioner&lt;/code&gt; as an input, &lt;code&gt;Chroma.sample()&lt;/code&gt; will then leverage Pytorch&#39;s automatic differentiation facilities to automaticallly furnish a diffusion-annealed MCMC sampling algorithm to sample with this conditioner (We note this isn&#39;t magic and taking care to scale and parameterize appropriately is &lt;a href=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/#note-on-conditioners&#34;&gt;important&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h5&gt;A minimal Conditioner: 2D lattice symmetry&lt;/h5&gt; &#xA;&lt;p&gt;The code snippet below shows how in a few lines of code we can add a conditioner that stipulates the generation of a 2D crystal-like object, where generated proteins are arrayed in an &lt;code&gt;M x N&lt;/code&gt; rectangular lattice.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from chroma.models import Chroma&#xA;from chroma.layers.structure import conditioners&#xA;&#xA;class Lattice2DConditioner(conditioners.Conditioner):&#xA;    def __init__(self, M, N, cell):&#xA;        super().__init__()&#xA;        # Setup the coordinates of a 2D lattice&#xA;        self.order = M*N&#xA;        x = torch.arange(M) * cell[0]&#xA;        y = torch.arange(N) * cell[1]&#xA;        xx, yy = torch.meshgrid(x, y, indexing=&#34;ij&#34;)&#xA;        dX = torch.stack([xx.flatten(), yy.flatten(), torch.zeros(M * N)], dim=1)&#xA;        self.register_buffer(&#34;dX&#34;, dX)&#xA;        &#xA;    def forward(self, X, C, O, U, t): &#xA;        # Tesselate the unit cell on the lattice&#xA;        X = (X[:,None,...] + self.dX[None,:,None,None]).reshape(1, -1, 4, 3)&#xA;        C = torch.cat([C + C.unique().max() * i for i in range(self.dX.shape[0])], dim=1)&#xA;        # Average the gradient across the group (simplifies force scaling)&#xA;        X.register_hook(lambda gradX: gradX / self.order)&#xA;        return X, C, O, U, t&#xA;    &#xA;chroma = Chroma().cuda()&#xA;conditioner = Lattice2DConditioner(M=3, N=4, cell=[20., 15.]).cuda()&#xA;protein = chroma.sample(&#xA;    chain_lengths=[70], conditioner=conditioner, sde_func=&#39;langevin&#39;,&#xA;    potts_symmetry_order=conditioner.order&#xA;)&#xA;&#xA;protein.to_CIF(&#34;lattice_protein.cif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/generatebio/chroma/main/assets/lattice.png&#34; alt=&#34;alt text&#34; width=&#34;700px&#34; align=&#34;middle&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Note on Conditioners&lt;/h4&gt; &#xA;&lt;p&gt;An attractive aspect of this conditioner framework is that it is very general, enabling both constraints (which involve operations on $x$) and restraints (which amount to changes to $U$). At the same time, generation under restraints can still be (and often is) challenging, as the resulting effective energy landscape can become arbitrarily rugged and difficult to integrate. We therefore advise caution when using and developing new conditioners or conditioner combinations. We find that inspecting diffusition trajectories (including unconstrained and denoised trajectories, $\hat{x}_t$ and $\tilde{x}_t$) can be a good tool for identifying integration challenges and defining either better conditioner forms or better sampling regimes.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Chroma&lt;/h2&gt; &#xA;&lt;p&gt;If you use Chroma in your research, please cite:&lt;/p&gt; &#xA;&lt;p&gt;J. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M. Lord, C. Ng-Thow-Hing, E. R. Van Vlack, S. Tie, V. Xue, S. C. Cowles, A. Leung, J. V. Rodrigues, C. L. Morales-Perez, A. M. Ayoub, R. Green, K. Puentes, F. Oplinger, N. V. Panwar, F. Obermeyer, A. R. Root, A. L. Beam, F. J. Poelwijk, and G. Grigoryan, &#34;Illuminating protein space with a programmable generative model&#34;, &lt;em&gt;Nature&lt;/em&gt;, 2023 (10.1038/s41586-023-06728-8).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{Chroma2023,&#xA;  author  = {Ingraham, John B. and Baranov, Max and Costello, Zak and Barber, Karl W. and Wang, Wujie and Ismail, Ahmed and Frappier, Vincent and Lord, Dana M. and Ng-Thow-Hing, Christopher and Van Vlack, Erik R. and Tie, Shan and Xue, Vincent and Cowles, Sarah C. and Leung, Alan and Rodrigues, Jo\~{a}o V. and Morales-Perez, Claudio L. and Ayoub, Alex M. and Green, Robin and Puentes, Katherine and Oplinger, Frank and Panwar, Nishant V. and Obermeyer, Fritz and Root, Adam R. and Beam, Andrew L. and Poelwijk, Frank J. and Grigoryan, Gevorg},&#xA;  journal = {Nature},&#xA;  title   = {Illuminating protein space with a programmable generative model},&#xA;  year    = {2023},&#xA;  volume  = {},&#xA;  number  = {},&#xA;  pages   = {},&#xA;  doi     = {10.1038/s41586-023-06728-8}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The Chroma codebase is the work of many contributers at Generate Biomedicines. We would like to acknowledge: Ahmed Ismail, Alan Witmer, Alex Ramos, Alexander Bock, Ameya Harmalkar, Brinda Monian, Craig Mackenzie, Dan Luu, David Moore, Frank Oplinger, Fritz Obermeyer, George Kent-Scheller, Gevorg Grigoryan, Jacob Feala, James Lucas, Jenhan Tao, John Ingraham, Martin Jankowiak, Max Baranov, Meghan Franklin, Mick Ward, Rudraksh Tuwani, Ryan Nelson, Shan Tie, Vincent Frappier, Vincent Xue, William Wolfe-McGuire, Wujie Wang, Zak Costello, Zander Harteveld.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright Generate Biomedicines, Inc.&lt;/p&gt; &#xA;&lt;h3&gt;Chroma Code License&lt;/h3&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this code except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h3&gt;Model Weights License&lt;/h3&gt; &#xA;&lt;p&gt;Chroma weights are freely available to academic researchers and non-profit entities who accept and agree to be bound under the terms of the Chroma Parameters License. Please visit the &lt;a href=&#34;https://chroma-weights.generatebiomedicines.com/&#34;&gt;weights download page&lt;/a&gt; for more information. If you are not eligible to use the Chroma Parameters under the terms of the provided License or if you would like to share the Chroma Parameters and/or otherwise use the Chroma Parameters beyond the scope of the rights granted in the License (including for commercial purposes), you may contact the Licensor at: &lt;a href=&#34;mailto:licensing@generatebiomedicines.com&#34;&gt;licensing@generatebiomedicines.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CVHub520/X-AnyLabeling</title>
    <updated>2023-11-19T01:41:58Z</updated>
    <id>tag:github.com,2023-11-19:/CVHub520/X-AnyLabeling</id>
    <link href="https://github.com/CVHub520/X-AnyLabeling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Effortless data labeling with AI support from Segment Anything and other awesome models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://user-images.githubusercontent.com/72010077/273420485-bdf4a930-8eca-4544-ae4b-0e15f3ebf095.png&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-LGPL%20v3-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/CVHub520/X-AnyLabeling?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277691916-58be8e7d-133c-4df8-9416-d3243fc7a335.gif&#34; alt=&#34;Grounding DINO&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   SOTA Zero-Shot Openset Object Detection Model&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277692001-b58832b3-4c21-4c6f-9121-02d9daf2b02b.gif&#34; alt=&#34;Recognize Anything Model&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   Strong Image Tagging Model&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277405591-5ebffdcf-83e8-4999-9594-ee4058627d47.gif&#34; alt=&#34;Segment Anything Model&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   Powerful Object Segmentation Anything Model&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/72010077/282393906-059920cc-0f65-4d2c-9350-941aaa8bbd02.png&#34; alt=&#34;PULC PersonAttribute Model&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   Advanced Multi-Label Classification Model&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìÑ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%A5%B3-whats-new-%E2%8F%8F%EF%B8%8F&#34;&gt;ü•≥ What&#39;s New&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%91%8B-brief-introduction-%E2%8F%8F%EF%B8%8F&#34;&gt;üëã Brief Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%94%A5-highlight-%E2%8F%8F%EF%B8%8F&#34;&gt;üî• Highlight&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%97%9D%EF%B8%8Fkey-features&#34;&gt;üóùÔ∏èKey Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%E2%9B%8F%EF%B8%8Fmodel-zoo&#34;&gt;‚õèÔ∏èModel Zoo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%96-tutorials-%E2%8F%8F%EF%B8%8F&#34;&gt;üìñ Tutorials&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%94%9Cquick-start&#34;&gt;üîúQuick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%91%A8%F0%9F%8F%BC%E2%80%8D%F0%9F%92%BBbuild-from-source&#34;&gt;üë®üèº‚ÄçüíªBuild from source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%A6build-executable&#34;&gt;üì¶Build executable&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%8B-usage-%E2%8F%8F%EF%B8%8F&#34;&gt;üìã Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%8Cbasic-usage&#34;&gt;üìåBasic usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%9A%80advanced-usage&#34;&gt;üöÄAdvanced usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%9Cdocs&#34;&gt;üìúDocs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%A7%B7hotkeys&#34;&gt;üß∑Hotkeys&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%A7-contact-%E2%8F%8F%EF%B8%8F&#34;&gt;üìß Contact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%E2%9C%85-license-%E2%8F%8F%EF%B8%8F&#34;&gt;‚úÖ License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%8F%B7%EF%B8%8F-citing-%E2%8F%8F%EF%B8%8F&#34;&gt;üè∑Ô∏è Citing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü•≥ What&#39;s New &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nov. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ü§óü§óü§ó Release the latest version &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.0.0&#34;&gt;2.0.0&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üî•üî•üî• Added support for Grounding-SAM, combining &lt;a href=&#34;https://github.com/wenyi5608/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/SysCV/sam-hq&#34;&gt;HQ-SAM&lt;/a&gt; to achieve sota zero-shot high-quality predictions!&lt;/li&gt; &#xA;   &lt;li&gt;üöÄüöÄüöÄ Enhanced support for &lt;a href=&#34;https://github.com/SysCV/sam-hq&#34;&gt;HQ-SAM&lt;/a&gt; model to achieve high-quality mask predictions.&lt;/li&gt; &#xA;   &lt;li&gt;üôåüôåüôå Support the &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleClas/raw/release%2F2.5/docs/en/PULC/PULC_person_attribute_en.md&#34;&gt;PersonAttribute&lt;/a&gt; and &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleClas/raw/release%2F2.5/docs/en/PULC/PULC_vehicle_attribute_en.md&#34;&gt;VehicleAttribute&lt;/a&gt; model for multi-label classification task.&lt;/li&gt; &#xA;   &lt;li&gt;üÜïüÜïüÜï Introducing a new multi-label attribute annotation functionality.&lt;/li&gt; &#xA;   &lt;li&gt;Release the latest version &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v1.1.0&#34;&gt;1.1.0&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Support pose estimation: &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8-Pose&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Support object-level tag with yolov5_ram.&lt;/li&gt; &#xA;   &lt;li&gt;Add a new feature enabling batch labeling for arbitrary unknown categories based on Grounding-DINO.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Oct. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release the latest version &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v1.0.0&#34;&gt;1.0.0&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Add a new feature for rotation box.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;a href=&#34;https://github.com/hukaixuan19970627/yolov5_obb&#34;&gt;YOLOv5-OBB&lt;/a&gt; with &lt;a href=&#34;https://github.com/VisDrone/DroneVehicle&#34;&gt;DroneVehicle&lt;/a&gt; and &lt;a href=&#34;https://captain-whu.github.io/DOTA/index.html&#34;&gt;DOTA&lt;/a&gt;-v1.0/v1.5/v2.0 model.&lt;/li&gt; &#xA;   &lt;li&gt;SOTA Zero-Shot Object Detection - &lt;a href=&#34;https://github.com/wenyi5608/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt; is released.&lt;/li&gt; &#xA;   &lt;li&gt;SOTA Image Tagging Model - &lt;a href=&#34;https://github.com/xinyu1205/Tag2Text&#34;&gt;Recognize Anything&lt;/a&gt; is released.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;strong&gt;YOLOv5-SAM&lt;/strong&gt; and &lt;strong&gt;YOLOv8-EfficientViT_SAM&lt;/strong&gt; union task.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;strong&gt;YOLOv5&lt;/strong&gt; and &lt;strong&gt;YOLOv8&lt;/strong&gt; segmentation task.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO&#34;&gt;Gold-YOLO&lt;/a&gt; and &lt;a href=&#34;https://github.com/tinyvision/DAMO-YOLO&#34;&gt;DAMO-YOLO&lt;/a&gt; models.&lt;/li&gt; &#xA;   &lt;li&gt;Release MOT algorithms: &lt;a href=&#34;https://github.com/noahcao/OC_SORT&#34;&gt;OC_Sort&lt;/a&gt; (&lt;strong&gt;CVPR&#39;23&lt;/strong&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Add a new feature for small object detection using &lt;a href=&#34;https://github.com/obss/sahi&#34;&gt;SAHI&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sep. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release version &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.2.4&#34;&gt;0.2.4&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/mit-han-lab/efficientvit&#34;&gt;EfficientViT-SAM&lt;/a&gt; (&lt;strong&gt;ICCV&#39;23&lt;/strong&gt;), &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D&#34;&gt;SAM-Med2D&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2304.12306&#34;&gt;MedSAM&lt;/a&gt; and YOLOv5-SAM.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;ByteTrack&lt;/a&gt; (&lt;strong&gt;ECCV&#39;22&lt;/strong&gt;) for MOT task.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PP-OCRv4&lt;/a&gt; model.&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;video&lt;/code&gt; annotation feature.&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;yolo&lt;/code&gt;/&lt;code&gt;coco&lt;/code&gt;/&lt;code&gt;voc&lt;/code&gt;/&lt;code&gt;mot&lt;/code&gt;/&lt;code&gt;dota&lt;/code&gt; export functionality.&lt;/li&gt; &#xA;   &lt;li&gt;Add the ability to process all images at once.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Aug. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release version &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/(https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.2.0)&#34;&gt;0.2.0&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://arxiv.org/abs/2306.11925&#34;&gt;LVMSAM&lt;/a&gt; and it&#39;s variants &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/buid&#34;&gt;BUID&lt;/a&gt;, &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/isic&#34;&gt;ISIC&lt;/a&gt;, &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/kvasir&#34;&gt;Kvasir&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Support lane detection algorithm: &lt;a href=&#34;https://github.com/Turoad/CLRNet&#34;&gt;CLRNet&lt;/a&gt; (&lt;strong&gt;CVPR&#39;22&lt;/strong&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Support 2D human whole-body pose estimation: &lt;a href=&#34;https://github.com/IDEA-Research/DWPose/tree/main&#34;&gt;DWPose&lt;/a&gt; (&lt;strong&gt;ICCV&#39;23 Workshop&lt;/strong&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Jul. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/tools/label_converter.py&#34;&gt;label_converter.py&lt;/a&gt; script.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/raw/develop/configs/rtdetr/README.md&#34;&gt;RT-DETR&lt;/a&gt; model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Jun. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/tree/master&#34;&gt;YOLO-NAS&lt;/a&gt; model.&lt;/li&gt; &#xA;   &lt;li&gt;Support instance segmentation: &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8-seg&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README_zh-CN.md&#34;&gt;README_zh-CN.md&lt;/a&gt; of X-AnyLabeling.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;May. 2023: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release version &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.1.0&#34;&gt;0.1.0&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/meituan/YOLOv6/tree/yolov6-face&#34;&gt;YOLOv6-Face&lt;/a&gt; for face detection and facial landmark detection.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;SAM&lt;/a&gt; and it&#39;s faster version &lt;a href=&#34;https://arxiv.org/abs/2306.14289&#34;&gt;MobileSAM&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;, &lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;YOLOv6&lt;/a&gt;, &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;YOLOv7&lt;/a&gt;, &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8&lt;/a&gt;, &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëã Brief Introduction &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;X-AnyLabeling&lt;/code&gt; is an exceptional annotation tool that draws inspiration from renowned projects like &lt;a href=&#34;https://github.com/HumanSignal/labelImg&#34;&gt;LabelImg&lt;/a&gt;, &lt;a href=&#34;https://github.com/cgvict/roLabelImg&#34;&gt;roLabelImg&lt;/a&gt;, &lt;a href=&#34;https://github.com/wkentaro/labelme&#34;&gt;Labelme&lt;/a&gt;, and &lt;a href=&#34;https://github.com/vietanhdev/anylabeling&#34;&gt;Anylabeling&lt;/a&gt;. It transcends the realm of ordinary annotation tools, representing a significant stride into the future of automated data annotation. This cutting-edge tool not only simplifies the annotation process but also seamlessly integrates state-of-the-art AI models to deliver superior results. With a strong focus on practical applications, X-AnyLabeling is purpose-built to provide developers with an industrial-grade, feature-rich solution for automating annotation and data processing across a wide range of complex tasks.&lt;/p&gt; &#xA;&lt;h2&gt;üî• Highlight &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;üóùÔ∏èKey Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for importing &lt;code&gt;images&lt;/code&gt; and &lt;code&gt;videos&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CPU&lt;/code&gt; and &lt;code&gt;GPU&lt;/code&gt; inference support with on-demand selection.&lt;/li&gt; &#xA; &lt;li&gt;Compatibility with multiple SOTA deep-learning algorithms.&lt;/li&gt; &#xA; &lt;li&gt;Single-frame prediction and &lt;code&gt;one-click&lt;/code&gt; processing for all images.&lt;/li&gt; &#xA; &lt;li&gt;Export options for formats like &lt;code&gt;COCO-JSON&lt;/code&gt;, &lt;code&gt;VOC-XML&lt;/code&gt;, &lt;code&gt;YOLOv5-TXT&lt;/code&gt;, &lt;code&gt;DOTA-TXT&lt;/code&gt; and &lt;code&gt;MOT-CSV&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Integration with popular frameworks such as &lt;a href=&#34;https://www.paddlepaddle.org.cn/&#34;&gt;PaddlePaddle&lt;/a&gt;, &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;timm&lt;/a&gt;, and others.&lt;/li&gt; &#xA; &lt;li&gt;Providing comprehensive &lt;code&gt;help documentation&lt;/code&gt; along with active &lt;code&gt;developer community support&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Accommodation of various visual tasks such as &lt;code&gt;detection&lt;/code&gt;, &lt;code&gt;segmentation&lt;/code&gt;, &lt;code&gt;face recognition&lt;/code&gt;, and so on.&lt;/li&gt; &#xA; &lt;li&gt;Modular design that empowers users to compile the system according to their specific needs and supports customization and further development.&lt;/li&gt; &#xA; &lt;li&gt;Image annotation capabilities for &lt;code&gt;polygons&lt;/code&gt;, &lt;code&gt;rectangles&lt;/code&gt;, &lt;code&gt;rotation&lt;/code&gt;, &lt;code&gt;circles&lt;/code&gt;, &lt;code&gt;lines&lt;/code&gt;, and &lt;code&gt;points&lt;/code&gt;, as well as &lt;code&gt;text detection&lt;/code&gt;, &lt;code&gt;recognition&lt;/code&gt;, and &lt;code&gt;KIE&lt;/code&gt; annotations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚õèÔ∏èModel Zoo&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Object Detection&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SOD with &lt;a href=&#34;https://github.com/obss/sahi&#34;&gt;SAHI&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Facial Landmark Detection&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;2D Pose Estimation&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273488633-fc31da5c-dfdd-434e-b5d0-874892807d95.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/61035602/206095892-934be83a-f869-4a31-8e52-1074184149d1.jpg&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/61035602/206095684-72f42233-c9c7-4bd8-9195-e34859bd08bf.jpg&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/61035602/206100220-ab01d347-9ff9-4f17-9718-290ec14d4205.gif&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2D Lane Detection&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;OCR&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;MOT&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Instance Segmentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273764641-65f456ed-27ce-4077-8fce-b30db093b988.jpg&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273421210-30d20e08-3b72-4f4d-8976-05b564e13d87.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/61035602/206111753-836e7827-968e-4c80-92ef-7a78766892fc.gif&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/61035602/206095831-cc439557-1a23-4a99-b6b0-b6f2e97e8c57.jpg&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Image Tagging&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Grounding DINO&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Recognition&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Rotation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277670825-8797ac7e-e593-45ea-be6a-65c3af17b12b.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277395884-4d500af3-3e4e-4fb3-aace-9a56a09c0595.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277396071-79daec2c-6b0a-4d42-97cf-69fd098b3400.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/277395955-aab54ea0-88f5-41af-ab0a-f4158a673f5e.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://segment-anything.com/&#34;&gt;SAM&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;BC-SAM&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Skin-SAM&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Polyp-SAM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273421331-2c0858b5-0b92-405b-aae6-d061bc25aa3c.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273764259-718dce97-d04d-4629-b6d2-95f17670ce2a.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273764288-e26767d1-3c44-45cb-a72e-124efb4e8263.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/72010077/273764318-e8b6a197-e733-478e-a210-e4386bafa1e4.png&#34; height=&#34;126px&#34; width=&#34;180px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;For more details, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/models_list.md&#34;&gt;models_list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìñ Tutorials &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;üîúQuick Start&lt;/h3&gt; &#xA;&lt;p&gt;Download and run the &lt;code&gt;GUI&lt;/code&gt; version directly from &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.0.0&#34;&gt;Release&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1qX1Q36EfHEJiTkS7xri3_g?pwd=e5it&#34;&gt;Baidu Disk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For MacOS:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After installation, go to the Applications folder.&lt;/li&gt; &#xA;   &lt;li&gt;Right-click on the application and choose Open.&lt;/li&gt; &#xA;   &lt;li&gt;From the second time onwards, you can open the application normally using Launchpad.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Due to the lack of necessary hardware, the current tool is only available in executable versions for &lt;code&gt;Windows&lt;/code&gt; and &lt;code&gt;Linux&lt;/code&gt;. If you require executable programs for other operating systems, e.g., &lt;code&gt;MacOS&lt;/code&gt;, please refer to the following steps for self-compilation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To obtain more stable performance and feature support, it is strongly recommended to build from source code.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üë®üèº‚ÄçüíªBuild from source&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the required libraries:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you need to use GPU inference, install the corresponding requirements-gpu.txt file and download the appropriate version of onnxruntime-gpu based on your local CUDA and CuDNN versions. For more details, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/Q&amp;amp;A.md&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate resources [Option]:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyrcc5 -o anylabeling/resources/resources.py anylabeling/resources/resources.qrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the application:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python anylabeling/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üì¶Build executable&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#Windows-CPU&#xA;bash scripts/build_executable.sh win-cpu&#xA;&#xA;#Windows-GPU&#xA;bash scripts/build_executable.sh win-gpu&#xA;&#xA;#Linux-CPU&#xA;bash scripts/build_executable.sh linux-cpu&#xA;&#xA;#Linux-GPU&#xA;bash scripts/build_executable.sh linux-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Note:&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Before compiling, please modify the &lt;code&gt;__preferred_device__&lt;/code&gt; parameter in the &#34;anylabeling/app_info.py&#34; file according to the appropriate GPU/CPU version.&lt;/li&gt; &#xA;  &lt;li&gt;If you need to compile the GPU version, install the corresponding environment using &#34;pip install -r requirements-gpu*.txt&#34;. Specifically, for compiling the GPU version, manually modify the &#34;datas&#34; list parameters in the &#34;anylabeling-&lt;em&gt;-gpu.spec&#34; file to include the relevant dynamic libraries (&lt;/em&gt;.dll or *.so) of your local onnxruntime-gpu. Additionally, when downloading the onnxruntime-gpu package, ensure compatibility with your CUDA version. You can refer to the official &lt;a href=&#34;https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html&#34;&gt;documentation&lt;/a&gt; for the specific compatibility table.&lt;/li&gt; &#xA;  &lt;li&gt;For macOS versions, you can make modifications by referring to the &#34;anylabeling-win-*.spec&#34; script.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üìã Usage &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;üìåBasic usage&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Build and launch using the instructions above.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Change Output Dir&lt;/code&gt; in the &lt;code&gt;Menu/File&lt;/code&gt; to specify a output directory; otherwise, it will save by default in the current image path.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Open&lt;/code&gt;/&lt;code&gt;Open Dir&lt;/code&gt;/&lt;code&gt;Open Video&lt;/code&gt; to select a specific file, folder, or video.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Start drawing xxx&lt;/code&gt; button on the left-hand toolbar or the &lt;code&gt;Auto Lalbeling&lt;/code&gt; control to initiate.&lt;/li&gt; &#xA; &lt;li&gt;Click and release left mouse to select a region to annotate the rect box. Alternatively, you can press the &#34;Run (i)&#34; key for one-click processing.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: The annotation will be saved to the folder you specify and you can refer to the below hotkeys to speed up your workflow.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;üöÄAdvanced usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Select &lt;strong&gt;AutoLalbeing Button&lt;/strong&gt; on the left side or press the shortcut key &#34;Ctrl + A&#34; to activate auto labeling.&lt;/li&gt; &#xA; &lt;li&gt;Select one of the &lt;code&gt;Segment Anything-liked Models&lt;/code&gt; from the dropdown menu Model, where the Quant indicates the quantization of the model.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;Auto segmentation marking tools&lt;/code&gt; to mark the object. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;+Point: Add a point that belongs to the object.&lt;/li&gt; &#xA;   &lt;li&gt;-Point: Remove a point that you want to exclude from the object.&lt;/li&gt; &#xA;   &lt;li&gt;+Rect: Draw a rectangle that contains the object. Segment Anything will automatically segment the object.&lt;/li&gt; &#xA;   &lt;li&gt;Clear: Clear all auto segmentation markings.&lt;/li&gt; &#xA;   &lt;li&gt;Finish Object (f): Finish the current marking. After finishing the object, you can enter the label name and save the object.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìúDocs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/Q&amp;amp;A.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/models_list.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/custom_model.md&#34;&gt;Loading Custom Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;Video Toturial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üß∑Hotkeys&lt;/h3&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Click to Expand/Collapse&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Shortcut&lt;/th&gt; &#xA;    &lt;th&gt;Function&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;d&lt;/td&gt; &#xA;    &lt;td&gt;Open next file&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;a&lt;/td&gt; &#xA;    &lt;td&gt;Open previous file&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;p&lt;/td&gt; &#xA;    &lt;td&gt;Create polygon&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;o&lt;/td&gt; &#xA;    &lt;td&gt;Create rotation&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;r&lt;/td&gt; &#xA;    &lt;td&gt;Create rectangle&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;i&lt;/td&gt; &#xA;    &lt;td&gt;Run model&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;r&lt;/td&gt; &#xA;    &lt;td&gt;Create rectangle&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;+&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;+point&lt;/code&gt; of SAM mode&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;-point&lt;/code&gt; of SAM mode&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;g&lt;/td&gt; &#xA;    &lt;td&gt;Group selected shapes&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;u&lt;/td&gt; &#xA;    &lt;td&gt;Ungroup selected shapes&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + q&lt;/td&gt; &#xA;    &lt;td&gt;Quit&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + i&lt;/td&gt; &#xA;    &lt;td&gt;Open image file&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + o&lt;/td&gt; &#xA;    &lt;td&gt;Open video file&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + u&lt;/td&gt; &#xA;    &lt;td&gt;Load all images from a directory&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + e&lt;/td&gt; &#xA;    &lt;td&gt;Edit label&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + j&lt;/td&gt; &#xA;    &lt;td&gt;Edit polygon&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + d&lt;/td&gt; &#xA;    &lt;td&gt;Duplicate polygon&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + p&lt;/td&gt; &#xA;    &lt;td&gt;Toggle keep previous mode&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + y&lt;/td&gt; &#xA;    &lt;td&gt;Toggle auto use last label&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + m&lt;/td&gt; &#xA;    &lt;td&gt;Run all images at once&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + a&lt;/td&gt; &#xA;    &lt;td&gt;Enable auto annotation&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + s&lt;/td&gt; &#xA;    &lt;td&gt;Save current information&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + Shift + s&lt;/td&gt; &#xA;    &lt;td&gt;Change output directory&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl -&lt;/td&gt; &#xA;    &lt;td&gt;Zoom out&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + 0&lt;/td&gt; &#xA;    &lt;td&gt;Zoom to Original&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;[Ctrl++, Ctrl+=]&lt;/td&gt; &#xA;    &lt;td&gt;Zoom in&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + f&lt;/td&gt; &#xA;    &lt;td&gt;Fit window&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + Shift + f&lt;/td&gt; &#xA;    &lt;td&gt;Fit width&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + z&lt;/td&gt; &#xA;    &lt;td&gt;Undo the last operation&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ctrl + Delete&lt;/td&gt; &#xA;    &lt;td&gt;Delete file&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Delete&lt;/td&gt; &#xA;    &lt;td&gt;Delete polygon&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Esc&lt;/td&gt; &#xA;    &lt;td&gt;Cancel the selected object&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Backspace&lt;/td&gt; &#xA;    &lt;td&gt;Remove selected point&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚Üë‚Üí‚Üì‚Üê&lt;/td&gt; &#xA;    &lt;td&gt;Keyboard arrows to move selected object&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;zxcv&lt;/td&gt; &#xA;    &lt;td&gt;Keyboard to rotate selected rect box&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üìß Contact &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó Enjoying this project? Please give it a star! ü§ó &lt;/p&gt; &#xA;&lt;p&gt;If you find this project helpful or interesting, consider starring it to show your support, and if you have any questions or encounter any issues while using this project, feel free to reach out for assistance using the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/issues&#34;&gt;Create an issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email: &lt;a href=&#34;mailto:cv_hub@163.com&#34;&gt;cv_hub@163.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WeChat: &lt;code&gt;ww10874&lt;/code&gt; (Please include &lt;code&gt;X-Anylabeing+brief description of the issue&lt;/code&gt; in your message)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚úÖ License &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE&#34;&gt;GPL-3.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üè∑Ô∏è Citing &lt;a href=&#34;https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#%F0%9F%93%84-table-of-contents&#34;&gt;‚èèÔ∏è&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;BibTeX&lt;/h3&gt; &#xA;&lt;p&gt;If you use this software in your research, please cite it as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{X-AnyLabeling,&#xA;  year = {2023},&#xA;  author = {Wei Wang},&#xA;  publisher = {Github},&#xA;  organization = {CVHub},&#xA;  journal = {Github repository},&#xA;  title = {Advanced Auto Labeling Solution with Added Features},&#xA;  howpublished = {\url{https://github.com/CVHub520/X-AnyLabeling}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>