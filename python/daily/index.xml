<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-04T01:46:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rmihaylov/falcontune</title>
    <updated>2023-06-04T01:46:18Z</updated>
    <id>tag:github.com,2023-06-04:/rmihaylov/falcontune</id>
    <link href="https://github.com/rmihaylov/falcontune" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tune any FALCON in 4-bit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;falcontune: 4-Bit Finetuning of FALCONs on a Consumer GPU&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;falcontune&lt;/strong&gt; allows finetuning FALCONs (e.g., falcon-40b-4bit) on as little as one consumer-grade A100 40GB.&lt;/p&gt; &#xA;&lt;p&gt;Its features tiny and easy-to-use codebase.&lt;/p&gt; &#xA;&lt;p&gt;One benefit of being able to finetune larger LLMs on one GPU is the ability to easily leverage data parallelism for large models.&lt;/p&gt; &#xA;&lt;p&gt;Underneath the hood, &lt;strong&gt;falcontune&lt;/strong&gt; implements the LoRA algorithm over an LLM compressed using the GPTQ algorithm, which requires implementing a backward pass for the quantized LLM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;falcontune&lt;/strong&gt; can generate a 50-token recipe on A100 40GB for ~ 10 seconds using triton backend&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ falcontune generate --interactive --model falcon-40b-instruct-4bit --weights gptq_model-4bit--1g.safetensors --max_new_tokens=50 --use_cache --do_sample --prompt &#34;How to prepare pasta?&#34;&#xA;&#xA;&#xA;How to prepare pasta?&#xA;Here&#39;s a simple recipe to prepare pasta:&#xA;&#xA;Ingredients:&#xA;- 1 pound of dry pasta&#xA;- 4-6 cups of water&#xA;- Salt (optional)&#xA;&#xA;Instructions:&#xA;1. Boil the water&#xA;&#xA;Took 10.042 s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example is based on the model: TheBloke/falcon-40b-instruct-GPTQ.&lt;/p&gt; &#xA;&lt;p&gt;Here is a &lt;a href=&#34;https://colab.research.google.com/drive/1Pv7Dn60u_ANgkhRojAIX-VOkU3J-2cYh?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;. You will need a A100 40GB to finetune the model.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt &#xA;python setup.py install         &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default backend is triton which is the fastest. For cuda support install also the CUDA kernels:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup_cuda.py install         &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running falcontune&lt;/h2&gt; &#xA;&lt;p&gt;The above process installs a &lt;code&gt;falcontune&lt;/code&gt; command in your environment.&lt;/p&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;First, start by downloading the weights of a FALCON model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/resolve/main/gptq_model-4bit--1g.safetensors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Text&lt;/h3&gt; &#xA;&lt;p&gt;You can generate text directly from the command line. This generates text from the base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-40b-instruct-4bit \&#xA;    --weights gptq_model-4bit--1g.safetensors \&#xA;    --max_new_tokens=50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;Who was the first person on the moon?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finetune A Base Model&lt;/h3&gt; &#xA;&lt;p&gt;You may also finetune a base model yourself. First, you need to download a dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can finetune any model of the FALCON family:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-7B&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ falcontune finetune \&#xA;    --model=falcon-7b \&#xA;    --weights=tiiuae/falcon-7b \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-7b-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-7b-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-7b \&#xA;    --weights tiiuae/falcon-7b \&#xA;    --lora_apply_dir falcon-7b-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-7B-INSTRUCT&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ falcontune finetune \&#xA;    --model=falcon-7b-instruct \&#xA;    --weights=tiiuae/falcon-7b-instruct \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-7b-instruct-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-7b-instruct-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-7b-instruct \&#xA;    --weights mosaicml/falcon-7b-instruct \&#xA;    --lora_apply_dir falcon-7b-instruct-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-40B&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ falcontune finetune \&#xA;    --model=falcon-40b \&#xA;    --weights=tiiuae/falcon-40b \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-40b-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-40b-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-40b \&#xA;    --weights tiiuae/falcon-40b\&#xA;    --lora_apply_dir falcon-40b-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-40B-INSTRUCT&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ falcontune finetune \&#xA;    --model=falcon-40b-instruct \&#xA;    --weights=tiiuae/falcon-40b-instruct \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-40b-instruct-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-40b-instruct-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-40b-instruct \&#xA;    --weights tiiuae/falcon-40b-instruct\&#xA;    --lora_apply_dir falcon-40b-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-7B-INSTRUCT-4BIT&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ wget https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors&#xA;&#xA;$ falcontune finetune \&#xA;    --model=falcon-7b-instruct-4bit \&#xA;    --weights=gptq_model-4bit-64g.safetensors \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-7b-instruct-4bit-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-7b-instruct-4bit-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-7b-instruct-4bit \&#xA;    --weights gptq_model-4bit-64g.safetensors \&#xA;    --lora_apply_dir falcon-7b-instruct-4bit-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;FALCON-40B-INSTRUCT-4BIT&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ wget https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/resolve/main/gptq_model-4bit--1g.safetensors&#xA;&#xA;$ falcontune finetune \&#xA;    --model=falcon-40b-instruct-4bit \&#xA;    --weights=gptq_model-4bit--1g.safetensors \&#xA;    --dataset=./alpaca_data_cleaned.json \&#xA;    --data_type=alpaca \&#xA;    --lora_out_dir=./falcon-40b-instruct-4bit-alpaca/ \&#xA;    --mbatch_size=1 \&#xA;    --batch_size=2 \&#xA;    --epochs=3 \&#xA;    --lr=3e-4 \&#xA;    --cutoff_len=256 \&#xA;    --lora_r=8 \&#xA;    --lora_alpha=16 \&#xA;    --lora_dropout=0.05 \&#xA;    --warmup_steps=5 \&#xA;    --save_steps=50 \&#xA;    --save_total_limit=3 \&#xA;    --logging_steps=5 \&#xA;    --target_modules=&#39;[&#34;query_key_value&#34;]&#39;&#xA;&#xA;The above commands will download the model and use LoRA to finetune the quantized model. The final adapters and the checkpoints will be saved in `falcon-40b-instruct-4bit-alpaca` and available for generation as follows:&#xA;&#xA;$ falcontune generate \&#xA;    --interactive \&#xA;    --model falcon-40b-instruct-4bit \&#xA;    --weights gptq_model-4bit--1g.safetensors \&#xA;    --lora_apply_dir falcon-40b-instruct-4bit-alpaca \&#xA;    --max_new_tokens 50 \&#xA;    --use_cache \&#xA;    --do_sample \&#xA;    --instruction &#34;How to prepare pasta?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;falcontune&lt;/strong&gt; is based on the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The GPTQ algorithm and codebase by the &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;IST-DASLAB&lt;/a&gt; with modifications by &lt;a href=&#34;https://github.com/qwopqwop200/&#34;&gt;@qwopqwop200&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;alpaca_lora_4bit&lt;/code&gt; repo by &lt;a href=&#34;https://github.com/johnsmith0031&#34;&gt;johnsmith0031&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The PEFT repo and its implementation of LoRA&lt;/li&gt; &#xA; &lt;li&gt;The LLAMA, OPT, and BLOOM models by META FAIR and the BigScience consortium&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;llmtune&lt;/code&gt; repo by &lt;a href=&#34;https://github.com/kuleshov-group/llmtune&#34;&gt;kuleshov-group&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Consultations&lt;/h2&gt; &#xA;&lt;p&gt;Need a custom solution? Let me know: &lt;code&gt;r.m.mihaylov@gmail.com&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>2004content/rarbg</title>
    <updated>2023-06-04T01:46:18Z</updated>
    <id>tag:github.com,2023-06-04:/2004content/rarbg</id>
    <link href="https://github.com/2004content/rarbg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Backup of magnets from RARBG&lt;/p&gt;&lt;hr&gt;&lt;p&gt;rarbg Backup of magnets from RARBG&lt;/p&gt; &#xA;&lt;p&gt;Currently: clean.py is my Python script for cleaning up magnets post-extraction. I think it might have some finnicky thing going on with the way it fixes two magnets in one line, but it works. moviesrarbg.txt holds my original post, cleaned up a lot. (117,392) showsother.txt holds my original post, cleaned up a little. (137,671) showsrarbg.txt holds my original post, cleaned up a lot. (11,699) everything.7z holds what i&#39;ve compiled so far from some of the sources given me (3,459,526)&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m confident that some of the stuff in everything.7z did not come from RARBG, and that will be my first step to remedy once I get everything in there. I&#39;m about a fourth of the way done adding to everything.7z. Then I&#39;ll filter it and sort it and split it into its relevant categories. Alongside the movies and shows, it also holds porn, music, and games, which will each get new .txt files. Thanks guys.&lt;/p&gt; &#xA;&lt;p&gt;This repository was mentioned on TorrentFreak - &lt;a href=&#34;https://torrentfreak.com/rarbg-over-267000-movie-tv-show-magnet-links-appear-online-230601/&#34;&gt;https://torrentfreak.com/rarbg-over-267000-movie-tv-show-magnet-links-appear-online-230601/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookincubator/AITemplate</title>
    <updated>2023-06-04T01:46:18Z</updated>
    <id>tag:github.com,2023-06-04:/facebookincubator/AITemplate</id>
    <link href="https://github.com/facebookincubator/AITemplate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AITemplate&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookincubator/AITemplate/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://facebookincubator.github.io/AITemplate&#34;&gt;&lt;img src=&#34;https://github.com/facebookincubator/AITemplate/actions/workflows/docs.yaml/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://app.circleci.com/pipelines/github/facebookincubator/AITemplate&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebookincubator/AITemplate.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookincubator/AITemplate/actions/workflows/pages.yaml&#34;&gt;&lt;img src=&#34;https://github.com/facebookincubator/AITemplate/actions/workflows/pages.yaml/badge.svg?sanitize=true&#34; alt=&#34;Deploy docs to Pages&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving. AITemplate highlights include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High performance: close to roofline fp16 TensorCore (NVIDIA GPU) / MatrixCore (AMD GPU) performance on major models, including ResNet, MaskRCNN, BERT, VisionTransformer, Stable Diffusion, etc.&lt;/li&gt; &#xA; &lt;li&gt;Unified, open, and flexible. Seamless fp16 deep neural network models for NVIDIA GPU or AMD GPU. Fully open source, Lego-style easily extendable high-performance primitives for new model support. Supports a significantly more comprehensive range of fusions than existing solutions for both GPU platforms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More about AITemplate&lt;/h2&gt; &#xA;&lt;h3&gt;Excellent Backward Capability&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate doesn&#39;t depend on third-party libraries or runtimes, such as cuBLAS, cuDNN, rocBLAS, MIOpen, TensorRT, MIGraphX, etc. Each model is compiled into a self-contained portable binary, which can be used on any software environment with the same hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Horizontal Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides unique advanced horizontal fusion. AITemplate can fuse parallel GEMM, LayerNorm, and other operators with different input shapes into a single GPU kernel.&lt;/p&gt; &#xA;&lt;h3&gt;Vertical Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides strong vertical fusion. AITemplate can fuse a large range of operations into TensorCore/MatrixCore operations, such as elementwise operations, reductions, and layout permutations. AITemplate also provides back-to-back style TensorCore / MatrixCore operation fusion.&lt;/p&gt; &#xA;&lt;h3&gt;Memory Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides innovative memory fusions. AITemplate can fuse GEMM, LayerNorm, and other operators, followed by memory operations such as concatenation, split, and slice into a single operator.&lt;/p&gt; &#xA;&lt;h3&gt;Working w/wo PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;The AITemplate-generated Python runtime can take PyTorch tensors as inputs and outputs without an extra copy. For environments without PyTorch, the AITemplate Python/C++ runtime is self-contained.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions without suffering&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides a straightforward approach for making an extension in codegen. To add a new operator or a new fused kernel into AITemplate, most of the time one only needs to add two Python files: one for a graph node definition and another for the backend codegen. The CUDA/HIP kernel in a text header file can be directly utilized in the codegen.&lt;/p&gt; &#xA;&lt;h2&gt;FX2AIT&lt;/h2&gt; &#xA;&lt;p&gt;FX2AIT is a Python-based tool that converts PyTorch models into AITemplate (AIT) engine for lightning-fast inference serving. Using FX2AIT&#39;s built-in AITLowerer, partial AIT acceleration can be achieved for models with unsupported operators in AITemplate.&lt;/p&gt; &#xA;&lt;p&gt;Key features of FX2AIT include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy Conversion: FX2AIT requires only a PyTorch model and input for conversion, generating an &#34;AITModule&#34; output for inference serving.&lt;/li&gt; &#xA; &lt;li&gt;Expanded Support: AITemplate does not support all PyTorch operators. FX2AIT&#39;s AITLowerer offers a solution for partial AIT conversion for models with unsupported operators. Check the &lt;code&gt;fx2ait/fx2ait/example/03_lowering_split&lt;/code&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More info can be found from &lt;a href=&#34;https://github.com/facebookincubator/AITemplate/tree/main/fx2ait&#34;&gt;https://github.com/facebookincubator/AITemplate/tree/main/fx2ait&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hardware requirements:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt;: AIT is only tested on SM80+ GPUs (Ampere etc). Not all kernels work with old SM75/SM70 (T4/V100) GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt;: AIT is only tested on CDNA2 (MI-210/250) GPUs. There may be compiler issues for old CDNA1 (MI-100) GPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Clone the code&lt;/h3&gt; &#xA;&lt;p&gt;When cloning the code, please use the following command to also clone the submodules:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/facebookincubator/AITemplate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;p&gt;We highly recommend using AITemplate with Docker to avoid accidentally using a wrong version of NVCC or HIPCC.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: &lt;code&gt;./docker/build.sh cuda&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ROCM: &lt;code&gt;DOCKER_BUILDKIT=1 ./docker/build.sh rocm&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This will build a docker image with tag &lt;code&gt;ait:latest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;The following command will create a Python wheel for AITemplate. Please ensure you have correct CUDA/ROCm compiler installed.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: CUDA 11.6&lt;/li&gt; &#xA; &lt;li&gt;ROCm: We tested on ROCm 5.2.3 with a customized build HIPCC with the command in docker/Dockerfile.rocm#L87-L96&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Incorrect compiler will lead performance regression.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please check all submodules are cloned correctly before go to next step.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python&#xA;python setup.py bdist_wheel&#xA;pip install dist/*.whl --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://facebookincubator.github.io/AITemplate&#34;&gt;AITemplate Documentation&lt;/a&gt; for API reference.&lt;/p&gt; &#xA;&lt;p&gt;There are a few tutorials for onboarding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;01: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_infer_pt.html&#34;&gt;How to inference a PyTorch model with AIT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_add_op.html&#34;&gt;How to add an op to AIT codegen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;03: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_visualize.html&#34;&gt;How to visualize AIT&#39;s optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples &amp;amp; Performance&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate provides the following model templates &amp;amp; reference performance data on A100/MI-250:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/01_resnet-50/&#34;&gt;01_ResNet-50&lt;/a&gt; with PyTorch Image Models (TIMM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/02_detectron2/&#34;&gt;02_MaskRCNN-FPN&lt;/a&gt; with Detectron2&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/03_bert/&#34;&gt;03_BERT&lt;/a&gt; with HuggingFace Transformer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/04_vit/&#34;&gt;04_Vision Transformer&lt;/a&gt; with PyTorch Image Models (TIMM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/05_stable_diffusion/&#34;&gt;05_Stable Diffusion&lt;/a&gt; with HuggingFace Diffusers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;All current development updates can be seen in the AITemplate repository. Releases are not on a set schedule and will only be tagged for significant feature releases.&lt;/p&gt; &#xA;&lt;p&gt;Mid-term plan:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Better dynamic shape support: Focus on the dynamic sequence in Transformers. Add symbolic shape support.&lt;/li&gt; &#xA; &lt;li&gt;More automatic graph passes: Relief manual rewrite models to obtain the best performance.&lt;/li&gt; &#xA; &lt;li&gt;Quantization: fp8/int8/int4.&lt;/li&gt; &#xA; &lt;li&gt;Sparsity pruning for Gemm.&lt;/li&gt; &#xA; &lt;li&gt;PT2 integration: Aten2AIT is under active development.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Long-term plan:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic ONNX, Open-XLA and other format model conversion.&lt;/li&gt; &#xA; &lt;li&gt;Composable Kernel CPU extension on AVX2/AVX-512 for AMD Epyc CPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check our &lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to learn about how to contribute to the project.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate is currently maintained by Meta engineers: &lt;a href=&#34;https://github.com/ipiszy&#34;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/chenyang78&#34;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/terrychenism&#34;&gt;Terry Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/muchulee8&#34;&gt;Mu-Chu Lee&lt;/a&gt;, &lt;a href=&#34;https://github.com/tenpercent&#34;&gt;Max Podkorytov&lt;/a&gt;, &lt;a href=&#34;https://github.com/aakhundov&#34;&gt;Adnan Akhundov&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;AITemplate is co-created by Meta engineers: &lt;a href=&#34;https://github.com/antinucleon&#34;&gt;Bing Xu&lt;/a&gt;, &lt;a href=&#34;https://github.com/ipiszy&#34;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/hlu1&#34;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&#34;https://github.com/chenyang78&#34;&gt;Yang Chen&lt;/a&gt;, and &lt;a href=&#34;https://github.com/terrychenism&#34;&gt;Terry Chen&lt;/a&gt;, with major contributions coming from more talented engineers. A non-exhaustive list to mention is Mike Iovine, Mu-Chu Lee, Scott Wolchok, Oleg Khabinov, Shirong Wu, Huaming Li, Hui Guo, Zhijing Li, Max Podkorytov. We also want to thank Andrew Tulloch, Yinghai Lu, Lu Fang for the valuable discussions.&lt;/p&gt; &#xA;&lt;p&gt;FX2AIT and Aten2AIT are co-created and maintained by Meta engineers: &lt;a href=&#34;https://github.com/frank-wei&#34;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&#34;https://github.com/wushirong&#34;&gt;Shirong Wu&lt;/a&gt; and &lt;a href=&#34;https://github.com/tissue3&#34;&gt;Zhijing Li&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate team works deeply with NVIDIA &lt;a href=&#34;https://github.com/NVIDIA/cutlass&#34;&gt;CUTLASS&lt;/a&gt; Team (led by Andrew Kerr, Haicheng Wu) and AMD &lt;a href=&#34;https://github.com/ROCmSoftwarePlatform/composable_kernel&#34;&gt;Composable Kernel&lt;/a&gt; Team (led by Chao Liu, Jing Zhang). We co-designed many advanced GPU optimizations specialized for each platform, and nothing is possible without our close collaboration.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate is licensed under the &lt;a href=&#34;https://github.com/facebookincubator/AITemplate/raw/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>