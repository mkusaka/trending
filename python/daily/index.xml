<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-12T02:32:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sdatkinson/neural-amp-modeler</title>
    <updated>2023-04-12T02:32:45Z</updated>
    <id>tag:github.com,2023-04-12:/sdatkinson/neural-amp-modeler</id>
    <link href="https://github.com/sdatkinson/neural-amp-modeler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural network emulator for guitar amplifiers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NAM: neural amp modeler&lt;/h1&gt; &#xA;&lt;p&gt;This repository handles training, reamping, and exporting the weights of a model. For playing trained models in real time in a standalone application or plugin, see the partner repo, &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;NeuralAmpModelerPlugin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use (Google Colab)&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have a good computer for training ML models, you use Google Colab to train in the cloud using the pre-made notebooks under &lt;code&gt;bin\train&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the very easiest experience, simply go to &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb&#34;&gt;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb&lt;/a&gt; and follow the steps!&lt;/p&gt; &#xA;&lt;p&gt;For a little more visibility under the hood, you can use &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/colab.ipynb&#34;&gt;colab.ipynb&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No local installation required!&lt;/li&gt; &#xA; &lt;li&gt;Decent GPUs are available if you don&#39;t have one on your computer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uploading your data can take a long time.&lt;/li&gt; &#xA; &lt;li&gt;The session will time out after a few hours (for free accounts), so extended training runs aren&#39;t really feasible. Also, there&#39;s a usage limit so you can&#39;t hang out all day. I&#39;ve tried to set you up with a good model that should train reasonably quickly!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use (Local)&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, you can clone this repo to your computer and use it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Installation uses &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; for package management.&lt;/p&gt; &#xA;&lt;p&gt;For computers with a CUDA-capable GPU (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_gpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, for a CPU-only install (will train much more slowly):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_cpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then activate the environment you&#39;ve created with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train models (GUI)&lt;/h3&gt; &#xA;&lt;p&gt;After installing, you can open a GUI trainer by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the terminal.&lt;/p&gt; &#xA;&lt;h3&gt;Train models (Python script)&lt;/h3&gt; &#xA;&lt;p&gt;For users looking to get more fine-grained control over the modeling process, NAM includes a training script that can be run from the terminal. In order to run it&lt;/p&gt; &#xA;&lt;h4&gt;Download audio files&lt;/h4&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/file/d/1v2xFXeQ9W2Ks05XrqsMCs2viQcKPAwBk/view?usp=share_link&#34;&gt;v1_1_1.wav&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/file/d/14w2utgL16NozmESzAJO_I0_VCt-5Wgpv/view?usp=share_link&#34;&gt;overdrive.wav&lt;/a&gt; to a folder of your choice&lt;/p&gt; &#xA;&lt;h4&gt;Update data configuration&lt;/h4&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;bin/train/data/single_pair.json&lt;/code&gt; to point to relevant audio files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;common&#34;: {&#xA;        &#34;x_path&#34;: &#34;C:\\path\\to\\v1_1_1.wav&#34;,&#xA;        &#34;y_path&#34;: &#34;C:\\path\\to\\overdrive.wav&#34;,&#xA;        &#34;delay&#34;: 0&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run training script&lt;/h4&gt; &#xA;&lt;p&gt;Open up a terminal. Activate your nam environment and call the training with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/demonet.json \&#xA;bin/train/inputs/learning/demo.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;data/single_pair.json&lt;/code&gt; contains the information about the data you&#39;re training on&lt;br&gt; &lt;code&gt;models/demonet.json&lt;/code&gt; contains information about the model architecture that is being trained. The example used here uses a &lt;code&gt;feather&lt;/code&gt; configured &lt;code&gt;wavenet&lt;/code&gt;.&lt;br&gt; &lt;code&gt;learning/demo.json&lt;/code&gt; contains information about the training run itself (e.g. number of epochs).&lt;/p&gt; &#xA;&lt;p&gt;The configuration above runs a short (demo) training. For a real training you may prefer to run something like,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/wavenet.json \&#xA;bin/train/inputs/learning/default.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a side note, NAM uses &lt;a href=&#34;https://lightning.ai/pages/open-source/&#34;&gt;PyTorch Lightning&lt;/a&gt; under the hood as a modeling framework, and you can control many of the Pytorch Lightning configuration options from &lt;code&gt;bin/train/inputs/learning/default.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Export a model (to use with &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;the plugin&lt;/a&gt;)&lt;/h4&gt; &#xA;&lt;p&gt;Exporting the trained model to a &lt;code&gt;.nam&lt;/code&gt; file for use with the plugin can be done with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/export.py \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/exported_models/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, point the plugin at the exported &lt;code&gt;model.nam&lt;/code&gt; file and you&#39;re good to go!&lt;/p&gt; &#xA;&lt;h3&gt;Other utilities&lt;/h3&gt; &#xA;&lt;h4&gt;Run a model on an input signal (&#34;reamping&#34;)&lt;/h4&gt; &#xA;&lt;p&gt;Handy if you want to just check it out without needing to use the plugin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/run.py \&#xA;path/to/source.wav \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/output.wav&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rokstrnisa/Robo-GPT</title>
    <updated>2023-04-12T02:32:45Z</updated>
    <id>tag:github.com,2023-04-12:/rokstrnisa/Robo-GPT</id>
    <link href="https://github.com/rokstrnisa/Robo-GPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple and extensible program that helps you run GPT-4 model autonomously.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Robo-GPT: A simple autonomous GPT-4 runner&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/follow?screen_name=rokstrnisa&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/rokstrnisa?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Robo-GPT is a simple and extensible program that helps you run GPT-4 model autonomously. It is designed to be easy to understand, so that anyone can use it and extend it.&lt;/p&gt; &#xA;&lt;p&gt;The program was inspired by &lt;a href=&#34;https://blog.rok.strnisa.com/2023/04/how-i-got-chatgpt-to-write-complete.html&#34;&gt;some of my earlier work&lt;/a&gt; and Auto-GPT.&lt;/p&gt; &#xA;&lt;h2&gt;Simple Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mi0D4l7JRtQ&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/mi0D4l7JRtQ/0.jpg&#34; alt=&#34;A simple demo of RoboGPT.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The features are very limited at the moment, but more can be added easily.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or later.&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key with access to &lt;a href=&#34;https://platform.openai.com/docs/models/gpt-4&#34;&gt;gpt-4 model&lt;/a&gt;. At the time of writing, you need to join a waitlist, and OpenAI will give you access when capacity is available.&lt;/li&gt; &#xA; &lt;li&gt;ElevenLabs key (optional for speech).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pypi.org/project/pipenv/&#34;&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone this repo and change directory to it.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pipenv shell&lt;/code&gt; to enter the Python virtual environment and &lt;code&gt;pipenv install&lt;/code&gt; to install the dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Rename &lt;code&gt;.env.template&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and fill in your &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/a&gt;, and optionally &lt;a href=&#34;https://elevenlabs.io&#34;&gt;&lt;code&gt;ELEVEN_LABS_API_KEY&lt;/code&gt;&lt;/a&gt; (for speech).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;python robo-gpt/main.py&lt;/code&gt; to start the program.&lt;/p&gt; &#xA;&lt;h2&gt;Continuous Mode&lt;/h2&gt; &#xA;&lt;p&gt;The program does not run in continuous mode by default. To run it in continuous mode, simply comment out the relevant lines in &lt;code&gt;main.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;To share your thoughts or simply stay up-to-date, follow &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=rokstrnisa&#34;&gt;@RokStrnisa&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rondinellimorais/facial-expression-recognition</title>
    <updated>2023-04-12T02:32:45Z</updated>
    <id>tag:github.com,2023-04-12:/rondinellimorais/facial-expression-recognition</id>
    <link href="https://github.com/rondinellimorais/facial-expression-recognition" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Facial Expression Recognition&lt;/h1&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Virtual Env&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Maybe you want to create a virtual environment using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; before run the &lt;code&gt;test.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Create Env with conda&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;conda create -n facial-expression-recognition python=3.8&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;conda activate facial-expression-recognition&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Categorical model test&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Test 1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=t6C-5M997eM&#34;&gt;Link #1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Test the Voice 1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zbtljjdheJ4&#34;&gt;Link the Voice #1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Test the Voice 2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3M9dKjkc3kA&#34;&gt;Link the Voice #2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Test the Voice 3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HypKzNChOkc&#34;&gt;Link the Voice #3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>