<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-17T01:42:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>e2b-dev/e2b</title>
    <updated>2023-05-17T01:42:56Z</updated>
    <id>tag:github.com,2023-05-17:/e2b-dev/e2b</id>
    <link href="https://github.com/e2b-dev/e2b" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Vercel for AI agents. We help developers to build, deploy, and monitor AI agents. Focusing on specialized AI agents that build software for you - your personal software developers.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/docs-assets/logoname-black.svg#gh-light-mode-only&#34; alt=&#34;e2b&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/docs-assets/logoname-white.svg#gh-dark-mode-only&#34; alt=&#34;e2b&#34;&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Open-source platform for building AI-powered virtual software developers&lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://e2b.dev&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/U7KEcGErtQ&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/e2b_dev&#34;&gt;Twitter&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/U7KEcGErtQ&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/chat-on%20Discord-blue&#34; alt=&#34;Discord community server&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/e2b_dev&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/infisical?label=Follow&#34; alt=&#34;e2b Twitter&#34;&gt; &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/docs-assets/preview.gif&#34; alt=&#34;e2b-editor&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://e2b.dev&#34;&gt;e2b&lt;/a&gt; or etob (&lt;em&gt;english2bits&lt;/em&gt;) allows you to create &amp;amp; deploy virtual software developers. These virtual developers are powered by specialized AI agents that build software based on your instructions and can use tools.&lt;/p&gt; &#xA;&lt;p&gt;Agents operate in our own secure sandboxed cloud environments that&#39;s powered by &lt;a href=&#34;https://github.com/firecracker-microvm/firecracker/&#34;&gt;Firecracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;e2b currently supports building only REST servers in Node.js. Specifically using the &lt;a href=&#34;https://expressjs.com/&#34;&gt;Express&lt;/a&gt; framework. We&#39;ll support more use-cases with time.&lt;/p&gt; &#xA;&lt;h1&gt;🚀 Get started&lt;/h1&gt; &#xA;&lt;p&gt;We&#39;re working on the cloud-hosted version. In the meantime, the fastest way try out e2b is to run it locally via Docker.&lt;/p&gt; &#xA;&lt;h2&gt;🐳 Start e2b with Docker&lt;/h2&gt; &#xA;&lt;p&gt;You will need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API key&lt;/a&gt; (support for more and custom models coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;Node.js &lt;em&gt;16+&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Free ports 3000 (Next.js app), 54321 (Supabase API Gateway), 54322 (Supabase Database)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start e2b run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open page on &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;npm start&lt;/code&gt; starts local Supabase in the background - to stop it you have to run &lt;code&gt;npm run stop&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💻 Development setup&lt;/h2&gt; &#xA;&lt;p&gt;For developing with hot reloading and contributing to the project you may want to run the app locally without Docker (&lt;code&gt;npm start&lt;/code&gt; command).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/DEVELOPMENT_SETUP.md&#34;&gt;Follow these steps&lt;/a&gt; to set it up.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;h2&gt;🛠 Bring your own X&lt;/h2&gt; &#xA;&lt;p&gt;While e2b will offer the &#34;batteries-included&#34; solution, our goal is to let users:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;BYOM&lt;/strong&gt; - Bring Your Own Model&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BYOP&lt;/strong&gt; - Bring Your Own Prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BYOT&lt;/strong&gt; - Bring Your Own Tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤖 Supported models and model hosting providers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GPT-4&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GTP-3.5&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://anthropic.com/&#34;&gt;Anthropic&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Claude v1.3&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Claude Instant v1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://replicate.com/&#34;&gt;Replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference API&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference Endpoints&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://www.banana.dev/&#34;&gt;Banana&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Model or model hosting provider you like isn&#39;t supported?&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;👉 Please open the &lt;a href=&#34;https://github.com/e2b-dev/e2b/issues/new?assignees=&amp;amp;labels=new+model+request&amp;amp;template=new-model-request.md&amp;amp;title=&#34;&gt;&#34;New model request&#34; issue&lt;/a&gt; 👈&lt;/p&gt; &#xA;&lt;p&gt;👉 Or open a PR and &lt;a href=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/CONTRIBUTING.md#%F0%9F%A4%96-adding-a-new-model-provider&#34;&gt;start contributing&lt;/a&gt; 👈&lt;/p&gt; &#xA;&lt;h2&gt;👀 Early demos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/mlejva/status/1636103084802822151&#34;&gt;AI Agent using coding tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/mlejva/status/1641151421830529042&#34;&gt;Build your custom &#34;Just-In-Time&#34; UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/mlejva/status/1641072535163875330&#34;&gt;Agent coded a full Stripe customer checkout by following a technical spec provided by user&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ℹ️ Community &amp;amp; Support&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/U7KEcGErtQ&#34;&gt;Discord&lt;/a&gt; - live discussion and support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/e2b-dev/e2b/issues&#34;&gt;GitHub issues&lt;/a&gt; - for reporting bugs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/e2b_dev&#34;&gt;Twitter&lt;/a&gt; - to stay up to date&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;🤝 Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome any contributions! If you want to contribute to the project check out the &lt;a href=&#34;https://raw.githubusercontent.com/e2b-dev/e2b/main/CONTRIBUTING.md&#34;&gt;contibution guide&lt;/a&gt; and join our &lt;a href=&#34;https://discord.gg/dSBY3ms2Qr&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;📆 Short-term Roadmap&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;✅ &lt;del&gt;Make sure people can run e2b locally without issues and the DX is smooth.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;🚧 Add support for more models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;✅ &lt;del&gt;OpenAI&lt;/del&gt;&lt;/li&gt; &#xA;   &lt;li&gt;✅ &lt;del&gt;Anthropic&lt;/del&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model hosting providers: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;✅ &lt;del&gt;Replicate&lt;/del&gt;&lt;/li&gt; &#xA;     &lt;li&gt;✅ &lt;del&gt;Hugging Face Inference API&lt;/del&gt;&lt;/li&gt; &#xA;     &lt;li&gt;✅ &lt;del&gt;Hugging Face Inference Endpoints&lt;/del&gt;&lt;/li&gt; &#xA;     &lt;li&gt;✅ &lt;del&gt;Banana&lt;/del&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Paperspace&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;🚧 Improve agent&#39;s reliability and quality of output.&lt;/li&gt; &#xA; &lt;li&gt;🚧 Improve the feedback loop (UX/UI) between the developer and agents.&lt;/li&gt; &#xA; &lt;li&gt;🚧 Improve agent&#39;s understanding of the context based on the instructions.&lt;/li&gt; &#xA; &lt;li&gt;🚧 Deployable agents.&lt;/li&gt; &#xA; &lt;li&gt;Support more tools and 3rd party integrations that agents can use.&lt;/li&gt; &#xA; &lt;li&gt;✅ &lt;del&gt;Let users edit prompts.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;Let users customize tools and build custom workflows for the agent.&lt;/li&gt; &#xA; &lt;li&gt;Release cloud version.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>jtsang4/claude-to-chatgpt</title>
    <updated>2023-05-17T01:42:56Z</updated>
    <id>tag:github.com,2023-05-17:/jtsang4/claude-to-chatgpt</id>
    <link href="https://github.com/jtsang4/claude-to-chatgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project converts the API of Anthropic&#39;s Claude model to the OpenAI Chat API format.&lt;/p&gt;&lt;hr&gt;&lt;h4 align=&#34;right&#34;&gt; &lt;strong&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/raw/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;div&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;Claude to ChatGPT&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/releases&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/jtsang4/claude-to-chatgpt/actions/workflows/docker.yaml/badge.svg?sanitize=true&#34; alt=&#34;release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/releases&#34;&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/jtsang4/claude-to-chatgpt?style=flat&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/releases&#34;&gt; &lt;img alt=&#34;GitHub Repo Badge&#34; src=&#34;https://img.shields.io/badge/anthropic-claude-orange?style=flat&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/releases&#34;&gt; &lt;img alt=&#34;GitHub Repo Language&#34; src=&#34;https://img.shields.io/badge/langurage-js/py-brightgreen?style=flat&amp;amp;color=blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This project converts the API of Anthropic&#39;s Claude model to the OpenAI Chat API format.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✨ Call Claude API like OpenAI ChatGPT API&lt;/li&gt; &#xA; &lt;li&gt;💦 Support streaming response&lt;/li&gt; &#xA; &lt;li&gt;🐻 Support &lt;code&gt;claude-v1.3&lt;/code&gt;, &lt;code&gt;claude-v1.3-100k&lt;/code&gt; models&lt;/li&gt; &#xA; &lt;li&gt;🌩️ Deploy by Cloudflare Workers or Docker&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can run this project using Cloudflare Workers or Docker:&lt;/p&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;h4&gt;Using Cloudflare Workers&lt;/h4&gt; &#xA;&lt;p&gt;By using Cloudflare Workers, you don&#39;t need a server to deploy this project.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a Cloudflare Worker&lt;/li&gt; &#xA; &lt;li&gt;Paste the code in &lt;a href=&#34;https://github.com/jtsang4/claude-to-chatgpt/raw/main/cloudflare-worker.js&#34;&gt;&lt;code&gt;cloudflare-worker.js&lt;/code&gt;&lt;/a&gt; to Cloudflare Worker &#34;Quick Edit&#34; Editor&lt;/li&gt; &#xA; &lt;li&gt;Save and deploy&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set custom domain for your Cloudflare Worker&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The Cloudfalre Workers support 100k requests a day, If you need to call more than that, you can use Docker to deploy as below.&lt;/p&gt; &#xA;&lt;h4&gt;Using Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 8000:8000 wtzeng/claude-to-chatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Docker Compose&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The API will then be available at &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;. API endpoint: &lt;code&gt;/v1/chat/completions&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;When you input the model parameter as &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; or &lt;code&gt;gpt-3.5-turbo-0301&lt;/code&gt;, it will be substituted with &lt;code&gt;claude-v1.3&lt;/code&gt;. otherwise, &lt;code&gt;claude-v1.3-100k&lt;/code&gt; will be utilized.&lt;/p&gt; &#xA;&lt;h4&gt;GUI&lt;/h4&gt; &#xA;&lt;p&gt;Here are some recommended GUI software that supports this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bin-Huang/chatbox&#34;&gt;Bin-Huang/chatbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;Yidadaa/ChatGPT-Next-Web&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CLI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -H &#34;Authorization: Bearer $CLAUDE_API_KEY&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,&#xA;    &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Conversion Details&lt;/h2&gt; &#xA;&lt;p&gt;The Claude Completion API has an endpoint &lt;code&gt;/v1/complete&lt;/code&gt; which takes the following JSON request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;prompt&#34;: &#34;\n\nHuman: Hello, AI.\n\nAssistant: &#34;,&#xA;  &#34;model&#34;: &#34;claude-v1.3&#34;,&#xA;  &#34;max_tokens_to_sample&#34;: 100,&#xA;  &#34;temperature&#34;: 1,&#xA;  &#34;stream&#34;: true&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And returns JSON with choices and completions.&lt;/p&gt; &#xA;&lt;p&gt;The OpenAI Chat API has a similar &lt;code&gt;/v1/chat/completions&lt;/code&gt; endpoint which takes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,&#xA;  &#34;messages&#34;: [&#xA;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: &#34;Hello, AI.&#34;&#xA;    }&#xA;  ],&#xA;  &#34;max_tokens&#34;: 100,&#xA;  &#34;temperature&#34;: 1,&#xA;  &#34;stream&#34;: true&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And returns JSON with a response string.&lt;/p&gt; &#xA;&lt;p&gt;This project converts between these two APIs, get completions from the Claude model and formatting them as OpenAI Chat responses.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kuleshov-group/llmtune</title>
    <updated>2023-05-17T01:42:56Z</updated>
    <id>tag:github.com,2023-05-17:/kuleshov-group/llmtune</id>
    <link href="https://github.com/kuleshov-group/llmtune" rel="alternate"></link>
    <summary type="html">&lt;p&gt;4-Bit Finetuning of Large Language Models on One Consumer GPU&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLMTune: 4-Bit Finetuning of LLMs on a Consumer GPU&lt;/h1&gt; &#xA;&lt;p&gt;LLMTune allows finetuning LLMs (e.g., the largest 65B LLAMA models) on as little as one consumer-grade GPU.&lt;/p&gt; &#xA;&lt;p&gt;Its features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modular support for multiple LLMs (currently LLAMA, OPT)&lt;/li&gt; &#xA; &lt;li&gt;Support for a wide range of consumer-grade NVidia GPUs; 65B LLAMAs finetune on one A6000&lt;/li&gt; &#xA; &lt;li&gt;Tiny and easy-to-use codebase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One benefit of being able to finetune larger LLMs (e.g., 65B params) on one GPU is the ability to easily leverage data parallelism for large models.&lt;/p&gt; &#xA;&lt;p&gt;Underneath the hood, LLMTune implements the LoRA algorithm over an LLM compressed using the GPTQ algorithm, which requires implementing a backward pass for the quantized LLM. See the hardware requirements for more information on which LLMs are supported by various GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Goals&lt;/h3&gt; &#xA;&lt;p&gt;LLMTune is a research project at Cornell Tech and Cornell University. Its goals are to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide an easy-to-use platform for creative experimentation with large language models&lt;/li&gt; &#xA; &lt;li&gt;Faciliate research on LLM alignment, bias mitigation, efficient inference, and other topics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;This is LLMTune running an instruction finetuned LLAMA-65B model on an NVidia A6000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ llmtune generate --model llama-65b-4bit --weights llama65b-4bit.pt --adapter alpaca-lora-65b-4bit --prompt &#34;Write a well-thought out abstract for a machine learning paper that proves that 42 is the optimal seed for training neural networks.&#34;&#xA;&#xA;The goal of this paper is to prove that 42 is the optimal seed for &#xA;training neural networks. To do so, a set of experiments was conducted &#xA;with various seeds ranging from 0 to 100. For each experiment, the &#xA;neural networks were trained on a standard benchmark dataset and &#xA;evaluated for accuracy, speed of training, and generalization accuracy. &#xA;The results were then collected and analyzed. The analysis revealed &#xA;that 42 consistently yielded the highest accuracy with the lowest &#xA;generalization error, as well as the fastest training times. &#xA;Furthermore, these results were consistent across multiple datasets &#xA;and neural network architectures. Based on this evidence, it can be &#xA;concluded that 42 is indeed the optimal seed for training neural &#xA;networks. This paper also discusses the implications of this finding &#xA;and its potential applications in the field of machine learning.&#xA;&#xA;In summary, this research provides concrete evidence to support the use&#xA;of 42 as the optimal seed for training neural networks, and provides &#xA;further insights into the optimal parameters for training neural networks &#xA;in general. The findings of this research may have significant implications &#xA;for the field of machine learning and the development of optimized training &#xA;strategies for neural networks.&#xA;&#xA;References&#xA;[1] X. Zhang, E. Rashid, and T. Yang, “An analysis of the optimal seed for training neural networks,” Machine Learning Journal, vol. 13, no. 1, pp. 21-34, 2022.&#xA;[2] C. Kim, T. Smith, and A. Vishwanathan, “A survey of optimization strategies for training neural networks,” Machine Learning Journal, vol. 8, no. 4, pp. 101-115, 2020.&#xA;[3] A. Krizhevsky, I. Sutskever, and G. H. Bradshaw, “Imagenet classification with deep convolutional neural networks,” J. Comput. Vis., vol. 5, no. 3, pp. 219–225, 2012.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example is based on an Alpaca demo prompt. See below for additional examples.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;LLMTune requires a UNIX environment supporting Python (3.8 or greater) and PyTorch (we tested with 1.13.1+cu116). See &lt;code&gt;requirements.txt&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;To ensure maximum reproducibility, consider creating a new conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n llmtune&#xA;conda activate llmtune&#xA;conda install git pip virtualenv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LLMTune also requries an NVIDIA GPU (Pascal architecture or newer); other platforms are currently unsupported.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;distutils&lt;/code&gt; to package LLMTune. If you are not running conda, you can also create a &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt   # installs torch and two other packages&#xA;python setup.py install           # installs llmtune in your environment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this process compiles and installs a custom CUDA kernel that is necessary to run quantized models.&lt;/p&gt; &#xA;&lt;h2&gt;Running LLMTune&lt;/h2&gt; &#xA;&lt;p&gt;The above process installs a &lt;code&gt;llmtune&lt;/code&gt; command in your environment.&lt;/p&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;First, start by downloading the weights of a base LLM model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://huggingface.co/kuleshov/llama-65b-4bit/resolve/main/llama-65b-4bit.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The pre-quantized models are available for download. We will add the quantization code to &lt;code&gt;llmtune&lt;/code&gt; if there is demand.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://huggingface.co/kuleshov/llama-13b-4bit/resolve/main/llama-13b-4bit.pt&#xA;wget https://huggingface.co/kuleshov/llama-30b-4bit/resolve/main/llama-30b-4bit.pt&#xA;wget https://huggingface.co/kuleshov/llama-65b-4bit/resolve/main/llama-65b-4bit.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can finetune these models yourself, or you can optionally download LoRA adapter weights that have already been finetuned for you using &lt;code&gt;llmtune&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir alpaca-adapter-65b-4bit &amp;amp;&amp;amp; cd alpaca-adapter-65b-4bit&#xA;wget https://huggingface.co/kuleshov/alpaca-adapter-65b-4bit/resolve/main/adapter_config.json&#xA;wget https://huggingface.co/kuleshov/alpaca-adapter-65b-4bit/resolve/main/adapter_model.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Text&lt;/h3&gt; &#xA;&lt;p&gt;You can generate text directly from the command line. This generates text from the base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llmtune generate --model llama-65b-4bit --weights llama-65b-4bit.pt --prompt &#34;the pyramids were built by&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More interestingly, we can generate output from an instruction-finetuned model by also providing a path to LoRA adapter weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llmtune generate --model llama-65b-4bit --weights llama-65b-4bit.pt --adapter alpaca-adapter-65b-4bit --instruction &#34;Write a well-thought out recipe for a blueberry lasagna dish.&#34; --max-length 500&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above example, &lt;code&gt;--instruct&lt;/code&gt; applies the Alpaca-style prompt template, although you can also use &lt;code&gt;--prompt&lt;/code&gt; to feed the model initial text without any pre-processing:&lt;/p&gt; &#xA;&lt;p&gt;The LLMTune interface also provides additional command-line options.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: llmtune generate [-h] --model {llama-7b-4bit,llama-13b-4bit,llama-30b-4bit,llama-65b-4bit,opt-6.7b-4bit} --weights WEIGHTS&#xA;                        [--adapter ADAPTER] [--prompt PROMPT] [--instruction INSTRUCTION] [--min-length MIN_LENGTH]&#xA;                        [--max-length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --model {llama-7b-4bit,llama-13b-4bit,llama-30b-4bit,llama-65b-4bit,opt-6.7b-4bit}&#xA;                        Type of model to load&#xA;  --weights WEIGHTS     Path to the base model weights.&#xA;  --adapter ADAPTER     Path to the folder with the Lora adapter.&#xA;  --prompt PROMPT       Text used to initialize generation&#xA;  --instruction INSTRUCTION&#xA;                        Instruction for an alpaca-style model&#xA;  --min-length MIN_LENGTH&#xA;                        Minimum length of the sequence to be generated.&#xA;  --max-length MAX_LENGTH&#xA;                        Maximum length of the sequence to be generated.&#xA;  --top_p TOP_P         Top p sampling parameter.&#xA;  --top_k TOP_K         Top p sampling parameter.&#xA;  --temperature TEMPERATURE&#xA;                        Sampling temperature.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finetune A Base Model&lt;/h3&gt; &#xA;&lt;p&gt;You may also finetune a base model yourself. First, you need to dowload a dataset. We currently support the Alpaca dataset, which we download from the HF hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://huggingface.co/datasets/kuleshov/alpaca-data/resolve/main/dataset.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may now finetune the base &lt;code&gt;llama-65b-4bit&lt;/code&gt; model on this dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir alpaca-adapter-folder-65b-4bit&#xA;llmtune finetune --model llama-65b-4bit --weights llama-65b-4bit.pt --adapter alpaca-adapter-folder-65b-4bit --dataset dataset.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will use LoRA to finetune the quantized 65-bit model. The final adapters and the checkpoints will be saved in &lt;code&gt;alpaca-adapter-folder-65b-4bit&lt;/code&gt; and available for generation as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llmtune generate --model llama-65b-4bit --weights llama-65b-4bit.pt --adapter alpaca-adapter-folder-65b-4bit --instruction &#34;Write an irrefutable proof that the meaning of life is 42.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The LLMTune interface provides many additional command-line options for finetuning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: llmtune finetune [-h] --model {llama-7b-4bit,llama-13b-4bit,llama-30b-4bit,llama-65b-4bit,opt-6.7b-4bit} --weights WEIGHTS&#xA;                        [--data-type {alpaca,gpt4all}] [--dataset DATASET] [--adapter ADAPTER] [--mbatch_size MBATCH_SIZE]&#xA;                        [--batch_size BATCH_SIZE] [--epochs EPOCHS] [--lr LR] [--cutoff_len CUTOFF_LEN] [--lora_r LORA_R]&#xA;                        [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT] [--val_set_size VAL_SET_SIZE]&#xA;                        [--warmup_steps WARMUP_STEPS] [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT]&#xA;                        [--logging_steps LOGGING_STEPS] [--resume_checkpoint]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --model {llama-7b-4bit,llama-13b-4bit,llama-30b-4bit,llama-65b-4bit,opt-6.7b-4bit}&#xA;                        Type of model to load&#xA;  --weights WEIGHTS     Path to the model weights.&#xA;  --data-type {alpaca,gpt4all}&#xA;                        Dataset format&#xA;  --dataset DATASET     Path to local dataset file.&#xA;  --adapter ADAPTER     Path to Lora adapter folder (also holds checkpoints)&#xA;  --mbatch_size MBATCH_SIZE&#xA;                        Micro-batch size.&#xA;  --batch_size BATCH_SIZE&#xA;                        Batch size.&#xA;  --epochs EPOCHS       Epochs.&#xA;  --lr LR               Learning rate.&#xA;  --cutoff_len CUTOFF_LEN&#xA;  --lora_r LORA_R&#xA;  --lora_alpha LORA_ALPHA&#xA;  --lora_dropout LORA_DROPOUT&#xA;  --val_set_size VAL_SET_SIZE&#xA;                        Validation set size.&#xA;  --warmup_steps WARMUP_STEPS&#xA;  --save_steps SAVE_STEPS&#xA;  --save_total_limit SAVE_TOTAL_LIMIT&#xA;  --logging_steps LOGGING_STEPS&#xA;  --resume_checkpoint   Resume from checkpoint.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Programmatic Usage&lt;/h3&gt; &#xA;&lt;p&gt;LLMTune can also be used as a Python library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import llmtune.executor as llmtune&#xA;&#xA;llm, llm_config = llmtune.load_llm(&#39;llama-7b-4bit&#39;, &#39;/path/to/llama-7b-4bit.pt&#39;)&#xA;output = llmtune.generate(&#xA;    llm, &#xA;    llm_config, &#xA;    prompt=&#34;the pyramids were built by&#34;, &#xA;    min_length=10, &#xA;    max_length=50, &#xA;    top_p=0.95, &#xA;    temperature=0.8,&#xA;)&#xA;print(output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hardware Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The following hardware is needed to run different models in LLMTune:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt; &#xA;   &lt;th&gt;GPU Memory Requirements&lt;/th&gt; &#xA;   &lt;th&gt;Compatible GPUs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7b-4bit&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;RTX 2060, 3050, 3060&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13b-4bit&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;GTX 1080, RTX 2060, 3060, 3080&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30b-4bit&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;RTX 3080, A5000, 3090, 4090, V100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65b-4bit&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;A100, 2x3090, 2x4090, A40, A6000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Only NVIDIA GPUs with the Pascal architecture or newer can run the current system.&lt;/p&gt; &#xA;&lt;h2&gt;Additional Examples&lt;/h2&gt; &#xA;&lt;p&gt;In this example, the LLM produces a recipe for blueberry lasagna:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ llmtune generate --model llama-65b-4bit --weights llama-65b-4bit.pt --adapter alpaca-lora-65b-4bit-e3 --instruction &#34;Write a well-thought out recipe for a new blueberry lasagna dish.&#34; --max-length 500&#xA;Ingredients:&#xA;* 1 lb lasagna noodles&#xA;* 1/2 cup ricotta cheese&#xA;* 2 eggs&#xA;* 4 tablespoons parmesan cheese&#xA;* 2 cups blueberries&#xA;* 2 tablespoons sugar&#xA;* 2 tablespoons flour&#xA;* 2 tablespoons butter&#xA;&#xA;Instructions:&#xA;1. Preheat oven to 350 degrees F.&#xA;2. Bring a large pot of salted water to a boil. Add the lasagna noodles and cook according to package directions.&#xA;3. In a medium bowl, combine the ricotta cheese, eggs, parmesan cheese, and 1 tablespoon of the flour; mix until combined.&#xA;4. In a skillet over medium-high heat, melt the butter and add the blueberries and sugar. Cook for 5 minutes, stirring occasionally.&#xA;5. Spread a layer of sauce in the bottom of a 9x13 inch baking dish. Layer with a single layer of lasagna noodles. Spread the ricotta mixture over the noodles and then layer with the blueberry mixture and another layer of noodles. Top with remaining parmesan cheese.&#xA;6. Bake for 25 minutes until cheese is melted and bubbly. Enjoy!&#xA;&#xA;Note: You can make the ricotta mixture ahead of time and store it in the fridge. You can also cook the blueberry mixture ahead of time and store it in the fridge. Both should be reheated before assembling the lasagna.&#xA;&#xA;Enjoy!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The 30B and 65B parameter models can do zero-shot chain-of-thought reasoning (i.e., &#34;let&#39;s think step-by-step&#34;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ llmtune generate --model llama-65b-4bit --weights /share/kuleshov/vk379/llama-65b-4bit.pt --prompt &#34;Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Let&#39;s think step-by-step.&#34;&#xA;Loading LLAMA model&#xA;Done&#xA;Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. &#xA;How many tennis balls does he have now? A: Let&#39;s think step-by-step.&#xA;Roger has 5 balls&#xA;Roger bought 2 cans&#xA;Each can has 3 balls&#xA;So, Roger has 5 + 2 x 3 = 11 balls now!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Todos&lt;/h2&gt; &#xA;&lt;p&gt;This is experimental work in progress. Work that stills needs to be done:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make it easy to load models directly from the HF hub&lt;/li&gt; &#xA; &lt;li&gt;Out-of-the-box support for additional LLMs&lt;/li&gt; &#xA; &lt;li&gt;Improve the interface with things like automatic termination&lt;/li&gt; &#xA; &lt;li&gt;Automated quantization scripts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;LLMTune is based on the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The GPTQ algorithm and codebase by the &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;IST-DASLAB&lt;/a&gt; with modifications by &lt;a href=&#34;https://github.com/qwopqwop200/&#34;&gt;@qwopqwop200&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;alpaca_lora_4bit&lt;/code&gt; repo by &lt;a href=&#34;https://github.com/johnsmith0031&#34;&gt;johnsmith0031&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The PEFT repo and its implementation of LoRA&lt;/li&gt; &#xA; &lt;li&gt;The LLAMA, OPT, and BLOOM models by META FAIR and the BigScience consortium&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;Please cite this repository if you use our code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{llmtune,&#xA;  author = {Volodymyr Kuleshov},&#xA;  title = {LLMTune: Fine-Tuning Large Language Models on One Consumer GPU},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/kuleshov-group/llmtune}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also recommend you cite the above projects on which this work is based.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Please send feedback to &lt;a href=&#34;https://twitter.com/volokuleshov&#34;&gt;Volodymyr Kuleshov&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>