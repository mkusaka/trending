<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-14T01:37:27Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bcdannyboy/CVE-2023-44487</title>
    <updated>2023-10-14T01:37:27Z</updated>
    <id>tag:github.com,2023-10-14:/bcdannyboy/CVE-2023-44487</id>
    <link href="https://github.com/bcdannyboy/CVE-2023-44487" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Basic vulnerability scanning to see if web servers may be vulnerable to CVE-2023-44487&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2023-44487&lt;/h1&gt; &#xA;&lt;p&gt;Basic vulnerability scanning to see if web servers may be vulnerable to CVE-2023-44487&lt;/p&gt; &#xA;&lt;p&gt;This tool checks to see if a website is vulnerable to CVE-2023-44487 completely non-invasively.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The tool checks if a web server accepts HTTP/2 requests without downgrading them&lt;/li&gt; &#xA; &lt;li&gt;If the web server accepts and does not downgrade HTTP/2 requests the tool attempts to open a connection stream and subsequently reset it&lt;/li&gt; &#xA; &lt;li&gt;If the web server accepts the creation and resetting of a connection stream then the server is definitely vulnerable, if it only accepts HTTP/2 requests but the stream connection fails it may be vulnerable if the server-side capabilities are enabled.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To run,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -r requirements.txt&#xA;&#xA;$ python3 cve202344487.py -i input_urls.txt -o output_results.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify an HTTP proxy to proxy all the requests through with the &lt;code&gt;--proxy&lt;/code&gt; flag&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 cve202344487.py -i input_urls.txt -o output_results.csv --proxy http://proxysite.com:1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script outputs a CSV file with the following columns&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Timestamp: a timestamp of the request&lt;/li&gt; &#xA; &lt;li&gt;Source Internal IP: The internal IP address of the host sending the HTTP requessts&lt;/li&gt; &#xA; &lt;li&gt;Source External IP: The external IP address of the host sending the HTTP requests&lt;/li&gt; &#xA; &lt;li&gt;URL: The URL being scanned&lt;/li&gt; &#xA; &lt;li&gt;Vulnerability Status: &#34;VULNERABLE&#34;/&#34;LIKELY&#34;/&#34;POSSIBLE&#34;/&#34;SAFE&#34;/&#34;ERROR&#34;&lt;/li&gt; &#xA; &lt;li&gt;Error/Downgrade Version: The error or the version the HTTP server downgrades the request to&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: &#34;Vulnerable&#34; in this context means that it is confirmed that an attacker can reset the a stream connection without issue, it does not take into account implementation-specific or volume-based detections&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Dockerized&lt;/h1&gt; &#xA;&lt;p&gt;Build&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker build -t py-cve-2023-44487 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --rm -v /path/to/urls:/shared py-cve-2023-44487 -i /shared/input_urls.txt -o /shared/output_results.csv&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CogVLM</title>
    <updated>2023-10-14T01:37:27Z</updated>
    <id>tag:github.com,2023-10-14:/THUDM/CogVLM</id>
    <link href="https://github.com/THUDM/CogVLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a state-of-the-art-level open visual language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogVLM&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM 是一个强大的开源视觉语言模型，利用视觉专家模块深度整合语言编码和视觉编码，在 14 项权威跨模态基准上取得了 SOTA 性能。目前仅支持英文，后续会提供中英双语版本支持，欢迎持续关注！&lt;/p&gt; &#xA;&lt;p&gt;📖 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper（论文）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🌐 &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demo（测试网址）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow-align method which maps image features into the input space of language model, &lt;strong&gt;CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers&lt;/strong&gt;. CogVLM enables deep fusion of visual language features without sacrificing any performance on NLP tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., &lt;strong&gt;surpassing or matching PaLI-X 55B&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We anticipate that the open-sourcing of CogVLM will greatly help the research and industrial application of visual understanding.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/metrics.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM is powerful for answering various types of visual questions, including &lt;strong&gt;Detailed Description &amp;amp; Visual Question Answering&lt;/strong&gt;, &lt;strong&gt;Complex Counting&lt;/strong&gt;, &lt;strong&gt;Visual Math Problem Solving&lt;/strong&gt;, &lt;strong&gt;OCR-Free Reasonging&lt;/strong&gt;, &lt;strong&gt;OCR-Free Visual Question Answering&lt;/strong&gt;, &lt;strong&gt;World Knowledge&lt;/strong&gt;, &lt;strong&gt;Referring Expression Comprehension&lt;/strong&gt;, &lt;strong&gt;Programming with Visual Input&lt;/strong&gt;, &lt;strong&gt;Grounding with Caption&lt;/strong&gt;, &lt;strong&gt;Grounding Visual Question Answering&lt;/strong&gt;, etc.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/compare.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![compare](assets/compare.png) --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand/collapse more examples&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/chat.png&#34; alt=&#34;Chat Examples&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module. See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/method.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m spacy download en_core_web_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Online Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demo&lt;/a&gt; based on &lt;a href=&#34;https://gradio.app&#34;&gt;Gradio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/web_demo.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Local Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We also offer a local web demo based on Gradio. First, install Gradio by running: &lt;code&gt;pip install gradio&lt;/code&gt;. Then download and enter this repository and run &lt;code&gt;web_demo.py&lt;/code&gt;. See the next section for detailed usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Terminal Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py --from_pretrained cogvlm-base-224 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-base-490 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-base --version base --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;# We also support model parallel inference, which splits model to multiple (2/4/8) GPUs.&#xA;torchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter &#39;clear&#39; to clear the conversation history and &#39;stop&#39; to stop the program.&lt;/p&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have trouble in accessing huggingface.co, you can add &lt;code&gt;--local_tokenizer /path/to/vicuna-7b-v1.5&lt;/code&gt; to load the tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;If you have trouble in automatically downloading model with 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, try downloading from 🤖&lt;a href=&#34;https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary&#34;&gt;modelscope&lt;/a&gt; or 🤗&lt;a href=&#34;https://huggingface.co/THUDM/CogVLM&#34;&gt;huggingface&lt;/a&gt; manually.&lt;/li&gt; &#xA; &lt;li&gt;Download model using 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, the model will be saved to the default location &lt;code&gt;~/.sat_models&lt;/code&gt;. Change the default location by setting the environment variable &lt;code&gt;SAT_HOME&lt;/code&gt;. For example, if you want to save the model to &lt;code&gt;/path/to/my/models&lt;/code&gt;, you can run &lt;code&gt;export SAT_HOME=/path/to/my/models&lt;/code&gt; before running the python command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The program provides the following hyperparameters to control the generation process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --max_length MAX_LENGTH&#xA;                        max length of the total sequence&#xA;  --top_p TOP_P         top p for nucleus sampling&#xA;  --top_k TOP_K         top k for top k sampling&#xA;  --temperature TEMPERATURE&#xA;                        temperature for sampling&#xA;  --english             only output English&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;Start by downloading the &lt;a href=&#34;https://www.kaggle.com/datasets/aadhavvignesh/captcha-images&#34;&gt;Captcha Images dataset&lt;/a&gt;. Once downloaded, extract the contents of the ZIP file.&lt;/p&gt; &#xA;&lt;p&gt;To create a train/validation/test split in the ratio of 80/5/15, execute the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/split_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Kickstart the fine-tuning process with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/finetune_(224/490)_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, merge the model to model_parallel_size=1: (replace 4 with your training MP_SIZE)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nnodes=1 --nproc-per-node=4 merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(224/490)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the performance of your model, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/evaluate_(224/490).sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is recommended to use 490 version. However, if you have limited GPU resources (such as only one node with eight 24GB 3090 cards), you can try 224 version with model parallel. The anticipated result is around 95% accuracy on test set. It is worth noting that the fine-tuning examples only tune limited parameters. If you want to improve performance, you can change trainable parameters in &lt;code&gt;finetune_demo.py&lt;/code&gt; as needed.&lt;/p&gt; &#xA;&lt;h2&gt;Model Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Model quantization is not possible right now, but we are working on it. We will release the quantized model as soon as possible.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is open source under the Apache-2.0 license, while the use of the CogVLM model weights must comply with the Model License.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing the following papers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the instruction fine-tuning phase of the CogVLM, there are some English image-text data from the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLAVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/FuxiaoLiu/LRV-Instruction&#34;&gt;LRV-Instruction&lt;/a&gt;, &lt;a href=&#34;https://github.com/SALT-NLP/LLaVAR&#34;&gt;LLaVAR&lt;/a&gt; and &lt;a href=&#34;https://github.com/shikras/shikra&#34;&gt;Shikra&lt;/a&gt; projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bokeh/bokeh</title>
    <updated>2023-10-14T01:37:27Z</updated>
    <id>tag:github.com,2023-10-14:/bokeh/bokeh</id>
    <link href="https://github.com/bokeh/bokeh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interactive Data Visualization in the browser, from Python&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/SVG/bokeh-logo-white-text-no-padding.svg&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/SVG/bokeh-logo-black-text-no-padding.svg?sanitize=true&#34; alt=&#34;Bokeh logo -- text is white in dark theme and black in light theme&#34; height=&#34;60/&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bokeh.org&#34;&gt;Bokeh&lt;/a&gt; is an&amp;nbsp;interactive visualization&amp;nbsp;library for modern web browsers. It provides elegant, concise&amp;nbsp;construction&amp;nbsp;of versatile graphics and affords high-performance interactivity across large or streaming datasets.&amp;nbsp;Bokeh can help anyone who wants to create interactive plots, dashboards, and data applications quickly and easily.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Package&lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/bokeh?label=Version&amp;amp;color=ECD078&amp;amp;style=for-the-badge&#34; alt=&#34;Latest package version&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/first_steps/installation.html&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/bokeh?color=ECD078&amp;amp;style=for-the-badge&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/bokeh/bokeh/raw/main/LICENSE.txt&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/bokeh/bokeh.svg?color=ECD078&amp;amp;style=for-the-badge&#34; alt=&#34;Bokeh license (BSD 3-clause)&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Project&lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://img.shields.io/github/contributors-anon/bokeh/bokeh?color=ECD078&amp;amp;style=for-the-badge&#34; alt=&#34;Github contributors&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://numfocus.org&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/sponsor-numfocus-ECD078?style=for-the-badge&#34; alt=&#34;Link to NumFOCUS&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.bokeh.org/en/latest/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/documentation-latest-ECD078?style=for-the-badge&#34; alt=&#34;Link to documentation&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Downloads&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/first_steps/installation.html&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/bokeh?color=D98B43&amp;amp;label=pypi&amp;amp;logo=python&amp;amp;logoColor=yellow&amp;amp;style=for-the-badge&#34; alt=&#34;PyPI downloads per month&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/first_steps/installation.html&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bokeh/badges/main/cache/bokeh-conda-monthly.svg?sanitize=true&#34; alt=&#34;Conda downloads per month&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.npmjs.com/package/@bokeh/bokehjs&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dm/%40bokeh/bokehjs?style=for-the-badge&amp;amp;logo=npm&amp;amp;label=NPM&amp;amp;color=D98B43&#34; alt=&#34;NPM downloads per month&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/bokeh/bokeh/actions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/bokeh/bokeh/bokeh-ci.yml?label=Bokeh-CI&amp;amp;logo=github&amp;amp;style=for-the-badge&#34; alt=&#34;Current Bokeh-CI github actions build status&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/bokeh/bokeh/actions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/bokeh/bokeh/bokehjs-ci.yml?label=BokehJS-CI&amp;amp;logo=github&amp;amp;style=for-the-badge&#34; alt=&#34;Current BokehJS-CI github actions build status&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://codecov.io/gh/bokeh/bokeh&#34;&gt; &lt;img src=&#34;https://img.shields.io/codecov/c/github/bokeh/bokeh?logo=codecov&amp;amp;style=for-the-badge&amp;amp;token=bhEzGkDUaw&#34; alt=&#34;Codecov coverage percentage&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Community&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://discourse.bokeh.org&#34;&gt; &lt;img src=&#34;https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg?color=blue&amp;amp;logo=discourse&amp;amp;style=for-the-badge&#34; alt=&#34;Community support on discourse.bokeh.org&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://stackoverflow.com/questions/tagged/bokeh&#34;&gt; &lt;img src=&#34;https://img.shields.io/stackexchange/stackoverflow/t/%5Bbokeh%5D?style=for-the-badge&amp;amp;logo=stackoverflow&amp;amp;label=stackoverflow&amp;amp;color=blue&#34; alt=&#34;Bokeh-tagged questions on Stack Overflow&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://twitter.com/bokeh&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-%40bokeh-blue?logo=twitter&amp;amp;style=for-the-badge&#34; alt=&#34;Follow Bokeh on Twitter&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Consider &lt;a href=&#34;https://opencollective.com/bokeh&#34;&gt;making a donation&lt;/a&gt; if you enjoy using Bokeh and want to support its development.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1078448/190840954-dc243c99-9295-44de-88e9-fafd0f4f7f8a.jpg&#34; alt=&#34;4x9 image grid of Bokeh plots&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install Bokeh and its required dependencies using &lt;code&gt;pip&lt;/code&gt;, enter the following command at a Bash or Windows command prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install bokeh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install &lt;code&gt;conda&lt;/code&gt;, enter the following command at a Bash or Windows command prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install bokeh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/first_steps/installation.html&#34;&gt;installation documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;Once Bokeh is installed, check out the &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/first_steps.html#first-steps-guides&#34;&gt;first steps guides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Visit the &lt;a href=&#34;https://docs.bokeh.org&#34;&gt;full documentation site&lt;/a&gt; to view the &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/user_guide.html&#34;&gt;User&#39;s Guide&lt;/a&gt; or &lt;a href=&#34;https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/HEAD?labpath=index.ipynb&#34;&gt;launch the Bokeh tutorial&lt;/a&gt; to learn about Bokeh in live Jupyter Notebooks.&lt;/p&gt; &#xA;&lt;p&gt;Community support is available on the &lt;a href=&#34;https://discourse.bokeh.org&#34;&gt;Project Discourse&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute to Bokeh, please review the &lt;a href=&#34;https://docs.bokeh.org/en/latest/docs/dev_guide.html&#34;&gt;Contributor Guide&lt;/a&gt; and &lt;a href=&#34;https://slack-invite.bokeh.org/&#34;&gt;request an invitation to the Bokeh Dev Slack workspace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Everyone who engages in the Bokeh project&#39;s discussion forums, codebases, and issue trackers is expected to follow the &lt;a href=&#34;https://github.com/bokeh/bokeh/raw/branch-3.0/docs/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Follow us&lt;/h2&gt; &#xA;&lt;p&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/bokeh&#34;&gt;@bokeh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;h3&gt;Fiscal Support&lt;/h3&gt; &#xA;&lt;p&gt;The Bokeh project is grateful for &lt;a href=&#34;https://opencollective.com/bokeh&#34;&gt;individual contributions&lt;/a&gt;, as well as for monetary support from the organizations and companies listed below:&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.numfocus.org/&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/numfocus.svg?sanitize=true&#34; alt=&#34;NumFocus Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://chanzuckerberg.com/&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/czi.svg?sanitize=true&#34; alt=&#34;CZI Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt; &lt;a href=&#34;https://www.blackstone.com/the-firm/&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/blackstone.png&#34; alt=&#34;Blackstone Logo&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://tidelift.com/&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/tidelift.svg?sanitize=true&#34; alt=&#34;TideLift Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.anaconda.com/&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/anaconda.png&#34; alt=&#34;Anaconda Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.nvidia.com&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/nvidia.png&#34; alt=&#34;NVidia Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://developer.nvidia.com/rapids&#34;&gt; &lt;img src=&#34;https://static.bokeh.org/sponsor/rapids.png&#34; alt=&#34;Rapids Logo&#34; width=&#34;200&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;If your company uses Bokeh and is able to sponsor the project, please contact &lt;a href=&#34;https://raw.githubusercontent.com/bokeh/bokeh/branch-3.4/info@bokeh.org&#34;&gt;&lt;/a&gt;&lt;a href=&#34;mailto:info@bokeh.org&#34;&gt;info@bokeh.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Bokeh is a Sponsored Project of NumFOCUS, a 501(c)(3) nonprofit charity in the United States. NumFOCUS provides Bokeh with fiscal, legal, and administrative support to help ensure the health and sustainability of the project. Visit &lt;a href=&#34;https://numfocus.org&#34;&gt;numfocus.org&lt;/a&gt; for more information.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Donations to Bokeh are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;In-kind Support&lt;/h3&gt; &#xA;&lt;p&gt;Non-monetary support can help with development, collaboration, infrastructure, security, and vulnerability management. The Bokeh project is grateful to the following companies for their donation of services:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/&#34;&gt;Amazon Web Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitguardian.com/&#34;&gt;GitGuardian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://makepath.com/&#34;&gt;makepath&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pingdom.com/website-monitoring&#34;&gt;Pingdom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slack.com&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.questionscout.com/&#34;&gt;QuestionScout&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://1password.com/&#34;&gt;1Password&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>