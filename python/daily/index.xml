<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-25T01:35:00Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SkyworkAI/SkyReels-V2</title>
    <updated>2025-07-25T01:35:00Z</updated>
    <id>tag:github.com,2025-07-25:/SkyworkAI/SkyReels-V2</id>
    <link href="https://github.com/SkyworkAI/SkyReels-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SkyReels-V2: Infinite-length Film Generative model&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/logo2.png&#34; alt=&#34;SkyReels Logo&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;  &lt;a href=&#34;https://arxiv.org/pdf/2504.13074&#34;&gt;Technical Report&lt;/a&gt; 路  &lt;a href=&#34;https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2&#34; target=&#34;_blank&#34;&gt;Playground&lt;/a&gt; 路  &lt;a href=&#34;https://discord.gg/PwM6NYtccQ&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; 路  &lt;a href=&#34;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; 路  &lt;a href=&#34;https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144&#34; target=&#34;_blank&#34;&gt;ModelScope&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;SkyReels V2&lt;/strong&gt; repository! Here, you&#39;ll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing &lt;strong&gt;AutoRegressive Diffusion-Forcing architecture&lt;/strong&gt; that achieves the &lt;strong&gt;SOTA performance&lt;/strong&gt; among publicly available models.&lt;/p&gt; &#xA;&lt;h2&gt;ヰヰ News!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Jun 1, 2025:  We published the technical report, &lt;a href=&#34;https://arxiv.org/pdf/2506.00830&#34;&gt;SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;May 16, 2025:  We release the inference code for &lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#ve&#34;&gt;video extension&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#se&#34;&gt;start/end frame control&lt;/a&gt; in diffusion forcing model.&lt;/li&gt; &#xA; &lt;li&gt;Apr 24, 2025:  We release the 720P models, &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;SkyReels-V2-DF-14B-720P&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;SkyReels-V2-I2V-14B-720P&lt;/a&gt;. The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.&lt;/li&gt; &#xA; &lt;li&gt;Apr 21, 2025:  We release the inference code and model weights of &lt;a href=&#34;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&#34;&gt;SkyReels-V2&lt;/a&gt; Series Models and the video captioning model &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; .&lt;/li&gt; &#xA; &lt;li&gt;Apr 3, 2025:  We also release &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A2&#34;&gt;SkyReels-A2&lt;/a&gt;. This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.&lt;/li&gt; &#xA; &lt;li&gt;Feb 18, 2025:  we released &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A1&#34;&gt;SkyReels-A1&lt;/a&gt;. This is an open-sourced and effective framework for portrait image animation.&lt;/li&gt; &#xA; &lt;li&gt;Feb 18, 2025:  We released &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-V1&#34;&gt;SkyReels-V1&lt;/a&gt;. This is the first and most advanced open-source human-centric video foundation model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt; Demos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model. &#xA;&lt;h2&gt; TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/pdf/2504.13074&#34;&gt;Technical Report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Checkpoints of the 14B and 1.3B Models Series&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Single-GPU &amp;amp; Multi-GPU Inference Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Prompt Enhancer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Diffusers integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the 5B Models Series&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the Camera Director Models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the Step &amp;amp; Guidance Distill Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt; Quickstart&lt;/h2&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# clone the repository.&#xA;git clone https://github.com/SkyworkAI/SkyReels-V2&#xA;cd SkyReels-V2&#xA;# Install dependencies. Test environment uses Python 3.10.12.&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Model Download&lt;/h4&gt; &#xA;&lt;p&gt;You can download our models from Hugging Face:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Model Variant&lt;/th&gt; &#xA;   &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Diffusion Forcing&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Text-to-Video&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Image-to-Video&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;Huggingface&lt;/a&gt;  &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Camera Director&lt;/td&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;After downloading, set the model path in your generation commands:&lt;/p&gt; &#xA;&lt;h4&gt;Single GPU Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusion Forcing for Long Video Generation&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/abs/2407.01392&#34;&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.&lt;/p&gt; &#xA;&lt;p&gt;synchronous generation for 10s video&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# synchronous inference&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 257 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --teacache \&#xA;  --use_ret_steps \&#xA;  --teacache_thresh 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;asynchronous generation for 30s video&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# asynchronous inference&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 5 \&#xA;  --causal_block_size 5 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 737 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --offload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;If you want to run the &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; task, add &lt;code&gt;--image ${image_path}&lt;/code&gt; to your command and it is also better to use &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt;-like prompt which includes some descriptions of the first-frame image.&lt;/li&gt; &#xA;  &lt;li&gt;For long video generation, you can just switch the &lt;code&gt;--num_frames&lt;/code&gt;, e.g., &lt;code&gt;--num_frames 257&lt;/code&gt; for 10s video, &lt;code&gt;--num_frames 377&lt;/code&gt; for 15s video, &lt;code&gt;--num_frames 737&lt;/code&gt; for 30s video, &lt;code&gt;--num_frames 1457&lt;/code&gt; for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &amp;gt; 1, the &lt;code&gt;--num_frames&lt;/code&gt; should be carefully set.&lt;/li&gt; &#xA;  &lt;li&gt;You can use &lt;code&gt;--ar_step 5&lt;/code&gt; to enable asynchronous inference. When asynchronous inference, &lt;code&gt;--causal_block_size 5&lt;/code&gt; is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.&lt;/li&gt; &#xA;  &lt;li&gt;To reduce peak VRAM, just lower the &lt;code&gt;--base_num_frames&lt;/code&gt;, e.g., to 77 or 57, while keeping the same generative length &lt;code&gt;--num_frames&lt;/code&gt; you want to generate. This may slightly reduce video quality, and it should not be set too small.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--addnoise_condition&lt;/code&gt; is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.&lt;/li&gt; &#xA;  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span id=&#34;ve&#34;&gt;Video Extention&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# video extention&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 120 \&#xA;  --overlap_history 17 \&#xA;  --prompt ${prompt} \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --use_ret_steps \&#xA;  --teacache \&#xA;  --teacache_thresh 0.3 \&#xA;  --video_path ${video_path}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When performing video extension, you need to pass the &lt;code&gt;--video_path ${video_path}&lt;/code&gt; parameter to specify the video to be extended.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span id=&#34;se&#34;&gt;Start/End Frame Control&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# start/end frame control&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 97 \&#xA;  --overlap_history 17 \&#xA;  --prompt ${prompt} \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --use_ret_steps \&#xA;  --teacache \&#xA;  --teacache_thresh 0.3 \&#xA;  --image ${image} \&#xA;  --end_image ${end_image}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When controlling the start and end frames, you need to pass the &lt;code&gt;--image ${image}&lt;/code&gt; parameter to control the generation of the start frame and the &lt;code&gt;--end_image ${end_image}&lt;/code&gt; parameter to control the generation of the end frame.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# run Text-to-Video Generation&#xA;model_id=Skywork/SkyReels-V2-T2V-14B-540P&#xA;python3 generate_video.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --num_frames 97 \&#xA;  --guidance_scale 6.0 \&#xA;  --shift 8.0 \&#xA;  --fps 24 \&#xA;  --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34; \&#xA;  --offload \&#xA;  --teacache \&#xA;  --use_ret_steps \&#xA;  --teacache_thresh 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; &#xA;  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Enhancer&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The prompt enhancer is implemented based on &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-32B-Instruct&#34;&gt;Qwen2.5-32B-Instruct&lt;/a&gt; and is utilized via the &lt;code&gt;--prompt_enhancer&lt;/code&gt; parameter. It works ideally for short prompts, while for long prompts, it might generate an excessively lengthy prompt that could lead to over-saturation in the generative video. Note the peak memory of GPU is 64G+ if you use &lt;code&gt;--prompt_enhancer&lt;/code&gt;. If you want to obtain the enhanced prompt separately, you can also run the prompt_enhancer script separately for testing. The steps are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd skyreels_v2_infer/pipelines&#xA;python3 prompt_enhancer.py --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--prompt_enhancer&lt;/code&gt; is not allowed if using &lt;code&gt;--use_usp&lt;/code&gt;. We recommend running the skyreels_v2_infer/pipelines/prompt_enhancer.py script first to generate enhanced prompt before enabling the &lt;code&gt;--use_usp&lt;/code&gt; parameter.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Advanced Configuration Options&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are the key parameters you can customize for video generation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Recommended Value&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text description for generating your video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input image for image-to-video generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--resolution&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;540P or 720P&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Output video resolution (select based on model type)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--num_frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97 or 121&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Total frames to generate (&lt;strong&gt;97 for 540P models&lt;/strong&gt;, &lt;strong&gt;121 for 720P models&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--inference_steps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Number of denoising steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--fps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Frames per second in the output video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--shift&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.0 or 5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flow matching scheduler parameter (&lt;strong&gt;8.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--guidance_scale&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0 or 5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Controls text adherence strength (&lt;strong&gt;6.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--seed&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fixed seed for reproducible results (omit for random generation)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--offload&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Offloads model components to CPU to reduce VRAM usage (recommended)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--use_usp&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Enables multi-GPU acceleration with xDiT USP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--outdir&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;./video_out&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Directory where generated videos will be saved&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--prompt_enhancer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Expand the prompt into a more detailed description&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--teacache&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Enables teacache for faster inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--teacache_thresh&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Higher speedup will cause to worse quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--use_ret_steps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Retention Steps for teacache&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Forcing Additional Parameters&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Recommended Value&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--ar_step&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Controls asynchronous inference (0 for synchronous mode)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--base_num_frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97 or 121&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Base frame count (&lt;strong&gt;97 for 540P&lt;/strong&gt;, &lt;strong&gt;121 for 720P&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--overlap_history&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Number of frames to overlap for smooth transitions in long videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--addnoise_condition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Improves consistency in long video generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--causal_block_size&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Recommended when using asynchronous inference (--ar_step &amp;gt; 0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--video_path&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input video for video extension&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--end_image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input image for end frame control&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Multi-GPU inference using xDiT USP&lt;/h4&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/xdit-project/xDiT&#34;&gt;xDiT&lt;/a&gt; USP to accelerate inference. For example, to generate a video with 2 GPUs, you can use the following command:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# diffusion forcing synchronous inference&#xA;torchrun --nproc_per_node=2 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 257 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --use_usp \&#xA;  --offload \&#xA;  --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# run Text-to-Video Generation&#xA;model_id=Skywork/SkyReels-V2-T2V-14B-540P&#xA;torchrun --nproc_per_node=2 generate_video.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --num_frames 97 \&#xA;  --guidance_scale 6.0 \&#xA;  --shift 8.0 \&#xA;  --fps 24 \&#xA;  --offload \&#xA;  --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34; \&#xA;  --use_usp \&#xA;  --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#methodology-of-skyreels-v2&#34;&gt;Methodology of SkyReels-V2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#key-contributions-of-skyreels-v2&#34;&gt;Key Contributions of SkyReels-V2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#video-captioner&#34;&gt;Video Captioner&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#diffusion-forcing&#34;&gt;Diffusion Forcing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#high-quality-supervised-fine-tuning-sft&#34;&gt;High-Quality Supervised Fine-Tuning(SFT)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs&#39; inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation.&lt;/p&gt; &#xA;&lt;p&gt;To address these limitations, we introduce SkyReels-V2, the world&#39;s first infinite-length film generative model using a Diffusion Forcing framework. Our approach synergizes Multi-modal Large Language Models (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing techniques to achieve comprehensive optimization. Beyond its technical innovations, SkyReels-V2 enables multiple practical applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and multi-subject consistent video generation through our &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A2&#34;&gt;Skyreels-A2&lt;/a&gt; system.&lt;/p&gt; &#xA;&lt;h2&gt;Methodology of SkyReels-V2&lt;/h2&gt; &#xA;&lt;p&gt;The SkyReels-V2 methodology consists of several interconnected components. It starts with a comprehensive data processing pipeline that prepares various quality training data. At its core is the Video Captioner architecture, which provides detailed annotations for video content. The system employs a multi-task pretraining strategy to build fundamental video generation capabilities. Post-training optimization includes Reinforcement Learning to enhance motion quality, Diffusion Forcing Training for generating extended videos, and High-quality Supervised Fine-Tuning (SFT) stages for visual refinement. The model runs on optimized computational infrastructure for efficient training and inference. SkyReels-V2 supports multiple applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and Elements-to-Video Generation.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/main_pipeline.jpg&#34; alt=&#34;mainpipeline&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Key Contributions of SkyReels-V2&lt;/h2&gt; &#xA;&lt;h4&gt;Video Captioner&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; serves as our video captioning model for data annotation. This model is trained on the captioning result from the base model &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt; and the sub-expert captioners on a balanced video data. The balanced video data is a carefully curated dataset of approximately 2 million videos to ensure conceptual balance and annotation quality. Built upon the &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL-7B-Instruct&lt;/a&gt; foundation model, &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; is fine-tuned to enhance performance in domain-specific video captioning tasks. To compare the performance with the SOTA models, we conducted a manual assessment of accuracy across different captioning fields using a test set of 1,000 samples. The proposed &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; achieves the highest average accuracy among the baseline models, and show a dramatic result in the shot related fields&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL-7B-Ins.&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;Qwen2.5-VL-72B-Ins.&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/omni-research/Tarsier2-Recap-7b&#34;&gt;Tarsier2-Recap-7b&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Avg accuracy&lt;/td&gt; &#xA;   &lt;td&gt;51.4%&lt;/td&gt; &#xA;   &lt;td&gt;58.7%&lt;/td&gt; &#xA;   &lt;td&gt;49.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;76.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot type&lt;/td&gt; &#xA;   &lt;td&gt;76.8%&lt;/td&gt; &#xA;   &lt;td&gt;82.5%&lt;/td&gt; &#xA;   &lt;td&gt;60.2%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;93.7%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot angle&lt;/td&gt; &#xA;   &lt;td&gt;60.0%&lt;/td&gt; &#xA;   &lt;td&gt;73.7%&lt;/td&gt; &#xA;   &lt;td&gt;52.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot position&lt;/td&gt; &#xA;   &lt;td&gt;28.4%&lt;/td&gt; &#xA;   &lt;td&gt;32.7%&lt;/td&gt; &#xA;   &lt;td&gt;23.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.1%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;camera motion&lt;/td&gt; &#xA;   &lt;td&gt;62.0%&lt;/td&gt; &#xA;   &lt;td&gt;61.2%&lt;/td&gt; &#xA;   &lt;td&gt;45.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;85.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;expression&lt;/td&gt; &#xA;   &lt;td&gt;43.6%&lt;/td&gt; &#xA;   &lt;td&gt;51.5%&lt;/td&gt; &#xA;   &lt;td&gt;54.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;68.8%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;text-align: center; border-bottom: 1px solid #ddd; padding: 8px;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TYPES_type&lt;/td&gt; &#xA;   &lt;td&gt;43.5%&lt;/td&gt; &#xA;   &lt;td&gt;49.7%&lt;/td&gt; &#xA;   &lt;td&gt;47.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;82.5%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TYPES_sub_type&lt;/td&gt; &#xA;   &lt;td&gt;38.9%&lt;/td&gt; &#xA;   &lt;td&gt;44.9%&lt;/td&gt; &#xA;   &lt;td&gt;45.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;75.4%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;appearance&lt;/td&gt; &#xA;   &lt;td&gt;40.9%&lt;/td&gt; &#xA;   &lt;td&gt;52.0%&lt;/td&gt; &#xA;   &lt;td&gt;45.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;action&lt;/td&gt; &#xA;   &lt;td&gt;32.4%&lt;/td&gt; &#xA;   &lt;td&gt;52.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;69.8%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;68.8%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;position&lt;/td&gt; &#xA;   &lt;td&gt;35.4%&lt;/td&gt; &#xA;   &lt;td&gt;48.6%&lt;/td&gt; &#xA;   &lt;td&gt;45.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57.5%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;is_main_subject&lt;/td&gt; &#xA;   &lt;td&gt;58.5%&lt;/td&gt; &#xA;   &lt;td&gt;68.7%&lt;/td&gt; &#xA;   &lt;td&gt;69.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;environment&lt;/td&gt; &#xA;   &lt;td&gt;70.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;72.7%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;61.4%&lt;/td&gt; &#xA;   &lt;td&gt;70.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lighting&lt;/td&gt; &#xA;   &lt;td&gt;77.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21.2%&lt;/td&gt; &#xA;   &lt;td&gt;76.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Reinforcement Learning&lt;/h4&gt; &#xA;&lt;p&gt;Inspired by the previous success in LLM, we propose to enhance the performance of the generative model by Reinforcement Learning. Specifically, we focus on the motion quality because we find that the main drawback of our generative model is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the generative model does not handle well with large, deformable motions.&lt;/li&gt; &#xA; &lt;li&gt;the generated videos may violate the physical law.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To avoid the degradation in other metrics, such as text alignment and video quality, we ensure the preference data pairs have comparable text alignment and video quality, while only the motion quality varies. This requirement poses greater challenges in obtaining preference annotations due to the inherently higher costs of human annotation. To address this challenge, we propose a semi-automatic pipeline that strategically combines automatically generated motion pairs and human annotation results. This hybrid approach not only enhances the data scale but also improves alignment with human preferences through curated quality control. Leveraging this enhanced dataset, we first train a specialized reward model to capture the generic motion quality differences between paired samples. This learned reward function subsequently guides the sample selection process for Direct Preference Optimization (DPO), enhancing the motion quality of the generative model.&lt;/p&gt; &#xA;&lt;h4&gt;Diffusion Forcing&lt;/h4&gt; &#xA;&lt;p&gt;We introduce the Diffusion Forcing Transformer to unlock our models ability to generate long videos. Diffusion Forcing is a training and sampling strategy where each token is assigned an independent noise level. This allows tokens to be denoised according to arbitrary, per-token schedules. Conceptually, this approach functions as a form of partial masking: a token with zero noise is fully unmasked, while complete noise fully masks it. Diffusion Forcing trains the model to &#34;unmask&#34; any combination of variably noised tokens, using the cleaner tokens as conditional information to guide the recovery of noisy ones. Building on this, our Diffusion Forcing Transformer can extend video generation indefinitely based on the last frames of the previous segment. Note that the synchronous full sequence diffusion is a special case of Diffusion Forcing, where all tokens share the same noise level. This relationship allows us to fine-tune the Diffusion Forcing Transformer from a full-sequence diffusion model.&lt;/p&gt; &#xA;&lt;h4&gt;High-Quality Supervised Fine-Tuning (SFT)&lt;/h4&gt; &#xA;&lt;p&gt;We implement two sequential high-quality supervised fine-tuning (SFT) stages at 540p and 720p resolutions respectively, with the initial SFT phase conducted immediately after pretraining but prior to reinforcement learning (RL) stage.This first-stage SFT serves as a conceptual equilibrium trainer, building upon the foundation models pretraining outcomes that utilized only fps24 video data, while strategically removing FPS embedding components to streamline thearchitecture. Trained with the high-quality concept-balanced samples, this phase establishes optimized initialization parameters for subsequent training processes. Following this, we execute a secondary high-resolution SFT at 720p after completing the diffusion forcing stage, incorporating identical loss formulations and the higher-quality concept-balanced datasets by the manually filter. This final refinement phase focuses on resolution increase such that the overall video quality will be further enhanced.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;To comprehensively evaluate our proposed method, we construct the SkyReels-Bench for human assessment and leveraged the open-source &lt;a href=&#34;https://github.com/Vchitect/VBench&#34;&gt;V-Bench&lt;/a&gt; for automated evaluation. This allows us to compare our model with the state-of-the-art (SOTA) baselines, including both open-source and proprietary models.&lt;/p&gt; &#xA;&lt;h4&gt;Human Evaluation&lt;/h4&gt; &#xA;&lt;p&gt;For human evaluation, we design SkyReels-Bench with 1,020 text prompts, systematically assessing three dimensions: Instruction Adherence, Motion Quality, Consistency and Visual Quality. This benchmark is designed to evaluate both text-to-video (T2V) and image-to-video (I2V) generation models, providing comprehensive assessment across different generation paradigms. To ensure fairness, all models were evaluated under default settings with consistent resolutions, and no post-generation filtering was applied.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text To Video Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;Instruction Adherence&lt;/th&gt; &#xA;   &lt;th&gt;Consistency&lt;/th&gt; &#xA;   &lt;th&gt;Visual Quality&lt;/th&gt; &#xA;   &lt;th&gt;Motion Quality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://runwayml.com/research/introducing-gen-3-alpha&#34;&gt;Runway-Gen3 Alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.53&lt;/td&gt; &#xA;   &lt;td&gt;2.19&lt;/td&gt; &#xA;   &lt;td&gt;2.57&lt;/td&gt; &#xA;   &lt;td&gt;3.23&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.82&lt;/td&gt; &#xA;   &lt;td&gt;2.64&lt;/td&gt; &#xA;   &lt;td&gt;2.81&lt;/td&gt; &#xA;   &lt;td&gt;3.20&lt;/td&gt; &#xA;   &lt;td&gt;2.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://klingai.com&#34;&gt;Kling-1.6 STD Mode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.99&lt;/td&gt; &#xA;   &lt;td&gt;2.77&lt;/td&gt; &#xA;   &lt;td&gt;3.05&lt;/td&gt; &#xA;   &lt;td&gt;3.39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.76&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hailuoai.video&#34;&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.8&lt;/td&gt; &#xA;   &lt;td&gt;3.08&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;   &lt;td&gt;2.91&lt;/td&gt; &#xA;   &lt;td&gt;3.31&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.54&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.15&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.35&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.34&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;The evaluation demonstrates that our model achieves significant advancements in &lt;strong&gt;instruction adherence (3.15)&lt;/strong&gt; compared to baseline methods, while maintaining competitive performance in &lt;strong&gt;motion quality (2.74)&lt;/strong&gt; without sacrificing the &lt;strong&gt;consistency (3.35)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image To Video Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;Instruction Adherence&lt;/th&gt; &#xA;   &lt;th&gt;Consistency&lt;/th&gt; &#xA;   &lt;th&gt;Visual Quality&lt;/th&gt; &#xA;   &lt;th&gt;Motion Quality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;   &lt;td&gt;2.97&lt;/td&gt; &#xA;   &lt;td&gt;2.95&lt;/td&gt; &#xA;   &lt;td&gt;2.87&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;3.10&lt;/td&gt; &#xA;   &lt;td&gt;2.81&lt;/td&gt; &#xA;   &lt;td&gt;3.00&lt;/td&gt; &#xA;   &lt;td&gt;2.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hailuoai.video&#34;&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.05&lt;/td&gt; &#xA;   &lt;td&gt;3.31&lt;/td&gt; &#xA;   &lt;td&gt;2.58&lt;/td&gt; &#xA;   &lt;td&gt;3.55&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://klingai.com&#34;&gt;Kling-1.6 Pro Mode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.4&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;3.03&lt;/td&gt; &#xA;   &lt;td&gt;3.58&lt;/td&gt; &#xA;   &lt;td&gt;3.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://runwayml.com/research/introducing-runway-gen-4&#34;&gt;Runway-Gen4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.39&lt;/td&gt; &#xA;   &lt;td&gt;3.75&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.4&lt;/td&gt; &#xA;   &lt;td&gt;3.37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2-DF&lt;/td&gt; &#xA;   &lt;td&gt;3.24&lt;/td&gt; &#xA;   &lt;td&gt;3.64&lt;/td&gt; &#xA;   &lt;td&gt;3.21&lt;/td&gt; &#xA;   &lt;td&gt;3.18&lt;/td&gt; &#xA;   &lt;td&gt;2.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2-I2V&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;3.42&lt;/td&gt; &#xA;   &lt;td&gt;3.18&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;3.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our results demonstrate that both &lt;strong&gt;SkyReels-V2-I2V (3.29)&lt;/strong&gt; and &lt;strong&gt;SkyReels-V2-DF (3.24)&lt;/strong&gt; achieve state-of-the-art performance among open-source models, significantly outperforming HunyuanVideo-13B (2.84) and Wan2.1-14B (2.85) across all quality dimensions. With an average score of 3.29, SkyReels-V2-I2V demonstrates comparable performance to proprietary models Kling-1.6 (3.4) and Runway-Gen4 (3.39).&lt;/p&gt; &#xA;&lt;h4&gt;VBench&lt;/h4&gt; &#xA;&lt;p&gt;To objectively compare SkyReels-V2 Model against other leading open-source Text-To-Video models, we conduct comprehensive evaluations using the public benchmark &lt;a href=&#34;https://github.com/Vchitect/VBench&#34;&gt;V-Bench&lt;/a&gt;. Our evaluation specifically leverages the benchmarks longer version prompt. For fair comparison with baseline models, we strictly follow their recommended setting for inference.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Total Score&lt;/th&gt; &#xA;   &lt;th&gt;Quality Score&lt;/th&gt; &#xA;   &lt;th&gt;Semantic Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;OpenSora 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;81.5 %&lt;/td&gt; &#xA;   &lt;td&gt;82.1 %&lt;/td&gt; &#xA;   &lt;td&gt;78.2 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;CogVideoX1.5-5B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80.3 %&lt;/td&gt; &#xA;   &lt;td&gt;80.9 %&lt;/td&gt; &#xA;   &lt;td&gt;77.9 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;82.7 %&lt;/td&gt; &#xA;   &lt;td&gt;84.4 %&lt;/td&gt; &#xA;   &lt;td&gt;76.2 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.7 %&lt;/td&gt; &#xA;   &lt;td&gt;84.2 %&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;81.4 %&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.9 %&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.7 %&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80.8 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;The VBench results demonstrate that SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B, With the highest &lt;strong&gt;total score (83.9%)&lt;/strong&gt; and &lt;strong&gt;quality score (84.7%)&lt;/strong&gt;. In this evaluation, the semantic score is slightly lower than Wan2.1-14B, while we outperform Wan2.1-14B in human evaluations, with the primary gap attributed to V-Benchs insufficient evaluation of shot-scenario semantic adherence.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors of &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan 2.1&lt;/a&gt;, &lt;a href=&#34;https://github.com/xdit-project/xDiT&#34;&gt;XDit&lt;/a&gt; and &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen 2.5&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2025skyreelsv2infinitelengthfilmgenerative,&#xA;      title={SkyReels-V2: Infinite-length Film Generative Model}, &#xA;      author={Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou},&#xA;      year={2025},&#xA;      eprint={2504.13074},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2504.13074}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen3</title>
    <updated>2025-07-25T01:35:00Z</updated>
    <id>tag:github.com,2025-07-25:/QwenLM/Qwen3</id>
    <link href="https://github.com/QwenLM/Qwen3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;  &lt;a href=&#34;https://chat.qwen.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://arxiv.org/abs/2505.09388&#34;&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://qwenlm.github.io/blog/qwen3/&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; 锝 &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; ワ &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen3-Demo&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (寰淇)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-&lt;/code&gt; or visit the &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&#34;&gt;Qwen3 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;p&gt;To learn more about Qwen3, feel free to read our documentation [&lt;a href=&#34;https://qwen.readthedocs.io/en/latest/&#34;&gt;EN&lt;/a&gt;|&lt;a href=&#34;https://qwen.readthedocs.io/zh-cn/latest/&#34;&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; &#xA; &lt;li&gt;Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;&lt;/li&gt; &#xA; &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; &#xA; &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; &#xA; &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We are excited to introduce the updated version of the &lt;strong&gt;Qwen3-235B-A22B non-thinking mode&lt;/strong&gt;, named &lt;strong&gt;Qwen3-235B-A22B-Instruct-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K-token long-context understanding&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-235B-A22B-Instruct-2507.jpeg&#34; alt=&#34;Qwen3-235B-A22B-Instruct-2507&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The updated versions of &lt;strong&gt;more Qwen3 model sizes&lt;/strong&gt; and for &lt;strong&gt;thinking mode&lt;/strong&gt; are also expected to be released very soon. Stay tuned&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Previous News for Qwen3 Release&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. &lt;br&gt;&lt;br&gt; The highlights from Qwen3 include: &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;b&gt;Dense and Mixture-of-Experts (MoE) models of various sizes&lt;/b&gt;, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Seamless switching between thinking mode&lt;/b&gt; (for complex logical reasoning, math, and coding) and &lt;b&gt;non-thinking mode&lt;/b&gt; (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Significantly enhancement in reasoning capabilities&lt;/b&gt;, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Superior human preference alignment&lt;/b&gt;, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Expertise in agent capabilities&lt;/b&gt;, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Support of 100+ languages and dialects&lt;/b&gt; with strong capabilities for &lt;b&gt;multilingual instruction following&lt;/b&gt; and &lt;b&gt;translation&lt;/b&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our &lt;a href=&#34;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&#34;&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; &#xA; &lt;li&gt;2025.04.29: We released the Qwen3 series. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen3&#34;&gt;blog&lt;/a&gt; for more details!&lt;/li&gt; &#xA; &lt;li&gt;2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5&#34;&gt;blog&lt;/a&gt; for more!&lt;/li&gt; &#xA; &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen-moe/&#34;&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; &#xA; &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Detailed evaluation results are reported in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen3/&#34;&gt; blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run Qwen3&lt;/h2&gt; &#xA;&lt;h3&gt; Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Transformers is a library of pretrained natural language processing for inference and training. The latest version of &lt;code&gt;transformers&lt;/code&gt; is recommended and &lt;code&gt;transformers&amp;gt;=4.51.0&lt;/code&gt; is required.&lt;/p&gt; &#xA;&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-235B-A22B-Instruct-2507 to generate content based on given inputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen3-235B-A22B-Instruct-2507&#34;&#xA;&#xA;# load the tokenizer and the model&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;&#xA;# prepare the model input&#xA;prompt = &#34;Give me a short introduction to large language model.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True,&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;# conduct text completion&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=16384&#xA;)&#xA;output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() &#xA;&#xA;content = tokenizer.decode(output_ids, skip_special_tokens=True)&#xA;&#xA;print(&#34;content:&#34;, content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] The updated version of Qwen3-235B-A22B, namely &lt;strong&gt;Qwen3-235B-A22B-Instruct-2507&lt;/strong&gt; supports &lt;strong&gt;only non-thinking mode&lt;/strong&gt; and &lt;strong&gt;does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks&lt;/strong&gt; in its output. Meanwhile, &lt;strong&gt;specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Switching Thinking/Non-thinking Modes for Previous Qwen3 Hybrid Models&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; By default, Qwen3 models will think before response. This could be controlled by &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;enable_thinking=False&lt;/code&gt;: Passing &lt;code&gt;enable_thinking=False&lt;/code&gt; to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. ModelScope adopts a Python API similar to Transformers. The CLI tool &lt;code&gt;modelscope download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;llama.cpp&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware. &lt;code&gt;llama.cpp&amp;gt;=b5092&lt;/code&gt; is required for the support of Qwen3 architecture. &lt;code&gt;llama.cpp&amp;gt;=b5401&lt;/code&gt; is recommended for the full support of the official Qwen3 chat template.&lt;/p&gt; &#xA;&lt;p&gt;To use the CLI, run the following in a terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift&#xA;# CTRL+C to exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the API server, run the following in a terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A simple web front end will be at &lt;code&gt;http://localhost:8080&lt;/code&gt; and an OpenAI-compatible API will be at &lt;code&gt;http://localhost:8080/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For additional guides, please refer to &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html&#34;&gt;our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] llama.cpp adopts &#34;rotating context management&#34; and infinite generation is made possible by evicting earlier tokens. It could configured by parameters and the commands above effectively disable it. For more details, please refer to &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli&#34;&gt;our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://ollama.com/&#34;&gt;installing Ollama&lt;/a&gt;, you can initiate the Ollama service with the following command (Ollama v0.6.6 or higher is required):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama serve&#xA;# You need to keep this service running whenever you are using ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen3&lt;/code&gt;, such as &lt;code&gt;:8b&lt;/code&gt; or &lt;code&gt;:30b-a3b&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run qwen3:8b&#xA;# Setting parameters, type &#34;/set parameter num_ctx 40960&#34; and &#34;/set parameter num_predict 32768&#34;&#xA;# To exit, type &#34;/bye&#34; and press ENTER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the Ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen3:8b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared. The API is at &lt;code&gt;http://localhost:11434/v1/&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;p&gt;For additional details, please visit &lt;a href=&#34;https://ollama.com/&#34;&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Ollama adopts the same &#34;rotating context management&#34; with llama.cpp. However, its default settings (&lt;code&gt;num_ctx&lt;/code&gt; 2048 and &lt;code&gt;num_predict&lt;/code&gt; -1), suggesting infinite generation with a 2048-token context, could lead to trouble for Qwen3 models. We recommend setting &lt;code&gt;num_ctx&lt;/code&gt; and &lt;code&gt;num_predict&lt;/code&gt; properly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;LMStudio&lt;/h3&gt; &#xA;&lt;p&gt;Qwen3 has already been supported by &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; &#xA;&lt;h3&gt;ExecuTorch&lt;/h3&gt; &#xA;&lt;p&gt;To export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this &lt;a href=&#34;https://github.com/pytorch/executorch/raw/main/examples/models/qwen3/README.md&#34;&gt;example&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;MNN&lt;/h3&gt; &#xA;&lt;p&gt;To export and run on MNN, which supports Qwen3 on mobile devices, please visit &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;Alibaba MNN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;MLX LM&lt;/h3&gt; &#xA;&lt;p&gt;If you are running on Apple Silicon, &lt;a href=&#34;https://github.com/ml-explore/mlx-lm&#34;&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt; also supports Qwen3 (&lt;code&gt;mlx-lm&amp;gt;=0.24.0&lt;/code&gt;). Look for models ending with MLX on Hugging Face Hub.&lt;/p&gt; &#xA;&lt;h3&gt;OpenVINO&lt;/h3&gt; &#xA;&lt;p&gt;If you are running on Intel CPU or GPU, &lt;a href=&#34;https://github.com/openvinotoolkit&#34;&gt;OpenVINO toolkit&lt;/a&gt; supports Qwen3. You can follow this &lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks/raw/latest/notebooks/llm-chatbot/llm-chatbot.ipynb&#34;&gt;chatbot example&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ### Text generation web UI&#xA;&#xA;You can directly use [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui) for creating a web UI demo. If you use GGUF, remember to install the latest wheel of `llama.cpp` with the support of Qwen2.5. --&gt; &#xA;&lt;!-- ### llamafile&#xA;&#xA;Clone [`llamafile`](https://github.com/Mozilla-Ocho/llamafile), run source install, and then create your own llamafile with the GGUF file following the guide [here](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles). You are able to run one line of command, say `./qwen.llamafile`, to create a demo. --&gt; &#xA;&lt;h2&gt;Deploy Qwen3&lt;/h2&gt; &#xA;&lt;p&gt;Qwen3 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;SGLang&lt;/code&gt;, &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;TensorRT-LLM&lt;/code&gt;. You can also find Qwen3 models from various inference providers, e.g., &lt;a href=&#34;https://www.alibabacloud.com/en/product/modelstudio&#34;&gt;Alibaba Cloud Model Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SGLang&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; is a fast serving framework for large language models and vision language models. SGLang could be used to launch a server with OpenAI-compatible API service. &lt;code&gt;sglang&amp;gt;=0.4.6.post1&lt;/code&gt; is required. It is as easy as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --reasoning-parser qwen3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:30000/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; is a high-throughput and memory-efficient inference and serving engine for LLMs. &lt;code&gt;vllm&amp;gt;=0.8.5&lt;/code&gt; is recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vllm serve Qwen/Qwen3-8B --port 8000 --enable-reasoning --reasoning-parser deepseek_r1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt; is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected &lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/torch.html&#34;&gt;PyTorch backend&lt;/a&gt;. &lt;code&gt;tensorrt_llm&amp;gt;=0.20.0rc3&lt;/code&gt; is recommended. Please refer to the &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/models/core/qwen/README.md#qwen3&#34;&gt;README&lt;/a&gt; page for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;MindIE&lt;/h3&gt; &#xA;&lt;p&gt;For deployment on Ascend NPUs, please visit &lt;a href=&#34;https://modelers.cn/&#34;&gt;Modelers&lt;/a&gt; and search for Qwen3.&lt;/p&gt; &#xA;&lt;!-- &#xA;### OpenLLM&#xA;&#xA;[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily runQwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:&#xA;&#xA;```bash&#xA;openllm serve qwen2.5:7b&#xA;```&#xA;&#xA;The server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). --&gt; &#xA;&lt;h2&gt;Build with Qwen3&lt;/h2&gt; &#xA;&lt;h3&gt;Tool Use&lt;/h3&gt; &#xA;&lt;p&gt;For tool use capabilities, we recommend taking a look at &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;, which provides a wrapper around these APIs to support tool use or function calling with MCP support. Tool use with Qwen3 can also be conducted with SGLang, vLLM, Transformers, llama.cpp, Ollama, etc. Follow guides in our documentation to see how to enable the support.&lt;/p&gt; &#xA;&lt;h3&gt;Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;We advise you to use training frameworks, including &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;, &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;UnSloth&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift&#34;&gt;Swift&lt;/a&gt;, &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;, etc., to finetune your models with SFT, DPO, GRPO, etc.&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;All our open-weight models are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{qwen3,&#xA;    title={Qwen3 Technical Report}, &#xA;    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},&#xA;    journal = {arXiv preprint arXiv:2505.09388},&#xA;    year={2025}&#xA;}&#xA;&#xA;@article{qwen2.5,&#xA;    title   = {Qwen2.5 Technical Report}, &#xA;    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},&#xA;    journal = {arXiv preprint arXiv:2412.15115},&#xA;    year    = {2024}&#xA;}&#xA;&#xA;@article{qwen2,&#xA;    title   = {Qwen2 Technical Report}, &#xA;    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},&#xA;    journal = {arXiv preprint arXiv:2407.10671},&#xA;    year    = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen3-Coder</title>
    <updated>2025-07-25T01:35:00Z</updated>
    <id>tag:github.com,2025-07-25:/QwenLM/Qwen3-Coder</id>
    <link href="https://github.com/QwenLM/Qwen3-Coder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;  &lt;a href=&#34;https://chat.qwenlm.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://qwenlm.github.io/blog/qwen3-coder&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; 锝 &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev&#34;&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (寰淇)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://arxiv.org/abs/2505.09388&#34;&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;  &lt;a href=&#34;https://github.com/QwenLM/qwen-code&#34;&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;h1&gt;Qwen3-Coder: Agentic Coding in the World.&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Today, we&#39;re announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, but we&#39;re excited to introduce its most powerful variant first: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;  a 480B-parameter Mixture-of-Experts model with 35B active parameters, offering exceptional performance in both coding and agentic tasks. &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet.&lt;/p&gt; &#xA;&lt;p&gt; &lt;strong&gt;Significant Performance&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet;&lt;/p&gt; &#xA;&lt;p&gt; &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding;&lt;/p&gt; &#xA;&lt;p&gt; &lt;strong&gt;Agentic Coding&lt;/strong&gt;: supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; &#xA;&lt;h2&gt;Basic information&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; &#xA; &lt;li&gt; Supporting 358 coding languages;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#39;ABAP&#39;, &#39;ActionScript&#39;, &#39;Ada&#39;, &#39;Agda&#39;, &#39;Alloy&#39;, &#39;ApacheConf&#39;, &#39;AppleScript&#39;, &#39;Arc&#39;, &#39;Arduino&#39;, &#39;AsciiDoc&#39;, &#39;AspectJ&#39;, &#39;Assembly&#39;, &#39;Augeas&#39;, &#39;AutoHotkey&#39;, &#39;AutoIt&#39;, &#39;Awk&#39;, &#39;Batchfile&#39;, &#39;Befunge&#39;, &#39;Bison&#39;, &#39;BitBake&#39;, &#39;BlitzBasic&#39;, &#39;BlitzMax&#39;, &#39;Bluespec&#39;, &#39;Boo&#39;, &#39;Brainfuck&#39;, &#39;Brightscript&#39;, &#39;Bro&#39;, &#39;C&#39;, &#39;C#&#39;, &#39;C++&#39;, &#39;C2hs Haskell&#39;, &#39;CLIPS&#39;, &#39;CMake&#39;, &#39;COBOL&#39;, &#39;CSS&#39;, &#39;CSV&#39;, &#34;Cap&#39;n Proto&#34;, &#39;CartoCSS&#39;, &#39;Ceylon&#39;, &#39;Chapel&#39;, &#39;ChucK&#39;, &#39;Cirru&#39;, &#39;Clarion&#39;, &#39;Clean&#39;, &#39;Click&#39;, &#39;Clojure&#39;, &#39;CoffeeScript&#39;, &#39;ColdFusion&#39;, &#39;ColdFusion CFC&#39;, &#39;Common Lisp&#39;, &#39;Component Pascal&#39;, &#39;Coq&#39;, &#39;Creole&#39;, &#39;Crystal&#39;, &#39;Csound&#39;, &#39;Cucumber&#39;, &#39;Cuda&#39;, &#39;Cycript&#39;, &#39;Cython&#39;, &#39;D&#39;, &#39;DIGITAL Command Language&#39;, &#39;DM&#39;, &#39;DNS Zone&#39;, &#39;Darcs Patch&#39;, &#39;Dart&#39;, &#39;Diff&#39;, &#39;Dockerfile&#39;, &#39;Dogescript&#39;, &#39;Dylan&#39;, &#39;E&#39;, &#39;ECL&#39;, &#39;Eagle&#39;, &#39;Ecere Projects&#39;, &#39;Eiffel&#39;, &#39;Elixir&#39;, &#39;Elm&#39;, &#39;Emacs Lisp&#39;, &#39;EmberScript&#39;, &#39;Erlang&#39;, &#39;F#&#39;, &#39;FLUX&#39;, &#39;FORTRAN&#39;, &#39;Factor&#39;, &#39;Fancy&#39;, &#39;Fantom&#39;, &#39;Forth&#39;, &#39;FreeMarker&#39;, &#39;G-code&#39;, &#39;GAMS&#39;, &#39;GAP&#39;, &#39;GAS&#39;, &#39;GDScript&#39;, &#39;GLSL&#39;, &#39;Genshi&#39;, &#39;Gentoo Ebuild&#39;, &#39;Gentoo Eclass&#39;, &#39;Gettext Catalog&#39;, &#39;Glyph&#39;, &#39;Gnuplot&#39;, &#39;Go&#39;, &#39;Golo&#39;, &#39;Gosu&#39;, &#39;Grace&#39;, &#39;Gradle&#39;, &#39;Grammatical Framework&#39;, &#39;GraphQL&#39;, &#39;Graphviz (DOT)&#39;, &#39;Groff&#39;, &#39;Groovy&#39;, &#39;Groovy Server Pages&#39;, &#39;HCL&#39;, &#39;HLSL&#39;, &#39;HTML&#39;, &#39;HTML+Django&#39;, &#39;HTML+EEX&#39;, &#39;HTML+ERB&#39;, &#39;HTML+PHP&#39;, &#39;HTTP&#39;, &#39;Haml&#39;, &#39;Handlebars&#39;, &#39;Harbour&#39;, &#39;Haskell&#39;, &#39;Haxe&#39;, &#39;Hy&#39;, &#39;IDL&#39;, &#39;IGOR Pro&#39;, &#39;INI&#39;, &#39;IRC log&#39;, &#39;Idris&#39;, &#39;Inform 7&#39;, &#39;Inno Setup&#39;, &#39;Io&#39;, &#39;Ioke&#39;, &#39;Isabelle&#39;, &#39;J&#39;, &#39;JFlex&#39;, &#39;JSON&#39;, &#39;JSON5&#39;, &#39;JSONLD&#39;, &#39;JSONiq&#39;, &#39;JSX&#39;, &#39;Jade&#39;, &#39;Jasmin&#39;, &#39;Java&#39;, &#39;Java Server Pages&#39;, &#39;JavaScript&#39;, &#39;Julia&#39;, &#39;Jupyter Notebook&#39;, &#39;KRL&#39;, &#39;KiCad&#39;, &#39;Kit&#39;, &#39;Kotlin&#39;, &#39;LFE&#39;, &#39;LLVM&#39;, &#39;LOLCODE&#39;, &#39;LSL&#39;, &#39;LabVIEW&#39;, &#39;Lasso&#39;, &#39;Latte&#39;, &#39;Lean&#39;, &#39;Less&#39;, &#39;Lex&#39;, &#39;LilyPond&#39;, &#39;Linker Script&#39;, &#39;Liquid&#39;, &#39;Literate Agda&#39;, &#39;Literate CoffeeScript&#39;, &#39;Literate Haskell&#39;, &#39;LiveScript&#39;, &#39;Logos&#39;, &#39;Logtalk&#39;, &#39;LookML&#39;, &#39;Lua&#39;, &#39;M&#39;, &#39;M4&#39;, &#39;MAXScript&#39;, &#39;MTML&#39;, &#39;MUF&#39;, &#39;Makefile&#39;, &#39;Mako&#39;, &#39;Maple&#39;, &#39;Markdown&#39;, &#39;Mask&#39;, &#39;Mathematica&#39;, &#39;Matlab&#39;, &#39;Max&#39;, &#39;MediaWiki&#39;, &#39;Metal&#39;, &#39;MiniD&#39;, &#39;Mirah&#39;, &#39;Modelica&#39;, &#39;Module Management System&#39;, &#39;Monkey&#39;, &#39;MoonScript&#39;, &#39;Myghty&#39;, &#39;NSIS&#39;, &#39;NetLinx&#39;, &#39;NetLogo&#39;, &#39;Nginx&#39;, &#39;Nimrod&#39;, &#39;Ninja&#39;, &#39;Nit&#39;, &#39;Nix&#39;, &#39;Nu&#39;, &#39;NumPy&#39;, &#39;OCaml&#39;, &#39;ObjDump&#39;, &#39;Objective-C++&#39;, &#39;Objective-J&#39;, &#39;Octave&#39;, &#39;Omgrofl&#39;, &#39;Opa&#39;, &#39;Opal&#39;, &#39;OpenCL&#39;, &#39;OpenEdge ABL&#39;, &#39;OpenSCAD&#39;, &#39;Org&#39;, &#39;Ox&#39;, &#39;Oxygene&#39;, &#39;Oz&#39;, &#39;PAWN&#39;, &#39;PHP&#39;, &#39;POV-Ray SDL&#39;, &#39;Pan&#39;, &#39;Papyrus&#39;, &#39;Parrot&#39;, &#39;Parrot Assembly&#39;, &#39;Parrot Internal Representation&#39;, &#39;Pascal&#39;, &#39;Perl&#39;, &#39;Perl6&#39;, &#39;Pickle&#39;, &#39;PigLatin&#39;, &#39;Pike&#39;, &#39;Pod&#39;, &#39;PogoScript&#39;, &#39;Pony&#39;, &#39;PostScript&#39;, &#39;PowerShell&#39;, &#39;Processing&#39;, &#39;Prolog&#39;, &#39;Propeller Spin&#39;, &#39;Protocol Buffer&#39;, &#39;Public Key&#39;, &#39;Pure Data&#39;, &#39;PureBasic&#39;, &#39;PureScript&#39;, &#39;Python&#39;, &#39;Python traceback&#39;, &#39;QML&#39;, &#39;QMake&#39;, &#39;R&#39;, &#39;RAML&#39;, &#39;RDoc&#39;, &#39;REALbasic&#39;, &#39;RHTML&#39;, &#39;RMarkdown&#39;, &#39;Racket&#39;, &#39;Ragel in Ruby Host&#39;, &#39;Raw token data&#39;, &#39;Rebol&#39;, &#39;Red&#39;, &#39;Redcode&#39;, &#34;Ren&#39;Py&#34;, &#39;RenderScript&#39;, &#39;RobotFramework&#39;, &#39;Rouge&#39;, &#39;Ruby&#39;, &#39;Rust&#39;, &#39;SAS&#39;, &#39;SCSS&#39;, &#39;SMT&#39;, &#39;SPARQL&#39;, &#39;SQF&#39;, &#39;SQL&#39;, &#39;STON&#39;, &#39;SVG&#39;, &#39;Sage&#39;, &#39;SaltStack&#39;, &#39;Sass&#39;, &#39;Scala&#39;, &#39;Scaml&#39;, &#39;Scheme&#39;, &#39;Scilab&#39;, &#39;Self&#39;, &#39;Shell&#39;, &#39;ShellSession&#39;, &#39;Shen&#39;, &#39;Slash&#39;, &#39;Slim&#39;, &#39;Smali&#39;, &#39;Smalltalk&#39;, &#39;Smarty&#39;, &#39;Solidity&#39;, &#39;SourcePawn&#39;, &#39;Squirrel&#39;, &#39;Stan&#39;, &#39;Standard ML&#39;, &#39;Stata&#39;, &#39;Stylus&#39;, &#39;SuperCollider&#39;, &#39;Swift&#39;, &#39;SystemVerilog&#39;, &#39;TOML&#39;, &#39;TXL&#39;, &#39;Tcl&#39;, &#39;Tcsh&#39;, &#39;TeX&#39;, &#39;Tea&#39;, &#39;Text&#39;, &#39;Textile&#39;, &#39;Thrift&#39;, &#39;Turing&#39;, &#39;Turtle&#39;, &#39;Twig&#39;, &#39;TypeScript&#39;, &#39;Unified Parallel C&#39;, &#39;Unity3D Asset&#39;, &#39;Uno&#39;, &#39;UnrealScript&#39;, &#39;UrWeb&#39;, &#39;VCL&#39;, &#39;VHDL&#39;, &#39;Vala&#39;, &#39;Verilog&#39;, &#39;VimL&#39;, &#39;Visual Basic&#39;, &#39;Volt&#39;, &#39;Vue&#39;, &#39;Web Ontology Language&#39;, &#39;WebAssembly&#39;, &#39;WebIDL&#39;, &#39;X10&#39;, &#39;XC&#39;, &#39;XML&#39;, &#39;XPages&#39;, &#39;XProc&#39;, &#39;XQuery&#39;, &#39;XS&#39;, &#39;XSLT&#39;, &#39;Xojo&#39;, &#39;Xtend&#39;, &#39;YAML&#39;, &#39;YANG&#39;, &#39;Yacc&#39;, &#39;Zephir&#39;, &#39;Zig&#39;, &#39;Zimpl&#39;, &#39;desktop&#39;, &#39;eC&#39;, &#39;edn&#39;, &#39;fish&#39;, &#39;mupad&#39;, &#39;nesC&#39;, &#39;ooc&#39;, &#39;reStructuredText&#39;, &#39;wisp&#39;, &#39;xBase&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; Retain strengths in math and general capabilities from base model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important]&lt;/p&gt; &#xA; &lt;p&gt;Qwen3-coder function calling relies on our new tool parser &lt;code&gt;qwen3coder_tool_parser.py&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/qwen3coder_tool_parser.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model name&lt;/th&gt; &#xA;   &lt;th&gt;type&lt;/th&gt; &#xA;   &lt;th&gt;length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;256k&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;&gt;Hugging Face&lt;/a&gt;   &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;256k&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8&#34;&gt;Hugging Face&lt;/a&gt;   &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen3-coder&#34;&gt;  blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; &#xA; &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.**&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt; Chat with Qwen3-Coder-480B-A35B-Instruct&lt;/h3&gt; &#xA;&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-480B-A35B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;write a quick sort algorithm.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=65536&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; &#xA;&lt;h4&gt;Fill in the middle with Qwen3-Coder-480B-A35B-Instruct&lt;/h4&gt; &#xA;&lt;p&gt;The code insertion task, also referred to as the &#34;fill-in-the-middle&#34; challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper &#34;Efficient Training of Language Models to Fill in the Middle&#34;[&lt;a href=&#34;https://arxiv.org/abs/2207.14255&#34;&gt;arxiv&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;&amp;lt;|fim_prefix|&amp;gt;&#39; + prefix_code + &#39;&amp;lt;|fim_suffix|&amp;gt;&#39; + suffix_code + &#39;&amp;lt;|fim_middle|&amp;gt;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;# load model&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;&#xA;input_text = &#34;&#34;&#34;&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[len(arr) // 2]&#xA;    &amp;lt;|fim_suffix|&amp;gt;&#xA;    middle = [x for x in arr if x == pivot]&#xA;    right = [x for x in arr if x &amp;gt; pivot]&#xA;    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;&#34;&#34;&#34;&#xA;            &#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a code completion assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: input_text}&#xA;]&#xA;&#xA;&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = TOKENIZER([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: {input_text}\n\nGenerated text: {output_text}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Use Cases&lt;/h2&gt; &#xA;&lt;h3&gt;Example: Physics-Based Chimney Demolition Simulation with Controlled Explosion&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;浣跨 three.js, cannon-es.js 涓涓肩3D寤虹ゆ绀恒&#xA;&#xA;## 烘璁剧疆锛&#xA;- 伴㈡涓涓娣辩拌叉贩骞抽锛灏哄80*80锛&#xA;- ╀涓ユ奸靛惊板╃瑙锛╂纰版妫娴ㄩ瀹&#xA;&#xA;## 寤虹缁锛&#xA;- 涓搴у褰㈤灞寤虹锛ㄩ垮瑰20涓瑰&#xA;- 寤虹婚搴60涓瑰&#xA;- 姣灞ㄧ缁锛瑰涓缁寤虹涓, 寮50%锛澧寮虹绋冲&#xA;- 寤虹澶澧浣跨ㄧ背叉瑰&#xA;- **瑕锛瑰濮跺椤荤‘淇绱у璐村锛撮锛浠ラ杩杞诲井璋村寰ュ**&#xA;- **瑕锛寤虹濮瀹锛瑰搴璇ュ浜╃&#34;＄&#34;舵锛纭淇寤虹ㄧ稿淇瀹缇姝㈢舵锛涓浼涓娌炬**&#xA;- 寤虹涔翠娇ㄧф濉锛涓瑙锛锛杩楂╂锛0.8+锛浣寮规э0.05浠ヤ锛ユā绮&#xA;- ㄥ缓绛濉翠浼ｆ锛寤虹浣涓轰涓翠ㄥ伴㈢跺杩澶цｆ&#xA;&#xA;## 瀹寸郴缁锛&#xA;- ㄥ缓绛绗1灞充晶瑰杩瀹瑁歌缃锛涓瑙锛&#xA;- 渚浣瑰荤&#xA;- **告跺ら稿虫瑰╃舵**&#xA;- 哥逛骇寰2寮哄插绘尝锛插绘尝褰卞扮瑰, 2-5浣插诲&#xA;&#xA;## 寤虹绋冲ц姹锛&#xA;- **纭淇寤虹ㄦ告跺ㄩ姝锛浠讳ㄦ涓娌**&#xA;- **╃涓濮缁寤虹涓╃姝ラゆヨ剁ǔ瀹锛浣跨ㄧ＄哄**&#xA;- **瑰寸ヨЕ搴锋楂╂浣寮规э妯℃寸娴绮**&#xA;&#xA;## 肩濉锛&#xA;- 瑰ㄧ稿插讳涓浠椋ｏ杩浼ㄧ┖涓缈绘纰版&#xA;- 灏浼寤虹濉娓╂ｏラ瀹ょ板烘&#xA;&#xA;## 澧寮虹瑙瑙锛&#xA;- 娣诲澧у锛哥翠寒搴婵澧锛跺琚灏″&#xA;- 绮瀛绯荤锛俱板&#xA;&#xA;## 瑕姹锛&#xA;- 绮瀛绯荤ㄤ惧板&#xA;- 浠ｇㄥ涓HTML浠朵腑锛蹇瑕CSS峰&#xA;- 娣诲绠UIу讹缃告鸿搴, 告, 榧宸у舵鸿搴锛抽у舵轰缃锛婊杞у舵虹璺&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo1.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example1.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: Multicolor and Interactive Animation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prompt with Cline [act mode] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Create an amazing animation multicolor and interactive using p5js&#xA;&#xA;use this cdn:&#xA;https://cdn.jsdelivr.net/npm/p5@1.7.0/lib/p5.min.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo2.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example2.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: 3D Google Earth&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;To create a 3D Google Earth, you need to load the terrain map correctly. You can use any online resource. The code is written into an HTML file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo3.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example3.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: Testing Your WPM with a Famous Quote&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Prompt with Qwen-Code CLI &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Create an interesting typing game with a keyboard in the lower middle of the screen and some famous articles in the upper middle. When the user types a word correctly, a cool reaction should be given to encourage him. Design a modern soft color scheme inspired by macarons. Come up with a very creative solution first, and then start writing code.&#xA;The game should be able to support typing, and you need to neglect upcase and lowercase.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo4.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example4.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: Bouncing Ball in Rotation Hypercube&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Make a page in HTML that shows an animation of a ball bouncing in a rotating hypercube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo5.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example5.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: Solar System Simulation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;write a web page to show the solar system simulation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo6.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example6.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Example: DUET Game&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Create a complete, single-file HTML game with CSS and JavaScript. The game is inspired by &#34;Duet&#34;.&#xA;&#xA;Gameplay:&#xA;&#xA;There are two balls, one red and one blue, rotating around a central point.&#xA;The player uses the &#39;A&#39; and &#39;D&#39; keys to rotate them counter-clockwise and clockwise.&#xA;White rectangular obstacles move down from the top of the screen.&#xA;The player must rotate the balls to avoid hitting the obstacles.&#xA;If a ball hits an obstacle, the game is over.&#xA;Visuals:&#xA;&#xA;Make the visual effects amazing.&#xA;Use a dark background with neon glowing effects for the balls and obstacles.&#xA;Animations should be very smooth.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo7.mp4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example7.png&#34; width=&#34;400&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{qwen3technicalreport,&#xA;      title={Qwen3 Technical Report}, &#xA;      author={Qwen Team},&#xA;      year={2025},&#xA;      eprint={2505.09388},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2505.09388},&#xA;}&#xA;@article{hui2024qwen2,&#xA;  title={Qwen2. 5-Coder Technical Report},&#xA;  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},&#xA;  journal={arXiv preprint arXiv:2409.12186},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt;  Back to Top  &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>