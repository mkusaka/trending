<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-17T01:34:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>roboflow/supervision</title>
    <updated>2023-08-17T01:34:32Z</updated>
    <id>tag:github.com,2023-08-17:/roboflow/supervision</id>
    <link href="https://github.com/roboflow/supervision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We write your reusable computer vision tools. üíú&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/supervision.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/supervision&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/supervision&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/supervision&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üëã hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù&lt;/p&gt; &#xA;&lt;h2&gt;üíª install&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the supervision package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;3.11&amp;gt;=Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install supervision[desktop]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read more about desktop, headless, and local installation in our &lt;a href=&#34;https://roboflow.github.io/supervision/&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üî• quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://roboflow.github.io/supervision/detection/core/&#34;&gt;detections processing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&amp;gt;&amp;gt;&amp;gt; from ultralytics import YOLO&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; model = YOLO(&#39;yolov8s.pt&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; result = model(IMAGE)[0]&#xA;&amp;gt;&amp;gt;&amp;gt; detections = sv.Detections.from_ultralytics(result)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; len(detections)&#xA;5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;üëâ more detections utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Easily switch inference pipeline between supported object detection/instance segmentation models&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&amp;gt;&amp;gt;&amp;gt; from segment_anything import sam_model_registry, SamAutomaticMaskGenerator&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)&#xA;&amp;gt;&amp;gt;&amp;gt; mask_generator = SamAutomaticMaskGenerator(sam)&#xA;&amp;gt;&amp;gt;&amp;gt; sam_result = mask_generator.generate(IMAGE)&#xA;&amp;gt;&amp;gt;&amp;gt; detections = sv.Detections.from_sam(sam_result=sam_result)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://roboflow.github.io/supervision/quickstart/detections/&#34;&gt;Advanced filtering&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; detections = detections[detections.class_id == 0]&#xA;&amp;gt;&amp;gt;&amp;gt; detections = detections[detections.confidence &amp;gt; 0.5]&#xA;&amp;gt;&amp;gt;&amp;gt; detections = detections[detections.area &amp;gt; 1000]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Image annotation&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; box_annotator = sv.BoxAnnotator()&#xA;&amp;gt;&amp;gt;&amp;gt; annotated_frame = box_annotator.annotate(&#xA;...     scene=IMAGE,&#xA;...     detections=detections&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://roboflow.github.io/supervision/dataset/core/&#34;&gt;datasets processing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_yolo(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;,&#xA;...     data_yaml_path=&#39;...&#39;&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset.classes&#xA;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; len(dataset)&#xA;1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;üëâ more dataset utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Load object detection/instance segmentation datasets in one of the supported formats&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_yolo(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;,&#xA;...     data_yaml_path=&#39;...&#39;&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_pascal_voc(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_coco(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_path=&#39;...&#39;&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Loop over dataset entries&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; for name, image, labels in dataset:&#xA;...     print(labels.xyxy)&#xA;&#xA;array([[404.      , 719.      , 538.      , 884.5     ],&#xA;       [155.      , 497.      , 404.      , 833.5     ],&#xA;       [ 20.154999, 347.825   , 416.125   , 915.895   ]], dtype=float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Split dataset for training, testing, and validation&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; train_dataset, test_dataset = dataset.split(split_ratio=0.7)&#xA;&amp;gt;&amp;gt;&amp;gt; test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; len(train_dataset), len(test_dataset), len(valid_dataset)&#xA;(700, 150, 150)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Merge multiple datasets&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; ds_1 = sv.DetectionDataset(...)&#xA;&amp;gt;&amp;gt;&amp;gt; len(ds_1)&#xA;100&#xA;&amp;gt;&amp;gt;&amp;gt; ds_1.classes&#xA;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; ds_2 = sv.DetectionDataset(...)&#xA;&amp;gt;&amp;gt;&amp;gt; len(ds_2)&#xA;200&#xA;&amp;gt;&amp;gt;&amp;gt; ds_2.classes&#xA;[&#39;cat&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])&#xA;&amp;gt;&amp;gt;&amp;gt; len(ds_merged)&#xA;300&#xA;&amp;gt;&amp;gt;&amp;gt; ds_merged.classes&#xA;[&#39;cat&#39;, &#39;dog&#39;, &#39;person&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Save object detection/instance segmentation datasets in one of the supported formats&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; dataset.as_yolo(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;,&#xA;...     data_yaml_path=&#39;...&#39;&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset.as_pascal_voc(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset.as_coco(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_path=&#39;...&#39;&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Convert labels between supported formats&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; sv.DetectionDataset.from_yolo(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;,&#xA;...     data_yaml_path=&#39;...&#39;&#xA;... ).as_pascal_voc(&#xA;...     images_directory_path=&#39;...&#39;,&#xA;...     annotations_directory_path=&#39;...&#39;&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Load classification datasets in one of the supported formats&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; cs = sv.ClassificationDataset.from_folder_structure(&#xA;...     root_directory_path=&#39;...&#39;&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Save classification datasets in one of the supported formats&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; cs.as_folder_structure(&#xA;...     root_directory_path=&#39;...&#39;&#xA;... )&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://roboflow.github.io/supervision/metrics/detection/&#34;&gt;model evaluation&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_yolo(...)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; def callback(image: np.ndarray) -&amp;gt; sv.Detections:&#xA;...     ...&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; confusion_matrix = sv.ConfusionMatrix.benchmark(&#xA;...     dataset = dataset,&#xA;...     callback = callback&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; confusion_matrix.matrix&#xA;array([&#xA;    [0., 0., 0., 0.],&#xA;    [0., 1., 0., 1.],&#xA;    [0., 1., 1., 0.],&#xA;    [1., 1., 0., 0.]&#xA;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;üëâ more metrics&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Mean average precision (mAP) for object detection tasks.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import supervision as sv&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; dataset = sv.DetectionDataset.from_yolo(...)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; def callback(image: np.ndarray) -&amp;gt; sv.Detections:&#xA;...     ...&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(&#xA;...     dataset = dataset,&#xA;...     callback = callback&#xA;... )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; mean_average_precision.map50_95&#xA;0.433&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üõ†Ô∏è built with supervision&lt;/h2&gt; &#xA;&lt;p&gt;Did you build something cool using supervision? &lt;a href=&#34;https://github.com/roboflow/supervision/discussions/categories/built-with-supervision&#34;&gt;Let us know!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&#34;&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üé¨ tutorials&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e&#34; alt=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34;&gt;&lt;strong&gt;Accelerate Image Annotation with SAM and Grounding DINO&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 20 Apr 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 20 Apr 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd&#34; alt=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34;&gt;&lt;strong&gt;SAM - Segment Anything Model by Meta AI: Complete Guide&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Apr 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 11 Apr 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; Discover the incredible potential of Meta AI&#39;s Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM&#39;s zero-shot performance is often competitive with or even superior to prior fully supervised results... &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìö documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://roboflow.github.io/supervision&#34;&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; &#xA;&lt;h2&gt;üèÜ contribution&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! Please see our &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started. Thank you üôè to all our contributors!&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://disuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>normal-computing/outlines</title>
    <updated>2023-08-17T01:34:32Z</updated>
    <id>tag:github.com,2023-08-17:/normal-computing/outlines</id>
    <link href="https://github.com/normal-computing/outlines" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative Model Programming&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/docs/source/_static/logo.png&#34; alt=&#34;Outlines Logo&#34; width=&#34;300&#34;&gt;&lt;/p&gt; &#xA; &lt;h1&gt;Outlines „Ä∞Ô∏è&lt;/h1&gt; &#xA; &lt;p&gt;Fast and reliable neural text generation.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#installation&#34;&gt;Install&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#guided-generation&#34;&gt;Guided generation&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#prompting&#34;&gt;Prompting primitives&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#examples&#34;&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#stay-tuned-for&#34;&gt;Stay tuned&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; „Ä∞ is a library for neural text generation. You can think of it as a more flexible replacement for the &lt;code&gt;generate&lt;/code&gt; method in the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; „Ä∞ helps developers &lt;em&gt;guide text generation&lt;/em&gt; to build robust interfaces with external systems. Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; „Ä∞ provides &lt;em&gt;robust prompting primitives&lt;/em&gt; that separate the prompting from the execution logic and lead to simple implementations of few-shot generations, ReAct, meta-prompting, agents, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; „Ä∞ is designed as a &lt;em&gt;library&lt;/em&gt; that is meant to be compatible the broader ecosystem, not to replace it. We use as few abstractions as possible, and generation can be interleaved with control flow, conditionals, custom Python functions and calls to other libraries.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; „Ä∞ is &lt;em&gt;compatible with all models&lt;/em&gt;. It only interfaces with models via the next-token logits. It can be used with API-based models as well.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üñçÔ∏èSimple and powerful prompting primitives based on the &lt;a href=&#34;https://jinja.palletsprojects.com/&#34;&gt;Jinja templating engine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üöÑ Guided generation, including multiple choice, type constraints and dynamic stopping&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ‚ö° Fast &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#efficient-regex-guided-generation&#34;&gt;regex-guided generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üî• Fast &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/#efficient-json-generation-following-a-pydantic-model&#34;&gt;JSON generation&lt;/a&gt; following a JSON schema or a Pydantic model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üêç Interleave completions with loops, conditionals, and custom Python functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üíæ Caching of generations&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ü§ó Integration with HuggingFace&#39;s &lt;code&gt;transformers&lt;/code&gt; models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Outlines „Ä∞ has new releases and features coming every week! Make sure to ‚≠ê star and üëÄ watch this repository to stay up to date.&lt;/p&gt; &#xA;&lt;h2&gt;Stay tuned for&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Context-Free Grammar guided generation (&lt;a href=&#34;https://github.com/normal-computing/outlines/pull/178&#34;&gt;#178&lt;/a&gt;);&lt;/li&gt; &#xA; &lt;li&gt;Prompt-token alignment so you don&#39;t have to think about tokenization details (&lt;a href=&#34;https://github.com/normal-computing/outlines/pull/201&#34;&gt;#201&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;An infilling DSL (&lt;a href=&#34;https://github.com/normal-computing/outlines/issues/182&#34;&gt;#182&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can follow &lt;a href=&#34;https://twitter.com/NormalComputing&#34;&gt;@NormalComputing&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/remilouf&#34;&gt;@remilouf&lt;/a&gt; or &lt;a href=&#34;https://twitter.com/BrandonTWillard&#34;&gt;@BrandonTWillard&lt;/a&gt; for regular updates!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlines&lt;/strong&gt; is available on PyPi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install outlines&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The dependencies needed to use models are not installed by default. You will need to run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install openai&lt;/code&gt; to be able to use OpenAI &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install transformers&lt;/code&gt; to be able to use HuggingFace &lt;code&gt;transformers&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&#34;&gt;models&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Guided generation&lt;/h2&gt; &#xA;&lt;p&gt;The first step towards reliability of systems that include large language models is to ensure that there is a well-defined interface between their output and user-defined code. &lt;strong&gt;Outlines&lt;/strong&gt; provides ways to control the generation of language models to make their output more predictable.&lt;/p&gt; &#xA;&lt;h3&gt;Early stopping&lt;/h3&gt; &#xA;&lt;p&gt;You can stop the generation after a given sequence has been found:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.text.generate as generate&#xA;import outlines.models as models&#xA;&#xA;model = models.transformers(&#34;gpt2&#34;)&#xA;answer = generate.continuation(model, stop=[&#34;.&#34;])(&#34;Tell me a one-sentence joke.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multiple choices&lt;/h3&gt; &#xA;&lt;p&gt;You can reduce the completion to a choice between multiple possibilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.text.generate as generate&#xA;import outlines.models as models&#xA;&#xA;model = models.transformers(&#34;gpt2&#34;)&#xA;&#xA;prompt = &#34;&#34;&#34;You are a sentiment-labelling assistant.&#xA;Is the following review positive or negative?&#xA;&#xA;Review: This restaurant is just awesome!&#xA;&#34;&#34;&#34;&#xA;answer = generate.choice(model, [&#34;Positive&#34;, &#34;Negative&#34;])(prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Type constraint&lt;/h3&gt; &#xA;&lt;p&gt;You can instruct the model to only return integers or floats:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.text.generate as generate&#xA;import outlines.models as models&#xA;&#xA;model = models.transformers(&#34;gpt2&#34;)&#xA;&#xA;prompt = &#34;1+1=&#34;&#xA;answer = generate.integer(model)(prompt)&#xA;&#xA;prompt = &#34;sqrt(2)=&#34;&#xA;answer = generate.float(model)(prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Efficient regex-guided generation&lt;/h3&gt; &#xA;&lt;p&gt;Outlines also comes with fast regex-guided generation. In fact, the &lt;code&gt;choice&lt;/code&gt;, &lt;code&gt;integer&lt;/code&gt; and &lt;code&gt;float&lt;/code&gt; functions above all use regex-guided generation under the hood:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.models as models&#xA;import outlines.text.generate as generate&#xA;&#xA;&#xA;model = models.transformers(&#34;gpt2-medium&#34;)&#xA;&#xA;prompt = &#34;Is 1+1=2? &#34;&#xA;unguided = generate.continuation(model, max_tokens=30)(prompt)&#xA;guided = generate.regex(model, r&#34;\s*([Yy]es|[Nn]o|[Nn]ever|[Aa]lways)&#34;, max_tokens=30)(&#xA;    prompt&#xA;)&#xA;&#xA;print(unguided)&#xA;# Is 1+1=2?&#xA;#&#xA;# This is probably the most perplexing question.&#xA;# As I said in one of my articles describing how&#xA;# I call 2 and 1, there isn&#39;t&#xA;&#xA;print(guided)&#xA;# Is 1+1=2? Always&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.models as models&#xA;import outlines.text.generate as generate&#xA;&#xA;&#xA;model = models.transformers(&#34;gpt2-medium&#34;)&#xA;&#xA;prompt = &#34;What is the IP address of the Google DNS servers? &#34;&#xA;unguided = generate.continuation(model, max_tokens=30)(prompt)&#xA;guided = generate.regex(&#xA;    model,&#xA;    r&#34;((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)&#34;,&#xA;    max_tokens=30,&#xA;)(prompt)&#xA;&#xA;print(unguided)&#xA;# What is the IP address of the Google DNS servers?&#xA;#&#xA;# Passive DNS servers are at DNS servers that are private.&#xA;# In other words, both IP servers are private. The database&#xA;# does not contain Chelsea Manning&#xA;&#xA;print(guided)&#xA;# What is the IP address of the Google DNS servers?&#xA;# 2.2.6.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unlike other libraries, regex-guided generation in Outlines is almost as fast as non-guided generation.&lt;/p&gt; &#xA;&lt;h3&gt;Efficient JSON generation following a Pydantic model&lt;/h3&gt; &#xA;&lt;p&gt;Outlines „Ä∞ allows to guide the generation process so the output is &lt;em&gt;guaranteed&lt;/em&gt; to follow a &lt;a href=&#34;https://json-schema.org/&#34;&gt;JSON schema&lt;/a&gt; or &lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic model&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List&#xA;from enum import Enum&#xA;from pydantic import BaseModel, constr&#xA;&#xA;import outlines.models as models&#xA;import outlines.text.generate as generate&#xA;&#xA;&#xA;class Weapon(str, Enum):&#xA;    sword = &#34;sword&#34;&#xA;    axe = &#34;axe&#34;&#xA;    mace = &#34;mace&#34;&#xA;    spear = &#34;spear&#34;&#xA;    bow = &#34;bow&#34;&#xA;    crossbow = &#34;crossbow&#34;&#xA;&#xA;&#xA;class Armor(str, Enum):&#xA;    leather = &#34;leather&#34;&#xA;    chainmail = &#34;chainmail&#34;&#xA;    plate = &#34;plate&#34;&#xA;&#xA;&#xA;class Character(BaseModel):&#xA;    name: constr(max_length=10)&#xA;    age: int&#xA;    armor: Armor&#xA;    weapon: Weapon&#xA;    strength: int&#xA;&#xA;&#xA;model = models.transformers(&#34;gpt2&#34;)&#xA;sequence = generate.json(model, Character)(&#34;Give me a character description&#34;)&#xA;print(sequence)&#xA;# {&#xA;#   &#34;name&#34;: &#34;ranbelt&#34;,&#xA;#   &#34;age&#34;: 26,&#xA;#   &#34;armor&#34;: &#34;chainmail&#34;,&#xA;#   &#34;weapon&#34;: &#34;bow&#34;,&#xA;#   &#34;strength&#34;: 5&#xA;# }&#xA;&#xA;parsed = Character.model_validate_json(sequence)&#xA;print(parsed)&#xA;# name=&#39;ranbelt&#39; age=26 armor=&amp;lt;Armor.chainmail: &#39;chainmail&#39;&amp;gt; weapon=&amp;lt;Weapon.bow: &#39;bow&#39;&amp;gt; strength=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The method works with union types, optional types, arrays, nested schemas, etc. Some field constraints are &lt;a href=&#34;https://github.com/normal-computing/outlines/issues/215&#34;&gt;not supported yet&lt;/a&gt;, but everything else should work.&lt;/p&gt; &#xA;&lt;h2&gt;Prompting&lt;/h2&gt; &#xA;&lt;p&gt;Writing prompts by concatenating strings in pure Python quickly becomes cumbersome: the prompt building logic gets entangled with the rest of the program, and the structure of the rendered prompt is obfuscated.&lt;strong&gt;Outlines&lt;/strong&gt; makes it easier to write and manage prompts by encapsulating templates inside &#34;template functions&#34;.&lt;/p&gt; &#xA;&lt;p&gt;These functions make it possible to neatly separate the prompt logic from the general program logic; they can be imported from other modules and libraries.&lt;/p&gt; &#xA;&lt;p&gt;Template functions require no superfluous abstraction, they use the Jinja2 templating engine to help build complex prompts in a concise manner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines.text as text&#xA;import outlines.models as models&#xA;&#xA;&#xA;examples = [&#xA;    (&#34;The food was digusting&#34;, &#34;Negative&#34;),&#xA;    (&#34;We had a fantastic night&#34;, &#34;Positive&#34;),&#xA;    (&#34;Recommended&#34;, &#34;Positive&#34;),&#xA;    (&#34;The waiter was rude&#34;, &#34;Negative&#34;)&#xA;]&#xA;&#xA;@text.prompt&#xA;def labelling(to_label, examples):&#xA;    &#34;&#34;&#34;You are a sentiment-labelling assistant.&#xA;&#xA;    {% for example in examples %}&#xA;    {{ example[0] }} // {{ example[1] }}&#xA;    {% endfor %}&#xA;    {{ to_label }} //&#xA;    &#34;&#34;&#34;&#xA;&#xA;model = models.transformers(&#34;gpt2&#34;)&#xA;prompt = labelling(&#34;Just awesome&#34;, examples)&#xA;answer = text.generate.continuation(model, max_tokens=100)(prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;We can teach language models to call external functions to get additional informations or perform tasks, by encoding the functions&#39; description in the prompt. To avoid duplicating information between the function definition and the description passed to the prompt, we define custom Jinja filters that can extract the function&#39;s name, description, signature and source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Callable, List&#xA;import outlines.text as text&#xA;&#xA;&#xA;def google_search(query: str):&#xA;    &#34;&#34;&#34;Google Search&#34;&#34;&#34;&#xA;    pass&#xA;&#xA;&#xA;def wikipedia_search(query: str):&#xA;    &#34;&#34;&#34;Wikipedia Search&#34;&#34;&#34;&#xA;    pass&#xA;&#xA;&#xA;@text.prompt&#xA;def agent(tools: List[Callable]):&#xA;    &#34;&#34;&#34;AVAILABLE COMMANDS:&#xA;&#xA;    {% for tool in tools %}&#xA;    TOOL&#xA;    {{ tool | name }}, {{ tool | description }}, args: {{ tool | signature }}&#xA;    {{ tool | source }}&#xA;    {% endfor %}&#xA;    &#34;&#34;&#34;&#xA;&#xA;&#xA;prompt = my_commands([google_search, wikipedia_search])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response models&lt;/h3&gt; &#xA;&lt;p&gt;We can instruct models to return their output in a pre-defined format, often JSON. To avoid duplicating information between the function definition and the description passed to the prompt we define a custom Jinja filter that can extract the expected response&#39;s schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydantic import BaseModel&#xA;import outlines.text as text&#xA;&#xA;&#xA;class Joke(BaseModel):&#xA;    joke: str&#xA;    explanation: str&#xA;&#xA;&#xA;@text.prompt&#xA;def joke_ppt(response_model):&#xA;    &#34;&#34;&#34;Tell a joke and explain why the joke is funny.&#xA;&#xA;    RESPONSE FORMAT:&#xA;    {{ response_model | schema }}&#xA;    &#34;&#34;&#34;&#xA;&#xA;&#xA;joke_ppt(Joke)&#xA;# Tell a joke and explain why the joke is funny.&#xA;#&#xA;# RESPONSE FORMAT:&#xA;# {&#xA;#    &#34;joke&#34;: &#34;The joke&#34;&#xA;#    &#34;explanation&#34;: &#34;The explanation of why the joke is funny&#34;&#xA;#  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With these prompting primitives &lt;strong&gt;Outlines&lt;/strong&gt; makes building agents like &lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;BabyAGI&lt;/a&gt;, &lt;a href=&#34;https://viper.cs.columbia.edu/&#34;&gt;ViperGPT&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/docs/transformers/transformers_agents&#34;&gt;Transformers Agent&lt;/a&gt; easier by removing boilerplate prompting code.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;What contributions?&lt;/h3&gt; &#xA;&lt;p&gt;We currently only accept bug fixes and documentation contributions. If you have a feature request, please start a new &lt;a href=&#34;https://github.com/normal-computing/outlines/discussions&#34;&gt;discussion&lt;/a&gt;. The issue tracker is only intended for actionable items.&lt;/p&gt; &#xA;&lt;h3&gt;How to contribute?&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;pip install -e .[test]&lt;/code&gt; or &lt;code&gt;conda env create -f environment.yml&lt;/code&gt;. To build the documentation you will also need to run &lt;code&gt;pip install -r requirements-doc.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Before pushing your code to repository please run &lt;code&gt;pre-commit run --all-files&lt;/code&gt; and &lt;code&gt;pytest&lt;/code&gt; to make sure that the code is formatted correctly and that the tests pass.&lt;/p&gt; &#xA;&lt;p&gt;Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/pick_odd_one_out.py&#34;&gt;Pick the odd one out&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/meta_prompting.py&#34;&gt;Meta prompting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/react.py&#34;&gt;ReAct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/dust/math-generate-code.py&#34;&gt;Generate code to solve math problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/babyagi.py&#34;&gt;BabyAGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/sampling.ipynb&#34;&gt;Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines/raw/main/examples/simulation_based_inference.ipynb&#34;&gt;Simulation-based inference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite Outlines&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{willard2023efficient,&#xA;  title={Efficient Guided Generation for LLMs},&#xA;  author={Willard, Brandon T and Louf, R{\&#39;e}mi},&#xA;  journal={arXiv preprint arXiv:2307.09702},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Outlines is open-source and licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/normal-computing/outlines/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>plbrault/youre-the-os</title>
    <updated>2023-08-17T01:34:32Z</updated>
    <id>tag:github.com,2023-08-17:/plbrault/youre-the-os</id>
    <link href="https://github.com/plbrault/youre-the-os" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A game where you are a computer&#39;s OS and you have to manage processes, memory and I/O events.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;You&#39;re the OS!&lt;/h1&gt; &#xA;&lt;p&gt;This is a game where you are the operating system of a computer. As such, you have to manage processes, memory and I/O events. Make sure not to leave processes idling for too long, or the user will get really impatient and reboot you!&lt;/p&gt; &#xA;&lt;p&gt;You can play the game here: &lt;a href=&#34;https://plbrault.github.io/youre-the-os&#34;&gt;https://plbrault.github.io/youre-the-os&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also available on &lt;a href=&#34;https://drfreckles42.itch.io/youre-the-os&#34;&gt;itch.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/plbrault/youre-the-os/main/readme-assets/in_game_screenshot.png&#34; alt=&#34;In-game screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.11&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pipenv/&#34;&gt;pipenv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An empty &lt;code&gt;.venv&lt;/code&gt; directory at the root of the project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv sync --dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run as a desktop app:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv run desktop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run web version:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv run web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build web version without running:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv run web build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Create &lt;code&gt;web.zip&lt;/code&gt; archive for itch.io:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv run web archive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2023 Pier-Luc Brault &lt;a href=&#34;mailto:pier-luc@brault.me&#34;&gt;pier-luc@brault.me&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt; &#xA;&lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt; &#xA;&lt;p&gt;You should have received a copy of the GNU General Public License along with this program. If not, see &lt;a href=&#34;https://www.gnu.org/licenses/&#34;&gt;https://www.gnu.org/licenses/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Asset Licenses&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Emojis used in the game are from &lt;a href=&#34;https://openmoji.org/&#34;&gt;OpenMoji&lt;/a&gt;. They are published under the &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/#&#34;&gt;Creative Commons Attribution-ShareAlike License 4.0&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The image used in the Game Over screen is by &lt;a href=&#34;https://pixabay.com/fr/users/lemonsandtea-10190089/&#34;&gt;Aleksandar Cvetanoviƒá&lt;/a&gt; and is published under the &lt;a href=&#34;https://pixabay.com/service/license/&#34;&gt;Pixabay License&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The game icon was created by user &lt;a href=&#34;https://www.flaticon.com/authors/itim2101&#34;&gt;itim2101&lt;/a&gt; on &lt;a href=&#34;https://www.flaticon.com/&#34;&gt;Flaticon&lt;/a&gt; and is published under the &lt;a href=&#34;https://www.freepikcompany.com/legal#nav-flaticon-agreement&#34;&gt;Flaticon license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The primary font used in the game is named &lt;em&gt;VT323&lt;/em&gt;, and was designed by Peter Hull. The secondary font is named &lt;em&gt;Victor Mono&lt;/em&gt; and was designed by Rune Bj√∏rner√•s. Both are published under the &lt;a href=&#34;https://scripts.sil.org/cms/scripts/page.php?item_id=OFL_web&#34;&gt;Open Font License&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>