<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-23T01:44:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LSH9832/edgeyolo</title>
    <updated>2023-02-23T01:44:46Z</updated>
    <id>tag:github.com,2023-02-23:/LSH9832/edgeyolo</id>
    <link href="https://github.com/LSH9832/edgeyolo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;an edge-real-time anchor-free object detector with decent performance&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/assets/visdrone.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://www.bit.edu.cn&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/assets/bit.png&#34; align=&#34;left&#34; height=&#34;60&#34; width=&#34;60&#34;&gt;&lt;/a&gt; EdgeYOLO: anchor-free, edge-friendly&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#intro&#34;&gt;1 Intro&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#updates&#34;&gt;2 Updates&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#coming-soon&#34;&gt;3 Coming Soon&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#models&#34;&gt;4 Models&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#quick-start&#34;&gt;5 Quick Start&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; $\quad$&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#setup&#34;&gt;5.1 setup&lt;/a&gt;&lt;br&gt; $\quad$&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#inference&#34;&gt;5.2 inference&lt;/a&gt;&lt;br&gt; $\quad$&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#train&#34;&gt;5.3 train&lt;/a&gt;&lt;br&gt; $\quad$&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#evaluate&#34;&gt;5.4 evaluate&lt;/a&gt;&lt;br&gt; $\quad$&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#export-onnx--tensorrt&#34;&gt;5.5 export onnx &amp;amp; tensorrt&lt;/a&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#cite-edgeyolo&#34;&gt;6 Cite EdgeYOLO&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/#bugs-found-currently&#34;&gt;7 Bugs found currently&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In embeded device such as Nvidia Jetson AGX Xavier, EdgeYOLO reaches 34FPS with &lt;strong&gt;50.6&lt;/strong&gt;% AP in COCO2017 dataset and &lt;strong&gt;25.9&lt;/strong&gt;% AP in VisDrone2019 &lt;strong&gt;(image input size is 640x640, batch=16, post-process included)&lt;/strong&gt;. And for smaller model EdgeYOLO-S, it reaches 53FPS with &lt;strong&gt;44.1&lt;/strong&gt;% AP and &lt;strong&gt;63.3&lt;/strong&gt;% AP&lt;sup&gt;0.5&lt;/sup&gt;(&lt;strong&gt;SOTA&lt;/strong&gt; in P5 small models) in COCO2017.&lt;/li&gt; &#xA; &lt;li&gt;we provide a more effective data augmentation during training.&lt;/li&gt; &#xA; &lt;li&gt;small object and medium object detect performace is imporved by using RH loss during the last few training epochs.&lt;/li&gt; &#xA; &lt;li&gt;Our pre-print paper is released on &lt;a href=&#34;https://arxiv.org/abs/2302.07483&#34;&gt;&lt;strong&gt;arxiv&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/2/20]&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/tree/main/cpp/console&#34;&gt;TensorRT cpp inference console demo&lt;/a&gt; (lib &lt;strong&gt;opencv&lt;/strong&gt; and &lt;strong&gt;qt5&lt;/strong&gt; required) &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix bugs when exporting models using Version 7.X TensorRT&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/2/19]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Publish TensorRT int8 export code with &lt;strong&gt;Calibration&lt;/strong&gt; (&lt;strong&gt;torch2trt&lt;/strong&gt; is required)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now evaluate.py doesn&#39;t support tensorrt model, we will update it in the near future&lt;/li&gt; &#xA; &lt;li&gt;MNN deployment code&lt;/li&gt; &#xA; &lt;li&gt;More different models&lt;/li&gt; &#xA; &lt;li&gt;C++ code for TensorRT inference with UI&lt;/li&gt; &#xA; &lt;li&gt;EdgeYOLO-mask for segmentation task&lt;/li&gt; &#xA; &lt;li&gt;Simple but effective pretrain method&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;models trained on COCO2017-train&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;sup&gt;AGX Xavier&lt;br&gt;trt fp16 batch=16 &lt;br&gt;include NMS&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;br&gt;train / infer&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-Tiny-LRELU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;br&gt;37.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.5&lt;br&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;206&lt;/strong&gt;&lt;br&gt;109&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.6 / 7.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_tiny_lrelu_coco.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-Tiny&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.2&lt;br&gt;41.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.4&lt;br&gt;60.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;br&gt;67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.8 / 5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_tiny_coco.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;63.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.9 / 9.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_s_coco.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.0 / 17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_m_coco.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.2 / 40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_coco.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;models trained on VisDrone2019 (pretrained backbone on COCO2017-train)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;we use &lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v1.0.0/visdrone_coco.zip&#34;&gt; VisDrone2019-DET dataset with COCO format &lt;/a&gt; in our training.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-Tiny-LRELU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.1&lt;br&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.8&lt;br&gt;33.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_tiny_lrelu_visdrone.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-Tiny&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.9&lt;br&gt;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.3&lt;br&gt;38.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_tiny_visdrone.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_s_visdrone.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO-M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_m_visdrone.pth&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EdgeYOLO&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;br&gt;26.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;br&gt;44.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v0.0.0/edgeyolo_visdrone.pth&#34;&gt;&lt;strong&gt;github(legacy)&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v1.0.0/edgeyolo_visdrone.pth&#34;&gt;&lt;strong&gt;github(new)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Some of our detect results in COCO2017&lt;/summary&gt; &#xA; &lt;p&gt;COCO2017 &lt;img src=&#34;https://raw.githubusercontent.com/LSH9832/edgeyolo/main/assets/coco.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/LSH9832/edgeyolo.git&#xA;cd edgeyolo&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you use tensorrt, please make sure torch2trt and TensorRT Development Toolkit(version&amp;gt;7.1.3.0) are installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/NVIDIA-AI-IOT/torch2trt.git&#xA;cd torch2trt&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to make sure you use the same version of torch2trt as ours, &lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/download/v1.0.0/torch2trt.zip&#34;&gt;download here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;First &lt;a href=&#34;https://github.com/LSH9832/edgeyolo/releases/tag/v0.0.0&#34;&gt;download weights here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python detect.py --weights edgeyolo_coco.pth --source XXX.mp4 --fp16&#xA;&#xA;# all options&#xA;python detect.py --weights edgeyolo_coco.pth &#xA;                 --source /XX/XXX.mp4     # or dir with images, such as /dataset/coco2017/val2017    (jpg/jpeg, png, bmp, webp is available)&#xA;                 --conf-thres 0.25 &#xA;                 --nms-thres 0.5 &#xA;                 --input-size 640 640 &#xA;                 --batch 1 &#xA;                 --save-dir ./output/detect/imgs    # if you press &#34;s&#34;, the current frame will be saved in this dir&#xA;                 --fp16 &#xA;                 --no-fuse                # do not reparameterize model&#xA;                 --no-label               # do not draw label with class name and confidence&#xA;                 --mp                     # use multi-process to show images more smoothly when batch &amp;gt; 1&#xA;                 --fps 30                 # max fps limitation, valid only when option --mp is used&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;train&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;first preparing your dataset and create dataset config file(./params/dataset/XXX.yaml), make sure your dataset config file contains:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(COCO, VOC, VisDrone and DOTA formats are supported)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;type: &#34;coco&#34;                        # dataset format(lowercase)，COCO, VOC, VisDrone and DOTA formats are supported currently&#xA;dataset_path: &#34;/dataset/coco2017&#34;   # root dir of your dataset&#xA;&#xA;kwargs:&#xA;  suffix: &#34;jpg&#34;        # suffix of your dataset&#39;s images&#xA;  use_cache: true      # test on i5-12490f: Total loading time: 52s -&amp;gt; 10s(seg enabled) and 39s -&amp;gt; 4s(seg disabled)&#xA;&#xA;train:&#xA;  image_dir: &#34;images/train2017&#34;                   # train set image dir&#xA;  label: &#34;annotations/instances_train2017.json&#34;   # train set label file(format with single label file) or directory(multi label files)&#xA;&#xA;val:&#xA;  image_dir: &#34;images/val2017&#34;                     # evaluate set image dir&#xA;  label: &#34;annotations/instances_val2017.json&#34;     # evaluate set label file or directory&#xA;&#xA;test:&#xA;  test_dir: &#34;test2017&#34;     # test set image dir (not used in code now, but will)&#xA;&#xA;segmentaion_enabled: true  # whether this dataset has segmentation labels and you are going to use them instead of bbox labels&#xA;&#xA;names: [&#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;,&#xA;        &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;,&#xA;        &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;,&#xA;        &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;,&#xA;        &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;,&#xA;        &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;,&#xA;        &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;,&#xA;        &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;,&#xA;        &#39;hair drier&#39;, &#39;toothbrush&#39;]    # category names&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;then edit file ./params/train/train_XXX.yaml&lt;/li&gt; &#xA; &lt;li&gt;finally&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --cfg ./params/train/train_XXX.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;evaluate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate.py --weights edgeyolo_coco.pth --dataset params/dataset/XXX.yaml --batch 16 --device 0&#xA;&#xA;# all options&#xA;python evaluate.py --weights edgeyolo_coco.pth &#xA;                   --dataset params/dataset/XXX.yaml &#xA;                   --batch 16   # batch size for each gpu&#xA;                   --device 0&#xA;                   --input-size 640 640   # height, width&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;export onnx &amp;amp; tensorrt&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ONNX&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python export.py --onnx --weights edgeyolo_coco.pth --batch 1&#xA;&#xA;# all options&#xA;python export.py --onnx   # or --onnx-only if tensorrt and torch2trt are not installed&#xA;                 --weights edgeyolo_coco.pth &#xA;                 --input-size 640 640   # height, width&#xA;                 --batch 1&#xA;                 --opset 11&#xA;                 --no-simplify    # do not simplify this model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;it generates&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;output/export/edgeyolo_coco/640x640_batch1.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TensorRT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# fp16&#xA;python export.py --trt --weights edgeyolo_coco.pth --batch 1 --workspace 8&#xA;&#xA;# int8&#xA;python export.py --trt --weights edgeyolo_coco.pth --batch 1 --workspace 8 --int8 --dataset params/dataset/coco.yaml --num-imgs 1024&#xA;&#xA;# all options&#xA;python export.py --trt                       # you can add --onnx and relative options to export both models&#xA;                 --weights edgeyolo_coco.pth&#xA;                 --input-size 640 640        # height, width&#xA;                 --batch 1&#xA;                 --workspace 10              # (GB)&#xA;                 --no-fp16        # fp16 mode in default, use this option to disable it(fp32)&#xA;                 --int8           # int8 mode, the following options are needed for calibration&#xA;                 --datset params/dataset/coco.yaml   # generates calibration images from its val images(upper limit：5120)&#xA;                 --train          # use train images instead of val images(upper limit：5120)&#xA;                 --all            # use all images(upper limit：5120)&#xA;                 --num-imgs 512   # (upper limit：5120)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;it generates&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(optional) output/export/edgeyolo_coco/640x640_batch1.onnx&#xA;output/export/edgeyolo_coco/640x640_batch1_fp16/int8.pt       # for python inference&#xA;output/export/edgeyolo_coco/640x640_batch1_fp16/int8.engine   # for c++ inference&#xA;output/export/edgeyolo_coco/640x640_batch1_fp16/int8.json     # for c++ inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Benchmark of TensorRT Int8 Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;enviroment: TensorRT Version 8.2.5.1, Windows, i5-12490F, RTX 3060 12GB&lt;/li&gt; &#xA; &lt;li&gt;increase workspace and the number of images for calibration may improve the performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;COCO2017-TensorRT-int8&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Int8 Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Calibration &lt;br&gt;Image number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Workspace&lt;br&gt;&lt;sup&gt;(GB)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;sup&gt;RTX 3060&lt;br&gt;trt int8 batch=16 &lt;br&gt;include NMS&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Tiny-LRELU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.5&lt;br&gt;36.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.7&lt;br&gt;55.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;730&lt;br&gt;360&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Tiny&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.9&lt;br&gt;39.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.1&lt;br&gt;59.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;549&lt;br&gt;288&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;233&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;211&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;176&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;for python inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python detect.py --trt --weights output/export/edgeyolo_coco/640x640_batch1_int8.pt --source XXX.mp4&#xA;&#xA;# all options&#xA;python detect.py --trt &#xA;                 --weights output/export/edgeyolo_coco/640x640_batch1_int8.pt &#xA;                 --source XXX.mp4&#xA;                 --legacy         # if &#34;img = img / 255&#34; when you train your train model&#xA;                 --use-decoder    # if use original yolox tensorrt model before version 0.3.0&#xA;                 --mp             # use multi-process to show images more smoothly when batch &amp;gt; 1&#xA;                 --fps 30         # max fps limitation, valid only when option --mp is used&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;for c++ inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# build&#xA;cd cpp/console/linux&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make -j4&#xA;&#xA;# help&#xA;./yolo -?&#xA;./yolo --help&#xA;&#xA;# run&#xA;# ./yolo [json file] [source] [--conf] [--nms] [--loop] [--no-label]&#xA;./yolo ../../../../output/export/edgeyolo_coco/640x640_batch1_int8.json ~/Videos/test.avi --conf 0.25 --nms 0.5 --loop --no-label&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite EdgeYOLO&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt; @article{edgeyolo2023,&#xA;  title={EdgeYOLO: An Edge-Real-Time Object Detector},&#xA;  author={Shihan Liu, Junlin Zha, Jian Sun, Zhuo Li, and Gang Wang},&#xA;  journal={arXiv preprint arXiv:2302.07483},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Bugs found currently&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sometimes it raises error as follows during training. Reduce pytorch version to 1.8.0 might solve this problem.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;File &#34;XXX/edgeyolo/edgeyolo/train/loss.py&#34;, line 667, in dynamic_k_matching&#xA;_, pos_idx = torch.topk(cost[gt_idx], k=dynamic_ks[gt_idx].item(), largest=False)&#xA;RuntimeError: CUDA error: device-side assert triggered&#xA;CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.&#xA;For debugging consider passing CUDA_LAUNCH_BLOCKING=1.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For DOTA dataset, we only support single GPU training mode now, please do not train DOTA dataset with distributed mode or model can not be trained correctly.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes converting to TensorRT fp16 model with 8.4.X.X or higher version might lose a lot of precision, please use TensorRT Verson 7.X.X.X or 8.2.X.X&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>miguelgrinberg/microdot</title>
    <updated>2023-02-23T01:44:46Z</updated>
    <id>tag:github.com,2023-02-23:/miguelgrinberg/microdot</id>
    <link href="https://github.com/miguelgrinberg/microdot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The impossibly small web framework for Python and MicroPython.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;microdot&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/miguelgrinberg/microdot/actions&#34;&gt;&lt;img src=&#34;https://github.com/miguelgrinberg/microdot/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/miguelgrinberg/microdot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/miguelgrinberg/microdot/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;“The impossibly small web framework for Python and MicroPython”&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Microdot is a minimalistic Python web framework inspired by Flask, and designed to run on systems with limited resources such as microcontrollers. It runs on standard Python and on MicroPython.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from microdot import Microdot&#xA;&#xA;app = Microdot()&#xA;&#xA;@app.route(&#39;/&#39;)&#xA;def index(request):&#xA;    return &#39;Hello, world!&#39;&#xA;&#xA;app.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microdot.readthedocs.io/en/latest/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/miguelgrinberg/microdot/raw/main/CHANGES.md&#34;&gt;Change Log&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>FMInference/FlexGen</title>
    <updated>2023-02-23T01:44:46Z</updated>
    <id>tag:github.com,2023-02-23:/FMInference/FlexGen</id>
    <link href="https://github.com/FMInference/FlexGen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Running large language models like OPT-175B/GPT-3 on a single GPU. Focusing on high-throughput large-batch generation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlexGen&lt;/h1&gt; &#xA;&lt;p&gt;FlexGen is a high-throughput generation engine for running large language models with limited GPU memory (e.g., a 16GB T4 GPU or a 24GB RTX3090 gaming card!). FlexGen allows high-throughput generation by increasing the effective batch size through IO-efficient offloading and compression.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This is a research project developed by &lt;a href=&#34;https://hazyresearch.stanford.edu/&#34;&gt;HazyResearch@Stanford&lt;/a&gt;, &lt;a href=&#34;https://sky.cs.berkeley.edu/&#34;&gt;SkyComputing@UC Berkeley&lt;/a&gt;, &lt;a href=&#34;https://ds3lab.inf.ethz.ch/&#34;&gt;DS3Lab@ETH Zurich&lt;/a&gt;, &lt;a href=&#34;https://ai.facebook.com/&#34;&gt;FAIR@Meta&lt;/a&gt;, &lt;a href=&#34;https://crfm.stanford.edu/&#34;&gt;CRFM@Stanford&lt;/a&gt;, and &lt;a href=&#34;https://www.together.xyz/&#34;&gt;TogetherCompute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hazyresearch.stanford.edu/&#34;&gt;&lt;img src=&#34;https://identity.stanford.edu/wp-content/uploads/sites/3/2020/06/wordmark-nospace-red.png&#34; height=&#34;25&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://sky.cs.berkeley.edu/&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/University_of_California%2C_Berkeley_logo.svg/1280px-University_of_California%2C_Berkeley_logo.svg.png&#34; height=&#34;25&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://ds3lab.inf.ethz.ch/&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1608867/220273382-c09669b3-42fd-47c2-b88c-7ed55cb43820.png&#34; height=&#34;30&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://www.together.xyz/&#34;&gt;&lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/eef09191-631f-40d9-9bfd-f875b25bcf0b/together-logo-black-transparent2.png&#34; height=&#34;25&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. FlexGen aims to lower the resource requirements of LLM inference down to a single commodity GPU (e.g., T4, 3090) and allow flexible deployment for various hardware setups. The key technique behind FlexGen is to trade off between &lt;strong&gt;latency&lt;/strong&gt; and &lt;strong&gt;throughput&lt;/strong&gt; by developing techniques to increase the effective batch size.&lt;/p&gt; &#xA;&lt;p&gt;The key features of FlexGen include:&lt;/p&gt; &#xA;&lt;p&gt;⚡ &lt;strong&gt;High-Throughput, Large-Batch Offloading&lt;/strong&gt;.&lt;br&gt; Higher-throughput generation than other offloading-based systems (e.g., Hugging Face Accelerate, DeepSpeed Zero-Inference) - sometimes by orders of magnitude. The key innovation is a new offloading technique that can effectively increase the batch size. This can be useful for batch inference scenarios, such as benchmarking (e.g., &lt;a href=&#34;https://github.com/stanford-crfm/helm&#34;&gt;HELM&lt;/a&gt;) and &lt;a href=&#34;https://arxiv.org/abs/2205.09911&#34;&gt;data wrangling&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;📦 &lt;strong&gt;Extreme Compression&lt;/strong&gt;.&lt;br&gt; Compress both the parameters and attention cache of models, such as OPT-175B, down to 4 bits with negligible accuracy loss.&lt;/p&gt; &#xA;&lt;p&gt;🚀 &lt;strong&gt;Scalability&lt;/strong&gt;.&lt;br&gt; Come with a distributed pipeline parallelism runtime to allow scaling if more GPUs are given.&lt;/p&gt; &#xA;&lt;p&gt;❌ &lt;strong&gt;Limitation&lt;/strong&gt;.&lt;br&gt; As an offloading-based system running on weak GPUs, FlexGen also has its limitations. The throughput of FlexGen is significantly lower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases. FlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.&lt;/p&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/docs/paper.pdf&#34;&gt;&lt;strong&gt;Read Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/JfphDTkBAh&#34;&gt;&lt;strong&gt;Join Discord&lt;/strong&gt;&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;h2&gt;Content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#benchmark-results&#34;&gt;Benchmark Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#get-started-with-a-single-gpu&#34;&gt;Get Started with a Single GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#run-chatbot-with-opt-models-on-a-single-gpu&#34;&gt;Run Chatbot with OPT models on a Single GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#scaling-to-distributed-gpus&#34;&gt;Scaling to Distributed GPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark Results&lt;/h2&gt; &#xA;&lt;h3&gt;Generation Throughput (token/s)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;OPT-6.7B&lt;/th&gt; &#xA;   &lt;th&gt;OPT-30B&lt;/th&gt; &#xA;   &lt;th&gt;OPT-175B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hugging Face Accelerate&lt;/td&gt; &#xA;   &lt;td&gt;25.12&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSpeed ZeRO-Inference&lt;/td&gt; &#xA;   &lt;td&gt;9.28&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;td&gt;0.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Petals*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexGen&lt;/td&gt; &#xA;   &lt;td&gt;25.26&lt;/td&gt; &#xA;   &lt;td&gt;7.32&lt;/td&gt; &#xA;   &lt;td&gt;0.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexGen with Compression&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;29.12&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.38&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.12&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hardware: an NVIDIA T4 (16GB) instance on GCP with 208GB of DRAM and 1.5TB of SSD.&lt;/li&gt; &#xA; &lt;li&gt;Workload: input sequence length = 512, output sequence length = 32. The batch size is tuned to &lt;strong&gt;a large value&lt;/strong&gt; that maximizes the generation throughput for each system. (e.g., 256 for OPT-175B on FlexGen).&lt;/li&gt; &#xA; &lt;li&gt;Metric: generation throughput (token/s) = number of the generated tokens / (time for processing prompts + time for generation).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/benchmark/flexgen&#34;&gt;reproduce&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Latency-throughput Trade-off&lt;/h3&gt; &#xA;&lt;p&gt;Since FlexGen increases throughput by increasing the batch size, it also increases latency - a classic and fundamental trade-off. The figure below shows the latency and throughput trade-off of three offloading-based systems on OPT-175B (left) and OPT-30B (right). FlexGen achieves higher maximum throughput for both models. Other systems cannot further increase throughput due to out-of-memory. &#34;FlexGen(c)&#34; is FlexGen with compression.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/FMInference/FlexGen/raw/main/docs/throughput_vs_latency.jpg&#34; alt=&#34;logo&#34; width=&#34;500&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value (KV) cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss.&lt;/p&gt; &#xA;&lt;p&gt;One key idea of FlexGen is to play the latency-throughput trade-off. Achieving low latency is inherently challenging for offloading methods, but the I/O efficiency of offloading can be greatly boosted for throughput-oriented scenarios (see the figure above). FlexGen utilizes a block schedule to reuse weight and overlap I/O with computation, as shown in figure (b) below, while other baseline systems use an inefficient row-by-row schedule, as shown in figure (a) below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/FMInference/FlexGen/raw/main/docs/block_schedule.jpg&#34; alt=&#34;logo&#34; width=&#34;500&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/docs/paper.pdf&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch &amp;gt;= 1.12 &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;(Help)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/FMInference/FlexGen.git&#xA;cd FlexGen&#xA;pip3 install -e .&#xA;&#xA;# (Optional) Install openmpi for multi-gpu execution&#xA;# sudo apt install openmpi-bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started with a Single GPU&lt;/h2&gt; &#xA;&lt;h3&gt;OPT-1.3B&lt;/h3&gt; &#xA;&lt;p&gt;To get started, you can try a small model like OPT-1.3B first. It fits into a single GPU so no offloading is required. FlexGen will automatically download weights from Hugging Face.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m flexgen.flex_opt --model facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see some text generated by OPT-1.3B and the benchmark results.&lt;/p&gt; &#xA;&lt;h3&gt;OPT-30B&lt;/h3&gt; &#xA;&lt;p&gt;To run large models like OPT-30B, you will need to use CPU offloading. You can try commands below. The &lt;code&gt;--percent&lt;/code&gt; argument specifies the offloading strategy for parameters, attention cache and hidden states separately. The exact meaning of this argument can be found &lt;a href=&#34;https://github.com/FMInference/FlexGen/raw/9d092d848f106cd9eaf305c12ef3590f7bcb0277/flexgen/flex_opt.py#L1271-L1279&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m flexgen.flex_opt --model facebook/opt-30b --percent 0 100 100 0 100 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OPT-175B&lt;/h3&gt; &#xA;&lt;p&gt;To run OPT-175B, you need to download the weights from &lt;a href=&#34;https://github.com/facebookresearch/metaseq/tree/main/projects/OPT&#34;&gt;metaseq&lt;/a&gt; and convert the weights into Alpa &lt;a href=&#34;https://alpa.ai/tutorials/opt_serving.html#convert-opt-175b-weights-into-alpa-formats&#34;&gt;format&lt;/a&gt;. You can then try to offloading all weights to disk by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m flexgen.flex_opt --model facebook/opt-175b --percent 0 0 100 0 100 0 --offload-dir YOUR_SSD_FOLDER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to set the offloading strategy and &lt;code&gt;--percent&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;We will release an automatic policy optimizer later, but now you have to manually try a few strategies. The idea of high-throughput generation is to offload parameters and attention cache as much as possible to the CPU and disk if necessary. You can see the reference strategies in our benchmark &lt;a href=&#34;https://github.com/FMInference/FlexGen/raw/9d092d848f106cd9eaf305c12ef3590f7bcb0277/benchmark/flexgen/bench_suite.py#L39-L79&#34;&gt;here&lt;/a&gt;. To avoid out-of-memory, you can tune the &lt;code&gt;--percent&lt;/code&gt; of offload more tensors to the CPU and disk.&lt;/p&gt; &#xA;&lt;h2&gt;Scaling to Distributed GPUs&lt;/h2&gt; &#xA;&lt;p&gt;If you have more GPUs, FlexGen can combine offloading with pipeline parallelism to allow scaling. For example, if you have 2 GPUs but the aggregated GPU memory is less than the model size, you still need offloading. FlexGen allow you to do pipeline parallelism with these 2 GPUs to accelerate the generation. See examples &lt;a href=&#34;https://github.com/FMInference/FlexGen/tree/main/benchmark/flexgen#distributed-gpus&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run Chatbot with OPT Models on a Single GPU&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/apps/chatbot.py&#34;&gt;apps/chatbot.py&lt;/a&gt; shows how to build a chatbot with FlexGen and OPT models. While FlexGen is mainly optimized for large-batch throughput-oriented scenarios like dataset evaluations and information extraction, FlexGen can also be used for interactive applications like chatbot with better performance than other offloading-based systems. Note that FlexGen cannot achieve its best throughput in this single-batch case.&lt;/p&gt; &#xA;&lt;h3&gt;Default Commands&lt;/h3&gt; &#xA;&lt;p&gt;You can use the default commands below. If you do not have enough GPU/CPU memory, see the &lt;a href=&#34;https://raw.githubusercontent.com/FMInference/FlexGen/main/#handle-out-of-memory&#34;&gt;Handle Out-of-memory&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Chat with OPT-6.7B. You need at least 15GB of GPU memory.&#xA;python3 chatbot.py --model facebook/opt-6.7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Chat with OPT-30B. You need about 90GB of CPU memory.&#xA;python3 chatbot.py --model facebook/opt-30b --percent 0 100 100 0 100 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Chat with instruction-tuned OPT-IML-MAX-30B. You need about 90GB of CPU memory.&#xA;python3 chatbot.py --model facebook/opt-iml-max-30b --percent 0 100 100 0 100 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;A chat between a curious human and a knowledgeable artificial intelligence assistant.&#xA;Human: Hello! What can you do?&#xA;Assistant: As an AI assistant, I can answer questions and chat with you.&#xA;Human: What is the name of the tallest mountain in the world?&#xA;Assistant: Everest.&#xA;Human: I am planning a trip for our anniversary. What things can we do?&#xA;Assistant: Well, there are a number of things you can do for your anniversary. First, you can play cards. Second, you can go for a hike. Third, you can go to a museum.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Handle Out-of-memory&lt;/h3&gt; &#xA;&lt;p&gt;If you do not have enough GPU/CPU memory, here are a few things you can try. They save more memory but run slower.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Do not pin weights by adding &lt;code&gt;--pin-weight 0&lt;/code&gt;. This can reduce the weight memory usage on CPU by around 20% or more.&lt;/li&gt; &#xA; &lt;li&gt;Enable weight compression by adding &lt;code&gt;--compress-weight&lt;/code&gt;. This can reduce the weight memory usage by around 70%.&lt;/li&gt; &#xA; &lt;li&gt;Offload weights to disk by using &lt;code&gt;--percent 0 0 100 0 100 0&lt;/code&gt;. This requires very little CPU and GPU memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;We plan to work on the following features. Community contributions are welcome.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Apple silicon M1/M2 deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Colab deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add a text summarization application and more throughput-oriented applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize the latency of the chatbot application&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more models (BLOOM, CodeGen, GLM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the cost model and policy optimizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release a pip installable package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Recent Changes&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to early feedback about this release, we realized that early versions of this README and our paper were a bit unclear about the purpose of FlexGen and why we&#39;re excited about it. Our primary contributions are increasing throughput on single GPU instances - by effectively increasing the batch size. We&#39;re really excited about our techniques for offloading and automatically searching through the design space, as well as our results that suggest it&#39;s possible to go down to 4-bit quantization without hurting accuracy. This naturally trades off latency, but we think it&#39;s a really interesting direction for future work. We&#39;d like to thank everyone for their feedback - keep it coming!&lt;/p&gt;</summary>
  </entry>
</feed>