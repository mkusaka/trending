<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-27T01:33:34Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>meta-llama/llama-stack-apps</title>
    <updated>2024-09-27T01:33:34Z</updated>
    <id>tag:github.com,2024-09-27:/meta-llama/llama-stack-apps</id>
    <link href="https://github.com/meta-llama/llama-stack-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agentic components of the Llama Stack APIs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama-stack-apps&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/TZAAYNVtrU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1257833999603335178&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo shows examples of applications built on top of &lt;a href=&#34;https://github.com/meta-llama/llama-stack&#34;&gt;Llama Stack&lt;/a&gt;. Starting Llama 3.1 you can build agentic applications capable of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;breaking a task down and performing multi-step reasoning.&lt;/li&gt; &#xA; &lt;li&gt;using tools to perform some actions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;built-in: the model has built-in knowledge of tools like search or code interpreter&lt;/li&gt; &#xA;   &lt;li&gt;zero-shot: the model can learn to call tools using previously unseen, in-context tool definitions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;providing system level safety protections using models like Llama Guard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The Llama Stack API is still evolving and may change. Feel free to build and experiment, but please don&#39;t rely on its stability just yet!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;An agentic app requires a few components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ability to run inference on the underlying Llama series of models&lt;/li&gt; &#xA; &lt;li&gt;ability to run safety checks using the Llama Guard series of models&lt;/li&gt; &#xA; &lt;li&gt;ability to execute tools, including a code execution environment, and loop using the model&#39;s multi-step reasoning process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of these components are now offered by a single Llama Stack Distribution. The &lt;a href=&#34;https://github.com/meta-llama/llama-stack&#34;&gt;Llama Stack&lt;/a&gt; defines and standardizes these components and many others that are needed to make building Generative AI applications smoother. Various implementations of these APIs are then assembled together via a &lt;strong&gt;Llama Stack Distribution&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Getting started with the Llama Stack Distributions&lt;/h1&gt; &#xA;&lt;p&gt;To get started with Llama Stack Distributions, you&#39;ll need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install prerequisites&lt;/li&gt; &#xA; &lt;li&gt;Download the model checkpoints&lt;/li&gt; &#xA; &lt;li&gt;Build and start a Llama Stack server&lt;/li&gt; &#xA; &lt;li&gt;Connect your client agentic app to Llama Stack server&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Once started, you can then just point your agentic app to the URL for this server (e.g. &lt;code&gt;http://localhost:5000&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;1. Install Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We recommend creating an isolated conda Python environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate a virtual environment&#xA;ENV=stack&#xA;conda create -n $ENV python=3.10&#xA;cd &amp;lt;path-to-llama-stack-apps-repo&amp;gt;&#xA;conda activate $ENV&#xA;&#xA;# Install dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install all dependencies required to (1) Build and start a Llama Stack server (2) Connect your client app to Llama Stack server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLI Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;llama-stack&lt;/code&gt; installed, you should be able to use the Llama Stack CLI and run &lt;code&gt;llama --help&lt;/code&gt;. Please checkout our &lt;a href=&#34;https://github.com/meta-llama/llama-stack/raw/main/docs/cli_reference.md&#34;&gt;CLI Reference&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: llama [-h] {download,model,stack} ...&#xA;&#xA;Welcome to the Llama CLI&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;&#xA;subcommands:&#xA;  {download,model,stack}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Model Checkpoints&lt;/h2&gt; &#xA;&lt;h4&gt;Downloading from &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;Meta&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Download the required checkpoints using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download the 8B model, this can be run on a single GPU&#xA;llama download --source meta --model-id Meta-Llama3.1-8B-Instruct --meta-url META_URL&#xA;&#xA;# you can also get the 70B model, this will require 8 GPUs however&#xA;llama download --source meta --model-id Meta-Llama3.1-70B-Instruct --meta-url META_URL&#xA;&#xA;# llama-agents have safety enabled by default. For this, you will need&#xA;# safety models -- Llama-Guard and Prompt-Guard&#xA;llama download --source meta --model-id Prompt-Guard-86M --meta-url META_URL&#xA;llama download --source meta --model-id Llama-Guard-3-8B --meta-url META_URL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For all the above, you will need to provide a URL (META_URL) which can be obtained from &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;https://llama.meta.com/llama-downloads/&lt;/a&gt; after signing an agreement.&lt;/p&gt; &#xA;&lt;h4&gt;Downloading from &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Huggingface&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Essentially, the same commands above work, just replace &lt;code&gt;--source meta&lt;/code&gt; with &lt;code&gt;--source huggingface&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama download --source huggingface --model-id  Meta-Llama3.1-8B-Instruct --hf-token &amp;lt;HF_TOKEN&amp;gt;&#xA;&#xA;llama download --source huggingface --model-id Meta-Llama3.1-70B-Instruct --hf-token &amp;lt;HF_TOKEN&amp;gt;&#xA;&#xA;llama download --source huggingface --model-id Llama-Guard-3-8B --ignore-patterns *original*&#xA;llama download --source huggingface --model-id Prompt-Guard-86M --ignore-patterns *original*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Set your environment variable &lt;code&gt;HF_TOKEN&lt;/code&gt; or pass in &lt;code&gt;--hf-token&lt;/code&gt; to the command to validate your access. You can find your token at &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; Default for &lt;code&gt;llama download&lt;/code&gt; is to run with &lt;code&gt;--ignore-patterns *.safetensors&lt;/code&gt; since we use the &lt;code&gt;.pth&lt;/code&gt; files in the &lt;code&gt;original&lt;/code&gt; folder. For Llama Guard and Prompt Guard, however, we need safetensors. Hence, please run with &lt;code&gt;--ignore-patterns original&lt;/code&gt; so that safetensors are downloaded and &lt;code&gt;.pth&lt;/code&gt; files are ignored.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Downloading via Ollama&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re already using ollama, we also have a supported Llama Stack distribution &lt;code&gt;local-ollama&lt;/code&gt; and you can continue to use ollama for managing model downloads.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama3.1:8b-instruct-fp16&#xA;ollama pull llama3.1:70b-instruct-fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Only the above two models are currently supported by Ollama.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Build, Configure, Run Llama Stack Distribution&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please see our &lt;a href=&#34;https://github.com/meta-llama/llama-stack/raw/main/docs/getting_started.md&#34;&gt;Getting Started Guide&lt;/a&gt; for more details on setting up a Llama Stack distribution and running server to serve API endpoints.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 1. Build&lt;/h3&gt; &#xA;&lt;p&gt;In the following steps, imagine we&#39;ll be working with a &lt;code&gt;Meta-Llama3.1-8B-Instruct&lt;/code&gt; model. We will name our build &lt;code&gt;8b-instruct&lt;/code&gt; to help us remember the config. We will start build our distribution (in the form of a Conda environment, or Docker image). In this step, we will specify:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: the name for our distribution (e.g. &lt;code&gt;8b-instruct&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_type&lt;/code&gt;: our build image type (&lt;code&gt;conda | docker&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;distribution_spec&lt;/code&gt;: our distribution specs for specifying API providers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;description&lt;/code&gt;: a short description of the configurations for the distribution&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;providers&lt;/code&gt;: specifies the underlying implementation for serving each API endpoint&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;image_type&lt;/code&gt;: &lt;code&gt;conda&lt;/code&gt; | &lt;code&gt;docker&lt;/code&gt; to specify whether to build the distribution in the form of Docker image or Conda environment.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Build a local distribution with conda&lt;/h4&gt; &#xA;&lt;p&gt;The following command and specifications allows you to get started with building.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llama stack build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You&#39;ll be prompted to enter build information interactively.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;llama stack build&#xA;&#xA;&amp;gt; Enter an unique name for identifying your Llama Stack build distribution (e.g. my-local-stack): 8b-instruct&#xA;&amp;gt; Enter the image type you want your distribution to be built with (docker or conda): conda&#xA;&#xA; Llama Stack is composed of several APIs working together. Let&#39;s configure the providers (implementations) you want to use for these APIs.&#xA;&amp;gt; Enter the API provider for the inference API: (default=meta-reference): meta-reference&#xA;&amp;gt; Enter the API provider for the safety API: (default=meta-reference): meta-reference&#xA;&amp;gt; Enter the API provider for the agents API: (default=meta-reference): meta-reference&#xA;&amp;gt; Enter the API provider for the memory API: (default=meta-reference): meta-reference&#xA;&amp;gt; Enter the API provider for the telemetry API: (default=meta-reference): meta-reference&#xA;&#xA; &amp;gt; (Optional) Enter a short description for your Llama Stack distribution:&#xA;&#xA;Build spec configuration saved at ~/.conda/envs/llamastack-my-local-stack/8b-instruct-build.yaml&#xA;You can now run `llama stack configure my-local-stack`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;(Alternative) Downloading Pre-built Docker image&lt;/h4&gt; &#xA;&lt;p&gt;We provide 2 pre-built Docker image of Llama Stack distribution, which can be found in the following links.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-gpu/general&#34;&gt;llamastack-local-gpu&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is a packaged version with our local meta-reference implementations, where you will be running inference locally with downloaded Llama model checkpoints.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-cpu/general&#34;&gt;llamastack-local-cpu&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is a lite version with remote inference where you can hook up to your favourite remote inference framework (e.g. ollama, fireworks, together, tgi) for running inference without GPU.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] For GPU inference, you need to set these environment variables for specifying local directory containing your model checkpoints, and enable GPU inference to start running docker container.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;export LLAMA_CHECKPOINT_DIR=~/.llama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download and start running a pre-built docker container, you may use the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker image pull llamastack/llamastack-local-gpu&#xA;llama stack configure llamastack-local-gpu&#xA;llama stack run local-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2. Configure&lt;/h3&gt; &#xA;&lt;p&gt;After our distribution is built (either in form of docker or conda environment), we will run the following command to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llama stack configure [&amp;lt;name&amp;gt; | &amp;lt;path/to/name.build.yaml&amp;gt; | &amp;lt;docker-image-name&amp;gt;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For &lt;code&gt;conda&lt;/code&gt; environments: &amp;lt;path/to/name.build.yaml&amp;gt; would be the generated build spec saved from Step 1.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;docker&lt;/code&gt; images downloaded from Dockerhub, you could also use &#xA;  &lt;docker-image-name&gt;&#xA;    as the argument. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Run &lt;code&gt;docker images&lt;/code&gt; to check list of available images on your machine.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/docker-image-name&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ llama stack configure 8b-instruct&#xA;&#xA;Configuring API: inference (meta-reference)&#xA;Enter value for model (existing: Meta-Llama3.1-8B-Instruct) (required):&#xA;Enter value for quantization (optional):&#xA;Enter value for torch_seed (optional):&#xA;Enter value for max_seq_len (existing: 4096) (required):&#xA;Enter value for max_batch_size (existing: 1) (required):&#xA;&#xA;Configuring API: memory (meta-reference-faiss)&#xA;&#xA;Configuring API: safety (meta-reference)&#xA;Do you want to configure llama_guard_shield? (y/n): y&#xA;Entering sub-configuration for llama_guard_shield:&#xA;Enter value for model (default: Llama-Guard-3-8B) (required):&#xA;Enter value for excluded_categories (default: []) (required):&#xA;Enter value for disable_input_check (default: False) (required):&#xA;Enter value for disable_output_check (default: False) (required):&#xA;Do you want to configure prompt_guard_shield? (y/n): y&#xA;Entering sub-configuration for prompt_guard_shield:&#xA;Enter value for model (default: Prompt-Guard-86M) (required):&#xA;&#xA;Configuring API: agentic_system (meta-reference)&#xA;Enter value for brave_search_api_key (optional):&#xA;Enter value for bing_search_api_key (optional):&#xA;Enter value for wolfram_api_key (optional):&#xA;&#xA;Configuring API: telemetry (console)&#xA;&#xA;YAML configuration has been written to ~/.llama/builds/conda/8b-instruct-run.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this step is successful, you should be able to find a run configuration spec in &lt;code&gt;~/.llama/builds/conda/8b-instruct-run.yaml&lt;/code&gt; with the following contents. You may edit this file to change the settings.&lt;/p&gt; &#xA;&lt;p&gt;As you can see, we did basic configuration above and configured:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;inference to run on model &lt;code&gt;Meta-Llama3.1-8B-Instruct&lt;/code&gt; (obtained from &lt;code&gt;llama model list&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Llama Guard safety shield with model &lt;code&gt;Llama-Guard-3-8B&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt Guard safety shield with model &lt;code&gt;Prompt-Guard-86M&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For how these configurations are stored as yaml, checkout the file printed at the end of the configuration.&lt;/p&gt; &#xA;&lt;p&gt;Note that all configurations as well as models are stored in &lt;code&gt;~/.llama&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run&lt;/h3&gt; &#xA;&lt;p&gt;Now, let&#39;s start the Llama Stack Distribution Server. You will need the YAML configuration file which was written out at the end by the &lt;code&gt;llama stack configure&lt;/code&gt; step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;llama stack run 8b-instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see the Llama Stack server start and print the APIs that it is supporting&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ llama stack run 8b-instruct&#xA;&#xA;&amp;gt; initializing model parallel with size 1&#xA;&amp;gt; initializing ddp with size 1&#xA;&amp;gt; initializing pipeline with size 1&#xA;Loaded in 19.28 seconds&#xA;NCCL version 2.20.5+cuda12.4&#xA;Finished model load YES READY&#xA;Serving POST /inference/batch_chat_completion&#xA;Serving POST /inference/batch_completion&#xA;Serving POST /inference/chat_completion&#xA;Serving POST /inference/completion&#xA;Serving POST /safety/run_shields&#xA;Serving POST /agents/memory_bank/attach&#xA;Serving POST /agents/create&#xA;Serving POST /agents/session/create&#xA;Serving POST /agents/turn/create&#xA;Serving POST /agents/delete&#xA;Serving POST /agents/session/delete&#xA;Serving POST /agents/memory_bank/detach&#xA;Serving POST /agents/session/get&#xA;Serving POST /agents/step/get&#xA;Serving POST /agents/turn/get&#xA;Listening on :::5000&#xA;INFO:     Started server process [453333]&#xA;INFO:     Waiting for application startup.&#xA;INFO:     Application startup complete.&#xA;INFO:     Uvicorn running on http://[::]:5000 (Press CTRL+C to quit)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Configuration is in &lt;code&gt;~/.llama/builds/local/conda/8b-instruct.yaml&lt;/code&gt;. Feel free to increase &lt;code&gt;max_seq_len&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The &#34;local&#34; distribution inference server currently only supports CUDA. It will not work on Apple Silicon machines.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You might need to use the flag &lt;code&gt;--disable-ipv6&lt;/code&gt; to Disable IPv6 support&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This server is running a Llama model locally.&lt;/p&gt; &#xA;&lt;h2&gt;Test agents demo script&lt;/h2&gt; &#xA;&lt;p&gt;We have built sample demo scripts for interating with the Stack server.&lt;/p&gt; &#xA;&lt;p&gt;With the server running, you may run to test out an simple Agent&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m examples.agents.hello localhost 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see outputs of the form --&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; created agents with agent_id=d050201b-0ca1-4abd-8eee-3cba2b8c0fbc&#xA;User&amp;gt; Hello&#xA;shield_call&amp;gt; No Violation&#xA;inference&amp;gt; How can I assist you today?&#xA;shield_call&amp;gt; No Violation&#xA;User&amp;gt; Which players played in the winning team of the NBA western conference semifinals of 2024, please use tools&#xA;shield_call&amp;gt; No Violation&#xA;inference&amp;gt; brave_search.call(query=&#34;NBA Western Conference Semifinals 2024 winning team players&#34;)&#xA;tool_execution&amp;gt; Tool:brave_search Args:{&#39;query&#39;: &#39;NBA Western Conference Semifinals 2024 winning team players&#39;}&#xA;tool_execution&amp;gt; Tool:brave_search Response:{&#34;query&#34;: &#34;NBA Western Conference Semifinals 2024 winning team players&#34;, &#34;top_k&#34;: [{&#34;title&#34;: &#34;2024 NBA Western Conference Semifinals - Mavericks vs. Thunder | Basketball-Reference.com&#34;, &#34;url&#34;: &#34;https://www.basketball-reference.com/playoffs/2024-nba-western-conference-semifinals-mavericks-vs-thunder.html&#34;, &#34;description&#34;: &#34;Summary and statistics for the &amp;lt;strong&amp;gt;2024&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;NBA&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;Western&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;Conference&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;Semifinals&amp;lt;/strong&amp;gt; - Mavericks vs. Thunder&#34;, &#34;type&#34;: &#34;search_result&#34;}, {&#34;title&#34;: &#34;2024 NBA playoffs - Wikipedia&#34;, &#34;url&#34;: &#34;https://en.wikipedia.org/wiki/2024_NBA_playoffs&#34;, &#34;description&#34;: &#34;Aged 20 years and 96 days old, ... youngest &amp;lt;strong&amp;gt;player&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;in&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;NBA&amp;lt;/strong&amp;gt; history to record 10+ points and 15+ rebounds in a playoff game, coming during game 6 of the Maverick&amp;amp;#x27;s &amp;lt;strong&amp;gt;Western&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;Conference&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;Semifinal&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;win&amp;lt;/strong&amp;gt; against the Thunder on May 18. The Timberwolves overcame a 20\u2013point deficit to &amp;lt;strong&amp;gt;win&amp;lt;/strong&amp;gt; game 7 against the Nuggets, the largest game 7 comeback in &amp;lt;strong&amp;gt;NBA&amp;lt;/strong&amp;gt; playoffs history. With the defending champion Nuggets losing to the Minnesota Timberwolves, the &amp;lt;strong&amp;gt;2024&amp;lt;/strong&amp;gt; playoffs marked ...&#34;, &#34;type&#34;: &#34;search_result&#34;}, {&#34;title&#34;: &#34;2024 NBA Playoffs | Official Bracket, Schedule and Series Matchups&#34;, &#34;url&#34;: &#34;https://www.nba.com/playoffs/2024&#34;, &#34;description&#34;: &#34;The official site of the &amp;lt;strong&amp;gt;2024&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;NBA&amp;lt;/strong&amp;gt; Playoffs. Latest news, schedules, matchups, highlights, bracket and more.&#34;, &#34;type&#34;: &#34;search_result&#34;}]}&#xA;shield_call&amp;gt; No Violation&#xA;inference&amp;gt; The players who played in the winning team of the NBA Western Conference Semifinals of 2024 are not specified in the search results provided. However, the search results suggest that the Mavericks played against the Thunder in the Western Conference Semifinals, and the Mavericks won the series.&#xA;shield_call&amp;gt; No Violation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Start an App and Interact with the Server&lt;/h2&gt; &#xA;&lt;p&gt;Now that the Stack server is setup, the next thing would be to run an agentic app using Agents APIs.&lt;/p&gt; &#xA;&lt;p&gt;We have built sample scripts, notebooks and a UI chat interface ( using &lt;a href=&#34;%5Burl%5D(https://google.github.io/mesop/)&#34;&gt;Mesop&lt;/a&gt; ! ) to help you get started.&lt;/p&gt; &#xA;&lt;p&gt;Start an app (local) and interact with it by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PYTHONPATH=. mesop app/main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start a mesop app and you can go to &lt;code&gt;localhost:32123&lt;/code&gt; to play with the chat interface.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack-apps/main/demo.png&#34; alt=&#34;Chat App&#34; width=&#34;600&#34;&gt; &#xA;&lt;p&gt;Optionally, you can setup API keys for custom tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.wolframalpha.com/&#34;&gt;WolframAlpha&lt;/a&gt;: store in &lt;code&gt;WOLFRAM_ALPHA_API_KEY&lt;/code&gt; environment variable&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://brave.com/search/api/&#34;&gt;Brave Search&lt;/a&gt;: store in &lt;code&gt;BRAVE_SEARCH_API_KEY&lt;/code&gt; environment variable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Similar to this main app, you can also try other variants&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;PYTHONPATH=. mesop app/chat_with_custom_tools.py&lt;/code&gt; to showcase how custom tools are integrated&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PYTHONPATH=. mesop app/chat_moderation_with_llama_guard.py&lt;/code&gt; to showcase how the app is modified to act as a chat moderator for safety&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Create agentic systems and interact with the Stack server&lt;/h2&gt; &#xA;&lt;p&gt;NOTE: Ensure that Stack server is still running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd &amp;lt;path-to-llama-agentic-system&amp;gt;&#xA;conda activate $ENV&#xA;llama stack run &amp;lt;name&amp;gt; # If not already started&#xA;&#xA;PYTHONPATH=. python examples/scripts/vacation.py localhost 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see outputs to stdout of the form --&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Environment: ipython&#xA;Tools: brave_search, wolfram_alpha, photogen&#xA;&#xA;Cutting Knowledge Date: December 2023&#xA;Today Date: 23 July 2024&#xA;&#xA;&#xA;User&amp;gt; I am planning a trip to Switzerland, what are the top 3 places to visit?&#xA;Final Llama Guard response shield_type=&amp;lt;BuiltinShield.llama_guard: &#39;llama_guard&#39;&amp;gt; is_violation=False violation_type=None violation_return_message=None&#xA;Ran PromptGuardShield and got Scores: Embedded: 0.9999765157699585, Malicious: 1.1110752893728204e-05&#xA;StepType.shield_call&amp;gt; No Violation&#xA;role=&#39;user&#39; content=&#39;I am planning a trip to Switzerland, what are the top 3 places to visit?&#39;&#xA;StepType.inference&amp;gt; Switzerland is a beautiful country with a rich history, culture, and natural beauty. Here are three must-visit places to add to your itinerary: ....&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt; You can optionally do &lt;code&gt;--disable-safety&lt;/code&gt; in the scripts to avoid running safety shields all the time.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Feel free to reach out if you have questions.&lt;/p&gt; &#xA;&lt;h2&gt;Develop in your preferred language&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out our client SDKs for connecting to Llama Stack server, you can choose from &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-python&#34;&gt;python&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-node&#34;&gt;node&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-swift&#34;&gt;swift&lt;/a&gt;, and &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-kotlin&#34;&gt;kotlin&lt;/a&gt; programming languages to quickly build your applications.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using VirtualEnv instead of Conda&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] While you can run the apps using &lt;code&gt;venv&lt;/code&gt;, installation of a distribution requires conda.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;In Linux&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate a virtual environment&#xA;python3 -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For Windows&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate a virtual environment&#xA;python -m venv venv&#xA;venv\Scripts\activate  # For Command Prompt&#xA;# or&#xA;.\venv\Scripts\Activate.ps1  # For PowerShell&#xA;# or&#xA;source venv\Scripts\activate  # For Git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The instructions thereafter (including &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; for installing the dependencies) remain the same.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/llama-stack</title>
    <updated>2024-09-27T01:33:34Z</updated>
    <id>tag:github.com,2024-09-27:/meta-llama/llama-stack</id>
    <link href="https://github.com/meta-llama/llama-stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Model components of the Llama Stack APIs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/llama-stack&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/TZAAYNVtrU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1257833999603335178&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the Llama Stack API specifications as well as API Providers and Llama Stack Distributions.&lt;/p&gt; &#xA;&lt;p&gt;The Llama Stack defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production. Beyond definition, we are building providers for the Llama Stack APIs. These were developing open-source versions and partnering with providers, ensuring developers can assemble AI solutions using consistent, interlocking pieces across platforms. The ultimate goal is to accelerate innovation in the AI space.&lt;/p&gt; &#xA;&lt;p&gt;The Stack APIs are rapidly improving, but still very much work in progress and we invite feedback as well as direct contributions.&lt;/p&gt; &#xA;&lt;h2&gt;APIs&lt;/h2&gt; &#xA;&lt;p&gt;The Llama Stack consists of the following set of APIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference&lt;/li&gt; &#xA; &lt;li&gt;Safety&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA; &lt;li&gt;Agentic System&lt;/li&gt; &#xA; &lt;li&gt;Evaluation&lt;/li&gt; &#xA; &lt;li&gt;Post Training&lt;/li&gt; &#xA; &lt;li&gt;Synthetic Data Generation&lt;/li&gt; &#xA; &lt;li&gt;Reward Scoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each of the APIs themselves is a collection of REST endpoints.&lt;/p&gt; &#xA;&lt;h2&gt;API Providers&lt;/h2&gt; &#xA;&lt;p&gt;A Provider is what makes the API real -- they provide the actual implementation backing the API.&lt;/p&gt; &#xA;&lt;p&gt;As an example, for Inference, we could have the implementation be backed by open source libraries like &lt;code&gt;[ torch | vLLM | TensorRT ]&lt;/code&gt; as possible options.&lt;/p&gt; &#xA;&lt;p&gt;A provider can also be just a pointer to a remote REST service -- for example, cloud providers or dedicated inference providers could serve these APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Stack Distribution&lt;/h2&gt; &#xA;&lt;p&gt;A Distribution is where APIs and Providers are assembled together to provide a consistent whole to the end application developer. You can mix-and-match providers -- some could be backed by local code and some could be remote. As a hobbyist, you can serve a small model locally, but can choose a cloud provider for a large model. Regardless, the higher level APIs your app needs to work with don&#39;t need to change at all. You can even imagine moving across the server / mobile-device boundary as well always using the same uniform set of APIs for developing Generative AI applications.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Llama Stack Implementations&lt;/h2&gt; &#xA;&lt;h3&gt;API Providers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;API Provider Builder&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Environments&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fireworks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AWS Bedrock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Together&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ollama&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted and Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chroma&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PG Vector&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PyTorch ExecuTorch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;On-device iOS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Distributions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Distribution Provider&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-gpu/general&#34;&gt;Local GPU&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-cpu/general&#34;&gt;Local CPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dell-TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-tgi-chroma/general&#34;&gt;Local TGI + Chroma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install this repository as a &lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;package&lt;/a&gt; with &lt;code&gt;pip install llama-stack&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/local&#xA;cd ~/local&#xA;git clone git@github.com:meta-llama/llama-stack.git&#xA;&#xA;conda create -n stack python=3.10&#xA;conda activate stack&#xA;&#xA;cd llama-stack&#xA;$CONDA_PREFIX/bin/pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The Llama CLI&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;llama&lt;/code&gt; CLI makes it easy to work with the Llama Stack set of tools, including installing and running Distributions, downloading models, studying model prompt formats, etc. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack/main/docs/cli_reference.md&#34;&gt;CLI reference&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hyperknot/openfreemap</title>
    <updated>2024-09-27T01:33:34Z</updated>
    <id>tag:github.com,2024-09-27:/hyperknot/openfreemap</id>
    <link href="https://github.com/hyperknot/openfreemap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free and open-source map hosting solution with custom styles for websites and apps, using OpenStreetMap data&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://openfreemap.org/&#34;&gt;&lt;img src=&#34;https://openfreemap.org/logo.jpg&#34; alt=&#34;logo&#34; height=&#34;200&#34; class=&#34;logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;OpenFreeMap&lt;/h1&gt; &#xA;&lt;p&gt;OpenFreeMap lets you display custom maps on your website and apps for free.&lt;/p&gt; &#xA;&lt;p&gt;You can either &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/docs/self_hosting.md&#34;&gt;self-host&lt;/a&gt; or use our public instance. Everything is &lt;strong&gt;open-source&lt;/strong&gt;, including the full production setup — there’s no &#39;open-core&#39; model here. The map data comes from OpenStreetMap.&lt;/p&gt; &#xA;&lt;p&gt;Using our &lt;strong&gt;public instance&lt;/strong&gt; is completely free: there are no limits on the number of map views or requests. There’s no registration, no user database, no API keys, and no cookies. We aim to cover the running costs of our public instance through donations.&lt;/p&gt; &#xA;&lt;p&gt;We also provide &lt;strong&gt;weekly&lt;/strong&gt; full planet downloads both in Btrfs and MBTiles formats.&lt;/p&gt; &#xA;&lt;h4&gt;Quick introduction and how to guide: &lt;a href=&#34;https://openfreemap.org/&#34;&gt;https://openfreemap.org/&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Goals of this project&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this project is to provide free, production-quality vector-tile hosting using existing tools.&lt;/p&gt; &#xA;&lt;p&gt;Currently these tools are: &lt;a href=&#34;https://www.openstreetmap.org/copyright&#34;&gt;OpenStreetMap&lt;/a&gt;, &lt;a href=&#34;https://github.com/openmaptiles/openmaptiles&#34;&gt;OpenMapTiles&lt;/a&gt;, &lt;a href=&#34;https://github.com/onthegomap/planetiler&#34;&gt;Planetiler&lt;/a&gt;, &lt;a href=&#34;https://maplibre.org/&#34;&gt;MapLibre&lt;/a&gt;, &lt;a href=&#34;https://www.naturalearthdata.com/&#34;&gt;Natural Earth&lt;/a&gt; and &lt;a href=&#34;https://www.wikidata.org/wiki/Wikidata:Main_Page&#34;&gt;Wikidata&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks go to &lt;a href=&#34;https://github.com/msbarry&#34;&gt;Michael Barry&lt;/a&gt; for developing &lt;a href=&#34;https://github.com/onthegomap/planetiler&#34;&gt;Planetiler&lt;/a&gt;. It made it possible to generate the tiles in 5 hours instead of 5 weeks.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this repo is limited (see below). Once we figure out the technical details, ideally, there should be few commits here, while everything continues to work: the map tiles are automatically generated, servers are automatically updated and load balancing takes care of any downtime.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/hyperknot/openfreemap-styles&#34;&gt;styles repo&lt;/a&gt;, on the other hand, is continuously being developed.&lt;/p&gt; &#xA;&lt;p&gt;Contributions are more than welcome!&lt;/p&gt; &#xA;&lt;h2&gt;Status of this project&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tile generation works&lt;/li&gt; &#xA; &lt;li&gt;The web servers work&lt;/li&gt; &#xA; &lt;li&gt;Weekly auto-updates work&lt;/li&gt; &#xA; &lt;li&gt;Servers in our public instance are currently: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;1 server running tile generation&lt;/li&gt; &#xA;   &lt;li&gt;2 servers running web hosting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Web servers are in Round-Robin DNS configuration with Let&#39;s Encrypt provided certificates.&lt;/li&gt; &#xA; &lt;li&gt;Load-balancer script works. Currently in monitoring-only mode, as Round-Robin DNS handles downtime.&lt;/li&gt; &#xA; &lt;li&gt;The public instance has been the production basemap service of &lt;a href=&#34;https://maphub.net/&#34;&gt;MapHub&lt;/a&gt; since June 2024.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsoring&lt;/h2&gt; &#xA;&lt;p&gt;Please consider sponsoring our project on &lt;a href=&#34;https://github.com/sponsors/hyperknot&#34;&gt;GitHub Sponsors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations of this project&lt;/h2&gt; &#xA;&lt;p&gt;The only way this project can possibly work is to be super focused about what it is and what it isn&#39;t. OpenFreeMap has the following limitations by design:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenFreeMap is not providing:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;search or geocoding&lt;/li&gt; &#xA;   &lt;li&gt;route calculation, navigation or directions&lt;/li&gt; &#xA;   &lt;li&gt;static image generation&lt;/li&gt; &#xA;   &lt;li&gt;raster tile hosting&lt;/li&gt; &#xA;   &lt;li&gt;satellite image hosting&lt;/li&gt; &#xA;   &lt;li&gt;elevation lookup&lt;/li&gt; &#xA;   &lt;li&gt;custom tile or dataset hosting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenFreeMap is not something you can install locally. This repo is a deploy script specifically made to set up clean Ubuntu servers or virtual machines. It uses &lt;a href=&#34;https://www.fabfile.org/&#34;&gt;Fabric&lt;/a&gt; and runs commands over SSH. With a single command it can set up a production-ready server, both for tile hosting and generation.&lt;/p&gt; &lt;p&gt;This repo is Docker-free on purpose. If someone wants to make a Docker-based version of this, I&#39;m more than happy to link it here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenFreeMap does not promise worry-free automatic updates for self-hosters. Only use the autoupdate version of http-host if you keep a close eye on this repo.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Self hosting&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/docs/self_hosting.md&#34;&gt;self hosting docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is the tech stack?&lt;/h2&gt; &#xA;&lt;p&gt;There is no tile server running; only Btrfs partition images with 300 million hard-linked files. This was my idea; I haven&#39;t read about anyone else doing this in production, but it works really well.&lt;/p&gt; &#xA;&lt;p&gt;There is no cloud, just dedicated servers. The web server is nginx on Ubuntu.&lt;/p&gt; &#xA;&lt;h2&gt;Btrfs images&lt;/h2&gt; &#xA;&lt;p&gt;Production-quality hosting of 300 million tiny files is hard. The average file size is just 450 byte. Dozens of tile servers have been written to tackle this problem, but they all have their limitations.&lt;/p&gt; &#xA;&lt;p&gt;The original idea of this project is to avoid using tile servers altogether. Instead, the tiles are directly served from Btrfs partition images + hard links using an optimised nginx config. I wrote &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/modules/tile_gen/scripts/extract_mbtiles.py&#34;&gt;extract_mbtiles&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/modules/tile_gen/scripts/shrink_btrfs.py&#34;&gt;shrink_btrfs&lt;/a&gt; scripts for this very purpose.&lt;/p&gt; &#xA;&lt;p&gt;This replaces a running service with a pure, file-system-level implementation. Since the Linux kernel&#39;s file caching is among the highest-performing and most thoroughly tested codes ever written, it delivers serious performance.&lt;/p&gt; &#xA;&lt;p&gt;I run some &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/docs/quick_notes/http_benchmark.md&#34;&gt;benchmarks&lt;/a&gt; on a Hetzner server, the aim was to saturate a gigabit connection. At the end, it was able to serve 30 Gbit on loopback interface, on cold nginx cache.&lt;/p&gt; &#xA;&lt;h2&gt;Code structure&lt;/h2&gt; &#xA;&lt;p&gt;The project has the following parts&lt;/p&gt; &#xA;&lt;h4&gt;deploy server - ssh_lib and init-server.py&lt;/h4&gt; &#xA;&lt;p&gt;This sets up everything on a clean Ubuntu server. You run it locally and it sets up the server via SSH.&lt;/p&gt; &#xA;&lt;h4&gt;HTTP host - modules/http_host&lt;/h4&gt; &#xA;&lt;p&gt;Inside &lt;code&gt;http_host&lt;/code&gt;, all work is done by &lt;code&gt;http_host.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It does the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Downloading btrfs images&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Downloading assets&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mounting downloaded btrfs images&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fetches version files&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Running the sync cron task (called every minute with http-host-autoupdate)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;./http_host.py --help&lt;/code&gt; to see which options are available.&lt;/p&gt; &#xA;&lt;h4&gt;tile generation - modules/tile_gen&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;note: Tile generation is 100% optional, as we are providing the processed full planet btrfs files for public download. You can download full planet images updated weekly, both in Btrfs and in MBTiles format.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;tile_gen&lt;/code&gt; script downloads a full planet OSM extract and runs it through Planetiler.&lt;/p&gt; &#xA;&lt;p&gt;The created .mbtiles file is then extracted into a Btrfs partition image using the custom &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/modules/tile_gen/scripts/extract_mbtiles.py&#34;&gt;extract_mbtiles&lt;/a&gt; script. The partition is shrunk using the &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/modules/tile_gen/scripts/shrink_btrfs.py&#34;&gt;shrink_btrfs&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;p&gt;Finally, it&#39;s uploaded to a public Cloudflare R2 bucket using rclone.&lt;/p&gt; &#xA;&lt;h4&gt;styles - &lt;a href=&#34;https://github.com/hyperknot/openfreemap-styles&#34;&gt;styles repo&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The default styles. I&#39;ve already put countless hours into tweaking up some nice looking styles. Still, it&#39;ll take probably the most work in the long term future.&lt;/p&gt; &#xA;&lt;p&gt;Of course, you are welcome to use custom styles.&lt;/p&gt; &#xA;&lt;h4&gt;load balancer script - modules/loadbalancer&lt;/h4&gt; &#xA;&lt;p&gt;A Round Robin DNS based load balancer script for health checking and updating records. It pushes status messages to a Telegram bot.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Full planet downloads&lt;/h3&gt; &#xA;&lt;p&gt;Full planet runs are uploaded weekly. You can download them both in Btrfs and in MBTiles formats. The files have the following URL patterns:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://btrfs.openfreemap.com/areas/planet/%7Bversion%7D/tiles.btrfs.gz&#34;&gt;https://btrfs.openfreemap.com/areas/planet/{version}/tiles.btrfs.gz&lt;/a&gt; (and .mbtiles)&lt;/p&gt; &#xA;&lt;p&gt;Use the &lt;a href=&#34;https://btrfs.openfreemap.com/files.txt&#34;&gt;index file&lt;/a&gt; to find out about versions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: MBTiles files are not required for this project. We provide them for your convenience, allowing you to use the processed planet tiles with any other tool of your choice.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Public buckets&lt;/h3&gt; &#xA;&lt;p&gt;There are two public buckets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://assets.openfreemap.com&#34;&gt;https://assets.openfreemap.com&lt;/a&gt; - contains fonts, sprites, styles, versions. index: &lt;a href=&#34;https://assets.openfreemap.com/dirs.txt&#34;&gt;dirs&lt;/a&gt;, &lt;a href=&#34;https://assets.openfreemap.com/files.txt&#34;&gt;files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://btrfs.openfreemap.com&#34;&gt;https://btrfs.openfreemap.com&lt;/a&gt; - full planet runs. index: &lt;a href=&#34;https://btrfs.openfreemap.com/dirs.txt&#34;&gt;dirs&lt;/a&gt;, &lt;a href=&#34;https://btrfs.openfreemap.com/files.txt&#34;&gt;files&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Domains&lt;/h3&gt; &#xA;&lt;p&gt;.org - not hosted through CloudFlare &lt;br&gt; .com - hosted through CloudFlare - serving the public buckets&lt;/p&gt; &#xA;&lt;h3&gt;What about PMTiles and using the Cloud?&lt;/h3&gt; &#xA;&lt;p&gt;I would have loved to use PMTiles; they are a brilliant idea for serverless map hosting!&lt;/p&gt; &#xA;&lt;p&gt;Unfortunately, on Cloudflare, range requests in 90 GB files have terrible latency, and on AWS, the egress costs can be prohibitive.&lt;/p&gt; &#xA;&lt;p&gt;Of course, with normal usage, you might fall within cloud vendor&#39;s free tier, but the internet is full of stories about people receiving surprise bills from AWS, sometimes amounting to thousands of dollars. It only takes one bad crawling bot getting stuck in a loop on your website to trigger such a bill.&lt;/p&gt; &#xA;&lt;p&gt;In short, using cloud vendors would make it impossible for me to offer this service for free — this project simply wouldn&#39;t exist.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributors welcome!&lt;/p&gt; &#xA;&lt;p&gt;Smaller tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloudflare worker for indexing the public buckets, instead of generating index files.&lt;/li&gt; &#xA; &lt;li&gt;[styles] Some of the POI icons are missing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Bigger tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[styles] Split the styles to building blocks. For example, there should be a POI block, a label block, a road-style related block.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Future:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Migrate to &lt;a href=&#34;https://shortbread-tiles.org/&#34;&gt;Shortbread schema&lt;/a&gt; and possibly &lt;a href=&#34;https://versatiles.org/&#34;&gt;VersaTiles&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Dev setup&lt;/h4&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hyperknot/openfreemap/main/docs/dev_setup.md&#34;&gt;dev setup docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.7&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;MBTiles are now uploaded, next to the btrfs image files.&lt;/p&gt; &#xA;&lt;h5&gt;v0.6&lt;/h5&gt; &#xA;&lt;p&gt;Load-balancer implemented with new config format. Implemented relaxed mode for checking while deployments are happening.&lt;/p&gt; &#xA;&lt;h5&gt;v0.5&lt;/h5&gt; &#xA;&lt;p&gt;Using a &#34;done&#34; file in the R2 buckets to mark the upload as finished. All scripts are checking for this file now.&lt;/p&gt; &#xA;&lt;p&gt;Monaco is generated daily, to avoid too frequent nginx reloads, which might be bad for the in-memory cache.&lt;/p&gt; &#xA;&lt;h5&gt;v0.4&lt;/h5&gt; &#xA;&lt;p&gt;Auto-update works!&lt;/p&gt; &#xA;&lt;p&gt;Monaco is generated hourly. Set-latest runs every minute.&lt;/p&gt; &#xA;&lt;p&gt;Planet is generated weekly, every Wednesday. Set-latest runs every Saturday.&lt;/p&gt; &#xA;&lt;h5&gt;v0.3&lt;/h5&gt; &#xA;&lt;p&gt;Lot of performance related problems with Cloudflare when using Round-Robin DNS. Works much better without any Cloudflare proxying, the browsers actually do a great job of client-side failover and selecting the best host.&lt;/p&gt; &#xA;&lt;p&gt;Load-balancing script running in check mode again.&lt;/p&gt; &#xA;&lt;h5&gt;v0.2&lt;/h5&gt; &#xA;&lt;p&gt;Load-balancing script is running in write mode, updating records when needed.&lt;/p&gt; &#xA;&lt;h5&gt;v0.1&lt;/h5&gt; &#xA;&lt;p&gt;Everything works. 1 server for tile gen, 2 servers for HTTP host. Load-balancing script is running in a read-only mode.&lt;/p&gt; &#xA;&lt;h2&gt;Attribution&lt;/h2&gt; &#xA;&lt;p&gt;Attribution is required. If you are using MapLibre, they are automatically added, you have nothing to do.&lt;/p&gt; &#xA;&lt;p&gt;If you are using alternative clients, or if you are using this in printed media or video, you must add the following attribution:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openfreemap.org&#34; target=&#34;_blank&#34;&gt;OpenFreeMap&lt;/a&gt; &lt;a href=&#34;https://www.openmaptiles.org/&#34; target=&#34;_blank&#34;&gt;© OpenMapTiles&lt;/a&gt; Data from &lt;a href=&#34;https://www.openstreetmap.org/copyright&#34; target=&#34;_blank&#34;&gt;OpenStreetMap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You do not need to display the OpenFreeMap part, but it is nice if you do.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The license of this project is &lt;a href=&#34;https://www.tldrlegal.com/license/mit-license&#34;&gt;MIT&lt;/a&gt;. Map data is from &lt;a href=&#34;https://www.openstreetmap.org/copyright&#34;&gt;OpenStreetMap&lt;/a&gt;. The licenses for included projects are listed in &lt;a href=&#34;https://github.com/hyperknot/openfreemap/raw/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>