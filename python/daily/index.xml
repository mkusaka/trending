<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-09T01:31:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>netbox-community/netbox</title>
    <updated>2022-07-09T01:31:18Z</updated>
    <id>tag:github.com,2022-07-09:/netbox-community/netbox</id>
    <link href="https://github.com/netbox-community/netbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Infrastructure resource modeling for network automation. Open source under Apache 2. Public demo: https://demo.netbox.dev&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/netbox_logo.svg?sanitize=true&#34; width=&#34;400&#34; alt=&#34;NetBox logo&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/netbox-community/netbox/workflows/CI/badge.svg?branch=master&#34; alt=&#34;Master branch build status&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;NetBox is an infrastructure resource modeling (IRM) tool designed to empower network automation, used by thousands of organizations around the world. Initially conceived by the network engineering team at &lt;a href=&#34;https://www.digitalocean.com/&#34;&gt;DigitalOcean&lt;/a&gt;, NetBox was developed specifically to address the needs of network and infrastructure engineers. It is intended to function as a domain-specific source of truth for network operations.&lt;/p&gt; &#xA;&lt;p&gt;Myriad infrastructure components can be modeled in NetBox, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hierarchical regions, site groups, sites, and locations&lt;/li&gt; &#xA; &lt;li&gt;Racks, devices, and device components&lt;/li&gt; &#xA; &lt;li&gt;Cables and wireless connections&lt;/li&gt; &#xA; &lt;li&gt;Power distribution&lt;/li&gt; &#xA; &lt;li&gt;Data circuits and providers&lt;/li&gt; &#xA; &lt;li&gt;Virtual machines and clusters&lt;/li&gt; &#xA; &lt;li&gt;IP prefixes, ranges, and addresses&lt;/li&gt; &#xA; &lt;li&gt;VRFs and route targets&lt;/li&gt; &#xA; &lt;li&gt;FHRP groups (VRRP, HSRP, etc.)&lt;/li&gt; &#xA; &lt;li&gt;AS numbers&lt;/li&gt; &#xA; &lt;li&gt;VLANs and scoped VLAN groups&lt;/li&gt; &#xA; &lt;li&gt;Organizational tenants and contacts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to its extensive built-in models and functionality, NetBox can be customized and extended through the use of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom fields&lt;/li&gt; &#xA; &lt;li&gt;Custom links&lt;/li&gt; &#xA; &lt;li&gt;Configuration contexts&lt;/li&gt; &#xA; &lt;li&gt;Custom model validation rules&lt;/li&gt; &#xA; &lt;li&gt;Reports&lt;/li&gt; &#xA; &lt;li&gt;Custom scripts&lt;/li&gt; &#xA; &lt;li&gt;Export templates&lt;/li&gt; &#xA; &lt;li&gt;Conditional webhooks&lt;/li&gt; &#xA; &lt;li&gt;Plugins&lt;/li&gt; &#xA; &lt;li&gt;Single sign-on (SSO) authentication&lt;/li&gt; &#xA; &lt;li&gt;NAPALM integration&lt;/li&gt; &#xA; &lt;li&gt;Detailed change logging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NetBox also features a complete REST API as well as a GraphQL API for easily integrating with other tools and systems.&lt;/p&gt; &#xA;&lt;p&gt;NetBox runs as a web application atop the &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; Python framework with a &lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; database. For a complete list of requirements, see &lt;code&gt;requirements.txt&lt;/code&gt;. The code is available &lt;a href=&#34;https://github.com/netbox-community/netbox&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The complete documentation for NetBox can be found at &lt;a href=&#34;https://docs.netbox.dev/&#34;&gt;docs.netbox.dev&lt;/a&gt;. A public demo instance is available at &lt;a href=&#34;https://demo.netbox.dev&#34;&gt;https://demo.netbox.dev&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;Thank you to our sponsors!&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://try.digitalocean.com/developer-cloud&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/netbox-community/netbox/images/sponsors/digitalocean.png&#34; alt=&#34;DigitalOcean&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://metal.equinix.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/netbox-community/netbox/images/sponsors/equinix.png&#34; alt=&#34;Equinix Metal&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://ns1.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/netbox-community/netbox/images/sponsors/ns1.png&#34; alt=&#34;NS1&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://sentry.io/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/netbox-community/netbox/images/sponsors/sentry.png&#34; alt=&#34;Sentry&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://stellar.tech/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/netbox-community/netbox/images/sponsors/stellar.png&#34; alt=&#34;Stellar Technologies&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Discussion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netbox-community/netbox/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; - Discussion forum hosted by GitHub; ideal for Q&amp;amp;A and other structured discussions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netdev.chat/&#34;&gt;Slack&lt;/a&gt; - Real-time chat hosted by the NetDev Community; best for unstructured discussion or just hanging out&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://docs.netbox.dev/&#34;&gt;the documentation&lt;/a&gt; for instructions on installing NetBox. To upgrade NetBox, please download the &lt;a href=&#34;https://github.com/netbox-community/netbox/releases&#34;&gt;latest release&lt;/a&gt; and run &lt;code&gt;upgrade.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Providing Feedback&lt;/h3&gt; &#xA;&lt;p&gt;The best platform for general feedback, assistance, and other discussion is our &lt;a href=&#34;https://github.com/netbox-community/netbox/discussions&#34;&gt;GitHub discussions&lt;/a&gt;. To report a bug or request a specific feature, please open a GitHub issue using the &lt;a href=&#34;https://github.com/netbox-community/netbox/issues/new/choose&#34;&gt;appropriate template&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in contributing to the development of NetBox, please read our &lt;a href=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; prior to beginning any work.&lt;/p&gt; &#xA;&lt;h3&gt;Screenshots&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/media/screenshots/home-light.png&#34; alt=&#34;Screenshot of main page (light mode)&#34; title=&#34;Main page (light mode)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/media/screenshots/home-dark.png&#34; alt=&#34;Screenshot of main page (dark mode)&#34; title=&#34;Main page (dark mode)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/media/screenshots/rack.png&#34; alt=&#34;Screenshot of rack elevation&#34; title=&#34;Rack elevation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/media/screenshots/prefixes-list.png&#34; alt=&#34;Screenshot of prefixes hierarchy&#34; title=&#34;Prefixes hierarchy&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/media/screenshots/cable-trace.png&#34; alt=&#34;Screenshot of cable trace&#34; title=&#34;Cable tracing&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Related projects&lt;/h3&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://github.com/netbox-community/netbox/wiki/Community-Contributions&#34;&gt;our wiki&lt;/a&gt; for a list of relevant community projects.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/fairseq</title>
    <updated>2022-07-09T01:31:18Z</updated>
    <id>tag:github.com,2022-07-09:/facebookresearch/fairseq</id>
    <link href="https://github.com/facebookresearch/fairseq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Facebook AI Research Sequence-to-Sequence Toolkit written in Python.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/fairseq_logo.png&#34; width=&#34;150&#34;&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://opensource.fb.com/support-ukraine&#34;&gt;&lt;img alt=&#34;Support Ukraine&#34; src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq/raw/main/LICENSE&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq/releases&#34;&gt;&lt;img alt=&#34;Latest Release&#34; src=&#34;https://img.shields.io/github/release/pytorch/fairseq.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq/actions?query=workflow:build&#34;&gt;&lt;img alt=&#34;Build Status&#34; src=&#34;https://github.com/pytorch/fairseq/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fairseq.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img alt=&#34;Documentation Status&#34; src=&#34;https://readthedocs.org/projects/fairseq/badge/?version=latest&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.circleci.com/pipelines/github/facebookresearch/fairseq/&#34;&gt;&lt;img alt=&#34;CicleCI Status&#34; src=&#34;https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;We provide reference implementations of various sequence modeling papers:&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;List of implemented papers&lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Convolutional Neural Networks (CNN)&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/conv_lm/README.md&#34;&gt;Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/conv_seq2seq/README.md&#34;&gt;Convolutional Sequence to Sequence Learning (Gehring et al., 2017)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/classic_seqlevel&#34;&gt;Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/stories/README.md&#34;&gt;Hierarchical Neural Story Generation (Fan et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;LightConv and DynamicConv models&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pay_less_attention_paper/README.md&#34;&gt;Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Long Short-Term Memory (LSTM) networks&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Transformer (self-attention) networks&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Attention Is All You Need (Vaswani et al., 2017)&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/scaling_nmt/README.md&#34;&gt;Scaling Neural Machine Translation (Ott et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/backtranslation/README.md&#34;&gt;Understanding Back-Translation at Scale (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.adaptive_inputs.md&#34;&gt;Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md&#34;&gt;Lexically constrained decoding with dynamic beam allocation (Post &amp;amp; Vilar, 2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/truncated_bptt/README.md&#34;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/adaptive_span/README.md&#34;&gt;Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation_moe/README.md&#34;&gt;Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md&#34;&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md&#34;&gt;Facebook FAIR&#39;s WMT19 News Translation Task Submission (Ng et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/joint_alignment_translation/README.md&#34;&gt;Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md&#34;&gt;Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md&#34;&gt;Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md&#34;&gt;Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pointer_generator/README.md&#34;&gt;Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/linformer/README.md&#34;&gt;Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md&#34;&gt;Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/latent_depth/README.md&#34;&gt;Deep Transformers with Latent Depth (Li et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.13979&#34;&gt;Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11430&#34;&gt;Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.01027&#34;&gt;Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.11084&#34;&gt;Unsupervised Speech Recognition (Baevski, et al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.11680&#34;&gt;Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2109.14084.pdf&#34;&gt;VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.findings-acl.370.pdf&#34;&gt;VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/normformer/README.md&#34;&gt;NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Non-autoregressive Transformers&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Non-Autoregressive Neural Machine Translation (Gu et al., 2017)&lt;/li&gt; &#xA;    &lt;li&gt;Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)&lt;/li&gt; &#xA;    &lt;li&gt;Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)&lt;/li&gt; &#xA;    &lt;li&gt;Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md&#34;&gt;Levenshtein Transformer (Gu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Finetuning&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/rxf/README.md&#34;&gt;Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;What&#39;s New:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;June 2022 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/unsupervised/README.md&#34;&gt;Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2022 &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;Integration with xFormers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;December 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/speech_to_speech/README.md&#34;&gt;Released Direct speech-to-speech translation code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/MMPT/README.md&#34;&gt;Released VideoCLIP and VLM models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;Released multilingual finetuned XLSR-53 model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;September 2021 &lt;a href=&#34;https://github.com/github/renaming&#34;&gt;&lt;code&gt;master&lt;/code&gt; branch renamed to &lt;code&gt;main&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;July 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/discriminative_reranking_nmt/README.md&#34;&gt;Released DrNMT code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;July 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;Released Robust wav2vec 2.0 model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;June 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/xlmr/README.md&#34;&gt;Released XLMR-XL and XLMR-XXL models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/unsupervised/README.md&#34;&gt;Released Unsupervised Speech Recognition code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;March 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md&#34;&gt;Added full parameter and optimizer state sharding + CPU offloading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;February 2021 &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/laser/README.md&#34;&gt;Added LASER training code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;December 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/adaptive_span/README.md&#34;&gt;Added Adaptive Attention Span code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;December 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/gottbert/README.md&#34;&gt;GottBERT model and code released&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;November 2020: Adopted the &lt;a href=&#34;https://github.com/facebookresearch/hydra&#34;&gt;Hydra&lt;/a&gt; configuration framework &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/hydra_integration.md&#34;&gt;see documentation explaining how to use it for new and existing projects&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;November 2020: &lt;a href=&#34;https://github.com/pytorch/fairseq/releases/tag/v0.10.0&#34;&gt;fairseq 0.10.0 released&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/rxf/README.md&#34;&gt;Added R3F/R4F (Better Fine-Tuning) code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/latent_depth/README.md&#34;&gt;Deep Transformer with Latent Depth code released&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md&#34;&gt;Added CRISS models and code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Previous updates&lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;September 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/linformer/README.md&#34;&gt;Added Linformer code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;September 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pointer_generator/README.md&#34;&gt;Added pointer-generator networks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;August 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md&#34;&gt;Added lexically constrained decoding&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;August 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec2 models and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;July 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md&#34;&gt;Unsupervised Quality Estimation code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;May 2020: &lt;a href=&#34;https://twitter.com/fairseq&#34;&gt;Follow fairseq on Twitter&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;April 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/simultaneous_translation/README.md&#34;&gt;Monotonic Multihead Attention code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;April 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/quant_noise/README.md&#34;&gt;Quant-Noise code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;April 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/megatron_11b/README.md&#34;&gt;Initial model parallel support and 11B parameters unidirectional LM released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;March 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md&#34;&gt;Byte-level BPE code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;February 2020: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md&#34;&gt;mBART model and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;February 2020: &lt;a href=&#34;https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german&#34;&gt;Added tutorial for back-translation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;December 2019: &lt;a href=&#34;https://github.com/pytorch/fairseq/releases/tag/v0.9.0&#34;&gt;fairseq 0.9.0 released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;November 2019: &lt;a href=&#34;https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example&#34;&gt;VizSeq released (a visual analysis toolkit for evaluating fairseq models)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;November 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/camembert/README.md&#34;&gt;CamemBERT model and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;November 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/bart/README.md&#34;&gt;BART model and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;November 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/xlmr/README.md&#34;&gt;XLM-R models and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;September 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md&#34;&gt;Nonautoregressive translation code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;August 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md&#34;&gt;WMT&#39;19 models released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;July 2019: fairseq relicensed under MIT license&lt;/li&gt; &#xA;  &lt;li&gt;July 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md&#34;&gt;RoBERTa models and code released&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;June 2019: &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec models and code released&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;multi-GPU training on one machine or across multiple machines (data and model parallel)&lt;/li&gt; &#xA; &lt;li&gt;fast generation on both CPU and GPU with multiple search algorithms implemented: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;beam search&lt;/li&gt; &#xA;   &lt;li&gt;Diverse Beam Search (&lt;a href=&#34;https://arxiv.org/abs/1610.02424&#34;&gt;Vijayakumar et al., 2016&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;sampling (unconstrained, top-k and top-p/nucleus)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md&#34;&gt;lexically constrained decoding&lt;/a&gt; (Post &amp;amp; Vilar, 2018)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates&#34;&gt;gradient accumulation&lt;/a&gt; enables training with large mini-batches even on a single GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16&#34;&gt;mixed precision training&lt;/a&gt; (trains faster with less GPU memory on &lt;a href=&#34;https://developer.nvidia.com/tensor-cores&#34;&gt;NVIDIA tensor cores&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fairseq.readthedocs.io/en/latest/overview.html&#34;&gt;extensible&lt;/a&gt;: easily register new models, criterions, tasks, optimizers and learning rate schedulers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/hydra_integration.md&#34;&gt;flexible configuration&lt;/a&gt; based on &lt;a href=&#34;https://github.com/facebookresearch/hydra&#34;&gt;Hydra&lt;/a&gt; allowing a combination of code, command-line and file based configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md&#34;&gt;full parameter and optimizer state sharding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md&#34;&gt;offloading parameters to CPU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also provide &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/#pre-trained-models-and-examples&#34;&gt;pre-trained models for translation and language modeling&lt;/a&gt; with a convenient &lt;code&gt;torch.hub&lt;/code&gt; interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;en2de = torch.hub.load(&#39;pytorch/fairseq&#39;, &#39;transformer.wmt19.en-de.single_model&#39;)&#xA;en2de.translate(&#39;Hello world&#39;, beam=5)&#xA;# &#39;Hallo Welt&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the PyTorch Hub tutorials for &lt;a href=&#34;https://pytorch.org/hub/pytorch_fairseq_translation/&#34;&gt;translation&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/hub/pytorch_fairseq_roberta/&#34;&gt;RoBERTa&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;h1&gt;Requirements and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; version &amp;gt;= 1.5.0&lt;/li&gt; &#xA; &lt;li&gt;Python version &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;For training new models, you&#39;ll also need an NVIDIA GPU and &lt;a href=&#34;https://github.com/NVIDIA/nccl&#34;&gt;NCCL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;To install fairseq&lt;/strong&gt; and develop locally:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/fairseq&#xA;cd fairseq&#xA;pip install --editable ./&#xA;&#xA;# on MacOS:&#xA;# CFLAGS=&#34;-stdlib=libc++&#34; pip install --editable ./&#xA;&#xA;# to install the latest stable release (0.10.x)&#xA;# pip install fairseq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;For faster training&lt;/strong&gt; install NVIDIA&#39;s &lt;a href=&#34;https://github.com/NVIDIA/apex&#34;&gt;apex&lt;/a&gt; library:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;pip install -v --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; \&#xA;  --global-option=&#34;--deprecated_fused_adam&#34; --global-option=&#34;--xentropy&#34; \&#xA;  --global-option=&#34;--fast_multihead_attn&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;For large datasets&lt;/strong&gt; install &lt;a href=&#34;https://arrow.apache.org/docs/python/install.html#using-pip&#34;&gt;PyArrow&lt;/a&gt;: &lt;code&gt;pip install pyarrow&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you use Docker make sure to increase the shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; as command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt; .&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://fairseq.readthedocs.io/&#34;&gt;full documentation&lt;/a&gt; contains instructions for getting started, training new models and extending fairseq with new model types and tasks.&lt;/p&gt; &#xA;&lt;h1&gt;Pre-trained models and examples&lt;/h1&gt; &#xA;&lt;p&gt;We provide pre-trained models and pre-processed, binarized test sets for several tasks listed below, as well as example training and evaluation commands.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation/README.md&#34;&gt;Translation&lt;/a&gt;: convolutional and transformer models are available&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.md&#34;&gt;Language Modeling&lt;/a&gt;: convolutional and transformer models are available&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also have more detailed READMEs to reproduce results from specific papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/xlsr/README.md&#34;&gt;XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md&#34;&gt;Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md&#34;&gt;Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/quant_noise/README.md&#34;&gt;Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md&#34;&gt;Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md&#34;&gt;Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/layerdrop/README.md&#34;&gt;Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/joint_alignment_translation/README.md&#34;&gt;Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md&#34;&gt;Levenshtein Transformer (Gu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md&#34;&gt;Facebook FAIR&#39;s WMT19 News Translation Task Submission (Ng et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md&#34;&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md&#34;&gt;wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation_moe/README.md&#34;&gt;Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pay_less_attention_paper/README.md&#34;&gt;Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/backtranslation/README.md&#34;&gt;Understanding Back-Translation at Scale (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq/tree/classic_seqlevel&#34;&gt;Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/stories/README.md&#34;&gt;Hierarchical Neural Story Generation (Fan et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/scaling_nmt/README.md&#34;&gt;Scaling Neural Machine Translation (Ott et al., 2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/conv_seq2seq/README.md&#34;&gt;Convolutional Sequence to Sequence Learning (Gehring et al., 2017)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.conv.md&#34;&gt;Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Join the fairseq community&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/fairseq&#34;&gt;https://twitter.com/fairseq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook page: &lt;a href=&#34;https://www.facebook.com/groups/fairseq.users&#34;&gt;https://www.facebook.com/groups/fairseq.users&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google group: &lt;a href=&#34;https://groups.google.com/forum/#!forum/fairseq-users&#34;&gt;https://groups.google.com/forum/#!forum/fairseq-users&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;Please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{ott2019fairseq,&#xA;  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},&#xA;  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},&#xA;  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},&#xA;  year = {2019},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>nsidnev/fastapi-realworld-example-app</title>
    <updated>2022-07-09T01:31:18Z</updated>
    <id>tag:github.com,2022-07-09:/nsidnev/fastapi-realworld-example-app</id>
    <link href="https://github.com/nsidnev/fastapi-realworld-example-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Backend logic implementation for https://github.com/gothinkster/realworld with awesome FastAPI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: ./.github/assets/logo.png&lt;/p&gt; &#xA;&lt;p&gt;|&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/API%20spec/badge.svg&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/API%20spec/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/Tests/badge.svg&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/Tests/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/Styles/badge.svg&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app/workflows/Styles/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://codecov.io/gh/nsidnev/fastapi-realworld-example-app/branch/master/graph/badge.svg&#34;&gt;https://codecov.io/gh/nsidnev/fastapi-realworld-example-app/branch/master/graph/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://codecov.io/gh/nsidnev/fastapi-realworld-example-app&#34;&gt;https://codecov.io/gh/nsidnev/fastapi-realworld-example-app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/github/license/Naereen/StrapDown.js.svg&#34;&gt;https://img.shields.io/github/license/Naereen/StrapDown.js.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/nsidnev/fastapi-realworld-example-app/raw/master/LICENSE&#34;&gt;https://github.com/nsidnev/fastapi-realworld-example-app/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/ambv/black&#34;&gt;https://github.com/ambv/black&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/badge/style-wemake-000000.svg&#34;&gt;https://img.shields.io/badge/style-wemake-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/wemake-services/wemake-python-styleguide&#34;&gt;https://github.com/wemake-services/wemake-python-styleguide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This repository is not actively maintained because this example is quite complete and does its primary goal - passing Conduit testsuite.&lt;/p&gt; &#xA;&lt;p&gt;More modern and relevant examples can be found in other repositories with &lt;code&gt;fastapi&lt;/code&gt; tag on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;First, run &lt;code&gt;PostgreSQL&lt;/code&gt;, set environment variables and create database. For example using &lt;code&gt;docker&lt;/code&gt;: ::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export POSTGRES_DB=rwdb POSTGRES_PORT=5432 POSTGRES_USER=postgres POSTGRES_PASSWORD=postgres&#xA;docker run --name pgdb --rm -e POSTGRES_USER=&#34;$POSTGRES_USER&#34; -e POSTGRES_PASSWORD=&#34;$POSTGRES_PASSWORD&#34; -e POSTGRES_DB=&#34;$POSTGRES_DB&#34; postgres&#xA;export POSTGRES_HOST=$(docker inspect -f &#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}&#39; pgdb)&#xA;createdb --host=$POSTGRES_HOST --port=$POSTGRES_PORT --username=$POSTGRES_USER $POSTGRES_DB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following commands to bootstrap your environment with &lt;code&gt;poetry&lt;/code&gt;: ::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/nsidnev/fastapi-realworld-example-app&#xA;cd fastapi-realworld-example-app&#xA;poetry install&#xA;poetry shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then create &lt;code&gt;.env&lt;/code&gt; file (or rename and modify &lt;code&gt;.env.example&lt;/code&gt;) in project root and set environment variables for application: ::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;touch .env&#xA;echo APP_ENV=dev&#xA;echo DATABASE_URL=postgresql://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB &amp;gt;&amp;gt; .env&#xA;echo SECRET_KEY=$(openssl rand -hex 32) &amp;gt;&amp;gt; .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the web application in debug use::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;alembic upgrade head&#xA;uvicorn app.main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run into the following error in your docker container:&lt;/p&gt; &#xA;&lt;p&gt;sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket &#34;/tmp/.s.PGSQL.5432&#34;?&lt;/p&gt; &#xA;&lt;p&gt;Ensure the DATABASE_URL variable is set correctly in the &lt;code&gt;.env&lt;/code&gt; file. It is most likely caused by POSTGRES_HOST not pointing to its localhost.&lt;/p&gt; &#xA;&lt;p&gt;DATABASE_URL=postgresql://postgres:postgres@0.0.0.0:5432/rwdb&lt;/p&gt; &#xA;&lt;h2&gt;Run tests&lt;/h2&gt; &#xA;&lt;p&gt;Tests for this project are defined in the &lt;code&gt;tests/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Set up environment variable &lt;code&gt;DATABASE_URL&lt;/code&gt; or set up &lt;code&gt;database_url&lt;/code&gt; in &lt;code&gt;app/core/settings/test.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project uses &lt;code&gt;pytest &amp;lt;https://docs.pytest.org/&amp;gt;&lt;/code&gt;_ to define tests because it allows you to use the &lt;code&gt;assert&lt;/code&gt; keyword with good formatting for failed assertations.&lt;/p&gt; &#xA;&lt;p&gt;To run all the tests of a project, simply run the &lt;code&gt;pytest&lt;/code&gt; command: ::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pytest&#xA;================================================= test session starts ==================================================&#xA;platform linux -- Python 3.8.3, pytest-5.4.2, py-1.8.1, pluggy-0.13.1&#xA;rootdir: /home/some-user/user-projects/fastapi-realworld-example-app, inifile: setup.cfg, testpaths: tests&#xA;plugins: env-0.6.2, cov-2.9.0, asyncio-0.12.0&#xA;collected 90 items&#xA;&#xA;tests/test_api/test_errors/test_422_error.py .                                                                   [  1%]&#xA;tests/test_api/test_errors/test_error.py .                                                                       [  2%]&#xA;tests/test_api/test_routes/test_articles.py .................................                                    [ 38%]&#xA;tests/test_api/test_routes/test_authentication.py ..                                                             [ 41%]&#xA;tests/test_api/test_routes/test_comments.py ....                                                                 [ 45%]&#xA;tests/test_api/test_routes/test_login.py ...                                                                     [ 48%]&#xA;tests/test_api/test_routes/test_profiles.py ............                                                         [ 62%]&#xA;tests/test_api/test_routes/test_registration.py ...                                                              [ 65%]&#xA;tests/test_api/test_routes/test_tags.py ..                                                                       [ 67%]&#xA;tests/test_api/test_routes/test_users.py ....................                                                    [ 90%]&#xA;tests/test_db/test_queries/test_tables.py ...                                                                    [ 93%]&#xA;tests/test_schemas/test_rw_model.py .                                                                            [ 94%]&#xA;tests/test_services/test_jwt.py .....                                                                            [100%]&#xA;&#xA;============================================ 90 passed in 70.50s (0:01:10) =============================================&#xA;$&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run a specific test, you can do this with &lt;code&gt;this &amp;lt;https://docs.pytest.org/en/latest/usage.html#specifying-tests-selecting-tests&amp;gt;&lt;/code&gt;_ pytest feature: ::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pytest tests/test_api/test_routes/test_users.py::test_user_can_not_take_already_used_credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment with Docker&lt;/h2&gt; &#xA;&lt;p&gt;You must have &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-compose&lt;/code&gt; tools installed to work with material in this section. First, create &lt;code&gt;.env&lt;/code&gt; file like in &lt;code&gt;Quickstart&lt;/code&gt; section or modify &lt;code&gt;.env.example&lt;/code&gt;. &lt;code&gt;POSTGRES_HOST&lt;/code&gt; must be specified as &lt;code&gt;db&lt;/code&gt; or modified in &lt;code&gt;docker-compose.yml&lt;/code&gt; also. Then just run::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up -d db&#xA;docker-compose up -d app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Application will be available on &lt;code&gt;localhost&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;h2&gt;Web routes&lt;/h2&gt; &#xA;&lt;p&gt;All routes are available on &lt;code&gt;/docs&lt;/code&gt; or &lt;code&gt;/redoc&lt;/code&gt; paths with Swagger or ReDoc.&lt;/p&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;p&gt;Files related to application are in the &lt;code&gt;app&lt;/code&gt; or &lt;code&gt;tests&lt;/code&gt; directories. Application parts are:&lt;/p&gt; &#xA;&lt;p&gt;::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;app&#xA; api              - web related stuff.&#xA;&amp;nbsp;&amp;nbsp;  dependencies - dependencies for routes definition.&#xA;&amp;nbsp;&amp;nbsp;  errors       - definition of error handlers.&#xA;&amp;nbsp;&amp;nbsp;  routes       - web routes.&#xA; core             - application configuration, startup events, logging.&#xA; db               - db related stuff.&#xA;&amp;nbsp;&amp;nbsp;  migrations   - manually written alembic migrations.&#xA;&amp;nbsp;&amp;nbsp;  repositories - all crud stuff.&#xA; models           - pydantic models for this application.&#xA;&amp;nbsp;&amp;nbsp;  domain       - main models that are used almost everywhere.&#xA;&amp;nbsp;&amp;nbsp;  schemas      - schemas for using in web routes.&#xA; resources        - strings that are used in web responses.&#xA; services         - logic that is not just crud related.&#xA; main.py          - FastAPI application creation and configuration.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>