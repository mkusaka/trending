<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-14T01:38:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KAIR-BAIR/nerfacc</title>
    <updated>2022-10-14T01:38:11Z</updated>
    <id>tag:github.com,2022-10-14:/KAIR-BAIR/nerfacc</id>
    <link href="https://github.com/KAIR-BAIR/nerfacc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A General NeRF Acceleration Toolbox in PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NerfAcc&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KAIR-BAIR/nerfacc/actions/workflows/code_checks.yml&#34;&gt;&lt;img src=&#34;https://github.com/KAIR-BAIR/nerfacc/actions/workflows/code_checks.yml/badge.svg?sanitize=true&#34; alt=&#34;Core Tests.&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.nerfacc.com/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.com/projects/plenoptix-nerfacc/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/nerfacc&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nerfacc&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nerfacc.com/&#34;&gt;https://www.nerfacc.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NerfAcc is a PyTorch Nerf acceleration toolbox for both training and inference. It focus on efficient volumetric rendering of radiance fields, which is universal and plug-and-play for most of the NeRFs.&lt;/p&gt; &#xA;&lt;p&gt;Using NerfAcc,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;vanilla NeRF&lt;/code&gt; model with 8-layer MLPs can be trained to &lt;em&gt;better quality&lt;/em&gt; (+~0.5 PNSR) in &lt;em&gt;1 hour&lt;/em&gt; rather than &lt;em&gt;days&lt;/em&gt; as in the paper.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;Instant-NGP NeRF&lt;/code&gt; model can be trained to &lt;em&gt;equal quality&lt;/em&gt; in &lt;em&gt;4.5 minutes&lt;/em&gt;, comparing to the official pure-CUDA implementation.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;D-NeRF&lt;/code&gt; model for &lt;em&gt;dynamic&lt;/em&gt; objects can also be trained in &lt;em&gt;1 hour&lt;/em&gt; rather than &lt;em&gt;2 days&lt;/em&gt; as in the paper, and with &lt;em&gt;better quality&lt;/em&gt; (+~2.5 PSNR).&lt;/li&gt; &#xA; &lt;li&gt;Both &lt;em&gt;bounded&lt;/em&gt; and &lt;em&gt;unbounded&lt;/em&gt; scenes are supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;And it is pure Python interface with flexible APIs!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nerfacc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The idea of NerfAcc is to perform efficient ray marching and volumetric rendering. So NerfAcc can work with any user-defined radiance field. To plug the NerfAcc rendering pipeline into your code and enjoy the acceleration, you only need to define two functions with your radience field.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sigma_fn&lt;/code&gt;: Compute density at each sample. It will be used by &lt;code&gt;nerfacc.ray_marching()&lt;/code&gt; to skip the empty and occluded space during ray marching, which is where the major speedup comes from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rgb_sigma_fn&lt;/code&gt;: Compute color and density at each sample. It will be used by &lt;code&gt;nerfacc.rendering()&lt;/code&gt; to conduct differentiable volumetric rendering. This function will receive gradients to update your network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An simple example is like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from torch import Tensor&#xA;import nerfacc &#xA;&#xA;radiance_field = ...  # network: a NeRF model&#xA;rays_o: Tensor = ...  # ray origins. (n_rays, 3)&#xA;rays_d: Tensor = ...  # ray normalized directions. (n_rays, 3)&#xA;optimizer = ...  # optimizer&#xA;&#xA;def sigma_fn(&#xA;    t_starts: Tensor, t_ends:Tensor, ray_indices: Tensor&#xA;) -&amp;gt; Tensor:&#xA;    &#34;&#34;&#34; Query density values from a user-defined radiance field.&#xA;    :params t_starts: Start of the sample interval along the ray. (n_samples, 1).&#xA;    :params t_ends: End of the sample interval along the ray. (n_samples, 1).&#xA;    :params ray_indices: Ray indices that each sample belongs to. (n_samples,).&#xA;    :returns The post-activation density values. (n_samples, 1).&#xA;    &#34;&#34;&#34;&#xA;    t_origins = rays_o[ray_indices]  # (n_samples, 3)&#xA;    t_dirs = rays_d[ray_indices]  # (n_samples, 3)&#xA;    positions = t_origins + t_dirs * (t_starts + t_ends) / 2.0&#xA;    sigmas = radiance_field.query_density(positions) &#xA;    return sigmas  # (n_samples, 1)&#xA;&#xA;def rgb_sigma_fn(&#xA;    t_starts: Tensor, t_ends: Tensor, ray_indices: Tensor&#xA;) -&amp;gt; Tuple[Tensor, Tensor]:&#xA;    &#34;&#34;&#34; Query rgb and density values from a user-defined radiance field.&#xA;    :params t_starts: Start of the sample interval along the ray. (n_samples, 1).&#xA;    :params t_ends: End of the sample interval along the ray. (n_samples, 1).&#xA;    :params ray_indices: Ray indices that each sample belongs to. (n_samples,).&#xA;    :returns The post-activation rgb and density values. &#xA;        (n_samples, 3), (n_samples, 1).&#xA;    &#34;&#34;&#34;&#xA;    t_origins = rays_o[ray_indices]  # (n_samples, 3)&#xA;    t_dirs = rays_d[ray_indices]  # (n_samples, 3)&#xA;    positions = t_origins + t_dirs * (t_starts + t_ends) / 2.0&#xA;    rgbs, sigmas = radiance_field(positions, condition=t_dirs)  &#xA;    return rgbs, sigmas  # (n_samples, 3), (n_samples, 1)&#xA;&#xA;# Efficient Raymarching: Skip empty and occluded space, pack samples from all rays.&#xA;# packed_info: (n_rays, 2). t_starts: (n_samples, 1). t_ends: (n_samples, 1).&#xA;with torch.no_grad():&#xA;    packed_info, t_starts, t_ends = nerfacc.ray_marching(&#xA;        rays_o, rays_d, sigma_fn=sigma_fn, near_plane=0.2, far_plane=1.0, &#xA;        early_stop_eps=1e-4, alpha_thre=1e-2, &#xA;    )&#xA;&#xA;# Differentiable Volumetric Rendering.&#xA;# colors: (n_rays, 3). opaicity: (n_rays, 1). depth: (n_rays, 1).&#xA;color, opacity, depth = nerfacc.rendering(rgb_sigma_fn, packed_info, t_starts, t_ends)&#xA;&#xA;# Optimize: Both the network and rays will receive gradients&#xA;optimizer.zero_grad()&#xA;loss = F.mse_loss(color, color_gt)&#xA;loss.backward()&#xA;optimizer.step()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples:&lt;/h2&gt; &#xA;&lt;p&gt;Before running those example scripts, please check the script about which dataset it is needed, and download the dataset first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Instant-NGP NeRF in 4.5 minutes with reproduced performance!&#xA;# See results at here: https://www.nerfacc.com/en/latest/examples/ngp.html&#xA;python examples/train_ngp_nerf.py --train_split train --scene lego&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Vanilla MLP NeRF in 1 hour with better performance!&#xA;# See results at here: https://www.nerfacc.com/en/latest/examples/vanilla.html&#xA;python examples/train_mlp_nerf.py --train_split train --scene lego&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# D-NeRF for Dynamic objects in 1 hour with better performance!&#xA;# See results at here: https://www.nerfacc.com/en/latest/examples/dnerf.html&#xA;python examples/train_mlp_dnerf.py --train_split train --scene lego&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Instant-NGP on unbounded scenes in 20 minutes!&#xA;# See results at here: https://www.nerfacc.com/en/latest/examples/unbounded.html&#xA;python examples/train_ngp_nerf.py --train_split train --scene garden --auto_aabb --unbounded --cone_angle=0.004&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2022nerfacc,&#xA;  title={NerfAcc: A General NeRF Accleration Toolbox.},&#xA;  author={Li, Ruilong and Tancik, Matthew and Kanazawa, Angjoo},&#xA;  journal={arXiv preprint arXiv:2210.04847},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>TachibanaYoshino/AnimeGANv2</title>
    <updated>2022-10-14T01:38:11Z</updated>
    <id>tag:github.com,2022-10-14:/TachibanaYoshino/AnimeGANv2</id>
    <link href="https://github.com/TachibanaYoshino/AnimeGANv2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[Open Source]. The improved version of AnimeGAN. Landscape photos/videos to anime&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AnimeGANv2&lt;/h1&gt; &#xA;&lt;p&gt;「Open Source」. The improved version of AnimeGAN.&lt;br&gt; 「&lt;a href=&#34;https://tachibanayoshino.github.io/AnimeGANv2/&#34;&gt;Project Page&lt;/a&gt;」 | Landscape photos/videos to anime&lt;/p&gt; &#xA;&lt;h3&gt;If you like what I&#39;m doing you can tip me on &lt;a href=&#34;https://www.patreon.com/Asher_Chan&#34;&gt;&lt;em&gt;patreon&lt;/em&gt;&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;p&gt;Photos &lt;a href=&#34;https://drive.google.com/file/d/1PbBkmj1EhULvEE8AXr2z84pZ2DQJN4hc/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Videos &lt;a href=&#34;https://drive.google.com/file/d/1qhBxA72Wxbh6Eyhd-V0zY_jTIblP9rHz/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab for videos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2022.08.03) Added the AnimeGANv2 &lt;a href=&#34;https://drive.google.com/file/d/1PbBkmj1EhULvEE8AXr2z84pZ2DQJN4hc/view?usp=sharing&#34;&gt;Colab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(2021.12.25) &lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGANv3&#34;&gt;&lt;strong&gt;AnimeGANv3&lt;/strong&gt;&lt;/a&gt; has been released.&lt;span&gt;🎄&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;(2021.02.21) &lt;a href=&#34;https://github.com/bryandlee/animegan2-pytorch&#34;&gt;The pytorch version of AnimeGANv2 has been released&lt;/a&gt;, Be grateful to @bryandlee for his contribution.&lt;/li&gt; &#xA; &lt;li&gt;(2020.12.25) AnimeGANv3 will be released along with its paper in the spring of 2021.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Focus:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table border=&#34;1px ridge&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;th&gt;Anime style&lt;/th&gt; &#xA;   &lt;th&gt;Film&lt;/th&gt; &#xA;   &lt;th&gt;Picture Number&lt;/th&gt; &#xA;   &lt;th&gt;Quality&lt;/th&gt; &#xA;   &lt;th&gt;Download Style Dataset&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Miyazaki Hayao&lt;/td&gt; &#xA;   &lt;td&gt;The Wind Rises&lt;/td&gt; &#xA;   &lt;td&gt;1752&lt;/td&gt; &#xA;   &lt;td&gt;1080p&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;&lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/releases/tag/1.0&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Makoto Shinkai&lt;/td&gt; &#xA;   &lt;td&gt;Your Name &amp;amp; Weathering with you&lt;/td&gt; &#xA;   &lt;td&gt;1445&lt;/td&gt; &#xA;   &lt;td&gt;BD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Kon Satoshi&lt;/td&gt; &#xA;   &lt;td&gt;Paprika&lt;/td&gt; &#xA;   &lt;td&gt;1284&lt;/td&gt; &#xA;   &lt;td&gt;BDRip&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;News:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;The improvement directions of AnimeGANv2 mainly include the following 4 points:  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;1. Solve the problem of high-frequency artifacts in the generated image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;2. It is easy to train and directly achieve the effects in the paper.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;3. Further reduce the number of parameters of the generator network. &lt;strong&gt;(generator size: 8.17 Mb)&lt;/strong&gt;, The lite version has a smaller generator model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;4. Use new high-quality style data, which come from BD movies as much as possible.&lt;/p&gt; &lt;p&gt;      AnimeGAN can be accessed from &lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGAN&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.6&lt;/li&gt; &#xA; &lt;li&gt;tensorflow-gpu 1.15.0 (GPU 2080Ti, cuda 10.0.130, cudnn 7.6.0)&lt;/li&gt; &#xA; &lt;li&gt;opencv&lt;/li&gt; &#xA; &lt;li&gt;tqdm&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;glob&lt;/li&gt; &#xA; &lt;li&gt;argparse&lt;/li&gt; &#xA; &lt;li&gt;onnxruntime (If onnx file needs to be run.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;1. Inference&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;python test.py --checkpoint_dir checkpoint/generator_Hayao_weight --test_dir dataset/test/HR_photo --save_dir Hayao/HR_photo&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Convert video to anime&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;python video2anime.py --video video/input/お花見.mp4 --checkpoint_dir checkpoint/generator_Hayao_weight --output video/output&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3. Train&lt;/h3&gt; &#xA;&lt;h4&gt;1. Download vgg19&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/vgg16%2F19.npy&#34;&gt;vgg19.npy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2. Download Train/Val Photo dataset&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/dataset-1&#34;&gt;Link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;3. Do edge_smooth&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;python edge_smooth.py --dataset Hayao --img_size 256&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;4. Train&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;python train.py --dataset Hayao --epoch 101 --init_epoch 10&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;5. Extract the weights of the generator&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;python get_generator_ckpt.py --checkpoint_dir ../checkpoint/AnimeGANv2_Shinkai_lsgan_300_300_1_2_10_1 --style_name Shinkai&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/AnimeGANv2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;span&gt;😍&lt;/span&gt; Photo to Paprika Style&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/37.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/38.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/6.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/7.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/9.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/21.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/44.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/1.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/8.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/11.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/5.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Paprika/concat/15.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;span&gt;😍&lt;/span&gt; Photo to Hayao Style&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/AE86.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/10.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/15.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/35.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/39.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/42.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/44.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/41.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/32.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/11.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/34.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Hayao/concat/18.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;span&gt;😍&lt;/span&gt; Photo to Shinkai Style&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/7.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/9.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/11.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/15.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/17.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/22.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/27.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/33.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/32.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/21.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/3.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/TachibanaYoshino/AnimeGANv2/raw/master/results/Shinkai/concat/26.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repo is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications. Permission is granted to use the AnimeGANv2 given that you agree to my license terms. Regarding the request for commercial use, please contact us via email to help you obtain the authorization letter.&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;Xin Chen&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AtsushiSakai/PythonRobotics</title>
    <updated>2022-10-14T01:38:11Z</updated>
    <id>tag:github.com,2022-10-14:/AtsushiSakai/PythonRobotics</id>
    <link href="https://github.com/AtsushiSakai/PythonRobotics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python sample codes for robotics algorithms.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&#34; align=&#34;right&#34; width=&#34;300&#34; alt=&#34;header pic&#34;&gt; &#xA;&lt;h1&gt;PythonRobotics&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg?sanitize=true&#34; alt=&#34;GitHub_Action_Linux_CI&#34;&gt; &lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg?sanitize=true&#34; alt=&#34;GitHub_Action_MacOS_CI&#34;&gt; &lt;a href=&#34;https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/AtsushiSakai/PythonRobotics&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/AtsushiSakai/PythonRobotics/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Language grade: Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics&#34;&gt;&lt;img src=&#34;https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics&#34; alt=&#34;tokei&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#what-is-this&#34;&gt;What is this?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#how-to-use&#34;&gt;How to use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#localization&#34;&gt;Localization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#extended-kalman-filter-localization&#34;&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#particle-filter-localization&#34;&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#histogram-filter-localization&#34;&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#mapping&#34;&gt;Mapping&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#gaussian-grid-map&#34;&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#ray-casting-grid-map&#34;&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lidar-to-grid-map&#34;&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#k-means-object-clustering&#34;&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rectangle-fitting&#34;&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#slam&#34;&gt;SLAM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#iterative-closest-point-icp-matching&#34;&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#fastslam-10&#34;&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-planning&#34;&gt;Path Planning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dynamic-window-approach&#34;&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-search&#34;&gt;Grid based search&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dijkstra-algorithm&#34;&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#a-algorithm&#34;&gt;A* algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-algorithm&#34;&gt;D* algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-lite-algorithm&#34;&gt;D* Lite algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#potential-field-algorithm&#34;&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-coverage-path-planning&#34;&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#state-lattice-planning&#34;&gt;State Lattice Planning&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#biased-polar-sampling&#34;&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lane-sampling&#34;&gt;Lane sampling&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#probabilistic-road-map-prm-planning&#34;&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rapidly-exploring-random-trees-rrt&#34;&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt&#34;&gt;RRT*&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt-with-reeds-shepp-path&#34;&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-rrt&#34;&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#quintic-polynomials-planning&#34;&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#reeds-shepp-planning&#34;&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-based-path-planning&#34;&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#optimal-trajectory-in-a-frenet-frame&#34;&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-tracking&#34;&gt;Path Tracking&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#move-to-a-pose-control&#34;&gt;move to a pose control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#stanley-control&#34;&gt;Stanley control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rear-wheel-feedback-control&#34;&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#linearquadratic-regulator-lqr-speed-and-steering-control&#34;&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#model-predictive-speed-and-steering-control&#34;&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#nonlinear-model-predictive-control-with-c-gmres&#34;&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation&#34;&gt;Arm Navigation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#n-joint-arm-to-point-control&#34;&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation-with-obstacle-avoidance&#34;&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#aerial-navigation&#34;&gt;Aerial Navigation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#drone-3d-trajectory-following&#34;&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rocket-powered-landing&#34;&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal&#34;&gt;Bipedal&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal-planner-with-inverted-pendulum&#34;&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#use-case&#34;&gt;Use-case&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#citing&#34;&gt;Citing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#Sponsors&#34;&gt;Sponsors&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#JetBrains&#34;&gt;JetBrains&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What is this?&lt;/h1&gt; &#xA;&lt;p&gt;This is a Python code collection of robotics algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Easy to read for understanding each algorithm&#39;s basic idea.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Minimum dependency.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See this paper for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1808.10703&#34;&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRoboticsPaper/raw/master/python_robotics.bib&#34;&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;For running each sample code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python 3.10.x&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://numpy.org/&#34;&gt;NumPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scipy.org/&#34;&gt;SciPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cvxpy.org/&#34;&gt;cvxpy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;pytest (for unit tests)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;pytest-xdist (for parallel unit tests)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;mypy (for type check)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;sphinx (for document generation)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;pycodestyle (for code style check)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt; &#xA;&lt;p&gt;You can check the full documentation online: &lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/index.html&#34;&gt;Welcome to PythonRobotics’s documentation! — PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All animation gifs are stored here: &lt;a href=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs&#34;&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;git clone &lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics.git&#34;&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the required libraries.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;using conda :&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;conda env create -f requirements/environment.yml&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;using pip :&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;pip install -r requirements/requirements.txt&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute python script in each directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add star to this repo if you like it &lt;span&gt;😃&lt;/span&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Localization&lt;/h1&gt; &#xA;&lt;h2&gt;Extended Kalman Filter localization&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&#34; width=&#34;640&#34; alt=&#34;EKF pic&#34;&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Particle filter localization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt; &#xA;&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt; &#xA;&lt;p&gt;and the red line is an estimated trajectory with PF.&lt;/p&gt; &#xA;&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt; &#xA;&lt;p&gt;These measurements are used for PF localization.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Histogram filter localization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt; &#xA;&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt; &#xA;&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt; &#xA;&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt; &#xA;&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt; &#xA;&lt;p&gt;Initial position is not needed.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Mapping&lt;/h1&gt; &#xA;&lt;h2&gt;Gaussian grid map&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Ray casting grid map&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Lidar to grid map&lt;/h2&gt; &#xA;&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;k-means object clustering&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Rectangle fitting&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SLAM&lt;/h1&gt; &#xA;&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt; &#xA;&lt;h2&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt; &#xA;&lt;p&gt;It can calculate a rotation matrix, and a translation vector between points and points.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf&#34;&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FastSLAM 1.0&lt;/h2&gt; &#xA;&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt; &#xA;&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm&#34;&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Path Planning&lt;/h1&gt; &#xA;&lt;h2&gt;Dynamic Window Approach&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf&#34;&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Grid based search&lt;/h2&gt; &#xA;&lt;h3&gt;Dijkstra algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with Dijkstra&#39;s algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; &#xA;&lt;h3&gt;A* algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with A star algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; &#xA;&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt; &#xA;&lt;h3&gt;D* algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with D star algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif&#34; alt=&#34;figure at master · nirnayroy/intelligentrobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/D*&#34;&gt;D* Algorithm Wikipedia&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;D* Lite algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif&#34; alt=&#34;D* Lite&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Refs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://idm-lab.org/bib/abstracts/papers/aaai02b.pd&#34;&gt;D* Lite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf&#34;&gt;Improved Fast Replanning for Robot Navigation in Unknown Terrain&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Potential Field algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif&#34; alt=&#34;PotentialField&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf&#34;&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Grid based coverage path planning&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif&#34; alt=&#34;PotentialField&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;State Lattice Planning&lt;/h2&gt; &#xA;&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt; &#xA;&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328&#34;&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf&#34;&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Biased polar sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Lane sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif&#34; alt=&#34;PRM&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt; &#xA;&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt; &#xA;&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt; &#xA;&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_roadmap&#34;&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;　　&lt;/p&gt; &#xA;&lt;h2&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt; &#xA;&lt;h3&gt;RRT*&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt; &#xA;&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1005.0416&#34;&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RRT* with reeds-shepp path&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif&#34; alt=&#34;Robotics/animation.gif at master · AtsushiSakai/PythonRobotics&#34;&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt; &#xA;&lt;h3&gt;LQR-RRT*&lt;/h3&gt; &#xA;&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt; &#xA;&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif&#34; alt=&#34;LQR_RRT&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://lis.csail.mit.edu/pubs/perez-icra12.pdf&#34;&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/MahanFathi/LQR-RRTstar&#34;&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quintic polynomials planning&lt;/h2&gt; &#xA;&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/637936/&#34;&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reeds Shepp planning&lt;/h2&gt; &#xA;&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true&#34; alt=&#34;RSPlanning&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://planning.cs.uiuc.edu/node822.html&#34;&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf&#34;&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ghliu/pyReedsShepp&#34;&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LQR based path planning&lt;/h2&gt; &#xA;&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true&#34; alt=&#34;RSPlanning&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt; &#xA;&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt; &#xA;&lt;p&gt;The red line is the predicted path.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf&#34;&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Cj6tAQe7UCY&#34;&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Path Tracking&lt;/h1&gt; &#xA;&lt;h2&gt;move to a pose control&lt;/h2&gt; &#xA;&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-642-20144-8&#34;&gt;P. I. Corke, &#34;Robotics, Vision and Control&#34; | SpringerLink p102&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stanley control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://robots.stanford.edu/papers/thrun.stanley05.pdf&#34;&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf&#34;&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Rear wheel feedback control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1604.07446&#34;&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/5940562/&#34;&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model predictive speed and steering control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif&#34; width=&#34;640&#34; alt=&#34;MPC pic&#34;&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb&#34;&gt;notebook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://grauonline.de/wordpress/?page_id=3244&#34;&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt; &#xA;&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb&#34;&gt;notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Arm Navigation&lt;/h1&gt; &#xA;&lt;h2&gt;N joint arm to point control&lt;/h2&gt; &#xA;&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt; &#xA;&lt;p&gt;This is an interactive simulation.&lt;/p&gt; &#xA;&lt;p&gt;You can set the goal position of the end effector with left-click on the plotting area.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt; &#xA;&lt;h2&gt;Arm navigation with obstacle avoidance&lt;/h2&gt; &#xA;&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Aerial Navigation&lt;/h1&gt; &#xA;&lt;h2&gt;drone 3d trajectory following&lt;/h2&gt; &#xA;&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;rocket powered landing&lt;/h2&gt; &#xA;&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb&#34;&gt;notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Bipedal&lt;/h1&gt; &#xA;&lt;h2&gt;bipedal planner with inverted pendulum&lt;/h2&gt; &#xA;&lt;p&gt;This is a bipedal planner for modifying footsteps for an inverted pendulum.&lt;/p&gt; &#xA;&lt;p&gt;You can set the footsteps, and the planner will modify those automatically.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h1&gt;Use-case&lt;/h1&gt; &#xA;&lt;p&gt;If this project helps your robotics project, please let me know with creating an issue.&lt;/p&gt; &#xA;&lt;p&gt;Your robot&#39;s video, which is using PythonRobotics, is very welcome!!&lt;/p&gt; &#xA;&lt;p&gt;This is a list of user&#39;s comment and references:&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/users_comments.md&#34;&gt;users_comments&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Any contribution is welcome!!&lt;/p&gt; &#xA;&lt;p&gt;Please check this document:&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/how_to_contribute.html&#34;&gt;How To Contribute — PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citing&lt;/h1&gt; &#xA;&lt;p&gt;If you use this project&#39;s code for your academic work, we encourage you to cite &lt;a href=&#34;https://arxiv.org/abs/1808.10703&#34;&gt;our papers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this project&#39;s code in industry, we&#39;d love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt; &#xA;&lt;h1&gt;Supporting this project&lt;/h1&gt; &#xA;&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/AtsushiSakai&#34;&gt;Sponsor @AtsushiSakai on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.patreon.com/myenigma&#34;&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.paypal.me/myenigmapay/&#34;&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would like to support us in some other way, please contact with creating an issue.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;They are providing a free license of their IDEs for this OSS development.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/1Password/1password-teams-open-source&#34;&gt;1Password&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;They are providing a free license of their 1Password team license for this OSS project.&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/graphs/contributors&#34;&gt;Contributors to AtsushiSakai/PythonRobotics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>