<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-22T01:43:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fkunn1326/openpose-editor</title>
    <updated>2023-02-22T01:43:26Z</updated>
    <id>tag:github.com,2023-02-22:/fkunn1326/openpose-editor</id>
    <link href="https://github.com/fkunn1326/openpose-editor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Openpose Editor for AUTOMATIC1111&#39;s stable-diffusion-webui&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Openpose Editor&lt;/h2&gt; &#xA;&lt;p&gt;日本語 | &lt;a href=&#34;https://raw.githubusercontent.com/fkunn1326/openpose-editor/master/README.en.md&#34;&gt;English&lt;/a&gt;|&lt;a href=&#34;https://raw.githubusercontent.com/fkunn1326/openpose-editor/master/README.zh-cn.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/92153597/219921945-468b2e4f-a3a0-4d44-a923-13ceb0258ddc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatic1111/stable-diffusion-webui用のOpenpose Editor&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ポーズの編集&lt;/li&gt; &#xA; &lt;li&gt;ポーズの検出&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ができます&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;「Add」: 人を追加する&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;「Detect from image」: 画像からポーズを検出する&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;「Add Background image」: 背景を追加する&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;「Save PNG」: PNGで保存する&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;「Send to ControlNet」: Controlnet拡張機能がインストールされている場合、画像をそこに送る&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;インストール方法&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&#34;Extension&#34; タブを開く&lt;/li&gt; &#xA; &lt;li&gt;&#34;Install from URL&#34; タブを開く&lt;/li&gt; &#xA; &lt;li&gt;&#34;URL for extension&#39;s git repository&#34; 欄にこのリポジトリの URL (&lt;a href=&#34;https://github.com/fkunn1326/openpose-editor.git&#34;&gt;https://github.com/fkunn1326/openpose-editor.git&lt;/a&gt;) を入れます。&lt;/li&gt; &#xA; &lt;li&gt;&#34;Install&#34; ボタンを押す&lt;/li&gt; &#xA; &lt;li&gt;WebUIを再起動する&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;p&gt;ConrtolNetの &#34;Preprocessor&#34; には、何も指定しないようにしてください。&lt;/p&gt; &#xA;&lt;h2&gt;エラーの対策&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;urllib.error.URLError: &amp;lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)&amp;gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;以下のファイルを開いてださい&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/Applications/Python\ $version /Install\ Certificates.command&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>danijar/dreamerv3</title>
    <updated>2023-02-22T01:43:26Z</updated>
    <id>tag:github.com,2023-02-22:/danijar/dreamerv3</id>
    <link href="https://github.com/danijar/dreamerv3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mastering Diverse Domains through World Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mastering Diverse Domains through World Models&lt;/h1&gt; &#xA;&lt;p&gt;A reimplementation of &lt;a href=&#34;https://arxiv.org/pdf/2301.04104v1.pdf&#34;&gt;DreamerV3&lt;/a&gt;, a scalable and general reinforcement learning algorithm that masters a wide range of applications with fixed hyperparameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/2111293/217647148-cbc522e2-61ad-4553-8e14-1ecdc8d9438b.gif&#34; alt=&#34;DreamerV3 Tasks&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find this code useful, please reference in your paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{hafner2023dreamerv3,&#xA;  title={Mastering Diverse Domains through World Models},&#xA;  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},&#xA;  journal={arXiv preprint arXiv:2301.04104},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2301.04104v1.pdf&#34;&gt;Research paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://danijar.com/dreamerv3&#34;&gt;Project website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/danijarh/status/1613161946223677441&#34;&gt;Twitter summary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;DreamerV3&lt;/h2&gt; &#xA;&lt;p&gt;DreamerV3 learns a world model from experiences and uses it to train an actor critic policy from imagined trajectories. The world model encodes sensory inputs into categorical representations and predicts future representations and rewards given actions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/2111293/217355673-4abc0ce5-1a4b-4366-a08d-64754289d659.png&#34; alt=&#34;DreamerV3 Method Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DreamerV3 masters a wide range of domains with a fixed set of hyperparameters, outperforming specialized methods. Removing the need for tuning reduces the amount of expert knowledge and computational resources needed to apply reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/2111293/217356042-536a693a-cb5e-42aa-a20f-5303a77cad9c.png&#34; alt=&#34;DreamerV3 Benchmark Scores&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Due to its robustness, DreamerV3 shows favorable scaling properties. Notably, using larger models consistently increases not only its final performance but also its data-efficiency. Increasing the number of gradient steps further increases data efficiency.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/2111293/217356063-0cf06b17-89f0-4d5f-85a9-b583438c98dd.png&#34; alt=&#34;DreamerV3 Scaling Behavior&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;h2&gt;Package&lt;/h2&gt; &#xA;&lt;p&gt;If you just want to run DreamerV3 on a custom environment, you can &lt;code&gt;pip install dreamerv3&lt;/code&gt; and copy &lt;a href=&#34;https://github.com/danijar/dreamerv3/raw/main/example.py&#34;&gt;&lt;code&gt;example.py&lt;/code&gt;&lt;/a&gt; from this repository as a starting point.&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;If you want to make modifications to the code, you can either use the provided &lt;code&gt;Dockerfile&lt;/code&gt; that contains instructions or follow the manual instructions below.&lt;/p&gt; &#xA;&lt;h2&gt;Manual&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda&#34;&gt;JAX&lt;/a&gt; and then the other dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Simple training script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python example.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Flexible training script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python dreamerv3/train.py \&#xA;  --logdir ~/logdir/$(date &#34;+%Y%m%d-%H%M%S&#34;) \&#xA;  --configs crafter --batch_size 16 --run.train_ratio 32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Tips&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All config options are listed in &lt;code&gt;configs.yaml&lt;/code&gt; and you can override them from the command line.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;debug&lt;/code&gt; config block reduces the network size, batch size, duration between logs, and so on for fast debugging (but does not learn a good model).&lt;/li&gt; &#xA; &lt;li&gt;By default, the code tries to run on GPU. You can switch to CPU or TPU using the &lt;code&gt;--jax.platform cpu&lt;/code&gt; flag. Note that multi-GPU support is untested.&lt;/li&gt; &#xA; &lt;li&gt;You can run with multiple config blocks that will override defaults in the order they are specified, for example &lt;code&gt;--configs crafter large&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;By default, metrics are printed to the terminal, appended to a JSON lines file, and written as TensorBoard summaries. Other outputs like WandB can be enabled in the training script.&lt;/li&gt; &#xA; &lt;li&gt;If you get a &lt;code&gt;Too many leaves for PyTreeDef&lt;/code&gt; error, it means you&#39;re reloading a checkpoint that is not compatible with the current config. This often happens when reusing an old logdir by accident.&lt;/li&gt; &#xA; &lt;li&gt;If you are getting CUDA errors, scroll up because the cause is often just an error that happened earlier, such as out of memory or incompatible JAX and CUDA versions.&lt;/li&gt; &#xA; &lt;li&gt;You can use the &lt;code&gt;small&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;large&lt;/code&gt; config blocks to reduce memory requirements. The default is &lt;code&gt;xlarge&lt;/code&gt;. See the scaling graph above to see how this affects performance.&lt;/li&gt; &#xA; &lt;li&gt;Many environments are included, some of which require installating additional packages. See the installation scripts in &lt;code&gt;scripts&lt;/code&gt; and the &lt;code&gt;Dockerfile&lt;/code&gt; for reference.&lt;/li&gt; &#xA; &lt;li&gt;When running on custom environments, make sure to specify the observation keys the agent should be using via &lt;code&gt;encoder.mlp_keys&lt;/code&gt;, &lt;code&gt;encode.cnn_keys&lt;/code&gt;, &lt;code&gt;decoder.mlp_keys&lt;/code&gt; and &lt;code&gt;decoder.cnn_keys&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To log metrics from environments without showing them to the agent or storing them in the replay buffer, return them as observation keys with &lt;code&gt;log_&lt;/code&gt; prefix and enable logging via the &lt;code&gt;run.log_keys_...&lt;/code&gt; options.&lt;/li&gt; &#xA; &lt;li&gt;To continue stopped training runs, simply run the same command line again and make sure that the &lt;code&gt;--logdir&lt;/code&gt; points to the same directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains a reimplementation of DreamerV3 based on the open source DreamerV2 code base. It is unrelated to Google or DeepMind. The implementation has been tested to reproduce the official results on a range of environments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dunbar12138/pix2pix3D</title>
    <updated>2023-02-22T01:43:26Z</updated>
    <id>tag:github.com,2023-02-22:/dunbar12138/pix2pix3D</id>
    <link href="https://github.com/dunbar12138/pix2pix3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;pix2pix3D: Generating 3D Objects from 2D User Inputs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;3D-aware Conditional Image Synthesis (pix2pix3D)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~pix2pix3D/&#34;&gt;&lt;strong&gt;Project&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2302.08509&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official PyTorch implementation of &#34;3D-aware Conditional Image Synthesis&#34;. Pix2pix3D synthesizes 3D objects (neural fields) given a 2D label map, such as a segmentation or edge map. We also provide an interactive 3D editing demo.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/28395429/219509580-650c19d1-0c63-4811-b492-e106ca3a9478.mp4&#34;&gt;https://user-images.githubusercontent.com/28395429/219509580-650c19d1-0c63-4811-b492-e106ca3a9478.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.08509&#34;&gt;3D-aware Conditional Image Synthesis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ArXiv, 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dunbar12138.github.io/&#34;&gt;Kangle Deng&lt;/a&gt;, &lt;a href=&#34;https://gengshan-y.github.io/&#34;&gt;Gengshan Yang&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~deva/&#34;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Carnegie Mellon University&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model learns to synthesize a corresponding image from different viewpoints. To enable explicit 3D user control, we extend conditional generative models with neural radiance fields. Given widely-available monocular images and label map pairs, our model learns to assign a label to every 3D point in addition to color and density, which enables it to render the image and pixel-aligned label map simultaneously. Finally, we build an interactive system that allows users to edit the label map from any viewpoint and generate outputs accordingly.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/teaser_jpg.jpg&#34; width=&#34;720&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;We provide a conda env file that contains all the other dependencies. You can use the following commands with Miniconda3 to create and activate your Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate pix2pix3d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;We provide our preprocessed datasets, including segmentation maps and edge maps. You can download the &lt;a href=&#34;https://drive.google.com/drive/folders/1mC6i4YmdpazJSmXrW8WFSfsImAJ8a_CF?usp=sharing&#34;&gt;CelebAMask&lt;/a&gt; dataset, &lt;a href=&#34;https://drive.google.com/drive/folders/1yjTTE57P9-hEe-IVcE-GXdh04WGo5lD9?usp=sharing&#34;&gt;AFHQ-Cat-Seg&lt;/a&gt; dataset, and &lt;a href=&#34;https://drive.google.com/drive/folders/1XTPuu784DIvk0ie094qyLrcF-v-jMe3_?usp=sharing&#34;&gt;Shapenet-Car-Edge&lt;/a&gt; dataset and put those zip files under &lt;code&gt;data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained Models&lt;/h3&gt; &#xA;&lt;p&gt;You can download our pre-trained models using the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash checkpoints/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;We provide several scripts to generate the results once you download the pre-trained models.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/teaser_gif.gif&#34; width=&#34;720&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Generate Samples&lt;/h4&gt; &#xA;&lt;p&gt;You can generate results based on the samples in the dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python applications/generate_samples.py --network &amp;lt;network_pkl&amp;gt; --outdir &amp;lt;output_dir&amp;gt; --random_seed &amp;lt;random_seeds list, e.g. 0 1&amp;gt; --cfg &amp;lt;configs, e.g., seg2cat, seg2face, edge2car&amp;gt; --input_id &amp;lt;sample_id in dataset&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Input Label Map&lt;/th&gt; &#xA;   &lt;th&gt;Generated Image&lt;/th&gt; &#xA;   &lt;th&gt;Generated Label Map&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1666_input.png&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1666_1_color.png&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1666_1_label.png&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can get the results above by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python applications/generate_samples.py --network checkpoints/pix2pix3d_seg2cat.pkl --outdir examples --random_seed 1 --cfg seg2cat --input_id 1666&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Render Videos&lt;/h4&gt; &#xA;&lt;p&gt;You can render a video result based on a specified input label map.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python applications/generate_video.py --network &amp;lt;network_pkl&amp;gt; --outdir &amp;lt;output_dir&amp;gt; --random_seed &amp;lt;random_seeds list, e.g. 0 1&amp;gt; --cfg &amp;lt;configs, e.g., seg2cat, seg2face, edge2car&amp;gt; --input &amp;lt;input label map&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Input Label Map&lt;/th&gt; &#xA;   &lt;th&gt;Generated Image&lt;/th&gt; &#xA;   &lt;th&gt;Generated Label Map&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1666_input.png&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1.gif&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1_label.gif&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can get the results above using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python applications/generate_video.py --network checkpoints/pix2pix3d_seg2cat.pkl --outdir examples --random_seed 1 --cfg seg2cat --input examples/example_input.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extract Semantic Mesh&lt;/h4&gt; &#xA;&lt;p&gt;You can also extract the mesh and color it using 3D semantic labels. Some extra packages (&lt;code&gt;pyrender&lt;/code&gt;, &lt;code&gt;trimesh&lt;/code&gt;, and &lt;code&gt;mcubes&lt;/code&gt;) are required for mesh extraction. You can install them by &lt;code&gt;pip&lt;/code&gt;. The extracted mesh will be saved as &lt;code&gt;semantic_mesh.ply&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Input Label Map&lt;/th&gt; &#xA;   &lt;th&gt;Semantic Mesh&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/seg2cat_1666_input.png&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dunbar12138/pix2pix3D/main/assets/rendered_mesh_colored.gif&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can get the results above with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python applications/extract_mesh.py --network checkpoints/pix2pix3d_seg2cat.pkl --outdir examples --cfg seg2cat --input examples/example_input.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- #### Interpolation --&gt; &#xA;&lt;h4&gt;Interactive Demo (Code coming soon)&lt;/h4&gt; &#xA;&lt;p&gt;You can also launch an interactive demo of 3D editing.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Code is coming soon.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful for your research, please cite the following work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{kangle2023pix2pix3d,&#xA;  title={3D-aware Conditional Image Synthesis},&#xA;  author={Deng, Kangle and Yang, Gengshan and Ramanan, Deva and Zhu, Jun-Yan},&#xA;  journal = {arXiv},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We thank Sheng-Yu Wang, Nupur Kumari, Gaurav Parmer, Ruihan Gao, Muyang Li, George Cazenavette, Andrew Song, Zhipeng Bao, Tamaki Kojima, Krishna Wadhwani, Takuya Narihira, and Tatsuo Fujiwara for their discussion and help. We are grateful for the support from Sony Corporation, Singapore DSTA, and the CMU Argo AI Center for Autonomous Vehicle Research. This codebase borrows heavily from &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;EG3D&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/StyleNeRF&#34;&gt;StyleNeRF&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>