<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-15T01:36:41Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>instructlab/instructlab</title>
    <updated>2024-09-15T01:36:41Z</updated>
    <id>tag:github.com,2024-09-15:/instructlab/instructlab</id>
    <link href="https://github.com/instructlab/instructlab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InstructLab Command-Line Interface. Use this to chat with a model and execute the InstructLab workflow to train a model using custom taxonomy data.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InstructLab 🐶 (&lt;code&gt;ilab&lt;/code&gt;)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/instructlab/instructlab/actions/workflows/lint.yml/badge.svg?branch=main&#34; alt=&#34;Lint&#34;&gt; &lt;img src=&#34;https://github.com/instructlab/instructlab/actions/workflows/test.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt; &lt;img src=&#34;https://github.com/instructlab/instructlab/actions/workflows/pypi.yaml/badge.svg?branch=main&#34; alt=&#34;Build&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/instructlab/instructlab&#34; alt=&#34;Release&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/instructlab/instructlab&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📖 Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#welcome-to-the-instructlab-cli&#34;&gt;Welcome to the InstructLab CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-what-is-ilab&#34;&gt;❓ What is &lt;code&gt;ilab&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-requirements&#34;&gt;📋 Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-getting-started&#34;&gt;✅ Getting started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-installing-ilab&#34;&gt;🧰 Installing &lt;code&gt;ilab&lt;/code&gt;&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#install-using-pytorch-without-cuda-bindings-and-no-gpu-acceleration&#34;&gt;Install with no GPU acceleration and PyTorch without CUDA bindings&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#install-with-amd-rocm&#34;&gt;Install with AMD ROCm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#install-with-apple-metal-on-m1m2m3-macs&#34;&gt;Install with Apple Metal on M1/M2/M3 Macs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#install-with-nvidia-cuda&#34;&gt;Install with Nvidia CUDA&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#%EF%B8%8F-initialize-ilab&#34;&gt;🏗️ Initialize &lt;code&gt;ilab&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-download-the-model&#34;&gt;📥 Download the model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-serving-the-model&#34;&gt;🍴 Serving the model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-chat-with-the-model-optional&#34;&gt;📣 Chat with the model (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-creating-new-knowledge-or-skills-and-training-the-model&#34;&gt;💻 Creating new knowledge or skills and training the model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-contribute-knowledge-or-compositional-skills&#34;&gt;🎁 Contribute knowledge or compositional skills&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-list-and-validate-your-new-data&#34;&gt;📜 List and validate your new data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-generate-a-synthetic-dataset&#34;&gt;🚀 Generate a synthetic dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-training-the-model&#34;&gt;👩‍🏫 Training the model&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#train-the-model-locally-on-linux&#34;&gt;Train the model locally on Linux&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#train-the-model-locally-on-an-m-series-mac&#34;&gt;Train the model locally on M-series Macs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#train-the-model-locally-with-gpu-acceleration&#34;&gt;Train the model locally with GPU acceleration&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#train-the-model-in-the-cloud&#34;&gt;Train the model in the cloud&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-test-the-newly-trained-model&#34;&gt;📜 Test the newly trained model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-evaluate-the-newly-trained-model&#34;&gt;🧪 Evaluate the newly trained model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-serve-the-newly-trained-model&#34;&gt;🍴 Serve the newly trained model&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-chat-with-the-new-model-not-optional-this-time&#34;&gt;📣 Chat with the new model (not optional this time)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-upgrade-instructlab-to-latest-version&#34;&gt;🚀 Upgrade InstructLab to latest version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-submit-your-new-knowledge-or-skills&#34;&gt;🎁 Submit your new knowledge or skills&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/#-contributing&#34;&gt;📬 Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Welcome to the InstructLab CLI&lt;/h2&gt; &#xA;&lt;p&gt;InstructLab 🐶 uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The &#34;&lt;strong&gt;lab&lt;/strong&gt;&#34; in Instruct&lt;strong&gt;Lab&lt;/strong&gt; 🐶 stands for &lt;a href=&#34;https://arxiv.org/abs/2403.01081&#34;&gt;&lt;strong&gt;L&lt;/strong&gt;arge-Scale &lt;strong&gt;A&lt;/strong&gt;lignment for Chat&lt;strong&gt;B&lt;/strong&gt;ots&lt;/a&gt; [1].&lt;/p&gt; &#xA;&lt;p&gt;[1] Shivchander Sudalairaj*, Abhishek Bhandwaldar*, Aldo Pareja*, Kai Xu, David D. Cox, Akash Srivastava*. &#34;LAB: Large-Scale Alignment for ChatBots&#34;, arXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)&lt;/p&gt; &#xA;&lt;h2&gt;❓ What is &lt;code&gt;ilab&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ilab&lt;/code&gt; is a Command-Line Interface (CLI) tool that allows you to perform the following actions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download a pre-trained Large Language Model (LLM).&lt;/li&gt; &#xA; &lt;li&gt;Chat with the LLM.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To add new knowledge and skills to the pre-trained LLM, add information to the companion &lt;a href=&#34;https://github.com/instructlab/taxonomy.git&#34;&gt;taxonomy&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;p&gt;After you have added knowledge and skills to the taxonomy, you can perform the following actions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use &lt;code&gt;ilab&lt;/code&gt; to generate new synthetic training data based on the changes in your local &lt;code&gt;taxonomy&lt;/code&gt; repository.&lt;/li&gt; &#xA; &lt;li&gt;Re-train the LLM with the new training data.&lt;/li&gt; &#xA; &lt;li&gt;Chat with the re-trained LLM to see the results.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;&#xA;  download--&amp;gt;chat&#xA;  chat[Chat with the LLM]--&amp;gt;add&#xA;  add[Add new knowledge\nor skill to taxonomy]--&amp;gt;generate[generate new\nsynthetic training data]&#xA;  generate--&amp;gt;train&#xA;  train[Re-train]--&amp;gt;|Chat with\nthe re-trained LLM\nto see the results|chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For an overview of the full workflow, see the &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/docs/workflow.png&#34;&gt;workflow diagram&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] We have optimized InstructLab so that community members with commodity hardware can perform these steps. However, running InstructLab on a laptop will provide a low-fidelity approximation of synthetic data generation (using the &lt;code&gt;ilab data generate&lt;/code&gt; command) and model instruction tuning (using the &lt;code&gt;ilab model train&lt;/code&gt; command, which uses QLoRA). To achieve higher quality, use more sophisticated hardware and configure InstructLab to use a larger teacher model &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/mixtral&#34;&gt;such as Mixtral&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;📋 Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;🍎 Apple M1/M2/M3 Mac or 🐧 Linux system&lt;/strong&gt; (tested on Fedora). We anticipate support for more operating systems in the future.&lt;/li&gt; &#xA; &lt;li&gt;C++ compiler&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10 or Python 3.11&lt;/li&gt; &#xA; &lt;li&gt;Approximately 60GB disk space (entire process)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Python 3.12 is currently not supported, because some dependencies don&#39;t work on Python 3.12, yet.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; When installing the &lt;code&gt;ilab&lt;/code&gt; CLI on macOS, you may have to run the &lt;code&gt;xcode-select --install&lt;/code&gt; command, installing the required packages previously listed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;✅ Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;🧰 Installing &lt;code&gt;ilab&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;When installing on Fedora Linux, install C++, Python 3.10 or 3.11, and other necessary tools by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are running on macOS, this installation is not necessary and you can begin your process with the following step.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a new directory called &lt;code&gt;instructlab&lt;/code&gt; to store the files the &lt;code&gt;ilab&lt;/code&gt; CLI needs when running and &lt;code&gt;cd&lt;/code&gt; into the directory by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir instructlab&#xA;cd instructlab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The following steps in this document use &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;Python venv&lt;/a&gt; for virtual environments. However, if you use another tool such as &lt;a href=&#34;https://github.com/pyenv/pyenv&#34;&gt;pyenv&lt;/a&gt; or &lt;a href=&#34;https://github.com/conda-forge/miniforge&#34;&gt;Conda Miniforge&lt;/a&gt; for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in &lt;code&gt;venv&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;There are a few ways you can locally install the &lt;code&gt;ilab&lt;/code&gt; CLI. Select your preferred installation method from the following instructions. You can then install &lt;code&gt;ilab&lt;/code&gt; and activate your &lt;code&gt;venv&lt;/code&gt; environment.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: ⏳ The &lt;code&gt;python3&lt;/code&gt; binary shown in the following steps is the Python version that you installed in the above step. The command can also be &lt;code&gt;python3.11&lt;/code&gt; or &lt;code&gt;python3.10&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt;. You can check Python&#39;s version by &lt;code&gt;python3 -V&lt;/code&gt;.&lt;/p&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: ⏳ &lt;code&gt;pip install&lt;/code&gt; may take some time, depending on your internet connection. In case installation fails with error &lt;code&gt;unsupported instruction `vpdpbusd&#39;&lt;/code&gt;, append &lt;code&gt;-C cmake.args=&#34;-DLLAMA_NATIVE=off&#34;&lt;/code&gt; to &lt;code&gt;pip install&lt;/code&gt; command.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/docs/gpu-acceleration.md&#34;&gt;the GPU acceleration documentation&lt;/a&gt; for how to to enable hardware acceleration for interaction and training on AMD ROCm, Apple Metal Performance Shaders (MPS), and Nvidia CUDA.&lt;/p&gt; &lt;h4&gt;Install using PyTorch without CUDA bindings and no GPU acceleration&lt;/h4&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv --upgrade-deps venv&#xA;source venv/bin/activate&#xA;pip install instructlab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;em&gt;Additional Build Argument for Intel Macs&lt;/em&gt;&lt;/p&gt; &#xA;   &lt;p&gt;If you have an Mac with an Intel CPU, you must add a prefix of &lt;code&gt;CMAKE_ARGS=&#34;-DLLAMA_METAL=off&#34;&lt;/code&gt; to the &lt;code&gt;pip install&lt;/code&gt; command to ensure that the build is done without Apple M-series GPU support.&lt;/p&gt; &#xA;   &lt;p&gt;&lt;code&gt;(venv) $ CMAKE_ARGS=&#34;-DLLAMA_METAL=off&#34; pip install ...&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;h4&gt;Install with AMD ROCm&lt;/h4&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv --upgrade-deps venv&#xA;source venv/bin/activate&#xA;pip cache remove llama_cpp_python&#xA;pip install &#39;instructlab[rocm]&#39; \&#xA;   --extra-index-url https://download.pytorch.org/whl/rocm6.0 \&#xA;   -C cmake.args=&#34;-DLLAMA_HIPBLAS=on&#34; \&#xA;   -C cmake.args=&#34;-DAMDGPU_TARGETS=all&#34; \&#xA;   -C cmake.args=&#34;-DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang&#34; \&#xA;   -C cmake.args=&#34;-DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++&#34; \&#xA;   -C cmake.args=&#34;-DCMAKE_PREFIX_PATH=/opt/rocm&#34; \&#xA;   -C cmake.args=&#34;-DLLAMA_NATIVE=off&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On Fedora 40+, use &lt;code&gt;-DCMAKE_C_COMPILER=clang-17&lt;/code&gt; and &lt;code&gt;-DCMAKE_CXX_COMPILER=clang++-17&lt;/code&gt;.&lt;/p&gt; &lt;h4&gt;Install with Apple Metal on M1/M2/M3 Macs&lt;/h4&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Make sure your system Python build is &lt;code&gt;Mach-O 64-bit executable arm64&lt;/code&gt; by using &lt;code&gt;file -b $(command -v python)&lt;/code&gt;, or if your system is setup with &lt;a href=&#34;https://github.com/pyenv/pyenv&#34;&gt;pyenv&lt;/a&gt; by using the &lt;code&gt;file -b $(pyenv which python)&lt;/code&gt; command.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv --upgrade-deps venv&#xA;source venv/bin/activate&#xA;pip cache remove llama_cpp_python&#xA;pip install &#39;instructlab[mps]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Install with Nvidia CUDA&lt;/h4&gt; &lt;p&gt;For the best CUDA experience, installing vLLM is necessary to serve Safetensors format models.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv --upgrade-deps venv&#xA;source venv/bin/activate&#xA;pip cache remove llama_cpp_python&#xA;pip install &#39;instructlab[cuda]&#39; \&#xA;   -C cmake.args=&#34;-DLLAMA_CUDA=on&#34; \&#xA;   -C cmake.args=&#34;-DLLAMA_NATIVE=off&#34;&#xA;pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;From your &lt;code&gt;venv&lt;/code&gt; environment, verify &lt;code&gt;ilab&lt;/code&gt; is installed correctly, by running the &lt;code&gt;ilab&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Example output of the &lt;code&gt;ilab&lt;/code&gt; command&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab&#xA;Usage: ilab [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;CLI for interacting with InstructLab.&#xA;&#xA;If this is your first time running InstructLab, it&#39;s best to start with `ilab config init` to create the environment.&#xA;&#xA;Options:&#xA;--config PATH  Path to a configuration file.  [default:&#xA;               /home/user/.config/instructlab/config.yaml]&#xA;-v, --verbose  Enable debug logging (repeat for even more verbosity)&#xA;--version      Show the version and exit.&#xA;--help         Show this message and exit.&#xA;&#xA;Commands:&#xA;config    Command Group for Interacting with the Config of InstructLab.&#xA;data      Command Group for Interacting with the Data generated by...&#xA;model     Command Group for Interacting with the Models in InstructLab.&#xA;system    Command group for all system-related command calls&#xA;taxonomy  Command Group for Interacting with the Taxonomy of InstructLab.&#xA;&#xA;Aliases:&#xA;chat      model chat&#xA;convert   model convert&#xA;diff      taxonomy diff&#xA;download  model download&#xA;evaluate  model evaluate&#xA;generate  data generate&#xA;init      config init&#xA;list      model model_list&#xA;serve     model serve&#xA;sysinfo   system info&#xA;test      model test&#xA;train     model train&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; Every &lt;code&gt;ilab&lt;/code&gt; command needs to be run from within your Python virtual environment. You can enter the Python environment by running the &lt;code&gt;source venv/bin/activate&lt;/code&gt; command.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Optional: You can enable tab completion for the &lt;code&gt;ilab&lt;/code&gt; command.&lt;/p&gt; &lt;h4&gt;Bash (version 4.4 or newer)&lt;/h4&gt; &lt;p&gt;Enable tab completion in &lt;code&gt;bash&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;eval &#34;$(_ILAB_COMPLETE=bash_source ilab)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To have this enabled automatically every time you open a new shell, you can save the completion script and source it from &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;_ILAB_COMPLETE=bash_source ilab &amp;gt; ~/.ilab-complete.bash&#xA;echo &#34;. ~/.ilab-complete.bash&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Zsh&lt;/h4&gt; &lt;p&gt;Enable tab completion in &lt;code&gt;zsh&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;eval &#34;$(_ILAB_COMPLETE=zsh_source ilab)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To have this enabled automatically every time you open a new shell, you can save the completion script and source it from &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;_ILAB_COMPLETE=zsh_source ilab &amp;gt; ~/.ilab-complete.zsh&#xA;echo &#34;. ~/.ilab-complete.zsh&#34; &amp;gt;&amp;gt; ~/.zshrc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Fish&lt;/h4&gt; &lt;p&gt;Enable tab completion in &lt;code&gt;fish&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;_ILAB_COMPLETE=fish_source ilab | source&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To have this enabled automatically every time you open a new shell, you can save the completion script and source it from &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;_ILAB_COMPLETE=fish_source ilab &amp;gt; ~/.config/fish/completions/ilab.fish&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;🏗️ Initialize &lt;code&gt;ilab&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Initialize &lt;code&gt;ilab&lt;/code&gt; by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab config init&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Example output&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Welcome to InstructLab CLI. This guide will help you set up your environment.&#xA;Please provide the following values to initiate the environment [press Enter for defaults]:&#xA;Path to taxonomy repo [taxonomy]: &amp;lt;ENTER&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted by the interface, press &lt;strong&gt;Enter&lt;/strong&gt; to add a new default &lt;code&gt;config.yaml&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted, clone the &lt;code&gt;https://github.com/instructlab/taxonomy.git&lt;/code&gt; repository into the current directory by typing &lt;strong&gt;y&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: If you want to point to an existing local clone of the &lt;code&gt;taxonomy&lt;/code&gt; repository, you can pass the path interactively or alternatively with the &lt;code&gt;--taxonomy-path&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example output after initializing &lt;code&gt;ilab&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab config init&#xA;Welcome to InstructLab CLI. This guide will help you set up your environment.&#xA;Please provide the following values to initiate the environment [press Enter for defaults]:&#xA;Path to taxonomy repo [taxonomy]: &amp;lt;ENTER&amp;gt;&#xA;`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y&#xA;Cloning https://github.com/instructlab/taxonomy.git...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;ilab&lt;/code&gt; will use the default configuration file unless otherwise specified. You can override this behavior with the &lt;code&gt;--config&lt;/code&gt; parameter for any &lt;code&gt;ilab&lt;/code&gt; command.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted, provide the path to your default model. Otherwise, the default of a quantized &lt;a href=&#34;https://huggingface.co/instructlab/merlinite-7b-lab-GGUF&#34;&gt;Merlinite&lt;/a&gt; model will be used - you can download this model with &lt;code&gt;ilab model download&lt;/code&gt; (see below).&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab config init&#xA;Welcome to InstructLab CLI. This guide will help you set up your environment.&#xA;Please provide the following values to initiate the environment [press Enter for defaults]:&#xA;Path to taxonomy repo [taxonomy]: &amp;lt;ENTER&amp;gt;&#xA;`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y&#xA;Cloning https://github.com/instructlab/taxonomy.git...&#xA;Path to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]: &amp;lt;ENTER&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. If you are on MacOS or a Linux machine without a dedicated GPU, please choose &lt;code&gt;No Profile (CPU, Apple Metal, AMD ROCm)&lt;/code&gt; by hitting Enter. There are various flags you can utilize with individual &lt;code&gt;ilab&lt;/code&gt; commands that allow you to utilize your GPU if applicable.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Welcome to InstructLab CLI. This guide will help you to setup your environment.&#xA;Please provide the following values to initiate the environment [press Enter for defaults]:&#xA;Path to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]:&#xA;Path to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:&#xA;Generating `/home/user/.config/instructlab/config.yaml` and `/home/user/.local/share/instructlab/internal/train_configuration/profiles`...&#xA;Please choose a train profile to use.&#xA;Train profiles assist with the complexity of configuring specific GPU hardware with the InstructLab Training library.&#xA;You can still take advantage of hardware acceleration for training even if your hardware is not listed.&#xA;[0] No profile (CPU, Apple Metal, AMD ROCm)&#xA;[1] Nvidia A100/H100 x2 (A100_H100_x2.yaml)&#xA;[2] Nvidia A100/H100 x4 (A100_H100_x4.yaml)&#xA;[3] Nvidia A100/H100 x8 (A100_H100_x8.yaml)&#xA;[4] Nvidia L40 x4 (L40_x4.yaml)&#xA;[5] Nvidia L40 x8 (L40_x8.yaml)&#xA;[6] Nvidia L4 x8 (L4_x8.yaml)&#xA;Enter the number of your choice [hit enter for no profile] [0]:&#xA;No profile selected - any hardware acceleration for training must be configured manually.&#xA;Initialization completed successfully, you&#39;re ready to start using `ilab`. Enjoy!&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GPU profiles are listed by GPU type and number of GPUs present. If you happen to have a GPU configuration with a similar amount of vRAM as any of the above profiles, feel free to try them out!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;code&gt;ilab&lt;/code&gt; directory layout after initializing your system&lt;/h3&gt; &#xA;&lt;p&gt;After running &lt;code&gt;ilab config init&lt;/code&gt; your directories will look like the following on a Linux system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;├─ ~/.cache/instructlab/models/ (1)&#xA;├─ ~/.local/share/instructlab/datasets (2)&#xA;├─ ~/.local/share/instructlab/taxonomy (3)&#xA;├─ ~/.local/share/instructlab/checkpoints (4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;~/.cache/instructlab/models/&lt;/code&gt;: Contains all downloaded large language models, including the saved output of ones you generate with ilab.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;~/.local/share/instructlab/datasets/&lt;/code&gt;: Contains data output from the SDG phase, built on modifications to the taxonomy repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;~/.local/share/instructlab/taxonomy/&lt;/code&gt;: Contains the skill and knowledge data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;~/.local/share/instructlab/checkpoints/&lt;/code&gt;: Contains the output of the training process&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📥 Download the model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;code&gt;ilab model download&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model download&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;ilab model download&lt;/code&gt; downloads a compact pre-trained version of the &lt;a href=&#34;https://huggingface.co/instructlab/&#34;&gt;model&lt;/a&gt; (~4.4G) from HuggingFace:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab model download&#xA;Downloading model from Hugging Face: instructlab/merlinite-7b-lab-GGUF@main to /home/user/.cache/instructlab/models...&#xA;...&#xA;INFO 2024-08-01 15:05:48,464 huggingface_hub.file_download:1893: Download complete. Moving file to /home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; ⏳ This command can take few minutes or immediately depending on your internet connection or model is cached. If you have issues connecting to Hugging Face, refer to the &lt;a href=&#34;https://discuss.huggingface.co/&#34;&gt;Hugging Face discussion forum&lt;/a&gt; for more details.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;h4&gt;Downloading a specific model from a Hugging Face repository&lt;/h4&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Specify repository, model, and a Hugging Face token if necessary. More information about Hugging Face tokens can be found &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;HF_TOKEN=&amp;lt;YOUR HUGGINGFACE TOKEN GOES HERE&amp;gt; ilab model download --repository=TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --filename=mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Downloading an entire Hugging Face repository (Safetensors Model)&lt;/h4&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Specify repository, and a Hugging Face token if necessary. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;HF_TOKEN=&amp;lt;YOUR HUGGINGFACE TOKEN GOES HERE&amp;gt; ilab model download --repository=instructlab/granite-7b-lab&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.&lt;/p&gt; &lt;h4&gt;Listing downloaded models&lt;/h4&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;All downloaded models can be seen with &lt;code&gt;ilab model list&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model list&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Example output of &lt;code&gt;ilab model list&lt;/code&gt; after &lt;code&gt;ilab model download&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab model list&#xA;+------------------------------+---------------------+--------+&#xA;| Model Name                   | Last Modified       | Size   |&#xA;+------------------------------+---------------------+--------+&#xA;| merlinite-7b-lab-Q4_K_M.gguf | 2024-08-01 15:05:48 | 4.1 GB |&#xA;+------------------------------+---------------------+--------+&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🍴 Serving the model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Serve the model by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model serve&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Serve a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model serve --model-path models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the model is served and ready, you&#39;ll see the following output:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab model serve&#xA;INFO 2024-03-02 02:21:11,352 lab.py:201 Using model &#39;models/ggml-merlinite-7b-lab-Q4_K_M.gguf&#39; with -1 gpu-layers and 4096 max context size.&#xA;Starting server process&#xA;After application startup complete see http://127.0.0.1:8000/docs for API.&#xA;Press CTRL+C to shut down the server.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If multiple &lt;code&gt;ilab&lt;/code&gt; clients try to connect to the same InstructLab server at the same time, the 1st will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Serve a non-default Safetensors model (e.g. granite-7b-lab). NOTE: this requires a GPU.&lt;/p&gt; &lt;p&gt;Ensure vllm is installed:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip show vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If it is not, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📣 Chat with the model (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;Because you&#39;re serving the model in one terminal window, you will have to create a new window and re-activate your Python virtual environment to run &lt;code&gt;ilab model chat&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;source venv/bin/activate&#xA;ilab model chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;source venv/bin/activate&#xA;ilab model chat --model models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that usage of &lt;code&gt;--model&lt;/code&gt; necessitates that the existing server has that model. If not, you must exit the server. &lt;code&gt;--model&lt;/code&gt; in &lt;code&gt;ilab model chat&lt;/code&gt; has the ability to start a server on your behalf with the specified model if one is not already running on the port.&lt;/p&gt; &#xA;&lt;p&gt;Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as &lt;code&gt;what is the capital of Canada?&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; the model needs to be trained with the generated synthetic data to use the new skills or knowledge&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab model chat&#xA;╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮&#xA;│ Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                             │&#xA;╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯&#xA;&amp;gt;&amp;gt;&amp;gt; what is the capital of Canada?                                                                                                                                                                                                [S][default]&#xA;╭────────────────────────────────────────────────────────────────────────────────────────────────────── ggml-merlinite-7b-lab-Q4_K_M ───────────────────────────────────────────────────────────────────────────────────────────────────────╮&#xA;│ The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to │&#xA;│ Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada&#39;s identity.                                   │&#xA;╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 12.008 seconds ─╯&#xA;&amp;gt;&amp;gt;&amp;gt;                                                                                                                                                                                                                               [S][default]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💻 Creating new knowledge or skills and training the model&lt;/h2&gt; &#xA;&lt;h3&gt;🎁 Contribute knowledge or compositional skills&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Contribute new knowledge or compositional skills to your local &lt;a href=&#34;https://github.com/instructlab/taxonomy.git&#34;&gt;taxonomy&lt;/a&gt; repository.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Detailed contribution instructions can be found in the &lt;a href=&#34;https://github.com/instructlab/taxonomy/raw/main/README.md&#34;&gt;taxonomy repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] There is a limit to how much content can exist in the question/answer pairs for the model to process. Due to this, only add a maximum of around 2300 words to your question and answer seed example pairs in the &lt;code&gt;qna.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;📜 List and validate your new data&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;ilab taxonomy diff&lt;/code&gt; command to ensure &lt;code&gt;ilab&lt;/code&gt; is registering your new knowledge or skills and your contributions are properly formatted. This command displays any new or modified YAML files within your taxonomy tree. For example, the following is the expected result of a valid compositional skill contribution after adding a new skill called &lt;code&gt;foo-lang&lt;/code&gt; to the freeform writing subdirectory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab taxonomy diff&#xA;compositional_skills/writing/freeform/foo-lang/qna.yaml&#xA;Taxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also validate your entire taxonomy by performing a diff against an empty base by using the &lt;code&gt;--taxonomy-base=empty&lt;/code&gt; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab taxonomy diff --taxonomy-base=empty&#xA;compositional_skills/general/tables/empty/qna.yaml&#xA;compositional_skills/general/tables/editing/add_remove/qna.yaml&#xA;...&#xA;Taxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🚀 Generate a synthetic dataset&lt;/h3&gt; &#xA;&lt;p&gt;Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, &lt;code&gt;ilab data generate&lt;/code&gt; can start a server for you if you provide a fully qualified model path via &lt;code&gt;--model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;To generate a synthetic dataset based on your newly added knowledge or skill set in &lt;a href=&#34;https://github.com/instructlab/taxonomy.git&#34;&gt;taxonomy&lt;/a&gt; repository, run the following command:&lt;/p&gt; &lt;p&gt;With GPU acceleration:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab data generate --pipeline full --gpus &amp;lt;NUM_OF_GPUS&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Without GPU acceleration:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab data generate --pipeline simple&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1) to generate data, run the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; ⏳ This can take from 15 minutes to 1+ hours to complete, depending on your computing resources.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;Example output of &lt;code&gt;ilab data generate&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ilab data generate&#xA;INFO 2024-07-30 19:57:44,093 numexpr.utils:161: NumExpr defaulting to 8 threads.&#xA;INFO 2024-07-30 19:57:44,452 datasets:58: PyTorch version 2.3.1 available.&#xA;Generating synthetic data using &#39;simple&#39; pipeline, &#39;$HOME/.cache/instructlab/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#39; model, &#39;./taxonomy&#39; taxonomy, against http://localhost:8000/v1 server&#xA;INFO 2024-07-30 19:57:45,084 instructlab.sdg:375: Synthesizing new instructions. If you aren&#39;t satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.&#xA;INFO 2024-07-30 19:57:45,090 instructlab.sdg.pipeline:153: Running pipeline single-threaded&#xA;INFO 2024-07-30 19:57:47,820 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False&#xA;INFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:197: Running block: gen_skill_freeform&#xA;INFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:198: Dataset({&#xA;   features: [&#39;task_description&#39;, &#39;seed_question&#39;, &#39;seed_response&#39;],&#xA;   num_rows: 5&#xA;})&#xA;INFO 2024-07-30 20:02:16,455 instructlab.sdg:411: Generated 1 samples&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The synthetic data set will be two files in the newly created in the datasets directory: &lt;code&gt;~/.local/share/instructlab/datasets&lt;/code&gt;. These files will be named &lt;code&gt;skills_train_msgs_*.jsonl&lt;/code&gt; and &lt;code&gt;knowledge_train_msgs_*.jsonl&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Verify the files have been created by running the &lt;code&gt;ls datasets&lt;/code&gt; command. Note: you must be in your &lt;code&gt;XDG_DATA_HOME/instructlab&lt;/code&gt; directory.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ls datasets/&#xA;node_datasets_2024-08-12T20_31_15                          test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_23_06.jsonl&#xA;knowledge_recipe_2024-08-12T20_31_15.yaml                      node_datasets_2024-08-13T19_51_48                          test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl&#xA;knowledge_recipe_2024-08-13T19_51_48.yaml                      skills_recipe_2024-08-12T20_31_15.yaml                     test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_47_59.jsonl&#xA;knowledge_train_msgs_2024-08-12T20_31_15.jsonl                 skills_recipe_2024-08-13T19_51_48.yaml                     test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl&#xA;knowledge_train_msgs_2024-08-13T19_51_48.jsonl                 skills_train_msgs_2024-08-12T20_31_15.jsonl                train_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl&#xA;messages_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl  skills_train_msgs_2024-08-13T19_51_48.jsonl                train_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl&#xA;messages_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl  test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_13_21.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: It is also possible to run the generate step against a different model via an OpenAI-compatible API. For example, the one spawned by &lt;code&gt;ilab model serve&lt;/code&gt; or any remote or locally hosted LLM (e.g. via &lt;a href=&#34;https://ollama.com/&#34;&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://lmstudio.ai&#34;&gt;&lt;code&gt;LM Studio&lt;/code&gt;&lt;/a&gt;, etc.). Run the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab data generate --endpoint-url http://localhost:8000/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note that it is also possible to generate a synthetic dataset based on the entire contents of the taxonomy repo using the &lt;code&gt;--taxonomy-base=empty&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab data generate --taxonomy-base=empty&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;👩‍🏫 Training the model&lt;/h3&gt; &#xA;&lt;p&gt;There are many options for training the model with your synthetic data-enhanced dataset.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;strong&gt;Every&lt;/strong&gt; &lt;code&gt;ilab&lt;/code&gt; command needs to run from within your Python virtual environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Train the model locally on Linux&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; ⏳ This step can potentially take &lt;strong&gt;several hours&lt;/strong&gt; to complete depending on your computing resources. Please stop &lt;code&gt;ilab model chat&lt;/code&gt; and &lt;code&gt;ilab model serve&lt;/code&gt; first to free resources.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are using &lt;code&gt;ilab model train --legacy&lt;/code&gt; or are on MacOS:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ilab model train&lt;/code&gt; outputs a brand-new model that can be served in the &lt;code&gt;models&lt;/code&gt; directory called &lt;code&gt;ggml-model-f16.gguf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are using &lt;code&gt;ilab model train&lt;/code&gt; with a GPU enabled system:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ilab model train&lt;/code&gt; outputs brand-new models that can be served in the &lt;code&gt;~/.local/share/instructlab/checkpoints&lt;/code&gt; directory. These models can be run through &lt;code&gt;ilab model evaluate&lt;/code&gt; to choose the best one.&lt;/p&gt; &#xA;&lt;p&gt;If you are using &lt;code&gt;ilab model train --strategy lab-multiphase&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Train the model locally on an M-series Mac&lt;/h4&gt; &#xA;&lt;p&gt;To train the model locally on your M-Series Mac is as easy as running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; ⏳ This process will take a little while to complete (time can vary based on hardware and output of &lt;code&gt;ilab data generate&lt;/code&gt; but on the order of 5 to 15 minutes)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;code&gt;ilab model train&lt;/code&gt; outputs a brand-new model that is saved in the &lt;code&gt;&amp;lt;model_name&amp;gt;-mlx-q&lt;/code&gt; directory called &lt;code&gt;adapters.npz&lt;/code&gt; (in &lt;code&gt;Numpy&lt;/code&gt; compressed array format). For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;(venv) $ ls instructlab-merlinite-7b-lab-mlx-q&#xA;adapters-010.npz        adapters-050.npz        adapters-090.npz        config.json             tokenizer.model&#xA;adapters-020.npz        adapters-060.npz        adapters-100.npz        model.safetensors       tokenizer_config.json&#xA;adapters-030.npz        adapters-070.npz        adapters.npz            special_tokens_map.json&#xA;adapters-040.npz        adapters-080.npz        added_tokens.json       tokenizer.jso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train the model locally with GPU acceleration&lt;/h4&gt; &#xA;&lt;p&gt;Training has experimental support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/docs/gpu-acceleration.md&#34;&gt;the GPU acceleration documentation&lt;/a&gt; for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model train --device=cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This version of &lt;code&gt;ilab model train&lt;/code&gt; outputs brand-new models that can be served in the &lt;code&gt;~/.local/share/instructlab/checkpoints&lt;/code&gt; directory. These models can be run through &lt;code&gt;ilab model evaluate&lt;/code&gt; to choose the best one.&lt;/p&gt; &#xA;&lt;h4&gt;Train the model locally with multi-phase training and GPU acceleration&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;ilab model train&lt;/code&gt; supports multi-phase training. This results in the following workflow:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We train the model on knowledge&lt;/li&gt; &#xA; &lt;li&gt;Evaluate the trained model to find the best checkpoint&lt;/li&gt; &#xA; &lt;li&gt;We train the model on skills&lt;/li&gt; &#xA; &lt;li&gt;We evaluate the model to find the best overall checkpoint&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model train --strategy lab-multiphase --phased-phase1-data &amp;lt;knowledge train messages jsonl&amp;gt; --phased-phase2-data &amp;lt;skills train messages jsonl&amp;gt; -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command takes in two &lt;code&gt;.jsonl&lt;/code&gt; files from your &lt;code&gt;datasets&lt;/code&gt; directory, one is the knowledge jsonl and the other is a skills jsonl. The &lt;code&gt;-y&lt;/code&gt; flag skips an interactive prompt asking the user if they are sure they want to run multi-phase training.&lt;/p&gt; &#xA;&lt;p&gt;Note: this command may take 3 or more hours depending on the size of the data and number of training epochs you run.&lt;/p&gt; &#xA;&lt;h4&gt;Train the model in the cloud&lt;/h4&gt; &#xA;&lt;p&gt;Follow the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/notebooks/README.md&#34;&gt;Training&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;⏳ Approximate amount of time taken on each platform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Google Colab&lt;/em&gt;: &lt;strong&gt;5-10 minutes&lt;/strong&gt; with a T4 GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Kaggle&lt;/em&gt;: &lt;strong&gt;~30 minutes&lt;/strong&gt; with a P100 GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After that&#39;s done, you can play with your model directly in the Google Colab or Kaggle notebook. Model trained on the cloud will be saved on the cloud. The model can also be downloaded and served locally.&lt;/p&gt; &#xA;&lt;h3&gt;📜 Test the newly trained model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command to test the model:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output from the command will consist of a series of outputs from the model before and after training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🧪 Evaluate the newly trained model&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;ilab model evaluate&lt;/code&gt; command to evaluate the models you are training with several benchmarks. Currently, four benchmarks are supported.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;Measures&lt;/th&gt; &#xA;   &lt;th&gt;Full Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;Knowledge&lt;/td&gt; &#xA;   &lt;td&gt;Massive Multitask Language Understanding&lt;/td&gt; &#xA;   &lt;td&gt;Tests a model against a standardized set of knowledge data and produces a score based on the model&#39;s performance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34;&gt;Measuring Massive Multitask Language Understanding&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLUBranch&lt;/td&gt; &#xA;   &lt;td&gt;Knowledge&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;Tests your knowledge contributions against a base model and produces a score based on the difference in performance&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MTBench&lt;/td&gt; &#xA;   &lt;td&gt;Skills&lt;/td&gt; &#xA;   &lt;td&gt;Multi-turn Benchmark&lt;/td&gt; &#xA;   &lt;td&gt;Tests a model&#39;s skill at applying its knowledge against a judge model and produces a score based on the model&#39;s performance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://klu.ai/glossary/mt-bench-eval&#34;&gt;MT-Bench (Multi-turn Benchmark)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MTBenchBranch&lt;/td&gt; &#xA;   &lt;td&gt;Skills&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;Tests your skill contributions against a judge model and produces a score based on the difference in performance&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] MTBench and MTBenchBranch use &lt;a href=&#34;https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0&#34;&gt;prometheus-8x7b-v2.0&lt;/a&gt; as the judge model by default. While you do not need to use this model as your judge, it is strongly recommended to do so if you have the necessary hardware resources. You can download it via &lt;code&gt;ilab model download&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Running MMLU&lt;/h4&gt; &#xA;&lt;p&gt;Below is an example of running MMLU on a local model with minimal tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export INSTRUCTLAB_EVAL_MMLU_MIN_TASKS=true   # don&#39;t set this if you want to run full MMLU&#xA;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ ilab model evaluate --benchmark mmlu --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab&#xA;...&#xA;# KNOWLEDGE EVALUATION REPORT&#xA;&#xA;## MODEL&#xA;/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;### AVERAGE:&#xA;0.45 (across 3)&#xA;&#xA;### SCORES:&#xA;mmlu_abstract_algebra - 0.35&#xA;mmlu_anatomy - 0.44&#xA;mmlu_astronomy - 0.55&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below is an example of running MMLU on a Hugging Face model with minimal tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export INSTRUCTLAB_EVAL_MMLU_MIN_TASKS=true   # don&#39;t set this if you want to run full MMLU&#xA;$ ilab model evaluate --benchmark mmlu --model instructlab/granite-7b-lab&#xA;...&#xA;# KNOWLEDGE EVALUATION REPORT&#xA;&#xA;## MODEL&#xA;instructlab/granite-7b-lab&#xA;&#xA;### AVERAGE:&#xA;0.45 (across 3)&#xA;&#xA;### SCORES:&#xA;mmlu_abstract_algebra - 0.35&#xA;mmlu_anatomy - 0.44&#xA;mmlu_astronomy - 0.55&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Currently, MMLU can only be run against a safetensors model directory, either locally or on Hugging Face. GGUFs are not currently supported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Running MMLUBranch&lt;/h4&gt; &#xA;&lt;p&gt;Below is an example of running MMLUBranch with a local safetensors model directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ ilab model evaluate --benchmark mmlu_branch --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab&#xA;...&#xA;# KNOWLEDGE EVALUATION REPORT&#xA;&#xA;## BASE MODEL&#xA;/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;## MODEL&#xA;/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;### AVERAGE:&#xA;+0.0 (across 1)&#xA;&#xA;### NO CHANGE:&#xA;1. tonsils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below is an example of running MMLUBranch with Hugging Face models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ilab model evaluate --benchmark mmlu_branch --model instructlab/granite-7b-lab --base-model instructlab/granite-7b-lab&#xA;...&#xA;# KNOWLEDGE EVALUATION REPORT&#xA;&#xA;## BASE MODEL&#xA;instructlab/granite-7b-lab&#xA;&#xA;## MODEL&#xA;instructlab/granite-7b-lab&#xA;&#xA;### AVERAGE:&#xA;+0.0 (across 1)&#xA;&#xA;### NO CHANGE:&#xA;1. tonsils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can mix and match running local models and remote models on Hugging Face, so long as a safetensors model is present.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Running MTBench&lt;/h4&gt; &#xA;&lt;p&gt;Below is an example of running MTBench with a local safetensors model directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ ilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab --judge-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab&#xA;...&#xA;# SKILL EVALUATION REPORT&#xA;&#xA;## MODEL&#xA;/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;### AVERAGE:&#xA;8.07 (across 91)&#xA;&#xA;### TURN ONE:&#xA;8.64&#xA;&#xA;### TURN TWO:&#xA;7.19&#xA;&#xA;### ERROR RATE:&#xA;0.43&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below is an example of running MTBench with local GGUF models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ ilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --judge-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf&#xA;...&#xA;# SKILL EVALUATION REPORT&#xA;&#xA;## MODEL&#xA;/home/example/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf&#xA;&#xA;### AVERAGE:&#xA;5.0 (across 1)&#xA;&#xA;### TURN ONE:&#xA;5.0&#xA;&#xA;### TURN TWO:&#xA;N/A&#xA;&#xA;### ERROR RATE:&#xA;0.99&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Currently, MTBench must be used with local models. Using models directly from Hugging Face without downloading them is unsupported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Running MTBenchBranch&lt;/h4&gt; &#xA;&lt;p&gt;Below is an example of running MTBenchBranch with a local safetensors model directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ export ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy&#xA;$ ilab model evaluate --benchmark mt_bench_branch \&#xA;   --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \&#xA;   --judge-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \&#xA;   --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \&#xA;   --taxonomy-path $ILAB_TAXONOMY_DIR \&#xA;   --branch rc \&#xA;   --base-branch main&#xA;...&#xA;# SKILL EVALUATION REPORT&#xA;&#xA;## BASE MODEL&#xA;/home/example/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;## MODEL&#xA;/home/example/.local/share/instructlab/models/instructlab/granite-7b-lab&#xA;&#xA;### IMPROVEMENTS:&#xA;1. compositional_skills/extraction/receipt/markdown/qna.yaml (+4.0)&#xA;2. compositional_skills/STEM/science/units_conversion/temperature_conversion/qna.yaml (+3.0)&#xA;3. compositional_skills/extraction/commercial_lease_agreement/bullet_points/qna.yaml (+3.0)&#xA;...&#xA;&#xA;### REGRESSIONS:&#xA;1. compositional_skills/extraction/abstractive/title/qna.yaml (-5.0)&#xA;2. compositional_skills/extraction/receipt/bullet_points/qna.yaml (-4.5)&#xA;3. compositional_skills/writing/grounded/summarization/wiki_insights/one_line/qna.yaml (-4.0)&#xA;...&#xA;&#xA;### NO CHANGE:&#xA;1. compositional_skills/STEM/math/reasoning/qna.yaml&#xA;2. compositional_skills/extraction/commercial_lease_agreement/csv/qna.yaml&#xA;3. compositional_skills/roleplay/explain_like_i_am/graduate/qna.yaml&#xA;...&#xA;&#xA;### NEW:&#xA;1. compositional_skills/linguistics/organize_lists/qna.yaml&#xA;2. compositional_skills/extraction/invoice/plain_text/qna.yaml&#xA;3. compositional_skills/writing/grounded/summarization/wiki_insights/concise/qna.yaml&#xA;...&#xA;&#xA;### ERROR RATE:&#xA;0.32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below is an example of running MTBenchBranch with local GGUF models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models&#xA;$ export ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy&#xA;$ ilab model evaluate --benchmark mt_bench_branch --model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --judge-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --base-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --taxonomy-path $ILAB_TAXONOMY_DIR --branch rc --base-branch main&#xA;...&#xA;# SKILL EVALUATION REPORT&#xA;&#xA;## BASE MODEL&#xA;/home/ec2-user/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf&#xA;&#xA;## MODEL&#xA;/home/ec2-user/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf&#xA;&#xA;### NO CHANGE:&#xA;1. compositional_skills/STEM/math/distance_conversion/qna.yaml&#xA;&#xA;### NEW:&#xA;1. compositional_skills/linguistics/organize_lists/qna.yaml&#xA;2. compositional_skills/extraction/annual_report/reasoning/qna.yaml&#xA;3. compositional_skills/extraction/email/plain_text/qna.yaml&#xA;4. compositional_skills/extraction/technical_paper/tables/bullet_points/qna.yaml&#xA;5. compositional_skills/extraction/technical_paper/abstract/reasoning/qna.yaml&#xA;&#xA;### ERROR RATE:&#xA;0.98&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Currently, MTBenchBranch must be used with local models. Using models directly from Hugging Face without downloading them is unsupported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;🍴 Serve the newly trained model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Stop the server you have running by entering &lt;code&gt;ctrl+c&lt;/code&gt; keys in the terminal running the server.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;🍎 This step is only implemented for macOS with M-series chips (for now).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Before serving the newly trained model you must convert it to work with the &lt;code&gt;ilab&lt;/code&gt; cli. The &lt;code&gt;ilab model convert&lt;/code&gt; command converts the new model into quantized &lt;a href=&#34;https://medium.com/@sandyeep70/ggml-to-gguf-a-leap-in-language-model-file-formats-cd5d3a6058f9&#34;&gt;GGUF&lt;/a&gt; format which is required by the server to host the model in the &lt;code&gt;ilab model serve&lt;/code&gt; command.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Convert the newly trained model by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model convert&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Serve the newly trained model locally via &lt;code&gt;ilab model serve&lt;/code&gt; command with the &lt;code&gt;--model-path&lt;/code&gt; argument to specify your new model:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model serve --model-path &amp;lt;new model path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which model should you select to serve? After running the &lt;code&gt;ilab model convert&lt;/code&gt; command, some files and a directory are generated. The model you will want to serve ends with an extension of &lt;code&gt;.gguf&lt;/code&gt; and exists in a directory with the suffix &lt;code&gt;trained&lt;/code&gt;. For example: &lt;code&gt;instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📣 Chat with the new model (not optional this time)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Try the fine-tuned model out live using the chat interface, and see if the results are better than the untrained version of the model with chat by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab model chat -m &amp;lt;New model path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are interested in optimizing the quality of the model&#39;s responses, please see &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/TROUBLESHOOTING.md#model-fine-tuning-and-response-optimization&#34;&gt;&lt;code&gt;TROUBLESHOOTING.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Upgrade InstructLab to latest version&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To upgrade InstructLab to the latest version, use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install instructlab --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎁 Submit your new knowledge or skills&lt;/h2&gt; &#xA;&lt;p&gt;Of course, the final step is, if you&#39;ve improved the model, to open a pull-request in the &lt;a href=&#34;https://github.com/instructlab/taxonomy&#34;&gt;taxonomy repository&lt;/a&gt; that includes the files (e.g. &lt;code&gt;qna.yaml&lt;/code&gt;) with your improved data.&lt;/p&gt; &#xA;&lt;h2&gt;📬 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://raw.githubusercontent.com/instructlab/instructlab/main/CONTRIBUTING/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; guide to learn how to contribute.&lt;/p&gt;</summary>
  </entry>
</feed>