<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-22T01:35:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>josh0xA/darkdump</title>
    <updated>2025-07-22T01:35:07Z</updated>
    <id>tag:github.com,2025-07-22:/josh0xA/darkdump</id>
    <link href="https://github.com/josh0xA/darkdump" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Intelligence Interface for Deep Web Scraping&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;darkdump&lt;/h1&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Darkdump is a OSINT interface for carrying out deep web investgations written in python in which it allows users to enter a search query in which darkdump provides the ability to scrape .onion sites relating to that query to try to extract emails, metadata, keywords, images, social media etc. Darkdump retrieves sites via Ahmia.fi and scrapes those .onion addresses when connected via the tor network.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/josh0xA/darkdump&lt;/code&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd darkdump&lt;/code&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python3 -m pip install -r requirements.txt&lt;/code&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python3 darkdump.py --help&lt;/code&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Tor Configuration&lt;/h3&gt; &#xA;&lt;p&gt;To use Darkdump effectively, you need to configure Tor to allow your script to control it via the Tor control port. Here&#39;s how to set up your &lt;code&gt;torrc&lt;/code&gt; file and verify that Tor is running: &lt;br&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Step 1: Install Tor&lt;/h4&gt; &#xA;&lt;p&gt;If Tor is not already installed on your system, you need to install it. Here&#39;s how you can install Tor on various operating systems:&lt;/p&gt; &#xA;&lt;p&gt;Debian/Kali/Ubuntu: &lt;code&gt;sudo apt install tor&lt;/code&gt;&lt;br&gt; MacOS: &lt;code&gt;brew install tor&lt;/code&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Step 2: Configure the Tor torrc File&lt;br&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Locate your torrc file. This file is usually found at &lt;code&gt;/etc/tor/torrc&lt;/code&gt;on Linux and sometimes Mac.&lt;/p&gt; &#xA;&lt;p&gt;Add the following lines to your torrc to enable the control port and set a control port password:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ControlPort 9051&#xA;HashedControlPassword [YourHashedPasswordHere]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;[YourHashedPasswordHere]&lt;/code&gt; with a hashed password which can be generated using the &lt;code&gt;tor --hash-password&lt;/code&gt; command: &lt;code&gt;tor --hash-password &#34;my_password&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Step 3: Start Tor Service&lt;/h4&gt; &#xA;&lt;p&gt;Linux: &lt;code&gt;sudo systemctl start tor.service&lt;/code&gt;&lt;br&gt; MacOS: &lt;code&gt;brew services start tor&lt;/code&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example Queries:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 darkdump.py -q &#34;hacking&#34; -a 10 --scrape --proxy&lt;/code&gt; - search for 10 links and scrape each site &lt;br&gt; &lt;code&gt;python3 darkdump.py -q &#34;free movies&#34; -a 25&lt;/code&gt; - don&#39;t scrape, just return 25 links for that query (does not require tor) &lt;br&gt; &lt;code&gt;python3 darkdump.py -q &#34;marketplaces&#34; -a 15 --scrape --proxy -i&lt;/code&gt; - search for 10 links and scrape each site as well as find and store images.&lt;/p&gt; &#xA;&lt;h2&gt;Menu&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;     _            _       _                            __&#xA;  __| | __ _ _ __| | ____| |_   _ _ __ ___  _ __      / /&#xA; / _` |/ _` | &#39;__| |/ / _` | | | | &#39;_ ` _ \| &#39;_ \    / / &#xA;| (_| | (_| | |  |   &amp;lt; (_| | |_| | | | | | | |_) |  / /  &#xA; \__,_|\__,_|_|  |_|\_\__,_|\__,_|_| |_| |_| .__/  /_/  v3 by Josh Schiavone &#xA;                                           |_|           &#xA;&#xA;usage: darkdump.py [-h] [-v] [-q QUERY] [-a AMOUNT] [-p] [-i] [-s]&#xA;&#xA;Darkdump is an interface for scraping the deepweb through Ahmia. Made by yours truly.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  -v, --version         returns darkdump&#39;s version&#xA;  -q QUERY, --query QUERY&#xA;                        the keyword or string you want to search on the deepweb&#xA;  -a AMOUNT, --amount AMOUNT&#xA;                        the amount of results you want to retrieve&#xA;  -p, --proxy           use tor proxy for scraping&#xA;  -i, --images          scrape images and visual content from the site&#xA;  -s, --scrape          scrape the actual site for content and look for keywords&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Visual&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/josh0xA/darkdump/main/imgs/darkdump_example.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Ethical Notice&lt;/h2&gt; &#xA;&lt;p&gt;The developer of this program, Josh Schiavone, is not resposible for misuse of this data gathering tool. Do not use darkdump to navigate websites that take part in any activity that is identified as illegal under the laws and regulations of your government. May God bless you all.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;br&gt; Copyright (c) Josh Schiavone&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Yaofang-Liu/Pusa-VidGen</title>
    <updated>2025-07-22T01:35:07Z</updated>
    <id>tag:github.com,2025-07-22:/Yaofang-Liu/Pusa-VidGen</id>
    <link href="https://github.com/Yaofang-Liu/Pusa-VidGen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pusa: Thousands Timesteps Video Diffusion Model&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/f867c49d9570b88e7bbce6e25583a0ad2417cdf7/icon.png&#34; width=&#34;70&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Pusa: Thousands Timesteps Video Diffusion Model&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://yaofang-liu.github.io/Pusa_Web/&#34;&gt;&lt;img alt=&#34;Project Page&#34; src=&#34;https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/e99c3dcf866789a2db7fbe2686888ec398076a82/PusaV1/PusaV1.0_Report.pdf&#34;&gt;&lt;img alt=&#34;Technical Report&#34; src=&#34;https://img.shields.io/badge/Technical_Report-%F0%9F%93%9C-B31B1B?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/RaphaelLiu/PusaV1&#34;&gt;&lt;img alt=&#34;Model&#34; src=&#34;https://img.shields.io/badge/Pusa_V1.0-Model-FFD700?style=for-the-badge&amp;amp;logo=huggingface&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/RaphaelLiu/PusaV1_training&#34;&gt;&lt;img alt=&#34;Dataset&#34; src=&#34;https://img.shields.io/badge/Pusa_V1.0-Dataset-6495ED?style=for-the-badge&amp;amp;logo=huggingface&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Yaofang-Liu/Mochi-Full-Finetuner&#34;&gt;&lt;img alt=&#34;Code&#34; src=&#34;https://img.shields.io/badge/Code-Training%20Scripts-32CD32?logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2410.03160&#34;&gt;&lt;img alt=&#34;Paper&#34; src=&#34;https://img.shields.io/badge/%F0%9F%93%9C-FVDM%2520Paper-B31B1B?logo=arxiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/stephenajason&#34;&gt;&lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/badge/%F0%9F%90%A6-Twitter-1DA1F2?logo=twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.xiaohongshu.com/user/profile/5c6f928f0000000010015ca1?xsec_token=YBEf_x-s5bOBQIMJuNQvJ6H23Anwey1nnDgC9wiLyDHPU=&amp;amp;xsec_source=app_share&amp;amp;xhsshare=CopyLink&amp;amp;appuid=5c6f928f0000000010015ca1&amp;amp;apptime=1752622393&amp;amp;share_id=60f9a8041f974cb7ac5e3f0f161bf748&#34;&gt;&lt;img alt=&#34;Xiaohongshu&#34; src=&#34;https://img.shields.io/badge/%F0%9F%93%95-Xiaohongshu-FF2442&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🔥🔥🔥🚀 Announcing Pusa V1.0 🚀🔥🔥🔥&lt;/h2&gt; &#xA;&lt;p&gt;We are excited to release &lt;strong&gt;Pusa V1.0&lt;/strong&gt;, a groundbreaking paradigm that leverages &lt;strong&gt;vectorized timestep adaptation (VTA)&lt;/strong&gt; to enable fine-grained temporal control within a unified video diffusion framework. By finetuning the SOTA &lt;strong&gt;Wan-T2V-14B&lt;/strong&gt; model with VTA, Pusa V1.0 achieves unprecedented efficiency --&lt;strong&gt;surpassing the performance of Wan-I2V-14B with ≤ 1/200 of the training cost ($500 vs. ≥ $100,000)&lt;/strong&gt; and &lt;strong&gt;≤ 1/2500 of the dataset size (4K vs. ≥ 10M samples)&lt;/strong&gt;. The codebase has been integrated into the &lt;code&gt;PusaV1&lt;/code&gt; directory, based on &lt;code&gt;DiffSynth-Studio&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1000&#34; alt=&#34;Image&#34; src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/d98ef44c1f7c11724a6887b71fe35152493c68b4/PusaV1/pusa_benchmark_figure_dark.png&#34;&gt; &#xA;&lt;p&gt;Pusa V1.0 not only sets a new standard for image-to-video generation but also unlocks many other zero-shot multi-task capabilities such as start-end frames and video extension, all without task-specific training while preserving the base model&#39;s T2V capabilities.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage and examples for Pusa V1.0, please see the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/PusaV1/README.md&#34;&gt;Pusa V1.0 README&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h4&gt;🔥🔥🔥 2025.07: Pusa V1.0 (Pusa-Wan) Code, Technical Report, and Dataset, all released!!! Check our &lt;a href=&#34;https://yaofang-liu.github.io/Pusa_Web/&#34;&gt;project page&lt;/a&gt; and &lt;a href=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/e99c3dcf866789a2db7fbe2686888ec398076a82/PusaV1/PusaV1.0_Report.pdf&#34;&gt;paper&lt;/a&gt; for more info.&lt;/h4&gt; &#xA;&lt;h4&gt;🔥🔥🔥 2025.04: Pusa V0.5 (Pusa-Mochi) released.&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/55de93a198427525e23a509e0f0d04616b10d71f/assets/demo0.gif&#34; width=&#34;1000&#34; autoplay loop muted&gt; &lt;br&gt; &lt;em&gt;Pusa V0.5 showcases &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/8d2af9cad78859361cb1bc6b8df56d06b2c2fbb8/assets/demo_T2V.gif&#34; width=&#34;1000&#34; autoplay loop muted&gt; &lt;br&gt; &lt;em&gt;Pusa V0.5 still can do text-to-video generation like base model Mochi &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pusa can do many more other things, you may check details below.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#pusa-v10-based-on-wan&#34;&gt;Pusa V1.0 (Based on Wan)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#pusa-v05-based-on-mochi&#34;&gt;Pusa V0.5 (Based on Mochi)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#current-status-and-roadmap&#34;&gt;Current Status and Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#related-work&#34;&gt;Related Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Pusa (&lt;em&gt;pu: &#39;sA:&lt;/em&gt;, from &#34;Thousand-Hand Guanyin&#34; in Chinese) introduces a paradigm shift in video diffusion modeling through frame-level noise control with vectorized timesteps, departing from conventional scalar timestep approaches. This shift was first presented in our &lt;a href=&#34;https://arxiv.org/abs/2410.03160&#34;&gt;FVDM&lt;/a&gt; paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pusa V1.0&lt;/strong&gt; is based on the SOTA &lt;strong&gt;Wan-T2V-14B&lt;/strong&gt; model and enhances it with our unique vectorized timestep adaptations (VTA), a non-destructive adaptation that fully preserves the capabilities of the base model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pusa V0.5&lt;/strong&gt; leverages this architecture, and it is based on &lt;a href=&#34;https://huggingface.co/genmo/mochi-1-preview&#34;&gt;Mochi1-Preview&lt;/a&gt;. We are open-sourcing this work to foster community collaboration, enhance methodologies, and expand capabilities.&lt;/p&gt; &#xA;&lt;p&gt;Pusa&#39;s novel frame-level noise architecture with vectorized timesteps compared with conventional video diffusion models with a scalar timestep&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7d751fd8-9a14-42e6-bcde-6db940df6537&#34;&gt;https://github.com/user-attachments/assets/7d751fd8-9a14-42e6-bcde-6db940df6537&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;✨ Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Comprehensive Multi-task Support&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text-to-Video&lt;/li&gt; &#xA;   &lt;li&gt;Image-to-Video&lt;/li&gt; &#xA;   &lt;li&gt;Start-End Frames&lt;/li&gt; &#xA;   &lt;li&gt;Video completion/transitions&lt;/li&gt; &#xA;   &lt;li&gt;Video Extension&lt;/li&gt; &#xA;   &lt;li&gt;And more...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unprecedented Efficiency&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Surpasses Wan-I2V-14B with &lt;strong&gt;≤ 1/200 of the training cost&lt;/strong&gt; ($500 vs. ≥ $100,000)&lt;/li&gt; &#xA;   &lt;li&gt;Trained on a dataset &lt;strong&gt;≤ 1/2500 of the size&lt;/strong&gt; (4K vs. ≥ 10M samples)&lt;/li&gt; &#xA;   &lt;li&gt;Achieves a &lt;strong&gt;VBench-I2V score of 87.32%&lt;/strong&gt; (vs. 86.86% for Wan-I2V-14B)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Complete Open-Source Release&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full codebase and training/inference scripts&lt;/li&gt; &#xA;   &lt;li&gt;LoRA model weights and dataset for Pusa V1.0&lt;/li&gt; &#xA;   &lt;li&gt;Detailed architecture specifications&lt;/li&gt; &#xA;   &lt;li&gt;Comprehensive training methodology&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🔍 Unique Architecture&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Novel Diffusion Paradigm&lt;/strong&gt;: Implements frame-level noise control with vectorized timesteps, originally introduced in the &lt;a href=&#34;https://arxiv.org/abs/2410.03160&#34;&gt;FVDM paper&lt;/a&gt;, enabling unprecedented flexibility and scalability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Non-destructive Modification&lt;/strong&gt;: Our adaptations to the base model preserve its original Text-to-Video generation capabilities. After this adaptation, we only need a slight fine-tuning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Universal Applicability&lt;/strong&gt;: The methodology can be readily applied to other leading video diffusion models including Hunyuan Video, Wan2.1, and others. &lt;em&gt;Collaborations enthusiastically welcomed!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;v1.0 (July 15, 2025)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Released Pusa V1.0, based on the Wan-Video models.&lt;/li&gt; &#xA; &lt;li&gt;Released Technical Report, V1.0 model weights and dataset.&lt;/li&gt; &#xA; &lt;li&gt;Integrated codebase as &lt;code&gt;/PusaV1&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added new examples and training scripts for Pusa V1.0 in &lt;code&gt;PusaV1/&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Updated documentation for the V1.0 release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.5 (June 3, 2025)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Released inference scripts for Start&amp;amp;End Frames Generation, Multi-Frames Generation, Video Transition, and Video Extension.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.5 (April 10, 2025)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Released our training codes and details &lt;a href=&#34;https://github.com/Yaofang-Liu/Mochi-Full-Finetuner&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support multi-nodes/single-node full finetuning code for both Pusa and Mochi&lt;/li&gt; &#xA; &lt;li&gt;Released our training dataset &lt;a href=&#34;https://huggingface.co/datasets/RaphaelLiu/PusaV0.5_Training&#34;&gt;dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pusa V1.0 (Based on Wan)&lt;/h2&gt; &#xA;&lt;p&gt;Pusa V1.0 leverages the powerful Wan-Video models and enhances them with our custom LoRA models and training scripts. For detailed instructions on installation, model preparation, usage examples, and training, please refer to the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/PusaV1/README.md&#34;&gt;Pusa V1.0 README&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pusa V0.5 (Based on Mochi)&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand for Pusa V0.5 details&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;You may install using &lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/genmoai/models&#xA;cd models &#xA;pip install uv&#xA;uv venv .venv&#xA;source .venv/bin/activate&#xA;uv pip install setuptools&#xA;uv pip install -e . --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you want to install flash attention, you can use:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;uv pip install -e .[flash] --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Download Weights&lt;/h3&gt; &#xA; &lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Use the Hugging Face CLI:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install huggingface_hub&#xA;huggingface-cli download RaphaelLiu/Pusa-V0.5 --local-dir &amp;lt;path_to_downloaded_directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Download directly from &lt;a href=&#34;https://huggingface.co/RaphaelLiu/Pusa-V0.5&#34;&gt;Hugging Face&lt;/a&gt; to your local machine.&lt;/p&gt; &#xA; &lt;h2&gt;Usage&lt;/h2&gt; &#xA; &lt;h3&gt;Image-to-Video Generation&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_ti2v_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;Your_prompt_here&#34; \&#xA;  --image_dir &#34;/path/to/input/image.jpg&#34; \&#xA;  --cond_position 0 \&#xA;  --num_steps 30 \&#xA;  --noise_multiplier 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note: We suggest you to try different &lt;code&gt;con_position&lt;/code&gt; here, and you may also modify the level of noise added to the condition image. You&#39;d be likely to get some surprises.&lt;/p&gt; &#xA; &lt;p&gt;Take &lt;code&gt;./demos/example.jpg&lt;/code&gt; as an example and run with 4 GPUs:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python ./demos/cli_test_ti2v_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;The camera remains still, the man is surfing on a wave with his surfboard.&#34; \&#xA;  --image_dir &#34;./demos/example.jpg&#34; \&#xA;  --cond_position 0 \&#xA;  --num_steps 30 \&#xA;  --noise_multiplier 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can get this result:&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/62526737953d9dc757414f2a368b94a0492ca6da/assets/example.gif&#34; width=&#34;300&#34; autoplay loop muted&gt; &lt;br&gt; &lt;/p&gt; &#xA; &lt;p&gt;You may ref to the baselines&#39; results from the &lt;a href=&#34;https://github.com/AILab-CVC/VideoGen-Eval&#34;&gt;VideoGen-Eval&lt;/a&gt; benchmark for comparison:&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Yaofang-Liu/Pusa-VidGen/raw/62526737953d9dc757414f2a368b94a0492ca6da/assets/example_baseline.gif&#34; width=&#34;1000&#34; autoplay loop muted&gt; &lt;br&gt; &lt;/p&gt; &#xA; &lt;h4&gt;Processing A Group of Images&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_ti2v_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --image_dir &#34;/path/to/image/directory&#34; \&#xA;  --prompt_dir &#34;/path/to/prompt/directory&#34; \&#xA;  --cond_position 1 \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For group processing, each image should have a corresponding text file with the same name in the prompt directory.&lt;/p&gt; &#xA; &lt;h4&gt;Using the Provided Shell Script&lt;/h4&gt; &#xA; &lt;p&gt;We also provide a shell script for convenience:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Edit cli_test_ti2v_release.sh to set your paths&#xA;# Then run:&#xA;bash ./demos/cli_test_ti2v_release.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Multi-frame Condition&lt;/h3&gt; &#xA; &lt;p&gt;Pusa supports generating videos from multiple keyframes (2 or more) placed at specific positions in the sequence. This is useful for both start-end frame generation and multi-keyframe interpolation.&lt;/p&gt; &#xA; &lt;h4&gt;Start &amp;amp; End Frame Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_multi_frames_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;Drone view of waves crashing against the rugged cliffs along Big Sur’s garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff’s edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff’s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.&#34; \&#xA;  --multi_cond &#39;{&#34;0&#34;: [&#34;./demos/example3.jpg&#34;, 0.3], &#34;20&#34;: [&#34;./demos/example5.jpg&#34;, 0.7]}&#39; \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The &lt;code&gt;multi_cond&lt;/code&gt; parameter specifies frame condition positions and their corresponding image paths and noise multipliers. In this example, the first frame (position 0) uses &lt;code&gt;./demos/example3.jpg&lt;/code&gt; with noise multiplier 0.3, and frame 20 uses &lt;code&gt;./demos/example5.jpg&lt;/code&gt; with noise multiplier 0.5.&lt;/p&gt; &#xA; &lt;p&gt;Alternatively, use the provided shell script:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Edit parameters in cli_test_multi_frames_release.sh first&#xA;bash ./demos/cli_test_multi_frames_release.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Multi-keyframe Interpolation&lt;/h4&gt; &#xA; &lt;p&gt;To generate videos with more than two keyframes (e.g., start, middle, and end):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_multi_frames_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;Drone view of waves crashing against the rugged cliffs along Big Sur’s garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff’s edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff’s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.&#34; \&#xA;  --multi_cond &#39;{&#34;0&#34;: [&#34;./demos/example3.jpg&#34;, 0.3], &#34;13&#34;: [&#34;./demos/example4.jpg&#34;, 0.7], &#34;27&#34;: [&#34;./demos/example5.jpg&#34;, 0.7]}&#39; \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Video Transition&lt;/h3&gt; &#xA; &lt;p&gt;Create smooth transitions between two videos:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_transition_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;A fluffy Cockapoo, perched atop a vibrant pink flamingo jumps into a crystal-clear pool.&#34; \&#xA;  --video_start_dir &#34;./demos/example1.mp4&#34; \&#xA;  --video_end_dir &#34;./demos/example2.mp4&#34; \&#xA;  --cond_position_start &#34;[0]&#34; \&#xA;  --cond_position_end &#34;[-3,-2,-1]&#34; \&#xA;  --noise_multiplier &#34;[0.3,0.8,0.8,0.8]&#34; \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Parameters:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;cond_position_start&lt;/code&gt;: Frame indices from the start video to use as conditioning&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;cond_position_end&lt;/code&gt;: Frame indices from the end video to use as conditioning&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;noise_multiplier&lt;/code&gt;: Noise level multipliers for each conditioning frame&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Alternatively, use the provided shell script:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Edit parameters in cli_test_transition_release.sh first&#xA;bash ./demos/cli_test_transition_release.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Video Extension&lt;/h3&gt; &#xA; &lt;p&gt;Extend existing videos with generated content:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_extension_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene.&#34; \&#xA;  --video_dir &#34;./demos/example1.mp4&#34; \&#xA;  --cond_position &#34;[0,1,2,3]&#34; \&#xA;  --noise_multiplier &#34;[0.1,0.2,0.3,0.4]&#34; \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Parameters:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;cond_position&lt;/code&gt;: Frame indices from the input video to use as conditioning&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;noise_multiplier&lt;/code&gt;: Noise level multipliers for each conditioning frame&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Alternatively, use the provided shell script:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Edit parameters in cli_test_v2v_release.sh first&#xA;bash ./demos/cli_test_v2v_release.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Text-to-Video Generation&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./demos/cli_test_ti2v_release.py \&#xA;  --model_dir &#34;/path/to/Pusa-V0.5&#34; \&#xA;  --dit_path &#34;/path/to/Pusa-V0.5/pusa_v0_dit.safetensors&#34; \&#xA;  --prompt &#34;A man is playing basketball&#34; \&#xA;  --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;For Pusa V1.0, please find the training details in the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Yaofang-Liu/Pusa-VidGen/main/PusaV1/README.md#training&#34;&gt;Pusa V1.0 README&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Pusa V0.5, you can find our training code and details &lt;a href=&#34;https://github.com/Yaofang-Liu/Mochi-Full-Finetuner&#34;&gt;here&lt;/a&gt;, which also supports training for the original Mochi model.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Pusa currently has several known limitations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Video generation quality is dependent on the base model (e.g., Wan-T2V-14B for V1.0).&lt;/li&gt; &#xA; &lt;li&gt;We anticipate significant quality improvements when applying our methodology to more advanced models.&lt;/li&gt; &#xA; &lt;li&gt;We welcome community contributions to enhance model performance and extend its capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Currently Available&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✅ Model weights for Pusa V1.0 and V0.5&lt;/li&gt; &#xA; &lt;li&gt;✅ Inference code for Text-to-Video generation&lt;/li&gt; &#xA; &lt;li&gt;✅ Inference code for Image-to-Video generation&lt;/li&gt; &#xA; &lt;li&gt;✅ Inference scripts for start &amp;amp; end frames, multi-frames, video transition, video extension&lt;/li&gt; &#xA; &lt;li&gt;✅ Training code and details&lt;/li&gt; &#xA; &lt;li&gt;✅ Model full fine-tuning guide (for Pusa V0.5)&lt;/li&gt; &#xA; &lt;li&gt;✅ Training datasets&lt;/li&gt; &#xA; &lt;li&gt;✅ Technical Report for Pusa V1.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TODO List&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔄 Release more advanced versions with SOTA models&lt;/li&gt; &#xA; &lt;li&gt;🔄 More capabilities like long video generation&lt;/li&gt; &#xA; &lt;li&gt;🔄 ....&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.03160&#34;&gt;FVDM&lt;/a&gt;: Introduces the groundbreaking frame-level noise control with vectorized timestep approach that inspired Pusa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio&#34;&gt;Wan-Video&lt;/a&gt;: The foundation model for Pusa V1.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/genmo/mochi-1-preview&#34;&gt;Mochi&lt;/a&gt;: The foundation model for Pusa V0.5, recognized as a leading open-source video generation system on the Artificial Analysis Leaderboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If you use this work in your project, please cite the following references.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{Liu2025pusa,&#xA;  title={Pusa: Thousands Timesteps Video Diffusion Model},&#xA;  author={Yaofang Liu and Rui Liu},&#xA;  year={2025},&#xA;  url={https://github.com/Yaofang-Liu/Pusa-VidGen},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2024redefining,&#xA;&amp;nbsp; title={Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach},&#xA;&amp;nbsp; author={Liu, Yaofang and Ren, Yumeng and Cun, Xiaodong and Artola, Aitor and Liu, Yang and Zeng, Tieyong and Chan, Raymond H and Morel, Jean-michel},&#xA;&amp;nbsp; journal={arXiv preprint arXiv:2410.03160},&#xA;&amp;nbsp; year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>