<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-04T01:33:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TMElyralab/Comfyui-MusePose</title>
    <updated>2024-06-04T01:33:50Z</updated>
    <id>tag:github.com,2024-06-04:/TMElyralab/Comfyui-MusePose</id>
    <link href="https://github.com/TMElyralab/Comfyui-MusePose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;MusePose&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TMElyralab/MusePose&#34;&gt;MusePose&lt;/a&gt; is an image-to-video generation framework for virtual human under control signal such as pose.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;MusePose&lt;/code&gt; is the last building block of &lt;strong&gt;the Muse opensource serie&lt;/strong&gt;. Together with &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt; and &lt;a href=&#34;https://github.com/TMElyralab/MuseTalk&#34;&gt;MuseTalk&lt;/a&gt;, we hope the community can join us and march towards the vision where a virtual human can be generated end2end with native ability of full body movement and interaction. Please stay tuned for our next milestone!&lt;/p&gt; &#xA;&lt;h3&gt;Comfyui-MusePose&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re running on Linux, or non-admin account on windows you&#39;ll want to ensure &lt;code&gt;/ComfyUI/custom_nodes&lt;/code&gt; and &lt;code&gt;Comfyui-MusePose&lt;/code&gt; has write permissions.&lt;/p&gt; &#xA;&lt;p&gt;Followed ComfyUI&#39;s manual installation steps and do the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;/ComfyUI/custom_nodes/&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;git clone https://github.com/TMElyralab/Comfyui-MusePose.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;/ComfyUI/custom_nodes/Comfyui-MusePose&lt;/code&gt; folder and run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; pip install -r requirements.txt&#xA;&#xA; pip install --no-cache-dir -U openmim &#xA; mim install mmengine &#xA; mim install &#34;mmcv&amp;gt;=2.0.1&#34; &#xA; mim install &#34;mmdet&amp;gt;=3.1.0&#34; &#xA; mim install &#34;mmpose&amp;gt;=1.1.0&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start ComfyUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Updates&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;requirements.txt: diffusers 0.27.2 is now suported&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/TMElyralab/MusePose&#34;&gt;weights&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the weights of other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/unet&#34;&gt;sd-image-variations-diffusers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/yzd-v/DWPose/tree/main&#34;&gt;dwpose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth&#34;&gt;yolox&lt;/a&gt; - Make sure to rename to &lt;code&gt;yolox_l_8x8_300e_coco.pth&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be organized in &lt;code&gt;pretrained_weights&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./pretrained_weights/&#xA;|-- MusePose&#xA;|   |-- denoising_unet.pth&#xA;|   |-- motion_module.pth&#xA;|   |-- pose_guider.pth&#xA;|   ‚îî‚îÄ‚îÄ reference_unet.pth&#xA;|-- dwpose&#xA;|   |-- dw-ll_ucoco_384.pth&#xA;|   ‚îî‚îÄ‚îÄ yolox_l_8x8_300e_coco.pth&#xA;|-- sd-image-variations-diffusers&#xA;|   ‚îî‚îÄ‚îÄ unet&#xA;|       |-- config.json&#xA;|       ‚îî‚îÄ‚îÄ diffusion_pytorch_model.bin&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   ‚îî‚îÄ‚îÄ pytorch_model.bin&#xA;‚îî‚îÄ‚îÄ sd-vae-ft-mse&#xA;    |-- config.json&#xA;    ‚îî‚îÄ‚îÄ diffusion_pytorch_model.bin&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;workflow demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TMElyralab/Comfyui-MusePose/raw/main/musepose-workflow-demo.json&#34;&gt;https://github.com/TMElyralab/Comfyui-MusePose/blob/main/musepose-workflow-demo.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TMElyralab/Comfyui-MusePose/assets/114042542/9cd8b9b8-6876-4281-b7a0-a7fbcb2de7e1&#34;&gt;https://github.com/TMElyralab/Comfyui-MusePose/assets/114042542/9cd8b9b8-6876-4281-b7a0-a7fbcb2de7e1&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>onuratakan/gpt-computer-assistant</title>
    <updated>2024-06-04T01:33:50Z</updated>
    <id>tag:github.com,2024-06-04:/onuratakan/gpt-computer-assistant</id>
    <link href="https://github.com/onuratakan/gpt-computer-assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;gpt-4o for windows, macos and ubuntu&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPT Computer Assistant&lt;/h1&gt; &#xA;&lt;p&gt;Hi, this is an alternative work for providing ChatGPT MacOS app to Windows and Linux. In this way this is a fresh and stable work. You can easily install as Python library for this time but we will prepare a pipeline to providing native install scripts (.exe).&lt;/p&gt; &#xA;&lt;p&gt;Powered by &lt;a href=&#34;https://github.com/Upsonic/Tiger&#34;&gt;Upsonic Tiger üêÖ&lt;/a&gt; An function hub for llm agents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qApFmWMt8x&#34;&gt;&lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/Discord-Join?style=social&amp;amp;logo=discord&#34; width=&#34;150&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video (1 min)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/082a4ee7-e09f-4426-9059-e49b353976a0&#34;&gt;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/082a4ee7-e09f-4426-9059-e49b353976a0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation &amp;amp;&amp;amp; Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip3 install gpt-computer-assistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;computerassistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profile Selection, different profiles have fully different message histories. Just like: work, personal, upsonic.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;computerassistant --profile personal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip3 install gpt-computer-assistant --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;At this time we have many infrastructure element. We just aim to provide whole thinks that already in ChatGPT app.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Screen Read&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Microphone&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;System Audio&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open and close app&lt;/li&gt; &#xA; &lt;li&gt;Open a url&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clipboard&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Search Engines&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python and SH Interpreters&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Writing and Running Scripts&lt;/li&gt; &#xA; &lt;li&gt;Using your Telegram Account&lt;/li&gt; &#xA; &lt;li&gt;Knowledge Management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Todo&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Reset Option&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Splitting long audios. (Whisper api just support &amp;lt;20mb)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Text input area&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Just text mode (no voice answer)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Added different profiles&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;More Effect&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Facial analyze with &lt;a href=&#34;https://github.com/serengil/deepface&#34;&gt;DeepFace&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Windows .exe&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Linux native&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;MacOS native&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use cases&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/b4a4f11e-5588-4656-b5d7-b612a9a2855b&#34; alt=&#34;Take Meeting Notes&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/49eeac70-b33a-4ec4-8125-64127621ed62&#34; alt=&#34;Daily Assistant&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/10b69a18-033c-4d81-8ac9-f4e3c65b59c3&#34; alt=&#34;Read Docs&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/0f483bae-ffaf-4311-8653-c0dc64fb5ebe&#34; alt=&#34;Coding Assistant&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/onuratakan/gpt-computer-assistant/assets/41792982/91fe041d-4a3f-4a6e-9294-20ce2fa1ca36&#34; alt=&#34;options&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;** After first click to an option that include microphone or system audio you need to stop with another click to same option.&lt;/p&gt; &#xA;&lt;h3&gt;About Macos&lt;/h3&gt; &#xA;&lt;p&gt;For now we have some problems with &lt;strong&gt;intel&lt;/strong&gt;, but we will solve it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/onuratakan/gpt-computer-assistant/issues/10&#34;&gt;https://github.com/onuratakan/gpt-computer-assistant/issues/10&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>isaac-sim/IsaacLab</title>
    <updated>2024-06-04T01:33:50Z</updated>
    <id>tag:github.com,2024-06-04:/isaac-sim/IsaacLab</id>
    <link href="https://github.com/isaac-sim/IsaacLab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified framework for robot learning built on NVIDIA Isaac Sim&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/source/_static/isaaclab.jpg&#34; alt=&#34;Isaac Lab&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Isaac Lab&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IsaacSim-4.0-silver.svg?sanitize=true&#34; alt=&#34;IsaacSim&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.python.org/3/whatsnew/3.10.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10-blue.svg?sanitize=true&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://releases.ubuntu.com/20.04/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux--64-orange.svg?sanitize=true&#34; alt=&#34;Linux platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pre-commit.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://isaac-sim.github.io/IsaacLab&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-passing-brightgreen.svg?sanitize=true&#34; alt=&#34;Docs status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD--3-yellow.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Isaac Lab&lt;/strong&gt; is a unified and modular framework for robot learning that aims to simplify common workflows in robotics research (such as RL, learning from demonstrations, and motion planning). It is built upon &lt;a href=&#34;https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html&#34;&gt;NVIDIA Isaac Sim&lt;/a&gt; to leverage the latest simulation capabilities for photo-realistic scenes and fast and accurate simulation.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://isaac-sim.github.io/IsaacLab&#34;&gt;documentation page&lt;/a&gt; to learn more about the installation steps, features, tutorials, and how to set up your project with Isaac Lab.&lt;/p&gt; &#xA;&lt;h2&gt;Announcements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[17.04.2024] &lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/releases/tag/v0.3.0&#34;&gt;&lt;strong&gt;v0.3.0&lt;/strong&gt;&lt;/a&gt;: Several improvements and bug fixes to the framework. Includes cabinet opening and dexterous manipulation environments, terrain-aware patch sampling, and animation recording.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[22.12.2023] &lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/releases/tag/v0.2.0&#34;&gt;&lt;strong&gt;v0.2.0&lt;/strong&gt;&lt;/a&gt;: Significant breaking updates to enhance the modularity and user-friendliness of the framework. Also includes procedural terrain generation, warp-based custom ray-casters, and legged-locomotion environments.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing to Isaac Lab&lt;/h2&gt; &#xA;&lt;p&gt;We wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone. These may happen as bug reports, feature requests, or code contributions. For details, please check our &lt;a href=&#34;https://isaac-sim.github.io/IsaacLab/source/refs/contributing.html&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://isaac-sim.github.io/IsaacLab/source/refs/troubleshooting.html&#34;&gt;troubleshooting&lt;/a&gt; section for common fixes or &lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/issues&#34;&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For issues related to Isaac Sim, we recommend checking its &lt;a href=&#34;https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/overview.html&#34;&gt;documentation&lt;/a&gt; or opening a question on its &lt;a href=&#34;https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67&#34;&gt;forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please use GitHub &lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/discussions&#34;&gt;Discussions&lt;/a&gt; for discussing ideas, asking questions, and requests for new features.&lt;/li&gt; &#xA; &lt;li&gt;Github &lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/issues&#34;&gt;Issues&lt;/a&gt; should only be used to track executable pieces of work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features, or general updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA Isaac Sim is available freely under &lt;a href=&#34;https://www.nvidia.com/en-us/omniverse/download/&#34;&gt;individual license&lt;/a&gt;. For more information about its license terms, please check &lt;a href=&#34;https://docs.omniverse.nvidia.com/app_isaacsim/common/NVIDIA_Omniverse_License_Agreement.html#software-support-supplement&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Isaac Lab framework is released under &lt;a href=&#34;https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE&#34;&gt;BSD-3 License&lt;/a&gt;. The license files of its dependencies and assets are present in the &lt;a href=&#34;https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses&#34;&gt;&lt;code&gt;docs/licenses&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt;</summary>
  </entry>
</feed>