<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-07T01:36:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>WZMIAOMIAO/deep-learning-for-image-processing</title>
    <updated>2022-09-07T01:36:42Z</updated>
    <id>tag:github.com,2022-09-07:/WZMIAOMIAO/deep-learning-for-image-processing</id>
    <link href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing" rel="alternate"></link>
    <summary type="html">&lt;p&gt;deep learning for image processing including classification and object-detection etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨æ•™ç¨‹&lt;/h1&gt; &#xA;&lt;h2&gt;å‰è¨€&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š&lt;br&gt; 1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹&lt;br&gt; 2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ&lt;br&gt; 3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ&lt;/li&gt; &#xA; &lt;li&gt;è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨&lt;code&gt;course_ppt&lt;/code&gt;æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ•™ç¨‹ç›®å½•ï¼Œç‚¹å‡»è·³è½¬ç›¸åº”è§†é¢‘ï¼ˆåæœŸä¼šæ ¹æ®å­¦ä¹ å†…å®¹å¢åŠ ï¼‰&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;å›¾åƒåˆ†ç±»&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;LeNetï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV187411T7Ye&#34;&gt;Pytorchå®˜æ–¹demo(Lenet)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1n7411T7o6&#34;&gt;Tensorflow2å®˜æ–¹demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;AlexNetï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1p7411T7Pc&#34;&gt;AlexNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1W7411T7qc&#34;&gt;Pytorchæ­å»ºAlexNet&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1s7411T7vs&#34;&gt;Tensorflow2æ­å»ºAlexnet&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;VggNetï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1q7411T7Y6&#34;&gt;VggNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1i7411T7ZN&#34;&gt;Pytorchæ­å»ºVGGç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1q7411T76b&#34;&gt;Tensorflow2æ­å»ºVGGç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;GoogLeNetï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1z7411T7ie&#34;&gt;GoogLeNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1r7411T7M5&#34;&gt;Pytorchæ­å»ºGoogLeNetç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1a7411T7Ht&#34;&gt;Tensorflow2æ­å»ºGoogLeNetç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ResNetï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1T7411T7wa&#34;&gt;ResNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV14E411H7Uw&#34;&gt;Pytorchæ­å»ºResNetç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1WE41177Ya&#34;&gt;Tensorflow2æ­å»ºResNetç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ResNeXt (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Ap4y1p71v/&#34;&gt;ResNeXtç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1rX4y1N7tE&#34;&gt;Pytorchæ­å»ºResNeXtç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;MobileNet_V1_V2ï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1yE411p7L7&#34;&gt;MobileNet_V1_V2ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1qE411T7qZ&#34;&gt;Pytorchæ­å»ºMobileNetV2ç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1NE411K7tX&#34;&gt;Tensorflow2æ­å»ºMobileNetV2ç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;MobileNet_V3ï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1GK4y1p7uE&#34;&gt;MobileNet_V3ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1zT4y1P7pd&#34;&gt;Pytorchæ­å»ºMobileNetV3ç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1KA411g7wX&#34;&gt;Tensorflow2æ­å»ºMobileNetV3ç½‘ç»œ&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ShuffleNet_V1_V2 (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV15y4y1Y7SY&#34;&gt;ShuffleNet_V1_V2ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1dh411r76X&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºShuffleNetV2&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1kr4y1N7bh&#34;&gt;ä½¿ç”¨Tensorflow2æ­å»ºShuffleNetV2&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;EfficientNet_V1ï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1XK4y1U7PX&#34;&gt;EfficientNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19z4y1179h/&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºEfficientNet&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1PK4y1S7Jf&#34;&gt;ä½¿ç”¨Tensorflow2æ­å»ºEfficientNet&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;EfficientNet_V2 (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/NDR7Ug&#34;&gt;EfficientNetV2ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/M4hagB&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºEfficientNetV2&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/KUPbdr&#34;&gt;ä½¿ç”¨Tensorflowæ­å»ºEfficientNetV2&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;RepVGGï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV15f4y1o7QR&#34;&gt;RepVGGç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Vision Transformer(å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/gucpvt&#34;&gt;Multi-Head Attentionè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Jh411Y7WQ&#34;&gt;Vision Transformerç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/TT4VBM&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºVision Transformer&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1q64y1X7GY&#34;&gt;ä½¿ç”¨tensorflow2æ­å»ºVision Transformer&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Swin Transformer(å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1pL4y1v7jC&#34;&gt;Swin Transformerç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/vZnpJf&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºSwin Transformer&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/UHLMSF&#34;&gt;ä½¿ç”¨Tensorflow2æ­å»ºSwin Transformer&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ConvNeXt(å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1SS4y157fu&#34;&gt;ConvNeXtç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/gzpCv5z&#34;&gt;ä½¿ç”¨Pytorchæ­å»ºConvNeXt&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/zikVoch&#34;&gt;ä½¿ç”¨Tensorflow2æ­å»ºConvNeXt&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;MobileViT(è§„åˆ’ä¸­)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç›®æ ‡æ£€æµ‹&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Faster-RCNN/FPNï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1af4y1m7iL&#34;&gt;Faster-RCNNç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/Qhn6xA&#34;&gt;FPNç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1of4y1m7nj&#34;&gt;Faster-RCNNæºç è§£æ(Pytorch)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;SSD/RetinaNet (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1fT4y1L7Gi&#34;&gt;SSDç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/ZYCfd2&#34;&gt;RetinaNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vK411H771&#34;&gt;SSDæºç è§£æ(Pytorch)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;YOLO Series (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1yi4y1g7ro&#34;&gt;YOLOç³»åˆ—ç½‘ç»œè®²è§£(V1~V3)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1t54y1C7ra&#34;&gt;YOLOv3 SPPæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/WLptQ7Q&#34;&gt;YOLOV4ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1T3411p7zR&#34;&gt;YOLOV5ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1JW4y1k76c&#34;&gt;YOLOX ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;FCOSï¼ˆå·²å®Œæˆï¼‰&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1G5411X7jw&#34;&gt;FCOSç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¯­ä¹‰åˆ†å‰²&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;FCN (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1J3411C7zd&#34;&gt;FCNç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19q4y1971Q&#34;&gt;FCNæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;DeepLabV3 (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1SU4y1N7Ao&#34;&gt;DeepLabV1ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1gP4y1G7TC&#34;&gt;DeepLabV2ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Jb4y1q7j7&#34;&gt;DeepLabV3ç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y1c7Wx&#34;&gt;DeepLabV3æºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;LR-ASPP (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1LS4y1M76E&#34;&gt;LR-ASPPç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/bv13D4y1F7ML&#34;&gt;LR-ASPPæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;U-Net (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Vq4y127fB/&#34;&gt;U-Netç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://b23.tv/PCJJmqN&#34;&gt;U-Netæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;U2Net (å·²å®Œæˆ)&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1yB4y1z7mj&#34;&gt;U2Netç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Kt4y137iS&#34;&gt;U2Netæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;SegFormer(è§„åˆ’ä¸­)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®ä¾‹åˆ†å‰²&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mask R-CNNï¼ˆå·²å®Œæˆï¼‰ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1ZY411774T&#34;&gt;Mask R-CNNç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1hY411E7wD&#34;&gt;Mask R-CNNæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å…³é”®ç‚¹æ£€æµ‹&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;HRNetï¼ˆå·²å®Œæˆï¼‰ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1bB4y1y7qP&#34;&gt;HRNetç½‘ç»œè®²è§£&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1ar4y157JM&#34;&gt;HRNetæºç è§£æ(Pytorchç‰ˆ)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://space.bilibili.com/18161609/channel/index&#34;&gt;æ›´å¤šç›¸å…³è§†é¢‘è¯·è¿›å…¥æˆ‘çš„bilibilié¢‘é“æŸ¥çœ‹&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;æ‰€éœ€ç¯å¢ƒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anaconda3ï¼ˆå»ºè®®ä½¿ç”¨ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;python3.6/3.7/3.8&lt;/li&gt; &#xA; &lt;li&gt;pycharm (IDE)&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.10 (pip package)&lt;/li&gt; &#xA; &lt;li&gt;torchvision 0.11.1 (pip package)&lt;/li&gt; &#xA; &lt;li&gt;tensorflow 2.4.1 (pip package)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ¬¢è¿å¤§å®¶å…³æ³¨ä¸‹æˆ‘çš„å¾®ä¿¡å…¬ä¼—å·ï¼ˆ&lt;strong&gt;é˜¿å–†å­¦ä¹ å°è®°&lt;/strong&gt;ï¼‰ï¼Œå¹³æ—¶ä¼šæ€»ç»“äº›ç›¸å…³å­¦ä¹ åšæ–‡ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜ï¼Œä¹Ÿå¯ä»¥åˆ°æˆ‘çš„CSDNä¸­ä¸€èµ·è®¨è®ºã€‚ &lt;a href=&#34;https://blog.csdn.net/qq_37541097/article/details/103482003&#34;&gt;https://blog.csdn.net/qq_37541097/article/details/103482003&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘çš„bilibilié¢‘é“ï¼š &lt;a href=&#34;https://space.bilibili.com/18161609/channel/index&#34;&gt;https://space.bilibili.com/18161609/channel/index&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lucidrains/vit-pytorch</title>
    <updated>2022-09-07T01:36:42Z</updated>
    <id>tag:github.com,2022-09-07:/lucidrains/vit-pytorch</id>
    <link href="https://github.com/lucidrains/vit-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/vit.gif&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#vision-transformer---pytorch&#34;&gt;Vision Transformer - Pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#simple-vit&#34;&gt;Simple ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#distillation&#34;&gt;Distillation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#deep-vit&#34;&gt;Deep ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#cait&#34;&gt;CaiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#token-to-token-vit&#34;&gt;Token-to-Token ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#cct&#34;&gt;CCT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#cross-vit&#34;&gt;Cross ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#pit&#34;&gt;PiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#levit&#34;&gt;LeViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#cvt&#34;&gt;CvT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#twins-svt&#34;&gt;Twins SVT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#crossformer&#34;&gt;CrossFormer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#regionvit&#34;&gt;RegionViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#scalablevit&#34;&gt;ScalableViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#sepvit&#34;&gt;SepViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#maxvit&#34;&gt;MaxViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#nest&#34;&gt;NesT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#mobilevit&#34;&gt;MobileViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#masked-autoencoder&#34;&gt;Masked Autoencoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#simple-masked-image-modeling&#34;&gt;Simple Masked Image Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#masked-patch-prediction&#34;&gt;Masked Patch Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#adaptive-token-sampling&#34;&gt;Adaptive Token Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#patch-merger&#34;&gt;Patch Merger&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#vision-transformer-for-small-datasets&#34;&gt;Vision Transformer for Small Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#parallel-vit&#34;&gt;Parallel ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#learnable-memory-vit&#34;&gt;Learnable Memory ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#dino&#34;&gt;Dino&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#esvit&#34;&gt;EsViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#accessing-attention&#34;&gt;Accessing Attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#research-ideas&#34;&gt;Research Ideas&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#efficient-attention&#34;&gt;Efficient Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#combining-with-other-transformer-improvements&#34;&gt;Combining with other Transformer improvements&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/#citations&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Vision Transformer - Pytorch&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://openreview.net/pdf?id=YicbFdNTTy&#34;&gt;Vision Transformer&lt;/a&gt;, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in &lt;a href=&#34;https://www.youtube.com/watch?v=TrdevFK_am4&#34;&gt;Yannic Kilcher&#39;s&lt;/a&gt; video. There&#39;s really not much to code here, but may as well lay it out for everyone so we expedite the attention revolution.&lt;/p&gt; &#xA;&lt;p&gt;For a Pytorch implementation with pretrained models, please see Ross Wightman&#39;s repository &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The official Jax repository is &lt;a href=&#34;https://github.com/google-research/vision_transformer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A tensorflow2 translation also exists &lt;a href=&#34;https://github.com/taki0112/vit-tensorflow&#34;&gt;here&lt;/a&gt;, created by research scientist &lt;a href=&#34;https://github.com/taki0112&#34;&gt;Junho Kim&lt;/a&gt;! ğŸ™&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/conceptofmind/vit-flax&#34;&gt;Flax translation&lt;/a&gt; by &lt;a href=&#34;https://github.com/conceptofmind&#34;&gt;Enrico Shippole&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install vit-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Parameters&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;image_size&lt;/code&gt;: int.&lt;br&gt; Image size. If you have rectangular images, make sure your image size is the maximum of the width and height&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;patch_size&lt;/code&gt;: int.&lt;br&gt; Number of patches. &lt;code&gt;image_size&lt;/code&gt; must be divisible by &lt;code&gt;patch_size&lt;/code&gt;.&lt;br&gt; The number of patches is: &lt;code&gt; n = (image_size // patch_size) ** 2&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; &lt;strong&gt;must be greater than 16&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_classes&lt;/code&gt;: int.&lt;br&gt; Number of classes to classify.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dim&lt;/code&gt;: int.&lt;br&gt; Last dimension of output tensor after linear transformation &lt;code&gt;nn.Linear(..., dim)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;depth&lt;/code&gt;: int.&lt;br&gt; Number of Transformer blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;heads&lt;/code&gt;: int.&lt;br&gt; Number of heads in Multi-head Attention layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mlp_dim&lt;/code&gt;: int.&lt;br&gt; Dimension of the MLP (FeedForward) layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;channels&lt;/code&gt;: int, default &lt;code&gt;3&lt;/code&gt;.&lt;br&gt; Number of image&#39;s channels.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dropout&lt;/code&gt;: float between &lt;code&gt;[0, 1]&lt;/code&gt;, default &lt;code&gt;0.&lt;/code&gt;.&lt;br&gt; Dropout rate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;emb_dropout&lt;/code&gt;: float between &lt;code&gt;[0, 1]&lt;/code&gt;, default &lt;code&gt;0&lt;/code&gt;.&lt;br&gt; Embedding dropout rate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pool&lt;/code&gt;: string, either &lt;code&gt;cls&lt;/code&gt; token pooling or &lt;code&gt;mean&lt;/code&gt; pooling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Simple ViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01580&#34;&gt;An update&lt;/a&gt; from some of the same authors of the original paper proposes simplifications to &lt;code&gt;ViT&lt;/code&gt; that allows it to train faster and better.&lt;/p&gt; &#xA;&lt;p&gt;Among these simplifications include 2d sinusoidal positional embedding, global average pooling (no CLS token), no dropout, batch sizes of 1024 rather than 4096, and use of RandAugment and MixUp augmentations. They also show that a simple linear at the end is not significantly worse than the original MLP head&lt;/p&gt; &#xA;&lt;p&gt;You can use it by importing the &lt;code&gt;SimpleViT&lt;/code&gt; as shown below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import SimpleViT&#xA;&#xA;v = SimpleViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Distillation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/distill.png&#34; width=&#34;300px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A recent &lt;a href=&#34;https://arxiv.org/abs/2012.12877&#34;&gt;paper&lt;/a&gt; has shown that use of a distillation token for distilling knowledge from convolutional nets to vision transformer can yield small and efficient vision transformers. This repository offers the means to do distillation easily.&lt;/p&gt; &#xA;&lt;p&gt;ex. distilling from Resnet50 (or any teacher) to a vision transformer&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from torchvision.models import resnet50&#xA;&#xA;from vit_pytorch.distill import DistillableViT, DistillWrapper&#xA;&#xA;teacher = resnet50(pretrained = True)&#xA;&#xA;v = DistillableViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;distiller = DistillWrapper(&#xA;    student = v,&#xA;    teacher = teacher,&#xA;    temperature = 3,           # temperature of distillation&#xA;    alpha = 0.5,               # trade between main loss and distillation loss&#xA;    hard = False               # whether to use soft or hard distillation&#xA;)&#xA;&#xA;img = torch.randn(2, 3, 256, 256)&#xA;labels = torch.randint(0, 1000, (2,))&#xA;&#xA;loss = distiller(img, labels)&#xA;loss.backward()&#xA;&#xA;# after lots of training above ...&#xA;&#xA;pred = v(img) # (2, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;DistillableViT&lt;/code&gt; class is identical to &lt;code&gt;ViT&lt;/code&gt; except for how the forward pass is handled, so you should be able to load the parameters back to &lt;code&gt;ViT&lt;/code&gt; after you have completed distillation training.&lt;/p&gt; &#xA;&lt;p&gt;You can also use the handy &lt;code&gt;.to_vit&lt;/code&gt; method on the &lt;code&gt;DistillableViT&lt;/code&gt; instance to get back a &lt;code&gt;ViT&lt;/code&gt; instance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;v = v.to_vit()&#xA;type(v) # &amp;lt;class &#39;vit_pytorch.vit_pytorch.ViT&#39;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deep ViT&lt;/h2&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2103.11886&#34;&gt;paper&lt;/a&gt; notes that ViT struggles to attend at greater depths (past 12 layers), and suggests mixing the attention of each head post-softmax as a solution, dubbed Re-attention. The results line up with the &lt;a href=&#34;https://github.com/lucidrains/x-transformers#talking-heads-attention&#34;&gt;Talking Heads&lt;/a&gt; paper from NLP.&lt;/p&gt; &#xA;&lt;p&gt;You can use it as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.deepvit import DeepViT&#xA;&#xA;v = DeepViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CaiT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.17239&#34;&gt;This paper&lt;/a&gt; also notes difficulty in training vision transformers at greater depths and proposes two solutions. First it proposes to do per-channel multiplication of the output of the residual block. Second, it proposes to have the patches attend to one another, and only allow the CLS token to attend to the patches in the last few layers.&lt;/p&gt; &#xA;&lt;p&gt;They also add &lt;a href=&#34;https://github.com/lucidrains/x-transformers#talking-heads-attention&#34;&gt;Talking Heads&lt;/a&gt;, noting improvements&lt;/p&gt; &#xA;&lt;p&gt;You can use this scheme as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cait import CaiT&#xA;&#xA;v = CaiT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 12,             # depth of transformer for patch to patch attention only&#xA;    cls_depth = 2,          # depth of cross attention of CLS tokens to patch&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1,&#xA;    layer_dropout = 0.05    # randomly dropout 5% of the layers&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Token-to-Token ViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/t2t.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.11986&#34;&gt;This paper&lt;/a&gt; proposes that the first couple layers should downsample the image sequence by unfolding, leading to overlapping image data in each token as shown in the figure above. You can use this variant of the &lt;code&gt;ViT&lt;/code&gt; as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.t2t import T2TViT&#xA;&#xA;v = T2TViT(&#xA;    dim = 512,&#xA;    image_size = 224,&#xA;    depth = 5,&#xA;    heads = 8,&#xA;    mlp_dim = 512,&#xA;    num_classes = 1000,&#xA;    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CCT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.05704&#34;&gt;CCT&lt;/a&gt; proposes compact transformers by using convolutions instead of patching and performing sequence pooling. This allows for CCT to have high accuracy and a low number of parameters.&lt;/p&gt; &#xA;&lt;p&gt;You can use this with two methods&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cct import CCT&#xA;&#xA;cct = CCT(&#xA;    img_size = (224, 448),&#xA;    embedding_dim = 384,&#xA;    n_conv_layers = 2,&#xA;    kernel_size = 7,&#xA;    stride = 2,&#xA;    padding = 3,&#xA;    pooling_kernel_size = 3,&#xA;    pooling_stride = 2,&#xA;    pooling_padding = 1,&#xA;    num_layers = 14,&#xA;    num_heads = 6,&#xA;    mlp_radio = 3.,&#xA;    num_classes = 1000,&#xA;    positional_embedding = &#39;learnable&#39;, # [&#39;sine&#39;, &#39;learnable&#39;, &#39;none&#39;]&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 448)&#xA;pred = cct(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can use one of several pre-defined models &lt;code&gt;[2,4,6,7,8,14,16]&lt;/code&gt; which pre-define the number of layers, number of attention heads, the mlp ratio, and the embedding dimension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cct import cct_14&#xA;&#xA;cct = cct_14(&#xA;    img_size = 224,&#xA;    n_conv_layers = 1,&#xA;    kernel_size = 7,&#xA;    stride = 2,&#xA;    padding = 3,&#xA;    pooling_kernel_size = 3,&#xA;    pooling_stride = 2,&#xA;    pooling_padding = 1,&#xA;    num_classes = 1000,&#xA;    positional_embedding = &#39;learnable&#39;, # [&#39;sine&#39;, &#39;learnable&#39;, &#39;none&#39;]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SHI-Labs/Compact-Transformers&#34;&gt;Official Repository&lt;/a&gt; includes links to pretrained model checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Cross ViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/cross_vit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.14899&#34;&gt;This paper&lt;/a&gt; proposes to have two vision transformers processing the image at different scales, cross attending to one every so often. They show improvements on top of the base vision transformer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cross_vit import CrossViT&#xA;&#xA;v = CrossViT(&#xA;    image_size = 256,&#xA;    num_classes = 1000,&#xA;    depth = 4,               # number of multi-scale encoding blocks&#xA;    sm_dim = 192,            # high res dimension&#xA;    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)&#xA;    sm_enc_depth = 2,        # high res depth&#xA;    sm_enc_heads = 8,        # high res heads&#xA;    sm_enc_mlp_dim = 2048,   # high res feedforward dimension&#xA;    lg_dim = 384,            # low res dimension&#xA;    lg_patch_size = 64,      # low res patch size&#xA;    lg_enc_depth = 3,        # low res depth&#xA;    lg_enc_heads = 8,        # low res heads&#xA;    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions&#xA;    cross_attn_depth = 2,    # cross attention rounds&#xA;    cross_attn_heads = 8,    # cross attention heads&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;pred = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;PiT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/pit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.16302&#34;&gt;This paper&lt;/a&gt; proposes to downsample the tokens through a pooling procedure using depth-wise convolutions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.pit import PiT&#xA;&#xA;v = PiT(&#xA;    image_size = 224,&#xA;    patch_size = 14,&#xA;    dim = 256,&#xA;    num_classes = 1000,&#xA;    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;# forward pass now returns predictions and the attention maps&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LeViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/levit.png&#34; width=&#34;300px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.01136&#34;&gt;This paper&lt;/a&gt; proposes a number of changes, including (1) convolutional embedding instead of patch-wise projection (2) downsampling in stages (3) extra non-linearity in attention (4) 2d relative positional biases instead of initial absolute positional bias (5) batchnorm in place of layernorm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/LeViT&#34;&gt;Official repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.levit import LeViT&#xA;&#xA;levit = LeViT(&#xA;    image_size = 224,&#xA;    num_classes = 1000,&#xA;    stages = 3,             # number of stages&#xA;    dim = (256, 384, 512),  # dimensions at each stage&#xA;    depth = 4,              # transformer of depth 4 at each stage&#xA;    heads = (4, 6, 8),      # heads at each stage&#xA;    mlp_mult = 2,&#xA;    dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;levit(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CvT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/cvt.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.15808&#34;&gt;This paper&lt;/a&gt; proposes mixing convolutions and attention. Specifically, convolutions are used to embed and downsample the image / feature map in three stages. Depthwise-convoltion is also used to project the queries, keys, and values for attention.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cvt import CvT&#xA;&#xA;v = CvT(&#xA;    num_classes = 1000,&#xA;    s1_emb_dim = 64,        # stage 1 - dimension&#xA;    s1_emb_kernel = 7,      # stage 1 - conv kernel&#xA;    s1_emb_stride = 4,      # stage 1 - conv stride&#xA;    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size&#xA;    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride&#xA;    s1_heads = 1,           # stage 1 - heads&#xA;    s1_depth = 1,           # stage 1 - depth&#xA;    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor&#xA;    s2_emb_dim = 192,       # stage 2 - (same as above)&#xA;    s2_emb_kernel = 3,&#xA;    s2_emb_stride = 2,&#xA;    s2_proj_kernel = 3,&#xA;    s2_kv_proj_stride = 2,&#xA;    s2_heads = 3,&#xA;    s2_depth = 2,&#xA;    s2_mlp_mult = 4,&#xA;    s3_emb_dim = 384,       # stage 3 - (same as above)&#xA;    s3_emb_kernel = 3,&#xA;    s3_emb_stride = 2,&#xA;    s3_proj_kernel = 3,&#xA;    s3_kv_proj_stride = 2,&#xA;    s3_heads = 4,&#xA;    s3_depth = 10,&#xA;    s3_mlp_mult = 4,&#xA;    dropout = 0.&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;pred = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Twins SVT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/twins_svt.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2104.13840&#34;&gt;paper&lt;/a&gt; proposes mixing local and global attention, along with position encoding generator (proposed in &lt;a href=&#34;https://arxiv.org/abs/2102.10882&#34;&gt;CPVT&lt;/a&gt;) and global average pooling, to achieve the same results as &lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34;&gt;Swin&lt;/a&gt;, without the extra complexity of shifted windows, CLS tokens, nor positional embeddings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.twins_svt import TwinsSVT&#xA;&#xA;model = TwinsSVT(&#xA;    num_classes = 1000,       # number of output classes&#xA;    s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension&#xA;    s1_patch_size = 4,        # stage 1 - patch size for patch embedding&#xA;    s1_local_patch_size = 7,  # stage 1 - patch size for local attention&#xA;    s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper&#xA;    s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -&amp;gt; ff -&amp;gt; global attn -&amp;gt; ff)&#xA;    s2_emb_dim = 128,         # stage 2 (same as above)&#xA;    s2_patch_size = 2,&#xA;    s2_local_patch_size = 7,&#xA;    s2_global_k = 7,&#xA;    s2_depth = 1,&#xA;    s3_emb_dim = 256,         # stage 3 (same as above)&#xA;    s3_patch_size = 2,&#xA;    s3_local_patch_size = 7,&#xA;    s3_global_k = 7,&#xA;    s3_depth = 5,&#xA;    s4_emb_dim = 512,         # stage 4 (same as above)&#xA;    s4_patch_size = 2,&#xA;    s4_local_patch_size = 7,&#xA;    s4_global_k = 7,&#xA;    s4_depth = 4,&#xA;    peg_kernel_size = 3,      # positional encoding generator kernel size&#xA;    dropout = 0.              # dropout&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;pred = model(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RegionViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/regionvit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/regionvit2.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02689&#34;&gt;This paper&lt;/a&gt; proposes to divide up the feature map into local regions, whereby the local tokens attend to each other. Each local region has its own regional token which then attends to all its local tokens, as well as other regional tokens.&lt;/p&gt; &#xA;&lt;p&gt;You can use it as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.regionvit import RegionViT&#xA;&#xA;model = RegionViT(&#xA;    dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage&#xA;    depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage&#xA;    window_size = 7,                # window size, which should be either 7 or 14&#xA;    num_classes = 1000,             # number of output classes&#xA;    tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models&#xA;    use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;pred = model(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CrossFormer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/crossformer.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/crossformer2.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2108.00154&#34;&gt;paper&lt;/a&gt; beats PVT and Swin using alternating local and global attention. The global attention is done across the windowing dimension for reduced complexity, much like the scheme used for axial attention.&lt;/p&gt; &#xA;&lt;p&gt;They also have cross-scale embedding layer, which they shown to be a generic layer that can improve all vision transformers. Dynamic relative positional bias was also formulated to allow the net to generalize to images of greater resolution.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.crossformer import CrossFormer&#xA;&#xA;model = CrossFormer(&#xA;    num_classes = 1000,                # number of output classes&#xA;    dim = (64, 128, 256, 512),         # dimension at each stage&#xA;    depth = (2, 2, 8, 2),              # depth of transformer at each stage&#xA;    global_window_size = (8, 4, 2, 1), # global window sizes at each stage&#xA;    local_window_size = 7,             # local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;pred = model(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ScalableViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/scalable-vit-1.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/scalable-vit-2.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This Bytedance AI &lt;a href=&#34;https://arxiv.org/abs/2203.10790&#34;&gt;paper&lt;/a&gt; proposes the Scalable Self Attention (SSA) and the Interactive Windowed Self Attention (IWSA) modules. The SSA alleviates the computation needed at earlier stages by reducing the key / value feature map by some factor (&lt;code&gt;reduction_factor&lt;/code&gt;), while modulating the dimension of the queries and keys (&lt;code&gt;ssa_dim_key&lt;/code&gt;). The IWSA performs self attention within local windows, similar to other vision transformer papers. However, they add a residual of the values, passed through a convolution of kernel size 3, which they named Local Interactive Module (LIM).&lt;/p&gt; &#xA;&lt;p&gt;They make the claim in this paper that this scheme outperforms Swin Transformer, and also demonstrate competitive performance against Crossformer.&lt;/p&gt; &#xA;&lt;p&gt;You can use it as follows (ex. ScalableViT-S)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.scalable_vit import ScalableViT&#xA;&#xA;model = ScalableViT(&#xA;    num_classes = 1000,&#xA;    dim = 64,                               # starting model dimension. at every stage, dimension is doubled&#xA;    heads = (2, 4, 8, 16),                  # number of attention heads at each stage&#xA;    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage&#xA;    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)&#xA;    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)&#xA;    window_size = (64, 32, None, None),     # window size of the IWSA at each stage. None means no windowing needed&#xA;    dropout = 0.1,                          # attention and feedforward dropout&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;preds = model(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;SepViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/sep-vit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Another &lt;a href=&#34;https://arxiv.org/abs/2203.15380&#34;&gt;Bytedance AI paper&lt;/a&gt;, it proposes a depthwise-pointwise self-attention layer that seems largely inspired by mobilenet&#39;s depthwise-separable convolution. The most interesting aspect is the reuse of the feature map from the depthwise self-attention stage as the values for the pointwise self-attention, as shown in the diagram above.&lt;/p&gt; &#xA;&lt;p&gt;I have decided to include only the version of &lt;code&gt;SepViT&lt;/code&gt; with this specific self-attention layer, as the grouped attention layers are not remarkable nor novel, and the authors were not clear on how they treated the window tokens for the group self-attention layer. Besides, it seems like with &lt;code&gt;DSSA&lt;/code&gt; layer alone, they were able to beat Swin.&lt;/p&gt; &#xA;&lt;p&gt;ex. SepViT-Lite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.sep_vit import SepViT&#xA;&#xA;v = SepViT(&#xA;    num_classes = 1000,&#xA;    dim = 32,               # dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite&#xA;    dim_head = 32,          # attention head dimension&#xA;    heads = (1, 2, 4, 8),   # number of heads per stage&#xA;    depth = (1, 2, 6, 2),   # number of transformer blocks per stage&#xA;    window_size = 7,        # window size of DSS Attention block&#xA;    dropout = 0.1           # dropout&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;MaxViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/max-vit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;This paper&lt;/a&gt; proposes a hybrid convolutional / attention network, using MBConv from the convolution side, and then block / grid axial sparse attention.&lt;/p&gt; &#xA;&lt;p&gt;They also claim this specific vision transformer is good for generative models (GANs).&lt;/p&gt; &#xA;&lt;p&gt;ex. MaxViT-S&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.max_vit import MaxViT&#xA;&#xA;v = MaxViT(&#xA;    num_classes = 1000,&#xA;    dim_conv_stem = 64,               # dimension of the convolutional stem, would default to dimension of first layer if not specified&#xA;    dim = 96,                         # dimension of first layer, doubles every layer&#xA;    dim_head = 32,                    # dimension of attention heads, kept at 32 in paper&#xA;    depth = (2, 2, 5, 2),             # number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention&#xA;    window_size = 7,                  # window size for block and grids&#xA;    mbconv_expansion_rate = 4,        # expansion rate of MBConv&#xA;    mbconv_shrinkage_rate = 0.25,     # shrinkage rate of squeeze-excitation in MBConv&#xA;    dropout = 0.1                     # dropout&#xA;)&#xA;&#xA;img = torch.randn(2, 3, 224, 224)&#xA;&#xA;preds = v(img) # (2, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;NesT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/nest.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2105.12723&#34;&gt;paper&lt;/a&gt; decided to process the image in hierarchical stages, with attention only within tokens of local blocks, which aggregate as it moves up the heirarchy. The aggregation is done in the image plane, and contains a convolution and subsequent maxpool to allow it to pass information across the boundary.&lt;/p&gt; &#xA;&lt;p&gt;You can use it with the following code (ex. NesT-T)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.nest import NesT&#xA;&#xA;nest = NesT(&#xA;    image_size = 224,&#xA;    patch_size = 4,&#xA;    dim = 96,&#xA;    heads = 3,&#xA;    num_hierarchies = 3,        # number of hierarchies&#xA;    block_repeats = (2, 2, 8),  # the number of transformer blocks at each heirarchy, starting from the bottom&#xA;    num_classes = 1000&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;pred = nest(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;MobileViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/mbvit.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2110.02178&#34;&gt;paper&lt;/a&gt; introduce MobileViT, a light-weight and general purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers.&lt;/p&gt; &#xA;&lt;p&gt;You can use it with the following code (ex. mobilevit_xs)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.mobile_vit import MobileViT&#xA;&#xA;mbvit_xs = MobileViT(&#xA;    image_size = (256, 256),&#xA;    dims = [96, 120, 144],&#xA;    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],&#xA;    num_classes = 1000&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;pred = mbvit_xs(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Simple Masked Image Modeling&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/simmim.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2111.09886&#34;&gt;paper&lt;/a&gt; proposes a simple masked image modeling (SimMIM) scheme, using only a linear projection off the masked tokens into pixel space followed by an L1 loss with the pixel values of the masked patches. Results are competitive with other more complicated approaches.&lt;/p&gt; &#xA;&lt;p&gt;You can use this as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT&#xA;from vit_pytorch.simmim import SimMIM&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048&#xA;)&#xA;&#xA;mim = SimMIM(&#xA;    encoder = v,&#xA;    masking_ratio = 0.5  # they found 50% to yield the best results&#xA;)&#xA;&#xA;images = torch.randn(8, 3, 256, 256)&#xA;&#xA;loss = mim(images)&#xA;loss.backward()&#xA;&#xA;# that&#39;s all!&#xA;# do the above in a for loop many times with a lot of images and your vision transformer will learn&#xA;&#xA;torch.save(v.state_dict(), &#39;./trained-vit.pt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Masked Autoencoder&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/mae.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;A new &lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;Kaiming He paper&lt;/a&gt; proposes a simple autoencoder scheme where the vision transformer attends to a set of unmasked patches, and a smaller decoder tries to reconstruct the masked pixel values.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LKixq2S2Pz8&#34;&gt;DeepReader quick paper review&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Dp6iICL2dVI&#34;&gt;AI Coffeebreak with Letitia&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use it with the following code&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT, MAE&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048&#xA;)&#xA;&#xA;mae = MAE(&#xA;    encoder = v,&#xA;    masking_ratio = 0.75,   # the paper recommended 75% masked patches&#xA;    decoder_dim = 512,      # paper showed good results with just 512&#xA;    decoder_depth = 6       # anywhere from 1 to 8&#xA;)&#xA;&#xA;images = torch.randn(8, 3, 256, 256)&#xA;&#xA;loss = mae(images)&#xA;loss.backward()&#xA;&#xA;# that&#39;s all!&#xA;# do the above in a for loop many times with a lot of images and your vision transformer will learn&#xA;&#xA;# save your improved vision transformer&#xA;torch.save(v.state_dict(), &#39;./trained-vit.pt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Masked Patch Prediction&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/zankner&#34;&gt;Zach&lt;/a&gt;, you can train using the original masked patch prediction task presented in the paper, with the following code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT&#xA;from vit_pytorch.mpp import MPP&#xA;&#xA;model = ViT(&#xA;    image_size=256,&#xA;    patch_size=32,&#xA;    num_classes=1000,&#xA;    dim=1024,&#xA;    depth=6,&#xA;    heads=8,&#xA;    mlp_dim=2048,&#xA;    dropout=0.1,&#xA;    emb_dropout=0.1&#xA;)&#xA;&#xA;mpp_trainer = MPP(&#xA;    transformer=model,&#xA;    patch_size=32,&#xA;    dim=1024,&#xA;    mask_prob=0.15,          # probability of using token in masked prediction task&#xA;    random_patch_prob=0.30,  # probability of randomly replacing a token being used for mpp&#xA;    replace_prob=0.50,       # probability of replacing a token being used for mpp with the mask token&#xA;)&#xA;&#xA;opt = torch.optim.Adam(mpp_trainer.parameters(), lr=3e-4)&#xA;&#xA;def sample_unlabelled_images():&#xA;    return torch.FloatTensor(20, 3, 256, 256).uniform_(0., 1.)&#xA;&#xA;for _ in range(100):&#xA;    images = sample_unlabelled_images()&#xA;    loss = mpp_trainer(images)&#xA;    opt.zero_grad()&#xA;    loss.backward()&#xA;    opt.step()&#xA;&#xA;# save your improved network&#xA;torch.save(model.state_dict(), &#39;./pretrained-net.pt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adaptive Token Sampling&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/ats.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2111.15667&#34;&gt;paper&lt;/a&gt; proposes to use the CLS attention scores, re-weighed by the norms of the value heads, as means to discard unimportant tokens at different layers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.ats_vit import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    max_tokens_per_depth = (256, 128, 64, 32, 16, 8), # a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;&#xA;preds = v(img) # (4, 1000)&#xA;&#xA;# you can also get a list of the final sampled patch ids&#xA;# a value of -1 denotes padding&#xA;&#xA;preds, token_ids = v(img, return_sampled_token_ids = True) # (4, 1000), (4, &amp;lt;=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Patch Merger&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/patch_merger.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2202.12015&#34;&gt;paper&lt;/a&gt; proposes a simple module (Patch Merger) for reducing the number of tokens at any layer of a vision transformer without sacrificing performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit_with_patch_merger import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 12,&#xA;    heads = 8,&#xA;    patch_merge_layer = 6,        # at which transformer layer to do patch merging&#xA;    patch_merge_num_tokens = 8,   # the output number of tokens from the patch merge&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;&#xA;preds = v(img) # (4, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One can also use the &lt;code&gt;PatchMerger&lt;/code&gt; module by itself&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit_with_patch_merger import PatchMerger&#xA;&#xA;merger = PatchMerger(&#xA;    dim = 1024,&#xA;    num_tokens_out = 8   # output number of tokens&#xA;)&#xA;&#xA;features = torch.randn(4, 256, 1024) # (batch, num tokens, dimension)&#xA;&#xA;out = merger(features) # (4, 8, 1024)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Vision Transformer for Small Datasets&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/vit_for_small_datasets.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2112.13492&#34;&gt;paper&lt;/a&gt; proposes a new image to patch function that incorporates shifts of the image, before normalizing and dividing the image into patches. I have found shifting to be extremely helpful in some other transformers work, so decided to include this for further explorations. It also includes the &lt;code&gt;LSA&lt;/code&gt; with the learned temperature and masking out of a token&#39;s attention to itself.&lt;/p&gt; &#xA;&lt;p&gt;You can use as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit_for_small_dataset import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the &lt;code&gt;SPT&lt;/code&gt; from this paper as a standalone module&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit_for_small_dataset import SPT&#xA;&#xA;spt = SPT(&#xA;    dim = 1024,&#xA;    patch_size = 16,&#xA;    channels = 3&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;&#xA;tokens = spt(img) # (4, 256, 1024)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Parallel ViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/parallel-vit.png&#34; width=&#34;350px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2203.09795&#34;&gt;paper&lt;/a&gt; propose parallelizing multiple attention and feedforward blocks per layer (2 blocks), claiming that it is easier to train without loss of performance.&lt;/p&gt; &#xA;&lt;p&gt;You can try this variant as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.parallel_vit import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048,&#xA;    num_parallel_branches = 2,  # in paper, they claimed 2 was optimal&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;&#xA;preds = v(img) # (4, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Learnable Memory ViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/learnable-memory-vit.png&#34; width=&#34;350px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2203.15243&#34;&gt;paper&lt;/a&gt; shows that adding learnable memory tokens at each layer of a vision transformer can greatly enhance fine-tuning results (in addition to learnable task specific CLS token and adapter head).&lt;/p&gt; &#xA;&lt;p&gt;You can use this with a specially modified &lt;code&gt;ViT&lt;/code&gt; as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.learnable_memory_vit import ViT, Adapter&#xA;&#xA;# normal base ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(4, 3, 256, 256)&#xA;logits = v(img) # (4, 1000)&#xA;&#xA;# do your usual training with ViT&#xA;# ...&#xA;&#xA;&#xA;# then, to finetune, just pass the ViT into the Adapter class&#xA;# you can do this for multiple Adapters, as shown below&#xA;&#xA;adapter1 = Adapter(&#xA;    vit = v,&#xA;    num_classes = 2,               # number of output classes for this specific task&#xA;    num_memories_per_layer = 5     # number of learnable memories per layer, 10 was sufficient in paper&#xA;)&#xA;&#xA;logits1 = adapter1(img) # (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head&#xA;&#xA;# yet another task to finetune on, this time with 4 classes&#xA;&#xA;adapter2 = Adapter(&#xA;    vit = v,&#xA;    num_classes = 4,&#xA;    num_memories_per_layer = 10&#xA;)&#xA;&#xA;logits2 = adapter2(img) # (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dino&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/dino.png&#34; width=&#34;350px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train &lt;code&gt;ViT&lt;/code&gt; with the recent SOTA self-supervised learning technique, &lt;a href=&#34;https://arxiv.org/abs/2104.14294&#34;&gt;Dino&lt;/a&gt;, with the following code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=h3ij3F3cPIk&#34;&gt;Yannic Kilcher&lt;/a&gt; video&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT, Dino&#xA;&#xA;model = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    mlp_dim = 2048&#xA;)&#xA;&#xA;learner = Dino(&#xA;    model,&#xA;    image_size = 256,&#xA;    hidden_layer = &#39;to_latent&#39;,        # hidden layer name or index, from which to extract the embedding&#xA;    projection_hidden_size = 256,      # projector network hidden dimension&#xA;    projection_layers = 4,             # number of layers in projection network&#xA;    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)&#xA;    student_temp = 0.9,                # student temperature&#xA;    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs&#xA;    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper &#xA;    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper&#xA;    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok&#xA;    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok&#xA;)&#xA;&#xA;opt = torch.optim.Adam(learner.parameters(), lr = 3e-4)&#xA;&#xA;def sample_unlabelled_images():&#xA;    return torch.randn(20, 3, 256, 256)&#xA;&#xA;for _ in range(100):&#xA;    images = sample_unlabelled_images()&#xA;    loss = learner(images)&#xA;    opt.zero_grad()&#xA;    loss.backward()&#xA;    opt.step()&#xA;    learner.update_moving_average() # update moving average of teacher encoder and teacher centers&#xA;&#xA;# save your improved network&#xA;torch.save(model.state_dict(), &#39;./pretrained-net.pt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;EsViT&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/vit-pytorch/main/images/esvit.png&#34; width=&#34;350px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09785&#34;&gt;&lt;code&gt;EsViT&lt;/code&gt;&lt;/a&gt; is a variant of Dino (from above) re-engineered to support efficient &lt;code&gt;ViT&lt;/code&gt;s with patch merging / downsampling by taking into an account an extra regional loss between the augmented views. To quote the abstract, it &lt;code&gt;outperforms its supervised counterpart on 17 out of 18 datasets&lt;/code&gt; at 3 times higher throughput.&lt;/p&gt; &#xA;&lt;p&gt;Even though it is named as though it were a new &lt;code&gt;ViT&lt;/code&gt; variant, it actually is just a strategy for training any multistage &lt;code&gt;ViT&lt;/code&gt; (in the paper, they focused on Swin). The example below will show how to use it with &lt;code&gt;CvT&lt;/code&gt;. You&#39;ll need to set the &lt;code&gt;hidden_layer&lt;/code&gt; to the name of the layer within your efficient ViT that outputs the non-average pooled visual representations, just before the global pooling and projection to logits.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cvt import CvT&#xA;from vit_pytorch.es_vit import EsViTTrainer&#xA;&#xA;cvt = CvT(&#xA;    num_classes = 1000,&#xA;    s1_emb_dim = 64,&#xA;    s1_emb_kernel = 7,&#xA;    s1_emb_stride = 4,&#xA;    s1_proj_kernel = 3,&#xA;    s1_kv_proj_stride = 2,&#xA;    s1_heads = 1,&#xA;    s1_depth = 1,&#xA;    s1_mlp_mult = 4,&#xA;    s2_emb_dim = 192,&#xA;    s2_emb_kernel = 3,&#xA;    s2_emb_stride = 2,&#xA;    s2_proj_kernel = 3,&#xA;    s2_kv_proj_stride = 2,&#xA;    s2_heads = 3,&#xA;    s2_depth = 2,&#xA;    s2_mlp_mult = 4,&#xA;    s3_emb_dim = 384,&#xA;    s3_emb_kernel = 3,&#xA;    s3_emb_stride = 2,&#xA;    s3_proj_kernel = 3,&#xA;    s3_kv_proj_stride = 2,&#xA;    s3_heads = 4,&#xA;    s3_depth = 10,&#xA;    s3_mlp_mult = 4,&#xA;    dropout = 0.&#xA;)&#xA;&#xA;learner = EsViTTrainer(&#xA;    cvt,&#xA;    image_size = 256,&#xA;    hidden_layer = &#39;layers&#39;,           # hidden layer name or index, from which to extract the embedding&#xA;    projection_hidden_size = 256,      # projector network hidden dimension&#xA;    projection_layers = 4,             # number of layers in projection network&#xA;    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)&#xA;    student_temp = 0.9,                # student temperature&#xA;    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs&#xA;    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper&#xA;    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper&#xA;    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok&#xA;    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok&#xA;)&#xA;&#xA;opt = torch.optim.AdamW(learner.parameters(), lr = 3e-4)&#xA;&#xA;def sample_unlabelled_images():&#xA;    return torch.randn(8, 3, 256, 256)&#xA;&#xA;for _ in range(1000):&#xA;    images = sample_unlabelled_images()&#xA;    loss = learner(images)&#xA;    opt.zero_grad()&#xA;    loss.backward()&#xA;    opt.step()&#xA;    learner.update_moving_average() # update moving average of teacher encoder and teacher centers&#xA;&#xA;# save your improved network&#xA;torch.save(cvt.state_dict(), &#39;./pretrained-net.pt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Accessing Attention&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to visualize the attention weights (post-softmax) for your research, just follow the procedure below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;# import Recorder and wrap the ViT&#xA;&#xA;from vit_pytorch.recorder import Recorder&#xA;v = Recorder(v)&#xA;&#xA;# forward pass now returns predictions and the attention maps&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;preds, attns = v(img)&#xA;&#xA;# there is one extra patch due to the CLS token&#xA;&#xA;attns # (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to cleanup the class and the hooks once you have collected enough data&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;v = v.eject()  # wrapper is discarded and original ViT instance is returned&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Accessing Embeddings&lt;/h2&gt; &#xA;&lt;p&gt;You can similarly access the embeddings with the &lt;code&gt;Extractor&lt;/code&gt; wrapper&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.vit import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;# import Recorder and wrap the ViT&#xA;&#xA;from vit_pytorch.extractor import Extractor&#xA;v = Extractor(v)&#xA;&#xA;# forward pass now returns predictions and the attention maps&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;logits, embeddings = v(img)&#xA;&#xA;# there is one extra token due to the CLS token&#xA;&#xA;embeddings # (1, 65, 1024) - (batch x patches x model dim)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or say for &lt;code&gt;CrossViT&lt;/code&gt;, which has a multi-scale encoder that outputs two sets of embeddings for &#39;large&#39; and &#39;small&#39; scales&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.cross_vit import CrossViT&#xA;&#xA;v = CrossViT(&#xA;    image_size = 256,&#xA;    num_classes = 1000,&#xA;    depth = 4,&#xA;    sm_dim = 192,&#xA;    sm_patch_size = 16,&#xA;    sm_enc_depth = 2,&#xA;    sm_enc_heads = 8,&#xA;    sm_enc_mlp_dim = 2048,&#xA;    lg_dim = 384,&#xA;    lg_patch_size = 64,&#xA;    lg_enc_depth = 3,&#xA;    lg_enc_heads = 8,&#xA;    lg_enc_mlp_dim = 2048,&#xA;    cross_attn_depth = 2,&#xA;    cross_attn_heads = 8,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;# wrap the CrossViT&#xA;&#xA;from vit_pytorch.extractor import Extractor&#xA;v = Extractor(v, layer_name = &#39;multi_scale_encoder&#39;) # take embedding coming from the output of multi-scale-encoder&#xA;&#xA;# forward pass now returns predictions and the attention maps&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;logits, embeddings = v(img)&#xA;&#xA;# there is one extra token due to the CLS token&#xA;&#xA;embeddings # ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) &amp;lt;- large and small scales respectively&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Research Ideas&lt;/h2&gt; &#xA;&lt;h3&gt;Efficient Attention&lt;/h3&gt; &#xA;&lt;p&gt;There may be some coming from computer vision who think attention still suffers from quadratic costs. Fortunately, we have a lot of new techniques that may help. This repository offers a way for you to plugin your own sparse attention transformer.&lt;/p&gt; &#xA;&lt;p&gt;An example with &lt;a href=&#34;https://arxiv.org/abs/2102.03902&#34;&gt;Nystromformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install nystrom-attention&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.efficient import ViT&#xA;from nystrom_attention import Nystromformer&#xA;&#xA;efficient_transformer = Nystromformer(&#xA;    dim = 512,&#xA;    depth = 12,&#xA;    heads = 8,&#xA;    num_landmarks = 256&#xA;)&#xA;&#xA;v = ViT(&#xA;    dim = 512,&#xA;    image_size = 2048,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    transformer = efficient_transformer&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 2048, 2048) # your high resolution picture&#xA;v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other sparse attention frameworks I would highly recommend is &lt;a href=&#34;https://github.com/lucidrains/routing-transformer&#34;&gt;Routing Transformer&lt;/a&gt; or &lt;a href=&#34;https://github.com/lucidrains/sinkhorn-transformer&#34;&gt;Sinkhorn Transformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Combining with other Transformer improvements&lt;/h3&gt; &#xA;&lt;p&gt;This paper purposely used the most vanilla of attention networks to make a statement. If you would like to use some of the latest improvements for attention nets, please use the &lt;code&gt;Encoder&lt;/code&gt; from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;this repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install x-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch.efficient import ViT&#xA;from x_transformers import Encoder&#xA;&#xA;v = ViT(&#xA;    dim = 512,&#xA;    image_size = 224,&#xA;    patch_size = 16,&#xA;    num_classes = 1000,&#xA;    transformer = Encoder(&#xA;        dim = 512,                  # set to be the same as the wrapper&#xA;        depth = 12,&#xA;        heads = 8,&#xA;        ff_glu = True,              # ex. feed forward GLU variant https://arxiv.org/abs/2002.05202&#xA;        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747&#xA;    )&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 224, 224)&#xA;v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How do I pass in non-square images?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can already pass in non-square images - you just have to make sure your height and width is less than or equal to the &lt;code&gt;image_size&lt;/code&gt;, and both divisible by the &lt;code&gt;patch_size&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT&#xA;&#xA;v = ViT(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 128) # &amp;lt;-- not a square&#xA;&#xA;preds = v(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How do I pass in non-square patches?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vit_pytorch import ViT&#xA;&#xA;v = ViT(&#xA;    num_classes = 1000,&#xA;    image_size = (256, 128),  # image size is a tuple of (height, width)&#xA;    patch_size = (32, 16),    # patch size is a tuple of (height, width)&#xA;    dim = 1024,&#xA;    depth = 6,&#xA;    heads = 16,&#xA;    mlp_dim = 2048,&#xA;    dropout = 0.1,&#xA;    emb_dropout = 0.1&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 128)&#xA;&#xA;preds = v(img)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;Coming from computer vision and new to transformers? Here are some resources that greatly accelerated my learning.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34;&gt;Illustrated Transformer&lt;/a&gt; - Jay Alammar&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://peterbloem.nl/blog/transformers&#34;&gt;Transformers from Scratch&lt;/a&gt; - Peter Bloem&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;The Annotated Transformer&lt;/a&gt; - Harvard NLP&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hassani2021escaping,&#xA;    title   = {Escaping the Big Data Paradigm with Compact Transformers},&#xA;    author  = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},&#xA;    year    = 2021,&#xA;    url     = {https://arxiv.org/abs/2104.05704},&#xA;    eprint  = {2104.05704},&#xA;    archiveprefix = {arXiv},&#xA;    primaryclass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{dosovitskiy2020image,&#xA;    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},&#xA;    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},&#xA;    year    = {2020},&#xA;    eprint  = {2010.11929},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{touvron2020training,&#xA;    title   = {Training data-efficient image transformers &amp;amp; distillation through attention}, &#xA;    author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and HervÃ© JÃ©gou},&#xA;    year    = {2020},&#xA;    eprint  = {2012.12877},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yuan2021tokenstotoken,&#xA;    title   = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},&#xA;    author  = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},&#xA;    year    = {2021},&#xA;    eprint  = {2101.11986},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhou2021deepvit,&#xA;    title   = {DeepViT: Towards Deeper Vision Transformer},&#xA;    author  = {Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng},&#xA;    year    = {2021},&#xA;    eprint  = {2103.11886},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{touvron2021going,&#xA;    title   = {Going deeper with Image Transformers}, &#xA;    author  = {Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and HervÃ© JÃ©gou},&#xA;    year    = {2021},&#xA;    eprint  = {2103.17239},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2021crossvit,&#xA;    title   = {CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},&#xA;    author  = {Chun-Fu Chen and Quanfu Fan and Rameswar Panda},&#xA;    year    = {2021},&#xA;    eprint  = {2103.14899},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{wu2021cvt,&#xA;    title   = {CvT: Introducing Convolutions to Vision Transformers},&#xA;    author  = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},&#xA;    year    = {2021},&#xA;    eprint  = {2103.15808},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{heo2021rethinking,&#xA;    title   = {Rethinking Spatial Dimensions of Vision Transformers}, &#xA;    author  = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},&#xA;    year    = {2021},&#xA;    eprint  = {2103.16302},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{graham2021levit,&#xA;    title   = {LeViT: a Vision Transformer in ConvNet&#39;s Clothing for Faster Inference},&#xA;    author  = {Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and HervÃ© JÃ©gou and Matthijs Douze},&#xA;    year    = {2021},&#xA;    eprint  = {2104.01136},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{li2021localvit,&#xA;    title   = {LocalViT: Bringing Locality to Vision Transformers},&#xA;    author  = {Yawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool},&#xA;    year    = {2021},&#xA;    eprint  = {2104.05707},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chu2021twins,&#xA;    title   = {Twins: Revisiting Spatial Attention Design in Vision Transformers},&#xA;    author  = {Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},&#xA;    year    = {2021},&#xA;    eprint  = {2104.13840},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{su2021roformer,&#xA;    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, &#xA;    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},&#xA;    year    = {2021},&#xA;    eprint  = {2104.09864},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhang2021aggregating,&#xA;    title   = {Aggregating Nested Transformers},&#xA;    author  = {Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister},&#xA;    year    = {2021},&#xA;    eprint  = {2105.12723},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2021regionvit,&#xA;    title   = {RegionViT: Regional-to-Local Attention for Vision Transformers}, &#xA;    author  = {Chun-Fu Chen and Rameswar Panda and Quanfu Fan},&#xA;    year    = {2021},&#xA;    eprint  = {2106.02689},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{wang2021crossformer,&#xA;    title   = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention}, &#xA;    author  = {Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},&#xA;    year    = {2021},&#xA;    eprint  = {2108.00154},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{caron2021emerging,&#xA;    title   = {Emerging Properties in Self-Supervised Vision Transformers},&#xA;    author  = {Mathilde Caron and Hugo Touvron and Ishan Misra and HervÃ© JÃ©gou and Julien Mairal and Piotr Bojanowski and Armand Joulin},&#xA;    year    = {2021},&#xA;    eprint  = {2104.14294},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{he2021masked,&#xA;    title   = {Masked Autoencoders Are Scalable Vision Learners}, &#xA;    author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr DollÃ¡r and Ross Girshick},&#xA;    year    = {2021},&#xA;    eprint  = {2111.06377},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{xie2021simmim,&#xA;    title   = {SimMIM: A Simple Framework for Masked Image Modeling}, &#xA;    author  = {Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},&#xA;    year    = {2021},&#xA;    eprint  = {2111.09886},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{fayyaz2021ats,&#xA;    title   = {ATS: Adaptive Token Sampling For Efficient Vision Transformers},&#xA;    author  = {Mohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall},&#xA;    year    = {2021},&#xA;    eprint  = {2111.15667},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mehta2021mobilevit,&#xA;    title   = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},&#xA;    author  = {Sachin Mehta and Mohammad Rastegari},&#xA;    year    = {2021},&#xA;    eprint  = {2110.02178},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{lee2021vision,&#xA;    title   = {Vision Transformer for Small-Size Datasets}, &#xA;    author  = {Seung Hoon Lee and Seunghyun Lee and Byung Cheol Song},&#xA;    year    = {2021},&#xA;    eprint  = {2112.13492},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{renggli2022learning,&#xA;    title   = {Learning to Merge Tokens in Vision Transformers},&#xA;    author  = {Cedric Renggli and AndrÃ© Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme},&#xA;    year    = {2022},&#xA;    eprint  = {2202.12015},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yang2022scalablevit,&#xA;    title   = {ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer}, &#xA;    author  = {Rui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li},&#xA;    year    = {2022},&#xA;    eprint  = {2203.10790},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Touvron2022ThreeTE,&#xA;    title   = {Three things everyone should know about Vision Transformers},&#xA;    author  = {Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv&#39;e J&#39;egou},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Sandler2022FinetuningIT,&#xA;    title   = {Fine-tuning Image Transformers using Learnable Memory},&#xA;    author  = {Mark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Li2022SepViTSV,&#xA;    title   = {SepViT: Separable Vision Transformer},&#xA;    author  = {Wei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Tu2022MaxViTMV,&#xA;    title   = {MaxViT: Multi-Axis Vision Transformer},&#xA;    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Li2021EfficientSV,&#xA;    title   = {Efficient Self-supervised Vision Transformers for Representation Learning},&#xA;    author  = {Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},&#xA;    journal = {ArXiv},&#xA;    year    = {2021},&#xA;    volume  = {abs/2106.09785}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Beyer2022BetterPlainViT&#xA;    title     = {Better plain ViT baselines for ImageNet-1k},&#xA;    author    = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},&#xA;    publisher = {arXiv},&#xA;    year      = {2022}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{vaswani2017attention,&#xA;    title   = {Attention Is All You Need},&#xA;    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},&#xA;    year    = {2017},&#xA;    eprint  = {1706.03762},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;I visualise a time when we will be to robots what dogs are to humans, and Iâ€™m rooting for the machines.&lt;/em&gt; â€” Claude Shannon&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kakaobrain/coyo-dataset</title>
    <updated>2022-09-07T01:36:42Z</updated>
    <id>tag:github.com,2022-09-07:/kakaobrain/coyo-dataset</id>
    <link href="https://github.com/kakaobrain/coyo-dataset" rel="alternate"></link>
    <summary type="html">&lt;p&gt;COYO-700M: Large-scale Image-Text Pair Dataset&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;span&gt;ğŸº&lt;/span&gt; COYO-700M: Image-Text Pair Dataset&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;COYO-700M&lt;/strong&gt; is a large-scale dataset that contains &lt;strong&gt;747M image-text pairs&lt;/strong&gt; as well as many other &lt;strong&gt;meta-attributes&lt;/strong&gt; to increase the usability to train various models. Our dataset follows a similar strategy to previous vision-and-language datasets, collecting many informative pairs of alt-text and its associated image in HTML documents. We expect COYO to be used to train popular large-scale foundation models complementary to other similar datasets.&lt;/p&gt; &#xA;&lt;p&gt;More details on the data acquisition process can be found in [our paper] (which will be updated soon).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/coyo-samples.png&#34; width=&#34;1020&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aug 2022: Release COYO-700M Dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Collection Process&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We collected about 10 billion pairs of alt-text and image sources in HTML documents in &lt;a href=&#34;https://commoncrawl.org/&#34;&gt;CommonCrawl&lt;/a&gt; from Oct. 2020 to Aug. 2021. and eliminated uninformative pairs through the image and text level filtering process with minimal cost. The following figure outlines our data collection procedure.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/alt-text-example.png&#34; width=&#34;800&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   https://en.wikipedia.org/wiki/Napoleon&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Data Filtering&lt;/h2&gt; &#xA;&lt;h3&gt;Image Level&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Included all image formats that &lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html&#34;&gt;Pillow library&lt;/a&gt; can decode. (JPEG, WEBP, PNG, BMP, ...)&lt;/li&gt; &#xA; &lt;li&gt;Removed images less than 5KB image size.&lt;/li&gt; &#xA; &lt;li&gt;Removed images with an aspect ratio greater than 3.0.&lt;/li&gt; &#xA; &lt;li&gt;Removed images with min(width, height) &amp;lt; 200.&lt;/li&gt; &#xA; &lt;li&gt;Removed images with a score of &lt;a href=&#34;https://github.com/bhky/opennsfw2&#34;&gt;OpenNSFW2&lt;/a&gt; or &lt;a href=&#34;https://github.com/GantMan/nsfw_model&#34;&gt;GantMan/NSFW&lt;/a&gt; higher than 0.5.&lt;/li&gt; &#xA; &lt;li&gt;Removed all duplicate images based on the image &lt;a href=&#34;http://www.phash.org/&#34;&gt;pHash&lt;/a&gt; value from external public datasets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ImageNet-1K/21K, Flickr-30K, MS-COCO, CC-3M, CC-12M&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text Level&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Collected only English text using &lt;a href=&#34;https://github.com/google/cld3&#34;&gt;cld3&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Replaced consecutive whitespace characters with a single whitespace and removed the whitespace before and after the sentence. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;e.g. &lt;code&gt;&#34;\n \n Load image into Gallery viewer, valentine&amp;amp;amp;#39;s day roses\n \n&#34; â†’ &#34;Load image into Gallery viewer, valentine&amp;amp;amp;#39;s day roses&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Removed texts with a length of 5 or less.&lt;/li&gt; &#xA; &lt;li&gt;Removed texts that do not have a noun form.&lt;/li&gt; &#xA; &lt;li&gt;Removed texts with less than 3 words or more than 256 words and texts over 1000 in length.&lt;/li&gt; &#xA; &lt;li&gt;Removed texts appearing more than 10 times. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;e.g. &lt;code&gt;â€œthumbnail forâ€, â€œimage forâ€, â€œpicture ofâ€&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Removed texts containing NSFW words collected from &lt;a href=&#34;https://github.com/rominf/profanity-filter/raw/master/profanity_filter/data/en_profane_words.txt&#34;&gt;profanity_filter&lt;/a&gt;, &lt;a href=&#34;https://github.com/snguyenthanh/better_profanity/raw/master/better_profanity/profanity_wordlist.txt&#34;&gt;better_profanity&lt;/a&gt;, and &lt;a href=&#34;https://gist.github.com/ryanlewis/a37739d710ccdb4b406d&#34;&gt;google_twunter_lol&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Image-Text Level&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed duplicated samples based on (image_phash, text). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Different text may exist for the same image URL.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset Preview&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;id&lt;/th&gt; &#xA;   &lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; url &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th&gt;text&lt;/th&gt; &#xA;   &lt;th&gt;width&lt;/th&gt; &#xA;   &lt;th&gt;height&lt;/th&gt; &#xA;   &lt;th&gt;image_phash&lt;/th&gt; &#xA;   &lt;th&gt;text_length&lt;/th&gt; &#xA;   &lt;th&gt;word_count&lt;/th&gt; &#xA;   &lt;th&gt;num_tokens_bert&lt;/th&gt; &#xA;   &lt;th&gt;num_tokens_gpt&lt;/th&gt; &#xA;   &lt;th&gt;num_faces&lt;/th&gt; &#xA;   &lt;th&gt;clip_similarity_vitb32&lt;/th&gt; &#xA;   &lt;th&gt;clip_similarity_vitl14&lt;/th&gt; &#xA;   &lt;th&gt;nsfw_score_opennsfw2&lt;/th&gt; &#xA;   &lt;th&gt;nsfw_score_gantman&lt;/th&gt; &#xA;   &lt;th&gt;watermark_score&lt;/th&gt; &#xA;   &lt;th&gt;aesthetic_score_laion_v2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2559800550877&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://artusa.com/files/store/product/phillipsfuelstatecritical.jpg&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;William Phillips - Fuel State Critical - Outcome in Doubt (B-25&lt;/td&gt; &#xA;   &lt;td&gt;600&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;e9e90c24f1d9c674&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.300293&lt;/td&gt; &#xA;   &lt;td&gt;0.258545&lt;/td&gt; &#xA;   &lt;td&gt;0.000364065&lt;/td&gt; &#xA;   &lt;td&gt;0.0101354&lt;/td&gt; &#xA;   &lt;td&gt;0.0191545&lt;/td&gt; &#xA;   &lt;td&gt;7.1703&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4896263451343&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://cdn.shopify.com/s/files/1/0190/8574/products/Art_Riley-Monterey_Fishing_Fleet_1_grande.jpg?v=1479962684&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fishing Fleet (Monterey), California art by Art Riley. HD giclee art prints for sale at CaliforniaWatercolor.com - original California paintings, &amp;amp; premium giclee prints for sale&lt;/td&gt; &#xA;   &lt;td&gt;600&lt;/td&gt; &#xA;   &lt;td&gt;447&lt;/td&gt; &#xA;   &lt;td&gt;bac58374982e0fc7&lt;/td&gt; &#xA;   &lt;td&gt;178&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.319336&lt;/td&gt; &#xA;   &lt;td&gt;0.248169&lt;/td&gt; &#xA;   &lt;td&gt;2.54512e-05&lt;/td&gt; &#xA;   &lt;td&gt;0.0293861&lt;/td&gt; &#xA;   &lt;td&gt;0.0406009&lt;/td&gt; &#xA;   &lt;td&gt;7.04812&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1425929344479&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://www.ephotozine.com/resize/2018/07/xlrg/121543_1518912380.jpg?RTUdGk5cXyJFBQgJVANtcQlnYF8JERFaGwJRNQh6SlYUAEw1cmsCdg1hAWoxXFNGLSI=&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The Gate by Pete2453&lt;/td&gt; &#xA;   &lt;td&gt;600&lt;/td&gt; &#xA;   &lt;td&gt;347&lt;/td&gt; &#xA;   &lt;td&gt;8374726575bc0f8a&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.24939&lt;/td&gt; &#xA;   &lt;td&gt;0.203735&lt;/td&gt; &#xA;   &lt;td&gt;6.97374e-06&lt;/td&gt; &#xA;   &lt;td&gt;0.00823276&lt;/td&gt; &#xA;   &lt;td&gt;0.0721415&lt;/td&gt; &#xA;   &lt;td&gt;6.98521&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7456063527931&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://www.boredart.com//wp-content/uploads/2014/06/Beautiful-Pictures-From-the-Shores-of-the-Mythical-Land-421.jpg&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Beautiful Pictures From the Shores of the Mythical Land (42&lt;/td&gt; &#xA;   &lt;td&gt;600&lt;/td&gt; &#xA;   &lt;td&gt;320&lt;/td&gt; &#xA;   &lt;td&gt;949d1fe559e2cc90&lt;/td&gt; &#xA;   &lt;td&gt;59&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.290771&lt;/td&gt; &#xA;   &lt;td&gt;0.179321&lt;/td&gt; &#xA;   &lt;td&gt;0.0130615&lt;/td&gt; &#xA;   &lt;td&gt;0.0178628&lt;/td&gt; &#xA;   &lt;td&gt;0.489642&lt;/td&gt; &#xA;   &lt;td&gt;6.94643&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3221225511175&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://homesfeed.com/wp-content/uploads/2017/12/contemporary-expensive-lighting-fixtures-with-minimum-lighting.jpg&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;contemporary expensive lighting fixtures with minimum lighting&lt;/td&gt; &#xA;   &lt;td&gt;800&lt;/td&gt; &#xA;   &lt;td&gt;499&lt;/td&gt; &#xA;   &lt;td&gt;e5ea35075ab912c6&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.263916&lt;/td&gt; &#xA;   &lt;td&gt;0.217896&lt;/td&gt; &#xA;   &lt;td&gt;0.000990868&lt;/td&gt; &#xA;   &lt;td&gt;0.0137114&lt;/td&gt; &#xA;   &lt;td&gt;0.0960748&lt;/td&gt; &#xA;   &lt;td&gt;4.57594&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5626407855002&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://api.time.com/wp-content/uploads/2015/03/168951187.jpg&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Nintendo Co.&#39;s Super Mario is displayed on coffee mugs for sale at the Nintendo World store in New York, U.S., on Friday, May 17, 2013.&lt;/td&gt; &#xA;   &lt;td&gt;2000&lt;/td&gt; &#xA;   &lt;td&gt;1309&lt;/td&gt; &#xA;   &lt;td&gt;9311891e9437f4f3&lt;/td&gt; &#xA;   &lt;td&gt;135&lt;/td&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;   &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.400878&lt;/td&gt; &#xA;   &lt;td&gt;0.316650&lt;/td&gt; &#xA;   &lt;td&gt;0.00362968&lt;/td&gt; &#xA;   &lt;td&gt;0.0317519&lt;/td&gt; &#xA;   &lt;td&gt;0.0022693&lt;/td&gt; &#xA;   &lt;td&gt;6.324910&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1125282207474&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://s.yimg.com/ny/api/res/1.2/mOZe9uKtwugmPrqeXBlxFg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTYzMA--/https://s.yimg.com/uu/api/res/1.2/JuTSVK74cI8II09Q75uzGA--~B/aD01MjU7dz04MDA7YXBwaWQ9eXRhY2h5b24-/https://media.zenfs.com/en/reuters.com/15941d3b47960da80f8033f4ddf9da64&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FILE PHOTO: A rainbow appears on the Auckland skyline featuring Sky Tower in New Zealand&lt;/td&gt; &#xA;   &lt;td&gt;800&lt;/td&gt; &#xA;   &lt;td&gt;525&lt;/td&gt; &#xA;   &lt;td&gt;85b89c0166ee63be&lt;/td&gt; &#xA;   &lt;td&gt;88&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0.4453125&lt;/td&gt; &#xA;   &lt;td&gt;0.3505859&lt;/td&gt; &#xA;   &lt;td&gt;2.640485e-05&lt;/td&gt; &#xA;   &lt;td&gt;0.012074&lt;/td&gt; &#xA;   &lt;td&gt;0.0219129&lt;/td&gt; &#xA;   &lt;td&gt;5.294523&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1434519186493&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://static.straitstimes.com.sg/s3fs-public/styles/article_pictrure_780x520_/public/articles/2013/07/24/CHINA24e_2x.jpg?itok=6ppRPBJs&amp;amp;timestamp=1436931188&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A man covers himself with algae as he poses for photographs on a beach in Qingdao, Shandong province on Tuesday, July 23, 2013. -- FILE PHOTO: REUTERS&lt;/td&gt; &#xA;   &lt;td&gt;860&lt;/td&gt; &#xA;   &lt;td&gt;573&lt;/td&gt; &#xA;   &lt;td&gt;f2c48dabbf93810a&lt;/td&gt; &#xA;   &lt;td&gt;150&lt;/td&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;0.4165039&lt;/td&gt; &#xA;   &lt;td&gt;0.3427734&lt;/td&gt; &#xA;   &lt;td&gt;0.025009&lt;/td&gt; &#xA;   &lt;td&gt;0.01608&lt;/td&gt; &#xA;   &lt;td&gt;0.072775&lt;/td&gt; &#xA;   &lt;td&gt;6.833739&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dataset Numbers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;count&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;ratio&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;# of image-text pairs&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;746,972,269&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;100.00%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;# of unique urls&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;656,114,783&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.84%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;# of unique image_phash&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;579,679,137&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.60%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;# of unique text&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;566,253,888&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.81%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Meta-Attributes&lt;/h2&gt; &#xA;&lt;h3&gt;Attributes&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;type&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;id&lt;/td&gt; &#xA;   &lt;td&gt;long&lt;/td&gt; &#xA;   &lt;td&gt;Unique 64-bit integer ID generated by &lt;a href=&#34;https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.monotonically_increasing_id.html&#34;&gt;monotonically_increasing_id()&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;url&lt;/td&gt; &#xA;   &lt;td&gt;string&lt;/td&gt; &#xA;   &lt;td&gt;The image URL extracted from the &lt;code&gt;src&lt;/code&gt; attribute of the &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text&lt;/td&gt; &#xA;   &lt;td&gt;string&lt;/td&gt; &#xA;   &lt;td&gt;The text extracted from the &lt;code&gt;alt&lt;/code&gt; attribute of the &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;width&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The width of the image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;height&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The height of the image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;image_phash&lt;/td&gt; &#xA;   &lt;td&gt;string&lt;/td&gt; &#xA;   &lt;td&gt;The &lt;a href=&#34;http://www.phash.org/&#34;&gt;perceptual hash(pHash)&lt;/a&gt; of the image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text_length&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The length of the text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;word_count&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of words separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;num_tokens_bert&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of tokens using &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer&#34;&gt;BertTokenizer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;num_tokens_gpt&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of tokens using &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast&#34;&gt;GPT2TokenizerFast&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;num_faces&lt;/td&gt; &#xA;   &lt;td&gt;integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of faces in the image detected by &lt;a href=&#34;https://insightface.ai/scrfd&#34;&gt;SCRFD&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clip_similarity_vitb32&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The cosine similarity between text and image(ViT-B/32) embeddings by &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;OpenAI CLIP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clip_similarity_vitl14&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The cosine similarity between text and image(ViT-L/14) embeddings by &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;OpenAI CLIP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nsfw_score_opennsfw2&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The NSFW score of the image by &lt;a href=&#34;https://github.com/bhky/opennsfw2&#34;&gt;OpenNSFW2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nsfw_score_gantman&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The NSFW score of the image by &lt;a href=&#34;https://github.com/GantMan/nsfw_model&#34;&gt;GantMan/NSFW&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;watermark_score&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The watermark probability of the image by our internal model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aesthetic_score_laion_v2&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;The aesthetic score of the image by &lt;a href=&#34;https://github.com/christophschuhmann/improved-aesthetic-predictor&#34;&gt;LAION-Aesthetics-Predictor-V2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Statistics for numeric columns&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;width&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;height&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;text_length&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;word_count&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;num_tokens_bert&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;num_tokens_gpt&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;num_faces&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;mean&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;621.78&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;540.99&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;68.53&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;11.13&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;15.75&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;17.24&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;min&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;200&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;200&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;6&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;max&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;21449&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;22507&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;323&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;811&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1523&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;736&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;watermark_score&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;clip_similarity_vitb32&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;clip_similarity_vitl14&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;aesthetic_score_laion_v2&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;nsfw_score_opennsfw2&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;mean&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.178544&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.291266&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.254632&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;4.769132&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.012903&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;min&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;-0.080871&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;-0.176269&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1.171712&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;max&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.591796&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.581542&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;8.082607&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;0.499755&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image Size &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_image_size.png&#34; alt=&#34;statistics_image_size.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CLIP Similarity &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_clip_similarity.png&#34; alt=&#34;statistics_clip_similarity.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text Length &amp;amp; Word Size &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_text.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Watermark score &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_watermark.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Aesthetic score &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_aesthetic.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Number of faces &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/statistics_faces.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For more detailed statistics on COYO-700M, please see the &lt;a href=&#34;https://datastudio.google.com/s/jvwkG5XCzYI&#34;&gt;Data Studio report on COYO-700M&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download the dataset from &lt;a href=&#34;https://huggingface.co/datasets/kakaobrain/coyo-700m&#34;&gt;Huggingface Dataset&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For more information on downloading the image dataset, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/download/README.md&#34;&gt;download/README.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/examples/webdataset_torch.py&#34;&gt;pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/examples/webdataset_tf.py&#34;&gt;tensorflow&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Experiments&lt;/h2&gt; &#xA;&lt;p&gt;We empirically validated the quality of COYO dataset by re-implementing popular models such as &lt;a href=&#34;https://arxiv.org/abs/2102.05918&#34;&gt;ALIGN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;unCLIP&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;ViT&lt;/a&gt;. We trained these models on COYO-700M or its subsets from scratch, achieving competitive performance to the reported numbers or generated samples in the original papers. Since this observation supports the high quality of our dataset, we hope it to be continuously updated with open collaboration. Our pre-trained models and training codes will be released soon along with the technical report.&lt;/p&gt; &#xA;&lt;h3&gt;ALIGN&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;ImageNet KNN&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;COCO I2T&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;COCO T2I&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EfficientNet-B7 + BERT-base&lt;/td&gt; &#xA;   &lt;td&gt;ALIGN-1.8B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;69.300&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;55.400&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;41.700&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EfficientNet-B7 + BERT-base&lt;/td&gt; &#xA;   &lt;td&gt;COYO-700M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;68.618&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;59.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;42.419&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our experiment setup followed &lt;a href=&#34;https://arxiv.org/abs/2102.05918&#34;&gt;ALIGN&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We increased the batch size from 16K to 64K and reduced training steps by 1/4 for faster training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;unCLIP (&lt;a href=&#34;https://openai.com/dall-e-2/&#34;&gt;OpenAI DALLÂ·E 2&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div style=&#34;width:150px&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/dalle2_knight.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div style=&#34;width:150px&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/dalle2_andywarhol.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A high quality picture of a medieval knight with golden armor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A person with the head of a cat in the style of Andy Warhol&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&lt;/tr&gt;  &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div style=&#34;width:150px&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/dalle2_astronaut.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div style=&#34;width:150px&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/assets/dalle2_darthvader.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A pencil drawing of an astronaut riding a horse&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Goryeo celadon in the shape of darth vader&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We implemented the smaller version of unCLIP to validate the effectiveness of COYO for the text-conditional generation tasks.&lt;/li&gt; &#xA; &lt;li&gt;Specifically, we tried to reproduce three components of the original &lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;unCLIP&lt;/a&gt;: diffusion-based prior, decoder with some modifications, and super-resolution model for upscaling 64x64 into 256x256px.&lt;/li&gt; &#xA; &lt;li&gt;Detailed information on our modified version of unCLIP and quantitative analysis would be included in the upcoming technical report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ViT&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;ImageNet &lt;br&gt; Validation &lt;br&gt;Top-1 Acc&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;JFT-300M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;87.76%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;COYO-Labels-300M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.09%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We also provide COYO-Labels-300M by adding &lt;strong&gt;machine-generated vision labels&lt;/strong&gt; to a subset of COYO-700M for comparison with the JFT-300M. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We first removed the duplicated images by &lt;code&gt;image_phash&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Then, we labeled 300M unique images into 21,841 classes by &lt;a href=&#34;https://arxiv.org.abs/2104.00298&#34;&gt;EfficientNetV2-XL&lt;/a&gt; trained with &lt;a href=&#34;https://www.image-net.org/&#34;&gt;ImageNet-21K&lt;/a&gt; dataset.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Our experiment setup followed &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;ViT&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We increased training epochs to 14.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you apply this dataset to any project and research, please cite our code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{kakaobrain2022coyo-700m,&#xA;  title         = {COYO-700M: Image-Text Pair Dataset},&#xA;  author        = {Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, Saehoon Kim},&#xA;  year          = {2022},&#xA;  howpublished  = {\url{https://github.com/kakaobrain/coyo-dataset}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;People&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minwoo Byeon (&lt;a href=&#34;https://github.com/mwbyeon&#34;&gt;@mwbyeon&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Beomhee Park (&lt;a href=&#34;https://github.com/beomheepark&#34;&gt;@beomheepark&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Haecheon Kim (&lt;a href=&#34;https://github.com/HaecheonKim&#34;&gt;@HaecheonKim&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sungjun Lee (&lt;a href=&#34;https://github.com/justHungryMan&#34;&gt;@justhungryman&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Woonhyuk Baek (&lt;a href=&#34;https://github.com/wbaek&#34;&gt;@wbaek&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Saehoon Kim (&lt;a href=&#34;https://github.com/saehoonkim&#34;&gt;@saehoonkim&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;and Kakao Brain Large-Scale AI Studio&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer &amp;amp; Content Warning&lt;/h2&gt; &#xA;&lt;p&gt;The COYO dataset is recommended to be used for research purposes. Kakao Brain tried to construct a &#34;Safe&#34; dataset when building the COYO dataset. (See &lt;a href=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/#data-filtering&#34;&gt;Data Filtering&lt;/a&gt; Section) Kakao Brain is constantly making efforts to create more &#34;Safe&#34; datasets. However, despite these efforts, this large-scale dataset was not hand-picked by humans to avoid the risk due to its very large size (over 700M). Keep in mind that the unscreened nature of the dataset means that the collected images can lead to strongly discomforting and disturbing content for humans. The COYO dataset may contain some inappropriate data, and any problems resulting from such data are the full responsibility of the user who used it. Therefore, it is strongly recommended that this dataset be used only for research, keeping this in mind when using the dataset, and Kakao Brain does not recommend using this dataset as it is without special processing to clear inappropriate data to create commercial products.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The COYO dataset of Kakao Brain is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;CC-BY-4.0 License&lt;/a&gt;. The full license can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/kakaobrain/coyo-dataset/main/LICENSE.cc-by-4.0&#34;&gt;LICENSE.cc-by-4.0 file&lt;/a&gt;. The dataset includes â€œImage URLâ€ and â€œTextâ€ collected from various sites by analyzing Common Crawl data, an open data web crawling project. The collected data (images and text) is subject to the license to which each content belongs.&lt;/p&gt; &#xA;&lt;h2&gt;Obligation to use&lt;/h2&gt; &#xA;&lt;p&gt;While Open Source may be free to use, that does not mean it is free of obligation. To determine whether your intended use of the COYO dataset is suitable for the CC-BY-4.0 license, please consider the license guide. If you violate the license, you may be subject to legal action such as the prohibition of use or claim for damages depending on the use.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;COYO dataset was released as an open source in the hope that it will be helpful to many research institutes and startups for research purposes. We look forward to contacting us from various places who wish to cooperate with us.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:coyo@kakaobrain.com&#34;&gt;coyo@kakaobrain.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>