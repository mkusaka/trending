<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-31T01:37:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hinthornw/trustcall</title>
    <updated>2025-03-31T01:37:07Z</updated>
    <id>tag:github.com,2025-03-31:/hinthornw/trustcall</id>
    <link href="https://github.com/hinthornw/trustcall" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tenacious tool calling built on LangGraph&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü§ùtrustcall&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hinthornw/trustcall/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/hinthornw/trustcall/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hinthornw/trustcall/main/_static/cover.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLMs struggle when asked to generate or modify large JSON blobs. &lt;code&gt;trustcall&lt;/code&gt; solves this by asking the LLM to generate &lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc6902&#34;&gt;JSON patch&lt;/a&gt; operations. This is a simpler task that can be done iteratively. This enables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö° Faster &amp;amp; cheaper generation of structured output.&lt;/li&gt; &#xA; &lt;li&gt;üê∫Resilient retrying of validation errors, even for complex, nested schemas (defined as pydantic, schema dictionaries, or regular python functions)&lt;/li&gt; &#xA; &lt;li&gt;üß©Acccurate updates to existing schemas, avoiding undesired deletions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Works flexibly across a number of common LLM workflows like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÇÔ∏è Extraction&lt;/li&gt; &#xA; &lt;li&gt;üß≠ LLM routing&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Multi-step agent tool use&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install trustcall&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hinthornw/trustcall/main/#complex-schema&#34;&gt;Extracting complex schemas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hinthornw/trustcall/main/#updating-schemas&#34;&gt;Updating schemas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hinthornw/trustcall/main/#simultanous-updates--insertions&#34;&gt;Simultanous updates &amp;amp; insertions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why trustcall?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python.langchain.com/docs/how_to/tool_calling/&#34;&gt;Tool calling&lt;/a&gt; makes it easier to compose LLM calls within reliable software systems, but LLM&#39;s today can be error prone and inefficient in two common scenarios:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Populating complex, nested schemas&lt;/li&gt; &#xA; &lt;li&gt;Updating existing schemas without information loss&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These problems are both exaggerated when you want to handle multiple tool calls.&lt;/p&gt; &#xA;&lt;p&gt;Trustcall increases structured extraction reliability without restricting you to a subset of the JSON schema.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see a couple examples to see what we mean.&lt;/p&gt; &#xA;&lt;h3&gt;Complex schema&lt;/h3&gt; &#xA;&lt;p&gt;Take the following example:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Schema definition&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;from typing import List, Optional&#xA;&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class OutputFormat(BaseModel):&#xA;    preference: str&#xA;    sentence_preference_revealed: str&#xA;&#xA;&#xA;class TelegramPreferences(BaseModel):&#xA;    preferred_encoding: Optional[List[OutputFormat]] = None&#xA;    favorite_telegram_operators: Optional[List[OutputFormat]] = None&#xA;    preferred_telegram_paper: Optional[List[OutputFormat]] = None&#xA;&#xA;&#xA;class MorseCode(BaseModel):&#xA;    preferred_key_type: Optional[List[OutputFormat]] = None&#xA;    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None&#xA;&#xA;&#xA;class Semaphore(BaseModel):&#xA;    preferred_flag_color: Optional[List[OutputFormat]] = None&#xA;    semaphore_skill_level: Optional[List[OutputFormat]] = None&#xA;&#xA;&#xA;class TrustFallPreferences(BaseModel):&#xA;    preferred_fall_height: Optional[List[OutputFormat]] = None&#xA;    trust_level: Optional[List[OutputFormat]] = None&#xA;    preferred_catching_technique: Optional[List[OutputFormat]] = None&#xA;&#xA;&#xA;class CommunicationPreferences(BaseModel):&#xA;    telegram: TelegramPreferences&#xA;    morse_code: MorseCode&#xA;    semaphore: Semaphore&#xA;&#xA;&#xA;class UserPreferences(BaseModel):&#xA;    communication_preferences: CommunicationPreferences&#xA;    trust_fall_preferences: TrustFallPreferences&#xA;&#xA;&#xA;class TelegramAndTrustFallPreferences(BaseModel):&#xA;    pertinent_user_preferences: UserPreferences&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; If you naively extract these values using `gpt-4o`, it&#39;s prone to failure: &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain_openai import ChatOpenAI&#xA;&#xA;llm = ChatOpenAI(model=&#34;gpt-4o&#34;)&#xA;bound = llm.with_structured_output(TelegramAndTrustFallPreferences)&#xA;&#xA;conversation = &#34;&#34;&#34;Operator: How may I assist with your telegram, sir?&#xA;Customer: I need to send a message about our trust fall exercise.&#xA;Operator: Certainly. Morse code or standard encoding?&#xA;Customer: Morse, please. I love using a straight key.&#xA;Operator: Excellent. What&#39;s your message?&#xA;Customer: Tell him I&#39;m ready for a higher fall, and I prefer the diamond formation for catching.&#xA;Operator: Done. Shall I use our &#34;Daredevil&#34; paper for this daring message?&#xA;Customer: Perfect! Send it by your fastest carrier pigeon.&#xA;Operator: It&#39;ll be there within the hour, sir.&#34;&#34;&#34;&#xA;&#xA;bound.invoke(f&#34;&#34;&#34;Extract the preferences from the following conversation:&#xA;&amp;lt;convo&amp;gt;&#xA;{conversation}&#xA;&amp;lt;/convo&amp;gt;&#34;&#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;ValidationError: 1 validation error for TelegramAndTrustFallPreferences&#xA;pertinent_user_preferences.communication_preferences.semaphore&#xA;  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]&#xA;    For further information visit https://errors.pydantic.dev/2.8/v/model_type&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you try to use &lt;strong&gt;strict&lt;/strong&gt; mode or OpenAI&#39;s &lt;code&gt;json_schema&lt;/code&gt;, it will give you an error as well, since their parser doesn&#39;t support the complex JSON schemas:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bound = llm.bind_tools([TelegramAndTrustFallPreferences], strict=True, response_format=TelegramAndTrustFallPreferences)&#xA;&#xA;bound.invoke(f&#34;&#34;&#34;Extract the preferences from the following conversation:&#xA;&amp;lt;convo&amp;gt;&#xA;{conversation}&#xA;&amp;lt;/convo&amp;gt;&#34;&#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;BadRequestError: Error code: 400 - {&#39;error&#39;: {&#39;message&#39;: &#34;Invalid schema for function &#39;TelegramAndTrustFallPreferences&#39;: &#34;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With &lt;code&gt;trustcall&lt;/code&gt;, this extraction task is easy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from trustcall import create_extractor&#xA;&#xA;bound = create_extractor(&#xA;    llm,&#xA;    tools=[TelegramAndTrustFallPreferences],&#xA;    tool_choice=&#34;TelegramAndTrustFallPreferences&#34;,&#xA;)&#xA;&#xA;result = bound.invoke(&#xA;    f&#34;&#34;&#34;Extract the preferences from the following conversation:&#xA;&amp;lt;convo&amp;gt;&#xA;{conversation}&#xA;&amp;lt;/convo&amp;gt;&#34;&#34;&#34;&#xA;)&#xA;result[&#34;responses&#34;][0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;pertinent_user_preferences&#34;: {&#xA;        &#34;communication_preferences&#34;: {&#xA;            &#34;telegram&#34;: {&#xA;                &#34;preferred_encoding&#34;: [&#xA;                    {&#xA;                        &#34;preference&#34;: &#34;morse&#34;,&#xA;                        &#34;sentence_preference_revealed&#34;: &#34;Morse, please.&#34;,&#xA;                    }&#xA;                ],&#xA;                &#34;favorite_telegram_operators&#34;: None,&#xA;                &#34;preferred_telegram_paper&#34;: [&#xA;                    {&#xA;                        &#34;preference&#34;: &#34;Daredevil&#34;,&#xA;                        &#34;sentence_preference_revealed&#34;: &#39;Shall I use our &#34;Daredevil&#34; paper for this daring message?&#39;,&#xA;                    }&#xA;                ],&#xA;            },&#xA;            &#34;morse_code&#34;: {&#xA;                &#34;preferred_key_type&#34;: [&#xA;                    {&#xA;                        &#34;preference&#34;: &#34;straight key&#34;,&#xA;                        &#34;sentence_preference_revealed&#34;: &#34;I love using a straight key.&#34;,&#xA;                    }&#xA;                ],&#xA;                &#34;favorite_morse_abbreviations&#34;: None,&#xA;            },&#xA;            &#34;semaphore&#34;: {&#34;preferred_flag_color&#34;: None, &#34;semaphore_skill_level&#34;: None},&#xA;        },&#xA;        &#34;trust_fall_preferences&#34;: {&#xA;            &#34;preferred_fall_height&#34;: [&#xA;                {&#xA;                    &#34;preference&#34;: &#34;higher&#34;,&#xA;                    &#34;sentence_preference_revealed&#34;: &#34;I&#39;m ready for a higher fall.&#34;,&#xA;                }&#xA;            ],&#xA;            &#34;trust_level&#34;: None,&#xA;            &#34;preferred_catching_technique&#34;: [&#xA;                {&#xA;                    &#34;preference&#34;: &#34;diamond formation&#34;,&#xA;                    &#34;sentence_preference_revealed&#34;: &#34;I prefer the diamond formation for catching.&#34;,&#xA;                }&#xA;            ],&#xA;        },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What&#39;s different? &lt;code&gt;trustcall&lt;/code&gt; handles prompt retries with a twist: rather than naively re-generating the full output, it prompts the LLM to generate a concise patch to fix the error in question. This is both &lt;strong&gt;more reliable&lt;/strong&gt; than naive reprompting and &lt;strong&gt;cheaper&lt;/strong&gt; since you only regenerate a subset of the full schema.&lt;/p&gt; &#xA;&lt;p&gt;The &#34;patch-don&#39;t-post&#34; mantra affords us better performance in other ways too! Let&#39;s see how it helps &lt;strong&gt;updates&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Updating schemas&lt;/h3&gt; &#xA;&lt;p&gt;Many tasks expect an LLM to correct or modify an existing object based on new information.&lt;/p&gt; &#xA;&lt;p&gt;Take memory management as an example. Suppose you structure memories as JSON objects. When new information is provided, the LLM must reconcile this information with the existing document. Let&#39;s try this using naive regeneration of the document. We&#39;ll model memory as a single user profile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Dict, List, Optional&#xA;&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class Address(BaseModel):&#xA;    street: str&#xA;    city: str&#xA;    country: str&#xA;    postal_code: str&#xA;&#xA;&#xA;class Pet(BaseModel):&#xA;    kind: str&#xA;    name: Optional[str]&#xA;    age: Optional[int]&#xA;&#xA;&#xA;class Hobby(BaseModel):&#xA;    name: str&#xA;    skill_level: str&#xA;    frequency: str&#xA;&#xA;&#xA;class FavoriteMedia(BaseModel):&#xA;    shows: List[str]&#xA;    movies: List[str]&#xA;    books: List[str]&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    preferred_name: str&#xA;    favorite_media: FavoriteMedia&#xA;    favorite_foods: List[str]&#xA;    hobbies: List[Hobby]&#xA;    age: int&#xA;    occupation: str&#xA;    address: Address&#xA;    favorite_color: Optional[str] = None&#xA;    pets: Optional[List[Pet]] = None&#xA;    languages: Dict[str, str] = {}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And set a starting profile state:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Starting profile&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;initial_user = User(&#xA;    preferred_name=&#34;Alex&#34;,&#xA;    favorite_media=FavoriteMedia(&#xA;        shows=[&#xA;            &#34;Friends&#34;,&#xA;            &#34;Game of Thrones&#34;,&#xA;            &#34;Breaking Bad&#34;,&#xA;            &#34;The Office&#34;,&#xA;            &#34;Stranger Things&#34;,&#xA;        ],&#xA;        movies=[&#34;The Shawshank Redemption&#34;, &#34;Inception&#34;, &#34;The Dark Knight&#34;],&#xA;        books=[&#34;1984&#34;, &#34;To Kill a Mockingbird&#34;, &#34;The Great Gatsby&#34;],&#xA;    ),&#xA;    favorite_foods=[&#34;sushi&#34;, &#34;pizza&#34;, &#34;tacos&#34;, &#34;ice cream&#34;, &#34;pasta&#34;, &#34;curry&#34;],&#xA;    hobbies=[&#xA;        Hobby(name=&#34;reading&#34;, skill_level=&#34;expert&#34;, frequency=&#34;daily&#34;),&#xA;        Hobby(name=&#34;hiking&#34;, skill_level=&#34;intermediate&#34;, frequency=&#34;weekly&#34;),&#xA;        Hobby(name=&#34;photography&#34;, skill_level=&#34;beginner&#34;, frequency=&#34;monthly&#34;),&#xA;        Hobby(name=&#34;biking&#34;, skill_level=&#34;intermediate&#34;, frequency=&#34;weekly&#34;),&#xA;        Hobby(name=&#34;swimming&#34;, skill_level=&#34;expert&#34;, frequency=&#34;weekly&#34;),&#xA;        Hobby(name=&#34;canoeing&#34;, skill_level=&#34;beginner&#34;, frequency=&#34;monthly&#34;),&#xA;        Hobby(name=&#34;sailing&#34;, skill_level=&#34;intermediate&#34;, frequency=&#34;monthly&#34;),&#xA;        Hobby(name=&#34;weaving&#34;, skill_level=&#34;beginner&#34;, frequency=&#34;weekly&#34;),&#xA;        Hobby(name=&#34;painting&#34;, skill_level=&#34;intermediate&#34;, frequency=&#34;weekly&#34;),&#xA;        Hobby(name=&#34;cooking&#34;, skill_level=&#34;expert&#34;, frequency=&#34;daily&#34;),&#xA;    ],&#xA;    age=28,&#xA;    occupation=&#34;Software Engineer&#34;,&#xA;    address=Address(&#xA;        street=&#34;123 Tech Lane&#34;, city=&#34;San Francisco&#34;, country=&#34;USA&#34;, postal_code=&#34;94105&#34;&#xA;    ),&#xA;    favorite_color=&#34;blue&#34;,&#xA;    pets=[Pet(kind=&#34;cat&#34;, name=&#34;Luna&#34;, age=3)],&#xA;    languages={&#34;English&#34;: &#34;native&#34;, &#34;Spanish&#34;: &#34;intermediate&#34;, &#34;Python&#34;: &#34;expert&#34;},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Giving the following conversation, we&#39;d expect the memory to be &lt;strong&gt;expanded&lt;/strong&gt; to include video gaming but not drop any other information:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;conversation = &#34;&#34;&#34;Friend: Hey Alex, how&#39;s the new job going? I heard you switched careers recently.&#xA;Alex: It&#39;s going great! I&#39;m loving my new role as a Data Scientist. The work is challenging but exciting. I&#39;ve moved to a new apartment in New York to be closer to the office.&#xA;Friend: That&#39;s a big change! Are you still finding time for your hobbies?&#xA;Alex: Well, I&#39;ve had to cut back on some. I&#39;m not doing much sailing or canoeing these days. But I&#39;ve gotten really into machine learning projects in my free time. I&#39;d say I&#39;m getting pretty good at it - probably an intermediate level now.&#xA;Friend: Sounds like you&#39;re keeping busy! How&#39;s Luna doing?&#xA;Alex: Oh, Luna&#39;s great. She just turned 4 last week. She&#39;s actually made friends with my new pet, Max the dog. He&#39;s a playful 2-year-old golden retriever.&#xA;Friend: Two pets now! That&#39;s exciting. Hey, want to catch the new season of Stranger Things this weekend?&#xA;Alex: Actually, I&#39;ve kind of lost interest in that show. But I&#39;m really into this new series called &#34;The Mandalorian&#34;. We could watch that instead! Oh, and I recently watched &#34;Parasite&#34; - it&#39;s become one of my favorite movies.&#xA;Friend: Sure, that sounds fun. Should I bring some food? I remember you love sushi.&#xA;Alex: Sushi would be perfect! Or maybe some Thai food - I&#39;ve been really into that lately. By the way, I&#39;ve been practicing my French. I&#39;d say I&#39;m at a beginner level now.&#xA;Friend: That&#39;s great! You&#39;re always learning something new. How&#39;s the cooking going?&#xA;Alex: It&#39;s going well! I&#39;ve been cooking almost every day now. I&#39;d say I&#39;ve become quite proficient at it.&#34;&#34;&#34;&#xA;&#xA;&#xA;# Naive approach&#xA;bound = llm.with_structured_output(User)&#xA;naive_result = bound.invoke(&#xA;    f&#34;&#34;&#34;Update the memory (JSON doc) to incorporate new information from the following conversation:&#xA;&amp;lt;user_info&amp;gt;&#xA;{initial_user.model_dump()}&#xA;&amp;lt;/user_info&amp;gt;&#xA;&amp;lt;convo&amp;gt;&#xA;{conversation}&#xA;&amp;lt;/convo&amp;gt;&#34;&#34;&#34;&#xA;)&#xA;print(&#34;Naive approach result:&#34;)&#xA;naive_output = naive_result.model_dump()&#xA;print(naive_output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Naive output&lt;/summary&gt; { &#34;preferred_name&#34;: &#34;Alex&#34;, &#34;favorite_media&#34;: { &#34;shows&#34;: [&#34;Friends&#34;, &#34;Game of Thrones&#34;, &#34;Breaking Bad&#34;, &#34;The Office&#34;], &#34;movies&#34;: [ &#34;The Shawshank Redemption&#34;, &#34;Inception&#34;, &#34;The Dark Knight&#34;, &#34;Parasite&#34;, ], &#34;books&#34;: [&#34;1984&#34;, &#34;To Kill a Mockingbird&#34;, &#34;The Great Gatsby&#34;], }, &#34;favorite_foods&#34;: [ &#34;sushi&#34;, &#34;pizza&#34;, &#34;tacos&#34;, &#34;ice cream&#34;, &#34;pasta&#34;, &#34;curry&#34;, &#34;Thai food&#34;, ], &#34;hobbies&#34;: [ {&#34;name&#34;: &#34;reading&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;daily&#34;}, {&#34;name&#34;: &#34;hiking&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;}, {&#34;name&#34;: &#34;photography&#34;, &#34;skill_level&#34;: &#34;beginner&#34;, &#34;frequency&#34;: &#34;monthly&#34;}, {&#34;name&#34;: &#34;biking&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;}, {&#34;name&#34;: &#34;swimming&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;weekly&#34;}, {&#34;name&#34;: &#34;weaving&#34;, &#34;skill_level&#34;: &#34;beginner&#34;, &#34;frequency&#34;: &#34;weekly&#34;}, {&#34;name&#34;: &#34;painting&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;}, {&#34;name&#34;: &#34;cooking&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;daily&#34;}, { &#34;name&#34;: &#34;machine learning projects&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;free time&#34;, }, ], &#34;age&#34;: 28, &#34;occupation&#34;: &#34;Data Scientist&#34;, &#34;address&#34;: { &#34;street&#34;: &#34;New Apartment&#34;, &#34;city&#34;: &#34;New York&#34;, &#34;country&#34;: &#34;USA&#34;, &#34;postal_code&#34;: &#34;unknown&#34;, }, &#34;favorite_color&#34;: &#34;blue&#34;, &#34;pets&#34;: [ {&#34;kind&#34;: &#34;cat&#34;, &#34;name&#34;: &#34;Luna&#34;, &#34;age&#34;: 4}, {&#34;kind&#34;: &#34;dog&#34;, &#34;name&#34;: &#34;Max&#34;, &#34;age&#34;: 2}, ], &#34;languages&#34;: {}, } &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You&#39;ll notice that all the &#34;languages&#34; section was dropped here, and &#34;The Mandalorian&#34; was omitted. Alex may be injured, but he didn&#39;t forget how to speak!&lt;/p&gt; &#xA;&lt;p&gt;When you run this code, it&#39;s &lt;em&gt;possible&lt;/em&gt; it will get it right: LLMs are stochastic after all (which is a good thing). And you could definitely prompt engineer it to be more reliable, but &lt;strong&gt;that&#39;s not good enough.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For memory management, you will be updating objects &lt;strong&gt;constantly&lt;/strong&gt;, and it&#39;s still &lt;strong&gt;too easy&lt;/strong&gt; for LLMs to &#34;accidentally&#34; omit information when generating updates, or to miss content in the conversation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;trustcall&lt;/code&gt; lets the LLM &lt;strong&gt;focus on what has changed&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Trustcall approach&#xA;from trustcall import create_extractor&#xA;&#xA;bound = create_extractor(llm, tools=[User])&#xA;&#xA;trustcall_result = bound.invoke(&#xA;    {&#xA;        &#34;messages&#34;: [&#xA;            {&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: f&#34;&#34;&#34;Update the memory (JSON doc) to incorporate new information from the following conversation:&#xA;&amp;lt;convo&amp;gt;&#xA;{conversation}&#xA;&amp;lt;/convo&amp;gt;&#34;&#34;&#34;,&#xA;            }&#xA;        ],&#xA;        &#34;existing&#34;: {&#34;User&#34;: initial_user.model_dump()},&#xA;    }&#xA;)&#xA;print(&#34;\nTrustcall approach result:&#34;)&#xA;trustcall_output = trustcall_result[&#34;responses&#34;][0].model_dump()&#xA;print(trustcall_output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;`trustcall` output&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#xA;&#34;preferred_name&#34;: &#34;Alex&#34;,&#xA;&#34;favorite_media&#34;: {&#xA;    &#34;shows&#34;: [&#xA;        &#34;Friends&#34;,&#xA;        &#34;Game of Thrones&#34;,&#xA;        &#34;Breaking Bad&#34;,&#xA;        &#34;The Office&#34;,&#xA;        &#34;The Mandalorian&#34;,&#xA;    ],&#xA;    &#34;movies&#34;: [&#xA;        &#34;The Shawshank Redemption&#34;,&#xA;        &#34;Inception&#34;,&#xA;        &#34;The Dark Knight&#34;,&#xA;        &#34;Parasite&#34;,&#xA;    ],&#xA;    &#34;books&#34;: [&#34;1984&#34;, &#34;To Kill a Mockingbird&#34;, &#34;The Great Gatsby&#34;],&#xA;},&#xA;&#34;favorite_foods&#34;: [&#xA;    &#34;sushi&#34;,&#xA;    &#34;pizza&#34;,&#xA;    &#34;tacos&#34;,&#xA;    &#34;ice cream&#34;,&#xA;    &#34;pasta&#34;,&#xA;    &#34;curry&#34;,&#xA;    &#34;Thai food&#34;,&#xA;],&#xA;&#34;hobbies&#34;: [&#xA;    {&#34;name&#34;: &#34;reading&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;daily&#34;},&#xA;    {&#34;name&#34;: &#34;hiking&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;},&#xA;    {&#34;name&#34;: &#34;photography&#34;, &#34;skill_level&#34;: &#34;beginner&#34;, &#34;frequency&#34;: &#34;monthly&#34;},&#xA;    {&#34;name&#34;: &#34;biking&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;},&#xA;    {&#34;name&#34;: &#34;swimming&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;weekly&#34;},&#xA;    {&#34;name&#34;: &#34;weaving&#34;, &#34;skill_level&#34;: &#34;beginner&#34;, &#34;frequency&#34;: &#34;weekly&#34;},&#xA;    {&#34;name&#34;: &#34;painting&#34;, &#34;skill_level&#34;: &#34;intermediate&#34;, &#34;frequency&#34;: &#34;weekly&#34;},&#xA;    {&#34;name&#34;: &#34;cooking&#34;, &#34;skill_level&#34;: &#34;expert&#34;, &#34;frequency&#34;: &#34;daily&#34;},&#xA;    {&#xA;        &#34;name&#34;: &#34;machine learning projects&#34;,&#xA;        &#34;skill_level&#34;: &#34;intermediate&#34;,&#xA;        &#34;frequency&#34;: &#34;daily&#34;,&#xA;    },&#xA;],&#xA;&#34;age&#34;: 28,&#xA;&#34;occupation&#34;: &#34;Data Scientist&#34;,&#xA;&#34;address&#34;: {&#xA;    &#34;street&#34;: &#34;New Apartment&#34;,&#xA;    &#34;city&#34;: &#34;New York&#34;,&#xA;    &#34;country&#34;: &#34;USA&#34;,&#xA;    &#34;postal_code&#34;: &#34;10001&#34;,&#xA;},&#xA;&#34;favorite_color&#34;: &#34;blue&#34;,&#xA;&#34;pets&#34;: [&#xA;    {&#34;kind&#34;: &#34;cat&#34;, &#34;name&#34;: &#34;Luna&#34;, &#34;age&#34;: 4},&#xA;    {&#34;kind&#34;: &#34;dog&#34;, &#34;name&#34;: &#34;Max&#34;, &#34;age&#34;: 2},&#xA;],&#xA;&#34;languages&#34;: {&#xA;    &#34;English&#34;: &#34;native&#34;,&#xA;    &#34;Spanish&#34;: &#34;intermediate&#34;,&#xA;    &#34;Python&#34;: &#34;expert&#34;,&#xA;    &#34;French&#34;: &#34;beginner&#34;,&#xA;},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;No fields omitted, and the important new information is seamlessly integrated.&lt;/p&gt; &#xA;&lt;h3&gt;Simultanous updates &amp;amp; insertions&lt;/h3&gt; &#xA;&lt;p&gt;Both problems above (difficulty with type-safe generation of complex schemas &amp;amp; difficulty with generating the correct edits to existing schemas) are compounded when you have to be prompting the LLM to handle &lt;strong&gt;both&lt;/strong&gt; updates &lt;strong&gt;and&lt;/strong&gt; inserts, as is often the case when extracting multiple memory &#34;events&#34; from conversations.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see an example below. Suppose you are managing a list of &#34;relationships&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import uuid&#xA;from typing import List, Optional&#xA;&#xA;from pydantic import BaseModel, Field&#xA;&#xA;&#xA;class Person(BaseModel):&#xA;    &#34;&#34;&#34;Someone the user knows or interacts with.&#34;&#34;&#34;&#xA;&#xA;    name: str&#xA;    relationship: str = Field(description=&#34;How they relate to the user.&#34;)&#xA;&#xA;    notes: List[str] = Field(&#xA;        description=&#34;Memories and other observations about the person&#34;&#xA;    )&#xA;&#xA;&#xA;# Initial data&#xA;initial_people = [&#xA;    Person(&#xA;        name=&#34;Emma Thompson&#34;,&#xA;        relationship=&#34;College friend&#34;,&#xA;        notes=[&#34;Loves hiking&#34;, &#34;Works in marketing&#34;, &#34;Has a dog named Max&#34;],&#xA;    ),&#xA;    Person(&#xA;        name=&#34;Michael Chen&#34;,&#xA;        relationship=&#34;Coworker&#34;,&#xA;        notes=[&#34;Great at problem-solving&#34;, &#34;Vegetarian&#34;, &#34;Plays guitar&#34;],&#xA;    ),&#xA;    Person(&#xA;        name=&#34;Sarah Johnson&#34;,&#xA;        relationship=&#34;Neighbor&#34;,&#xA;        notes=[&#34;Has two kids&#34;, &#34;Loves gardening&#34;, &#34;Makes amazing cookies&#34;],&#xA;    ),&#xA;]&#xA;&#xA;# Convert to the format expected by the extractor&#xA;existing_data = [&#xA;    (str(i), &#34;Person&#34;, person.model_dump()) for i, person in enumerate(initial_people)&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conversation = &#34;&#34;&#34;&#xA;Me: I ran into Emma Thompson at the park yesterday. She was walking her new puppy, a golden retriever named Sunny. She mentioned she got promoted to Senior Marketing Manager last month.&#xA;Friend: That&#39;s great news for Emma! How&#39;s she enjoying the new role?&#xA;Me: She seems to be thriving. Oh, and did you know she&#39;s taken up rock climbing? She invited me to join her at the climbing gym sometime.&#xA;Friend: Wow, rock climbing? That&#39;s quite a change from hiking. Speaking of friends, have you heard from Michael Chen recently?&#xA;Me: Actually, yes. We had a video call last week. He&#39;s switched jobs and is now working as a Data Scientist at a startup. He&#39;s also mentioned he&#39;s thinking of going vegan.&#xA;Friend: That&#39;s a big change for Michael! Both career and diet-wise. How about your neighbor, Sarah? Is she still teaching?&#xA;Me: Sarah&#39;s doing well. Her kids are growing up fast - her oldest just started middle school. She&#39;s still teaching, but now she&#39;s focusing on special education. She&#39;s really passionate about it.&#xA;Friend: That&#39;s wonderful. Oh, before I forget, I wanted to introduce you to my cousin who just moved to town. Her name is Olivia Davis, she&#39;s a 27-year-old graphic designer. She&#39;s looking to meet new people and expand her social circle. I thought you two might get along well.&#xA;Me: That sounds great! I&#39;d love to meet her. Maybe we could all get together for coffee next week?&#xA;Friend: Perfect! I&#39;ll set it up. Olivia loves art and is always sketching in her free time. She also volunteers at the local animal shelter on weekends.&#xA;&#34;&#34;&#34;&#xA;&#xA;from langchain_openai import ChatOpenAI&#xA;&#xA;# Now, let&#39;s use the extractor to update existing entries and create new ones&#xA;from trustcall import create_extractor&#xA;&#xA;llm = ChatOpenAI(model=&#34;gpt-4o&#34;)&#xA;&#xA;extractor = create_extractor(&#xA;    llm,&#xA;    tools=[Person],&#xA;    tool_choice=&#34;any&#34;,&#xA;    enable_inserts=True,&#xA;)&#xA;&#xA;result = extractor.invoke(&#xA;    {&#xA;        &#34;messages&#34;: [&#xA;            {&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: f&#34;Update existing person records and create new ones based on the following conversation:\n\n{conversation}&#34;,&#xA;            }&#xA;        ],&#xA;        &#34;existing&#34;: existing_data,&#xA;    }&#xA;)&#xA;&#xA;# Print the results&#xA;print(&#34;Updated and new person records:&#34;)&#xA;for r, rmeta in zip(result[&#34;responses&#34;], result[&#34;response_metadata&#34;]):&#xA;    print(f&#34;ID: {rmeta.get(&#39;json_doc_id&#39;, &#39;New&#39;)}&#34;)&#xA;    print(r.model_dump_json(indent=2))&#xA;    print()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The LLM is able to update existing values while also inserting new ones!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Updated and new person records:&#xA;ID: 0&#xA;{&#xA;  &#34;name&#34;: &#34;Emma Thompson&#34;,&#xA;  &#34;relationship&#34;: &#34;College friend&#34;,&#xA;  &#34;notes&#34;: [&#xA;    &#34;Loves hiking&#34;,&#xA;    &#34;Works in marketing&#34;,&#xA;    &#34;Has a dog named Max&#34;,&#xA;    &#34;Walking her new puppy, a golden retriever named Sunny&#34;,&#xA;    &#34;Promoted to Senior Marketing Manager&#34;,&#xA;    &#34;Taken up rock climbing&#34;&#xA;  ]&#xA;}&#xA;&#xA;ID: 1&#xA;{&#xA;  &#34;name&#34;: &#34;Michael Chen&#34;,&#xA;  &#34;relationship&#34;: &#34;Coworker&#34;,&#xA;  &#34;notes&#34;: [&#xA;    &#34;Great at problem-solving&#34;,&#xA;    &#34;Vegetarian&#34;,&#xA;    &#34;Plays guitar&#34;,&#xA;    &#34;Working as a Data Scientist at a startup&#34;,&#xA;    &#34;Thinking of going vegan&#34;&#xA;  ]&#xA;}&#xA;&#xA;ID: 2&#xA;{&#xA;  &#34;name&#34;: &#34;Sarah Johnson&#34;,&#xA;  &#34;relationship&#34;: &#34;Neighbor&#34;,&#xA;  &#34;notes&#34;: [&#xA;    &#34;Has two kids&#34;,&#xA;    &#34;Loves gardening&#34;,&#xA;    &#34;Makes amazing cookies&#34;,&#xA;    &#34;Oldest child started middle school&#34;,&#xA;    &#34;Focusing on special education&#34;,&#xA;    &#34;Passionate about teaching&#34;&#xA;  ]&#xA;}&#xA;&#xA;ID: New&#xA;{&#xA;  &#34;name&#34;: &#34;Olivia Davis&#34;,&#xA;  &#34;relationship&#34;: &#34;Friend&#39;s cousin&#34;,&#xA;  &#34;notes&#34;: [&#xA;    &#34;27-year-old graphic designer&#34;,&#xA;    &#34;Looking to meet new people&#34;,&#xA;    &#34;Loves art and sketching&#34;,&#xA;    &#34;Volunteers at the local animal shelter on weekends&#34;&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More Examples&lt;/h2&gt; &#xA;&lt;p&gt;Trustcall works out of the box with any tool-calling LLM from the LangChain ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;First, install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U trustcall langchain-fireworks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, set up your schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List&#xA;&#xA;from langchain_fireworks import ChatFireworks&#xA;from pydantic.v1 import BaseModel, Field, validator&#xA;from trustcall import create_extractor&#xA;&#xA;&#xA;class Preferences(BaseModel):&#xA;    foods: List[str] = Field(description=&#34;Favorite foods&#34;)&#xA;&#xA;    @validator(&#34;foods&#34;)&#xA;    def at_least_three_foods(cls, v):&#xA;        # Just a silly example to show how it can recover from a&#xA;        # validation error.&#xA;        if len(v) &amp;lt; 3:&#xA;            raise ValueError(&#34;Must have at least three favorite foods&#34;)&#xA;        return v&#xA;&#xA;&#xA;llm = ChatFireworks(model=&#34;accounts/fireworks/models/firefunction-v2&#34;)&#xA;&#xA;extractor = create_extractor(llm, tools=[Preferences], tool_choice=&#34;Preferences&#34;)&#xA;res = extractor.invoke({&#34;messages&#34;: [(&#34;user&#34;, &#34;I like apple pie and ice cream.&#34;)]})&#xA;msg = res[&#34;messages&#34;][-1]&#xA;print(msg.tool_calls)&#xA;print(res[&#34;responses&#34;])&#xA;# [{&#39;id&#39;: &#39;call_pBrHTBNHNLnGCv7UBKBJz6xf&#39;, &#39;name&#39;: &#39;Preferences&#39;, &#39;args&#39;: {&#39;foods&#39;: [&#39;apple pie&#39;, &#39;ice cream&#39;, &#39;pizza&#39;, &#39;sushi&#39;]}}]&#xA;# [Preferences(foods=[&#39;apple pie&#39;, &#39;ice cream&#39;, &#39;pizza&#39;, &#39;sushi&#39;])]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since the extractor also returns the chat message (with validated and cleaned tools), you can easily use the abstraction for conversational agent applications:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import operator&#xA;from datetime import datetime&#xA;from typing import List&#xA;&#xA;import pytz&#xA;from langchain_fireworks import ChatFireworks&#xA;from langgraph.checkpoint.memory import MemorySaver&#xA;from langgraph.graph import START, StateGraph&#xA;from langgraph.prebuilt import ToolNode, tools_condition&#xA;from pydantic.v1 import BaseModel, Field, validator&#xA;from trustcall import create_extractor&#xA;from typing_extensions import Annotated, TypedDict&#xA;&#xA;&#xA;class Preferences(BaseModel):&#xA;    foods: List[str] = Field(description=&#34;Favorite foods&#34;)&#xA;&#xA;    @validator(&#34;foods&#34;)&#xA;    def at_least_three_foods(cls, v):&#xA;        if len(v) &amp;lt; 3:&#xA;            raise ValueError(&#34;Must have at least three favorite foods&#34;)&#xA;        return v&#xA;&#xA;&#xA;llm = ChatFireworks(model=&#34;accounts/fireworks/models/firefunction-v2&#34;)&#xA;&#xA;&#xA;def save_user_information(preferences: Preferences):&#xA;    &#34;&#34;&#34;Save user information to a database.&#34;&#34;&#34;&#xA;    return &#34;User information saved&#34;&#xA;&#xA;&#xA;def lookup_time(tz: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Lookup the current time in a given timezone.&#34;&#34;&#34;&#xA;    try:&#xA;        # Convert the timezone string to a timezone object&#xA;        timezone = pytz.timezone(tz)&#xA;        # Get the current time in the given timezone&#xA;        tm = datetime.now(timezone)&#xA;        return f&#34;The current time in {tz} is {tm.strftime(&#39;%H:%M:%S&#39;)}&#34;&#xA;    except pytz.UnknownTimeZoneError:&#xA;        return f&#34;Unknown timezone: {tz}&#34;&#xA;&#xA;&#xA;agent = create_extractor(llm, tools=[save_user_information, lookup_time])&#xA;&#xA;&#xA;class State(TypedDict):&#xA;    messages: Annotated[list, operator.add]&#xA;&#xA;&#xA;builder = StateGraph(State)&#xA;builder.add_node(&#34;agent&#34;, agent)&#xA;builder.add_node(&#34;tools&#34;, ToolNode([save_user_information, lookup_time]))&#xA;builder.add_edge(&#34;tools&#34;, &#34;agent&#34;)&#xA;builder.add_edge(START, &#34;agent&#34;)&#xA;builder.add_conditional_edges(&#34;agent&#34;, tools_condition)&#xA;&#xA;graph = builder.compile(checkpointer=MemorySaver())&#xA;config = {&#34;configurable&#34;: {&#34;thread_id&#34;: &#34;1234&#34;}}&#xA;res = graph.invoke({&#34;messages&#34;: [(&#34;user&#34;, &#34;Hi there!&#34;)]}, config)&#xA;res[&#34;messages&#34;][-1].pretty_print()&#xA;# ================================== Ai Message ==================================&#xA;&#xA;# I&#39;m happy to help you with any questions or tasks you have. What&#39;s on your mind today?&#xA;res = graph.invoke(&#xA;    {&#34;messages&#34;: [(&#34;user&#34;, &#34;Curious; what&#39;s the time in denver right now?&#34;)]}, config&#xA;)&#xA;res[&#34;messages&#34;][-1].pretty_print()&#xA;# ================================== Ai Message ==================================&#xA;&#xA;# The current time in Denver is 00:57:25.&#xA;res = graph.invoke(&#xA;    {&#xA;        &#34;messages&#34;: [&#xA;            (&#34;user&#34;, &#34;Did you know my favorite foods are spinach and potatoes?&#34;)&#xA;        ]&#xA;    },&#xA;    config,&#xA;)&#xA;res[&#34;messages&#34;][-1].pretty_print()&#xA;# ================================== Ai Message ==================================&#xA;&#xA;# I&#39;ve saved your favorite foods, spinach and potatoes.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you check out the &lt;a href=&#34;https://smith.langchain.com/public/b83d6db1-ffb9-4817-a166-bbc5004bbc25/r/5a05f73b-1d7e-47d4-9e40-0e8aaa3faa28&#34;&gt;last call in that conversation&lt;/a&gt;, you can see that the agent initially generated an invalid tool call, but our validation was able to fix up the output before passing the payload on to our tools.&lt;/p&gt; &#xA;&lt;p&gt;These are just a couple examples to highlight what you can accomplish with &lt;code&gt;trustcall&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Explanation&lt;/h4&gt; &#xA;&lt;p&gt;You can write this yourself (I wrote and tested this in a few hours, but I bet you&#39;re faster)!&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the basic logic of the library, simply:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prompt the LLM to generate parameters for the schemas of zero or more tools.&lt;/li&gt; &#xA; &lt;li&gt;If any of these schemas raise validation errors, re-prompt the LLM to fix by generating a JSON Patch.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The extractor also accepts a dictionary of &lt;strong&gt;existing&lt;/strong&gt; schemas it can update (for situations where you have some structured representation of an object and you want to extend or update parts of it using new information.)&lt;/p&gt; &#xA;&lt;p&gt;The dictionary format is &lt;code&gt;**schema_name**: **current_schema**&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this case, the logic is simpler:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prompt the LLM to generate one or more JSON Patches for any (or all) of the existing schemas.&lt;/li&gt; &#xA; &lt;li&gt;After applying the patches, if any of these schemas are invalid, re-prompt the LLM to fix using more patches.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;trustcall&lt;/code&gt; also uses + extends some convenient utilities to let you define schemas in several ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Regular python functions (with typed arguments to apply the validation).&lt;/li&gt; &#xA; &lt;li&gt;Pydantic objects&lt;/li&gt; &#xA; &lt;li&gt;JSON schemas (we will still validate your calls using the schemas&#39; typing and constraints).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;as well as providing support for &lt;code&gt;langchain-core&lt;/code&gt;&#39;s tools.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluating&lt;/h2&gt; &#xA;&lt;p&gt;We have a simple evaluation benchmark in &lt;a href=&#34;https://raw.githubusercontent.com/hinthornw/trustcall/main/tests/evals/test_evals.py&#34;&gt;test_evals.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run, first clone the dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langsmith import Client&#xA;&#xA;Client().clone_public_dataset(&#34;https://smith.langchain.com/public/0544c02f-9617-4095-bc15-3a9af1189819/d&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the evals:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make evals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This requires some additional dependencies, as well as API keys for the models being compared.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aigc-apps/VideoX-Fun</title>
    <updated>2025-03-31T01:37:07Z</updated>
    <id>tag:github.com,2025-03-31:/aigc-apps/VideoX-Fun</id>
    <link href="https://github.com/aigc-apps/VideoX-Fun" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìπ A more flexible framework that can generate videos at any resolution and creates videos from images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VideoX-Fun&lt;/h1&gt; &#xA;&lt;p&gt;üòä Welcome!&lt;/p&gt; &#xA;&lt;p&gt;CogVideoX-Fun: &lt;a href=&#34;https://huggingface.co/spaces/alibaba-pai/CogVideoX-Fun-5b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Wan-Fun: &lt;a href=&#34;https://huggingface.co/spaces/alibaba-pai/Wan2.1-Fun-1.3B-InP&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/README_ja-JP.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#video-result&#34;&gt;Video Result&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#how-to-use&#34;&gt;How to use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#model-zoo&#34;&gt;Model zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;VideoX-Fun is a video generation pipeline that can be used to generate AI images and videos, as well as to train baseline and Lora models for Diffusion Transformer. We support direct prediction from pre-trained baseline models to generate videos with different resolutions, durations, and FPS. Additionally, we also support users in training their own baseline and Lora models to perform specific style transformations.&lt;/p&gt; &#xA;&lt;p&gt;We will support quick pull-ups from different platforms, refer to &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;What&#39;s New:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update Wan2.1-Fun-V1.0: Support I2V and Control models for 14B and 1.3B models, with support for start and end frame prediction. [2025.03.26]&lt;/li&gt; &#xA; &lt;li&gt;Update CogVideoX-Fun-V1.5: Upload I2V model and related training/prediction code. [2024.12.16]&lt;/li&gt; &#xA; &lt;li&gt;Reward Lora Support: Train Lora using reward backpropagation techniques to optimize generated videos, making them better aligned with human preferences. &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/README_TRAIN_REWARD.md&#34;&gt;More Information&lt;/a&gt;. New version of the control model supports various control conditions such as Canny, Depth, Pose, MLSD, etc. [2024.11.21]&lt;/li&gt; &#xA; &lt;li&gt;Diffusers Support: CogVideoX-Fun Control is now supported in diffusers. Thanks to &lt;a href=&#34;https://github.com/a-r-r-o-w&#34;&gt;a-r-r-o-w&lt;/a&gt; for contributing support in this &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/9671&#34;&gt;PR&lt;/a&gt;. Check out the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/cogvideox&#34;&gt;documentation&lt;/a&gt; for more details. [2024.10.16]&lt;/li&gt; &#xA; &lt;li&gt;Update CogVideoX-Fun-V1.1: Retrain i2v model, add Noise to increase the motion amplitude of the video. Upload control model training code and Control model. [2024.09.29]&lt;/li&gt; &#xA; &lt;li&gt;Update CogVideoX-Fun-V1.0: Initial code release! Now supports Windows and Linux. Supports video generation at arbitrary resolutions from 256x256x49 to 1024x1024x49 for 2B and 5B models. [2024.09.18]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FunctionÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#data-preprocess&#34;&gt;Data Preprocessing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#dit-train&#34;&gt;Train DiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#video-gen&#34;&gt;Video Generation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our UI interface is as follows: &lt;img src=&#34;https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/ui.jpg&#34; alt=&#34;ui&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h3&gt;1. Cloud usage: AliyunDSW/Docker&lt;/h3&gt; &#xA;&lt;h4&gt;a. From AliyunDSW&lt;/h4&gt; &#xA;&lt;p&gt;DSW has free GPU time, which can be applied once by a user and is valid for 3 months after applying.&lt;/p&gt; &#xA;&lt;p&gt;Aliyun provide free GPU time in &lt;a href=&#34;https://free.aliyun.com/?product=9602825&amp;amp;crowd=enterprise&amp;amp;spm=5176.28055625.J_5831864660.1.e939154aRgha4e&amp;amp;scm=20140722.M_9974135.P_110.MO_1806-ID_9974135-MID_9974135-CID_30683-ST_8512-V_1&#34;&gt;Freetier&lt;/a&gt;, get it and use in Aliyun PAI-DSW to start CogVideoX-Fun within 5min!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/cv/cogvideox_fun&#34;&gt;&lt;img src=&#34;https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/asset/dsw.png&#34; alt=&#34;DSW Notebook&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;b. From ComfyUI&lt;/h4&gt; &#xA;&lt;p&gt;Our ComfyUI is as follows, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/comfyui/README.md&#34;&gt;ComfyUI README&lt;/a&gt; for details. &lt;img src=&#34;https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/cogvideoxfunv1_workflow_i2v.jpg&#34; alt=&#34;workflow graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;c. From docker&lt;/h4&gt; &#xA;&lt;p&gt;If you are using docker, please make sure that the graphics card driver and CUDA environment have been installed correctly in your machine.&lt;/p&gt; &#xA;&lt;p&gt;Then execute the following commands in this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# pull image&#xA;docker pull mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:cogvideox_fun&#xA;&#xA;# enter image&#xA;docker run -it -p 7860:7860 --network host --gpus all --security-opt seccomp:unconfined --shm-size 200g mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:cogvideox_fun&#xA;&#xA;# clone code&#xA;git clone https://github.com/aigc-apps/CogVideoX-Fun.git&#xA;&#xA;# enter CogVideoX-Fun&#39;s dir&#xA;cd CogVideoX-Fun&#xA;&#xA;# download weights&#xA;mkdir models/Diffusion_Transformer&#xA;mkdir models/Personalized_Model&#xA;&#xA;# Please use the hugginface link or modelscope link to download the model.&#xA;# CogVideoX-Fun&#xA;# https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-5b-InP&#xA;# https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-5b-InP&#xA;&#xA;# Wan&#xA;# https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP&#xA;# https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Local install: Environment Check/Downloading/Installation&lt;/h3&gt; &#xA;&lt;h4&gt;a. Environment Check&lt;/h4&gt; &#xA;&lt;p&gt;We have verified this repo execution on the following environment:&lt;/p&gt; &#xA;&lt;p&gt;The detailed of Windows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Windows 10&lt;/li&gt; &#xA; &lt;li&gt;python: python3.10 &amp;amp; python3.11&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.2.0&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.8 &amp;amp; 12.1&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;GPUÔºö Nvidia-3060 12G &amp;amp; Nvidia-3090 24G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The detailed of Linux:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS&lt;/li&gt; &#xA; &lt;li&gt;python: python3.10 &amp;amp; python3.11&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.2.0&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.8 &amp;amp; 12.1&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;GPUÔºöNvidia-V100 16G &amp;amp; Nvidia-A10 24G &amp;amp; Nvidia-A100 40G &amp;amp; Nvidia-A100 80G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We need about 60GB available on disk (for saving weights), please check!&lt;/p&gt; &#xA;&lt;h4&gt;b. Weights&lt;/h4&gt; &#xA;&lt;p&gt;We&#39;d better place the &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#model-zoo&#34;&gt;weights&lt;/a&gt; along the specified path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;üì¶ models/&#xA;‚îú‚îÄ‚îÄ üìÇ Diffusion_Transformer/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ üìÇ CogVideoX-Fun-V1.1-2b-InP/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ üìÇ CogVideoX-Fun-V1.1-5b-InP/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Wan2.1-Fun-14B-InP&#xA;‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Wan2.1-Fun-1.3B-InP/&#xA;‚îú‚îÄ‚îÄ üìÇ Personalized_Model/&#xA;‚îÇ   ‚îî‚îÄ‚îÄ your trained trainformer model / your trained lora model (for UI load)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Video Result&lt;/h1&gt; &#xA;&lt;h3&gt;Wan2.1-Fun-14B-InP &amp;amp;&amp;amp; Wan2.1-Fun-1.3B-InP&lt;/h3&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/bd72a276-e60e-4b5d-86c1-d0f67e7425b9&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/cb7aef09-52c2-4973-80b4-b2fb63425044&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/4e10d491-f1cf-4b08-a7c5-1e01e5418140&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/f7e363a9-be09-4b72-bccf-cce9c9ebeb9b&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/28f3e720-8acc-4f22-a5d0-ec1c571e9466&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/fb6e4cb9-270d-47cd-8501-caf8f3e91b5c&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/989a4644-e33b-4f0c-b68e-2ff6ba37ac7e&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/9c604fa7-8657-49d1-8066-b5bb198b28b6&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Wan2.1-Fun-14B-Control &amp;amp;&amp;amp; Wan2.1-Fun-1.3B-Control&lt;/h3&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/f35602c4-9f0a-4105-9762-1e3a88abbac6&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/8b0f0e87-f1be-4915-bb35-2d53c852333e&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/972012c1-772b-427a-bce6-ba8b39edcfad&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/53002ce2-dd18-4d4f-8135-b6f68364cabd&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/fce43c0b-81fa-4ab2-9ca7-78d786f520e6&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/b208b92c-5add-4ece-a200-3dbbe47b93c3&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/c6c5d557-9772-483e-ae47-863d8a26db4a&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/af617971-597c-4be4-beb5-f9e8aaca2d14&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/8411151e-f491-4264-8368-7fc3c5a6992b&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CogVideoX-Fun-V1.1-5B&lt;/h3&gt; &#xA;&lt;p&gt;Resolution-1024&lt;/p&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/34e7ec8f-293e-4655-bb14-5e1ee476f788&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/7809c64f-eb8c-48a9-8bdc-ca9261fd5434&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/8e76aaa4-c602-44ac-bcb4-8b24b72c386c&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/19dba894-7c35-4f25-b15c-384167ab3b03&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Resolution-768&lt;/p&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/0bc339b9-455b-44fd-8917-80272d702737&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/70a043b9-6721-4bd9-be47-78b7ec5c27e9&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/d5dd6c09-14f3-40f8-8b6d-91e26519b8ac&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/9327e8bc-4f17-46b0-b50d-38c250a9483a&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Resolution-512&lt;/p&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/ef407030-8062-454d-aba3-131c21e6b58c&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/7610f49e-38b6-4214-aa48-723ae4d1b07e&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/1fff0567-1e15-415c-941e-53ee8ae2c841&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/bcec48da-b91b-43a0-9d50-cf026e00fa4f&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CogVideoX-Fun-V1.1-5B-Control&lt;/h3&gt; &#xA;&lt;table border=&#34;0&#34; style=&#34;width: 100%; text-align: left; margin-top: 20px;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/53002ce2-dd18-4d4f-8135-b6f68364cabd&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/fce43c0b-81fa-4ab2-9ca7-78d786f520e6&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/b208b92c-5add-4ece-a200-3dbbe47b93c3&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; A young woman with beautiful clear eyes and blonde hair, wearing white clothes and twisting her body, with the camera focused on her face. High quality, masterpiece, best quality, high resolution, ultra-fine, dreamlike. &lt;/td&gt; &#xA;   &lt;td&gt; A young woman with beautiful clear eyes and blonde hair, wearing white clothes and twisting her body, with the camera focused on her face. High quality, masterpiece, best quality, high resolution, ultra-fine, dreamlike. &lt;/td&gt; &#xA;   &lt;td&gt; A young bear. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/ea908454-684b-4d60-b562-3db229a250a9&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/ffb7c6fc-8b69-453b-8aad-70dfae3899b9&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/d3f757a3-3551-4dcb-9372-7a61469813f5&#34; width=&#34;100%&#34; controls autoplay loop&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h1&gt;How to Use&lt;/h1&gt; &#xA;&lt;h3 id=&#34;video-gen&#34;&gt;1. Generation&lt;/h3&gt; &#xA;&lt;h4&gt;a. GPU Memory Optimization&lt;/h4&gt; &#xA;&lt;p&gt;Since Wan2.1 has a very large number of parameters, we need to consider memory optimization strategies to adapt to consumer-grade GPUs. We provide &lt;code&gt;GPU_memory_mode&lt;/code&gt; for each prediction file, allowing you to choose between &lt;code&gt;model_cpu_offload&lt;/code&gt;, &lt;code&gt;model_cpu_offload_and_qfloat8&lt;/code&gt;, and &lt;code&gt;sequential_cpu_offload&lt;/code&gt;. This solution is also applicable to CogVideoX-Fun generation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_cpu_offload&lt;/code&gt;: The entire model is moved to the CPU after use, saving some GPU memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_cpu_offload_and_qfloat8&lt;/code&gt;: The entire model is moved to the CPU after use, and the transformer model is quantized to float8, saving more GPU memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sequential_cpu_offload&lt;/code&gt;: Each layer of the model is moved to the CPU after use. It is slower but saves a significant amount of GPU memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;qfloat8&lt;/code&gt; may slightly reduce model performance but saves more GPU memory. If you have sufficient GPU memory, it is recommended to use &lt;code&gt;model_cpu_offload&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;b. Using ComfyUI&lt;/h4&gt; &#xA;&lt;p&gt;For details, refer to &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/comfyui/README.md&#34;&gt;ComfyUI README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;c. Running Python Files&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Download the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#model-zoo&#34;&gt;weights&lt;/a&gt; and place them in the &lt;code&gt;models&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Use different files for prediction based on the weights and prediction goals. This library currently supports CogVideoX-Fun, Wan2.1, and Wan2.1-Fun. Different models are distinguished by folder names under the &lt;code&gt;examples&lt;/code&gt; folder, and their supported features vary. Use them accordingly. Below is an example using CogVideoX-Fun: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Text-to-Video&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Modify &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;neg_prompt&lt;/code&gt;, &lt;code&gt;guidance_scale&lt;/code&gt;, and &lt;code&gt;seed&lt;/code&gt; in the file &lt;code&gt;examples/cogvideox_fun/predict_t2v.py&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Run the file &lt;code&gt;examples/cogvideox_fun/predict_t2v.py&lt;/code&gt; and wait for the results. The generated videos will be saved in the folder &lt;code&gt;samples/cogvideox-fun-videos&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Image-to-Video&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Modify &lt;code&gt;validation_image_start&lt;/code&gt;, &lt;code&gt;validation_image_end&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;neg_prompt&lt;/code&gt;, &lt;code&gt;guidance_scale&lt;/code&gt;, and &lt;code&gt;seed&lt;/code&gt; in the file &lt;code&gt;examples/cogvideox_fun/predict_i2v.py&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;validation_image_start&lt;/code&gt; is the starting image of the video, and &lt;code&gt;validation_image_end&lt;/code&gt; is the ending image of the video.&lt;/li&gt; &#xA;     &lt;li&gt;Run the file &lt;code&gt;examples/cogvideox_fun/predict_i2v.py&lt;/code&gt; and wait for the results. The generated videos will be saved in the folder &lt;code&gt;samples/cogvideox-fun-videos_i2v&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Video-to-Video&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Modify &lt;code&gt;validation_video&lt;/code&gt;, &lt;code&gt;validation_image_end&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;neg_prompt&lt;/code&gt;, &lt;code&gt;guidance_scale&lt;/code&gt;, and &lt;code&gt;seed&lt;/code&gt; in the file &lt;code&gt;examples/cogvideox_fun/predict_v2v.py&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;validation_video&lt;/code&gt; is the reference video for video-to-video generation. You can use the following demo video: &lt;a href=&#34;https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/play_guitar.mp4&#34;&gt;Demo Video&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Run the file &lt;code&gt;examples/cogvideox_fun/predict_v2v.py&lt;/code&gt; and wait for the results. The generated videos will be saved in the folder &lt;code&gt;samples/cogvideox-fun-videos_v2v&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Controlled Video Generation (Canny, Pose, Depth, etc.)&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Modify &lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;validation_image_end&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;neg_prompt&lt;/code&gt;, &lt;code&gt;guidance_scale&lt;/code&gt;, and &lt;code&gt;seed&lt;/code&gt; in the file &lt;code&gt;examples/cogvideox_fun/predict_v2v_control.py&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;control_video&lt;/code&gt; is the control video extracted using operators such as Canny, Pose, or Depth. You can use the following demo video: &lt;a href=&#34;https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1.1/pose.mp4&#34;&gt;Demo Video&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Run the file &lt;code&gt;examples/cogvideox_fun/predict_v2v_control.py&lt;/code&gt; and wait for the results. The generated videos will be saved in the folder &lt;code&gt;samples/cogvideox-fun-videos_v2v_control&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: If you want to integrate other backbones or Loras trained by yourself, modify &lt;code&gt;lora_path&lt;/code&gt; and relevant paths in &lt;code&gt;examples/{model_name}/predict_t2v.py&lt;/code&gt; or &lt;code&gt;examples/{model_name}/predict_i2v.py&lt;/code&gt; as needed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;d. Using the Web UI&lt;/h4&gt; &#xA;&lt;p&gt;The web UI supports text-to-video, image-to-video, video-to-video, and controlled video generation (Canny, Pose, Depth, etc.). This library currently supports CogVideoX-Fun, Wan2.1, and Wan2.1-Fun. Different models are distinguished by folder names under the &lt;code&gt;examples&lt;/code&gt; folder, and their supported features vary. Use them accordingly. Below is an example using CogVideoX-Fun:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Download the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/#model-zoo&#34;&gt;weights&lt;/a&gt; and place them in the &lt;code&gt;models&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Run the file &lt;code&gt;examples/cogvideox_fun/app.py&lt;/code&gt; to access the Gradio interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Select the generation model on the page, fill in &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;neg_prompt&lt;/code&gt;, &lt;code&gt;guidance_scale&lt;/code&gt;, and &lt;code&gt;seed&lt;/code&gt;, click &#34;Generate,&#34; and wait for the results. The generated videos will be saved in the &lt;code&gt;sample&lt;/code&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Model Training&lt;/h3&gt; &#xA;&lt;p&gt;A complete model training pipeline should include data preprocessing and Video DiT training. The training process for different models is similar, and the data formats are also similar:&lt;/p&gt; &#xA;&lt;h4 id=&#34;data-preprocess&#34;&gt;a. data preprocessing&lt;/h4&gt; &#xA;&lt;p&gt;We have provided a simple demo of training the Lora model through image data, which can be found in the &lt;a href=&#34;https://github.com/aigc-apps/CogVideoX-Fun/wiki/Training-Lora&#34;&gt;wiki&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;A complete data preprocessing link for long video segmentation, cleaning, and description can refer to &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/cogvideox/video_caption/README.md&#34;&gt;README&lt;/a&gt; in the video captions section.&lt;/p&gt; &#xA;&lt;p&gt;If you want to train a text to image and video generation model. You need to arrange the dataset in this format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;üì¶ project/&#xA;‚îú‚îÄ‚îÄ üìÇ datasets/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ üìÇ internal_datasets/&#xA;‚îÇ       ‚îú‚îÄ‚îÄ üìÇ train/&#xA;‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 00000001.mp4&#xA;‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 00000002.jpg&#xA;‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ .....&#xA;‚îÇ       ‚îî‚îÄ‚îÄ üìÑ json_of_internal_datasets.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The json_of_internal_datasets.json is a standard JSON file. The file_path in the json can to be set as relative path, as shown in below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#xA;      &#34;file_path&#34;: &#34;train/00000001.mp4&#34;,&#xA;      &#34;text&#34;: &#34;A group of young men in suits and sunglasses are walking down a city street.&#34;,&#xA;      &#34;type&#34;: &#34;video&#34;&#xA;    },&#xA;    {&#xA;      &#34;file_path&#34;: &#34;train/00000002.jpg&#34;,&#xA;      &#34;text&#34;: &#34;A group of young men in suits and sunglasses are walking down a city street.&#34;,&#xA;      &#34;type&#34;: &#34;image&#34;&#xA;    },&#xA;    .....&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also set the path as absolute path as follow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#xA;      &#34;file_path&#34;: &#34;/mnt/data/videos/00000001.mp4&#34;,&#xA;      &#34;text&#34;: &#34;A group of young men in suits and sunglasses are walking down a city street.&#34;,&#xA;      &#34;type&#34;: &#34;video&#34;&#xA;    },&#xA;    {&#xA;      &#34;file_path&#34;: &#34;/mnt/data/train/00000001.jpg&#34;,&#xA;      &#34;text&#34;: &#34;A group of young men in suits and sunglasses are walking down a city street.&#34;,&#xA;      &#34;type&#34;: &#34;image&#34;&#xA;    },&#xA;    .....&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4 id=&#34;dit-train&#34;&gt;b. Video DiT training &lt;/h4&gt; &#xA;&lt;p&gt;If the data format is relative path during data preprocessing, please set &lt;code&gt;scripts/{model_name}/train.sh&lt;/code&gt; as follow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export DATASET_NAME=&#34;datasets/internal_datasets/&#34;&#xA;export DATASET_META_NAME=&#34;datasets/internal_datasets/json_of_internal_datasets.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the data format is absolute path during data preprocessing, please set &lt;code&gt;scripts/train.sh&lt;/code&gt; as follow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export DATASET_NAME=&#34;&#34;&#xA;export DATASET_META_NAME=&#34;/mnt/data/json_of_internal_datasets.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we run scripts/train.sh.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For details on some parameter settings: Wan2.1-Fun can be found in &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/wan2.1_fun/README_TRAIN.md&#34;&gt;Readme Train&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/wan2.1_fun/README_TRAIN_LORA.md&#34;&gt;Readme Lora&lt;/a&gt;. Wan2.1 can be found in &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/wan2.1/README_TRAIN.md&#34;&gt;Readme Train&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/wan2.1/README_TRAIN_LORA.md&#34;&gt;Readme Lora&lt;/a&gt;. CogVideoX-Fun can be found in &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/cogvideox_fun/README_TRAIN.md&#34;&gt;Readme Train&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/scripts/cogvideox_fun/README_TRAIN_LORA.md&#34;&gt;Readme Lora&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Model zoo&lt;/h1&gt; &#xA;&lt;h2&gt;1. Wan2.1-Fun&lt;/h2&gt; &#xA;&lt;p&gt;V1.0:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Storage Space&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Model Scope&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-1.3B-InP&lt;/td&gt; &#xA;   &lt;td&gt;19.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-1.3B text-to-video weights, trained at multiple resolutions, supporting start and end frame prediction.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-14B-InP&lt;/td&gt; &#xA;   &lt;td&gt;47.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-14B text-to-video weights, trained at multiple resolutions, supporting start and end frame prediction.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-1.3B-Control&lt;/td&gt; &#xA;   &lt;td&gt;19.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-1.3B video control weights, supporting various control conditions such as Canny, Depth, Pose, MLSD, etc., and trajectory control. Supports multi-resolution (512, 768, 1024) video prediction at 81 frames, trained at 16 frames per second, with multilingual prediction support.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-14B-Control&lt;/td&gt; &#xA;   &lt;td&gt;47.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wan2.1-Fun-14B video control weights, supporting various control conditions such as Canny, Depth, Pose, MLSD, etc., and trajectory control. Supports multi-resolution (512, 768, 1024) video prediction at 81 frames, trained at 16 frames per second, with multilingual prediction support.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;2. Wan2.1&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Model Scope&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-T2V-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wanxiang 2.1-1.3B text-to-video weights&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-T2V-14B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wanxiang 2.1-14B text-to-video weights&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-I2V-14B-480P&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wanxiang 2.1-14B-480P image-to-video weights&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-I2V-14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Wanxiang 2.1-14B-720P image-to-video weights&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;3. CogVideoX-Fun&lt;/h2&gt; &#xA;&lt;p&gt;V1.5:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Storage Space&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Model Scope&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.5-5b-InP&lt;/td&gt; &#xA;   &lt;td&gt;20.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.5-5b-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.5-5b-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official graph-generated video model is capable of predicting videos at multiple resolutions (512, 768, 1024) and has been trained on 85 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.5-Reward-LoRAs&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-Reward-LoRAs&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.5-Reward-LoRAs&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The official reward backpropagation technology model optimizes the videos generated by CogVideoX-Fun-V1.5 to better match human preferences. ÔΩú&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;V1.1:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Storage Space&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Model Scope&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-2b-InP&lt;/td&gt; &#xA;   &lt;td&gt;13.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-2b-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-2b-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official graph-generated video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-5b-InP&lt;/td&gt; &#xA;   &lt;td&gt;20.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-5b-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-5b-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official graph-generated video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second. Noise has been added to the reference image, and the amplitude of motion is greater compared to V1.0.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-2b-Pose&lt;/td&gt; &#xA;   &lt;td&gt;13.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-2b-Pose&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-2b-Pose&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official pose-control video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-2b-Control&lt;/td&gt; &#xA;   &lt;td&gt;13.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-2b-Control&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-2b-Control&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official control video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second. Supporting various control conditions such as Canny, Depth, Pose, MLSD, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-5b-Pose&lt;/td&gt; &#xA;   &lt;td&gt;20.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-5b-Pose&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-5b-Pose&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official pose-control video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-5b-Control&lt;/td&gt; &#xA;   &lt;td&gt;20.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-5b-Control&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-5b-Control&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our official control video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second. Supporting various control conditions such as Canny, Depth, Pose, MLSD, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideoX-Fun-V1.1-Reward-LoRAs&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-Reward-LoRAs&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-Reward-LoRAs&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The official reward backpropagation technology model optimizes the videos generated by CogVideoX-Fun-V1.1 to better match human preferences. ÔΩú&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;(Obsolete) V1.0:&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Name&lt;/th&gt; &#xA;    &lt;th&gt;Storage Space&lt;/th&gt; &#xA;    &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;    &lt;th&gt;Model Scope&lt;/th&gt; &#xA;    &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CogVideoX-Fun-2b-InP&lt;/td&gt; &#xA;    &lt;td&gt;13.0 GB&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-2b-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-2b-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Our official graph-generated video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CogVideoX-Fun-5b-InP&lt;/td&gt; &#xA;    &lt;td&gt;20.0 GB&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/alibaba-pai/CogVideoX-Fun-5b-InP&#34;&gt;ü§óLink&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://modelscope.cn/models/PAI/CogVideoX-Fun-5b-InP&#34;&gt;üòÑLink&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Our official graph-generated video model is capable of predicting videos at multiple resolutions (512, 768, 1024, 1280) and has been trained on 49 frames at a rate of 8 frames per second.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Reference&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CogVideo: &lt;a href=&#34;https://github.com/THUDM/CogVideo/&#34;&gt;https://github.com/THUDM/CogVideo/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EasyAnimate: &lt;a href=&#34;https://github.com/aigc-apps/EasyAnimate&#34;&gt;https://github.com/aigc-apps/EasyAnimate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wan2.1: &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1/&#34;&gt;https://github.com/Wan-Video/Wan2.1/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The CogVideoX-2B model (including its corresponding Transformers module and VAE module) is released under the &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/VideoX-Fun/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The CogVideoX-5B model (Transformers module) is released under the &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE&#34;&gt;CogVideoX LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>leminlimez/Nugget</title>
    <updated>2025-03-31T01:37:07Z</updated>
    <id>tag:github.com,2025-03-31:/leminlimez/Nugget</id>
    <link href="https://github.com/leminlimez/Nugget" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unlock the fullest potential of your device&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nugget&lt;/h1&gt; &#xA;&lt;p&gt;Unlock your device&#39;s full potential!&lt;/p&gt; &#xA;&lt;p&gt;Sparserestore works on all versions iOS 17.0-18.2 developer beta 2. There is partial support for iOS 18.2 developer beta 3 and newer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mobilegestalt and AI Enabler tweaks are not supported on iOS 18.2+.&lt;/strong&gt; It will never be supported, do not make issues asking for when it is supported.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have installed the &lt;a href=&#34;https://raw.githubusercontent.com/leminlimez/Nugget/main/#requirements&#34;&gt;requirements&lt;/a&gt; if you are on Windows or Linux.&lt;/p&gt; &#xA;&lt;p&gt;This uses the sparserestore exploit to write to files outside of the intended restore location, like mobilegestalt. Read the &lt;a href=&#34;https://raw.githubusercontent.com/leminlimez/Nugget/main/#getting-the-file&#34;&gt;Getting the File&lt;/a&gt; section to learn how to get your mobilegestalt file.&lt;/p&gt; &#xA;&lt;p&gt;Note: I am not responsible if your device bootloops. Please back up your data before using!&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;iOS 17.0+&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enable Dynamic Island on any device&lt;/li&gt; &#xA; &lt;li&gt;Enable iPhone X gestures on iPhone SEs&lt;/li&gt; &#xA; &lt;li&gt;Change Device Model Name (ie what shows in the Settings app)&lt;/li&gt; &#xA; &lt;li&gt;Enable Boot Chime&lt;/li&gt; &#xA; &lt;li&gt;Enable Charge Limit&lt;/li&gt; &#xA; &lt;li&gt;Enable Tap to Wake on unsupported devices (ie iPhone SEs)&lt;/li&gt; &#xA; &lt;li&gt;Enable Collision SOS&lt;/li&gt; &#xA; &lt;li&gt;Enable Stage Manager&lt;/li&gt; &#xA; &lt;li&gt;Disable the Wallpaper Parallax&lt;/li&gt; &#xA; &lt;li&gt;Disable Region Restrictions (ie. Shutter Sound) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note: This does not include enabling EU sideloading outside the EU. That will come later.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Show the Apple Pencil options in Settings app&lt;/li&gt; &#xA; &lt;li&gt;Show the Action Button options in Settings app&lt;/li&gt; &#xA; &lt;li&gt;Show Internal Storage info (Might cause problems on some devices, use at your own risk)&lt;/li&gt; &#xA; &lt;li&gt;EU Enabler (iOS 17.6-)&lt;/li&gt; &#xA; &lt;li&gt;Springboard Options (from &lt;a href=&#34;https://github.com/leminlimez/CowabungaLite&#34;&gt;Cowabunga Lite&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set Lock Screen Footnote&lt;/li&gt; &#xA;   &lt;li&gt;Disable Lock After Respring&lt;/li&gt; &#xA;   &lt;li&gt;Disable Screen Dimming While Charging&lt;/li&gt; &#xA;   &lt;li&gt;Disable Low Battery Alerts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Internal Options (from &lt;a href=&#34;https://github.com/leminlimez/CowabungaLite&#34;&gt;Cowabunga Lite&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Build Version in Status Bar&lt;/li&gt; &#xA;   &lt;li&gt;Force Right to Left&lt;/li&gt; &#xA;   &lt;li&gt;Force Metal HUD Debug&lt;/li&gt; &#xA;   &lt;li&gt;iMessage Diagnostics&lt;/li&gt; &#xA;   &lt;li&gt;IDS Diagnostics&lt;/li&gt; &#xA;   &lt;li&gt;VC Diagnostics&lt;/li&gt; &#xA;   &lt;li&gt;App Store Debug Gesture&lt;/li&gt; &#xA;   &lt;li&gt;Notes App Debug Mode&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Disable Daemons: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OTAd&lt;/li&gt; &#xA;   &lt;li&gt;UsageTrackingAgent&lt;/li&gt; &#xA;   &lt;li&gt;Game Center&lt;/li&gt; &#xA;   &lt;li&gt;Screen Time Agent&lt;/li&gt; &#xA;   &lt;li&gt;Logs, Dumps, and Crash Reports&lt;/li&gt; &#xA;   &lt;li&gt;ATWAKEUP&lt;/li&gt; &#xA;   &lt;li&gt;Tipsd&lt;/li&gt; &#xA;   &lt;li&gt;VPN&lt;/li&gt; &#xA;   &lt;li&gt;Chinese WLAN service&lt;/li&gt; &#xA;   &lt;li&gt;HealthKit&lt;/li&gt; &#xA;   &lt;li&gt;AirPrint&lt;/li&gt; &#xA;   &lt;li&gt;Assistive Touch&lt;/li&gt; &#xA;   &lt;li&gt;iCloud&lt;/li&gt; &#xA;   &lt;li&gt;Internet Tethering (aka Personal Hotspot)&lt;/li&gt; &#xA;   &lt;li&gt;PassBook&lt;/li&gt; &#xA;   &lt;li&gt;Spotlight&lt;/li&gt; &#xA;   &lt;li&gt;Voice Control&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PosterBoard: Animated wallpapers and descriptors. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Community wallpapers can be found &lt;a href=&#34;https://cowabun.ga/wallpapers&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;See documentation on the structure of tendies files in &lt;code&gt;documentation.md&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Risky (Hidden) Options: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Disable thermalmonitord&lt;/li&gt; &#xA;   &lt;li&gt;OTA Killer&lt;/li&gt; &#xA;   &lt;li&gt;Custom Resolution&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;iOS 18.0+&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enable iPhone 16 camera button page in the Settings app&lt;/li&gt; &#xA; &lt;li&gt;Enable AOD &amp;amp; AOD Vibrancy on any device&lt;/li&gt; &#xA; &lt;li&gt;Feature Flags (iOS 18.1b4-): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enabling lock screen clock animation, lock screen page duplication button, and more!&lt;/li&gt; &#xA;   &lt;li&gt;Disabling the new iOS 18 Photos UI (iOS 18.0 betas only, unknown which patched it)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;iOS 18.1+&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AI Enabler + Device Spoofing (fixed in iOS 18.2db3)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Either &lt;a href=&#34;https://apps.microsoft.com/detail/9np83lwlpz9k%3Fhl%3Den-US%26gl%3DUS&amp;amp;ved=2ahUKEwjE-svo7qyJAxWTlYkEHQpbH3oQFnoECBoQAQ&amp;amp;usg=AOvVaw0rZTXCFmRaHAifkEEu9tMI&#34;&gt;Apple Devices (from Microsoft Store)&lt;/a&gt; app or &lt;a href=&#34;https://support.apple.com/en-us/106372&#34;&gt;iTunes (from Apple website)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/libimobiledevice/usbmuxd&#34;&gt;usbmuxd&lt;/a&gt; and &lt;a href=&#34;https://github.com/libimobiledevice/libimobiledevice&#34;&gt;libimobiledevice&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Running Python:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;pymobiledevice3&lt;/li&gt; &#xA;   &lt;li&gt;PySide6&lt;/li&gt; &#xA;   &lt;li&gt;Python 3.8 or newer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running the Python Program&lt;/h2&gt; &#xA;&lt;p&gt;Note: It is highly recommended to use a virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .env # only needed once&#xA;# macOS/Linux:  source .env/bin/activate&#xA;# Windows:      .env/Scripts/activate.bat&#xA;pip3 install -r requirements.txt # only needed once&#xA;python3 main_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: It may be either &lt;code&gt;python&lt;/code&gt;/&lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;python3&lt;/code&gt;/&lt;code&gt;pip3&lt;/code&gt; depending on your path.&lt;/p&gt; &#xA;&lt;p&gt;The CLI version can be ran with &lt;code&gt;python3 cli_app.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting the File&lt;/h2&gt; &#xA;&lt;p&gt;You need to get the mobilegestalt file that is specific to your device. To do that, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the &lt;code&gt;Shortcuts&lt;/code&gt; app from the iOS app store.&lt;/li&gt; &#xA; &lt;li&gt;Download this shortcut: &lt;a href=&#34;https://www.icloud.com/shortcuts/d6f0a136ddda4714a80750512911c53b&#34;&gt;https://www.icloud.com/shortcuts/d6f0a136ddda4714a80750512911c53b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Save the file and share it to your computer.&lt;/li&gt; &#xA; &lt;li&gt;Place it in the same folder as the python file (or specify the path in the program)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;To compile &lt;code&gt;mainwindow.ui&lt;/code&gt; for Python, run the following command: &lt;code&gt;pyside6-uic qt/mainwindow.ui -o qt/ui_mainwindow.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To compile the resources file for Python, run the following command: &lt;code&gt;pyside6-rcc qt/resources.qrc -o resources_rc.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The application itself can be compiled by running &lt;code&gt;compile.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Read More&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to read more about the inner workings of the exploit and iOS restore system, I made a write up which you can read &lt;a href=&#34;https://gist.github.com/leminlimez/c602c067349140fe979410ef69d39c28&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JJTech0130&#34;&gt;JJTech&lt;/a&gt; for Sparserestore/&lt;a href=&#34;https://github.com/JJTech0130/TrollRestore&#34;&gt;TrollRestore&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/gWtzTVhMvh&#34;&gt;PosterRestore&lt;/a&gt; for their help with PosterBoard &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Special thanks to dootskyre, &lt;a href=&#34;https://twitter.com/MWRevamped&#34;&gt;Middo&lt;/a&gt;, &lt;a href=&#34;https://github.com/dularkian&#34;&gt;dulark&lt;/a&gt;, forcequitOS, and pingubow for their work on this. It would not have been possible without them!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x.com/disfordottie&#34;&gt;disfordottie&lt;/a&gt; for some global flag features&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mikasa-san&#34;&gt;Mikasa-san&lt;/a&gt; for &lt;a href=&#34;https://github.com/Mikasa-san/QuietDaemon&#34;&gt;Quiet Daemon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/f1shy-dev&#34;&gt;sneakyf1shy&lt;/a&gt; for &lt;a href=&#34;https://gist.github.com/f1shy-dev/23b4a78dc283edd30ae2b2e6429129b5&#34;&gt;AI Eligibility&lt;/a&gt; (iOS 18.1 beta 4 and below)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lrdsnow&#34;&gt;lrdsnow&lt;/a&gt; for &lt;a href=&#34;https://github.com/Lrdsnow/EUEnabler&#34;&gt;EU Enabler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/doronz88/pymobiledevice3&#34;&gt;pymobiledevice3&lt;/a&gt; for restoring and device algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doc.qt.io/qtforpython-6/&#34;&gt;PySide6&lt;/a&gt; for the GUI library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>