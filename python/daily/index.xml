<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-15T01:35:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVlabs/neuralangelo</title>
    <updated>2023-08-15T01:35:42Z</updated>
    <id>tag:github.com,2023-08-15:/NVlabs/neuralangelo</id>
    <link href="https://github.com/NVlabs/neuralangelo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;Neuralangelo: High-Fidelity Neural Surface Reconstruction&#34; (CVPR 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neuralangelo&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/neuralangelo/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2306.03092/&#34;&gt;Paper&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This is the official repo for the implementation of &lt;strong&gt;Neuralangelo: High-Fidelity Neural Surface Reconstruction&lt;/strong&gt;.&lt;br&gt; The code is built upon the Imaginaire library from the Deep Imagination Research Group at NVIDIA.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/neuralangelo/main/assets/teaser.gif&#34;&gt; &#xA;&lt;p&gt;For business inquiries, please submit the &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA research licensing form&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We offer two ways to setup the environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;We provide prebuilt Docker images, where&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker.io/chenhsuanlin/colmap:3.8&lt;/code&gt; is for running COLMAP and the data preprocessing scripts. This includes the prebuilt COLMAP library (CUDA-supported).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker.io/chenhsuanlin/neuralangelo:23.04-py3&lt;/code&gt; is for running the main Neuralangelo pipeline.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;The corresponding Dockerfiles can be found in the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The conda environment for Neuralangelo. Install the dependencies and activate the environment &lt;code&gt;neuralangelo&lt;/code&gt; with&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create --file neuralangelo.yaml&#xA;conda activate neuralangelo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For COLMAP, alternative installation options are also available on the &lt;a href=&#34;https://colmap.github.io/&#34;&gt;COLMAP website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/neuralangelo/main/DATA_PROCESSING.md&#34;&gt;Data Preparation&lt;/a&gt; for step-by-step instructions.&lt;br&gt; We assume known camera poses for each extracted frame from the video. The code uses the same json format as &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Instant NGP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run Neuralangelo!&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;EXPERIMENT=toy_example&#xA;GROUP=example_group&#xA;NAME=example_name&#xA;CONFIG=projects/neuralangelo/configs/custom/${EXPERIMENT}.yaml&#xA;GPUS=1  # use &amp;gt;1 for multi-GPU training!&#xA;torchrun --nproc_per_node=${GPUS} train.py \&#xA;    --logdir=logs/${GROUP}/${NAME} \&#xA;    --config=${CONFIG} \&#xA;    --show_pbar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some useful notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This codebase supports logging with &lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;. You should have a W&amp;amp;B account for this. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--wandb&lt;/code&gt; to the command line argument to enable W&amp;amp;B logging.&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--wandb_name&lt;/code&gt; to specify the W&amp;amp;B project name.&lt;/li&gt; &#xA;   &lt;li&gt;More detailed control can be found in the &lt;code&gt;init_wandb()&lt;/code&gt; function in &lt;code&gt;imaginaire/trainers/base.py&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Configs can be overridden through the command line (e.g. &lt;code&gt;--optim.params.lr=1e-2&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;--checkpoint={CHECKPOINT_PATH}&lt;/code&gt; to initialize with a certain checkpoint; set &lt;code&gt;--resume=True&lt;/code&gt; to resume training.&lt;/li&gt; &#xA; &lt;li&gt;If appearance embeddings are enabled, make sure &lt;code&gt;data.num_images&lt;/code&gt; is set to the number of training images.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Isosurface extraction&lt;/h2&gt; &#xA;&lt;p&gt;Use the following command to run isosurface mesh extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CHECKPOINT=logs/${GROUP}/${NAME}/xxx.pt&#xA;OUTPUT_MESH=xxx.ply&#xA;CONFIG=projects/neuralangelo/configs/custom/${EXPERIMENT}.yaml&#xA;RESOLUTION=2048&#xA;BLOCK_RES=128&#xA;GPUS=1  # use &amp;gt;1 for multi-GPU mesh extraction&#xA;torchrun --nproc_per_node=${GPUS} projects/neuralangelo/scripts/extract_mesh.py \&#xA;    --logdir=logs/${GROUP}/${NAME} \&#xA;    --config=${CONFIG} \&#xA;    --checkpoint=${CHECKPOINT} \&#xA;    --output_file=${OUTPUT_MESH} \&#xA;    --resolution=${RESOLUTION} \&#xA;    --block_res=${BLOCK_RES}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you find our code useful for your research, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{li2023neuralangelo,&#xA;  title={Neuralangelo: High-Fidelity Neural Surface Reconstruction},&#xA;  author={Li, Zhaoshuo and M\&#34;uller, Thomas and Evans, Alex and Taylor, Russell H and Unberath, Mathias and Liu, Ming-Yu and Lin, Chen-Hsuan},&#xA;  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lllyasviel/Fooocus</title>
    <updated>2023-08-15T01:35:42Z</updated>
    <id>tag:github.com,2023-08-15:/lllyasviel/Fooocus</id>
    <link href="https://github.com/lllyasviel/Fooocus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Focus on prompting and generating&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fooocus&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lllyasviel/misc_files/main/202308/fsm2.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p&gt;Fooocus is an image generating software.&lt;/p&gt; &#xA;&lt;p&gt;Fooocus is a rethinking of Stable Diffusion and Midjourney’s designs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Learned from Stable Diffusion, the software is offline, open source, and free.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Learned from Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Fooocus has included and automated &lt;a href=&#34;https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#tech_list&#34;&gt;lots of inner optimizations and quality improvements&lt;/a&gt;. Users can forget all those difficult technical parameters, and just enjoy the interaction between human and computer to &#34;explore new mediums of thought and expanding the imaginative powers of the human species&#34; &lt;code&gt;[1]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Fooocus has simplified the installation. Between pressing &#34;download&#34; and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;[1]&lt;/code&gt; David Holz, 2019.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;You can directly download Fooocus with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus/releases/download/release/Fooocus_win64_1-1-10.7z&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Click here to download &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you download the file, please uncompress it, and then run the &#34;run.bat&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/c49269c4-c274-4893-b368-047c401cc58c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the first time you launch the software, it will automatically download models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It will download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors&#34;&gt;sd_xl_base_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_base_1.0_0.9vae.safetensors&#34;.&lt;/li&gt; &#xA; &lt;li&gt;It will download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors&#34;&gt;sd_xl_refiner_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_refiner_1.0_0.9vae.safetensors&#34;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/d386f817-4bd7-490c-ad89-c1e228c23447&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you already have these files, you can copy them to the above locations to speed up installation.&lt;/p&gt; &#xA;&lt;p&gt;Below is a test on a relatively low-end laptop with &lt;strong&gt;16GB System RAM&lt;/strong&gt; and &lt;strong&gt;6GB VRAM&lt;/strong&gt; (Nvidia 3060 laptop). The speed on this machine is about 1.35 seconds per iteration. Pretty impressive – nowadays laptops with 3060 are usually at very acceptable price.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/938737a5-b105-4f19-b051-81356cb7c495&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that the minimal requirement is &lt;strong&gt;4GB Nvidia GPU memory (4GB VRAM)&lt;/strong&gt; and &lt;strong&gt;8GB system memory (8GB RAM)&lt;/strong&gt;. This requires using Microsoft’s Virtual Swap technique, which is automatically enabled by your Windows installation in most cases, so you often do not need to do anything about it. However, if you are not sure, or if you manually turned it off (would anyone really do that?), you can enable it here:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click here to the see the image instruction. &lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/2a06b130-fe9b-4504-94f1-2763be4476e9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Please open an issue if you use similar devices but still cannot achieve acceptable performances.&lt;/p&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;(Last tested - 2023 Aug 14)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fooocus Colab (Official Version)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that sometimes this Colab will say like &#34;you must restart the runtime in order to use newly installed XX&#34;. This can be safely ignored.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/camenduru&#34;&gt;camenduru&lt;/a&gt;&#39;s codes!&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;The command lines are&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git&#xA;cd Fooocus&#xA;conda env create -f environment.yaml&#xA;conda activate fooocus&#xA;pip install -r requirements_versions.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download the models: download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors&#34;&gt;sd_xl_base_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_base_1.0_0.9vae.safetensors&#34;, and download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors&#34;&gt;sd_xl_refiner_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_refiner_1.0_0.9vae.safetensors&#34;. &lt;strong&gt;Or let Fooocus automatically download the models&lt;/strong&gt; using the launcher:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python launch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you want to open a remote port, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python launch.py --listen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mac/Windows(AMD GPUs)&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon ...&lt;/p&gt; &#xA;&lt;h2&gt;List of &#34;Hidden&#34; Tricks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;tech_list&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below things are already inside the software, and &lt;strong&gt;users do not need to do anything about these&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that some of these tricks are currently (2023 Aug 11) impossible to reproduce in Automatic1111&#39;s interface or ComfyUI&#39;s node system. You may expect better results from Fooocus than other software even when they use similar models/pipelines.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Native refiner swap inside one single k-sampler. The advantage is that now the refiner model can reuse the base model&#39;s momentum (or ODE&#39;s history parameters) collected from k-sampling to achieve more coherent sampling. In Automatic1111&#39;s high-res fix and ComfyUI&#39;s node system, the base model and refiner use two independent k-samplers, which means the momentum is largely wasted, and the sampling continuity is broken. Fooocus uses its own advanced k-diffusion sampling that ensures seamless, native, and continuous swap in a refiner setup. (Update Aug 13: Actually I discussed this with Automatic1111 several days ago and it seems that the “native refiner swap inside one single k-sampler” is &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12371&#34;&gt;merged&lt;/a&gt; into the dev branch of webui. Great!)&lt;/li&gt; &#xA; &lt;li&gt;Negative ADM guidance. Because the highest resolution level of XL Base does not have cross attentions, the positive and negative signals for XL&#39;s highest resolution level cannot receive enough contrasts during the CFG sampling, causing the results look a bit plastic or overly smooth in certain cases. Fortunately, since the XL&#39;s highest resolution level is still conditioned on image aspect ratios (ADM), we can modify the adm on the positive/negative side to compensate for the lack of CFG contrast in the highest resolution level.&lt;/li&gt; &#xA; &lt;li&gt;We implemented a carefully tuned variation of the Section 5.1 of &lt;a href=&#34;https://arxiv.org/pdf/2210.00939.pdf&#34;&gt;&#34;Improving Sample Quality of Diffusion Models Using Self-Attention Guidance&#34;&lt;/a&gt;. The weight is set to very low, but this is Fooocus&#39;s final guaranteen to make sure than the XL will never yield overly smooth or plastic appearance. This can almostly eliminate all cases that XL still occasionally produce overly smooth results even with negative ADM guidance.&lt;/li&gt; &#xA; &lt;li&gt;We modified the style templates a bit and added the &#34;cinematic-default&#34;.&lt;/li&gt; &#xA; &lt;li&gt;We tested the &#34;sd_xl_offset_example-lora_1.0.safetensors&#34; and it seems that when the lora weight is below 0.5, the results are always better than XL without lora.&lt;/li&gt; &#xA; &lt;li&gt;The parameters of samplers are carefully tuned.&lt;/li&gt; &#xA; &lt;li&gt;Because XL uses positional encoding for generation resolution, images generated by several fixed resolutions look a bit better than that from arbitrary resolutions (because the positional encoding is not very good at handling int numbers that are unseen during training). This suggests that the resolutions in UI may be hard coded for best results.&lt;/li&gt; &#xA; &lt;li&gt;Separated prompts for two different text encoders seem unnecessary. Separated prompts for base model and refiner may work but the effects are random, and we refrain from implement this.&lt;/li&gt; &#xA; &lt;li&gt;DPM family seems well-suited for XL, since XL sometimes generates overly smooth texture but DPM family sometimes generate overly dense detail in texture. Their joint effect looks neutral and appealing to human perception.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;The codebase starts from an odd mixture of &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Automatic1111&lt;/a&gt; and &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;. (And they both use GPL license.)&lt;/p&gt; &#xA;&lt;h2&gt;Update Log&lt;/h2&gt; &#xA;&lt;p&gt;The log is &lt;a href=&#34;https://raw.githubusercontent.com/lllyasviel/Fooocus/main/update_log.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SeargeDP/SeargeSDXL</title>
    <updated>2023-08-15T01:35:42Z</updated>
    <id>tag:github.com,2023-08-15:/SeargeDP/SeargeSDXL</id>
    <link href="https://github.com/SeargeDP/SeargeSDXL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Custom nodes and workflows for SDXL in ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Searge-SDXL v3.x - &#34;Truly Reborn&#34;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Custom nodes extension&lt;/em&gt; for &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; including &lt;em&gt;a workflow&lt;/em&gt; to use &lt;em&gt;SDXL 1.0&lt;/em&gt; with both the &lt;em&gt;base and refiner&lt;/em&gt; checkpoints.&lt;/p&gt; &#xA;&lt;h1&gt;Version 3.4&lt;/h1&gt; &#xA;&lt;p&gt;Instead of having separate workflows for different tasks, everything is now integrated in &lt;strong&gt;one workflow file&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Always use the latest version of the workflow json file with the latest version of the custom nodes!&lt;/h3&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-Example.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;h2&gt;What&#39;s new in v3.4?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minor tweaks and fixes and the beginnings of some code restructuring, nothing user should notice in the workflows&lt;/li&gt; &#xA; &lt;li&gt;Preparations for more upcoming improvements in a compatible way&lt;/li&gt; &#xA; &lt;li&gt;Added compatibility with v1.x workflows, these have been used in some tutorials and did not work anymore with newer versions of the extension&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;(backwards compatibility with v2.x and older v3.x version - before v3.3 - is unfortunately not possible)&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What about v3.3?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting from v3.3 the custom node extension will always be compatible with workflows created with v3.3 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;(backwards compatibility with v2.x, v3.0, v3.1. and v3.2 workflows is unfortunately not possible)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Going forward, older versions of workflow will remain in the &lt;code&gt;workflow&lt;/code&gt; folder, I still highly recommend to &lt;strong&gt;always use the latest version&lt;/strong&gt; and loading it &lt;strong&gt;from the JSON file&lt;/strong&gt; instead of the example images&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Version 3.3 has never been publicly released&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in v3.2?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More prompting modes, including the &#34;3-prompt&#34; style that&#39;s common in other workflows using separate prompts for the 2 CLIP models in SDXL (CLIP G &amp;amp; CLIP L) and a negative prompt &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;3-Prompt G+L-N&lt;/strong&gt; - Similar to simple mode, but cares about &lt;em&gt;a main, a secondary, and a negative prompt&lt;/em&gt; and &lt;strong&gt;ignores&lt;/strong&gt; the &lt;em&gt;additional style prompting fields&lt;/em&gt;, this is great to get similar results as on other workflows and makes it easier to compare the images&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subject - Style&lt;/strong&gt; - The &lt;em&gt;subject focused&lt;/em&gt; positives with the &lt;em&gt;style focused&lt;/em&gt; negatives&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Style - Subject&lt;/strong&gt; - The &lt;em&gt;style focused&lt;/em&gt; positives with the &lt;em&gt;subject focused&lt;/em&gt; negatives&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Style Only&lt;/strong&gt; - &lt;strong&gt;Only&lt;/strong&gt; the positive and negative &lt;strong&gt;style prompts&lt;/strong&gt; are used and &lt;em&gt;main/secondary/negative are ignored&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Weighted - Overlay&lt;/strong&gt; - The positive prompts are &lt;em&gt;weighted&lt;/em&gt; and the negative prompts are &lt;em&gt;overlaid&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Overlay - Weighted&lt;/strong&gt; - The positive prompts are &lt;em&gt;overlaid&lt;/em&gt; and the negative prompts are &lt;em&gt;weighted&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Better bug fix for the &#34;exploding&#34; the search box issue, should finally be fixed &lt;em&gt;(for real)&lt;/em&gt; now&lt;/li&gt; &#xA; &lt;li&gt;Some additional node types to make it easier to still use my nodes in other custom workflows&lt;/li&gt; &#xA; &lt;li&gt;The custom node extension should now also work on &lt;strong&gt;Python 3.9&lt;/strong&gt; again, it required 3.10 before&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in v3.1?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fixed the issue with &#34;exploding&#34; the search box when this extension is installed&lt;/li&gt; &#xA; &lt;li&gt;Loading of Checkpoints, VAE, Upscalers, and Loras through custom nodes&lt;/li&gt; &#xA; &lt;li&gt;Updated workflow to make use of the added node types&lt;/li&gt; &#xA; &lt;li&gt;Adjusted the default settings for some parameters in the workflow&lt;/li&gt; &#xA; &lt;li&gt;Fixed some reported issues with the workflow and custom nodes&lt;/li&gt; &#xA; &lt;li&gt;Prepared the workflow for an upcoming feature&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in v3.0?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Completely overhauled &lt;strong&gt;user interface&lt;/strong&gt;, now even easier to use than before&lt;/li&gt; &#xA; &lt;li&gt;More organized workflow graph - if you want to understand how it is designed &#34;under the hood&#34;, it should now be easier to figure out what is where and how things are connected&lt;/li&gt; &#xA; &lt;li&gt;New settings that help to tweak the generated images &lt;em&gt;without changing the composition&lt;/em&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Quickly iterate between &lt;em&gt;sharper&lt;/em&gt; results and &lt;em&gt;softer&lt;/em&gt; results of the same image without changing the composition or subject&lt;/li&gt; &#xA;   &lt;li&gt;Easily make colors pop where needed, or render a softer image where it fits the mood better&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Three operating modes in &lt;strong&gt;ONE&lt;/strong&gt; workflow &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;text-to-image&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;image-to-image&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;inpainting&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Different prompting modes (&lt;strong&gt;5 modes&lt;/strong&gt; available) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; - Just cares about &lt;strong&gt;a positive and a negative prompt&lt;/strong&gt; and &lt;em&gt;ignores the additional prompting fields&lt;/em&gt;, this is great to get started with SDXL, ComfyUI, and this workflow&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subject Focus&lt;/strong&gt; - In this mode the &lt;em&gt;main/secondary prompts&lt;/em&gt; are more important than the &lt;em&gt;style prompts&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Style Focus&lt;/strong&gt; - In this mode the &lt;em&gt;style prompts&lt;/em&gt; are more important than the &lt;em&gt;main/secondary prompts&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Weighted&lt;/strong&gt; - In this mode the balance between &lt;em&gt;main/secondary prompts&lt;/em&gt; and &lt;em&gt;style prompts&lt;/em&gt; can be influenced with the &lt;em&gt;style prompt power&lt;/em&gt; and &lt;em&gt;negative prompt power&lt;/em&gt; option&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Overlay&lt;/strong&gt; - In this mode the main*/secondary prompts* and the &lt;em&gt;style prompts&lt;/em&gt; are competing with each other&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Greatly &lt;em&gt;improved Hires-Fix&lt;/em&gt; - now with more options to influence the results&lt;/li&gt; &#xA; &lt;li&gt;A (rather limited for now) alpha test for &lt;em&gt;style templates&lt;/em&gt;, this is work in progress and only includes one style for now (called &lt;em&gt;test&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Options to change the &lt;strong&gt;intensity of the refiner&lt;/strong&gt; when used together with the base model, separate for &lt;em&gt;main pass&lt;/em&gt; and &lt;em&gt;hires-fix pass&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;(... many more things probably, since the workflow was almost completely re-made)&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-UI.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;h1&gt;Installing and Updating:&lt;/h1&gt; &#xA;&lt;h3&gt;Recommended Installation:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;ComfyUI/custom_nodes/&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Open a command line window in the &lt;em&gt;custom_nodes&lt;/em&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;git clone https://github.com/SeargeDP/SeargeSDXL.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Restart ComfyUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Alternative Installation (not recommended):&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and unpack the latest release from the &lt;a href=&#34;https://civitai.com/models/111463&#34;&gt;Searge SDXL CivitAI page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Drop the &lt;code&gt;SeargeSDXL&lt;/code&gt; folder into the &lt;code&gt;ComfyUI/custom_nodes&lt;/code&gt; directory and restart ComfyUI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updating an Existing Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;ComfyUI/custom_nodes/&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;If you installed via &lt;code&gt;git clone&lt;/code&gt; before &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open a command line window in the &lt;em&gt;custom_nodes&lt;/em&gt; directory&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;git pull&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you installed from a zip file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unpack the &lt;code&gt;SeargeSDXL&lt;/code&gt; folder from the latest release into &lt;code&gt;ComfyUI/custom_nodes&lt;/code&gt;, overwrite existing files&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Restart ComfyUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Checkpoints and Models for these Workflows&lt;/h2&gt; &#xA;&lt;h3&gt;Direct Downloads&lt;/h3&gt; &#xA;&lt;p&gt;(from Huggingface)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors&#34;&gt;SDXL 1.0 base&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/checkpoints&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors&#34;&gt;SDXL 1.0 refiner&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/checkpoints&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl_vae.safetensors&#34;&gt;Fixed SDXL 0.9 vae&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/vae&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors&#34;&gt;SDXL Offset Noise LoRA&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/loras&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x_NMKD-Siax_200k.pth&#34;&gt;4x_NMKD-Siax_200k upscaler&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/upscale_models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;download &lt;a href=&#34;https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x-UltraSharp.pth&#34;&gt;4x-UltraSharp upscaler&lt;/a&gt; and copy it into &lt;code&gt;ComfyUI/models/upscale_models&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;p&gt;Now &lt;strong&gt;3&lt;/strong&gt; operating modes are included in the workflow, the &lt;em&gt;.json-file&lt;/em&gt; for it is in the &lt;code&gt;workflow&lt;/code&gt; folder. They are called &lt;em&gt;text2image&lt;/em&gt;, &lt;em&gt;image2image&lt;/em&gt;, and &lt;em&gt;inpainting&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The simple workflow has not returned as a separate workflow, but is now also &lt;em&gt;fully integrated&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To enable it, switch the &lt;strong&gt;prompt mode&lt;/strong&gt; option to &lt;strong&gt;simple&lt;/strong&gt; and it will only pay attention to the &lt;em&gt;main prompt&lt;/em&gt; and the &lt;em&gt;negative prompt&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or switch the &lt;strong&gt;prompt mode&lt;/strong&gt; to &lt;strong&gt;3 prompts&lt;/strong&gt; and only the &lt;em&gt;main prompt&lt;/em&gt;, the &lt;em&gt;secondary prompt&lt;/em&gt;, and the &lt;em&gt;negative prompt&lt;/em&gt; are used.&lt;/p&gt; &#xA;&lt;h1&gt;The Workflow&lt;/h1&gt; &#xA;&lt;p&gt;The workflow is included in the &lt;code&gt;workflow&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;After updating Searge SDXL, always make sure to load the latest version of the json file. Older versions of the workflow are often not compatible anymore with the updated node extension.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-Overview.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;h1&gt;Searge SDXL Reborn Workflow Description&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Reborn v3.x&lt;/strong&gt; workflow is a new workflow, created from scratch. It requires the latest additions to the SeargeSDXL custom node extension, because it makes use of some new node types.&lt;/p&gt; &#xA;&lt;p&gt;The interface for using this new workflow is also designed in a different way, with all parameters that are usually tweaked to generate images tightly packed together. This should make it easier to have every important element on the screen at the same time without scrolling.&lt;/p&gt; &#xA;&lt;p&gt;Starting from version 3.0 all 3 operating modes (text-to-image, image-to-image, and inpainting) are available from the same workflow and can be switched with an option.&lt;/p&gt; &#xA;&lt;h2&gt;Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_Qi0Dgrz1TM&#34;&gt;The amazing Youtube channel Nerdy Rodent has a video about this workflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(and while you are watching the video, don&#39;t forget to subscribe to their channel)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reborn Workflow v3.x Operating Modes&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/UI-operation-mode.png&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Text to Image Mode&lt;/h3&gt; &#xA;&lt;p&gt;In this mode you can generate images from text descriptions. The source image and the mask (next to the prompt inputs) are not used in this mode.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-workflow-1.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-reborn.png&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Image to Image Mode&lt;/h3&gt; &#xA;&lt;p&gt;In this mode you should first copy an image into the &lt;code&gt;ConfyUI/input&lt;/code&gt; directory. Alternatively you can change the option for the &lt;strong&gt;save directory&lt;/strong&gt; to &lt;strong&gt;input folder&lt;/strong&gt; when generating images, in that case you have to press the ComfyUI &lt;em&gt;Refresh&lt;/em&gt; button and it should show up in the image loader node.&lt;/p&gt; &#xA;&lt;p&gt;Then select that image as the &lt;em&gt;Source Image&lt;/em&gt; (next to the prompt inputs). If it does not show up, press the &lt;em&gt;Refresh&lt;/em&gt; button on the Comfy UI control box.&lt;/p&gt; &#xA;&lt;p&gt;For image to image the parameter &lt;em&gt;Denoise&lt;/em&gt; will determine how much the source image should be changed according to the prompt. Ranges are from &lt;em&gt;0.0&lt;/em&gt; for &#34;no change&#34; to &lt;em&gt;1.0&lt;/em&gt; for &#34;completely change&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Good values to try are probably in the &lt;em&gt;0.2&lt;/em&gt; to &lt;em&gt;0.8&lt;/em&gt; range. With examples of &lt;em&gt;0.25&lt;/em&gt; for &#34;very little change&#34;, &lt;em&gt;0.5&lt;/em&gt; for &#34;some changes&#34;, or &lt;em&gt;0.75&lt;/em&gt; for &#34;a lot of changes&#34;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-workflow-2.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-img2img.png&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Inpainting Mode&lt;/h3&gt; &#xA;&lt;p&gt;This is similar to the image to image mode. But it also lets you define a mask for selective changes of only parts of the image.&lt;/p&gt; &#xA;&lt;p&gt;To use this mode, prepare a source image the same way as described in the image to image workflow. Then &lt;strong&gt;right click&lt;/strong&gt; on the &lt;em&gt;Inpainting Mask&lt;/em&gt; image (the bottom one next to the input prompts) and select &lt;strong&gt;Open in Mask Editor&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Paint your mask and then press the &lt;em&gt;Save to node&lt;/em&gt; button when you are done. The &lt;em&gt;Denoise&lt;/em&gt; parameter works the same way as in image to image, but only masked areas will be changed.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-workflow-3.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-inpaint.png&#34; width=&#34;512&#34;&gt; &#xA;&lt;h1&gt;Prompting Modes&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/UI-prompt-style.png&#34; width=&#34;512&#34;&gt; &#xA;&lt;h2&gt;Reborn Workflow v3.x Prompting Modes&lt;/h2&gt; &#xA;&lt;h3&gt;Simple&lt;/h3&gt; &#xA;&lt;p&gt;Just cares about the &lt;strong&gt;main&lt;/strong&gt; and the &lt;strong&gt;negative&lt;/strong&gt; prompt and &lt;strong&gt;ignores&lt;/strong&gt; the &lt;em&gt;additional prompting fields&lt;/em&gt;, this is great to get started with SDXL, ComfyUI, and this workflow&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/01-simple.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;3-Prompt G+L-N&lt;/h3&gt; &#xA;&lt;p&gt;Similar to simple mode, but cares about the &lt;strong&gt;main &amp;amp; secondary&lt;/strong&gt; and the &lt;strong&gt;negative&lt;/strong&gt; prompt and &lt;strong&gt;ignores&lt;/strong&gt; the &lt;em&gt;additional style prompting fields&lt;/em&gt;, this is great to get similar results as on other workflows and makes it easier to compare the images&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/02-3_prompts.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Subject Focus&lt;/h3&gt; &#xA;&lt;p&gt;In this mode the &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; prompts are &lt;strong&gt;more important&lt;/strong&gt; than the &lt;em&gt;style&lt;/em&gt; prompts&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/03-subject_focus.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Style Focus&lt;/h3&gt; &#xA;&lt;p&gt;In this mode the &lt;em&gt;style&lt;/em&gt; prompts are &lt;strong&gt;more important&lt;/strong&gt; than the &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; prompts&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/04-style_focus.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Weighted&lt;/h3&gt; &#xA;&lt;p&gt;In this mode the &lt;strong&gt;balance&lt;/strong&gt; between &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; prompts and &lt;em&gt;style prompts&lt;/em&gt; can be influenced with the &lt;strong&gt;style prompt power&lt;/strong&gt; and &lt;strong&gt;negative prompt power&lt;/strong&gt; option&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/05-weighted.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Overlay&lt;/h3&gt; &#xA;&lt;p&gt;In this mode the &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; prompts and the &lt;em&gt;style&lt;/em&gt; prompts are &lt;strong&gt;competing with each other&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/06-overlay.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Subject - Style&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; positives with the &lt;em&gt;style&lt;/em&gt; negatives&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/07-subject-style.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Style - Subject&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;em&gt;style&lt;/em&gt; positives with the &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; negatives&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/08-style-subject.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Style Only&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Only&lt;/strong&gt; the &lt;em&gt;style&lt;/em&gt; prompt and &lt;em&gt;negative style&lt;/em&gt; prompt are used, the &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; and &lt;em&gt;negative&lt;/em&gt; are ignored&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/09-style_only.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Weighted - Overlay&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; and &lt;em&gt;style&lt;/em&gt; prompts are &lt;strong&gt;weighted&lt;/strong&gt;, the &lt;em&gt;negative&lt;/em&gt; and &lt;em&gt;negative style&lt;/em&gt; prompts are &lt;strong&gt;overlaid&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/10-weighted-overlay.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h3&gt;Overlay - Weighted&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;em&gt;main &amp;amp; secondary&lt;/em&gt; and &lt;em&gt;style&lt;/em&gt; prompts are &lt;strong&gt;overlaid&lt;/strong&gt;, the &lt;em&gt;negative&lt;/em&gt; and &lt;em&gt;negative style&lt;/em&gt; prompts are &lt;strong&gt;weighted&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/11-overlay-weighted.jpg&#34; width=&#34;512&#34;&gt; &#xA;&lt;h1&gt;Custom Nodes&lt;/h1&gt; &#xA;&lt;p&gt;These custom node types are available in the extension.&lt;/p&gt; &#xA;&lt;p&gt;The details about them are only important if you want to use them in your own workflow or if you want to understand better how the included workflows work.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-Nodetypes.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;h2&gt;SDXL Sampler Node&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-Node-1.png&#34; width=&#34;407&#34;&gt; &#xA;&lt;h3&gt;Inputs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_model&lt;/strong&gt; - connect the SDXL base model here, provided via a &lt;code&gt;Load Checkpoint&lt;/code&gt; node&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_positive&lt;/strong&gt; - recommended to use a &lt;code&gt;CLIPTextEncodeSDXL&lt;/code&gt; with 4096 for &lt;code&gt;width&lt;/code&gt;, &lt;code&gt;height&lt;/code&gt;, &lt;code&gt;target_width&lt;/code&gt;, and &lt;code&gt;target_height&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_negative&lt;/strong&gt; - recommended to use a &lt;code&gt;CLIPTextEncodeSDXL&lt;/code&gt; with 4096 for &lt;code&gt;width&lt;/code&gt;, &lt;code&gt;height&lt;/code&gt;, &lt;code&gt;target_width&lt;/code&gt;, and &lt;code&gt;target_height&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_model&lt;/strong&gt; - connect the SDXL refiner model here, provided via a &lt;code&gt;Load Checkpoint&lt;/code&gt; node&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_positive&lt;/strong&gt; - recommended to use a &lt;code&gt;CLIPTextEncodeSDXLRefiner&lt;/code&gt; with 2048 for &lt;code&gt;width&lt;/code&gt;, and &lt;code&gt;height&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_negative&lt;/strong&gt; - recommended to use a &lt;code&gt;CLIPTextEncodeSDXLRefiner&lt;/code&gt; with 2048 for &lt;code&gt;width&lt;/code&gt;, and &lt;code&gt;height&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;latent_image&lt;/strong&gt; - either an empty latent image or a VAE-encoded latent from a source image for img2img&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;noise_seed&lt;/strong&gt; - the random seed for generating the image&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;steps&lt;/strong&gt; - total steps for the sampler, it will internally be split into base steps and refiner steps&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;cfg&lt;/strong&gt; - CFG scale (classifier free guidance), values between 3.0 and 12.0 are most commonly used&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sampler_name&lt;/strong&gt; - the noise sampler &lt;em&gt;(I prefer dpmpp_2m with the karras scheduler, sometimes ddim with the ddim_uniform scheduler)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;scheduler&lt;/strong&gt; - the scheduler to use with the sampler selected in &lt;code&gt;sampler_name&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_ratio&lt;/strong&gt; - the ratio between base model steps and refiner model steps &lt;em&gt;(0.8 = 80% base model and 20% refiner model, with 30 total steps that&#39;s 24 base steps and 6 refiner steps)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;denoise&lt;/strong&gt; - denoising factor, keep this at 1.0 when creating new images from an empty latent and between 0.0-1.0 in the img2img workflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Outputs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LATENT&lt;/strong&gt; - the generated latent image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SDXL Prompt Node&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/SeargeDP/SeargeSDXL/raw/main/example/Searge-SDXL-Node-2.png&#34; width=&#34;434&#34;&gt; &#xA;&lt;h3&gt;Inputs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_clip&lt;/strong&gt; - connect the SDXL base CLIP here, provided via a &lt;code&gt;Load Checkpoint&lt;/code&gt; node&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_clip&lt;/strong&gt; - connect the SDXL refiner CLIP here, provided via a &lt;code&gt;Load Checkpoint&lt;/code&gt; node&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pos_g&lt;/strong&gt; - the text for the positive base prompt G&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pos_l&lt;/strong&gt; - the text for the positive base prompt L&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pos_r&lt;/strong&gt; - the text for the positive refiner prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;neg_g&lt;/strong&gt; - the text for the negative base prompt G&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;neg_l&lt;/strong&gt; - the text for the negative base prompt L&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;neg_r&lt;/strong&gt; - the text for the negative refiner prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_width&lt;/strong&gt; - the width for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_height&lt;/strong&gt; - the height for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;crop_w&lt;/strong&gt; - crop width for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;crop_h&lt;/strong&gt; - crop height for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;target_width&lt;/strong&gt; - the target width for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;target_height&lt;/strong&gt; - the target height for the base conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pos_ascore&lt;/strong&gt; - the positive aesthetic score for the refiner conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;neg_ascore&lt;/strong&gt; - the negative aesthetic score for the refiner conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_width&lt;/strong&gt; - the width for the refiner conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;refiner_height&lt;/strong&gt; - the height for the refiner conditioning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Outputs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONDITIONING&lt;/strong&gt; 1 - the positive base prompt conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONDITIONING&lt;/strong&gt; 2 - the negative base prompt conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONDITIONING&lt;/strong&gt; 3 - the positive refiner prompt conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONDITIONING&lt;/strong&gt; 4 - the negative refiner prompt conditioning&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>