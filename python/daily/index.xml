<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-08T01:43:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zwq2018/Data-Copilot</title>
    <updated>2023-07-08T01:43:33Z</updated>
    <id>tag:github.com,2023-07-08:/zwq2018/Data-Copilot</id>
    <link href="https://github.com/zwq2018/Data-Copilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data-Copilot&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/zwq2018/Data-Copilot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.07209&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Data-Copilot is a LLM-based system that help you address data-related tasks.&lt;/p&gt; &#xA;&lt;p&gt;Data-Copilot connects data sources from different domains and diverse user tastes, with the ability to autonomously manage, process, analyze, predict, and visualize data.&lt;/p&gt; &#xA;&lt;img src=&#34;./assets/Word Art.png&#34; alt=&#34;Image&#34; style=&#34;width: 900px;&#34;&gt; &#xA;&lt;p&gt;See our paper: &lt;a href=&#34;https://arxiv.org/abs/2306.07209&#34;&gt;Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow&lt;/a&gt;, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang&lt;/p&gt; &#xA;&lt;h2&gt;üî•Demo &lt;a href=&#34;https://zhuanlan.zhihu.com/p/636906119&#34;&gt;video&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Since gpt3.5 has only a 4k input token limit, it currently can access to Chinese stocks, funds and some economic data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Data-Copilot can query and predict data autonomously&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zwq2018/Data-Copilot/main/demo1.png&#34; alt=&#34;Image&#34; style=&#34;width: 900px;&#34;&gt; &#xA;&lt;p&gt;Support model and data sources:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CHN Stock&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CHN Fund&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CHN Economic data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CHN Financial data&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Openai-GPT3.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Azure-GPT3.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zwq2018/Data-Copilot/main/assets/fig1.png&#34; alt=&#34;Image&#34; style=&#34;width: 900px;&#34;&gt; &#xA;&lt;p&gt;We propose Data-Copilot, an LLM-based system linking Chinese financial markets such as stock, funds, economic, financial data, and live news&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;‚≠ê Data-Copilot can &lt;strong&gt;autonomously manage, process, analyze, predict, and visualize data&lt;/strong&gt;. When a request is received, it transforms raw data into informative results that best match the user‚Äôs intent.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚≠ê Acting as a &lt;strong&gt;designer&lt;/strong&gt;: Data-Copilot independently designs versatile interface tools with different functions through self-request and iterative refinement.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚≠ê As a &lt;strong&gt;dispatcher&lt;/strong&gt;: DataCopilot adeptly invokes the corresponding interfaces sequentially or in parallel and transforms raw data from heterogeneous sources into graphics, tables, and text, without human assistance.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üå≥ QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;First replace openai.key and Tushare token in main.py with your personal Openai key and &lt;a href=&#34;https://tushare.pro/&#34;&gt;Tushare token&lt;/a&gt;. The organization of the whole project is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;|-- README.md&#xA;|-- app.py&#xA;|-- create_tool&#xA;|   |-- Atomic_api_json.py&#xA;|   `-- all_atomic_api.json&#xA;|-- lab_gpt4_call.py&#xA;|-- main.py&#xA;|-- output&#xA;|-- prompt_lib&#xA;|   |-- prompt_economic.json&#xA;|   |-- prompt_financial.json&#xA;|   |-- prompt_fund.json&#xA;|   |-- prompt_intent_detection.json&#xA;|   |-- prompt_stock.json&#xA;|   |-- prompt_task.json&#xA;|   `-- prompt_visualization.json&#xA;|-- requirements.txt&#xA;|-- tool.py&#xA;|-- tool_lib&#xA;|   |-- atomic_api.json&#xA;|   |-- tool_backup.json&#xA;|   |-- tool_economic.json&#xA;|   |-- tool_financial.json&#xA;|   |-- tool_fund.json&#xA;|   |-- tool_stock.json&#xA;|   `-- tool_visualization.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;app.py is the file to start gradio. main.py is the processing flow of interface scheduling, and lab_gpt4_call.py is the file to call the GPT35 model. The tool_lib and tool.py contain the interface tools obtained after the first phase of interface design. The folder prompt_lib contains the design of the prompt and the in context demonstration.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;h3&gt;For Local&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remember to fill in the key of Openai and Tushare token before running the code In &lt;code&gt;main.py&lt;/code&gt; for Openai-key&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;openai_key = os.getenv(&#34;OPENAI_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In &lt;code&gt;tool.py&lt;/code&gt; for Tushare token&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;tushare_token = os.getenv(&#39;TUSHARE_TOKEN&#39;)&#xA;pro = ts.pro_api(tushare_token)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For Gradio&lt;/h3&gt; &#xA;&lt;p&gt;The Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üåø How to play&lt;/h2&gt; &#xA;&lt;p&gt;You can try our Data-Copilot for Chinese financial markets in Hugging Face Space:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/zwq2018/Data-Copilot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt; It has access to Chinese stocks, funds and some economic data. But because gpt3.5 only has 4k input token length, the current data access is still relatively small. In the future, data-copilot will support more data from foreign financial markets.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Enter your Openai or Openai-Azure key, please try to use openai&#39;s paid API. If you plan to use azure&#39;s services, please remember to input both api-base and engine, except for key.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Click the OK button to submit&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt; Enter the request you want to query in the text box, or select a question directly from the example box and it will appear in the text box.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt; Click the Start button to submit the request&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt; Data-Copilot will display the intermediate scheduling process in the Solving Step, and the final will present text (Summary and Result), images and tables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zwq2018/Data-Copilot/main/assets/app.png&#34; alt=&#34;Image&#34; style=&#34;width: 900px&#34;&gt; &#xA;&lt;h2&gt;üç∫ Some cases&lt;/h2&gt; &#xA;&lt;p&gt;A case for Check the inflow of northbound every trading date&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zwq2018/Data-Copilot/main/assets/case2.png&#34; alt=&#34;Image&#34; style=&#34;width: 900px&#34;&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful in your method, you can cite the paper as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Data-Copilot,&#xA;    title   = {Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow},&#xA;    author  = {Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang},&#xA;    journal = {arXiv preprint arXiv:2306.07209},&#xA;    year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please contact us by email: &lt;a href=&#34;mailto:zhangwenqi@zju.edu.cn&#34;&gt;zhangwenqi@zju.edu.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tushare.pro/&#34;&gt;Tushare&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>RUCAIBox/LLMSurvey</title>
    <updated>2023-07-08T01:43:33Z</updated>
    <id>tag:github.com,2023-07-08:/RUCAIBox/LLMSurvey</id>
    <link href="https://github.com/RUCAIBox/LLMSurvey" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official GitHub page for the survey paper &#34;A Survey of Large Language Models&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLMSurvey&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A collection of papers and resources related to Large Language Models.&lt;/p&gt; &#xA; &lt;p&gt;The organization of papers refers to our survey &lt;a href=&#34;https://arxiv.org/abs/2303.18223&#34;&gt;&lt;strong&gt;&#34;A Survey of Large Language Models&#34;&lt;/strong&gt;&lt;/a&gt;. &lt;a href=&#34;https://huggingface.co/papers/2303.18223&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/paper-page-sm-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Please let us know if you find out a mistake or have any suggestions by e-mail: &lt;a href=&#34;mailto:batmanfly@gmail.com&#34;&gt;batmanfly@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;(we suggest ccing another email &lt;a href=&#34;mailto:francis_kun_zhou@163.com&#34;&gt;francis_kun_zhou@163.com&lt;/a&gt; meanwhile, in case of any unsuccessful delivery issue.)&lt;/p&gt; &#xA; &lt;p&gt;If you find our survey useful for your research, please cite the following paper:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{LLMSurvey,&#xA;    title={A Survey of Large Language Models},&#xA;    author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},&#xA;    year={2023},&#xA;    journal={arXiv preprint arXiv:2303.18223},&#xA;    url={http://arxiv.org/abs/2303.18223}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Chinese Version&lt;/h2&gt; &#xA;&lt;p&gt;To facilitate the reading of our (English-verison) survey, we also employ LLMs + some human checking to generate a &lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/LLM_Survey__Chinese_V1.pdf&#34;&gt;&lt;strong&gt;Chinese version&lt;/strong&gt;&lt;/a&gt; for this survey. While, since it is mainly generated by LLMs, please don&#39;t forward or post its content on the Web.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/chinese_version.png&#34; alt=&#34;chinese_version&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ(New) The trends of the number of papers related to LLMs on arXiv&lt;/h2&gt; &#xA;&lt;p&gt;Here are the trends of the cumulative numbers of arXiv papers that contain the keyphrases ‚Äúlanguage model‚Äù (since June 2018) and ‚Äúlarge language model‚Äù (since October 2019), respectively.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/arxiv_llms.png&#34; alt=&#34;arxiv_llms&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The statistics are calculated using exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because ‚Äúlanguage models‚Äù have been explored at an earlier time. We label the points corresponding to important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain ‚Äúlarge language model‚Äù in title or abstract goes from 0.40 per day to 8.58 per day.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ(New) Technical Evolution of GPT-series Models&lt;/h2&gt; &#xA;&lt;p&gt;A brief illustration for the technical evolution of GPT-series models. We plot this figure mainly based on the papers, blog articles and official APIs from OpenAI. Here, solid lines denote that there exists an explicit evidence (e.g., the official statement that a new model is developed based on a base model) on the evolution path between two models, while dashed lines denote a relatively weaker evolution relation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/gpt-series.png&#34; alt=&#34;gpt-series&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ(New) Evolutionary Graph of LLaMA Family&lt;/h2&gt; &#xA;&lt;p&gt;An evolutionary graph of the research work conducted on LLaMA. Due to the huge number, we cannot include all the LLaMA variants in this figure, even much excellent work.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/llama-0628-final.png&#34; alt=&#34;LLaMA_family&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To support incremental update, &lt;strong&gt;we share the source file of this figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page. If you&#39;re instrested, please request by application.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ(New) Prompts&lt;/h2&gt; &#xA;&lt;p&gt;We collect some useful tips for designing prompts that are collected from online notes and experiences from our authors, where we also show the related ingredients and principles (introduced in Section 8.1).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/prompts_main.png&#34; alt=&#34;prompt examples&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please click &lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/Prompts/README.md&#34;&gt;here&lt;/a&gt; to view more detailed information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Welcome everyone to provide us with more relevant tips in the form of &lt;a href=&#34;https://github.com/RUCAIBox/LLMSurvey/issues/34&#34;&gt;issues&lt;/a&gt;&lt;/strong&gt;. After selection, we will regularly update them on GitHub and indicate the source.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ(New) Experiments&lt;/h2&gt; &#xA;&lt;h3&gt;Instruction Tuning Experiments&lt;/h3&gt; &#xA;&lt;p&gt;We will explore the effect of different types of instructions in fine-tuning LLMs (i.e., 7B LLaMA26), as well as examine the usefulness of several instruction improvement strategies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/instruction_tuning_table.png&#34; alt=&#34;instruction_tuning_table&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please click &lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/Experiments/README.md&#34;&gt;here&lt;/a&gt; to view more detailed information.&lt;/p&gt; &#xA;&lt;h3&gt;Ability Evaluaition Experiments&lt;/h3&gt; &#xA;&lt;p&gt;We conduct a fine-grained evaluation on the abilities discussed in Section 7.1 and Section 7.2. For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/ability_main.png&#34; alt=&#34;ability_main&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please click &lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/Experiments/README.md&#34;&gt;here&lt;/a&gt; to view more detailed information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We also call for support of computing power for conducting more comprehensive experiments.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#llmsurvey&#34;&gt;LLMSurvey&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#chinese-version&#34;&gt;Chinese Version&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#new-the-trends-of-the-number-of-papers-related-to-llms-on-arxiv&#34;&gt;üöÄ(New) The trends of the number of papers related to LLMs on arXiv&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#new-technical-evolution-of-gpt-series-models&#34;&gt;üöÄ(New) Technical Evolution of GPT-series Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#new-evolutionary-graph-of-llama-family&#34;&gt;üöÄ(New) Evolutionary Graph of LLaMA Family&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#new-prompts&#34;&gt;üöÄ(New) Prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#new-experiments&#34;&gt;üöÄ(New) Experiments&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#instruction-tuning-experiments&#34;&gt;Instruction Tuning Experiments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#ability-evaluaition-experiments&#34;&gt;Ability Evaluaition Experiments&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#timeline-of-llms&#34;&gt;Timeline of LLMs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#list-of-llms&#34;&gt;List of LLMs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#paper-list&#34;&gt;Paper List&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#resources-of-llms&#34;&gt;Resources of LLMs&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#publicly-available-models&#34;&gt;Publicly Available Models&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#closed-source-models&#34;&gt;Closed-source Models&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#commonly-used-corpora&#34;&gt;Commonly Used Corpora&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#library-resource&#34;&gt;Library Resource&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#deep-learning-frameworks&#34;&gt;Deep Learning Frameworks&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#pre-training&#34;&gt;Pre-training&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#data-collection&#34;&gt;Data Collection&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#architecture&#34;&gt;Architecture&lt;/a&gt; &#xA;        &lt;ul&gt; &#xA;         &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#mainstream-architectures&#34;&gt;Mainstream Architectures&lt;/a&gt;&lt;/li&gt; &#xA;         &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#detailed-configuration&#34;&gt;Detailed Configuration&lt;/a&gt;&lt;/li&gt; &#xA;         &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#analysis&#34;&gt;Analysis&lt;/a&gt;&lt;/li&gt; &#xA;        &lt;/ul&gt; &lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#training-algorithms&#34;&gt;Training Algorithms&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#pre-training-on-code&#34;&gt;Pre-training on Code&lt;/a&gt; &#xA;        &lt;ul&gt; &#xA;         &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#llms-for-program-synthesis&#34;&gt;LLMs for Program Synthesis&lt;/a&gt;&lt;/li&gt; &#xA;         &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#nlp-tasks-formatted-as-code&#34;&gt;NLP Tasks Formatted as Code&lt;/a&gt;&lt;/li&gt; &#xA;        &lt;/ul&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#adaptation-tuning&#34;&gt;Adaptation Tuning&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#instruction-tuning&#34;&gt;Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#alignment-tuning&#34;&gt;Alignment Tuning&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#parameter-efficient-model-adaptation&#34;&gt;Parameter-Efficient Model Adaptation&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#memory-efficient-model-adaptation&#34;&gt;Memory-Efficient Model Adaptation&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#utilization&#34;&gt;Utilization&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#in-context-learning-icl&#34;&gt;In-Context Learning (ICL)&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#chain-of-thought-reasoning-cot&#34;&gt;Chain-of-Thought Reasoning (CoT)&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#planning-for-complex-task-solving&#34;&gt;Planning for Complex Task Solving&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#capacity-evaluation&#34;&gt;Capacity Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/#update-log&#34;&gt;Update Log&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Timeline of LLMs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/LLMs-0623-final.png&#34; alt=&#34;LLMs_timeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;List of LLMs&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-nrix&#34; align=&#34;center&#34; rowspan=&#34;2&#34;&gt;Category&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-baqh&#34; align=&#34;center&#34; rowspan=&#34;2&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-0lax&#34; align=&#34;center&#34; rowspan=&#34;2&#34;&gt;Release Time&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-baqh&#34; align=&#34;center&#34; rowspan=&#34;2&#34;&gt;Size(B)&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-0lax&#34; align=&#34;center&#34; rowspan=&#34;2&#34;&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-nrix&#34; align=&#34;center&#34; rowspan=&#34;25&#34;&gt;Publicly &lt;br&gt;Accessbile&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;T5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2019/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;mT5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11934&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;PanGu-Œ±&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/05&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.12369&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;CPM-2&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/05&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;198&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10715&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;T0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.08207&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GPT-NeoX-20B&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/02&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.06745&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;CodeGen&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.13474&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Tk-Instruct&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/04&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.07705&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;UL2&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/02&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.05131&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;OPT&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/05&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;175&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;YaLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/06&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yandex/YaLM-100B&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;NLLB&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/07&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;55&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.04672&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;BLOOM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/07&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;176&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.05100&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/08&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;130&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02414&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Flan-T5&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;mT0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.01786&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Galatica&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;120&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.09085&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;BLOOMZ&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/11&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;176&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.01786&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;OPT-IML&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;175&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.12017&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Pythia&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/01&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/02&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;65&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://vicuna.lmsys.org/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;CodeGeeX&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17568&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Koala&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/04&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-nrix&#34; align=&#34;center&#34; rowspan=&#34;31&#34;&gt;Closed&lt;br&gt;Source&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GShard&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2020/01&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;600&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2006.16668v1&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GPT-3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2020/05&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;175&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;LaMDA&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/05&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;137&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.08239&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;HyperCLOVA&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/06&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;82&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.04650&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Codex&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/07&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;ERNIE 3.0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/07&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Jurassic-1&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/08&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;178&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;FLAN&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;137&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.01652&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;MT-NLG&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;530&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.11990&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Yuan 1.0&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;245&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.04725&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Anthropic&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;52&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.00861&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;WebGPT&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;175&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.09332&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Gopher&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;280&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2112.11446v2&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;ERNIE 3.0 Titan&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;260&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.12731&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GLaM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2021/12&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;1200&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.06905&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;InstructGPT&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/01&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;175&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2203.02155v1&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;AlphaCode&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/02&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;41&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2203.07814v1&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Chinchilla&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;70&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15556&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;PaLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/04&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;540&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Cohere&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/06&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;54&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://cohere.ai/&#34;&gt;Homepage&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;AlexaTM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/08&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.01448&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Luminous&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/09&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;70&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.aleph-alpha.com/docs/introduction/luminous/&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Sparrow&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/09&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;70&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2209.14375v1&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;WeLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/09&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.10372&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;U-PaLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;540&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11399&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Flan-PaLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;540&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Flan-U-PaLM&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2022/10&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;540&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/03&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2303.08774v2&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;PanGU-Œ£&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;2023/3&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-baqh&#34; align=&#34;center&#34;&gt;1085&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-0lax&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.10845&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Paper List&lt;/h2&gt; &#xA;&lt;h3&gt;Resources of LLMs&lt;/h3&gt; &#xA;&lt;h4&gt;Publicly Available Models&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;T5&lt;/u&gt;: &lt;strong&gt;&#34;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#34;&lt;/strong&gt;. &lt;em&gt;Colin Raffel et al.&lt;/em&gt; JMLR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/t5-11b&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;mT5&lt;/u&gt;: &lt;strong&gt;&#34;mT5: A massively multilingual pre-trained text-to-text transformer&#34;&lt;/strong&gt;. &lt;em&gt;Linting Xue&lt;/em&gt; et al. NAACL 2021. [&lt;a href=&#34;https://arxiv.org/abs/2010.11934&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/google/mt5-xxl/tree/main&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;PanGu-Œ±&lt;/u&gt;: &lt;strong&gt;&#34;PanGu-Œ±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation&#34;&lt;/strong&gt;. &lt;em&gt;Wei Zeng et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.12369&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CPM-2&lt;/u&gt;: &lt;strong&gt;&#34;CPM-2: Large-scale Cost-effective Pre-trained Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhengyan Zhang et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.10715&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TsinghuaAI/CPM&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;T0&lt;/u&gt;: &lt;strong&gt;&#34;Multitask Prompted Training Enables Zero-Shot Task Generalization&#34;&lt;/strong&gt;. &lt;em&gt;Victor Sanh et al.&lt;/em&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08207&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/bigscience/T0&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;GPT-NeoX-20B&lt;/u&gt;: &lt;strong&gt;&#34;GPT-NeoX-20B: An Open-Source Autoregressive Language Model&#34;&lt;/strong&gt;. &lt;em&gt;Sid Black et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.06745&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b/tree/main&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CodeGen&lt;/u&gt;: &lt;strong&gt;&#34;CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis&#34;&lt;/strong&gt;. &lt;em&gt;Erik Nijkamp et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13474&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/Salesforce/codegen-16B-nl&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Tk-Instruct&lt;/u&gt;: &lt;strong&gt;&#34;Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks&#34;&lt;/strong&gt;. &lt;em&gt;Yizhong Wang et al.&lt;/em&gt; EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.07705&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/allenai/tk-instruct-11b-def-pos&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;UL2&lt;/u&gt;: &lt;strong&gt;&#34;UL2: Unifying Language Learning Paradigms&#34;&lt;/strong&gt;. &lt;em&gt;Yi Tay et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.05131&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/ul2&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;OPT&lt;/u&gt;: &lt;strong&gt;&#34;OPT: Open Pre-trained Transformer Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Susan Zhang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/metaseq/tree/main/projects/OPT&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;NLLB&lt;/u&gt;: &lt;strong&gt;&#34;No Language Left Behind: Scaling Human-Centered Machine Translation&#34;&lt;/strong&gt;. &lt;em&gt;NLLB Team.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.04672&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/nllb&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;BLOOM&lt;/u&gt;: &lt;strong&gt;&#34;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&#34;&lt;/strong&gt;. &lt;em&gt;BigScience Workshop&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.05100&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;GLM&lt;/u&gt;: &lt;strong&gt;&#34;GLM-130B: An Open Bilingual Pre-trained Model&#34;&lt;/strong&gt;. &lt;em&gt;Aohan Zeng et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02414&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THUDM/GLM-130B&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Flan-T5&lt;/u&gt;: &lt;strong&gt;&#34;Scaling Instruction-Finetuned Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Hyung Won Chung et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/t5x/raw/main/docs/models.md#flan-t5-checkpoints&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;mT0 &amp;amp;&amp;amp; BLOOMZ&lt;/u&gt;: &lt;strong&gt;&#34;Crosslingual Generalization through Multitask Finetuning&#34;&lt;/strong&gt;. &lt;em&gt;Niklas Muennighoff et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01786&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bigscience-workshop/xmtf&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Galactica&lt;/u&gt;: &lt;strong&gt;&#34;Galactica: A Large Language Model for Science&#34;&lt;/strong&gt;. &lt;em&gt;Ross Taylor et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09085&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/facebook/galactica-120b&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;OPT-IML&lt;/u&gt;: &lt;strong&gt;&#34;OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization&#34;&lt;/strong&gt;. &lt;em&gt;Srinivasan et al.&lt;/em&gt; . arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12017&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/facebook/opt-iml-30b&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CodeGeeX&lt;/u&gt;: &lt;strong&gt;&#34;CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X&#34;&lt;/strong&gt;. &lt;em&gt;Qinkai Zheng et al.&lt;/em&gt; . arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17568&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THUDM/CodeGeeX&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Pythia&lt;/u&gt;: &lt;strong&gt;&#34;Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling&#34;&lt;/strong&gt;. &lt;em&gt;Stella Biderman et al.&lt;/em&gt; . arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;LLaMA&lt;/u&gt;: &lt;strong&gt;&#34;LLaMA: Open and Efficient Foundation Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Hugo Touvron et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Closed-source Models&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;GShard&lt;/u&gt;: &lt;strong&gt;&#34;GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding&#34;&lt;/strong&gt;. &lt;em&gt;Dmitry Lepikhin et al.&lt;/em&gt; ICLR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2006.16668v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;GPT-3&lt;/u&gt;: &lt;strong&gt;&#34;Language Models are Few-Shot Learners&#34;&lt;/strong&gt;. &lt;em&gt;Tom B. Brown et al.&lt;/em&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;LaMDA&lt;/u&gt;: &lt;strong&gt;&#34;LaMDA: Language Models for Dialog Applications&#34;&lt;/strong&gt;. &lt;em&gt;Romal Thoppilan et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2201.08239&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;HyperCLOVA&lt;/u&gt;: &lt;strong&gt;&#34;What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers&#34;&lt;/strong&gt;. &lt;em&gt;Boseop Kim et al.&lt;/em&gt; EMNLP 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.04650&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CodeX&lt;/u&gt;: &lt;strong&gt;&#34;Evaluating Large Language Models Trained on Code&#34;&lt;/strong&gt;. &lt;em&gt;Mark Chen et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;ERNIE 3.0&lt;/u&gt;: &lt;strong&gt;&#34;ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation&#34;&lt;/strong&gt;. &lt;em&gt;Yu Sun et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Jurassic-1&lt;/u&gt;: &lt;strong&gt;&#34;Jurassic-1: Technical details and evaluation&#34;&lt;/strong&gt;. &lt;em&gt;Opher Lieber et al.&lt;/em&gt; 2021. [&lt;a href=&#34;https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;FLAN&lt;/u&gt;: &lt;strong&gt;&#34;Finetuned Language Models Are Zero-Shot Learners&#34;&lt;/strong&gt;. &lt;em&gt;Jason Wei et al.&lt;/em&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.01652&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;MT-NLG&lt;/u&gt;: &lt;strong&gt;&#34;Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model&#34;&lt;/strong&gt;. &lt;em&gt;Shaden Smith et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2201.11990&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Yuan 1.0&lt;/u&gt;: &lt;strong&gt;&#34;Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning&#34;&lt;/strong&gt;. &lt;em&gt;Shaohua Wu et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.04725&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Anthropic&lt;/u&gt;: &lt;strong&gt;&#34;A General Language Assistant as a Laboratory for Alignment&#34;&lt;/strong&gt; . &lt;em&gt;Amanda Askell et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.00861&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;WebGPT&lt;/u&gt;: &lt;strong&gt;&#34;WebGPT: Browser-assisted question-answering with human feedback&#34;&lt;/strong&gt; . &lt;em&gt;Reiichiro Nakano et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09332&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Gopher&lt;/u&gt;: &lt;strong&gt;&#34;Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher&#34;&lt;/strong&gt;. &lt;em&gt;Jack W. Rae et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2112.11446v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;ERNIE 3.0 Titan&lt;/u&gt;: &lt;strong&gt;&#34;ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation&#34;&lt;/strong&gt;. *Shuohuan Wang et al. *arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.12731&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;GLaM&lt;/u&gt;: &lt;strong&gt;&#34;GLaM: Efficient Scaling of Language Models with Mixture-of-Experts&#34;&lt;/strong&gt;. &lt;em&gt;Nan Du et al.&lt;/em&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.06905&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;InstructGPT&lt;/u&gt;: &lt;strong&gt;&#34;Training language models to follow instructions with human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Long Ouyang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.02155v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;AlphaCode&lt;/u&gt;: &lt;strong&gt;&#34;Competition-Level Code Generation with AlphaCode&#34;&lt;/strong&gt;. &lt;em&gt;Yujia Li et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.07814v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Chinchilla&lt;/u&gt;: &lt;strong&gt;&#34;Training Compute-Optimal Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jordan Hoffmann et al.&lt;/em&gt; arXiv. [&lt;a href=&#34;https://arxiv.org/abs/2203.15556&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;PaLM&lt;/u&gt;: &lt;strong&gt;&#34;PaLM: Scaling Language Modeling with Pathways&#34;&lt;/strong&gt;. &lt;em&gt;Aakanksha Chowdhery et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;AlexaTM&lt;/u&gt;: &lt;strong&gt;&#34;AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model&#34;&lt;/strong&gt;. &lt;em&gt;Saleh Soltan et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.01448&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Sparrow&lt;/u&gt;: &lt;strong&gt;&#34;Improving alignment of dialogue agents via targeted human judgements&#34;&lt;/strong&gt;. &lt;em&gt;Amelia Glaese et al.&lt;/em&gt; . arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2209.14375v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;WeLM&lt;/u&gt;: &lt;strong&gt;&#34;WeLM: A Well-Read Pre-trained Language Model for Chinese&#34;&lt;/strong&gt;. &lt;em&gt;Hui Su et al.&lt;/em&gt; . arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.10372&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;U-PaLM&lt;/u&gt;: &lt;strong&gt;&#34;Transcending Scaling Laws with 0.1% Extra Compute&#34;&lt;/strong&gt;. &lt;em&gt;Yi Tay et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11399&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Flan-PaLM &amp;amp;&amp;amp; Flan-U-PaLM&lt;/u&gt;: &lt;strong&gt;&#34;Scaling Instruction-Finetuned Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Hyung Won Chung et al.&lt;/em&gt; arXiv. [&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;GPT-4&lt;/u&gt;: &lt;strong&gt;&#34;GPT-4 Technical Report&#34;&lt;/strong&gt;. &lt;em&gt;OpenAI&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.08774v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;PanGu-Œ£&lt;/u&gt;: &lt;strong&gt;&#34;PanGu-Œ£: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing&#34;&lt;/strong&gt;. &lt;em&gt;Xiaozhe Ren et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10845&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Commonly Used Corpora&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;BookCorpus&lt;/u&gt;: &lt;strong&gt;&#34;Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books&#34;&lt;/strong&gt;. &lt;em&gt;Yukun Zhu et al.&lt;/em&gt; ICCV 2015. [&lt;a href=&#34;http://arxiv.org/abs/1506.06724v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/bookcorpus&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Guntenburg&lt;/u&gt;: [&lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CommonCrawl&lt;/u&gt;: [&lt;a href=&#34;https://commoncrawl.org/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;C4&lt;/u&gt;: &lt;strong&gt;&#34;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#34;&lt;/strong&gt;. &lt;em&gt;Colin Raffel et al.&lt;/em&gt; JMLR 2019. [&lt;a href=&#34;http://arxiv.org/abs/1910.10683v3&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/c4&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CC-stories-R&lt;/u&gt;: &lt;strong&gt;&#34;A Simple Method for Commonsense Reasoning&#34;&lt;/strong&gt;. &lt;em&gt;Trieu H. Trinh el al.&lt;/em&gt; arXiv 2018. [&lt;a href=&#34;http://arxiv.org/abs/1806.02847v2&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/spacemanidol/cc-stories&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;CC-NEWS&lt;/u&gt;: &lt;strong&gt;&#34;RoBERTa: A Robustly Optimized BERT Pretraining Approach&#34;&lt;/strong&gt;. &lt;em&gt;Yinhan Liu et al.&lt;/em&gt; arXiv 2019. [&lt;a href=&#34;http://arxiv.org/abs/1907.11692v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/cc_news&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;REALNEWs&lt;/u&gt;: &lt;strong&gt;&#34;Defending Against Neural Fake News&#34;&lt;/strong&gt;. &lt;em&gt;Rowan Zellers et al.&lt;/em&gt; NeurIPS 2019. [&lt;a href=&#34;http://arxiv.org/abs/1905.12616v3&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rowanz/grover/tree/master/realnews&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;OpenWebText&lt;/u&gt;: [&lt;a href=&#34;https://skylion007.github.io/OpenWebTextCorpus/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Pushshift.io&lt;/u&gt;: &lt;strong&gt;&#34;The Pushshift Reddit Dataset&#34;&lt;/strong&gt;. &lt;em&gt;Jason Baumgartner et al&lt;/em&gt;. AAAI 2020. [&lt;a href=&#34;http://arxiv.org/abs/2001.08435v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://files.pushshift.io/reddit/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Wikipedia&lt;/u&gt;: [&lt;a href=&#34;https://dumps.wikimedia.org/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;BigQuery&lt;/u&gt;: [&lt;a href=&#34;https://cloud.google.com/bigquery/public-data?hl=zh-cn&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;The Pile&lt;/u&gt;: &lt;strong&gt;&#34;The Pile: An 800GB Dataset of Diverse Text for Language Modeling&#34;&lt;/strong&gt;. &lt;em&gt;Leo Gao et al&lt;/em&gt;. arxiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2101.00027v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;ROOTS&lt;/u&gt;: &lt;strong&gt;&#34;The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset&#34;&lt;/strong&gt;. &lt;em&gt;Lauren√ßon et al&lt;/em&gt;. NeurIPS 2022 Datasets and Benchmarks Track. [&lt;a href=&#34;https://arxiv.org/abs/2303.03915&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Library Resource&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;Transformers&lt;/u&gt;: &lt;strong&gt;&#34;Transformers: State-of-the-Art Natural Language Processing&#34;&lt;/strong&gt;. &lt;em&gt;Thomas Wolf et al.&lt;/em&gt; EMNLP 2020. [&lt;a href=&#34;https://arxiv.org/abs/1910.03771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;DeepSpeed&lt;/u&gt;: &lt;strong&gt;&#34;Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters&#34;&lt;/strong&gt;. &lt;em&gt;Rasley et al.&lt;/em&gt; KDD 2020. [&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3394486.3406703&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Megatron-LM&lt;/u&gt;: &lt;strong&gt;&#34;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism&#34;&lt;/strong&gt;. &lt;em&gt;Mohammad Shoeybi et al.&lt;/em&gt; arXiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1909.08053&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;JAX&lt;/u&gt;: [&lt;a href=&#34;https://github.com/google/jax&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Colossal-AI&lt;/u&gt;: &lt;strong&gt;&#34;Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training&#34;&lt;/strong&gt;. &lt;em&gt;Zhengda Bian et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2110.14883v2&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;BMTrain&lt;/u&gt;: [&lt;a href=&#34;https://github.com/OpenBMB/BMTrain&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;FastMoE&lt;/u&gt;: &lt;strong&gt;&#34;FastMoE: A Fast Mixture-of-Expert Training System&#34;&lt;/strong&gt;. &lt;em&gt;Jiaao He et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.13262&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/laekov/fastmoe&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Deep Learning Frameworks&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;Pytorch&lt;/u&gt;: &lt;strong&gt;&#34;PyTorch: An Imperative Style, High-Performance Deep Learning Library&#34;&lt;/strong&gt;. &lt;em&gt;Adam Paszke el al.&lt;/em&gt; NeurIPS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1912.01703&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pytorch.org/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;TensorFlow&lt;/u&gt;: &lt;strong&gt;&#34;TensorFlow: A system for large-scale machine learning&#34;&lt;/strong&gt;. &lt;em&gt;Mart√≠n Abadi et al.&lt;/em&gt; OSDI 2016. [&lt;a href=&#34;https://arxiv.org/abs/1605.08695&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;MXNet&lt;/u&gt;: &lt;strong&gt;&#34;MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems&#34;&lt;/strong&gt;. &lt;em&gt;Tianqi Chen et al.&lt;/em&gt; arXiv 2015. [&lt;a href=&#34;https://arxiv.org/abs/1512.01274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/apache/mxnet&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;PaddlePaddle&lt;/u&gt;: &lt;strong&gt;&#34;PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice&#34;&lt;/strong&gt; . &lt;em&gt;Yanjun Ma et al.&lt;/em&gt; Frontiers of Data and Domputing 2019. [&lt;a href=&#34;http://www.jfdc.cnic.cn/EN/abstract/abstract2.shtml&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;MindSpore&lt;/u&gt;: &lt;strong&gt;&#34;Huawei MindSpore AI Development Framework&#34;&lt;/strong&gt; . &lt;em&gt;Huawei Technologies Co., Ltd.&lt;/em&gt; Artificial Intelligence Technology 2022. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-981-19-2879-6_5&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mindspore-ai/mindspore&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;OneFlow&lt;/u&gt;: &lt;strong&gt;&#34;OneFlow: Redesign the Distributed Deep Learning Framework from Scratch&#34;&lt;/strong&gt; . &lt;em&gt;Jinhui Yuan et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.15032&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Oneflow-Inc/oneflow&#34;&gt;Source&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;h4&gt;Data Collection&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset&#34;&lt;/strong&gt;. &lt;em&gt;Lauren√ßon et al&lt;/em&gt;. NeurIPS 2022 Datasets and Benchmarks Track. [&lt;a href=&#34;https://arxiv.org/abs/2303.03915&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Deduplicating Training Data Makes Language Models Better&#34;&lt;/strong&gt;. &lt;em&gt;Katherine Lee et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.06499&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Deduplicating Training Data Mitigates Privacy Risks in Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Nikhil Kandpal et al&lt;/em&gt;. ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.06539&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Laws and Interpretability of Learning from Repeated Data&#34;&lt;/strong&gt;. &lt;em&gt;Danny Hernandez et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.10487&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Pretrainer&#39;s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp;amp; Toxicity&#34;&lt;/strong&gt;. &lt;em&gt;Shayne Longpre et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13169&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Architecture&lt;/h4&gt; &#xA;&lt;h5&gt;Mainstream Architectures&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Causal Decoder&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models are Few-Shot Learners&#34;&lt;/strong&gt;. &lt;em&gt;Tom B. Brown et al&lt;/em&gt;. NeurIPS 2020. [&lt;a href=&#34;http://arxiv.org/abs/2005.14165&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;OPT: Open Pre-trained Transformer Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Susan Zhang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2205.01068&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&#34;&lt;/strong&gt;. &lt;em&gt;Teven Le Scao et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2211.05100&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training Compute-Optimal Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jordan Hoffmann et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.15556&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher&#34;&lt;/strong&gt;. &lt;em&gt;Jack W. Rae et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2112.11446&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Galactica: A Large Language Model for Science&#34;&lt;/strong&gt;. &lt;em&gt;Ross Taylor et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2211.09085&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;PaLM: Scaling Language Modeling with Pathways&#34;&lt;/strong&gt;. &lt;em&gt;Aakanksha Chowdhery et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2204.02311&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Jurassic-1: Technical Details and Evaluation&#34;&lt;/strong&gt;. &lt;em&gt;Opher Lieber et al&lt;/em&gt;. AI21 Labs. [&lt;a href=&#34;https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LaMDA: Language Models for Dialog Applications&#34;&lt;/strong&gt;. &lt;em&gt;Romal Thoppilan et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2201.08239&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prefix Decoder&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GLM-130B: An Open Bilingual Pre-trained Model&#34;&lt;/strong&gt;. &lt;em&gt;Aohan Zeng et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.02414&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GLM: General Language Model Pretraining with Autoregressive Blank Infilling&#34;&lt;/strong&gt;. &lt;em&gt;Zhengxiao Du et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;http://arxiv.org/abs/2103.10360&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Transcending Scaling Laws with 0.1% Extra Compute&#34;&lt;/strong&gt;. &lt;em&gt;Yi Tay et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.11399&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoE&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity&#34;&lt;/strong&gt;. &lt;em&gt;William Fedus et al&lt;/em&gt;. JMLR. [&lt;a href=&#34;http://arxiv.org/abs/2101.03961&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Unified Scaling Laws for Routed Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Aidan Clark et al&lt;/em&gt;. ICML 2022. [&lt;a href=&#34;http://arxiv.org/abs/2202.01169&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;SSM&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Pretraining Without Attention&#34;&lt;/strong&gt;. &lt;em&gt;Junxiong Wang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2212.10544&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Efficiently Modeling Long Sequences with Structured State Spaces&#34;&lt;/strong&gt;. &lt;em&gt;Albert Gu et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;http://arxiv.org/abs/2111.00396&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Long Range Language Modeling via Gated State Spaces&#34;&lt;/strong&gt;. &lt;em&gt;Harsh Mehta et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2206.13947&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Hungry Hungry Hippos: Towards Language Modeling with State Space Models&#34;&lt;/strong&gt;. &lt;em&gt;Daniel Y. Fu et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.14052&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Detailed Configuration&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;RMSNorm&lt;/u&gt;: &lt;strong&gt;&#34;Root Mean Square Layer Normalization&#34;&lt;/strong&gt;. &lt;em&gt;Biao Zhang et al&lt;/em&gt;. NeurIPS 2019. [&lt;a href=&#34;http://arxiv.org/abs/1910.07467&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;DeepNorm&lt;/u&gt;: &lt;strong&gt;&#34;DeepNet: Scaling Transformers to 1,000 Layers&#34;&lt;/strong&gt;. &lt;em&gt;Hongyu Wang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.00555&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;Sandwich-LN&lt;/u&gt;: &lt;strong&gt;&#34;CogView: Mastering Text-to-Image Generation via Transformers&#34;&lt;/strong&gt;. &lt;em&gt;Ming Ding et al&lt;/em&gt;. NeirIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.13290&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Position Encoding&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;T5 bias&lt;/u&gt;: &lt;strong&gt;&#34;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#34;&lt;/strong&gt;. &lt;em&gt;Colin Raffel et al.&lt;/em&gt; JMLR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;ALiBi&lt;/u&gt;: &lt;strong&gt;&#34;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&#34;&lt;/strong&gt;. &lt;em&gt;Ofir Press et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;http://arxiv.org/abs/2108.12409&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;RoPE&lt;/u&gt;: &lt;strong&gt;&#34;RoFormer: Enhanced Transformer with Rotary Position Embedding&#34;&lt;/strong&gt;. &lt;em&gt;Jianlin Su et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.09864&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;xPos&lt;/u&gt;: &lt;strong&gt;&#34;A Length-Extrapolatable Transformer&#34;&lt;/strong&gt;. &lt;em&gt;Yutao Sun et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10554&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;u&gt;Multi-query attention&lt;/u&gt;: &lt;strong&gt;&#34;Fast Transformer Decoding: One Write-Head is All You Need&#34;&lt;/strong&gt;. &lt;em&gt;Noam Shazeer&lt;/em&gt;. arXiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;u&gt;FlashAttention&lt;/u&gt;: &lt;strong&gt;&#34;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&#34;&lt;/strong&gt;. &lt;em&gt;Tri Dao et al&lt;/em&gt;. NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Analysis&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?&#34;&lt;/strong&gt;. &lt;em&gt;Thomas Wang et al&lt;/em&gt;. ICML 2022. [&lt;a href=&#34;http://arxiv.org/abs/2204.05832&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;What Language Model to Train if You Have One Million GPU Hours?&#34;&lt;/strong&gt;. &lt;em&gt;Teven Le Scao et al&lt;/em&gt;. Findings of EMNLP 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.15424&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Examining Scaling and Transfer of Language Model Architectures for Machine Translation&#34;&lt;/strong&gt;. &lt;em&gt;Biao Zhang et al&lt;/em&gt;. ICML 2022. [&lt;a href=&#34;http://arxiv.org/abs/2202.00528&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?&#34;&lt;/strong&gt;. &lt;em&gt;Yi Tay et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2207.10551&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Do Transformer Modifications Transfer Across Implementations and Applications?&#34;&lt;/strong&gt;. &lt;em&gt;Sharan Narang et al&lt;/em&gt;. EMNLP 2021. [&lt;a href=&#34;http://arxiv.org/abs/2102.11972&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Training Algorithms&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism&#34;&lt;/strong&gt;. &lt;em&gt;Mohammad Shoeybi et al&lt;/em&gt;. arXiv 2019. [&lt;a href=&#34;http://arxiv.org/abs/1909.08053&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;An Efficient 2D Method for Training Super-Large Deep Learning Models&#34;&lt;/strong&gt;. &lt;em&gt;Qifan Xu et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.05343&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Tesseract: Parallelize the Tensor Parallelism Efficiently&#34;&lt;/strong&gt;. &lt;em&gt;Boxiang Wang et al&lt;/em&gt;. ICPP 2022. [&lt;a href=&#34;http://arxiv.org/abs/2105.14500&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Maximizing Parallelism in Distributed Training for Huge Neural Networks&#34;&lt;/strong&gt;. &lt;em&gt;Zhengda Bian et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2105.14450&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&#34;&lt;/strong&gt;. &lt;em&gt;Yanping Huang et al&lt;/em&gt;. NeurIPS 2019. [&lt;a href=&#34;http://arxiv.org/abs/1811.06965&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;PipeDream: Fast and Efficient Pipeline Parallel DNN Training&#34;&lt;/strong&gt;. &lt;em&gt;Aaron Harlap et al&lt;/em&gt;. arXiv 2018. [&lt;a href=&#34;http://arxiv.org/abs/1806.03377&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&#34;&lt;/strong&gt;. &lt;em&gt;Samyam Rajbhandari et al&lt;/em&gt;. SC 2020. [&lt;a href=&#34;http://arxiv.org/abs/1910.02054&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ZeRO-Offload: Democratizing Billion-Scale Model Training&#34;&lt;/strong&gt;. &lt;em&gt;Jie Ren et al&lt;/em&gt;. USENIX 2021. [&lt;a href=&#34;http://arxiv.org/abs/2101.06840&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Pre-training on Code&lt;/h4&gt; &#xA;&lt;h5&gt;LLMs for Program Synthesis&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Evaluating Large Language Models Trained on Code&#34;&lt;/strong&gt;. &lt;em&gt;Mark Chen et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2107.03374&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Program Synthesis with Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jacob Austin et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2108.07732&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Show Your Work: Scratchpads for Intermediate Computation with Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Maxwell Nye et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2112.00114&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Systematic Evaluation of Large Language Models of Code&#34;&lt;/strong&gt;. &lt;em&gt;Frank F. Xu et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2202.13169&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Competition-Level Code Generation with AlphaCode&#34;&lt;/strong&gt;. &lt;em&gt;Yujia Li et al&lt;/em&gt;. Science. [&lt;a href=&#34;http://arxiv.org/abs/2203.07814&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis&#34;&lt;/strong&gt;. &lt;em&gt;Erik Nijkamp et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2203.13474&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;InCoder: A Generative Model for Code Infilling and Synthesis&#34;&lt;/strong&gt;. &lt;em&gt;Daniel Fried et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2204.05999&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;CodeT: Code Generation with Generated Tests&#34;&lt;/strong&gt;. &lt;em&gt;Bei Chen et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2207.10397&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;StarCoder: may the source be with you!&#34;&lt;/strong&gt;. &lt;em&gt;Raymond Li et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06161&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;NLP Tasks Formatted as Code&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models of Code are Few-Shot Commonsense Learners&#34;&lt;/strong&gt;. &lt;em&gt;Aman Madaan et al&lt;/em&gt;. EMNLP 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.07128&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Autoformalization with Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Yuhuai Wu et al&lt;/em&gt;. NeurIPS 2022. [&lt;a href=&#34;http://arxiv.org/abs/2205.12615&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Adaptation Tuning&lt;/h3&gt; &#xA;&lt;h4&gt;Instruction Tuning&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Multi-Task Deep Neural Networks for Natural Language Understanding&#34;&lt;/strong&gt;. &lt;em&gt;Xiaodong Liu et al&lt;/em&gt;. ACL 2019. [&lt;a href=&#34;https://arxiv.org/abs/1901.11504&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/namisan/mt-dnn&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#34;&lt;/strong&gt;. &lt;em&gt;Colin Raffel et al&lt;/em&gt;. JMLR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Muppet: Massive Multi-task Representations with Pre-Finetuning&#34;&lt;/strong&gt;. &lt;em&gt;Armen Aghajanyan et al&lt;/em&gt;. EMNLP 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.11038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/models?other=arxiv:2101.11038&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Cross-Task Generalization via Natural Language Crowdsourcing Instructions&#34;&lt;/strong&gt;. &lt;em&gt;Swaroop Mishra et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.08773&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instructions.apps.allenai.org/#data&#34;&gt;Collection&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Finetuned Language Models Are Zero-Shot Learners&#34;&lt;/strong&gt;. &lt;em&gt;Jason Wei et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.01652&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/FLAN&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Multitask Prompted Training Enables Zero-Shot Task Generalization&#34;&lt;/strong&gt;. &lt;em&gt;Victor Sanh et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08207&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/bigscience/T0#how-to-use&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts&#34;&lt;/strong&gt;. &lt;em&gt;Stephen H. Bach et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.01279&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bigscience-workshop/promptsource&#34;&gt;Collection&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training language models to follow instructions with human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Long Ouyang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks&#34;&lt;/strong&gt;. &lt;em&gt;Yizhong Wang et al&lt;/em&gt;. EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.07705&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instructions.apps.allenai.org/#data&#34;&gt;Collection&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/models?search=tk-instruct-&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;MVP: Multi-task Supervised Pre-training for Natural Language Generation&#34;&lt;/strong&gt;. &lt;em&gt;Tianyi Tang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.12131&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/RUCAIBox&#34;&gt;Collection&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/RUCAIBox&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Crosslingual Generalization through Multitask Finetuning&#34;&lt;/strong&gt;. &lt;em&gt;Niklas Muennighoff et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01786&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bigscience-workshop/xmtf#data&#34;&gt;Collection&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bigscience-workshop/xmtf#models&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Instruction-Finetuned Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Hyung Won Chung et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/FLAN&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor&#34;&lt;/strong&gt;. &lt;em&gt;Or Honovich et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09689&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/orhonovich/unnatural-instructions&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Self-Instruct: Aligning Language Model with Self Generated Instructions&#34;&lt;/strong&gt;. &lt;em&gt;Yizhong Wang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization&#34;&lt;/strong&gt;. &lt;em&gt;Srinivasan Iyer et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12017&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML&#34;&gt;Checkpoint&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The Flan Collection: Designing Data and Methods for Effective Instruction Tuning&#34;&lt;/strong&gt;. &lt;em&gt;Shayne Longpre et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13688&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/FLAN&#34;&gt;Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Is Prompt All You Need No. A Comprehensive and Broader View of Instruction Learning&#34;&lt;/strong&gt;. &lt;em&gt;Renze Lou et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10475&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning&#34;&lt;/strong&gt;. &lt;em&gt;Hao Chen et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09246&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LIMA: Less Is More for Alignment&#34;&lt;/strong&gt;. &lt;em&gt;Chunting Zhou&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11206&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Alignment Tuning&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;TAMER: Training an Agent Manually via Evaluative Reinforcement&#34;&lt;/strong&gt;. &lt;em&gt;W. Bradley Knox et al&lt;/em&gt;. ICDL 2008. [&lt;a href=&#34;https://www.cs.utexas.edu/~bradknox/papers/icdl08-knox.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Interactive Learning from Policy-Dependent Human Feedback&#34;&lt;/strong&gt;. &lt;em&gt;James MacGlashan et al&lt;/em&gt;. ICML 2017. [&lt;a href=&#34;https://arxiv.org/abs/1701.06049&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Deep Reinforcement Learning from Human Preferences&#34;&lt;/strong&gt;. &lt;em&gt;Paul Christiano et al&lt;/em&gt;. NIPS 2017. [&lt;a href=&#34;https://arxiv.org/abs/1706.03741&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces&#34;&lt;/strong&gt;. &lt;em&gt;Garrett Warnell et al&lt;/em&gt;. AAAI 2018. [&lt;a href=&#34;https://arxiv.org/abs/1709.10163&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Fine-Tuning Language Models from Human Preferences&#34;&lt;/strong&gt;. &lt;em&gt;Daniel M. Ziegler et al&lt;/em&gt;. arXiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1909.08593&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Learning to summarize from human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Nisan Stiennon et al&lt;/em&gt;. NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.01325&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Alignment of Language Agents&#34;&lt;/strong&gt;. &lt;em&gt;Zachary Kenton et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14659&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Recursively Summarizing Books with Human Feedback&#34;&lt;/strong&gt;. &lt;em&gt;Jeff Wu et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.10862&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A General Language Assistant as a Laboratory for Alignment&#34;&lt;/strong&gt;. &lt;em&gt;Amanda Askell et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.00861&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;WebGPT: Browser-assisted question-answering with human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Reiichiro Nakano et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09332&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training language models to follow instructions with human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Long Ouyang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Teaching language models to support answers with verified quotes&#34;&lt;/strong&gt;. &lt;em&gt;Jacob Menick et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11147&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback&#34;&lt;/strong&gt;. &lt;em&gt;Yuntao Bai et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.05862&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning&#34;&lt;/strong&gt;. &lt;em&gt;Deborah Cohen et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.02294&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned&#34;&lt;/strong&gt;. &lt;em&gt;Deep Ganguli et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.07858&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Improving alignment of dialogue agents via targeted human judgements&#34;&lt;/strong&gt;. &lt;em&gt;Amelia Glaese et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14375&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization&#34;&lt;/strong&gt;. &lt;em&gt;Rajkumar Ramamurthy et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01241&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Laws for Reward Model Overoptimization&#34;&lt;/strong&gt;. &lt;em&gt;Leo Gao et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.10760&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The Wisdom of Hindsight Makes Language Models Better Instruction Followers&#34;&lt;/strong&gt;. &lt;em&gt;Tianjun Zhang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05206&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&#34;&lt;/strong&gt;. &lt;em&gt;Hanze Dong et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06767&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Parameter-Efficient Model Adaptation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Parameter-Efficient Transfer Learning for NLP&#34;&lt;/strong&gt;. &lt;em&gt;Neil Houlsby et al&lt;/em&gt;. ICML 2019. [&lt;a href=&#34;https://arxiv.org/abs/1902.00751&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/adapter-bert&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer&#34;&lt;/strong&gt;. &lt;em&gt;Jonas Pfeiffer et al&lt;/em&gt;. EMNLP 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.00052&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Adapter-Hub/adapter-transformers&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts&#34;&lt;/strong&gt;. &lt;em&gt;Taylor Shin et al&lt;/em&gt;. EMNLP 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.15980&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ucinlp.github.io/autoprompt/&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Prefix-Tuning: Optimizing Continuous Prompts for Generation&#34;&lt;/strong&gt;. &lt;em&gt;Xiang Lisa Li et al&lt;/em&gt;. ACL 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.00190&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XiangLi1999/PrefixTuning&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GPT Understands, Too&#34;&lt;/strong&gt;. &lt;em&gt;Xiao Liu et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.10385&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THUDM/P-tuning&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The Power of Scale for Parameter-Efficient Prompt Tuning&#34;&lt;/strong&gt;. &lt;em&gt;Brian Lester et al&lt;/em&gt;. EMNLP 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.08691&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LoRA: Low-Rank Adaptation of Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Edward J. Hu et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Towards a Unified View of Parameter-Efficient Transfer Learning&#34;&lt;/strong&gt;. &lt;em&gt;Junxian He et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.04366&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jxhe/unify-parameter-efficient-tuning&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks&#34;&lt;/strong&gt;. &lt;em&gt;Xiao Liu et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.07602&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation&#34;&lt;/strong&gt;. &lt;em&gt;Mojtaba Valipour et al&lt;/em&gt;. EACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.07558&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Parameter-efficient fine-tuning of large-scale pre-trained language models&#34;&lt;/strong&gt;. &lt;em&gt;Ning Ding et al&lt;/em&gt;. Nat Mach Intell. [&lt;a href=&#34;https://www.nature.com/articles/s42256-023-00626-4&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thunlp/OpenDelta&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning&#34;&lt;/strong&gt;. &lt;em&gt;Qingru Zhang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10512&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/QingruZhang/AdaLoRA&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention&#34;&lt;/strong&gt;. &lt;em&gt;Renrui Zhang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16199&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhiqiang Hu et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01933&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AGI-Edgerunners/LLM-Adapters&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Memory-Efficient Model Adaptation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Survey of Quantization Methods for Efficient Neural Network Inference&#34;&lt;/strong&gt;. &lt;em&gt;Amir Gholami et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.13630&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;8-bit Optimizers via Block-wise Quantization&#34;&lt;/strong&gt;. &lt;em&gt;Tim Dettmers et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.02861&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Compression of Generative Pre-trained Language Models via Quantization&#34;&lt;/strong&gt;. &lt;em&gt;Chaofan Tao et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.10705&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers&#34;&lt;/strong&gt;. &lt;em&gt;Zhewei Yao et al&lt;/em&gt;. NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.01861&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale&#34;&lt;/strong&gt;. &lt;em&gt;Tim Dettmers et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07339&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers&#34;&lt;/strong&gt;. &lt;em&gt;Elias Frantar et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Guangxuan Xiao et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mit-han-lab/smoothquant&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The case for 4-bit precision: k-bit Inference Scaling Laws&#34;&lt;/strong&gt;. &lt;em&gt;Tim Dettmers et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09720&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation&#34;&lt;/strong&gt;. &lt;em&gt;Zhewei Yao et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08302&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34;&lt;/strong&gt;. &lt;em&gt;Tim Dettmers et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;LLM-QAT: Data-Free Quantization Aware Training for Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zechun Liu et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17888&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration&#34;&lt;/strong&gt;. &lt;em&gt;Ji Lin et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mit-han-lab/llm-awq&#34;&gt;GitHub&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Utilization&lt;/h3&gt; &#xA;&lt;h4&gt;In-Context Learning (ICL)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels&#34;&lt;/strong&gt;. &lt;em&gt;Taylor Sorensen et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11364&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;What Makes Good In-Context Examples for GPT-3?&#34;&lt;/strong&gt;. &lt;em&gt;Jiachang Liu et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2101.06804&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Learning to retrieve prompts for in-context learning&#34;&lt;/strong&gt;. &lt;em&gt;Ohad Rubin et al&lt;/em&gt;. NAACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.08633&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Diverse demonstrations improve in-context compositional generalization&#34;&lt;/strong&gt;. &lt;em&gt;Itay Levy et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06800&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Demystifying Prompts in Language Models via Perplexity Estimation&#34;&lt;/strong&gt;. &lt;em&gt;Hila Gonen et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04037&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Active Example Selection for In-Context Learning&#34;&lt;/strong&gt;. &lt;em&gt;Yiming Zhang et al&lt;/em&gt;. EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.04486&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Self-adaptive In-context Learning&#34;&lt;/strong&gt;. &lt;em&gt;Zhiyong Wu et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10375&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity&#34;&lt;/strong&gt;. &lt;em&gt;Yao Lu et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.08786&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Structured Prompting: Scaling In-Context Learning to 1,000 Examples&#34;&lt;/strong&gt;. &lt;em&gt;Hao, Yaru et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06713&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning&#34;&lt;/strong&gt;. &lt;em&gt;Ye, Xi et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.03401&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Cross-Task Generalization via Natural Language Crowdsourcing Instructions&#34;&lt;/strong&gt;. &lt;em&gt;Swaroop Mishra et al&lt;/em&gt;. ACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.08773&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Prompt-Augmented Linear Probing: Scaling Beyond the Limit of Few-shot In-Context Learner&#34;&lt;/strong&gt;. &lt;em&gt;Hyunsoo Cho et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10873&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;An Explanation of In-context Learning as Implicit Bayesian Inference&#34;&lt;/strong&gt;. S&lt;em&gt;ang Michael Xie et al&lt;/em&gt;. ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.02080&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Calibrate Before Use: Improving Few-Shot Performance of Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zihao Zhao et al&lt;/em&gt;. ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.09690&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Data distributional properties drive emergent in-context learning in transformers&#34;&lt;/strong&gt;. &lt;em&gt;Stephanie C. Y. Chan et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.05055&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;In-context Learning and Induction Heads&#34;&lt;/strong&gt;. &lt;em&gt;Catherine Olsson et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2209.11895&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model&#34;&lt;/strong&gt;. &lt;em&gt;Seongjin Shin et al&lt;/em&gt;. NAACL 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.13509&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?&#34;&lt;/strong&gt;. &lt;em&gt;Sewon Min et al&lt;/em&gt;. EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.12837&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale&#34;&lt;/strong&gt;. &lt;em&gt;Hritik Bansal et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09095&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Transformers as algorithms: Generalization and implicit model selection in in-context learning&#34;&lt;/strong&gt;. &lt;em&gt;Yingcong Li et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07067&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Transformers learn in-context by gradient descent&#34;&lt;/strong&gt;. &lt;em&gt;Johannes von Oswald et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07677&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;What learning algorithm is in-context learning? investigations with linear models&#34;&lt;/strong&gt;. &lt;em&gt;Ekin Aky{&#34;{u}}rek et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15661&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Survey for In-context Learning&#34;&lt;/strong&gt;. &lt;em&gt;Qingxiu Dong et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00234&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What In-Context Learning &#34;Learns&#34; In-Context: Disentangling Task Recognition and Task Learning&lt;/strong&gt;. &lt;em&gt;Jane Pan et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09731&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Learnability of In-Context Learning&lt;/strong&gt;. &lt;em&gt;Noam Wies et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07895&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Do Prompt-Based Models Really Understand the Meaning of Their Prompts?&lt;/strong&gt; &lt;em&gt;Albert Webson et al&lt;/em&gt;. NAACL 2022. [&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.167/&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Larger language models do in-context learning differently&lt;/strong&gt;. &lt;em&gt;Jerry Wei&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03846&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meta-in-context learning in large language models&lt;/strong&gt;. &lt;em&gt;Julian Coda-Forno&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12907&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Symbol tuning improves in-context learning in language models&lt;/strong&gt;. &lt;em&gt;Jerry Wei&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08298&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Chain-of-Thought Reasoning (CoT)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Automatic Chain of Thought Prompting in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhuosheng Zhang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03493&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Chain of Thought Prompting Elicits Reasoning in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jason Wei et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&#34;&lt;/strong&gt;. &lt;em&gt;Zelikman et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14465&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large language models are zero-shot reasoners&#34;&lt;/strong&gt;. &lt;em&gt;Takeshi Kojima et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11916&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Automatic Chain of Thought Prompting in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhuosheng Zhang et al&lt;/em&gt;. arXiv. [&lt;a href=&#34;http://arxiv.org/abs/2210.03493&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Complexity-Based Prompting for Multi-Step Reasoning&#34;&lt;/strong&gt;. &lt;em&gt;Yao Fu et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00720&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models are Multilingual Chain-of-Thought Reasoners&#34;&lt;/strong&gt;. &lt;em&gt;Freda Shi et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03057&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Rationale-Augmented Ensembles in Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Xuezhi Wang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.00747&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Denny Zhou et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.10625&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Multimodal Chain-of-Thought Reasoning in Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhuosheng Zhang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00923&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Self-Consistency Improves Chain of Thought Reasoning in Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Xuezhi Wang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11171&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large Language Models Can Self-Improve&#34;&lt;/strong&gt;. &lt;em&gt;Jiaxin Huang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11610&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training Verifiers to Solve Math Word Problems&#34;&lt;/strong&gt;. &lt;em&gt;Karl Cobbe et al&lt;/em&gt;. arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.14168&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;On the Advance of Making Language Models Better Reasoners&#34;&lt;/strong&gt;. &lt;em&gt;Yifei Li et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02336&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large Language Models are reasoners with Self-Verification&#34;&lt;/strong&gt;. &lt;em&gt;Yixuan Weng et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09561&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Teaching small language models to reason&#34;&lt;/strong&gt;. &lt;em&gt;Lucie Charlotte Magister et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08410&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large language models are reasoning teachers&#34;&lt;/strong&gt;. &lt;em&gt;Namgyu Ho et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10071&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning&#34;&lt;/strong&gt;. &lt;em&gt;Ye, Xi et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.03401&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Scaling Instruction-Finetuned Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Hyung Won Chung et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Solving Quantitative Reasoning Problems with Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Aitor Lewkowycz et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.14858&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Text and patterns: For effective chain of thought, it takes two to tango&#34;&lt;/strong&gt;. &lt;em&gt;Aman Madaan et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.07686&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Challenging BIG-Bench tasks and whether chain-of-thought can solve them&#34;&lt;/strong&gt;. &lt;em&gt;Mirac Suzgun et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.09261&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Reasoning with Language Model Prompting: A Survey&#34;&lt;/strong&gt;. &lt;em&gt;Shuofei Qiao et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09597&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Towards Reasoning in Large Language Models: A Survey&#34;&lt;/strong&gt;. &lt;em&gt;Jie Huang et al&lt;/em&gt;. arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10403&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Planning for Complex Task Solving&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/strong&gt;. &lt;em&gt;Denny Zhou et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;https://openreview.net/forum?id=WZH7099tgfM&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PAL: Program-aided Language Models&lt;/strong&gt;. &lt;em&gt;Luyu Gao et al&lt;/em&gt;. ICML 2023. [&lt;a href=&#34;https://openreview.net/forum?id=M1fd9Z00sj&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models&lt;/strong&gt;. &lt;em&gt;Lei Wang et al&lt;/em&gt;. ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ProgPrompt: Generating Situated Robot Task Plans using Large Language Models&lt;/strong&gt;. &lt;em&gt;Ishika Singh et al&lt;/em&gt;. ICRA 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11302&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&lt;/strong&gt;. &lt;em&gt;Shunyu Yao et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10601&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voyager: An Open-Ended Embodied Agent with Large Language Models&lt;/strong&gt;. &lt;em&gt;Guanzhi Wang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/strong&gt;. &lt;em&gt;Noah Shinn et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11366&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Procedural Planning via Dual Text-Image Prompting&lt;/strong&gt;. &lt;em&gt;Yujie Lu et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01795&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-planning Code Generation with Large Language Model&lt;/strong&gt;. &lt;em&gt;Xue Jiang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06689&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decomposed Prompting: A Modular Approach for Solving Complex Tasks&lt;/strong&gt;. &lt;em&gt;Tushar Khot et al&lt;/em&gt;. ICLR 2023 [&lt;a href=&#34;https://openreview.net/forum?id=_nGgzQjzaRy&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Toolformer: Language Models Can Teach Themselves to Use Tools&lt;/strong&gt;. &lt;em&gt;Timo Schick et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04761&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face&lt;/strong&gt;. &lt;em&gt;Yongliang Shen et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17580&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faithful Chain-of-Thought Reasoning&lt;/strong&gt;. &lt;em&gt;Qing Lyu et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13379&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM+P: Empowering Large Language Models with Optimal Planning Proficiency&lt;/strong&gt;. &lt;em&gt;Bo Liu et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11477&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reasoning with Language Model is Planning with World Model&lt;/strong&gt;. &lt;em&gt;Shibo Hao et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14992&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/strong&gt;. &lt;em&gt;Joon Sung Park et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/strong&gt;. &lt;em&gt;Shunyu Yao et al&lt;/em&gt;. ICLR 2023. [&lt;a href=&#34;https://openreview.net/forum?id=WE_vluYUL-X&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models&lt;/strong&gt;. &lt;em&gt;Zhipeng Chen et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14323&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents&lt;/strong&gt;. &lt;em&gt;Zihao Wang et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01560&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AdaPlanner: Adaptive Planning from Feedback with Language Models&lt;/strong&gt;. &lt;em&gt;Haotian Sun et al&lt;/em&gt;. arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16653&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Capacity Evaluation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Measuring Massive Multitask Language Understanding&#34;&lt;/strong&gt;. &lt;em&gt;Dan Hendrycks et al.&lt;/em&gt; ICLR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2009.03300v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Persistent Anti-Muslim Bias in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Abubakar Abid et al.&lt;/em&gt; AIES 2021. [&lt;a href=&#34;http://arxiv.org/abs/2101.05783v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Alex Tamkin et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2102.02503v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments&#34;&lt;/strong&gt;. &lt;em&gt;Sanjana Srivastava et al.&lt;/em&gt; CoRL 2021. [&lt;a href=&#34;http://arxiv.org/abs/2108.03332v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Program Synthesis with Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jacob Austin et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2108.07732v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training Verifiers to Solve Math Word Problems&#34;&lt;/strong&gt;. &lt;em&gt;Karl Cobbe et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2110.14168v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Show Your Work: Scratchpads for Intermediate Computation with Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Maxwell I. Nye et al.&lt;/em&gt; arXiv 2021. [&lt;a href=&#34;http://arxiv.org/abs/2112.00114v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents&#34;&lt;/strong&gt;. &lt;em&gt;Wenlong Huang et al.&lt;/em&gt; ICML 2022. [&lt;a href=&#34;http://arxiv.org/abs/2201.07207v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Jason Wei et al.&lt;/em&gt; NeurIPS 2022. [&lt;a href=&#34;http://arxiv.org/abs/2201.11903v6&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training language models to follow instructions with human feedback&#34;&lt;/strong&gt;. &lt;em&gt;Long Ouyang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.02155v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Competition-Level Code Generation with AlphaCode&#34;&lt;/strong&gt;. &lt;em&gt;Yujia Li et al.&lt;/em&gt; Science 2022. [&lt;a href=&#34;http://arxiv.org/abs/2203.07814v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Do As I Can, Not As I Say: Grounding Language in Robotic Affordances&#34;&lt;/strong&gt;. &lt;em&gt;Michael Ahn et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2204.01691v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback&#34;&lt;/strong&gt;. &lt;em&gt;Yuntao Bai et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2204.05862v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Autoformalization with Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Yuhuai Wu et al.&lt;/em&gt; NeurIPS 2022. [&lt;a href=&#34;http://arxiv.org/abs/2205.12615v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models&#34;&lt;/strong&gt;. &lt;em&gt;Aarohi Srivastava et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04615&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Exploring Length Generalization in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Cem Anil et al.&lt;/em&gt; NeurIPS 2022. [&lt;a href=&#34;http://arxiv.org/abs/2207.04901v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Few-shot Learning with Retrieval Augmented Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Gautier Izacard et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.03299&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Limitations of Language Models in Arithmetic and Symbolic Induction&#34;&lt;/strong&gt;. &lt;em&gt;Jing Qian et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2208.05051v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Code as Policies: Language Model Programs for Embodied Control&#34;&lt;/strong&gt;. &lt;em&gt;Jacky Liang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2209.07753v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ProgPrompt: Generating Situated Robot Task Plans using Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Ishika Singh et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2209.11302v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans&#34;&lt;/strong&gt;. &lt;em&gt;John J. Nay et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2209.13020v13&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought&#34;&lt;/strong&gt;. &lt;em&gt;Abulhair Saparov et al.&lt;/em&gt; ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2210.01240v4&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models are Multilingual Chain-of-Thought Reasoners&#34;&lt;/strong&gt;. &lt;em&gt;Freda Shi et al.&lt;/em&gt; ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2210.03057v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Re3: Generating Longer Stories With Recursive Reprompting and Revision&#34;&lt;/strong&gt;. &lt;em&gt;Kevin Yang et al.&lt;/em&gt; EMNLP 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.06774v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Language Models of Code are Few-Shot Commonsense Learners&#34;&lt;/strong&gt;. &lt;em&gt;Aman Madaan et al.&lt;/em&gt; EMNLP 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.07128v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&#34;&lt;/strong&gt;. &lt;em&gt;Mirac Suzgun et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2210.09261v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large Language Models Can Self-Improve&#34;&lt;/strong&gt;. &lt;em&gt;Jiaxin Huang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11610&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs&#34;&lt;/strong&gt;. &lt;em&gt;Albert Q. Jiang et al.&lt;/em&gt; ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2210.12283v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Holistic Evaluation of Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Percy Liang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09110&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;PAL: Program-aided Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Luyu Gao et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10435&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Legal Prompt Engineering for Multilingual Legal Judgement Prediction&#34;&lt;/strong&gt;. &lt;em&gt;Dietrich Trautmann et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2212.02199v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;How Does ChatGPT Perform on the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment&#34;&lt;/strong&gt;. &lt;em&gt;Aidan Gilson et al.&lt;/em&gt; medRxiv 2022. [&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2022.12.23.22283901v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT: The End of Online Exam Integrity?&#34;&lt;/strong&gt;. &lt;em&gt;Teo Susnjak et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2212.09292v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Large Language Models are reasoners with Self-Verification&#34;&lt;/strong&gt;. &lt;em&gt;Yixuan Weng et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09561&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Self-Instruct: Aligning Language Model with Self Generated Instructions&#34;&lt;/strong&gt;. &lt;em&gt;Yizhong Wang et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2212.10560v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports&#34;&lt;/strong&gt;. &lt;em&gt;Katharina Jeblick et al.&lt;/em&gt; arXiv 2022. [&lt;a href=&#34;http://arxiv.org/abs/2212.14882v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The End of Programming&#34;&lt;/strong&gt;. &lt;em&gt;Matt Welsh et al.&lt;/em&gt; ACM 2023. [&lt;a href=&#34;https://cacm.acm.org/magazines/2023/1/267976-the-end-of-programming/fulltext&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Chatgpt goes to law school&#34;&lt;/strong&gt;. &lt;em&gt;Choi Jonathan H et al.&lt;/em&gt; SSRN 2023. [&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4335905&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection&#34;&lt;/strong&gt;. &lt;em&gt;Biyang Guo et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07597v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Is ChatGPT A Good Translator? A Preliminary Study&#34;&lt;/strong&gt;. &lt;em&gt;Wenxiang Jiao et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08745v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Could an Artificial-Intelligence agent pass an introductory physics course?&#34;&lt;/strong&gt;. &lt;em&gt;Gerd Kortemeyer et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12127v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Mathematical Capabilities of ChatGPT&#34;&lt;/strong&gt;. &lt;em&gt;Simon Frieder et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13867v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Zhihong Shao et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.00618v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning&#34;&lt;/strong&gt;. &lt;em&gt;Thomas Carta et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02662v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Evaluating ChatGPT as an Adjunct for Radiologic Decision-Making&#34;&lt;/strong&gt;. &lt;em&gt;Arya Yao et al.&lt;/em&gt; medRxiv 2023. [&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2023.02.02.23285399v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Theory of Mind May Have Spontaneously Emerged in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Michal Kosinski et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.02083v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Categorical Archive of ChatGPT Failures&#34;&lt;/strong&gt;. &lt;em&gt;Ali Borji et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03494v7&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity&#34;&lt;/strong&gt;. &lt;em&gt;Yejin Bang et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.04023v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Toolformer: Language Models Can Teach Themselves to Use Tools&#34;&lt;/strong&gt;. &lt;em&gt;Timo Schick et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.04761v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Is ChatGPT a General-Purpose Natural Language Processing Task Solver?&#34;&lt;/strong&gt;. &lt;em&gt;Chengwei Qin et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.06476v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation&#34;&lt;/strong&gt;. &lt;em&gt;Hendy Amr et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.09210&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT&#34;&lt;/strong&gt;. &lt;em&gt;Qihuang Zhong et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10198v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Zero-Shot Information Extraction via Chatting with ChatGPT&#34;&lt;/strong&gt;. &lt;em&gt;Xiang Wei et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10205v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT: Jack of all trades, master of none&#34;&lt;/strong&gt;. &lt;em&gt;Jan Kocon et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.10724v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective&#34;&lt;/strong&gt;. &lt;em&gt;Jindong Wang et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12095v4&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback&#34;&lt;/strong&gt;. &lt;em&gt;Baolin Peng et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2302.12813v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)&#34;&lt;/strong&gt;. &lt;em&gt;Paulo Shakarian et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13814v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks&#34;&lt;/strong&gt;. &lt;em&gt;Chen Xuanting et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.00293v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;The utility of ChatGPT for cancer treatment information&#34;&lt;/strong&gt;. &lt;em&gt;Shen Chen et al.&lt;/em&gt; medRxiv 2023. [&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2023.03.16.23287316v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Can ChatGPT Assess Human Personalities? A General Evaluation Framework&#34;&lt;/strong&gt;. &lt;em&gt;Haocong Rao et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01248v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT.&#34;&lt;/strong&gt;. &lt;em&gt;Mostafa M. Amin et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03186v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Exploring the Feasibility of ChatGPT for Event Extraction.&#34;&lt;/strong&gt;. &lt;em&gt;Jun Gao et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03836v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Does Synthetic Data Generation of LLMs Help Clinical Text Mining?&#34;&lt;/strong&gt;. &lt;em&gt;Tang Ruixiang et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.04360v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Consistency Analysis of ChatGPT&#34;&lt;/strong&gt;. &lt;em&gt;Myeongjun Jang et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.06273v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Self-planning Code Generation with Large Language Model&#34;&lt;/strong&gt;. &lt;em&gt;Shun Zhang et al.&lt;/em&gt; ICLR 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.06689v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions&#34;&lt;/strong&gt;. &lt;em&gt;Yiming Tan et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07992&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GPT-4 Technical Report&#34;&lt;/strong&gt;. &lt;em&gt;OpenAI et al.&lt;/em&gt; OpenAI 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.08774v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Short Survey of Viewing Large Language Models in Legal Aspect&#34;&lt;/strong&gt;. &lt;em&gt;Zhongxiang Sun et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.09136v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT Participates in a Computer Science Exam&#34;&lt;/strong&gt;. &lt;em&gt;Sebastian Bordt et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09461v2&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models&#34;&lt;/strong&gt;. &lt;em&gt;Junjie Ye et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10420v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?&#34;&lt;/strong&gt;. &lt;em&gt;Kamil Malinka et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.11146v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Sparks of Artificial General Intelligence: Early experiments with GPT-4&#34;&lt;/strong&gt;. &lt;em&gt;S&#39;ebastien Bubeck et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.12712v3&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Is ChatGPT A Good Keyphrase Generator? A Preliminary Study&#34;&lt;/strong&gt;. &lt;em&gt;Mingyang Song et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13001v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Capabilities of GPT-4 on Medical Challenge Problems&#34;&lt;/strong&gt;. &lt;em&gt;Harsha Nori et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13375v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Can we trust the evaluation on ChatGPT?&#34;&lt;/strong&gt;. &lt;em&gt;Rachith Aiyappa et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12767&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks&#34;&lt;/strong&gt;. &lt;em&gt;Fabrizio Gilardi et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.15056v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Evaluation of ChatGPT for NLP-based Mental Health Applications&#34;&lt;/strong&gt;. &lt;em&gt;Bishal Lamichhane et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15727v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models&#34;&lt;/strong&gt;. &lt;em&gt;Bian Ning et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;http://arxiv.org/abs/2303.16421v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams&#34;&lt;/strong&gt;. &lt;em&gt;Desnes Nunes et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17003v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure&#34;&lt;/strong&gt;. &lt;em&gt;Philipp Koralus et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17276v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Yes but.. Can ChatGPT Identify Entities in Historical Documents?&#34;&lt;/strong&gt;. &lt;em&gt;Carlos-Emiliano Gonz√°lez-Gallardo et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17322v1&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Uncovering ChatGPT&#39;s Capabilities in Recommender Systems&#34;&lt;/strong&gt;. &lt;em&gt;Sunhao Dai et al.&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02182&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;The Team&lt;/h3&gt; &#xA;&lt;p&gt;Here is the list of our student contributors in each section.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Student Contributors&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;The whole paper&lt;/td&gt; &#xA;   &lt;td&gt;Kun Zhou, Junyi Li&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Overview &amp;amp;&amp;amp; Resources of LLMs&lt;/td&gt; &#xA;   &lt;td&gt;Yingqian Min (Lead), Chen Yang&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pretraining&lt;/td&gt; &#xA;   &lt;td&gt;Yupeng Hou (Lead), Junjie Zhang, Zican Dong, Yushuo Chen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Adaptaion Tuning&lt;/td&gt; &#xA;   &lt;td&gt;Tianyi Tang (Lead), Jinhao Jiang, Ruiyang Ren, Zikang Liu, Peiyu Liu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Utilization&lt;/td&gt; &#xA;   &lt;td&gt;Xiaolei Wang (Lead), Yifan Du, Xinyu Tang&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Capacity Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;Beichen Zhang (Lead), Zhipeng Chen, Yifan Li&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper. Since the first release of this paper, we have received a number of valuable comments from the readers. We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding, Stella Biderman, Kevin Gray, Jay Alammar and Yubo Feng.&lt;/p&gt; &#xA;&lt;h2&gt;Update Log&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Update Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V1&lt;/td&gt; &#xA;   &lt;td&gt;2023/03/31&lt;/td&gt; &#xA;   &lt;td&gt;The initial version.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V2&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/09&lt;/td&gt; &#xA;   &lt;td&gt;Add the affiliation information.&lt;br&gt;Revise Figure 1 and Table 1 and clarify the &lt;br&gt;corresponding selection criterion for LLMs.&lt;br&gt;Improve the writing.&lt;br&gt;Correct some minor errors.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V3&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/11&lt;/td&gt; &#xA;   &lt;td&gt;Correct the errors for library resources.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V4&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/12&lt;/td&gt; &#xA;   &lt;td&gt;Revise Figure 1 and Table 1 and clarify the release date of LLMs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V5&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/16&lt;/td&gt; &#xA;   &lt;td&gt;Add a new Section 2.2 about&lt;br&gt;the technical evolution of GPT-series models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V6&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/24&lt;/td&gt; &#xA;   &lt;td&gt;Add some new models in Table 1 and Figure 1.&lt;br&gt;Add the discussion about scaling laws.&lt;br&gt;Add some explanations about the&lt;br&gt;model sizes for emergent abilities (Section 2.1).&lt;br&gt;Add an illustrative figure for the attention patterns &lt;br&gt;for different architectures in Figure 4.&lt;br&gt;Add the detailed formulas in Table 4.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V7&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/25&lt;/td&gt; &#xA;   &lt;td&gt;Revise some copy errors in figures and tables.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V8&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/27&lt;/td&gt; &#xA;   &lt;td&gt;Add efficient tuning in Section 5.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V9&lt;/td&gt; &#xA;   &lt;td&gt;2023/04/28&lt;/td&gt; &#xA;   &lt;td&gt;Revise Section 5.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V10&lt;/td&gt; &#xA;   &lt;td&gt;2023/05/07&lt;/td&gt; &#xA;   &lt;td&gt;Revise Table 1, Table 2, and some minor points.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;V11 &lt;br&gt;(major revision)&lt;/td&gt; &#xA;   &lt;td&gt;2023/06/29&lt;/td&gt; &#xA;   &lt;td&gt;‚Äì Section 1: add Figure 1 for the trends of published&lt;br&gt;LLM papers in arXiv;&lt;br&gt;‚Äì Section 2: add Figure 3 for GPT‚Äôs evolution and the&lt;br&gt;corresponding discussion;&lt;br&gt;‚Äì Section 3: add Figure 4 for LLaMA family and the&lt;br&gt;corresponding discussion;&lt;br&gt;‚Äì Section 5: add latest discussion about the synthetic&lt;br&gt;data formatting of instruction tuning in Section 5.1.1,&lt;br&gt;the empirical analysis for instruction tuning in Sec-&lt;br&gt;tion 5.1.4, parameter-efficient model adaptation in&lt;br&gt;Section 5.3 and memory-efficient adaptation in Sec-&lt;br&gt;tion 5.4;&lt;br&gt;‚Äì Section 6: add latest discussion about the underlying&lt;br&gt;mechanism of ICL 6.1.3, planning for complex task&lt;br&gt;solving in Section 6.3;&lt;br&gt;‚Äì Section 7: add Table 10 for representative datasets for&lt;br&gt;evaluating advanced abilities of LLMs, and empirical&lt;br&gt;ability evaluation in Section 7.3.2;&lt;br&gt;‚Äì Section 8: add prompt design;&lt;br&gt;‚Äì Section 9: add the discussions on applications of&lt;br&gt;LLMs in finance and scientific research domains;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>HazyResearch/flash-attention</title>
    <updated>2023-07-08T01:43:33Z</updated>
    <id>tag:github.com,2023-07-08:/HazyResearch/flash-attention</id>
    <link href="https://github.com/HazyResearch/flash-attention" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast and memory-efficient exact attention&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlashAttention&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides the official implementation of FlashAttention from the following paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/strong&gt;&lt;br&gt; Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√©&lt;br&gt; Paper: &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;&lt;br&gt; IEEE Spectrum &lt;a href=&#34;https://spectrum.ieee.org/mlperf-rankings-2022&#34;&gt;article&lt;/a&gt; about our submission to the MLPerf 2.0 benchmark using FlashAttention. &lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_banner.jpg&#34; alt=&#34;FlashAttention&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve been very happy to see FlashAttention being widely adopted in such a short time after its release. This &lt;a href=&#34;https://github.com/HazyResearch/flash-attention/raw/main/usage.md&#34;&gt;page&lt;/a&gt; contains a partial list of places where FlashAttention is being used.&lt;/p&gt; &#xA;&lt;h2&gt;Full model code and training script&lt;/h2&gt; &#xA;&lt;p&gt;We have released the full GPT model &lt;a href=&#34;https://github.com/HazyResearch/flash-attention/raw/main/flash_attn/models/gpt.py&#34;&gt;implementation&lt;/a&gt;. We also provide optimized implementations of other layers (e.g., MLP, LayerNorm, cross-entropy loss, rotary embedding). Overall this speeds up training by 3-5x compared to the baseline implementation from Huggingface, reaching up to 189 TFLOPs/sec per A100, equivalent to 60.6% model FLOPs utilization (we don&#39;t need any activation checkpointing).&lt;/p&gt; &#xA;&lt;p&gt;We also include a training &lt;a href=&#34;https://github.com/HazyResearch/flash-attention/tree/main/training&#34;&gt;script&lt;/a&gt; to train GPT2 on Openwebtext and GPT3 on The Pile.&lt;/p&gt; &#xA;&lt;h2&gt;Triton implementation of FlashAttention&lt;/h2&gt; &#xA;&lt;p&gt;Phil Tillet (OpenAI) has an experimental implementation of FlashAttention in Triton: &lt;a href=&#34;https://github.com/openai/triton/raw/master/python/tutorials/06-fused-attention.py&#34;&gt;https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;As Triton is a higher-level language than CUDA, it might be easier to understand and experiment with. The notations in the Triton implementation are also closer to what&#39;s used in our paper.&lt;/p&gt; &#xA;&lt;p&gt;We also have an experimental implementation in Triton that support attention bias (e.g. ALiBi): &lt;a href=&#34;https://github.com/HazyResearch/flash-attention/raw/main/flash_attn/flash_attn_triton.py&#34;&gt;https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation and features&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 11.4 and above.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.12 and above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We recommend the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch&#34;&gt;Pytorch&lt;/a&gt; container from Nvidia, which has all the required tools to install FlashAttention.&lt;/p&gt; &#xA;&lt;p&gt;To install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can compile from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Interface: &lt;code&gt;src/flash_attention.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the benchmark against PyTorch standard attention:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYTHONPATH=$PWD python benchmarks/benchmark_flash_attention.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;FlashAttention currently supports:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Turing, Ampere, Ada, or Hopper GPUs (e.g., H100, A100, RTX 3090, T4, RTX 2080).&lt;/li&gt; &#xA; &lt;li&gt;fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).&lt;/li&gt; &#xA; &lt;li&gt;Head dimensions that are multiples of 8, up to 128 (e.g., 8, 16, 24, ..., 128). Head dim &amp;gt; 64 backward requires A100 or H100.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Our tentative roadmap:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jun 2022] Make package pip-installable&lt;/del&gt;[Done, thanks to lucidrains].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jun 2022] Support SM86 GPUs (e.g., RTX 3080, 3090)&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jun 2022] Support SM75 GPUs (e.g. T4)&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jun 2022] Support bf16&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jul 2022] Implement cross-attention&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Jul 2022] Support head dimension 128&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Aug 2022] Fuse rotary embedding&lt;/del&gt;[Done].&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;[Mar 2023] Support SM90 GPUs (H100)&lt;/del&gt;[Done].&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to use FlashAttention&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a simple example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from flash_attn.flash_attention import FlashMHA&#xA;&#xA;# Replace this with your correct GPU device&#xA;device = &#34;cuda:0&#34;&#xA;&#xA;# Create attention layer. This is similar to torch.nn.MultiheadAttention,&#xA;# and it includes the input and output linear layers&#xA;flash_mha = FlashMHA(&#xA;    embed_dim=128, # total channels (= num_heads * head_dim)&#xA;    num_heads=8, # number of heads&#xA;    device=device,&#xA;    dtype=torch.float16,&#xA;)&#xA;&#xA;# Run forward pass with dummy data&#xA;x = torch.randn(&#xA;    (64, 256, 128), # (batch, seqlen, embed_dim)&#xA;    device=device,&#xA;    dtype=torch.float16&#xA;)&#xA;&#xA;output = flash_mha(x)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can import the inner attention layer only (so that the input and output linear layers are not included):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flash_attn.flash_attention import FlashAttention&#xA;&#xA;# Create the nn.Module&#xA;flash_attention = FlashAttention()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, if you need more fine-grained control, you can import one of the lower-level functions (this is more similar to the &lt;code&gt;torch.nn.functional&lt;/code&gt; style):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flash_attn.flash_attn_interface import flash_attn_unpadded_func&#xA;&#xA;# or&#xA;&#xA;from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_split_func&#xA;&#xA;# etc.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are also separate Python files with various FlashAttention extensions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the triton implementation (torch.nn.functional version only)&#xA;from flash_attn.flash_attn_triton import flash_attn_func&#xA;&#xA;# Import block sparse attention (nn.Module version)&#xA;from flash_attn.flash_blocksparse_attention import FlashBlocksparseMHA, FlashBlocksparseAttention&#xA;&#xA;# Import block sparse attention (torch.nn.functional version)&#xA;from flash_attn.flash_blocksparse_attn_interface import flash_blocksparse_attn_func&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speedup and Memory Savings&lt;/h2&gt; &#xA;&lt;p&gt;We present expected speedup (combined forward + backward pass) and memory savings from using FlashAttention against PyTorch standard attention, depending on sequence length, on different GPUs (speedup depends on memory bandwidth - we see more speedup on slower GPU memory).&lt;/p&gt; &#xA;&lt;p&gt;We currently have benchmarks for these GPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/#a100&#34;&gt;A100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/#rtx-3090&#34;&gt;RTX 3090&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/#t4&#34;&gt;T4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;A100&lt;/h3&gt; &#xA;&lt;p&gt;We display FlashAttention speedup using these parameters (similar to BERT-base):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch size 8&lt;/li&gt; &#xA; &lt;li&gt;Head dimension 64&lt;/li&gt; &#xA; &lt;li&gt;12 attention heads&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our graphs show sequence lengths between 128 and 4096 (when standard attention runs out of memory on an A100), but FlashAttention can scale up to sequence length 64K.&lt;/p&gt; &#xA;&lt;h4&gt;Speedup&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_speedup.jpg&#34; alt=&#34;FlashAttention speedup&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We generally see 2-4X speedup at sequence lengths between 128 and 4K, and we see more speedup when using dropout and masking, since we fuse the kernels. At sequence lengths that are popular with language models like 512 and 1K, we see speedups up to 4X when using dropout and masking.&lt;/p&gt; &#xA;&lt;h4&gt;Memory&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_memory.jpg&#34; alt=&#34;FlashAttention memory&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We show memory savings in this graph (note that memory footprint is the same no matter if you use dropout or masking). Memory savings are proportional to sequence length -- since standard attention has memory quadratic in sequence length, whereas FlashAttention has memory linear in sequence length. We see 10X memory savings at sequence length 2K, and 20X at 4K. As a result, FlashAttention can scale to much longer sequence lengths.&lt;/p&gt; &#xA;&lt;h4&gt;Head Dimension 128&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_speedup_a100_d128.jpg&#34; alt=&#34;FlashAttention speedup, head dimension 128&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We show speedup with head dimension 128. Here we show batch size 16 with 12 heads. Speedup is less than with the smaller head sizes, since we have to make the block size smaller in the tiling. But speedup is still significant, especially with a causal mask.&lt;/p&gt; &#xA;&lt;h3&gt;RTX 3090&lt;/h3&gt; &#xA;&lt;p&gt;For the RTX 3090, we use batch size 12 with 12 attention heads. Memory savings are the same as on an A100, so we&#39;ll only show speedup here.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_speedup_3090.jpg&#34; alt=&#34;FlashAttention speedup GTX 3090&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We see slightly higher speedups (between 2.5-4.5x) on the GTX 3090, since memory bandwidth on the GDDR6X is lower than A100 HBM (~900 GB/s vs. ~1.5 TB/s).&lt;/p&gt; &#xA;&lt;h3&gt;T4&lt;/h3&gt; &#xA;&lt;p&gt;We again use batch size 12 with 12 attention heads.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_speedup_t4.jpg&#34; alt=&#34;Flashattention speedup T4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;T4 SRAM is smaller than the newer GPUs (64 KB), so we see less speedup (we need to make the block sizes smaller, so we end up doing more R/W). This matches the IO complexity analysis from section 3.2 of &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;T4 GPUs are commonly used for inference, so we also measure speedup on the forward pass only (note that these are not directly comparable to the graphs above):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/flash-attention/main/assets/flashattn_speedup_t4_fwd.jpg&#34; alt=&#34;FlashAttention speedup T4 fwd&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We see speedups between 2.5x-4.5x on the forward pass.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;We test that FlashAttention produces the same output and gradient as a reference implementation, up to some numerical tolerance. In particular, we check that the maximum numerical error of FlashAttention is at most twice the numerical error of a baseline implementation in Pytorch (for different head dimensions, input dtype, sequence length, causal / non-causal).&lt;/p&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -q -s tests/test_flash_attn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;When you encounter issues&lt;/h2&gt; &#xA;&lt;p&gt;This alpha release of FlashAttention contains code written for a research project to validate ideas on speeding up attention. We have tested it on several models (BERT, GPT2, ViT). However, there might still be bugs in the implementation that we hope to iron out in the next few months.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter any of these bugs, please open a respective GitHub Issue!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Our implementation uses Apex&#39;s &lt;a href=&#34;https://github.com/NVIDIA/apex/tree/master/apex/contrib/csrc/fmha&#34;&gt;FMHA&lt;/a&gt; code as a starting point.&lt;/p&gt; &#xA;&lt;p&gt;We thank &lt;a href=&#34;https://yjk21.github.io/&#34;&gt;Young-Jun Ko&lt;/a&gt; for the in-depth explanation of his FMHA implementation and for his thoughtful answers to our questions about CUDA.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{dao2022flashattention,&#xA;  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},&#xA;  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\&#39;e}, Christopher},&#xA;  booktitle={Advances in Neural Information Processing Systems},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>