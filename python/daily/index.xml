<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-26T01:33:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cluic/wxauto</title>
    <updated>2024-06-26T01:33:28Z</updated>
    <id>tag:github.com,2024-06-26:/cluic/wxauto</id>
    <link href="https://github.com/cluic/wxauto" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Windows版本微信客户端（非网页版）自动化，可实现简单的发送、接收微信消息，简单微信机器人&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;wxauto (适用PC微信3.9.11.17版本）&lt;/h1&gt; &#xA;&lt;h3&gt;欢迎指出bug，欢迎pull requests&lt;/h3&gt; &#xA;&lt;p&gt;Windows版本微信客户端自动化，可实现简单的发送、接收微信消息、保存聊天图片&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.9.11.17版本微信安装包下载&lt;/strong&gt;： &lt;a href=&#34;https://github.com/tom-snow/wechat-windows-versions/releases/download/v3.9.11.17/WeChatSetup-3.9.11.17.exe&#34;&gt;点击下载&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;文档&lt;/strong&gt;： &lt;a href=&#34;https://wxauto.loux.cc/docs/intro&#34;&gt;使用文档&lt;/a&gt; | &lt;a href=&#34;https://wxauto.loux.cc/docs/advanced/deploy&#34;&gt;云服务器wxauto部署指南&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;环境&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;版本&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.microsoft.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Windows-10%7C11%7CServer2016+-white?logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;微信&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1FvSw0Fk54GGvmQq8xSrNjA?pwd=vsmj&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-3.9.11.X-07c160?logo=wechat&amp;amp;logoColor=white&#34; alt=&#34;Wechat&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.X-blue?logo=python&amp;amp;logoColor=white&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;strong&gt;(不支持3.7.6和3.8.1)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#cluic/wxauto&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=cluic/wxauto&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;获取wxauto&lt;/h2&gt; &#xA;&lt;p&gt;cmd窗口：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install wxauto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;python窗口：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import wxauto&#xA;&amp;gt;&amp;gt;&amp;gt; wxauto.VERSION&#xA;&#39;3.9.11.17&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; wx = wxauto.WeChat()&#xA;初始化成功，获取到已登录窗口：xxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;示例&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 如有问题请先查看&lt;a href=&#34;https://wxauto.loux.cc/docs/intro&#34;&gt;使用文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;请先登录PC微信客户端&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from wxauto import *&#xA;&#xA;&#xA;# 获取当前微信客户端&#xA;wx = WeChat()&#xA;&#xA;&#xA;# 获取会话列表&#xA;wx.GetSessionList()&#xA;&#xA;# 向某人发送消息（以`文件传输助手`为例）&#xA;msg = &#39;你好~&#39;&#xA;who = &#39;文件传输助手&#39;&#xA;wx.SendMsg(msg, who)  # 向`文件传输助手`发送消息：你好~&#xA;&#xA;&#xA;# 向某人发送文件（以`文件传输助手`为例，发送三个不同类型文件）&#xA;files = [&#xA;    &#39;D:/test/wxauto.py&#39;,&#xA;    &#39;D:/test/pic.png&#39;,&#xA;    &#39;D:/test/files.rar&#39;&#xA;]&#xA;who = &#39;文件传输助手&#39;&#xA;wx.SendFiles(filepath=files, who=who)  # 向`文件传输助手`发送上述三个文件&#xA;&#xA;&#xA;# 下载当前聊天窗口的聊天记录及图片&#xA;msgs = wx.GetAllMessage(savepic=True)   # 获取聊天记录，及自动下载图片&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;注意事项&lt;/h2&gt; &#xA;&lt;p&gt;目前还在开发中，测试案例较少，使用过程中可能遇到各种Bug&lt;/p&gt; &#xA;&lt;p&gt;如果遇到问题或者有新的想法，希望您可以通过以下两种方式联系我进行改进：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;微信：louxinghao（请备注wxauto，加群请备注加入交流群，合作、定制请备注合作）&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://github.com/cluic/wxauto/raw/WeChat3.9.8/utils/wxqrcode.png&#34; alt=&#34;微信&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;最后&lt;/h2&gt; &#xA;&lt;p&gt;如果对您有帮助，希望可以帮忙点个Star，如果您正在使用这个项目，可以将右上角的 Unwatch 点为 Watching，以便在我更新或修复某些 Bug 后即使收到反馈，感谢您的支持，非常感谢！&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;代码仅供交流学习使用，请勿用于非法用途和商业用途！如因此产生任何法律纠纷，均与作者无关！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CollegesChat/university-information</title>
    <updated>2024-06-26T01:33:28Z</updated>
    <id>tag:github.com,2024-06-26:/CollegesChat/university-information</id>
    <link href="https://github.com/CollegesChat/university-information" rel="alternate"></link>
    <summary type="html">&lt;p&gt;收集全国各高校招生时不会写明，却会实实在在影响大学生活质量的要求与细节&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;一些大学的生活质量&lt;/h1&gt; &#xA;&lt;p&gt;这是什么项目？&lt;/p&gt; &#xA;&lt;p&gt;这是一个受 &lt;a href=&#34;https://t.me/RiNGNiR/3571&#34;&gt;https://t.me/RiNGNiR/3571&lt;/a&gt; 和 &lt;a href=&#34;https://t.me/RiNGNiR/3572&#34;&gt;https://t.me/RiNGNiR/3572&lt;/a&gt; 启发的项目，意在收集全国各高校招生时不会写明，却会实实在在影响大学生活质量的要求与细节。&lt;/p&gt; &#xA;&lt;h2&gt;查询 &amp;amp; 贡献 &amp;amp; 提问&lt;/h2&gt; &#xA;&lt;p&gt;问卷资料查询请前往 &lt;a href=&#34;https://colleges.chat&#34;&gt;https://colleges.chat&lt;/a&gt; (本项目自动生成的 .md 资料文件请前往 &lt;a href=&#34;https://github.com/CollegesChat/university-information/tree/generated&#34;&gt;generated 分支&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;贡献、提问、部分资料查询请前往 &lt;a href=&#34;https://github.com/YanWQ-monad/university-information/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;参考 FAQ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;以下内容摘自 &lt;a href=&#34;https://t.me/RiNGNiR/3571&#34;&gt;https://t.me/RiNGNiR/3571&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;一些很多人填报志愿时候不会问但是真的很影响大学生活质量的问题&lt;/p&gt; &#xA; &lt;ol start=&#34;0&#34;&gt; &#xA;  &lt;li&gt;宿舍是上床下桌吗&lt;/li&gt; &#xA;  &lt;li&gt;教室和宿舍有没有空调&lt;/li&gt; &#xA;  &lt;li&gt;有独立卫浴吗？没有独立浴室的话，澡堂离宿舍多远&lt;/li&gt; &#xA;  &lt;li&gt;有早自习、晚自习吗&lt;/li&gt; &#xA;  &lt;li&gt;有晨跑吗&lt;/li&gt; &#xA;  &lt;li&gt;每学期跑步打卡的要求是多少公里，可以骑车吗&lt;/li&gt; &#xA;  &lt;li&gt;寒暑假放多久，每年小学期有多长&lt;/li&gt; &#xA;  &lt;li&gt;学校允许点外卖吗，取外卖的地方离宿舍楼多远&lt;/li&gt; &#xA;  &lt;li&gt;学校附近有地铁站吗&lt;/li&gt; &#xA;  &lt;li&gt;宿舍楼有洗衣机吗&lt;/li&gt; &#xA;  &lt;li&gt;校园网怎么样&lt;/li&gt; &#xA;  &lt;li&gt;每天断电断网吗，几点开始断&lt;/li&gt; &#xA;  &lt;li&gt;食堂价格贵吗，会吃出异物吗&lt;/li&gt; &#xA;  &lt;li&gt;洗澡热水供应时间&lt;/li&gt; &#xA;  &lt;li&gt;校园内可以骑电瓶车吗，电池在哪能充电&lt;/li&gt; &#xA;  &lt;li&gt;宿舍限电情况&lt;/li&gt; &#xA;  &lt;li&gt;通宵自习有去处吗&lt;/li&gt; &#xA;  &lt;li&gt;大一能带电脑吗&lt;/li&gt; &#xA;  &lt;li&gt;学校里面用什么卡，饭堂怎样消费&lt;/li&gt; &#xA;  &lt;li&gt;学校会给学生发银行卡吗&lt;/li&gt; &#xA;  &lt;li&gt;学校的超市怎么样&lt;/li&gt; &#xA;  &lt;li&gt;学校的收发快递政策怎么样&lt;/li&gt; &#xA;  &lt;li&gt;学校里面的共享单车数目与种类如何&lt;/li&gt; &#xA;  &lt;li&gt;现阶段学校的门禁情况如何&lt;/li&gt; &#xA;  &lt;li&gt;宿舍晚上查寝吗，封寝吗，晚归能回去吗&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;待补充&lt;/p&gt; &#xA; &lt;p&gt;大学的条件真的不像很多人想象的那么好，尤其是很多老校区，除了学校本身实力以外，还是建议大家多了解了解，不然大学真的会和服刑一样&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-Hans&#34;&gt;CC BY-NC-SA 4.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如有其它 LICENSE 会注明。&lt;/p&gt; &#xA;&lt;p&gt;如果有侵权、不实信息请联系进行删除。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/table-transformer</title>
    <updated>2024-06-26T01:33:28Z</updated>
    <id>tag:github.com,2024-06-26:/microsoft/table-transformer</id>
    <link href="https://github.com/microsoft/table-transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Table Transformer (TATR) is a deep learning model for extracting tables from unstructured documents (PDFs and images). This is also the official repository for the PubTables-1M dataset and GriTS evaluation metric.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Table Transformer (TATR)&lt;/h1&gt; &#xA;&lt;p&gt;A deep learning model based on object detection for extracting tables from PDFs and images.&lt;/p&gt; &#xA;&lt;p&gt;First proposed in &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Smock_PubTables-1M_Towards_Comprehensive_Table_Extraction_From_Unstructured_Documents_CVPR_2022_paper.html&#34;&gt;&#34;PubTables-1M: Towards comprehensive table extraction from unstructured documents&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10793386/139559159-cd23c972-8731-48ed-91df-f3f27e9f4d79.jpg&#34; alt=&#34;table_extraction_v2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository also contains the official code for these papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12555&#34;&gt;&#34;GriTS: Grid table similarity metric for table structure recognition&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.00716&#34;&gt;&#34;Aligning benchmark datasets for table structure recognition&#34;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: If you are looking to use Table Transformer to extract your own tables, here are some helpful things to know:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TATR can be trained to work well across many document domains and everything needed to train your own model is included here. But at the moment pre-trained model weights are only available for TATR trained on the PubTables-1M dataset. (See the additional documentation for how to train your own multi-domain model.)&lt;/li&gt; &#xA; &lt;li&gt;TATR is an object detection model that recognizes tables from image input. The inference code built on TATR needs text extraction (from OCR or directly from PDF) as a separate input in order to include text in its HTML or CSV output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional information about this project for both users and researchers, including data, training, evaluation, and inference code is provided below.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;08/22/2023&lt;/code&gt;: We have released 3 new pre-trained models for TATR-v1.1 (trained on 1. PubTables-1M, 2. FinTabNet.c, and 3. both datasets combined) according to the details in &lt;a href=&#34;https://arxiv.org/abs/2303.00716&#34;&gt;our paper&lt;/a&gt;.&lt;br&gt; &lt;code&gt;04/19/2023&lt;/code&gt;: Our latest papers (&lt;a href=&#34;https://arxiv.org/abs/2203.12555&#34;&gt;link&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2303.00716&#34;&gt;link&lt;/a&gt;) have been accepted at &lt;a href=&#34;https://icdar2023.org/&#34;&gt;ICDAR 2023&lt;/a&gt;.&lt;br&gt; &lt;code&gt;03/09/2023&lt;/code&gt;: We have added more image cropping to the official training script (like we do in our most recent paper) and updated the code and environment.yml to use Python 3.10.9, PyTorch 1.13.1, and Torchvision 0.14.1, among others.&lt;br&gt; &lt;code&gt;03/07/2023&lt;/code&gt;: We have released a new simple &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/table-transformer/main/src/inference.py&#34;&gt;inference pipeline&lt;/a&gt; for TATR. Now you can easily detect and recognize tables from images and convert them to HTML or CSV.&lt;br&gt; &lt;code&gt;03/07/2023&lt;/code&gt;: We have released a &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/table-transformer/main/scripts/&#34;&gt;collection of scripts&lt;/a&gt; to create training data for TATR and to canonicalize pre-existing datasets, such as FinTabNet and SciTSR.&lt;br&gt; &lt;code&gt;03/01/2023&lt;/code&gt;: New paper &#34;Aligning benchmark datasets for table structure recognition&#34; is now available on &lt;a href=&#34;https://arxiv.org/abs/2303.00716&#34;&gt;arXiv&lt;/a&gt;.&lt;br&gt; &lt;code&gt;11/25/2022&lt;/code&gt;: We have made the full PubTables-1M dataset alternatively available for download from &lt;a href=&#34;https://huggingface.co/datasets/bsmock/pubtables-1m&#34;&gt;Hugging Face&lt;/a&gt;.&lt;br&gt; &lt;code&gt;05/05/2022&lt;/code&gt;: We have released the pre-trained weights for the table structure recognition model trained on PubTables-1M.&lt;br&gt; &lt;code&gt;03/23/2022&lt;/code&gt;: Our paper &#34;GriTS: Grid table similarity metric for table structure recognition&#34; is now available on &lt;a href=&#34;https://arxiv.org/abs/2203.12555&#34;&gt;arXiv&lt;/a&gt;&lt;br&gt; &lt;code&gt;03/04/2022&lt;/code&gt;: We have released the pre-trained weights for the table detection model trained on PubTables-1M.&lt;br&gt; &lt;code&gt;03/03/2022&lt;/code&gt;: &#34;PubTables-1M: Towards comprehensive table extraction from unstructured documents&#34; has been accepted at &lt;a href=&#34;https://cvpr2022.thecvf.com/&#34;&gt;CVPR 2022&lt;/a&gt;.&lt;br&gt; &lt;code&gt;11/21/2021&lt;/code&gt;: Our updated paper &#34;PubTables-1M: Towards comprehensive table extraction from unstructured documents&#34; is available on &lt;a href=&#34;https://arxiv.org/pdf/2110.00061.pdf&#34;&gt;arXiv&lt;/a&gt;.&lt;br&gt; &lt;code&gt;10/21/2021&lt;/code&gt;: The full PubTables-1M dataset has been officially released on &lt;a href=&#34;https://msropendata.com/datasets/505fcbe3-1383-42b1-913a-f651b8b712d3&#34;&gt;Microsoft Research Open Data&lt;/a&gt;.&lt;br&gt; &lt;code&gt;06/08/2021&lt;/code&gt;: Initial version of the Table Transformer (TATR) project is released.&lt;/p&gt; &#xA;&lt;h1&gt;PubTables-1M&lt;/h1&gt; &#xA;&lt;p&gt;The goal of PubTables-1M is to create a large, detailed, high-quality dataset for training and evaluating a wide variety of models for the tasks of &lt;strong&gt;table detection&lt;/strong&gt;, &lt;strong&gt;table structure recognition&lt;/strong&gt;, and &lt;strong&gt;functional analysis&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;575,305 annotated document pages containing tables for table detection.&lt;/li&gt; &#xA; &lt;li&gt;947,642 fully annotated tables including text content and complete location (bounding box) information for table structure recognition and functional analysis.&lt;/li&gt; &#xA; &lt;li&gt;Full bounding boxes in both image and PDF coordinates for all table rows, columns, and cells (including blank cells), as well as other annotated structures such as column headers and projected row headers.&lt;/li&gt; &#xA; &lt;li&gt;Rendered images of all tables and pages.&lt;/li&gt; &#xA; &lt;li&gt;Bounding boxes and text for all words appearing in each table and page image.&lt;/li&gt; &#xA; &lt;li&gt;Additional cell properties not used in the current model training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, cells in the headers are &lt;em&gt;canonicalized&lt;/em&gt; and we implement multiple &lt;em&gt;quality control&lt;/em&gt; steps to ensure the annotations are as free of noise as possible. For more details, please see &lt;a href=&#34;https://arxiv.org/pdf/2110.00061.pdf&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-trained Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;We provide different pre-trained models for table detection and table structure recognition.&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;Table Detection:&lt;/b&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training Data&lt;/th&gt; &#xA;   &lt;th&gt;Model Card&lt;/th&gt; &#xA;   &lt;th&gt;File&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;td&gt;DETR R18&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/tatr-pubtables1m-v1.0&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/tatr-pubtables1m-v1.0/resolve/main/pubtables1m_detection_detr_r18.pth&#34;&gt;Weights&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;b&gt;Table Structure Recognition:&lt;/b&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: left;&#34;&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training Data&lt;/th&gt; &#xA;   &lt;th&gt;Model Card&lt;/th&gt; &#xA;   &lt;th&gt;File&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr style=&#34;text-align: left;&#34;&gt; &#xA;   &lt;td&gt;TATR-v1.0&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/tatr-pubtables1m-v1.0&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/tatr-pubtables1m-v1.0/resolve/main/pubtables1m_structure_detr_r18.pth&#34;&gt;Weights&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;text-align: left;&#34;&gt; &#xA;   &lt;td&gt;TATR-v1.1-Pub&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-Pub&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-Pub/resolve/main/TATR-v1.1-Pub-msft.pth&#34;&gt;Weights&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;text-align: left;&#34;&gt; &#xA;   &lt;td&gt;TATR-v1.1-Fin&lt;/td&gt; &#xA;   &lt;td&gt;FinTabNet.c&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-Fin&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-Fin/resolve/main/TATR-v1.1-Fin-msft.pth&#34;&gt;Weights&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;text-align: left;&#34;&gt; &#xA;   &lt;td&gt;TATR-v1.1-All&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M + FinTabNet.c&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-All&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bsmock/TATR-v1.1-All/resolve/main/TATR-v1.1-All-msft.pth&#34;&gt;Weights&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Evaluation Metrics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;b&gt;Table Detection:&lt;/b&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Test Data&lt;/th&gt; &#xA;   &lt;th&gt;AP50&lt;/th&gt; &#xA;   &lt;th&gt;AP75&lt;/th&gt; &#xA;   &lt;th&gt;AP&lt;/th&gt; &#xA;   &lt;th&gt;AR&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;td&gt;DETR R18&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.989&lt;/td&gt; &#xA;   &lt;td&gt;0.970&lt;/td&gt; &#xA;   &lt;td&gt;0.985&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;b&gt;Table Structure Recognition:&lt;/b&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Test Data&lt;/th&gt; &#xA;   &lt;th&gt;AP50&lt;/th&gt; &#xA;   &lt;th&gt;AP75&lt;/th&gt; &#xA;   &lt;th&gt;AP&lt;/th&gt; &#xA;   &lt;th&gt;AR&lt;/th&gt; &#xA;   &lt;th&gt;GriTS&lt;sub&gt;Top&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;GriTS&lt;sub&gt;Con&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;GriTS&lt;sub&gt;Loc&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Acc&lt;sub&gt;Con&lt;/sub&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;td&gt;TATR-v1.0&lt;/td&gt; &#xA;   &lt;td&gt;PubTables-1M&lt;/td&gt; &#xA;   &lt;td&gt;0.970&lt;/td&gt; &#xA;   &lt;td&gt;0.941&lt;/td&gt; &#xA;   &lt;td&gt;0.902&lt;/td&gt; &#xA;   &lt;td&gt;0.935&lt;/td&gt; &#xA;   &lt;td&gt;0.9849&lt;/td&gt; &#xA;   &lt;td&gt;0.9850&lt;/td&gt; &#xA;   &lt;td&gt;0.9786&lt;/td&gt; &#xA;   &lt;td&gt;0.8243&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training and Evaluation Data&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://msropendata.com/datasets/505fcbe3-1383-42b1-913a-f651b8b712d3&#34;&gt;PubTables-1M&lt;/a&gt; is available for download from &lt;a href=&#34;https://msropendata.com/&#34;&gt;Microsoft Research Open Data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have also uploaded &lt;a href=&#34;https://huggingface.co/datasets/bsmock/pubtables-1m&#34;&gt;the full set of archives&lt;/a&gt; to Hugging Face.&lt;/p&gt; &#xA;&lt;p&gt;The dataset on Microsoft Research Open Data comes in 5 tar.gz files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PubTables-1M-Image_Page_Detection_PASCAL_VOC.tar.gz: Training and evaluation data for the detection model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;/images&lt;/code&gt;: 575,305 JPG files; one file for each page image&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/train&lt;/code&gt;: 460,589 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/test&lt;/code&gt;: 57,125 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/val&lt;/code&gt;: 57,591 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PubTables-1M-Image_Page_Words_JSON.tar.gz: Bounding boxes and text content for all of the words in each page image &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;One JSON file per page image (plus some extra unused files)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PubTables-1M-Image_Table_Structure_PASCAL_VOC.tar.gz: Training and evaluation data for the structure (and functional analysis) model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;/images&lt;/code&gt;: 947,642 JPG files; one file for each page image&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/train&lt;/code&gt;: 758,849 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/test&lt;/code&gt;: 93,834 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;/val&lt;/code&gt;: 94,959 XML files containing bounding boxes in PASCAL VOC format&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PubTables-1M-Image_Table_Words_JSON.tar.gz: Bounding boxes and text content for all of the words in each cropped table image &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;One JSON file per cropped table image (plus some extra unused files)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PubTables-1M-PDF_Annotations_JSON.tar.gz: Detailed annotations for all of the tables appearing in the source PubMed PDFs. All annotations are in PDF coordinates. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;401,733 JSON files; one file per source PDF&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To download from the command line:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Visit the &lt;a href=&#34;https://msropendata.com/datasets/505fcbe3-1383-42b1-913a-f651b8b712d3&#34;&gt;dataset home page&lt;/a&gt; with a web browser and click Download in the top left corner. This will create a link to download the dataset from Azure with a unique access token for you that looks like &lt;code&gt;https://msropendataset01.blob.core.windows.net/pubtables1m?[SAS_TOKEN_HERE]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can then use the command line tool &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10&#34;&gt;azcopy&lt;/a&gt; to download all of the files with the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;azcopy copy &#34;https://msropendataset01.blob.core.windows.net/pubtables1m?[SAS_TOKEN_HERE]&#34; &#34;/path/to/your/download/folder/&#34; --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then unzip each of the archives from the command line using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tar -xzvf yourfile.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code Installation&lt;/h2&gt; &#xA;&lt;p&gt;Create a conda environment from the yml file and activate it as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate tables-detr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;The code trains models for 2 different sets of table extraction tasks:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Table Detection&lt;/li&gt; &#xA; &lt;li&gt;Table Structure Recognition + Functional Analysis&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For a detailed description of these tasks and the models, please refer to the paper.&lt;/p&gt; &#xA;&lt;p&gt;To train, you need to &lt;code&gt;cd&lt;/code&gt; to the &lt;code&gt;src&lt;/code&gt; directory and specify: 1. the path to the dataset, 2. the task (detection or structure), and 3. the path to the config file, which contains the hyperparameters for the architecture and training.&lt;/p&gt; &#xA;&lt;p&gt;To train the detection model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --data_type detection --config_file detection_config.json --data_root_dir /path/to/detection_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train the structure recognition model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --data_type structure --config_file structure_config.json --data_root_dir /path/to/structure_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The evaluation code computes standard object detection metrics (AP, AP50, etc.) for both the detection model and the structure model. When running evaluation for the structure model it also computes grid table similarity (GriTS) metrics for table structure recognition. GriTS is a measure of table cell correctness and is defined as the average correctness of each cell averaged over all tables. GriTS can measure the correctness of predicted cells based on: 1. cell topology alone, 2. cell topology and the reported bounding box location of each cell, or 3. cell topology and the reported text content of each cell. For more details on GriTS, please see our papers.&lt;/p&gt; &#xA;&lt;p&gt;To compute object detection metrics for the detection model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --mode eval --data_type detection --config_file detection_config.json --data_root_dir /path/to/pascal_voc_detection_data --model_load_path /path/to/detection_model  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compute object detection and GriTS metrics for the structure recognition model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --mode eval --data_type structure --config_file structure_config.json --data_root_dir /path/to/pascal_voc_structure_data --model_load_path /path/to/structure_model --table_words_dir /path/to/json_table_words_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally you can add flags for things like controlling parallelization, saving detailed metrics, and saving visualizations:&lt;br&gt; &lt;code&gt;--device cpu&lt;/code&gt;: Change the default device from cuda to cpu.&lt;br&gt; &lt;code&gt;--batch_size 4&lt;/code&gt;: Control the batch size to use during the forward pass of the model.&lt;br&gt; &lt;code&gt;--eval_pool_size 4&lt;/code&gt;: Control the worker pool size for CPU parallelization during GriTS metric computation.&lt;br&gt; &lt;code&gt;--eval_step 2&lt;/code&gt;: Control the number of batches of processed input data to accumulate before passing all samples to the parallelized worker pool for GriTS metric computation.&lt;br&gt; &lt;code&gt;--debug&lt;/code&gt;: Create and save visualizations of the model inference. For each input image &#34;PMC1234567_table_0.jpg&#34;, this will save two visualizations: &#34;PMC1234567_table_0_bboxes.jpg&#34; containing the bounding boxes output by the model, and &#34;PMC1234567_table_0_cells.jpg&#34; containing the final table cell bounding boxes after post-processing. By default these are saved to a new folder &#34;debug&#34; in the current directory.&lt;br&gt; &lt;code&gt; --debug_save_dir /path/to/folder&lt;/code&gt;: Specify the folder to save visualizations to.&lt;br&gt; &lt;code&gt;--test_max_size 500&lt;/code&gt;: Run evaluation on a randomly sampled subset of the data. Useful for quick verifications and checks.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning and Other Model Training Scenarios&lt;/h2&gt; &#xA;&lt;p&gt;If model training is interrupted, it can be easily resumed by using the flag &lt;code&gt;--model_load_path /path/to/model.pth&lt;/code&gt; and specifying the path to the saved dictionary file that contains the saved optimizer state.&lt;/p&gt; &#xA;&lt;p&gt;If you want to restart training by fine-tuning a saved checkpoint, such as &lt;code&gt;model_20.pth&lt;/code&gt;, use the flag &lt;code&gt;--model_load_path /path/to/model_20.pth&lt;/code&gt; and the flag &lt;code&gt;--load_weights_only&lt;/code&gt; to indicate that the previous optimizer state is not needed for resuming training.&lt;/p&gt; &#xA;&lt;p&gt;Whether fine-tuning or training a new model from scratch, you can optionally create a new config file with different training parameters than the default ones we used. Specify the new config file using: &lt;code&gt;--config_file /path/to/new_structure_config.json&lt;/code&gt;. Creating a new config file is useful, for example, if you want to use a different learning rate &lt;code&gt;lr&lt;/code&gt; during fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, many of the arguments in the config file can be specified as command line arguments using their associated flags. Any argument specified as a command line argument overrides the value of the argument in the config file.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;Our work can be cited using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{smock2021tabletransformer,&#xA;  author = {Smock, Brandon and Pesala, Rohith},&#xA;  month = {06},&#xA;  title = {{Table Transformer}},&#xA;  url = {https://github.com/microsoft/table-transformer},&#xA;  version = {1.0.0},&#xA;  year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{smock2022pubtables,&#xA;  title={Pub{T}ables-1{M}: Towards comprehensive table extraction from unstructured documents},&#xA;  author={Smock, Brandon and Pesala, Rohith and Abraham, Robin},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  pages={4634-4642},&#xA;  year={2022},&#xA;  month={June}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{smock2023grits,&#xA;  title={Gri{TS}: Grid table similarity metric for table structure recognition},&#xA;  author={Smock, Brandon and Pesala, Rohith and Abraham, Robin},&#xA;  booktitle={International Conference on Document Analysis and Recognition},&#xA;  pages={535--549},&#xA;  year={2023},&#xA;  organization={Springer}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{smock2023aligning,&#xA;  title={Aligning benchmark datasets for table structure recognition},&#xA;  author={Smock, Brandon and Pesala, Rohith and Abraham, Robin},&#xA;  booktitle={International Conference on Document Analysis and Recognition},&#xA;  pages={371--386},&#xA;  year={2023},&#xA;  organization={Springer}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>