<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-13T01:34:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aniketrajnish/MultiWindowSync-PyQt</title>
    <updated>2024-02-13T01:34:51Z</updated>
    <id>tag:github.com,2024-02-13:/aniketrajnish/MultiWindowSync-PyQt</id>
    <link href="https://github.com/aniketrajnish/MultiWindowSync-PyQt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Windows GUI multiple window synchronization using pyqtSignal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MultiWindowSync-PyQt&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows GUI application developed using &lt;code&gt;PyQt5&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It demonstrates the synchronization of multiple windows using &lt;code&gt;pyqtSignal&lt;/code&gt;. &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inspired by the &lt;a href=&#34;https://twitter.com/_nonfigurativ_/status/1727322594570027343&#34;&gt;work&lt;/a&gt; of Bjørn Staal.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aniketrajnish/MultiWindowSync-PyQt/assets/58925008/e00e10e9-6373-46d8-9669-77466ee8bd90&#34;&gt;https://github.com/aniketrajnish/MultiWindowSync-PyQt/assets/58925008/e00e10e9-6373-46d8-9669-77466ee8bd90&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repository &lt;pre&gt;&lt;code&gt;git clone https://github.com/aniketrajnish/MultiWindowSync-PyQt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Open Terminal and change directory to the script&#39;s folder. &lt;pre&gt;&lt;code&gt;cd &amp;lt;path-to-repo&amp;gt;\src&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install Dependencies &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the main script &lt;pre&gt;&lt;code&gt;python MultiWindowTest.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use your own image/GIF. &lt;pre&gt;&lt;code&gt;File -&amp;gt; Open Image/GIF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;In case you don&#39;t wanna go through all of this hassle, I&#39;ve added an executable file in the &lt;a href=&#34;https://github.com/aniketrajnish/MultiWindowSync-PyQt/releases/tag/v001&#34;&gt;Releases Section&lt;/a&gt; that you can directly try on your machine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to the project are welcome. Currently working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expanding the environment to 3D using PyOpenGL.&lt;/li&gt; &#xA; &lt;li&gt;Fix the bug where the parent image window always moves the image along with it for ref to other windows even if &lt;code&gt;Move With Window&lt;/code&gt; is unchecked.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>uclaml/SPIN</title>
    <updated>2024-02-13T01:34:51Z</updated>
    <id>tag:github.com,2024-02-13:/uclaml/SPIN</id>
    <link href="https://github.com/uclaml/SPIN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official implementation of Self-Play Fine-Tuning (SPIN)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/images/spin_dalle.png&#34; width=&#34;30%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/collections/UCLA-AGI/zephyr-7b-sft-full-spin-65c361dfca65637272a02c40&#34; target=&#34;_blank&#34;&gt;Models&lt;/a&gt; | 🤗 &lt;a href=&#34;https://huggingface.co/collections/UCLA-AGI/datasets-spin-65c3624e98d4b589bbc76f3a&#34; target=&#34;_blank&#34;&gt;Datasets&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Self-Play Fine-Tuning (SPIN)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Mistral--7B--v0.1-green&#34; alt=&#34;Mistral-7B&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Task-Open_LLM_Leaderboard-red&#34; alt=&#34;Open LLM&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Task-MT--Bench-red&#34; alt=&#34;MT-Bench&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the official code for the paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2401.01335&#34;&gt;Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Authors: &lt;a href=&#34;https://sites.google.com/view/zxchen&#34;&gt;Zixiang Chen&lt;/a&gt;*, &lt;a href=&#34;https://sites.google.com/g.ucla.edu/yihedeng/&#34;&gt;Yihe Deng&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?user=8foZzX4AAAAJ&#34;&gt;Huizhuo Yuan&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?user=FOoKDukAAAAJ&#34;&gt;Kaixuan Ji&lt;/a&gt;, &lt;a href=&#34;https://web.cs.ucla.edu/~qgu/&#34;&gt;Quanquan Gu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://uclaml.github.io/SPIN/&#34;&gt;Webpage&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/papers/2401.01335&#34;&gt;Huggingface&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;🔔 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[01/02/2024]&lt;/strong&gt; Our paper is released on arXiv: &lt;a href=&#34;https://arxiv.org/abs/2401.01335&#34;&gt;https://arxiv.org/abs/2401.01335&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#%F0%9F%8C%80-about-spin&#34;&gt;About SPIN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#step-1-generation&#34;&gt;Step 1: Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#step-15-gather-generations-and-convert-data-type&#34;&gt;Step 1.5: Gather generations and convert data type&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#step-2-fine-tuning&#34;&gt;Step 2: Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#Acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌀 About SPIN&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIN&lt;/strong&gt; utilizes a self-play mechanism, allowing an LLM to improve itself by playing against its previous iterations, without needing additional human-annotated preference data than the SFT dataset itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from the original SFT data.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/images/iter_openllm.png&#34; width=&#34;35%&#34;&gt; &lt;br&gt; Average score of &lt;b&gt;SPIN&lt;/b&gt; at different iterations on the HuggingFace Open LLM leaderboard. &lt;/p&gt; SPIN can significantly enhance the performance of an LLM after SFT across various benchmarks, outperforming the model trained with direct preference optimization (DPO) on labelled preference datasets. The approach is theoretically grounded, ensuring that the LLM aligns with the target data distribution, and empirically validated through extensive evaluations on multiple datasets. &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/images/dpo_compare.png&#34; width=&#34;80%&#34;&gt; &lt;br&gt; Performance comparison with DPO training across the six benchmark datasets. SPIN at iteration 0 achieves comparable performance to DPO training with 62k new data. At iteration 1, SPIN has already surpassed DPO training on the majority of datasets. &lt;/p&gt; &#xA;&lt;p&gt;For more details, you can check our paper &lt;a href=&#34;https://arxiv.org/abs/2401.01335&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;The following steps provide the necessary setup to run our codes.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a Python virtual environment with Conda:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n myenv python=3.10&#xA;conda activate myenv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install PyTorch &lt;code&gt;v2.1.0&lt;/code&gt; with compatible cuda version, following instructions from &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch Installation Page&lt;/a&gt;. For example with cuda 11:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the following Python dependencies to run the codes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install .&#xA;python -m pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Login to your huggingface account for downloading models&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli login --token &#34;${your_access_token}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;We provide the data used in our experiments along with the synthetic data we generated in this repo as well as on HuggingFace. These data is converted to .parquet format for fine-tuning.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPIN_iter0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/datasets/UCLA-AGI/SPIN_iter0&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPIN_iter1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/datasets/UCLA-AGI/SPIN_iter1&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPIN_iter2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/datasets/UCLA-AGI/SPIN_iter2&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPIN_iter3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/datasets/UCLA-AGI/SPIN_iter3&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The input data for our code is required to be of the same format where each data contains the following attributes, as similar to &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized&#34;&gt;HuggingFaceH4/ultrafeedback_binarized&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;real&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &amp;lt;prompt&amp;gt;}, &#xA;               {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &amp;lt;ground truth&amp;gt;}],&#xA;    &#34;generated&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &amp;lt;prompt&amp;gt;}, &#xA;                 {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &amp;lt;generation&amp;gt;}]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;🔍 Note: During data generation, the content for generated response can be empty, as we only uses prompt to generate model responses.&lt;/p&gt; &#xA;&lt;h3&gt;Model&lt;/h3&gt; &#xA;&lt;p&gt;We also provide our model checkpoints at iteration 0,1,2,3 on HuggingFace.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;zephyr-7b-sft-full-SPIN-iter0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;zephyr-7b-sft-full-SPIN-iter1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter1&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;zephyr-7b-sft-full-SPIN-iter2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter2&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;zephyr-7b-sft-full-SPIN-iter3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🤗 &lt;a href=&#34;https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter3&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;🔍 &lt;strong&gt;Note&lt;/strong&gt;: With the provided data, you can directly jump to &lt;a href=&#34;https://raw.githubusercontent.com/uclaml/SPIN/main/#step-2-fine-tuning&#34;&gt;Step 2: Fine-tuning&lt;/a&gt; without doing data generation on your own. You may also start from any iteration to reproduce our results using our open-sourced model checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;For SPIN, we generate all synthetic data at once for an iteration, and fine-tune the LLM based on the real and synthetic data pairs.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0 (optional): Reformatting SFT dataset&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python spin/reformat.py [options]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data&lt;/code&gt;: directory to the SFT dataset (local or huggingface) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;HuggingFaceH4/ultrachat_200k&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--output_dir&lt;/code&gt;: local directory to the reformated data files &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;UCLA-AGI/SPIN_iter0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;🔍 Note: If choosing to use SPIN on the entire dataset of &lt;code&gt;HuggingFaceH4/ultrachat_200k&lt;/code&gt; instead of our 50k subset, one can reformat the original data with &lt;code&gt;spin/reformat.py&lt;/code&gt;. To use other datasets, simply convert the data into the same format and resume with the following steps.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Generation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch spin/generate.py [options]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: load model checkpoint for generation. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;alignment-handbook/zephyr-7b-sft-full&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--input_dir&lt;/code&gt;: directory to the data files with prompts for generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The code is for generation based on data of the format given below.&lt;/li&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;UCLA-AGI/SPIN_iter0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--output_dir&lt;/code&gt;: directory to save the output data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: per device batch size &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: 16&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_frac&lt;/code&gt;: break data into fractions for generations across server. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--frac_len&lt;/code&gt;: length of the data fraction. Default is 0 which uses the entire dataset for generation. Set &lt;code&gt;frac_len&lt;/code&gt; to a positive number to generate only for a fraction of data. &lt;strong&gt;Note&lt;/strong&gt;: we recommend using a smaller frac_len (e.g. 800) to generate data by small batches to avoid unexpected crashes as data generation can be very time-consuming.&lt;/li&gt; &#xA;   &lt;li&gt;Setting &lt;code&gt;data_frac&lt;/code&gt; to be 0, 1, 2... to generate for different fractions of length &lt;code&gt;frac_len&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Note: maintain the same frac length when doing generation using data_frac. It&#39;s recommended to set a smaller &lt;code&gt;frac_len&lt;/code&gt; to 800.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--split&lt;/code&gt;: choose the split for data generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;train&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The generated data is in json format where each data contains the following attributes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;real&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &amp;lt;prompt&amp;gt;}, &#xA;               {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &amp;lt;ground truth&amp;gt;}],&#xA;    &#34;generated&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &amp;lt;prompt&amp;gt;}, &#xA;                 {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &amp;lt;generation&amp;gt;}]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example&lt;/h4&gt; &#xA;&lt;p&gt;The following code generates 8k synthetic data for iteration 1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/generate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1.5: Gather generations and convert data type&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python spin/convert_data.py [options]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_fracs&lt;/code&gt;: number of files to load in.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--input_dir&lt;/code&gt;: directory to the generated data files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--output_dir&lt;/code&gt;: directory for the unified data that will be used for fine-tuning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code will generate the two final data files including &lt;code&gt;train_prefs-00000-of-00001.parquet&lt;/code&gt; and &lt;code&gt;test_prefs-00000-of-00001.parquet&lt;/code&gt;, which will be used for fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;Note: make sure to collect the generated data filed into the same directory of &lt;code&gt;--input_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python spin/convert_data.py --output_dir new_data/iter0 --input_dir generated/iter0 --num_fracs 63&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Fine-tuning&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --config_file configs/multi_gpu.yaml --num_processes=8 --main_process_port 29500 spin/run_spin.py configs/config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- **[TODO]**: wrap up necessary codes into the folder spin. Add explainations/instructions here.  --&gt; &#xA;&lt;p&gt;You might need to change the configuration in &lt;code&gt;configs/config.yaml&lt;/code&gt;. Here are some key configs you might need to customize:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name_or_path&lt;/code&gt;: load model checkpoint for finetuning. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;alignment-handbook/zephyr-7b-sft-full&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset_mixer&lt;/code&gt;: choose data to mix for fine-tuning. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;UCLA-AGI/SPIN_iter0: 1.0&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For SPIN at iteration 1 and 2, we included both the current iteration and the previous iteration (e.g. for iteration 1 we included both &lt;code&gt;UCLA-AGI/SPIN_iter0: 1.0&lt;/code&gt; and &lt;code&gt;UCLA-AGI/SPIN_iter1: 1.0&lt;/code&gt;, summing to 100k data.)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_dir&lt;/code&gt;: the output directory of finetuned model and checkpoints. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: &lt;code&gt;outputs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;per_device_train_batch_size&lt;/code&gt;: batch size on one GPU. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: 16&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gradient_accumulation_steps&lt;/code&gt;: make sure that the product per_device_train_batch_size*num_processes*gradient_accumulation_steps equals to your true batch size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_train_epochs&lt;/code&gt;: the training epochs of this iteration. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: 3&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: beta in SPIN. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;default: 0.1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In our experiments, we do full fine-tuning on a multi-GPU machine with DeepSpeed ZeRO-3 (requires A100 (80GB)).&lt;/p&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/finetune.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful for your research, please consider citing the paper&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{chen2024selfplay,&#xA;      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, &#xA;      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},&#xA;      year={2024},&#xA;      eprint={2401.01335},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo is built upon &lt;a href=&#34;https://github.com/huggingface/alignment-handbook&#34;&gt;The Alignment Handbook&lt;/a&gt;. We thank the authors for their great work.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LAION-AI/natural_voice_assistant</title>
    <updated>2024-02-13T01:34:51Z</updated>
    <id>tag:github.com,2024-02-13:/LAION-AI/natural_voice_assistant</id>
    <link href="https://github.com/LAION-AI/natural_voice_assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BUD-E: A conversational and empathic AI Voice Assistant&lt;/h1&gt; &#xA;&lt;p&gt;BUD-E (Buddy for Understanding and Digital Empathy) is an open-source AI voice assistant which aims for the following goals:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;replies to user requests in real-time&lt;/li&gt; &#xA; &lt;li&gt;uses natural voices, empathy &amp;amp; emotional intelligence&lt;/li&gt; &#xA; &lt;li&gt;works with long-term context of previous conversations&lt;/li&gt; &#xA; &lt;li&gt;handles multi-speaker conversations with interruptions, affirmations and thinking pauses&lt;/li&gt; &#xA; &lt;li&gt;runs fully local, on consumer hardware.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This project is a collaboration between LAION, the ELLIS Institute Tübingen, Collabora and the Tübingen AI Center.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; , style=&#34;margin-top:30px;&#34;&gt; &lt;a href=&#34;https://laion.ai/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LAION-AI/natural_voice_assistant/main/icons/laion.png&#34; alt=&#34;Image 1&#34; width=&#34;110&#34; style=&#34;padding:20;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://institute-tue.ellis.eu/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LAION-AI/natural_voice_assistant/main/icons/ellis.jpg&#34; alt=&#34;Image 2&#34; width=&#34;250&#34; style=&#34;padding:20px;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.collabora.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LAION-AI/natural_voice_assistant/main/icons/collabora.png&#34; alt=&#34;Image 3&#34; width=&#34;200&#34; style=&#34;paddin:20px; margin-bottom:15px;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tuebingen.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LAION-AI/natural_voice_assistant/main/icons/Tuebingen_AI_Center.png&#34; alt=&#34;Image 4&#34; width=&#34;130&#34; style=&#34;padding:20px; margin-left:15px;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/SYWDucn8RL8&#34;&gt;This demo&lt;/a&gt; shows an interaction with the current version of BUD-E on an NVIDIA RTX 4090. With this setup, the voice assistant answers with a latency of &lt;em&gt;300 to 500 milliseconds&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and follow the installation guide in the readme.&lt;/li&gt; &#xA; &lt;li&gt;Start the voice assistant by running the &lt;em&gt;main.py&lt;/em&gt; file in the repository root.&lt;/li&gt; &#xA; &lt;li&gt;Wait until &#34;Listening..&#34; is printed to the console and start speaking.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Altough the conversations with the current version of BUD-E already feel quite natural, there are still a lot of components and features missing what we need to tackle on the way to a truly and naturally feeling voice assistant. The immediate open work packages we&#39;d like to tackle are as follows:&lt;/p&gt; &#xA;&lt;h3&gt;Reducing Latency &amp;amp; minimizing systems requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Quantization&lt;/em&gt;. Implement more sophisticated quantization techniques to reduce VRAM requirements and reduce latency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Fine-tuning streaming TTS&lt;/em&gt;. TTS systems normally consume full sentences to have enough context for responses. To enable high-quality low-latency streaming we give the TTS context from hidden layers of the LLM and then fine-tune the streaming model on a high-quality teacher (following &lt;a href=&#34;https://arxiv.org/abs/2309.11210&#34;&gt;https://arxiv.org/abs/2309.11210&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Fine-tuning streaming STT&lt;/em&gt;. Connect hidden layers from STT and LLM system and then fine-tune on voice tasks to maximize accuracy in low-latency configurations of STT model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;End-of-Speech detection&lt;/em&gt;. Train and implement a light-weight end-of-speech detection model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Implement Speculative Decoding&lt;/em&gt;. Implement speculative decoding to increase inference speed in particular for the STT and LLM models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Increasing Naturalness of Speech and Responses&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Dataset of natural human dialogues&lt;/em&gt;. Build a dataset (e.g., Youtube, Mediathek, etc.) with recorded dialogues between two or more humans for fine-tuning BUD-E.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Reliable speaker-diarization&lt;/em&gt;. Develop a reliable speaker-diarization system that can separate speakers, including utterances and affirmations that might overlap between speakers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Fine-tune on dialogues&lt;/em&gt;. Finetune STT -&amp;gt; LLM -&amp;gt; TTS pipeline on natural human dialogues to allow the model to respond similarly to humans, including interruptions and utterances.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Keeping track of conversations over days, months and years&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Retrieval Augmented Generation (RAG)&lt;/em&gt;. Implement RAG to extend knowledge of BUD-E, unlocking strong performance gains (cp. &lt;a href=&#34;https://www.pinecone.io/blog/rag-study/&#34;&gt;https://www.pinecone.io/blog/rag-study/&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Conversation Memory&lt;/em&gt;. Enable model to save information from previous conversations in vector database to keep track of previous conversations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enhancing functionality and ability of voice assistant&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Tool use&lt;/em&gt;. Implement tool use into LLM and the framework, e.g., to allow the agent to perform internet searches&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enhancing multi-modal and emotional context understanding&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Incorporate visual input&lt;/em&gt;. Use a light-weight but effective vision encoder (e.g., CLIP or a Captioning Model) to incorporate static image and/or video input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Continuous vision-audio responses&lt;/em&gt;. Similar to the (not genuine) &lt;a href=&#34;https://www.youtube.com/watch?v=UIZAiXYceBI&#34;&gt;Gemini demo&lt;/a&gt; it would be great if BUD-E would naturally and continuously take into account audio and vision inputs and flexibly respond in a natural manner just like humans.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Evaluate user emotions&lt;/em&gt;. Capture webcam images from the user to determine the user’s emotional state and incorporate this in the response. This could be an extension of training on dialogues from video platforms, using training samples where the speaker’s face is well visible.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Building a UI, CI and easy packaging infrastructure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;LLamaFile&lt;/em&gt;. Allow easy cross-platform installation and deployment through a single-file distribution mechanism like Mozilla’s LLamaFile.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Animated Avatar&lt;/em&gt;. Add a speaking and naturally articulating avatar similar to Meta’s Audio2Photoreal but using simpler avatars using 3DGS-Avatar [https://neuralbodies.github.io/3DGS-Avatar/].&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;User Interface&lt;/em&gt;. Capture the conversation in writing in a chat-based interface and ideally include ways to capture user feedback.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Minimize Dependencies&lt;/em&gt;. Minimize the amount of third-party dependencies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Cross-Platform Support&lt;/em&gt;. Enable usage on Linux, MacOS and Windows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Continuous Integration&lt;/em&gt;. Build continuous integration pipeline with cross-platform speed tests and standardized testing scenarios to track development progress.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extending to multi-language and multi-speaker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Extend streaming STT to more languages&lt;/em&gt;. Extending to more languages, including low-resource ones, would be crucial.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;em&gt;Multi-speaker&lt;/em&gt;. The baseline currently expects only a single speaker, which should be extended towards multi-speaker environments and consistent re-identification of speakers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The current version of BUD-E contains the following pretrained models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech to Text Model: &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_fastconformer_hybrid_large_streaming_80ms&#34;&gt;FastConformer Streaming STT by NVIDIA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Language Model: &lt;a href=&#34;https://huggingface.co/microsoft/phi-2&#34;&gt;Microsoft Phi-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text to Speech Model: &lt;a href=&#34;https://github.com/yl4579/StyleTTS2&#34;&gt;StyleTTS2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The model weights are downloaded and cached automatically when running the inference script for the first time.&lt;/p&gt; &#xA;&lt;p&gt;To install BUD-E on your system follow these steps:&lt;/p&gt; &#xA;&lt;h3&gt;1) Setup Environment and Clone the Repo&lt;/h3&gt; &#xA;&lt;p&gt;We recommend to create a fresh conda environment with python 3.10.12.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name bud_e python==3.10.12&#xA;conda activate bud_e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, clone this repository. Make sure to pass the &lt;em&gt;-recurse-submodules&lt;/em&gt; argument to clone the required submodules as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/brendel-group/natural_voice_assistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2) Install espeak-ng&lt;/h3&gt; &#xA;&lt;h4&gt;Ubuntu:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install festival espeak-ng mbrola &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and run the latest espeak-ng msi installer via &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng/releases&#34;&gt;https://github.com/espeak-ng/espeak-ng/releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the path to the libespeak-ng.dll file to your conda environment:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env config vars set PHONEMIZER_ESPEAK_LIBRARY=&#34;C:\Program Files\eSpeak NG\libespeak-ng.dll&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reactivate your conda environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3) Install pytorch&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;em&gt;torch&lt;/em&gt; and &lt;em&gt;torchaudio&lt;/em&gt; using the configurator on &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4) Install Required Python Packages&lt;/h3&gt; &#xA;&lt;p&gt;Inside the repository run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Ubuntu, you might install portaudio which is required by pyaudio. If you encounter any errors with pyaudio, try to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install portaudio19-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5) Start your AI conversation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start BUD-E by running the &lt;em&gt;main.py&lt;/em&gt; file inside the repository:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wait until all checkpoints are downloaded and all models are initialized. When &lt;em&gt;&#34;## Listening...&#34;&lt;/em&gt; is prompted to the console, you can start speaking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Collaborating to Build the Future of Conversational AI&lt;/h2&gt; &#xA;&lt;p&gt;The development of BUD-E is an ongoing process that requires the collective effort of a diverse community. We invite open-source developers, researchers, and enthusiasts to join us in refining BUD-E&#39;s individual modules and contributing to its growth. Together, we can create an AI voice assistants that engage with us in natural, intuitive, and empathetic conversations.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in contributing to this project, join our &lt;a href=&#34;https://discord.com/invite/jJpvt6R8cp&#34;&gt;Discord community&lt;/a&gt; or reach out to us at &lt;a href=&#34;mailto:bud-e@laion.ai&#34;&gt;bud-e@laion.ai&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>