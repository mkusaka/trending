<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-21T01:41:55Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dabeaz-course/python-mastery</title>
    <updated>2023-07-21T01:41:55Z</updated>
    <id>tag:github.com,2023-07-21:/dabeaz-course/python-mastery</id>
    <link href="https://github.com/dabeaz-course/python-mastery" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Advanced Python Mastery (course by @dabeaz)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Advanced Python Mastery&lt;/h1&gt; &#xA;&lt;p&gt;A course by David Beazley (&lt;a href=&#34;https://www.dabeaz.com&#34;&gt;https://www.dabeaz.com&lt;/a&gt;)&lt;br&gt; Copyright (C) 2007-2023&lt;/p&gt; &#xA;&lt;h2&gt;Synopsis&lt;/h2&gt; &#xA;&lt;p&gt;An exercise-driven course on Advanced Python Programming that was battle-tested several hundred times on the corporate-training circuit for more than a decade. Written by David Beazley, author of the Python Cookbook, 3rd Edition (O&#39;Reilly) and Python Distilled (Addison-Wesley). Released under a Creative Commons license. Free of ads, tracking, pop-ups, newsletters, and AI.&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;This course is for Python programmers who want to move beyond short scripts to writing more sophisticated programs. Topics focus on programming techniques that get used in popular libraries and frameworks. The primary goal is to better understand the Python language itself so that you can understand other people&#39;s code and so that you can apply your newfound knowledge to your own projects.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;You already know some Python. This is not a course for beginners. For more introductory material, you might consider the &lt;a href=&#34;https://dabeaz-course.github.io/practical-python&#34;&gt;Practical Python Programming&lt;/a&gt; course.&lt;/p&gt; &#xA;&lt;h2&gt;How to Take the Course&lt;/h2&gt; &#xA;&lt;p&gt;To take the course, you should first fork/clone the GitHub repo to your own machine.&lt;/p&gt; &#xA;&lt;p&gt;It is assumed that you are working locally in a proper Python development environment. That means a proper installation of Python, an editor/IDE, and whatever other tools that you would normally install to work on Python. Due to the use of multiple files and module imports, the use of Notebooks is not recommended.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/PythonMastery.pdf&#34;&gt;&lt;code&gt;PythonMastery.pdf&lt;/code&gt;&lt;/a&gt; file contains detailed presentation slides. Course exercises and suggested timings are clearly indicated. You&#39;ll want to keep this by your side (I recommend downloading and viewing it with a local PDF viewer). Start here!&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Exercises/index.md&#34;&gt;Exercises/&lt;/a&gt; directory has all of the course exercises.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Solutions/&#34;&gt;Solutions/&lt;/a&gt; directory has fully worked out solution code.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Data/&#34;&gt;Data/&lt;/a&gt; directory has some datafiles used during the course.&lt;/p&gt; &#xA;&lt;p&gt;The course was originally taught over 4-5 days in an in-person classroom setting with a mix of lecture and hands-on exercises. Successful completion of the course will likely require 30-50 hours of work. Exercises tend to build upon each other. Solutions are always provided in case you get stuck.&lt;/p&gt; &#xA;&lt;h2&gt;Supplemental Material&lt;/h2&gt; &#xA;&lt;p&gt;The Advanced Python Mastery course often suggested more in-depth tutorials on selected topics. These were presented at the PyCon conference and might be of interest:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dabeaz.com/generators/&#34;&gt;Generator Tricks for Systems Programmers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dabeaz.com/coroutines/index.html&#34;&gt;A Curious Course on Coroutines and Concurrency&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/py3meta/index.html&#34;&gt;Python3 Metaprogramming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/finalgenerator/index.html&#34;&gt;Generators: The Final Frontier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/modulepackage/index.html&#34;&gt;Modules and Packages: Live and Let Die&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions and Answers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Are any videos available?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. You will be able to more quickly read the presentation slides which contain technical information. However, the &lt;a href=&#34;https://www.safaribooksonline.com/library/view/python-programming-language/9780134217314/&#34;&gt;Python Programming Language: LiveLessons&lt;/a&gt; video available on O&#39;Reilly&#39;s Safari site is closely related to the material in this course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Can I use these materials in my own course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. I just kindly ask that you give proper attribution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Do you accept bug reports or pull requests?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; If you&#39;ve found a bug, please report it! However, I&#39;m not looking to expand or reorganize the course content with new topics or exercises.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Are the presentation slides available in any format other than PDF?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Is there any forum/chat where the course can be discussed?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You can use &lt;a href=&#34;https://github.com/dabeaz-course/python-mastery/discussions&#34;&gt;GitHub discussions&lt;/a&gt; to discuss the course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why wasn&#39;t topic/tool/library X covered?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The course was designed to be completed in an intense 4-day in-person format. It simply isn&#39;t possible to cover absolutely everything. As such, the course is focused primarily on the core Python language, not third party libraries or tooling.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why aren&#39;t features like typing, async, or pattern matching covered?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Mainly, it&#39;s an issue of calendar timing and scope. Course material was primarily developed pre-pandemic and represents Python as it was at that time. Some topics (e.g., typing or async) are sufficiently complex that they would be bettered covered on their own in a separate course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: How can I help?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; If you like the course, the best way to support it is to tell other people about it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; Advanced Python Mastery&lt;br&gt; &lt;code&gt;...&lt;/code&gt; A course by &lt;a href=&#34;https://www.dabeaz.com&#34;&gt;dabeaz&lt;/a&gt;&lt;br&gt; &lt;code&gt;...&lt;/code&gt; Copyright 2007-2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.creativecommons.org/l/by-sa/4.0/88x31.png&#34; alt=&#34;&#34;&gt;. This work is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/llama-recipes</title>
    <updated>2023-07-21T01:41:55Z</updated>
    <id>tag:github.com,2023-07-21:/facebookresearch/llama-recipes</id>
    <link href="https://github.com/facebookresearch/llama-recipes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples and recipes for Llama 2 model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama 2 Fine-tuning / Inference Recipes and Examples&lt;/h1&gt; &#xA;&lt;p&gt;The &#39;llama-recipes&#39; repository is a companion to the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 model&lt;/a&gt;. The goal of this repository is to provide examples to quickly get started with fine-tuning for domain adaptation and how to run inference for the fine-tuned models. For ease of use, the examples use Hugging Face converted versions of the models. See steps for conversion of the model &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research paper as well. For downloading the models, follow the instructions on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#quick-start&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#single-gpu&#34;&gt;Single GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multiple-gpus-one-node&#34;&gt;Multi GPU One Node&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multi-gpu-multi-node&#34;&gt;Multi GPU Multi Node&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;Model Conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#repository-organization&#34;&gt;Repository Organization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#license&#34;&gt;License and Acceptable Use Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/quickstart.ipynb&#34;&gt;Llama 2 Jupyter Notebook&lt;/a&gt;: This jupyter notebook steps you through how to finetune a Llama 2 model on the text summarization task using the &lt;a href=&#34;https://huggingface.co/datasets/samsum&#34;&gt;samsum&lt;/a&gt;. The notebook uses parameter efficient finetuning (PEFT) and int8 quantization to finetune a 7B on a single GPU like an A10 with 24GB gpu memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; All the setting defined in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/&#34;&gt;config files&lt;/a&gt; can be passed as args through CLI when running the script, there is no need to change from config files directly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; In case need to run PEFT model with FSDP, please make sure to use the PyTorch Nightlies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more in depth information checkout the following:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md&#34;&gt;Single GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/mutli_gpu.md&#34;&gt;Multi-GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md&#34;&gt;Adding custom datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To run the examples, make sure to install the requirements using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;pip install -r requirements.txt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the above requirements.txt will install PyTorch 2.0.1 version, in case you want to run FSDP + PEFT, please make sure to install PyTorch nightlies.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;For fine-tuning Llama 2 models for your domain-specific use cases recipes for PEFT, FSDP, PEFT+FSDP have been included along with a few test datasets. For details see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Single and Multi GPU Finetune&lt;/h2&gt; &#xA;&lt;p&gt;If you want to dive right into single or multi GPU fine-tuning, run the examples below on a single GPU like A10, T4, V100, A100 etc. All the parameters in the examples and recipes below need to be further tuned to have desired results based on the model, method, data and task at hand.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To change the dataset in the commands below pass the &lt;code&gt;dataset&lt;/code&gt; arg. Current options for dataset are &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt;and &lt;code&gt;samsum_dataset&lt;/code&gt;. A description of the datasets and how to add custom datasets can be found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md&#34;&gt;Dataset.md&lt;/a&gt;. For &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt; please make sure you use the suggested instructions from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md#how-to-run-with-different-datasets&#34;&gt;here&lt;/a&gt; to set them up.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Default dataset and other LORA config has been set to &lt;code&gt;samsum_dataset&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure to set the right path to the model in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;training config&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Single GPU:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#if running on multi-gpu machine&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;&#xA;python llama_finetuning.py  --use_peft --peft_method lora --quantization --model_name /patht_of_model_folder/7B --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we make use of Parameter Efficient Methods (PEFT) as described in the next section. To run the command above make sure to pass the &lt;code&gt;peft_method&lt;/code&gt; arg which can be set to &lt;code&gt;lora&lt;/code&gt;, &lt;code&gt;llama_adapter&lt;/code&gt; or &lt;code&gt;prefix&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; if you are running on a machine with multiple GPUs please make sure to only make one of them visible using &lt;code&gt;export CUDA_VISIBLE_DEVICES=GPU:id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you set &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;save_model&lt;/a&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;training.py&lt;/a&gt; to save the model. Be sure to check the other training settings in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;train config&lt;/a&gt; as well as others in the config folder as needed or they can be passed as args to the training script as well.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple GPUs One Node:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; please make sure to use PyTorch Nightlies for using PEFT+FSDP. Also, note that int8 quantization from bit&amp;amp;bytes currently is not supported in FSDP.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 4  llama_finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name /patht_of_model_folder/7B --pure_bf16 --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we use FSDP as discussed in the next section which can be used along with PEFT methods. To make use of PEFT methods with FSDP make sure to pass &lt;code&gt;use_peft&lt;/code&gt; and &lt;code&gt;peft_method&lt;/code&gt; args along with &lt;code&gt;enable_fsdp&lt;/code&gt;. Here we are using &lt;code&gt;BF16&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning using FSDP Only&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in running full parameter fine-tuning without making use of PEFT methods, please use the following command. Make sure to change the &lt;code&gt;nproc_per_node&lt;/code&gt; to your available GPUs. This has been tested with &lt;code&gt;BF16&lt;/code&gt; on 8xA100, 40GB GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 8  llama_finetuning.py --enable_fsdp --model_name /patht_of_model_folder/7B --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multi GPU Multi Node:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;sbatch multi_node.slurm&#xA;# Change the num nodes and GPU per nodes in the script before running.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about our fine-tuning strategies &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Model conversion to Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;The recipes and notebooks in this folder are using the Llama 2 model definition provided by Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Given that the original checkpoint resides under models/7B you can install all requirements and convert the checkpoint with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Install HuggingFace Transformers from source&#xA;pip install git+https://github.com/huggingface/transformers&#xA;cd transformers&#xA;&#xA;python src/transformers/models/llama/convert_llama_weights_to_hf.py \&#xA;    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir models_hf/7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Repository Organization&lt;/h1&gt; &#xA;&lt;p&gt;This repository is organized in the following way:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/&#34;&gt;configs&lt;/a&gt;: Contains the configuration files for PEFT methods, FSDP, Datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/&#34;&gt;docs&lt;/a&gt;: Example recipes for single and multi-gpu fine-tuning recipes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/ft_datasets/&#34;&gt;ft_datasets&lt;/a&gt;: Contains individual scripts for each dataset to download and process. Note: Use of any of the datasets should be in compliance with the dataset&#39;s underlying licenses (including but not limited to non-commercial uses)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/inference/&#34;&gt;inference&lt;/a&gt;: Includes examples for inference for the fine-tuned models and how to use them safely.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/model_checkpointing/&#34;&gt;model_checkpointing&lt;/a&gt;: Contains FSDP checkpoint handlers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/policies/&#34;&gt;policies&lt;/a&gt;: Contains FSDP scripts to provide different policies, such as mixed precision, transformer wrapping policy and activation checkpointing along with any precision optimizer (used for running FSDP with pure bf16 mode).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/utils/&#34;&gt;utils&lt;/a&gt;: Utility files for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;train_utils.py&lt;/code&gt; provides training/eval loop and more train utils.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;dataset_utils.py&lt;/code&gt; to get preprocessed datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;config_utils.py&lt;/code&gt; to override the configs received from CLI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;fsdp_utils.py&lt;/code&gt; provides FSDP wrapping policy for PEFT methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;memory_utils.py&lt;/code&gt; context manager to track different memory stats in train loop.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;See the License file &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scaleapi/llm-engine</title>
    <updated>2023-07-21T01:41:55Z</updated>
    <id>tag:github.com,2023-07-21:/scaleapi/llm-engine</id>
    <link href="https://github.com/scaleapi/llm-engine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scale LLM Engine public repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;⚡ LLM Engine ⚡&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;The open source engine for fine-tuning and serving large language models&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Scale&#39;s LLM Engine is the easiest way to customize and serve LLMs. In LLM Engine, models can be accessed via Scale&#39;s hosted version or by using the Helm charts in this repository to run model inference and fine-tuning in your own infrastructure.&lt;/p&gt; &#xA;&lt;h2&gt;💻 Quick Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install scale-llm-engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤔 About&lt;/h2&gt; &#xA;&lt;p&gt;Foundation models are emerging as the building blocks of AI. However, deploying these models to the cloud and fine-tuning them are expensive operations that require infrastructure and ML expertise. It is also difficult to maintain over time as new models are released and new techniques for both inference and fine-tuning are made available.&lt;/p&gt; &#xA;&lt;p&gt;LLM Engine is a Python library, CLI, and Helm chart that provides everything you need to serve and fine-tune foundation models, whether you use Scale&#39;s hosted infrastructure or do it in your own cloud infrastructure using Kubernetes.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;p&gt;🎁 &lt;strong&gt;Ready-to-use APIs for your favorite models&lt;/strong&gt;: Deploy and serve open-source foundation models — including LLaMA, MPT and Falcon. Use Scale-hosted models or deploy to your own infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;🔧 &lt;strong&gt;Fine-tune foundation models&lt;/strong&gt;: Fine-tune open-source foundation models on your own data for optimized performance.&lt;/p&gt; &#xA;&lt;p&gt;🎙️ &lt;strong&gt;Optimized Inference&lt;/strong&gt;: LLM Engine provides inference APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.&lt;/p&gt; &#xA;&lt;p&gt;🤗 &lt;strong&gt;Open-Source Integrations&lt;/strong&gt;: Deploy any &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; model with a single command.&lt;/p&gt; &#xA;&lt;h3&gt;Features Coming Soon&lt;/h3&gt; &#xA;&lt;p&gt;🐳 &lt;strong&gt;K8s Installation Documentation&lt;/strong&gt;: We are working hard to document installation and maintenance of inference and fine-tuning functionality on your own infrastructure. For now, our documentation covers using our client libraries to access Scale&#39;s hosted infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;❄ &lt;strong&gt;Fast Cold-Start Times&lt;/strong&gt;: To prevent GPUs from idling, LLM Engine automatically scales your model to zero when it&#39;s not in use and scales up within seconds, even for large foundation models.&lt;/p&gt; &#xA;&lt;p&gt;💸 &lt;strong&gt;Cost Optimization&lt;/strong&gt;: Deploy AI models cheaper than commercial ones, including cold-start and warm-down times.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;https://spellbook.scale.com/&#34;&gt;Scale Spellbook&lt;/a&gt; to first create an account, and then grab your API key on the &lt;a href=&#34;https://spellbook.scale.com/settings&#34;&gt;Settings&lt;/a&gt; page. Set this API key as the &lt;code&gt;SCALE_API_KEY&lt;/code&gt; environment variable by adding the following line to your &lt;code&gt;.zshrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;export SCALE_API_KEY=&#34;[Your API key]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run into an &#34;Invalid API Key&#34; error, you may need to run the &lt;code&gt;. ~/.zshrc&lt;/code&gt; command to re-read your updated &lt;code&gt;.zshrc&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With your API key set, you can now send LLM Engine requests using the Python client. Try out this starter code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from llmengine import Completion&#xA;&#xA;response = Completion.create(&#xA;    model=&#34;falcon-7b-instruct&#34;,&#xA;    prompt=&#34;I&#39;m opening a pancake restaurant that specializes in unique pancake shapes, colors, and flavors. List 3 quirky names I could name my restaurant.&#34;,&#xA;    max_new_tokens=100,&#xA;    temperature=0.2,&#xA;)&#xA;&#xA;print(response.output.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see a successful completion of your given prompt!&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;What&#39;s next?&lt;/em&gt; Visit the &lt;a href=&#34;https://scaleapi.github.io/llm-engine/&#34;&gt;LLM Engine documentation pages&lt;/a&gt; for more on the &lt;code&gt;Completion&lt;/code&gt; and &lt;code&gt;FineTune&lt;/code&gt; APIs and how to use them.&lt;/p&gt;</summary>
  </entry>
</feed>