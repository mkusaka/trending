<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-30T01:42:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zhiyiYo/PyQt-Fluent-Widgets</title>
    <updated>2023-06-30T01:42:20Z</updated>
    <id>tag:github.com,2023-06-30:/zhiyiYo/PyQt-Fluent-Widgets</id>
    <link href="https://github.com/zhiyiYo/PyQt-Fluent-Widgets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fluent design widgets library based on PyQt/PySide. Make Qt Great Again.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;18%&#34; align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/zhiyiYo/PyQt-Fluent-Widgets/master/docs/source/_static/logo.png&#34; alt=&#34;logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; PyQt-Fluent-Widgets &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A fluent design widgets library based on PyQt5 &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a style=&#34;text-decoration:none&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Platform-Win32%20|%20Linux%20|%20macOS-blue?color=#4ec820&#34; alt=&#34;Platform Win32 | Linux | macOS&#34;&gt; &lt;/a&gt; &lt;a style=&#34;text-decoration:none&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/pyqt-fluent-widgets?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=Downloads&#34; alt=&#34;Download&#34;&gt; &lt;/a&gt; &lt;a style=&#34;text-decoration:none&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-GPLv3-blue?color=#4ec820&#34; alt=&#34;GPLv3&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/zhiyiYo/PyQt-Fluent-Widgets/master/docs/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhiyiYo/PyQt-Fluent-Widgets/master/docs/source/_static/Interface.jpg&#34; alt=&#34;Interface&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;To install lite version (&lt;code&gt;AcrylicLabel&lt;/code&gt; is not available):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install PyQt-Fluent-Widgets -i https://pypi.org/simple/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install full-featured version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#34;PyQt-Fluent-Widgets[full]&#34; -i https://pypi.org/simple/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using PySide2, PySide6 or PyQt6, you can download the code in &lt;a href=&#34;https://github.com/zhiyiYo/PyQt-Fluent-Widgets/tree/PySide2&#34;&gt;PySide2&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhiyiYo/PyQt-Fluent-Widgets/tree/PySide6&#34;&gt;PySide6&lt;/a&gt; or &lt;a href=&#34;https://github.com/zhiyiYo/PyQt-Fluent-Widgets/tree/PyQt6&#34;&gt;PyQt6&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Don&#39;t install PyQt-Fluent-Widgets, PyQt6-Fluent-Widgets, PySide2-Fluent-Widgets and PySide6-Fluent-Widgets at the same time, because their package names are all &lt;code&gt;qfluentwidgets&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Run Example&lt;/h2&gt; &#xA;&lt;p&gt;After installing PyQt-Fluent-Widgets package using pip, you can run any demo in the examples directory, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd examples/gallery&#xA;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter &lt;code&gt;ImportError: cannot import name &#39;XXX&#39; from &#39;qfluentwidgets&#39;&lt;/code&gt;, it indicates that the package version you installed is too low. You can replace the mirror source with &lt;a href=&#34;https://pypi.org/simple&#34;&gt;https://pypi.org/simple&lt;/a&gt; and reinstall again.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Want to know more about PyQt-Fluent-Widgets? Please read the &lt;a href=&#34;https://pyqt-fluent-widgets.readthedocs.io/&#34;&gt;help document&lt;/a&gt; üëà&lt;/p&gt; &#xA;&lt;h2&gt;Video Demonstration&lt;/h2&gt; &#xA;&lt;p&gt;Check out this &lt;a href=&#34;https://www.bilibili.com/video/BV12c411L73q&#34;&gt;‚ñ∂ example video&lt;/a&gt; that shows off what PyQt-Fluent-Widgets are capable of üéâ&lt;/p&gt; &#xA;&lt;h2&gt;Work with QtDesigner&lt;/h2&gt; &#xA;&lt;p&gt;You can use PyQt-Fluent-Widgets in QtDesigner directly by running &lt;code&gt;python ./tools/designer.py&lt;/code&gt;. If the operation is successful, you should be able to see the PyQt-Fluent-Widgets in the sidebar of QtDesigner.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; It is recommended to install pyqt5-tools and PyQt-Fluent-Widgets in a virtual environment. Please make sure &lt;strong&gt;PyQt-Frameless-Window &amp;gt;= 0.2.7&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If this project helps you a lot and you want to support the development and maintenance of this project, feel free to sponsor me via &lt;a href=&#34;https://afdian.net/a/zhiyiYo&#34;&gt;Áà±ÂèëÁîµ&lt;/a&gt; or &lt;a href=&#34;https://ko-fi.com/zhiyiYo&#34;&gt;ko-fi&lt;/a&gt;. Your support is highly appreciated ü•∞&lt;/p&gt; &#xA;&lt;h2&gt;See Also&lt;/h2&gt; &#xA;&lt;p&gt;Here are some projects that use PyQt-Fluent-Widgets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhiyiYo/Groove&#34;&gt;&lt;strong&gt;zhiyiYo/Groove&lt;/strong&gt;: A cross-platform music player based on PyQt5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhiyiYo/Alpha-Gobang-Zero&#34;&gt;&lt;strong&gt;zhiyiYo/Alpha-Gobang-Zero&lt;/strong&gt;: A gobang robot based on reinforcement learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://copyask.com/&#34;&gt;&lt;strong&gt;ËØ≠Ê†∏ÁßëÊäÄ/CopyAsk&lt;/strong&gt;: Your AI Assistant at Fingertips&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/apps/design/&#34;&gt;&lt;strong&gt;Windows design&lt;/strong&gt;: Design guidelines and toolkits for creating native app experiences&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/WinUI-Gallery&#34;&gt;&lt;strong&gt;Microsoft/WinUI-Gallery&lt;/strong&gt;: An app demonstrates the controls available in WinUI and the Fluent Design System&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyQt-Fluent-Widgets is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/zhiyiYo/PyQt-Fluent-Widgets/master/LICENSE&#34;&gt;GPLv3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Copyright ¬© 2021 by zhiyiYo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenGVLab/DragGAN</title>
    <updated>2023-06-30T01:42:20Z</updated>
    <id>tag:github.com,2023-06-30:/OpenGVLab/DragGAN</id>
    <link href="https://github.com/OpenGVLab/DragGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial Implementation of DragGAN - &#34;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&#34; ÔºàDragGAN ÂÖ®ÂäüËÉΩÂÆûÁé∞ÔºåÂú®Á∫øDemoÔºåÊú¨Âú∞ÈÉ®ÁΩ≤ËØïÁî®Ôºå‰ª£Á†Å„ÄÅÊ®°ÂûãÂ∑≤ÂÖ®ÈÉ®ÂºÄÊ∫êÔºåÊîØÊåÅWindows, macOS, LinuxÔºâ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DragGAN&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/draggan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/draggan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/#running-locally&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Support-macOS%20%7C%20Windows%20%7C%20Linux-blue&#34; alt=&#34;support&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;a href=&#34;https://colab.research.google.com/github/Zeqiang-Lai/DragGAN/blob/master/colab.ipynb&#34;&gt;&lt;code&gt;Colab Demo&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/Awesome-DragGAN&#34;&gt;&lt;code&gt;Awesome-DragGAN&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;&lt;code&gt;InternGPT Demo&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/#running-locally&#34;&gt;&lt;code&gt;Local Deployment&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note for Colab, remember to select a GPU via &lt;code&gt;Runtime/Change runtime type&lt;/code&gt; (&lt;code&gt;‰ª£Á†ÅÊâßË°åÁ®ãÂ∫è/Êõ¥ÊîπËøêË°åÊó∂Á±ªÂûã&lt;/code&gt;).&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;If you want to upload custom image, please install 1.1.0 via &lt;code&gt;pip install draggan==1.1.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/DragGAN/&#34;&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/assets/mouse.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/assets/nose.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/assets/cat.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/assets/horse.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;How it Work ?&lt;/h2&gt; &#xA;&lt;p&gt;Here is a simple tutorial video showing how to use our implementation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check out the original &lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/DragGAN/&#34;&gt;paper&lt;/a&gt; for the backend algorithm and math.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/assets/paper.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;What&#39;s New&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/6/25] Relase version 1.1.1, it includes a major bug fix and speed improvement.&lt;/li&gt; &#xA; &lt;li&gt;[2023/6/25] &lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Official Code&lt;/a&gt; is released, check it out.&lt;/li&gt; &#xA; &lt;li&gt;[2023/5/29] A new version is in beta, install via &lt;code&gt;pip install draggan==1.1.0b2&lt;/code&gt;, includes speed improvement and more models.&lt;/li&gt; &#xA; &lt;li&gt;[2023/5/25] DragGAN is on PyPI, simple install via &lt;code&gt;pip install draggan&lt;/code&gt;. Also addressed the common CUDA problems &lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/issues/38&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/issues/38&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/issues/12&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/issues/12&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/5/25] We now support StyleGAN2-ada with much higher quality and more types of images. Try it by selecting models started with &#34;ada&#34;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/5/24] An out-of-box online demo is integrated in &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt; - a super cool pointing-language-driven visual interactive system. Enjoy for free.&lt;span&gt;üç≠&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/5/24] Custom Image with GAN inversion is supported, but it is possible that your custom images are distorted due to the limitation of GAN inversion. Besides, it is also possible the manipulations fail due to the limitation of our implementation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;Changelog&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a docker image, thanks &lt;a href=&#34;https://github.com/egbaydarov&#34;&gt;@egbaydarov&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; PTI GAN inversion &lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/issues/71#issuecomment-1573461314&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/issues/71#issuecomment-1573461314&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tweak performance, See &lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/tree/v2&#34;&gt;v2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Improving installation experience, DragGAN is now on &lt;a href=&#34;https://pypi.org/project/draggan&#34;&gt;PyPI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically determining the number of iterations, See &lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/tree/v2&#34;&gt;v2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allow to save video without point annotations, custom image size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support StyleGAN2-ada.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate into &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Custom Image with GAN inversion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Download generated image and generation trajectory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Controlling generation process with GUI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically download stylegan2 checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support movable region, multiple handle points.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio and Colab Demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This project is now a sub-project of &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt; for interactive image editing. Future updates of more cool tools beyond DragGAN would be added in &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/DragGAN/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{pan2023draggan,&#xA;    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold}, &#xA;    author={Pan, Xingang and Tewari, Ayush, and Leimk{\&#34;u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},&#xA;    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Official DragGAN&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/skimai/DragGAN&#34;&gt;DragGAN-Streamlit&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;StyleGAN2&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;StyleGAN2-pytorch&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada-pytorch&#34;&gt;StyleGAN2-Ada&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human&#34;&gt;StyleGAN-Human&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/self-distilled-stylegan/self-distilled-internet-photos&#34;&gt;Self-Distilled-StyleGAN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- Welcome to discuss with us and continuously improve the user experience of DragGAN.&#xA;Reach us with this WeChat QR Code. --&gt; &#xA;&lt;p align=&#34;left&#34;&gt;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/3cb5b7d6-50d1-4e80-9943-ad2e61b69395&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>abacaj/mpt-30B-inference</title>
    <updated>2023-06-30T01:42:20Z</updated>
    <id>tag:github.com,2023-06-30:/abacaj/mpt-30B-inference</id>
    <link href="https://github.com/abacaj/mpt-30B-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run inference on MPT-30B using CPU&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MPT 30B inference code using CPU&lt;/h1&gt; &#xA;&lt;p&gt;Run inference on the latest MPT-30B model using your CPU. This inference code uses a &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; quantized model. To run the model we&#39;ll use a library called &lt;a href=&#34;https://github.com/marella/ctransformers&#34;&gt;ctransformers&lt;/a&gt; that has bindings to ggml in python.&lt;/p&gt; &#xA;&lt;p&gt;Turn style with history on latest commit:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7272343/248859199-28a82f3d-ee54-44e4-b22d-ca348ac667e3.png&#34; alt=&#34;Inference Chat&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video of initial demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abacaj/mpt-30B-inference/assets/7272343/486fc9b1-8216-43cc-93c3-781677235502&#34;&gt;Inference Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;I recommend you use docker for this model, it will make everything easier for you. Minimum specs system with 32GB of ram. Recommend to use &lt;code&gt;python 3.10&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tested working on&lt;/h2&gt; &#xA;&lt;p&gt;Will post some numbers for these two later.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD Epyc 7003 series CPU&lt;/li&gt; &#xA; &lt;li&gt;AMD Ryzen 5950x CPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;First create a venv.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m venv env &amp;amp;&amp;amp; source env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next install dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next download the quantized model weights (about 19GB).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python download_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ready to rock, run inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next modify inference script prompt and generation parameters.&lt;/p&gt;</summary>
  </entry>
</feed>