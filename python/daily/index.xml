<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-25T01:44:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>docarray/docarray</title>
    <updated>2023-02-25T01:44:28Z</updated>
    <id>tag:github.com,2023-02-25:/docarray/docarray</id>
    <link href="https://github.com/docarray/docarray" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üß¨ The data structure for multimodal data ¬∑ Neural Search ¬∑ Vector Search ¬∑ Document Store&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/docarray/docarray/raw/main/docs/_static/logo-light.svg?raw=true&#34; alt=&#34;DocArray logo: The data structure for unstructured data&#34; width=&#34;150px&#34;&gt; &lt;br&gt; &lt;b&gt;The data structure for multimodal data&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/docarray/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/docarray?style=flat-square&amp;amp;label=Release&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/docarray/docarray&#34;&gt;&lt;img alt=&#34;Codecov branch&#34; src=&#34;https://img.shields.io/codecov/c/github/docarray/docarray/main?logo=Codecov&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/6554&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/6554/badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/docarray&#34;&gt;&lt;img alt=&#34;PyPI - Downloads from official pypistats&#34; src=&#34;https://img.shields.io/pypi/dm/docarray?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WaMp6PVPgR&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/WaMp6PVPgR?theme=default-inverted&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- start elevator-pitch --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚¨ÜÔ∏è &lt;strong&gt;DocArray v2&lt;/strong&gt;: We are currently working on v2 of DocArray. Keep reading here if you are interested in the current (stable) version, or check out the &lt;a href=&#34;https://github.com/docarray/docarray/tree/feat-rewrite-v2#readme&#34;&gt;v2 alpha branch&lt;/a&gt; and &lt;a href=&#34;https://github.com/docarray/docarray/issues/780&#34;&gt;v2 roadmap&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;DocArray is a library for nested, unstructured, multimodal data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer multimodal data with a Pythonic API.&lt;/p&gt; &#xA;&lt;p&gt;üö™ &lt;strong&gt;Door to multimodal world&lt;/strong&gt;: super-expressive data structure for representing complicated/mixed/nested text, image, video, audio, 3D mesh data. The foundation data structure of &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/clip-as-service&#34;&gt;CLIP-as-service&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/dalle-flow&#34;&gt;DALL¬∑E Flow&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/discoart&#34;&gt;DiscoArt&lt;/a&gt; etc.&lt;/p&gt; &#xA;&lt;p&gt;üßë‚Äçüî¨ &lt;strong&gt;Data science powerhouse&lt;/strong&gt;: greatly accelerate data scientists&#39; work on embedding, k-NN matching, querying, visualizing, evaluating via Torch/TensorFlow/ONNX/PaddlePaddle on CPU/GPU.&lt;/p&gt; &#xA;&lt;p&gt;üö° &lt;strong&gt;Data in transit&lt;/strong&gt;: optimized for network communication, ready-to-wire at anytime with fast and compressed serialization in Protobuf, bytes, base64, JSON, CSV, DataFrame. Perfect for streaming and out-of-memory data.&lt;/p&gt; &#xA;&lt;p&gt;üîé &lt;strong&gt;One-stop k-NN&lt;/strong&gt;: Unified and consistent API for mainstream vector databases that allows nearest neighbor search including Elasticsearch, Redis, AnnLite, Qdrant, Weaviate.&lt;/p&gt; &#xA;&lt;p&gt;üëí &lt;strong&gt;For modern apps&lt;/strong&gt;: GraphQL support makes your server versatile on request and response; built-in data validation and JSON Schema (OpenAPI) help you build reliable web services.&lt;/p&gt; &#xA;&lt;p&gt;üêç &lt;strong&gt;Pythonic experience&lt;/strong&gt;: as easy as a Python list. If you can Python, you can DocArray. Intuitive idioms and type annotation simplify the code you write.&lt;/p&gt; &#xA;&lt;p&gt;üõ∏ &lt;strong&gt;IDE integration&lt;/strong&gt;: pretty-print and visualization on Jupyter notebook and Google Colab; comprehensive autocomplete and type hints in PyCharm and VS Code.&lt;/p&gt; &#xA;&lt;p&gt;Read more on &lt;a href=&#34;https://docs.docarray.org/get-started/what-is/&#34;&gt;why should you use DocArray&lt;/a&gt; and &lt;a href=&#34;https://docs.docarray.org/get-started/what-is/#comparing-to-alternatives&#34;&gt;comparison to alternatives&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- end elevator-pitch --&gt; &#xA;&lt;p&gt;DocArray was released under the open-source &lt;a href=&#34;https://github.com/docarray/docarray/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt; in January 2022. It is currently a sandbox project under &lt;a href=&#34;https://lfaidata.foundation/&#34;&gt;LF AI &amp;amp; Data Foundation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://docs.docarray.org&#34;&gt;Documentation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Requires Python 3.7+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install docarray&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or via Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install -c conda-forge docarray&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docarray.org/#install&#34;&gt;Commonly used features&lt;/a&gt; can be enabled via &lt;code&gt;pip install &#34;docarray[common]&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;DocArray consists of three simple concepts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document&lt;/strong&gt;: a data structure for easily representing nested, unstructured data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DocumentArray&lt;/strong&gt;: a container for efficiently accessing, manipulating, and understanding multiple Documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataclass&lt;/strong&gt;: a high-level API for intuitively representing multimodal data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s see DocArray in action with some examples.&lt;/p&gt; &#xA;&lt;h3&gt;Example 1: represent multimodal data in a dataclass&lt;/h3&gt; &#xA;&lt;p&gt;You can easily represent the following news article card with &lt;code&gt;docarray.dataclass&lt;/code&gt; and type annotation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/docarray/docarray/raw/main/docs/fundamentals/dataclass/img/image-mmdoc-example.png?raw=true&#34; alt=&#34;A example multimodal document&#34; width=&#34;300px&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import dataclass, Document&#xA;from docarray.typing import Image, Text, JSON&#xA;&#xA;&#xA;@dataclass&#xA;class WPArticle:&#xA;    banner: Image&#xA;    headline: Text&#xA;    meta: JSON&#xA;&#xA;&#xA;a = WPArticle(&#xA;    banner=&#39;https://.../cat-dog-flight.png&#39;,&#xA;    headline=&#39;Everything to know about flying with pets, ...&#39;,&#xA;    meta={&#xA;        &#39;author&#39;: &#39;Nathan Diller&#39;,&#xA;        &#39;Column&#39;: &#39;By the Way - A Post Travel Destination&#39;,&#xA;    },&#xA;)&#xA;&#xA;d = Document(a)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Example 2: text matching in 10 lines&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s search for top-5 similar sentences of &lt;kbd&gt;she smiled too much&lt;/kbd&gt; in &#34;Pride and Prejudice&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import Document, DocumentArray&#xA;&#xA;d = Document(uri=&#39;https://www.gutenberg.org/files/1342/1342-0.txt&#39;).load_uri_to_text()&#xA;da = DocumentArray(Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip())&#xA;da.apply(Document.embed_feature_hashing, backend=&#39;process&#39;)&#xA;&#xA;q = (&#xA;    Document(text=&#39;she smiled too much&#39;)&#xA;    .embed_feature_hashing()&#xA;    .match(da, metric=&#39;jaccard&#39;, use_scipy=True)&#xA;)&#xA;&#xA;print(q.matches[:5, (&#39;text&#39;, &#39;scores__jaccard__value&#39;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[[&#39;but she smiled too much.&#39;, &#xA;  &#39;_little_, she might have fancied too _much_.&#39;, &#xA;  &#39;She perfectly remembered everything that had passed in&#39;, &#xA;  &#39;tolerably detached tone. While she spoke, an involuntary glance&#39;, &#xA;  &#39;much as she chooses.‚Äù&#39;], &#xA;  [0.3333333333333333, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here the feature embedding is done by simple &lt;a href=&#34;https://en.wikipedia.org/wiki/Feature_hashing&#34;&gt;feature hashing&lt;/a&gt; and distance metric is &lt;a href=&#34;https://en.wikipedia.org/wiki/Jaccard_index&#34;&gt;Jaccard distance&lt;/a&gt;. You have better embeddings? Of course you do! We look forward to seeing your results!&lt;/p&gt; &#xA;&lt;h3&gt;Example 3: external storage for out-of-memory data&lt;/h3&gt; &#xA;&lt;p&gt;When your data is too big, storing in memory is not the best idea. DocArray supports &lt;a href=&#34;https://docs.docarray.org/advanced/document-store/&#34;&gt;multiple storage backends&lt;/a&gt; such as SQLite, Weaviate, Qdrant and AnnLite. They&#39;re all unified under &lt;strong&gt;the exact same user experience and API&lt;/strong&gt;. Take the above snippet: you only need to change one line to use SQLite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da = DocumentArray(&#xA;    (Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip()),&#xA;    storage=&#39;sqlite&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The code snippet can still run &lt;strong&gt;as-is&lt;/strong&gt;. All APIs remain the same, the subsequent code then runs in an &#34;in-database&#34; manner.&lt;/p&gt; &#xA;&lt;p&gt;Besides saving memory, you can leverage storage backends for persistence and faster retrieval (e.g. on nearest-neighbor queries).&lt;/p&gt; &#xA;&lt;h3&gt;Example 4: complete workflow of visual search&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s use DocArray and the &lt;a href=&#34;https://sites.google.com/view/totally-looks-like-dataset&#34;&gt;Totally Looks Like&lt;/a&gt; dataset to build a simple meme image search. The dataset contains 6,016 image-pairs stored in &lt;code&gt;/left&lt;/code&gt; and &lt;code&gt;/right&lt;/code&gt;. Images that share the same filename appear similar to the human eye. For example:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;left/00018.jpg&lt;/th&gt; &#xA;   &lt;th&gt;right/00018.jpg&lt;/th&gt; &#xA;   &lt;th&gt;left/00131.jpg&lt;/th&gt; &#xA;   &lt;th&gt;right/00131.jpg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/left-00018.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/right-00018.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/left-00131.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/right-00131.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given an image from &lt;code&gt;/left&lt;/code&gt;, can we find the most-similar image to it in &lt;code&gt;/right&lt;/code&gt;? (without looking at the filename).&lt;/p&gt; &#xA;&lt;h3&gt;Load images&lt;/h3&gt; &#xA;&lt;p&gt;First we load images. You &lt;em&gt;can&lt;/em&gt; go to &lt;a href=&#34;https://sites.google.com/view/totally-looks-like-dataset&#34;&gt;Totally Looks Like&lt;/a&gt;&#39;s website, unzip and load images as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import DocumentArray&#xA;&#xA;left_da = DocumentArray.from_files(&#39;left/*.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can simply pull it from Jina AI Cloud:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da = DocumentArray.pull(&#39;jina-ai/demo-leftda&#39;, show_progress=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; If you have more than 15GB of RAM and want to try using the whole dataset instead of just the first 1,000 images, remove &lt;code&gt;[:1000]&lt;/code&gt; when loading the files into the DocumentArrays &lt;code&gt;left_da&lt;/code&gt; and &lt;code&gt;right_da&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll see a progress bar to indicate how much has downloaded.&lt;/p&gt; &#xA;&lt;p&gt;To get a feeling of the data, we can plot them in one sprite image. You need matplotlib and torch installed to run this snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.plot_image_sprites()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.docarray.org&#34;&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/sprite.png?raw=true&#34; alt=&#34;Load totally looks like dataset with docarray API&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Apply preprocessing&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s do some standard computer vision pre-processing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import Document&#xA;&#xA;&#xA;def preproc(d: Document):&#xA;    return (&#xA;        d.load_uri_to_image_tensor()  # load&#xA;        .set_image_tensor_normalization()  # normalize color&#xA;        .set_image_tensor_channel_axis(-1, 0)&#xA;    )  # switch color axis for the PyTorch model later&#xA;&#xA;&#xA;left_da.apply(preproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Did I mention &lt;code&gt;apply&lt;/code&gt; works in parallel?&lt;/p&gt; &#xA;&lt;h3&gt;Embed images&lt;/h3&gt; &#xA;&lt;p&gt;Now let&#39;s convert images into embeddings using a pretrained ResNet50:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torchvision&#xA;&#xA;model = torchvision.models.resnet50(pretrained=True)  # load ResNet50&#xA;left_da.embed(model, device=&#39;cuda&#39;)  # embed via GPU to speed up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This step takes ~30 seconds on GPU. Beside PyTorch, you can also use TensorFlow, PaddlePaddle, or ONNX models in &lt;code&gt;.embed(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize embeddings&lt;/h3&gt; &#xA;&lt;p&gt;You can visualize the embeddings via tSNE in an interactive embedding projector. You will need to have pydantic, uvicorn and FastAPI installed to run this snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.plot_embeddings(image_sprites=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.docarray.org&#34;&gt;&lt;img src=&#34;https://github.com/docarray/docarray/raw/main/.github/README-img/tsne.gif?raw=true&#34; alt=&#34;Visualizing embedding via tSNE and embedding projector&#34; width=&#34;90%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Fun is fun, but our goal is to match left images against right images, and so far we have only handled the left. Let&#39;s repeat the same procedure for the right:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Pull from Cloud &lt;/th&gt; &#xA;   &lt;th&gt; Download, unzip, load from local &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;right_da = (&#xA;    DocumentArray.pull(&#39;jina-ai/demo-rightda&#39;, show_progress=True)&#xA;    .apply(preproc)&#xA;    .embed(model, device=&#39;cuda&#39;)[:1000]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;right_da = (&#xA;    DocumentArray.from_files(&#39;right/*.jpg&#39;)[:1000]&#xA;    .apply(preproc)&#xA;    .embed(model, device=&#39;cuda&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Match nearest neighbors&lt;/h3&gt; &#xA;&lt;p&gt;Now we can match the left to the right and take the top-9 results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.match(right_da, limit=9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s inspect what&#39;s inside &lt;code&gt;left_da&lt;/code&gt; matches now:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for m in left_da[0].matches:&#xA;    print(d.uri, m.uri, m.scores[&#39;cosine&#39;].value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;left/02262.jpg right/03459.jpg 0.21102&#xA;left/02262.jpg right/02964.jpg 0.13871843&#xA;left/02262.jpg right/02103.jpg 0.18265384&#xA;left/02262.jpg right/04520.jpg 0.16477376&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or shorten the loop to a one-liner using the element and attribute selector:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(left_da[&#39;@m&#39;, (&#39;uri&#39;, &#39;scores__cosine__value&#39;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Better see it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(&#xA;    DocumentArray(left_da[8].matches, copy=True)&#xA;    .apply(&#xA;        lambda d: d.set_image_tensor_channel_axis(&#xA;            0, -1&#xA;        ).set_image_tensor_inv_normalization()&#xA;    )&#xA;    .plot_image_sprites()&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.docarray.org&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/9nn-left.jpeg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; height=&#34;250px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.docarray.org&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/9nn.png?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; height=&#34;250px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Here we reversed the preprocessing steps (i.e. switching axis and normalizing) on the copied matches, so you can visualize them using image sprites.&lt;/p&gt; &#xA;&lt;h3&gt;Quantitative evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Serious as you are, visual inspection is surely not enough. Let&#39;s calculate the recall@K. First we construct the groundtruth matches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;groundtruth = DocumentArray(&#xA;    Document(uri=d.uri, matches=[Document(uri=d.uri.replace(&#39;left&#39;, &#39;right&#39;))])&#xA;    for d in left_da&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we created a new DocumentArray with real matches by simply replacing the filename, e.g. &lt;code&gt;left/00001.jpg&lt;/code&gt; to &lt;code&gt;right/00001.jpg&lt;/code&gt;. That&#39;s all we need: if the predicted match has the identical &lt;code&gt;uri&lt;/code&gt; as the groundtruth match, then it is correct.&lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s check recall rate from 1 to 5 over the full dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for k in range(1, 6):&#xA;    print(&#xA;        f&#39;recall@{k}&#39;,&#xA;        left_da.evaluate(&#xA;            groundtruth, hash_fn=lambda d: d.uri, metric=&#39;recall_at_k&#39;, k=k, max_rel=1&#xA;        ),&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;recall@1 0.02726063829787234&#xA;recall@2 0.03873005319148936&#xA;recall@3 0.04670877659574468&#xA;recall@4 0.052194148936170214&#xA;recall@5 0.0573470744680851&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use other metrics like &lt;code&gt;precision_at_k&lt;/code&gt;, &lt;code&gt;ndcg_at_k&lt;/code&gt;, &lt;code&gt;hit_at_k&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you think a pretrained ResNet50 is good enough, let me tell you with &lt;a href=&#34;https://github.com/jina-ai/finetuner&#34;&gt;Finetuner&lt;/a&gt; you can do much better with &lt;a href=&#34;https://finetuner.jina.ai/notebooks/image_to_image/&#34;&gt;just another ten lines of code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Save results&lt;/h3&gt; &#xA;&lt;p&gt;You can save a DocumentArray to binary, JSON, dict, DataFrame, CSV or Protobuf message with/without compression. In its simplest form:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.save(&#39;left_da.bin&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reuse that DocumentArray&#39;s data, use &lt;code&gt;left_da = DocumentArray.load(&#39;left_da.bin&#39;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to transfer a DocumentArray from one machine to another or share it with your colleagues, you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.push(&#39;my_shared_da&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now anyone who knows the token &lt;code&gt;my_shared_da&lt;/code&gt; can pull and work on it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da = DocumentArray.pull(&#39;&amp;lt;username&amp;gt;/my_shared_da&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Intrigued? That&#39;s only scratching the surface of what DocArray is capable of. &lt;a href=&#34;https://docs.docarray.org&#34;&gt;Read our docs to learn more&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- start support-pitch --&gt; &#xA;&lt;h2&gt;Support &amp;amp; talk to us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://discord.gg/WaMp6PVPgR&#34;&gt;Discord server&lt;/a&gt; and chat with other community members about ideas.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://calendar.google.com/calendar/u/2?cid=Y180NmJjYjQ3ZjEzN2QzOThjZjhjZmM2MzM0YTYyMjRkMjVhMjY1NTBlMGZkNjZkOGFmOWUyNjZiMDU4ODkyYmIxQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20&#34;&gt;public meetings&lt;/a&gt; where we discuss the future of the project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;DocArray is a trademark of LF AI Projects, LLC&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>Plachtaa/VITS-fast-fine-tuning</title>
    <updated>2023-02-25T01:44:28Z</updated>
    <id>tag:github.com,2023-02-25:/Plachtaa/VITS-fast-fine-tuning</id>
    <link href="https://github.com/Plachtaa/VITS-fast-fine-tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo is a pipeline of VITS finetuning for fast speaker adaptation TTS, and any-to-any voice conversion&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning/raw/main/README_ZH.md&#34;&gt;‰∏≠ÊñáÊñáÊ°£ËØ∑ÁÇπÂáªËøôÈáå&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;VITS Fast Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;This repo will guide you to add your own character voices, or even your own voice, into an existing VITS TTS model to make it able to do the following tasks in less than 1 hour:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Any-to-any voice conversion between you &amp;amp; any characters you added &amp;amp; preset characters&lt;/li&gt; &#xA; &lt;li&gt;English, Japanese &amp;amp; Chinese Text-to-Speech synthesis with the characters you added &amp;amp; preset characters&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Welcome to play around with the base model, a Trilingual Anime VITS! &lt;a href=&#34;https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Currently Supported Tasks:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Convert user&#39;s voice to characters listed &lt;a href=&#34;https://github.com/SongtingLiu/VITS_voice_conversion/raw/main/configs/finetune_speaker.json&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Chinese, English, Japanese TTS with user&#39;s voice&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Chinese, English, Japanese TTS with custom characters!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Currently Supported Characters for TTS &amp;amp; VC:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Umamusume Pretty Derby (Used as base model pretraining)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Sanoba Witch (Used as base model pretraining)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Genshin Impact (Used as base model pretraining)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Any character you wish as long as you have their voices!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s recommended to perform fine-tuning on &lt;a href=&#34;https://colab.research.google.com/drive/1omMhfYKrAAQ7a6zOCsyqpla-wU-QyfZn?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; because the original VITS has some dependencies that are difficult to configure.&lt;/p&gt; &#xA;&lt;h3&gt;How long does it take?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies (2 min)&lt;/li&gt; &#xA; &lt;li&gt;Record at least 20 your own voice, the content to read will be presented in UI, less than 20 words per sentence. (5~10 min)&lt;/li&gt; &#xA; &lt;li&gt;Upload your character voices, which should be a &lt;code&gt;.zip&lt;/code&gt; file, it&#39;s file structure should be like:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Your-zip-file.zip&#xA;‚îú‚îÄ‚îÄ‚îÄCharacter_name_1&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄxxx.wav&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄ...&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄyyy.mp3&#xA;‚îú   ‚îî‚îÄ‚îÄ‚îÄzzz.wav&#xA;‚îú‚îÄ‚îÄ‚îÄCharacter_name_2&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄxxx.wav&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄ...&#xA;‚îú   ‚îú‚îÄ‚îÄ‚îÄyyy.mp3&#xA;‚îú   ‚îî‚îÄ‚îÄ‚îÄzzz.wav&#xA;‚îú‚îÄ‚îÄ‚îÄ...&#xA;‚îú&#xA;‚îî‚îÄ‚îÄ‚îÄCharacter_name_n&#xA;    ‚îú‚îÄ‚îÄ‚îÄxxx.wav&#xA;    ‚îú‚îÄ‚îÄ‚îÄ...&#xA;    ‚îú‚îÄ‚îÄ‚îÄyyy.mp3&#xA;    ‚îî‚îÄ‚îÄ‚îÄzzz.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the format &amp;amp; name of the audio files does not matter as long as they are audio files.&lt;br&gt; Audio quality requirements: &amp;gt;=2s, &amp;lt;=20s per audio, background noise should be as less as possible. Audio quantity requirements: at least 10 per character, better if 20+ per character.&lt;br&gt; You can either choose to perform step 2, 3, or both, depending on your needs.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Fine-tune (30 min)&lt;br&gt; After everything is done, download the fine-tuned model &amp;amp; model config&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Inference or Usage (Currently support Windows only)&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Remember to download your fine-tuned model!&lt;/li&gt; &#xA; &lt;li&gt;Download the latest release&lt;/li&gt; &#xA; &lt;li&gt;Put your model &amp;amp; config file into the folder &lt;code&gt;inference&lt;/code&gt;, make sure to rename the model to &lt;code&gt;G_latest.pth&lt;/code&gt; and config file to &lt;code&gt;finetune_speaker.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The file structure should be as follows:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;inference&#xA;‚îú‚îÄ‚îÄ‚îÄinference.exe&#xA;‚îú‚îÄ‚îÄ‚îÄ...&#xA;‚îú‚îÄ‚îÄ‚îÄfinetune_speaker.json&#xA;‚îî‚îÄ‚îÄ‚îÄG_latest.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;run &lt;code&gt;inference.exe&lt;/code&gt;, the browser should pop up automatically.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>yunjey/pytorch-tutorial</title>
    <updated>2023-02-25T01:44:28Z</updated>
    <id>tag:github.com,2023-02-25:/yunjey/pytorch-tutorial</id>
    <link href="https://github.com/yunjey/pytorch-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch Tutorial for Deep Learning Researchers&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;40%&#34; src=&#34;https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/logo/pytorch_logo_2018.svg?sanitize=true&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repository provides tutorial code for deep learning researchers to learn &lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt;. In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish &lt;a href=&#34;http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;Official Pytorch Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;h4&gt;1. Basics&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py&#34;&gt;PyTorch Basics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23&#34;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49&#34;&gt;Feedforward Neural Network&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Intermediate&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56&#34;&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/deep_residual_network/main.py#L76-L113&#34;&gt;Deep Residual Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58&#34;&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58&#34;&gt;Bidirectional Recurrent Neural Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model/main.py#L30-L50&#34;&gt;Language Model (RNN-LM)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. Advanced&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/generative_adversarial_network/main.py#L41-L57&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65&#34;&gt;Variational Auto-Encoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer&#34;&gt;Neural Style Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning&#34;&gt;Image Captioning (CNN-RNN)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4. Utilities&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard&#34;&gt;TensorBoard in PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/yunjey/pytorch-tutorial.git&#xA;$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT&#xA;$ python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Python 2.7 or 3.5+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pytorch.org/&#34;&gt;PyTorch 0.4.0+&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>