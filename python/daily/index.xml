<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-27T01:42:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenDriveLab/UniAD</title>
    <updated>2023-06-27T01:42:42Z</updated>
    <id>tag:github.com,2023-06-27:/OpenDriveLab/UniAD</id>
    <link href="https://github.com/OpenDriveLab/UniAD" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2023 Best Paper] Planning-oriented Autonomous Driving&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Planning-oriented Autonomous Driving&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://opendrivelab.github.io/UniAD/&#34;&gt;&#xA;    &lt;img alt=&#34;Project Page&#34; src=&#34;https://img.shields.io/badge/Project%20Page-Open-yellowgreen.svg&#34; target=&#34;_blank&#34; /&gt;&#xA;  &lt;/a&gt;&#xA;  &lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/blob/master/LICENSE&#34;&gt;&#xA;    &lt;img alt=&#34;License: Apache2.0&#34; src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue.svg&#34; target=&#34;_blank&#34; /&gt;&#xA;  &lt;/a&gt;&#xA;  &lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&#34;&gt;&#xA;    &lt;img alt=&#34;Good first issue&#34; src=&#34;https://img.shields.io/github/issues/OpenDriveLab/UniAD/good%20first%20issue&#34; target=&#34;_blank&#34; /&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://opendrivelab.github.io/UniAD/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2212.10156&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=cyrxJJ_nnaQ&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://opendrivelab.com/e2ead/UniAD_plenary_talk_slides.pdf&#34;&gt;Slides&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/assets/48089846/bcf685e4-2471-450e-8b77-e028a46bd0f7&#34;&gt;https://github.com/OpenDriveLab/UniAD/assets/48089846/bcf685e4-2471-450e-8b77-e028a46bd0f7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/sources/pipeline.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#high&#34;&gt;Highlights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#start&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/INSTALL.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/DATA_PREP.md&#34;&gt;Prepare Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md#example&#34;&gt;Evaluation Example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md#gpu&#34;&gt;GPU Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md&#34;&gt;Train/Eval&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#models&#34;&gt;Results and Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#todos&#34;&gt;TODO List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Highlights &lt;a name=&#34;high&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üöò&lt;/span&gt; &lt;strong&gt;Planning-oriented philosophy&lt;/strong&gt;: UniAD is a Unified Autonomous Driving algorithm framework following a planning-oriented philosophy. Instead of standalone modular design and multi-task learning, we cast a series of tasks, including perception, prediction and planning tasks hierarchically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üèÜ&lt;/span&gt; &lt;strong&gt;SOTA performance&lt;/strong&gt;: All tasks within UniAD achieve SOTA performance, especially prediction and planning (motion: 0.71m minADE, occ: 63.4% IoU, planning: 0.31% avg.Col)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News &lt;a name=&#34;news&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Paper Title Change&lt;/code&gt;&lt;/strong&gt;: To avoid confusion with the &#34;goal-point&#34; navigation in Robotics, we change the title from &#34;Goal-oriented&#34; to &#34;Planning-oriented&#34; suggested by Reviewers. Thank you!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/06/12] Bugfix [Ref: https://github.com/OpenDriveLab/UniAD/issues/21]: Previously, the performance of the stage1 model (track_map) could not be replicated when trained from scratch, due to mistakenly adding &lt;code&gt;loss_past_traj&lt;/code&gt; and &lt;code&gt;freezing img_neck&lt;/code&gt; and &lt;code&gt;BN&lt;/code&gt;. By removing &lt;code&gt;loss_past_traj&lt;/code&gt; and unfreezing &lt;code&gt;img_neck&lt;/code&gt; and &lt;code&gt;BN&lt;/code&gt; in training, the reported results could be reproduced (AMOTA: 0.393, &lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/releases/download/v1.0/uniad_reproduce_stage1_gpu16_train.log&#34;&gt;stage1_train_log&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/04/18] New feature: You can replace BEVFormer with other BEV Encoding methods, e.g., LSS, as long as you provide the &lt;code&gt;bev_embed&lt;/code&gt; and &lt;code&gt;bev_pos&lt;/code&gt; in &lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/raw/cb4e3dc336ac9f94897ef3c7d85edba85a507726/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py#L394&#34;&gt;track_train&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/raw/cb4e3dc336ac9f94897ef3c7d85edba85a507726/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py#L661&#34;&gt;track_inference&lt;/a&gt;. Make sure your bevs and ours are of the same shape.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/04/18] Base-model checkpoints are released.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/03/29] Code &amp;amp; model initial release &lt;code&gt;v1.0&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/03/21] &lt;span&gt;üöÄ&lt;/span&gt;&lt;span&gt;üöÄ&lt;/span&gt; UniAD is accepted by CVPR 2023, as an &lt;strong&gt;Award Candidate&lt;/strong&gt; (12 out of 2360 accepted papers)!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2022/12/21] UniAD &lt;a href=&#34;https://arxiv.org/abs/2212.10156&#34;&gt;paper&lt;/a&gt; is available on arXiv.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started &lt;a name=&#34;start&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/INSTALL.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/DATA_PREP.md&#34;&gt;Prepare Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md#example&#34;&gt;Evaluation Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md#gpu&#34;&gt;GPU Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md&#34;&gt;Train/Eval&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results and Pre-trained Models &lt;a name=&#34;models&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;UniAD is trained in two stages. Pretrained checkpoints of both stages will be released and the results of each model are listed in the following tables.&lt;/p&gt; &#xA;&lt;h3&gt;Stage1: Perception training&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We first train the perception modules (i.e., track and map) to obtain a stable weight initlization for the next stage. BEV features are aggregated with 5 frames (queue_length = 5).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tracking&lt;br&gt;AMOTA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mapping&lt;br&gt;IoU-lane&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UniAD-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;R101&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.390&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.297&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/projects/configs/stage1_track_map/base_track_map.py&#34;&gt;base-stage1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/releases/download/v1.0/uniad_base_track_map.pth&#34;&gt;base-stage1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Stage2: End-to-end training&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We optimize all task modules together, including track, map, motion, occupancy and planning. BEV features are aggregated with 3 frames (queue_length = 3).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- &#xA;Pre-trained models and results under main metrics are provided below. We refer you to the [paper](https://arxiv.org/abs/2212.10156) for more details. --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tracking&lt;br&gt;AMOTA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mapping&lt;br&gt;IoU-lane&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Motion&lt;br&gt;minADE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Occupancy&lt;br&gt;IoU-n.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Planning&lt;br&gt;avg.Col.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UniAD-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;R101&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.358&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.317&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.709&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/projects/configs/stage2_e2e/base_e2e.py&#34;&gt;base-stage2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenDriveLab/UniAD/releases/download/v1.0/uniad_base_e2e.pth&#34;&gt;base-stage2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Checkpoint Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the checkpoints you need into &lt;code&gt;UniAD/ckpts/&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;You can evaluate these checkpoints to reproduce the results, following the &lt;code&gt;evaluation&lt;/code&gt; section in &lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md&#34;&gt;TRAIN_EVAL.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can also initialize your own model with the provided weights. Change the &lt;code&gt;load_from&lt;/code&gt; field to &lt;code&gt;path/of/ckpt&lt;/code&gt; in the config and follow the &lt;code&gt;train&lt;/code&gt; section in &lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/docs/TRAIN_EVAL.md&#34;&gt;TRAIN_EVAL.md&lt;/a&gt; to start training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Model Structure&lt;/h3&gt; &#xA;&lt;p&gt;The overall pipeline of UniAD is controlled by &lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py&#34;&gt;uniad_e2e.py&lt;/a&gt; which coordinates all the task modules in &lt;code&gt;UniAD/projects/mmdet3d_plugin/uniad/dense_heads&lt;/code&gt;. If you are interested in the implementation of a specific task module, please refer to its corresponding file, e.g., &lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/projects/mmdet3d_plugin/uniad/dense_heads/motion_head.py&#34;&gt;motion_head&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;TODO List &lt;a name=&#34;todos&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; All configs &amp;amp; checkpoints [Soon]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Upgrade the implementation of MapFormer from Panoptic SegFormer to &lt;a href=&#34;https://github.com/OpenDriveLab/TopoNet&#34;&gt;TopoNet&lt;/a&gt;, which features the vectorized map representations and topology reasoning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support larger batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; [Long-term] Improve flexibility for future extensions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix bug: Unable to reproduce the results of stage1 track-map model when training from scratch. [Ref: https://github.com/OpenDriveLab/UniAD/issues/21]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization codes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Separating BEV encoder and tracking module&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Base-model configs &amp;amp; checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Code initialization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License &lt;a name=&#34;license&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;All assets and code are under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenDriveLab/UniAD/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt; unless specified otherwise.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &lt;a name=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing our paper if the project helps your research with the following BibTex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{hu2023_uniad,&#xA; title={Planning-oriented Autonomous Driving}, &#xA; author={Yihan Hu and Jiazhi Yang and Li Chen and Keyu Li and Chonghao Sima and Xizhou Zhu and Siqi Chai and Senyao Du and Tianwei Lin and Wenhai Wang and Lewei Lu and Xiaosong Jia and Qiang Liu and Jifeng Dai and Yu Qiao and Hongyang Li},&#xA; booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA; year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related resources&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fundamentalvision/BEVFormer&#34;&gt;BEVFormer&lt;/a&gt; (&lt;span&gt;üöÄ&lt;/span&gt;Ours!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenPerceptionX/ST-P3&#34;&gt;ST-P3&lt;/a&gt; (&lt;span&gt;üöÄ&lt;/span&gt;Ours!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wayveai/fiery&#34;&gt;FIERY&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/megvii-research/MOTR&#34;&gt;MOTR&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>h2oai/h2o-llmstudio</title>
    <updated>2023-06-27T01:42:42Z</updated>
    <id>tag:github.com,2023-06-27:/h2oai/h2o-llmstudio</id>
    <link href="https://github.com/h2oai/h2o-llmstudio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/app_utils/static/llm-studio-logo-light.png#gh-dark-mode-only&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/app_utils/static/llm-studio-logo.png#gh-light-mode-only&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;Welcome to H2O LLM Studio, a framework and no-code GUI designed for&lt;br&gt; fine-tuning state-of-the-art large language models (LLMs). &lt;/p&gt; &lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1069138/233859311-32aa1f8c-4d68-47ac-8cd9-9313171ff9f9.png&#34;&gt;&lt;img width=&#34;50%&#34; alt=&#34;home&#34; src=&#34;https://user-images.githubusercontent.com/1069138/233859311-32aa1f8c-4d68-47ac-8cd9-9313171ff9f9.png&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1069138/233859315-e6928aa7-28d2-420b-8366-bc7323c368ca.png&#34;&gt;&lt;img width=&#34;50%&#34; alt=&#34;logs&#34; src=&#34;https://user-images.githubusercontent.com/1069138/233859315-e6928aa7-28d2-420b-8366-bc7323c368ca.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Jump to&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#with-h2o-llm-studio-you-can&#34;&gt;With H2O LLM Studio, you can&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#whats-new&#34;&gt;What&#39;s New&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#recommended-install&#34;&gt;Recommended Install&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#using-requirementstxt&#34;&gt;Using requirements.txt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#run-h2o-llm-studio-gui&#34;&gt;Run H2O LLM Studio GUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#run-h2o-llm-studio-gui-using-docker-from-a-nightly-build&#34;&gt;Run H2O LLM Studio GUI using Docker from a nightly build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#run-h2o-llm-studio-gui-by-building-your-own-docker-image&#34;&gt;Run H2O LLM Studio GUI by building your own Docker image&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#run-h2o-llm-studio-with-command-line-interface-cli&#34;&gt;Run H2O LLM Studio with command line interface (CLI)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#data-format&#34;&gt;Data Format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#training-your-model&#34;&gt;Training your model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#starting-an-experiment&#34;&gt;Starting an experiment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#monitoring-the-experiment&#34;&gt;Monitoring the experiment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#push-to-hugging-face-%F0%9F%A4%97&#34;&gt;Push to Hugging Face ü§ó&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#compare-experiments&#34;&gt;Compare experiments&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#example-run-on-oasst-data-via-cli&#34;&gt;Example: Run on OASST data via CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#model-checkpoints&#34;&gt;Model checkpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;With H2O LLM Studio, you can&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;easily and effectively fine-tune LLMs &lt;strong&gt;without the need for any coding experience&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;use a &lt;strong&gt;graphic user interface (GUI)&lt;/strong&gt; specially designed for large language models.&lt;/li&gt; &#xA; &lt;li&gt;finetune any LLM using a large variety of hyperparameters.&lt;/li&gt; &#xA; &lt;li&gt;use recent finetuning techniques such as &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;Low-Rank Adaptation (LoRA)&lt;/a&gt; and 8-bit model training with a low memory footprint.&lt;/li&gt; &#xA; &lt;li&gt;use Reinforcement Learning (RL) to finetune your model (experimental)&lt;/li&gt; &#xA; &lt;li&gt;use advanced evaluation metrics to judge generated answers by the model.&lt;/li&gt; &#xA; &lt;li&gt;track and compare your model performance visually. In addition, &lt;a href=&#34;https://neptune.ai/&#34;&gt;Neptune&lt;/a&gt; integration can be used.&lt;/li&gt; &#xA; &lt;li&gt;chat with your model and get instant feedback on your model performance.&lt;/li&gt; &#xA; &lt;li&gt;easily export your model to the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt; and share it with the community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;For questions, discussing, or just hanging out, come and join our &lt;a href=&#34;https://discord.gg/WKhYMWcVbq&#34;&gt;Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;We offer several ways of getting started quickly.&lt;/p&gt; &#xA;&lt;p&gt;Using CLI for fine-tuning LLMs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/philippsinger/h2o-llm-studio-cli/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio/pull/12&#34;&gt;PR 12&lt;/a&gt;. Experiment configurations are now stored in yaml format, allowing for more flexibility in the configuration while making it much easier to be backward compatible. Old experiment configurations that are stored in pickle format will be converted to yaml format automatically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio/pull/40&#34;&gt;PR 40&lt;/a&gt; Added functionality for supporting nested conversations in data. A new &lt;code&gt;parent_id_column&lt;/code&gt; can be selected for datasets to support tree-like structures in your conversational data. Additional &lt;code&gt;augmentation&lt;/code&gt; settings have been added for this feature.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio/pull/131&#34;&gt;PR 132&lt;/a&gt; Add 4bit training that allows training of larger LLM backbones with less GPU memory. See &lt;a href=&#34;https://huggingface.co/blog/4bit-transformers-bitsandbytes&#34;&gt;here&lt;/a&gt; for a comprehensive summary of this method.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that due to current rapid development we cannot guarantee full backwards compatibility of new functionality. We thus recommend to pin the version of the framework to the one you used for your experiments. For resetting, please delete/backup your &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; folders.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;H2O LLM Studio requires a machine with Ubuntu 16.04+ and at least one recent Nvidia GPU with Nvidia drivers version &amp;gt;= 470.57.02. For larger models, we recommend at least 24GB of GPU memory.&lt;/p&gt; &#xA;&lt;h3&gt;Recommended Install&lt;/h3&gt; &#xA;&lt;p&gt;The recommended way to install H2O LLM Studio is using pipenv with Python 3.10. To install Python 3.10 on Ubuntu 16.04+, execute the following commands:&lt;/p&gt; &#xA;&lt;h4&gt;System installs (Python 3.10)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo add-apt-repository ppa:deadsnakes/ppa&#xA;sudo apt install python3.10&#xA;sudo apt-get install python3.10-distutils&#xA;curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Installing NVIDIA Drivers (if required)&lt;/h4&gt; &#xA;&lt;p&gt;If deploying on a &#39;bare metal&#39; machine running Ubuntu, one may need to install the required Nvidia drivers and CUDA. The following commands show how to retrieve the latest drivers for a machine running Ubuntu 20.04 as an example. One can update the following based on their OS.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin&#xA;sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600&#xA;wget https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda-repo-ubuntu2004-11-4-local_11.4.3-470.82.01-1_amd64.deb&#xA;sudo dpkg -i cuda-repo-ubuntu2004-11-4-local_11.4.3-470.82.01-1_amd64.deb&#xA;sudo apt-key add /var/cuda-repo-ubuntu2004-11-4-local/7fa2af80.pub&#xA;sudo apt-get -y update&#xA;sudo apt-get -y install cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create virtual environment (pipenv)&lt;/h4&gt; &#xA;&lt;p&gt;The following command will create a virtual environment using pipenv and will install the dependencies using pipenv:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using requirements.txt&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to use conda or another virtual environment, you can also install the dependencies using the requirements.txt file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run H2O LLM Studio GUI&lt;/h2&gt; &#xA;&lt;p&gt;You can start H2O LLM Studio using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make wave&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will start the &lt;a href=&#34;https://github.com/h2oai/wave&#34;&gt;H2O wave&lt;/a&gt; server and app. Navigate to &lt;a href=&#34;http://localhost:10101/&#34;&gt;http://localhost:10101/&lt;/a&gt; (we recommend using Chrome) to access H2O LLM Studio and start fine-tuning your models!&lt;/p&gt; &#xA;&lt;p&gt;If you are running H2O LLM Studio with a custom environment other than Pipenv, you need to start the app as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;H2O_WAVE_MAX_REQUEST_SIZE=25MB \&#xA;H2O_WAVE_NO_LOG=True \&#xA;H2O_WAVE_PRIVATE_DIR=&#34;/download/@output/download&#34; \&#xA;wave run app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run H2O LLM Studio GUI using Docker from a nightly build&lt;/h2&gt; &#xA;&lt;p&gt;Install Docker first by following instructions from &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker&#34;&gt;NVIDIA Containers&lt;/a&gt;. H2O LLM Studio images are stored in the h2oai GCR vorvan container repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p `pwd`/data&#xA;mkdir -p `pwd`/output&#xA;docker run \&#xA;    --runtime=nvidia \&#xA;    --shm-size=64g \&#xA;    --init \&#xA;    --rm \&#xA;    -u `id -u`:`id -g` \&#xA;    -p 10101:10101 \&#xA;    -v `pwd`/data:/workspace/data \&#xA;    -v `pwd`/output:/workspace/output \&#xA;    gcr.io/vorvan/h2oai/h2o-llmstudio:nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;http://localhost:10101/&#34;&gt;http://localhost:10101/&lt;/a&gt; (we recommend using Chrome) to access H2O LLM Studio and start fine-tuning your models!&lt;/p&gt; &#xA;&lt;p&gt;(Note other helpful docker commands are &lt;code&gt;docker ps&lt;/code&gt; and &lt;code&gt;docker kill&lt;/code&gt;.)&lt;/p&gt; &#xA;&lt;h2&gt;Run H2O LLM Studio GUI by building your own Docker image&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t h2o-llmstudio .&#xA;docker run \&#xA;    --runtime=nvidia \&#xA;    --shm-size=64g \&#xA;    --init \&#xA;    --rm \&#xA;    -u `id -u`:`id -g` \&#xA;    -p 10101:10101 \&#xA;    -v `pwd`/data:/workspace/data \&#xA;    -v `pwd`/output:/workspace/output \&#xA;    h2o-llmstudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run H2O LLM Studio with command line interface (CLI)&lt;/h2&gt; &#xA;&lt;p&gt;You can also use H2O LLM Studio with the command line interface (CLI) and specify the configuration file that contains all the experiment parameters. To finetune using H2O LLM Studio with CLI, activate the pipenv environment by running &lt;code&gt;make shell&lt;/code&gt;, and then use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py -C {path_to_config_file}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run on multiple GPUs in DDP mode, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash distributed_train.sh {NR_OF_GPUS} -C {path_to_config_file}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the framework will run on the first &lt;code&gt;k&lt;/code&gt; GPUs. If you want to specify specific GPUs to run on, use the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable before the command.&lt;/p&gt; &#xA;&lt;p&gt;To start an interactive chat with your trained model, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python prompt.py -e {experiment_name}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;experiment_name&lt;/code&gt; is the output folder of the experiment you want to chat with (see configuration). The interactive chat will also work with model that were finetuned using the UI.&lt;/p&gt; &#xA;&lt;h2&gt;Data Format&lt;/h2&gt; &#xA;&lt;p&gt;H2O LLM studio expects a csv file with at least two columns, one being the instruct column, the other being the answer that the model should generate. You can also provide an extra validation dataframe using the same format or use an automatic train/validation split to evaluate the model performance.&lt;/p&gt; &#xA;&lt;p&gt;During an experiment you can adapt the data representation with the following settings&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Column:&lt;/strong&gt; The column in the dataset containing the user prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Answer Column:&lt;/strong&gt; The column in the dataset containing the expected output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parent Id Column:&lt;/strong&gt; An optional column specifying the parent id to be used for chained conversations. The value of this column needs to match an additional column with the name &lt;code&gt;id&lt;/code&gt;. If provided, the prompt will be concatenated after preceeding parent rows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example data&lt;/h3&gt; &#xA;&lt;p&gt;We provide an example dataset (converted dataset from &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant/oasst1&lt;/a&gt;) that can be downloaded &lt;a href=&#34;https://www.kaggle.com/code/philippsinger/openassistant-conversations-dataset-oasst1?scriptVersionId=127047926&#34;&gt;here&lt;/a&gt;. It is recommended to use &lt;code&gt;train_full.csv&lt;/code&gt; for training. This dataset is also downloaded and prepared by default when first starting the GUI. Multiple dataframes can be uploaded into a single dataset by uploading a &lt;code&gt;.zip&lt;/code&gt; archive.&lt;/p&gt; &#xA;&lt;h2&gt;Training your model&lt;/h2&gt; &#xA;&lt;p&gt;With H2O LLM Studio, training your large language model is easy and intuitive. First, upload your dataset and then start training your model.&lt;/p&gt; &#xA;&lt;h3&gt;Starting an experiment&lt;/h3&gt; &#xA;&lt;p&gt;H2O LLM Studio allows to tune a variety of parameters and enables fast iterations to be able to explore different hyperparameters easily. The default settings are chosen with care and should give a good baseline. The most important parameters are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Backbone&lt;/strong&gt;: This parameter determines the LLM architecture to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mask Prompt Labels&lt;/strong&gt;: This option controls whether to mask the prompt labels during training and only train on the loss of the answer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hyperparameters&lt;/strong&gt; such as learning rate, batch size, and number of epochs determine the training process. Please consult the tooltips of each hyperparameter to learn more about them. The tooltips are shown next to each hyperparameter in the GUI and can be found as plain text &lt;code&gt;.mdx&lt;/code&gt; files in the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/tooltips/&#34;&gt;tooltips/&lt;/a&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluate Before Training&lt;/strong&gt; This option lets you evaluate the model before training, which can help you judge the quality of the LLM backbone before fine-tuning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide several metric options for evaluating the performance of your model. In addition to the BLEU score, we offer the GPT3.5 and GPT4 metrics that utilize the OpenAI API to determine whether the predicted answer is more favorable than the ground truth answer. To use these metrics, you can either export your OpenAI API key as an environment variable before starting LLM Studio, or you can specify it in the Settings Menu within the UI.&lt;/p&gt; &#xA;&lt;h3&gt;Monitoring the experiment&lt;/h3&gt; &#xA;&lt;p&gt;During the experiment, you can monitor the training progress and model performance in several ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Charts&lt;/strong&gt; tab displays train/validation loss, metrics, and learning rate.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Train Data Insights&lt;/strong&gt; tab shows you the first batch of the model to verify that the input data representation is correct.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Validation Prediction Insights&lt;/strong&gt; tab displays model predictions for random/best/worst validation samples. This tab is available after the first validation run.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logs&lt;/strong&gt; and &lt;strong&gt;Config&lt;/strong&gt; show the logs and the configuration of the experiment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt; tab lets you chat with your model and get instant feedback on its performance. This tab becomes available after the training is completed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Push to Hugging Face ü§ó&lt;/h3&gt; &#xA;&lt;p&gt;If you want to publish your model, you can export it with a single click to the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt; and share it with the community. To be able to push your model to the Hub, you need to have an API token with write access. You can also click the &lt;strong&gt;Download model&lt;/strong&gt; button to download the model locally. To use a converted model, you can use the following code snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;path_to_downloaded_model&#34;  # either local folder or huggingface model name&#xA;&#xA;# Important: The prompt needs to be in the same format the model was trained with.&#xA;# You can find an example prompt in the experiment logs.&#xA;prompt = &#34;&amp;lt;|prompt|&amp;gt;How are you?&amp;lt;|endoftext|&amp;gt;&amp;lt;|answer|&amp;gt;&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name)&#xA;model.cuda().eval()&#xA;&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;, add_special_tokens=False).to(&#34;cuda&#34;)&#xA;# generate configuration can be modified to your needs&#xA;tokens = model.generate(&#xA;    **inputs,&#xA;    max_new_tokens=256,&#xA;    temperature=0.3,&#xA;    repetition_penalty=1.2,&#xA;    num_beams=1&#xA;)[0]&#xA;tokens = tokens[inputs[&#34;input_ids&#34;].shape[1]:]&#xA;answer = tokenizer.decode(tokens, skip_special_tokens=True)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Compare experiments&lt;/h3&gt; &#xA;&lt;p&gt;In the &lt;strong&gt;View Experiments&lt;/strong&gt; view, you can compare your experiments and see how different model parameters affect the model performance. In addition, you can track your experiments with &lt;a href=&#34;https://neptune.ai/&#34;&gt;Neptune&lt;/a&gt; by enabling neptune logging when starting an experiment.&lt;/p&gt; &#xA;&lt;h2&gt;Example: Run on OASST data via CLI&lt;/h2&gt; &#xA;&lt;p&gt;As an example, you can run an experiment on the OASST data via CLI.&lt;/p&gt; &#xA;&lt;p&gt;First, get the training dataset (&lt;code&gt;train_full.csv&lt;/code&gt;) &lt;a href=&#34;https://www.kaggle.com/code/philippsinger/openassistant-conversations-dataset-oasst1?scriptVersionId=126228752&#34;&gt;here&lt;/a&gt; and place it into the &lt;code&gt;examples/data_oasst1&lt;/code&gt; folder; or download it directly via &lt;a href=&#34;https://www.kaggle.com/docs/api&#34;&gt;Kaggle API&lt;/a&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kaggle kernels output philippsinger/openassistant-conversations-dataset-oasst1 -p examples/data_oasst1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, go into the interactive shell. If not already done earlier, install the dependencies first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make setup  # installs all dependencies&#xA;make shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now run the experiment via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py -C examples/cfg_example_oasst1.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the experiment finishes, you can find all output artifacts in the &lt;code&gt;examples/output_oasst1&lt;/code&gt; folder. You can then use the &lt;code&gt;prompt.py&lt;/code&gt; script to chat with your model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python prompt.py -e examples/output_oasst1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;All open-source datasets and models are posted on &lt;a href=&#34;https://huggingface.co/h2oai/&#34;&gt;H2O.ai&#39;s Hugging Face page&lt;/a&gt; and our &lt;a href=&#34;https://github.com/h2oai/h2ogpt&#34;&gt;H2OGPT&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì How much data is generally required to fine-tune a model?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;There is no clear answer. As a rule of thumb, 1000 to 50000 samples of conversational data should be enough. Quality and diversity is very important. Make sure to try training on a subsample of data using the &#34;sample&#34; parameter to see how big the impact of the dataset size is. Recent &lt;a href=&#34;https://arxiv.org/abs/2305.11206&#34;&gt;studies&lt;/a&gt; suggest that less data is needed for larger foundation models.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì Is there any recommendations for which backbone to use? For example, are some better for certain types of tasks?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The majority of the LLM backbones are trained on a very similar corpus of data. The main difference is the size of the model and the number of parameters. Usually, the larger the model, the better they are. The larger models also take longer to train. We recommend starting with the smallest model and then increasing the size if the performance is not satisfactory. If you are looking to train for tasks that are not directly english question answering, it is also a good idea to look for specialized LLM backbones.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì What if my data is not in question and answer form, I just have documents? How can I fine-tune the LLM model?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To train a chatbot style model, you need to convert your data into a question and answer format.&lt;/p&gt; &#xA;&lt;p&gt;If you really want to continue pretraining on your own data without teaching a question answering style, prepare a dataset with all your data in a single column Dataframe. Make sure that the length of the text in each row is not too long. In the experiment setup, remove all additional tokens (e.g. &lt;code&gt;&amp;lt;|prompt|&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;|answer|&amp;gt;&lt;/code&gt;, for &lt;code&gt;Text Prompt Start&lt;/code&gt; and &lt;code&gt;Text Answer Start&lt;/code&gt; respectively) and disable &lt;code&gt;Add Eos Token To Prompt&lt;/code&gt; and &lt;code&gt;Add Eos Token To Answer&lt;/code&gt;. Deselect everything in the &lt;code&gt;Prompt Column&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Your setup should look like &lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio/assets/1069138/316c380d-76e4-4264-a64e-8ae9be893e76&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì I encounter GPU out-of-memory issues. What can I change to be able to train large models?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;There are various parameters that can be tuned while keeping a specific LLM backbone fixed. It is advised to choose 4bit/8bit precision as a backbone dtype to be able to train models &amp;gt;=7B on a consumer type GPU. LORA should be enabled. Besides that there are the usual parameters such as batch size and maximum sequence length that can be decreased to save GPU memory (please ensure that your prompt+answer text is not truncated too much by checking the train data insights).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì Where does H2O LLM Studio store its data?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;H2O LLM Studio stores its data in two folders located in the root directory. The folders are named &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt;. Here is the breakdown of the data storage structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;data/dbs&lt;/code&gt;: This folder contains the user database used within the app.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data/user&lt;/code&gt;: This folder is where uploaded datasets from the user are stored.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output/user&lt;/code&gt;: All experiments conducted in H2O LLM Studio are stored in this folder. For each experiment, a separate folder is created within the &lt;code&gt;output/user&lt;/code&gt; directory, which contains all the relevant data associated with that particular experiment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output/download&lt;/code&gt;: Utility folder that is used to store data the user downloads within the app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is possible to change the default folders &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/app_utils/config.py&#34;&gt;app_utils/config.py&lt;/a&gt; (change &lt;code&gt;data_folder&lt;/code&gt; and &lt;code&gt;output_folder&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùì How can I update H2O LLM Studio?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To update H2O LLM Studio, you have two options:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Using the latest main branch: Execute the commands &lt;code&gt;git checkout main&lt;/code&gt; and &lt;code&gt;git pull&lt;/code&gt; to obtain the latest updates from the main branch.&lt;/li&gt; &#xA; &lt;li&gt;Using the latest release tag: Execute the commands &lt;code&gt;git pull&lt;/code&gt; and &lt;code&gt;git checkout v0.0.3&lt;/code&gt; (replace &#39;v0.0.3&#39; with the desired version number) to switch to the latest release branch.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The update process does not remove or erase any existing data folders or experiment records. This means that all your old data, including the user database, uploaded datasets, and experiment results, will still be available to you within the updated version of H2O LLM Studio.&lt;/p&gt; &#xA;&lt;p&gt;Before updating, we recommend running the command &lt;code&gt;git rev-parse --short HEAD&lt;/code&gt; and saving the commit hash. This will allow you to revert to your existing version if needed.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;H2O LLM Studio is licensed under the Apache 2.0 license. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2o-llmstudio/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>XingangPan/DragGAN</title>
    <updated>2023-06-27T01:42:42Z</updated>
    <id>tag:github.com,2023-06-27:/XingangPan/DragGAN</id>
    <link href="https://github.com/XingangPan/DragGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Code for DragGAN (SIGGRAPH 2023)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://xingangpan.github.io/&#34;&gt;&lt;strong&gt;Xingang Pan&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://ayushtewari.com/&#34;&gt;&lt;strong&gt;Ayush Tewari&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://people.mpi-inf.mpg.de/~tleimkue/&#34;&gt;&lt;strong&gt;Thomas Leimk√ºhler&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;&lt;strong&gt;Lingjie Liu&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://www.meka.page/&#34;&gt;&lt;strong&gt;Abhimitra Meka&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;http://www.mpi-inf.mpg.de/~theobalt/&#34;&gt;&lt;strong&gt;Christian Theobalt&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;SIGGRAPH 2023 Conference Proceedings&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/XingangPan/DragGAN/main/DragGAN.gif&#34; , width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;img alt=&#34;PyTorch&#34; src=&#34;https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/XingangP&#34;&gt;&lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/XingangP?label=%40XingangP&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.10973&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&amp;amp;logo=adobeacrobatreader&amp;amp;logoWidth=20&amp;amp;logoColor=white&amp;amp;labelColor=66cc00&amp;amp;color=94DD15&#34; alt=&#34;Paper PDF&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/DragGAN/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/DragGAN-Page-orange?style=for-the-badge&amp;amp;logo=Google%20chrome&amp;amp;logoColor=white&amp;amp;labelColor=D35400&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/radames/DragGan&#34;&gt;&lt;img alt=&#34;Huggingface&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DragGAN-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1mey-IXPwQC_qSthI5hO-LTX7QL4ivtPh?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the requirements of &lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;https://github.com/NVlabs/stylegan3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download pre-trained StyleGAN2 weights&lt;/h2&gt; &#xA;&lt;p&gt;To download pre-trained weights, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh scripts/download_model.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: &lt;a href=&#34;https://drive.google.com/file/d/1dlFEHbu-WzQWJl7nBBZYcTyo000H9hVm/view?usp=sharing&#34;&gt;StyleGAN-Human&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/16twEf0T9QINAEoMsWefoWiyhcTd-aiWc/view?usp=sharing&#34;&gt;LHQ&lt;/a&gt;, and put them under &lt;code&gt;./checkpoints&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to try other pretrained StyleGAN.&lt;/p&gt; &#xA;&lt;h2&gt;Run DragGAN GUI&lt;/h2&gt; &#xA;&lt;p&gt;To start the DragGAN GUI, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh scripts/gui.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like &lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;PTI&lt;/a&gt;. Then load the new latent code and model weights to the GUI.&lt;/p&gt; &#xA;&lt;p&gt;You can run DragGAN Gradio demo as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python visualizer_drag_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This code is developed based on &lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;StyleGAN3&lt;/a&gt;. Part of the code is borrowed from &lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human&#34;&gt;StyleGAN-Human&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code related to the DragGAN algorithm is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;CC-BY-NC&lt;/a&gt;. However, most of this project are available under a separate license terms: all codes used or modified from &lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;StyleGAN3&lt;/a&gt; is under the &lt;a href=&#34;https://github.com/NVlabs/stylegan3/raw/main/LICENSE.txt&#34;&gt;Nvidia Source Code License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Any form of use and derivative of this code must preserve the watermarking functionality showing &#34;AI Generated&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{pan2023draggan,&#xA;    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold},&#xA;    author={Pan, Xingang and Tewari, Ayush, and Leimk{\&#34;u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},&#xA;    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>