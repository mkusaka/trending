<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-23T01:35:27Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yl4579/HiFTNet</title>
    <updated>2023-09-23T01:35:27Z</updated>
    <id>tag:github.com,2023-09-23:/yl4579/HiFTNet</id>
    <link href="https://github.com/yl4579/HiFTNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter and Inverse Short Time Fourier Transform&lt;/h1&gt; &#xA;&lt;h3&gt;Yinghao Aaron Li, Cong Han, Xilin Jiang, Nima Mesgarani&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Recent advancements in speech synthesis have leveraged GAN-based networks like HiFi-GAN and BigVGAN to produce high-fidelity waveforms from mel-spectrograms. However, these networks are computationally expensive and parameter-heavy. iSTFTNet addresses these limitations by integrating inverse short-time Fourier transform (iSTFT) into the network, achieving both speed and parameter efficiency. In this paper, we introduce an extension to iSTFTNet, termed HiFTNet, which incorporates a harmonic-plus-noise source filter in the time-frequency domain that uses a sinusoidal source from the fundamental frequency (F0) inferred via a pre-trained F0 estimation network for fast inference speed. Subjective evaluations on LJSpeech show that our model significantly outperforms both iSTFTNet and HiFi-GAN, achieving ground-truth-level performance. HiFTNet also outperforms BigVGAN-base on LibriTTS for unseen speakers and achieves comparable performance to BigVGAN while being four times faster with only 1/6 of the parameters. Our work sets a new benchmark for efficient, high-quality neural vocoding, paving the way for real-time applications that demand high quality speech synthesis.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2309.09493&#34;&gt;https://arxiv.org/abs/2309.09493&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audio samples: &lt;a href=&#34;https://hiftnet.github.io/&#34;&gt;https://hiftnet.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/yl4579/HiFTNet.git&#xA;cd HiFTNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install python requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py --config config_v1.json --[args]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the F0 model training, please refer to &lt;a href=&#34;https://github.com/yl4579/PitchExtractor&#34;&gt;yl4579/PitchExtractor&lt;/a&gt;. This repo includes a pre-trained F0 model on LibriTTS. Still, you may want to train your own F0 model for the best performance, particularly for noisy or non-speech data, as we found that F0 estimation accuracy is essential for the vocoder performance.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the notebook &lt;a href=&#34;https://github.com/yl4579/HiFTNet/raw/main/inference.ipynb&#34;&gt;inference.ipynb&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-Trained Models&lt;/h3&gt; &#xA;&lt;p&gt;You can download the pre-trained LJSpeech model &lt;a href=&#34;https://drive.google.com/drive/folders/1gW9Qba0jEdH_2E0UrHyqsaNEbRkDS4nQ?usp=sharing&#34;&gt;here&lt;/a&gt; and the pre-trained LibriTTS model &lt;a href=&#34;https://drive.google.com/drive/folders/1vOJKNpzYzbv8J_hh3ox5RvlhHjmf8PcX?usp=sharing&#34;&gt;here&lt;/a&gt;. The pre-trained models contain parameters of the optimizers and discriminators that can be used for fine-tuning.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rishikksh20/iSTFTNet-pytorch&#34;&gt;rishikksh20/iSTFTNet-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf&#34;&gt;nii-yamagishilab/project-NN-Pytorch-scripts/project/01-nsf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/alphafold</title>
    <updated>2023-09-23T01:35:27Z</updated>
    <id>tag:github.com,2023-09-23:/google-deepmind/alphafold</id>
    <link href="https://github.com/google-deepmind/alphafold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source code for AlphaFold.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/imgs/header.jpg&#34; alt=&#34;header&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;AlphaFold&lt;/h1&gt; &#xA;&lt;p&gt;This package provides an implementation of the inference pipeline of AlphaFold v2. For simplicity, we refer to this model as AlphaFold throughout the rest of this document.&lt;/p&gt; &#xA;&lt;p&gt;We also provide:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;An implementation of AlphaFold-Multimer. This represents a work in progress and AlphaFold-Multimer isn&#39;t expected to be as stable as our monomer AlphaFold system. &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#updating-existing-installation&#34;&gt;Read the guide&lt;/a&gt; for how to upgrade and update code.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/docs/technical_note_v2.3.0.md&#34;&gt;technical note&lt;/a&gt; containing the models and inference procedure for an updated AlphaFold v2.3.0.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/docs/casp15_predictions.zip&#34;&gt;CASP15 baseline&lt;/a&gt; set of predictions along with documentation of any manual interventions performed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Any publication that discloses findings arising from using this source code or the model parameters should &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#citing-this-work&#34;&gt;cite&lt;/a&gt; the &lt;a href=&#34;https://doi.org/10.1038/s41586-021-03819-2&#34;&gt;AlphaFold paper&lt;/a&gt; and, if applicable, the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1&#34;&gt;AlphaFold-Multimer paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please also refer to the &lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf&#34;&gt;Supplementary Information&lt;/a&gt; for a detailed description of the method.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can use a slightly simplified version of AlphaFold with &lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb&#34;&gt;this Colab notebook&lt;/a&gt;&lt;/strong&gt; or community-supported versions (see below).&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions, please contact the AlphaFold team at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/imgs/casp14_predictions.gif&#34; alt=&#34;CASP14 predictions&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation and running your first prediction&lt;/h2&gt; &#xA;&lt;p&gt;You will need a machine running Linux, AlphaFold does not support other operating systems. Full installation requires up to 3 TB of disk space to keep genetic databases (SSD storage is recommended) and a modern NVIDIA GPU (GPUs with more memory can predict larger protein structures).&lt;/p&gt; &#xA;&lt;p&gt;Please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; for GPU support.&lt;/li&gt; &#xA;   &lt;li&gt;Setup running &lt;a href=&#34;https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user&#34;&gt;Docker as a non-root user&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository and &lt;code&gt;cd&lt;/code&gt; into it.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/deepmind/alphafold.git&#xA;cd ./alphafold&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download genetic databases and model parameters:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Install &lt;code&gt;aria2c&lt;/code&gt;. On most Linux distributions it is available via the package manager as the &lt;code&gt;aria2&lt;/code&gt; package (on Debian-based distributions this can be installed by running &lt;code&gt;sudo apt install aria2&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Please use the script &lt;code&gt;scripts/download_all_data.sh&lt;/code&gt; to download and set up full databases. This may take substantial time (download size is 556 GB), so we recommend running this script in the background:&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/download_all_data.sh &amp;lt;DOWNLOAD_DIR&amp;gt; &amp;gt; download.log 2&amp;gt; download_all.log &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Note: The download directory &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt; should &lt;em&gt;not&lt;/em&gt; be a subdirectory in the AlphaFold repository directory.&lt;/strong&gt; If it is, the Docker build will be slow as the large databases will be copied into the docker build context.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;It is possible to run AlphaFold with reduced databases; please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#genetic-databases&#34;&gt;complete documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check that AlphaFold will be able to use a GPU by running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should show a list of your GPUs. If it doesn&#39;t, check if you followed all steps correctly when setting up the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; or take a look at the following &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker/issues/1447#issuecomment-801479573&#34;&gt;NVIDIA Docker issue&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you wish to run AlphaFold using Singularity (a common containerization platform on HPC systems) we recommend using some of the third party Singularity setups as linked in &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/10&#34;&gt;https://github.com/deepmind/alphafold/issues/10&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/24&#34;&gt;https://github.com/deepmind/alphafold/issues/24&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the Docker image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f docker/Dockerfile -t alphafold .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you encounter the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease: The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC&#xA;E: The repository &#39;https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease&#39; is not signed.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;use the workaround described in &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/463#issuecomment-1124881779&#34;&gt;https://github.com/deepmind/alphafold/issues/463#issuecomment-1124881779&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;code&gt;run_docker.py&lt;/code&gt; dependencies. Note: You may optionally wish to create a &lt;a href=&#34;https://docs.python.org/3/tutorial/venv.html&#34;&gt;Python Virtual Environment&lt;/a&gt; to prevent conflicts with your system&#39;s Python environment.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install -r docker/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure that the output directory exists (the default is &lt;code&gt;/tmp/alphafold&lt;/code&gt;) and that you have sufficient permissions to write into it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;run_docker.py&lt;/code&gt; pointing to a FASTA file containing the protein sequence(s) for which you wish to predict the structure (&lt;code&gt;--fasta_paths&lt;/code&gt; parameter). AlphaFold will search for the available templates before the date specified by the &lt;code&gt;--max_template_date&lt;/code&gt; parameter; this could be used to avoid certain templates during modeling. &lt;code&gt;--data_dir&lt;/code&gt; is the directory with downloaded genetic databases and &lt;code&gt;--output_dir&lt;/code&gt; is the absolute path to the output directory.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=your_protein.fasta \&#xA;  --max_template_date=2022-01-01 \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the run is over, the output directory shall contain predicted structures of the target protein. Please check the documentation below for additional options and troubleshooting tips.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Genetic databases&lt;/h3&gt; &#xA;&lt;p&gt;This step requires &lt;code&gt;aria2c&lt;/code&gt; to be installed on your machine.&lt;/p&gt; &#xA;&lt;p&gt;AlphaFold needs multiple genetic (sequence) databases to run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebi.ac.uk/metagenomics/&#34;&gt;MGnify&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/&#34;&gt;PDB70&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rcsb.org/&#34;&gt;PDB&lt;/a&gt; (structures in the mmCIF format),&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rcsb.org/&#34;&gt;PDB seqres&lt;/a&gt; â€“ only for AlphaFold-Multimer,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://uniclust.mmseqs.com/&#34;&gt;UniRef30 (FKA UniClust30)&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uniprot.org/uniprot/&#34;&gt;UniProt&lt;/a&gt; â€“ only for AlphaFold-Multimer,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uniprot.org/help/uniref&#34;&gt;UniRef90&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide a script &lt;code&gt;scripts/download_all_data.sh&lt;/code&gt; that can be used to download and set up all of these databases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Recommended default:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/download_all_data.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will download the full databases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;With &lt;code&gt;reduced_dbs&lt;/code&gt; parameter:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/download_all_data.sh &amp;lt;DOWNLOAD_DIR&amp;gt; reduced_dbs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will download a reduced version of the databases to be used with the &lt;code&gt;reduced_dbs&lt;/code&gt; database preset. This shall be used with the corresponding AlphaFold parameter &lt;code&gt;--db_preset=reduced_dbs&lt;/code&gt; later during the AlphaFold run (please see &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#running-alphafold&#34;&gt;AlphaFold parameters&lt;/a&gt; section).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ“’&lt;/span&gt; &lt;strong&gt;Note: The download directory &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt; should &lt;em&gt;not&lt;/em&gt; be a subdirectory in the AlphaFold repository directory.&lt;/strong&gt; If it is, the Docker build will be slow as the large databases will be copied during the image creation.&lt;/p&gt; &#xA;&lt;p&gt;We don&#39;t provide exactly the database versions used in CASP14 â€“ see the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#note-on-casp14-reproducibility&#34;&gt;note on reproducibility&lt;/a&gt;. Some of the databases are mirrored for speed, see &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#mirrored-databases&#34;&gt;mirrored databases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ“’&lt;/span&gt; &lt;strong&gt;Note: The total download size for the full databases is around 556 GB and the total size when unzipped is 2.62 TB. Please make sure you have a large enough hard drive space, bandwidth and time to download. We recommend using an SSD for better genetic search performance.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ“’&lt;/span&gt; &lt;strong&gt;Note: If the download directory and datasets don&#39;t have full read and write permissions, it can cause errors with the MSA tools, with opaque (external) error messages. Please ensure the required permissions are applied, e.g. with the &lt;code&gt;sudo chmod 755 --recursive &#34;$DOWNLOAD_DIR&#34;&lt;/code&gt; command.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;download_all_data.sh&lt;/code&gt; script will also download the model parameter files. Once the script has finished, you should have the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$DOWNLOAD_DIR/                             # Total: ~ 2.62 TB (download: 556 GB)&#xA;    bfd/                                   # ~ 1.8 TB (download: 271.6 GB)&#xA;        # 6 files.&#xA;    mgnify/                                # ~ 120 GB (download: 67 GB)&#xA;        mgy_clusters_2022_05.fa&#xA;    params/                                # ~ 5.3 GB (download: 5.3 GB)&#xA;        # 5 CASP14 models,&#xA;        # 5 pTM models,&#xA;        # 5 AlphaFold-Multimer models,&#xA;        # LICENSE,&#xA;        # = 16 files.&#xA;    pdb70/                                 # ~ 56 GB (download: 19.5 GB)&#xA;        # 9 files.&#xA;    pdb_mmcif/                             # ~ 238 GB (download: 43 GB)&#xA;        mmcif_files/&#xA;            # About 199,000 .cif files.&#xA;        obsolete.dat&#xA;    pdb_seqres/                            # ~ 0.2 GB (download: 0.2 GB)&#xA;        pdb_seqres.txt&#xA;    small_bfd/                             # ~ 17 GB (download: 9.6 GB)&#xA;        bfd-first_non_consensus_sequences.fasta&#xA;    uniref30/                              # ~ 206 GB (download: 52.5 GB)&#xA;        # 7 files.&#xA;    uniprot/                               # ~ 105 GB (download: 53 GB)&#xA;        uniprot.fasta&#xA;    uniref90/                              # ~ 67 GB (download: 34 GB)&#xA;        uniref90.fasta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;bfd/&lt;/code&gt; is only downloaded if you download the full databases, and &lt;code&gt;small_bfd/&lt;/code&gt; is only downloaded if you download the reduced databases.&lt;/p&gt; &#xA;&lt;h3&gt;Model parameters&lt;/h3&gt; &#xA;&lt;p&gt;While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#license-and-disclaimer&#34;&gt;Disclaimer&lt;/a&gt; below for more detail.&lt;/p&gt; &#xA;&lt;p&gt;The AlphaFold parameters are available from &lt;a href=&#34;https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar&#34;&gt;https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar&lt;/a&gt;, and are downloaded as part of the &lt;code&gt;scripts/download_all_data.sh&lt;/code&gt; script. This script will download parameters for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5 models which were used during CASP14, and were extensively validated for structure prediction quality (see Jumper et al. 2021, Suppl. Methods 1.12 for details).&lt;/li&gt; &#xA; &lt;li&gt;5 pTM models, which were fine-tuned to produce pTM (predicted TM-score) and (PAE) predicted aligned error values alongside their structure predictions (see Jumper et al. 2021, Suppl. Methods 1.9.7 for details).&lt;/li&gt; &#xA; &lt;li&gt;5 AlphaFold-Multimer models that produce pTM and PAE values alongside their structure predictions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updating existing installation&lt;/h3&gt; &#xA;&lt;p&gt;If you have a previous version you can either reinstall fully from scratch (remove everything and run the setup from scratch) or you can do an incremental update that will be significantly faster but will require a bit more work. Make sure you follow these steps in the exact order they are listed below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the code.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Go to the directory with the cloned AlphaFold repository and run &lt;code&gt;git fetch origin main&lt;/code&gt; to get all code updates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the UniProt, UniRef, MGnify and PDB seqres databases.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/uniprot&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_uniprot.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/uniclust30&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_uniref30.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/uniref90&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_uniref90.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/mgnify&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_mgnify.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/pdb_mmcif&lt;/code&gt;. It is needed to have PDB SeqRes and PDB from exactly the same date. Failure to do this step will result in potential errors when searching for templates when running AlphaFold-Multimer.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_pdb_mmcif.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_pdb_seqres.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the model parameters.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove the old model parameters in &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/params&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Download new model parameters using &lt;code&gt;scripts/download_alphafold_params.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#running-alphafold&#34;&gt;Running AlphaFold&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Using deprecated model weights&lt;/h4&gt; &#xA;&lt;p&gt;To use the deprecated v2.2.0 AlphaFold-Multimer model weights:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Change &lt;code&gt;SOURCE_URL&lt;/code&gt; in &lt;code&gt;scripts/download_alphafold_params.sh&lt;/code&gt; to &lt;code&gt;https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar&lt;/code&gt;, and download the old parameters.&lt;/li&gt; &#xA; &lt;li&gt;Change the &lt;code&gt;_v3&lt;/code&gt; to &lt;code&gt;_v2&lt;/code&gt; in the multimer &lt;code&gt;MODEL_PRESETS&lt;/code&gt; in &lt;code&gt;config.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To use the deprecated v2.1.0 AlphaFold-Multimer model weights:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Change &lt;code&gt;SOURCE_URL&lt;/code&gt; in &lt;code&gt;scripts/download_alphafold_params.sh&lt;/code&gt; to &lt;code&gt;https://storage.googleapis.com/alphafold/alphafold_params_2022-01-19.tar&lt;/code&gt;, and download the old parameters.&lt;/li&gt; &#xA; &lt;li&gt;Remove the &lt;code&gt;_v3&lt;/code&gt; in the multimer &lt;code&gt;MODEL_PRESETS&lt;/code&gt; in &lt;code&gt;config.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running AlphaFold&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The simplest way to run AlphaFold is using the provided Docker script.&lt;/strong&gt; This was tested on Google Cloud with a machine using the &lt;code&gt;nvidia-gpu-cloud-image&lt;/code&gt; with 12 vCPUs, 85 GB of RAM, a 100 GB boot disk, the databases on an additional 3 TB disk, and an A100 GPU. For your first run, please follow the instructions from &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#installation-and-running-your-first-prediction&#34;&gt;Installation and running your first prediction&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;By default, Alphafold will attempt to use all visible GPU devices. To use a subset, specify a comma-separated list of GPU UUID(s) or index(es) using the &lt;code&gt;--gpu_devices&lt;/code&gt; flag. See &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#gpu-enumeration&#34;&gt;GPU enumeration&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can control which AlphaFold model to run by adding the &lt;code&gt;--model_preset=&lt;/code&gt; flag. We provide the following models:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer&lt;/strong&gt;: This is the original model used at CASP14 with no ensembling.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer_casp14&lt;/strong&gt;: This is the original model used at CASP14 with &lt;code&gt;num_ensemble=8&lt;/code&gt;, matching our CASP14 configuration. This is largely provided for reproducibility as it is 8x more computationally expensive for limited accuracy gain (+0.1 average GDT gain on CASP14 domains).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer_ptm&lt;/strong&gt;: This is the original CASP14 model fine tuned with the pTM head, providing a pairwise confidence measure. It is slightly less accurate than the normal monomer model.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;multimer&lt;/strong&gt;: This is the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#citing-this-work&#34;&gt;AlphaFold-Multimer&lt;/a&gt; model. To use this model, provide a multi-sequence FASTA file. In addition, the UniProt database should have been downloaded.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can control MSA speed/quality tradeoff by adding &lt;code&gt;--db_preset=reduced_dbs&lt;/code&gt; or &lt;code&gt;--db_preset=full_dbs&lt;/code&gt; to the run command. We provide the following presets:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;reduced_dbs&lt;/strong&gt;: This preset is optimized for speed and lower hardware requirements. It runs with a reduced version of the BFD database. It requires 8 CPU cores (vCPUs), 8 GB of RAM, and 600 GB of disk space.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;full_dbs&lt;/strong&gt;: This runs with all genetic databases used at CASP14.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Running the command above with the &lt;code&gt;monomer&lt;/code&gt; model preset and the &lt;code&gt;reduced_dbs&lt;/code&gt; data preset would look like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=T1050.fasta \&#xA;  --max_template_date=2020-05-14 \&#xA;  --model_preset=monomer \&#xA;  --db_preset=reduced_dbs \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After generating the predicted model, AlphaFold runs a relaxation step to improve local geometry. By default, only the best model (by pLDDT) is relaxed (&lt;code&gt;--models_to_relax=best&lt;/code&gt;), but also all of the models (&lt;code&gt;--models_to_relax=all&lt;/code&gt;) or none of the models (&lt;code&gt;--models_to_relax=none&lt;/code&gt;) can be relaxed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The relaxation step can be run on GPU (faster, but could be less stable) or CPU (slow, but stable). This can be controlled with &lt;code&gt;--enable_gpu_relax=true&lt;/code&gt; (default) or &lt;code&gt;--enable_gpu_relax=false&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;AlphaFold can re-use MSAs (multiple sequence alignments) for the same sequence via &lt;code&gt;--use_precomputed_msas=true&lt;/code&gt; option; this can be useful for trying different AlphaFold parameters. This option assumes that the directory structure generated by the first AlphaFold run in the output directory exists and that the protein sequence is the same.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running AlphaFold-Multimer&lt;/h3&gt; &#xA;&lt;p&gt;All steps are the same as when running the monomer system, but you will have to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;provide an input fasta with multiple sequences,&lt;/li&gt; &#xA; &lt;li&gt;set &lt;code&gt;--model_preset=multimer&lt;/code&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example that folds a protein complex &lt;code&gt;multimer.fasta&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=multimer.fasta \&#xA;  --max_template_date=2020-05-14 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the multimer system will run 5 seeds per model (25 total predictions) for a small drop in accuracy you may wish to run a single seed per model. This can be done via the &lt;code&gt;--num_multimer_predictions_per_model&lt;/code&gt; flag, e.g. set it to &lt;code&gt;--num_multimer_predictions_per_model=1&lt;/code&gt; to run a single seed per model.&lt;/p&gt; &#xA;&lt;h3&gt;AlphaFold prediction speed&lt;/h3&gt; &#xA;&lt;p&gt;The table below reports prediction runtimes for proteins of various lengths. We only measure unrelaxed structure prediction with three recycles while excluding runtimes from MSA and template search. When running &lt;code&gt;docker/run_docker.py&lt;/code&gt; with &lt;code&gt;--benchmark=true&lt;/code&gt;, this runtime is stored in &lt;code&gt;timings.json&lt;/code&gt;. All runtimes are from a single A100 NVIDIA GPU. Prediction speed on A100 for smaller structures can be improved by increasing &lt;code&gt;global_config.subbatch_size&lt;/code&gt; in &lt;code&gt;alphafold/model/config.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;No. residues&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prediction time (s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;200&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;400&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;500&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;600&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;700&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;800&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;900&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;91&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,000&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;140&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,500&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;280&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2,000&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;450&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2,500&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;969&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3,000&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,240&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3,500&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2,465&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4,000&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5,660&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4,500&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12,475&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5,000&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18,824&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Below are examples on how to use AlphaFold in different scenarios.&lt;/p&gt; &#xA;&lt;h4&gt;Folding a monomer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a monomer with the sequence &lt;code&gt;&amp;lt;SEQUENCE&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_name&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=monomer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=monomer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding a homomer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a homomer with 3 copies of the same sequence &lt;code&gt;&amp;lt;SEQUENCE&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_1&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&amp;gt;sequence_2&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&amp;gt;sequence_3&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=homomer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding a heteromer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have an A2B3 heteromer, i.e. with 2 copies of &lt;code&gt;&amp;lt;SEQUENCE A&amp;gt;&lt;/code&gt; and 3 copies of &lt;code&gt;&amp;lt;SEQUENCE B&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_1&#xA;&amp;lt;SEQUENCE A&amp;gt;&#xA;&amp;gt;sequence_2&#xA;&amp;lt;SEQUENCE A&amp;gt;&#xA;&amp;gt;sequence_3&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&amp;gt;sequence_4&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&amp;gt;sequence_5&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=heteromer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding multiple monomers one after another&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a two monomers, &lt;code&gt;monomer1.fasta&lt;/code&gt; and &lt;code&gt;monomer2.fasta&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can fold both sequentially by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=monomer1.fasta,monomer2.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=monomer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding multiple multimers one after another&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a two multimers, &lt;code&gt;multimer1.fasta&lt;/code&gt; and &lt;code&gt;multimer2.fasta&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can fold both sequentially by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=multimer1.fasta,multimer2.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR \&#xA;  --output_dir=/home/user/absolute_path_to_the_output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;AlphaFold output&lt;/h3&gt; &#xA;&lt;p&gt;The outputs will be saved in a subdirectory of the directory provided via the &lt;code&gt;--output_dir&lt;/code&gt; flag of &lt;code&gt;run_docker.py&lt;/code&gt; (defaults to &lt;code&gt;/tmp/alphafold/&lt;/code&gt;). The outputs include the computed MSAs, unrelaxed structures, relaxed structures, ranked structures, raw model outputs, prediction metadata, and section timings. The &lt;code&gt;--output_dir&lt;/code&gt; directory will have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;target_name&amp;gt;/&#xA;    features.pkl&#xA;    ranked_{0,1,2,3,4}.pdb&#xA;    ranking_debug.json&#xA;    relax_metrics.json&#xA;    relaxed_model_{1,2,3,4,5}.pdb&#xA;    result_model_{1,2,3,4,5}.pkl&#xA;    timings.json&#xA;    unrelaxed_model_{1,2,3,4,5}.pdb&#xA;    msas/&#xA;        bfd_uniref_hits.a3m&#xA;        mgnify_hits.sto&#xA;        uniref90_hits.sto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The contents of each output file are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;features.pkl&lt;/code&gt; â€“ A &lt;code&gt;pickle&lt;/code&gt; file containing the input feature NumPy arrays used by the models to produce the structures.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;unrelaxed_model_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the predicted structure, exactly as outputted by the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;relaxed_model_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the predicted structure, after performing an Amber relaxation procedure on the unrelaxed structure prediction (see Jumper et al. 2021, Suppl. Methods 1.8.6 for details).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ranked_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the predicted structures, after reordering by model confidence. Here &lt;code&gt;ranked_i.pdb&lt;/code&gt; should contain the prediction with the (&lt;code&gt;i + 1&lt;/code&gt;)-th highest confidence (so that &lt;code&gt;ranked_0.pdb&lt;/code&gt; has the highest confidence). To rank model confidence, we use predicted LDDT (pLDDT) scores (see Jumper et al. 2021, Suppl. Methods 1.9.6 for details). If &lt;code&gt;--models_to_relax=all&lt;/code&gt; then all ranked structures are relaxed. If &lt;code&gt;--models_to_relax=best&lt;/code&gt; then only &lt;code&gt;ranked_0.pdb&lt;/code&gt; is relaxed (the rest are unrelaxed). If &lt;code&gt;--models_to_relax=none&lt;/code&gt;, then the ranked structures are all unrelaxed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ranking_debug.json&lt;/code&gt; â€“ A JSON format text file containing the pLDDT values used to perform the model ranking, and a mapping back to the original model names.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;relax_metrics.json&lt;/code&gt; â€“ A JSON format text file containing relax metrics, for instance remaining violations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;timings.json&lt;/code&gt; â€“ A JSON format text file containing the times taken to run each section of the AlphaFold pipeline.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;msas/&lt;/code&gt; - A directory containing the files describing the various genetic tool hits that were used to construct the input MSA.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;result_model_*.pkl&lt;/code&gt; â€“ A &lt;code&gt;pickle&lt;/code&gt; file containing a nested dictionary of the various NumPy arrays directly produced by the model. In addition to the output of the structure module, this includes auxiliary outputs such as:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Distograms (&lt;code&gt;distogram/logits&lt;/code&gt; contains a NumPy array of shape [N_res, N_res, N_bins] and &lt;code&gt;distogram/bin_edges&lt;/code&gt; contains the definition of the bins).&lt;/li&gt; &#xA;   &lt;li&gt;Per-residue pLDDT scores (&lt;code&gt;plddt&lt;/code&gt; contains a NumPy array of shape [N_res] with the range of possible values from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;100&lt;/code&gt;, where &lt;code&gt;100&lt;/code&gt; means most confident). This can serve to identify sequence regions predicted with high confidence or as an overall per-target confidence score when averaged across residues.&lt;/li&gt; &#xA;   &lt;li&gt;Present only if using pTM models: predicted TM-score (&lt;code&gt;ptm&lt;/code&gt; field contains a scalar). As a predictor of a global superposition metric, this score is designed to also assess whether the model is confident in the overall domain packing.&lt;/li&gt; &#xA;   &lt;li&gt;Present only if using pTM models: predicted pairwise aligned errors (&lt;code&gt;predicted_aligned_error&lt;/code&gt; contains a NumPy array of shape [N_res, N_res] with the range of possible values from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;max_predicted_aligned_error&lt;/code&gt;, where &lt;code&gt;0&lt;/code&gt; means most confident). This can serve for a visualisation of domain packing confidence within the structure.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The pLDDT confidence measure is stored in the B-factor field of the output PDB files (although unlike a B-factor, higher pLDDT is better, so care must be taken when using for tasks such as molecular replacement).&lt;/p&gt; &#xA;&lt;p&gt;This code has been tested to match mean top-1 accuracy on a CASP14 test set with pLDDT ranking over 5 model predictions (some CASP targets were run with earlier versions of AlphaFold and some had manual interventions; see our forthcoming publication for details). Some targets such as T1064 may also have high individual run variance over random seeds.&lt;/p&gt; &#xA;&lt;h2&gt;Inferencing many proteins&lt;/h2&gt; &#xA;&lt;p&gt;The provided inference script is optimized for predicting the structure of a single protein, and it will compile the neural network to be specialized to exactly the size of the sequence, MSA, and templates. For large proteins, the compile time is a negligible fraction of the runtime, but it may become more significant for small proteins or if the multi-sequence alignments are already precomputed. In the bulk inference case, it may make sense to use our &lt;code&gt;make_fixed_size&lt;/code&gt; function to pad the inputs to a uniform size, thereby reducing the number of compilations required.&lt;/p&gt; &#xA;&lt;p&gt;We do not provide a bulk inference script, but it should be straightforward to develop on top of the &lt;code&gt;RunModel.predict&lt;/code&gt; method with a parallel system for precomputing multi-sequence alignments. Alternatively, this script can be run repeatedly with only moderate overhead.&lt;/p&gt; &#xA;&lt;h2&gt;Note on CASP14 reproducibility&lt;/h2&gt; &#xA;&lt;p&gt;AlphaFold&#39;s output for a small number of proteins has high inter-run variance, and may be affected by changes in the input data. The CASP14 target T1064 is a notable example; the large number of SARS-CoV-2-related sequences recently deposited changes its MSA significantly. This variability is somewhat mitigated by the model selection process; running 5 models and taking the most confident.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the results of our CASP14 system as closely as possible you must use the same database versions we used in CASP. These may not match the default versions downloaded by our scripts.&lt;/p&gt; &#xA;&lt;p&gt;For genetics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UniRef90: &lt;a href=&#34;https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2020_01/uniref/&#34;&gt;v2020_01&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MGnify: &lt;a href=&#34;http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/&#34;&gt;v2018_12&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Uniclust30: &lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/&#34;&gt;v2018_08&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;BFD: &lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;only version available&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For templates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDB: (downloaded 2020-05-14)&lt;/li&gt; &#xA; &lt;li&gt;PDB70: &lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200513.tar.gz&#34;&gt;2020-05-13&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An alternative for templates is to use the latest PDB and PDB70, but pass the flag &lt;code&gt;--max_template_date=2020-05-14&lt;/code&gt;, which restricts templates only to structures that were available at the start of CASP14.&lt;/p&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;p&gt;If you use the code or data in this package, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaFold2021,&#xA;  author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\&#39;\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},&#xA;  journal = {Nature},&#xA;  title   = {Highly accurate protein structure prediction with {AlphaFold}},&#xA;  year    = {2021},&#xA;  volume  = {596},&#xA;  number  = {7873},&#xA;  pages   = {583--589},&#xA;  doi     = {10.1038/s41586-021-03819-2}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition, if you use the AlphaFold-Multimer mode, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article {AlphaFold-Multimer2021,&#xA;  author       = {Evans, Richard and O{\textquoteright}Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and {\v{Z}}{\&#39;\i}dek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},&#xA;  journal      = {bioRxiv},&#xA;  title        = {Protein complex prediction with AlphaFold-Multimer},&#xA;  year         = {2021},&#xA;  elocation-id = {2021.10.04.463034},&#xA;  doi          = {10.1101/2021.10.04.463034},&#xA;  URL          = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034},&#xA;  eprint       = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034.full.pdf},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community contributions&lt;/h2&gt; &#xA;&lt;p&gt;Colab notebooks provided by the community (please note that these notebooks may vary from our full AlphaFold system and we did not validate their accuracy):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb&#34;&gt;ColabFold AlphaFold2 notebook&lt;/a&gt; by Martin Steinegger, Sergey Ovchinnikov and Milot Mirdita, which uses an API hosted at the SÃ¶dinglab based on the MMseqs2 server &lt;a href=&#34;https://academic.oup.com/bioinformatics/article/35/16/2856/5280135&#34;&gt;(Mirdita et al. 2019, Bioinformatics)&lt;/a&gt; for the multiple sequence alignment creation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;AlphaFold communicates with and/or references the following separate libraries and packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abseil/abseil-py&#34;&gt;Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://biopython.org&#34;&gt;Biopython&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/chex&#34;&gt;Chex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/soedinglab/hh-suite&#34;&gt;HH Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://eddylab.org/software/hmmer&#34;&gt;HMMER Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/dm-haiku&#34;&gt;Haiku&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/corenting/immutabledict&#34;&gt;Immutabledict&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/jax/&#34;&gt;JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://msa.sbc.su.se/cgi-bin/msa.cgi&#34;&gt;Kalign&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/ml_collections&#34;&gt;ML Collections&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openmm/openmm&#34;&gt;OpenMM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openstructure.org&#34;&gt;OpenStructure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/avirshup/py3dmol&#34;&gt;pymol3d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scipy.org&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/sonnet&#34;&gt;Sonnet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tree&#34;&gt;Tree&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tqdm/tqdm&#34;&gt;tqdm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank all their contributors and maintainers!&lt;/p&gt; &#xA;&lt;h2&gt;Get in Touch&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions not covered in this overview, please contact the AlphaFold team at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We would love to hear your feedback and understand how AlphaFold has been useful in your research. Share your stories with us at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License and Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2022 DeepMind Technologies Limited.&lt;/p&gt; &#xA;&lt;h3&gt;AlphaFold Code License&lt;/h3&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h3&gt;Model Parameters License&lt;/h3&gt; &#xA;&lt;p&gt;The AlphaFold parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Third-party software&lt;/h3&gt; &#xA;&lt;p&gt;Use of the third-party software, libraries or code referred to in the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphafold/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt; section above may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use.&lt;/p&gt; &#xA;&lt;h3&gt;Mirrored Databases&lt;/h3&gt; &#xA;&lt;p&gt;The following databases have been mirrored by DeepMind, and are available with reference to the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt; (unmodified), by Steinegger M. and SÃ¶ding J., available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt; (modified), by Steinegger M. and SÃ¶ding J., modified by DeepMind, available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;. See the Methods section of the &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03828-1&#34;&gt;AlphaFold proteome paper&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/uniclust/2021_03/&#34;&gt;Uniref30: v2021_03&lt;/a&gt; (unmodified), by Mirdita M. et al., available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2022_05/README.txt&#34;&gt;MGnify: v2022_05&lt;/a&gt; (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;CC0 1.0 Universal (CC0 1.0) Public Domain Dedication&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>zhudotexe/kani</title>
    <updated>2023-09-23T01:35:27Z</updated>
    <id>tag:github.com,2023-09-23:/zhudotexe/kani</id>
    <link href="https://github.com/zhudotexe/kani" rel="alternate"></link>
    <summary type="html">&lt;p&gt;kani (ã‚«ãƒ‹) is a highly hackable microframework for chat-based language models with tool usage/function calling.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;256&#34; height=&#34;256&#34; alt=&#34;kani&#34; src=&#34;https://raw.githubusercontent.com/zhudotexe/kani/main/docs/_static/kani-logo@256.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml&#34;&gt; &lt;img alt=&#34;Test Package&#34; src=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://kani.readthedocs.io/en/latest/?badge=latest&#34;&gt; &lt;img alt=&#34;Documentation Status&#34; src=&#34;https://readthedocs.org/projects/kani/badge/?version=latest&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/kani/&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/kani&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/zhudotexe/kani/blob/main/examples/colab_examples.ipynb&#34;&gt; &lt;img alt=&#34;Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/eTepTNDxYT&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1150902904773935214?color=5865F2&amp;amp;label=discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;kani (ã‚«ãƒ‹)&lt;/h1&gt; &#xA;&lt;p&gt;kani (ã‚«ãƒ‹) is a lightweight and highly hackable framework for chat-based language models with tool usage/function calling.&lt;/p&gt; &#xA;&lt;p&gt;Compared to other LM frameworks, kani is less opinionated and offers more fine-grained customizability over the parts of the control flow that matter, making it the perfect choice for NLP researchers, hobbyists, and developers alike.&lt;/p&gt; &#xA;&lt;p&gt;kani comes with support for OpenAI models and LLaMA v2 out of the box, with a model-agnostic framework to add support for many more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://kani.readthedocs.io/&#34;&gt;Read the docs on ReadTheDocs!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05542&#34;&gt;Read our preprint on arXiv!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight and high-level&lt;/strong&gt; - kani implements common boilerplate to interface with language models without forcing you to use opinionated prompt frameworks or complex library-specific tooling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model agnostic&lt;/strong&gt; - kani provides a simple interface to implement: token counting and completion generation. Implement these two, and kani can run with any language model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic chat memory management&lt;/strong&gt; - Allow chat sessions to flow without worrying about managing the number of tokens in the history - kani takes care of it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Function calling with model feedback and retry&lt;/strong&gt; - Give models access to functions in just one line of code. kani elegantly provides feedback about hallucinated parameters and errors and allows the model to retry calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;You control the prompts&lt;/strong&gt; - There are no hidden prompt hacks. We will never decide for you how to format your own data, unlike other popular language model libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast to iterate and intuitive to learn&lt;/strong&gt; - With kani, you only write Python - we handle the rest.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous design from the start&lt;/strong&gt; - kani can scale to run multiple chat sessions in parallel easily, without having to manage multiple processes or programs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;a href=&#34;https://colab.research.google.com/github/zhudotexe/kani/blob/main/examples/colab_examples.ipynb&#34;&gt; &lt;img alt=&#34;Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;kani requires Python 3.10 or above.&lt;/p&gt; &#xA;&lt;p&gt;First, install the library. In this quickstart, we&#39;ll use the OpenAI engine, though kani is &lt;a href=&#34;https://kani.readthedocs.io/en/latest/engines.html&#34;&gt;model-agnostic&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install &#34;kani[openai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, let&#39;s use kani to create a simple chatbot using ChatGPT as a backend.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import the library&#xA;from kani import Kani, chat_in_terminal&#xA;from kani.engines.openai import OpenAIEngine&#xA;&#xA;# Replace this with your OpenAI API key: https://platform.openai.com/account/api-keys&#xA;api_key = &#34;sk-...&#34;&#xA;&#xA;# kani uses an Engine to interact with the language model. You can specify other model &#xA;# parameters here, like temperature=0.7.&#xA;engine = OpenAIEngine(api_key, model=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;# The kani manages the chat state, prompting, and function calling. Here, we only give &#xA;# it the engine to call ChatGPT, but you can specify other parameters like &#xA;# system_prompt=&#34;You are...&#34; here.&#xA;ai = Kani(engine)&#xA;&#xA;# kani comes with a utility to interact with a kani through your terminal! Check out &#xA;# the docs for how to use kani programmatically.&#xA;chat_in_terminal(ai)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;kani makes the time to set up a working chat model short, while offering the programmer deep customizability over every prompt, function call, and even the underlying language model.&lt;/p&gt; &#xA;&lt;h2&gt;Function Calling&lt;/h2&gt; &#xA;&lt;p&gt;Function calling gives language models the ability to choose when to call a function you provide based off its documentation.&lt;/p&gt; &#xA;&lt;p&gt;With kani, you can write functions in Python and expose them to the model with just one line of code: the &lt;code&gt;@ai_function&lt;/code&gt; decorator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import the library&#xA;from typing import Annotated&#xA;from kani import AIParam, Kani, ai_function, chat_in_terminal&#xA;from kani.engines.openai import OpenAIEngine&#xA;&#xA;# set up the engine as above&#xA;api_key = &#34;sk-...&#34;&#xA;engine = OpenAIEngine(api_key, model=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;&#xA;# subclass Kani to add AI functions&#xA;class MyKani(Kani):&#xA;    # Adding the annotation to a method exposes it to the AI&#xA;    @ai_function()&#xA;    def get_weather(&#xA;        self,&#xA;        # and you can provide extra documentation about specific parameters&#xA;        location: Annotated[str, AIParam(desc=&#34;The city and state, e.g. San Francisco, CA&#34;)],&#xA;    ):&#xA;        &#34;&#34;&#34;Get the current weather in a given location.&#34;&#34;&#34;&#xA;        # In this example, we mock the return, but you could call a real weather API&#xA;        return f&#34;Weather in {location}: Sunny, 72 degrees fahrenheit.&#34;&#xA;&#xA;&#xA;ai = MyKani(engine)&#xA;chat_in_terminal(ai)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;kani guarantees that function calls are valid by the time they reach your methods while allowing you to focus on writing code. For more information, check out &lt;a href=&#34;https://kani.readthedocs.io/en/latest/function_calling.html&#34;&gt;the function calling docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why kani?&lt;/h2&gt; &#xA;&lt;p&gt;Existing frameworks for language models like langchain and simpleaichat are opinionated and/or heavyweight - they edit developers&#39; prompts under the hood, are challenging to learn, and are difficult to customize without adding a lot of high-maintenance bloat to your codebase.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img style=&#34;max-width: 800px;&#34; alt=&#34;kani&#34; src=&#34;https://raw.githubusercontent.com/zhudotexe/kani/main/docs/_static/lib-comparison_white.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We built kani to be more flexible, simple, and robust. kani is appropriate for everyone from academic researchers to industry professionals to hobbyists to use without worrying about under-the-hood hacks.&lt;/p&gt; &#xA;&lt;h2&gt;Docs&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about how to &lt;a href=&#34;https://kani.readthedocs.io/en/latest/customization.html&#34;&gt;customize kani with your own prompt wrappers&lt;/a&gt;, &lt;a href=&#34;https://kani.readthedocs.io/en/latest/function_calling.html&#34;&gt;function calling&lt;/a&gt;, and more, &lt;a href=&#34;http://kani.readthedocs.io/&#34;&gt;read the docs!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or take a look at the hands-on examples &lt;a href=&#34;https://github.com/zhudotexe/kani/tree/main/examples&#34;&gt;in this repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Want to see kani in action? Using 4-bit quantization to shrink the model, we run LLaMA v2 as part of our test suite right on GitHub Actions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml?query=branch%3Amain+is%3Asuccess&#34;&gt;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml?query=branch%3Amain+is%3Asuccess&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simply click on the latest build to see LLaMA&#39;s output!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Kani, please cite us as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhu2023kani,&#xA;      title={Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications}, &#xA;      author={Andrew Zhu and Liam Dugan and Alyssa Hwang and Chris Callison-Burch},&#xA;      year={2023},&#xA;      eprint={2309.05542},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;For developers:&#xA;&#xA;## Build and Publish&#xA;&#xA;`fastlmi` uses Hatchling to build.&#xA;&#xA;Make sure to bump the version in pyproject.toml before publishing.&#xA;&#xA;```shell&#xA;rm -r dist/&#xA;python -m build&#xA;python -m twine upload dist/*&#xA;```&#xA;--&gt;</summary>
  </entry>
</feed>