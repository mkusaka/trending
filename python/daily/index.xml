<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-05T01:43:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>binary-husky/gpt_academic</title>
    <updated>2023-07-05T01:43:33Z</updated>
    <id>tag:github.com,2023-07-05:/binary-husky/gpt_academic</id>
    <link href="https://github.com/binary-husky/gpt_academic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;为ChatGPT/GLM提供图形交互界面，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮&amp;函数插件，支持Python和C++等项目剖析&amp;自译解功能，PDF/LaTex论文翻译&amp;总结功能，支持并行问询多种LLM模型，支持清华chatglm等本地模型。兼容复旦MOSS, llama, rwkv, 盘古, newbing, claude等&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;2023.5.27 对Gradio依赖进行了调整，Fork并解决了官方Gradio的若干Bugs。请及时&lt;strong&gt;更新代码&lt;/strong&gt;并重新更新pip依赖。安装依赖时，请严格选择&lt;code&gt;requirements.txt&lt;/code&gt;中&lt;strong&gt;指定的版本&lt;/strong&gt;：&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/logo.png&#34; width=&#34;40&#34;&gt; GPT 学术优化 (GPT Academic)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果喜欢这个项目，请给它一个Star；如果你发明了更好用的快捷键或函数插件，欢迎发pull requests&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like this project, please give it a Star. If you&#39;ve come up with more useful academic shortcuts or functional plugins, feel free to open an issue or pull request. We also have a README in &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README_EN.md&#34;&gt;English|&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README_JP.md&#34;&gt;日本語|&lt;/a&gt;&lt;a href=&#34;https://github.com/mldljyh/ko_gpt_academic&#34;&gt;한국어|&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README_RS.md&#34;&gt;Русский|&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README_FR.md&#34;&gt;Français&lt;/a&gt; translated by this project itself. To translate this project to arbitary language with GPT, read and run &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/multi_language.py&#34;&gt;&lt;code&gt;multi_language.py&lt;/code&gt;&lt;/a&gt; (experimental).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;1.请注意只有&lt;strong&gt;红颜色&lt;/strong&gt;标识的函数插件（按钮）才支持读取文件，部分插件位于插件区的&lt;strong&gt;下拉菜单&lt;/strong&gt;中。另外我们以&lt;strong&gt;最高优先级&lt;/strong&gt;欢迎和处理任何新插件的PR！&lt;/p&gt; &#xA; &lt;p&gt;2.本项目中每个文件的功能都在自译解&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/chatgpt-academic%E9%A1%B9%E7%9B%AE%E8%87%AA%E8%AF%91%E8%A7%A3%E6%8A%A5%E5%91%8A&#34;&gt;&lt;code&gt;self_analysis.md&lt;/code&gt;&lt;/a&gt;详细说明。随着版本的迭代，您也可以随时自行点击相关函数插件，调用GPT重新生成项目的自我解析报告。常见问题汇总在&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;&lt;code&gt;wiki&lt;/code&gt;&lt;/a&gt;当中。&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/#installation&#34;&gt;安装方法&lt;/a&gt;。&lt;/p&gt; &#xA; &lt;p&gt;3.本项目兼容并鼓励尝试国产大语言模型chatglm和RWKV, 盘古等等。支持多个api-key共存，可在配置文件中填写如&lt;code&gt;API_KEY=&#34;openai-key1,openai-key2,api2d-key3&#34;&lt;/code&gt;。需要临时更换&lt;code&gt;API_KEY&lt;/code&gt;时，在输入区输入临时的&lt;code&gt;API_KEY&lt;/code&gt;然后回车键提交后即可生效。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;功能&lt;/th&gt; &#xA;    &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键润色&lt;/td&gt; &#xA;    &lt;td&gt;支持一键润色、一键查找论文语法错误&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键中英互译&lt;/td&gt; &#xA;    &lt;td&gt;一键中英互译&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键代码解释&lt;/td&gt; &#xA;    &lt;td&gt;显示代码、解释代码、生成代码、给代码加注释&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV14s4y1E7jN&#34;&gt;自定义快捷键&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义快捷键&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;模块化设计&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义强大的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/tree/master/crazy_functions&#34;&gt;函数插件&lt;/a&gt;，插件支持&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97&#34;&gt;热更新&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1cj411A7VW&#34;&gt;自我程序剖析&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/chatgpt-academic%E9%A1%B9%E7%9B%AE%E8%87%AA%E8%AF%91%E8%A7%A3%E6%8A%A5%E5%91%8A&#34;&gt;一键读懂&lt;/a&gt;本项目的源代码&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1cj411A7VW&#34;&gt;程序剖析&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键可以剖析其他Python/C/C++/Java/Lua/...项目树&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;读论文、&lt;a href=&#34;https://www.bilibili.com/video/BV1KT411x7Wn&#34;&gt;翻译&lt;/a&gt;论文&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键解读latex/pdf论文全文并生成摘要&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Latex全文&lt;a href=&#34;https://www.bilibili.com/video/BV1nk4y1Y7Js/&#34;&gt;翻译&lt;/a&gt;、&lt;a href=&#34;https://www.bilibili.com/video/BV1FT411H7c5/&#34;&gt;润色&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键翻译或润色latex论文&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;批量注释生成&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键批量生成函数注释&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Markdown&lt;a href=&#34;https://www.bilibili.com/video/BV1yo4y157jV/&#34;&gt;中英互译&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 看到上面5种语言的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/raw/master/docs/README_EN.md&#34;&gt;README&lt;/a&gt;了吗？&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;chat分析报告生成&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 运行后自动生成总结汇报&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1KT411x7Wn&#34;&gt;PDF论文全文翻译功能&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] PDF论文提取题目&amp;amp;摘要+翻译全文（多线程）&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1LM4y1279X&#34;&gt;Arxiv小助手&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 输入arxiv文章url即可一键翻译摘要+下载PDF&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19L411U7ia&#34;&gt;谷歌学术统合小助手&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 给定任意谷歌学术搜索页面URL，让gpt帮你&lt;a href=&#34;https://www.bilibili.com/video/BV1GP411U7Az/&#34;&gt;写relatedworks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;互联网信息聚合+GPT&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键&lt;a href=&#34;https://www.bilibili.com/video/BV1om4y127ck&#34;&gt;让GPT先从互联网获取信息&lt;/a&gt;，再回答问题，让信息永不过时&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐Arxiv论文精细翻译&lt;/td&gt; &#xA;    &lt;td&gt;[函数插件] 一键&lt;a href=&#34;https://www.bilibili.com/video/BV1dz4y1v77A/&#34;&gt;以超高质量翻译arxiv论文&lt;/a&gt;，迄今为止最好的论文翻译工具⭐&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;公式/图片/表格显示&lt;/td&gt; &#xA;    &lt;td&gt;可以同时显示公式的&lt;a href=&#34;https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png&#34;&gt;tex形式和渲染形式&lt;/a&gt;，支持公式、代码高亮&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;多线程函数插件支持&lt;/td&gt; &#xA;    &lt;td&gt;支持多线调用chatgpt，一键处理&lt;a href=&#34;https://www.bilibili.com/video/BV1FT411H7c5/&#34;&gt;海量文本&lt;/a&gt;或程序&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;启动暗色gradio&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/issues/173&#34;&gt;主题&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;在浏览器url后面添加&lt;code&gt;/?__theme=dark&lt;/code&gt;可以切换dark主题&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wT411p7yf&#34;&gt;多LLM模型&lt;/a&gt;支持&lt;/td&gt; &#xA;    &lt;td&gt;同时被GPT3.5、GPT4、&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;清华ChatGLM&lt;/a&gt;、&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;&gt;复旦MOSS&lt;/a&gt;同时伺候的感觉一定会很不错吧？&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;更多LLM模型接入，支持&lt;a href=&#34;https://huggingface.co/spaces/qingxu98/gpt-academic&#34;&gt;huggingface部署&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;加入Newbing接口(新必应)，引入清华&lt;a href=&#34;https://github.com/Jittor/JittorLLMs&#34;&gt;Jittorllms&lt;/a&gt;支持&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;，&lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV&#34;&gt;RWKV&lt;/a&gt;和&lt;a href=&#34;https://openi.org.cn/pangu/&#34;&gt;盘古α&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;更多新功能展示(图像生成等) ……&lt;/td&gt; &#xA;    &lt;td&gt;见本文档结尾处 ……&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新界面（修改&lt;code&gt;config.py&lt;/code&gt;中的LAYOUT选项即可实现“左右布局”和“上下布局”的切换）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/230361456-61078362-a966-4eb5-b49e-3c62ef18b860.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;所有按钮都通过读取functional.py动态生成，可随意加自定义功能，解放粘贴板&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/231975334-b4788e91-4887-412f-8b43-2b9c5f41d248.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;润色/纠错&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/231980294-f374bdcb-3309-4560-b424-38ef39f04ebd.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如果输出包含公式，会同时以tex形式和渲染形式显示，方便复制和阅读&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;懒得看项目代码？整个工程直接给chatgpt炫嘴里&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;多种大语言模型混合调用（ChatGLM + OpenAI-GPT3.5 + &lt;a href=&#34;https://api2d.com/&#34;&gt;API2D&lt;/a&gt;-GPT4）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/232537274-deca0563-7aa6-4b5d-94a2-b7c453c47794.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;安装-方法1：直接运行 (Windows, Linux or MacOS)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;下载项目&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/binary-husky/gpt_academic.git&#xA;cd gpt_academic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;配置API_KEY&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;在&lt;code&gt;config.py&lt;/code&gt;中，配置API KEY等设置，&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/issues/1&#34;&gt;点击查看特殊网络环境设置方法&lt;/a&gt; 。&lt;/p&gt; &#xA;&lt;p&gt;(P.S. 程序运行时会优先检查是否存在名为&lt;code&gt;config_private.py&lt;/code&gt;的私密配置文件，并用其中的配置覆盖&lt;code&gt;config.py&lt;/code&gt;的同名配置。因此，如果您能理解我们的配置读取逻辑，我们强烈建议您在&lt;code&gt;config.py&lt;/code&gt;旁边创建一个名为&lt;code&gt;config_private.py&lt;/code&gt;的新配置文件，并把&lt;code&gt;config.py&lt;/code&gt;中的配置转移（复制）到&lt;code&gt;config_private.py&lt;/code&gt;中。&lt;code&gt;config_private.py&lt;/code&gt;不受git管控，可以让您的隐私信息更加安全。P.S.项目同样支持通过&lt;code&gt;环境变量&lt;/code&gt;配置大多数选项，环境变量的书写格式参考&lt;code&gt;docker-compose&lt;/code&gt;文件。读取优先级: &lt;code&gt;环境变量&lt;/code&gt; &amp;gt; &lt;code&gt;config_private.py&lt;/code&gt; &amp;gt; &lt;code&gt;config.py&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;安装依赖&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# （选择I: 如熟悉python）（python版本3.9以上，越新越好），备注：使用官方pip源或者阿里pip源,临时换源方法：python -m pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/&#xA;python -m pip install -r requirements.txt&#xA;&#xA;# （选择II: 如不熟悉python）使用anaconda，步骤也是类似的 (https://www.bilibili.com/video/BV1rc411W7Dr)：&#xA;conda create -n gptac_venv python=3.11    # 创建anaconda环境&#xA;conda activate gptac_venv                 # 激活anaconda环境&#xA;python -m pip install -r requirements.txt # 这个步骤和pip安装一样的步骤&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;如果需要支持清华ChatGLM/复旦MOSS作为后端，请点击展开此处&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;【可选步骤】如果需要支持清华ChatGLM/复旦MOSS作为后端，需要额外安装更多依赖（前提条件：熟悉Python + 用过Pytorch + 电脑配置够强）：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 【可选步骤I】支持清华ChatGLM。清华ChatGLM备注：如果遇到&#34;Call ChatGLM fail 不能正常加载ChatGLM的参数&#34; 错误，参考如下： 1：以上默认安装的为torch+cpu版，使用cuda需要卸载torch重新安装torch+cuda； 2：如因本机配置不够无法加载模型，可以修改request_llm/bridge_chatglm.py中的模型精度, 将 AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True) 都修改为 AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b-int4&#34;, trust_remote_code=True)&#xA;python -m pip install -r request_llm/requirements_chatglm.txt  &#xA;&#xA;# 【可选步骤II】支持复旦MOSS&#xA;python -m pip install -r request_llm/requirements_moss.txt&#xA;git clone https://github.com/OpenLMLab/MOSS.git request_llm/moss  # 注意执行此行代码时，必须处于项目根路径&#xA;&#xA;# 【可选步骤III】确保config.py配置文件的AVAIL_LLM_MODELS包含了期望的模型，目前支持的全部模型如下(jittorllms系列目前仅支持docker方案)：&#xA;AVAIL_LLM_MODELS = [&#34;gpt-3.5-turbo&#34;, &#34;api2d-gpt-3.5-turbo&#34;, &#34;gpt-4&#34;, &#34;api2d-gpt-4&#34;, &#34;chatglm&#34;, &#34;newbing&#34;, &#34;moss&#34;] # + [&#34;jittorllms_rwkv&#34;, &#34;jittorllms_pangualpha&#34;, &#34;jittorllms_llama&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;运行&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;安装-方法2：使用Docker&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;仅ChatGPT（推荐大多数人选择，等价于docker-compose方案1）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/binary-husky/gpt_academic.git  # 下载项目&#xA;cd gpt_academic                                 # 进入路径&#xA;nano config.py                                      # 用任意文本编辑器编辑config.py, 配置 “Proxy”， “API_KEY” 以及 “WEB_PORT” (例如50923) 等&#xA;docker build -t gpt-academic .                      # 安装&#xA;&#xA;#（最后一步-选择1）在Linux环境下，用`--net=host`更方便快捷&#xA;docker run --rm -it --net=host gpt-academic&#xA;#（最后一步-选择2）在macOS/windows环境下，只能用-p选项将容器上的端口(例如50923)暴露给主机上的端口&#xA;docker run --rm -it -e WEB_PORT=50923 -p 50923:50923 gpt-academic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;P.S. 如果需要依赖Latex的插件功能，请见Wiki。另外，您也可以直接使用docker-compose获取Latex功能（修改docker-compose.yml，保留方案4并删除其他方案）。&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;ChatGPT + ChatGLM + MOSS（需要熟悉Docker）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改docker-compose.yml，保留方案2并删除其他方案。修改docker-compose.yml中方案2的配置，参考其中注释即可&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;ChatGPT + LLAMA + 盘古 + RWKV（需要熟悉Docker）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改docker-compose.yml，保留方案3并删除其他方案。修改docker-compose.yml中方案3的配置，参考其中注释即可&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;安装-方法3：其他部署姿势&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;一键运行脚本。 完全不熟悉python环境的Windows用户可以下载&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/releases&#34;&gt;Release&lt;/a&gt;中发布的一键运行脚本安装无本地模型的版本。 脚本的贡献来源是&lt;a href=&#34;https://github.com/oobabooga/one-click-installers&#34;&gt;oobabooga&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用docker-compose运行。 请阅读docker-compose.yml后，按照其中的提示操作即可&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如何使用反代URL 按照&lt;code&gt;config.py&lt;/code&gt;中的说明配置API_URL_REDIRECT即可。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;微软云AzureAPI 按照&lt;code&gt;config.py&lt;/code&gt;中的说明配置即可（AZURE_ENDPOINT等四个配置）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;远程云服务器部署（需要云服务器知识与经验）。 请访问&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97&#34;&gt;部署wiki-1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用WSL2（Windows Subsystem for Linux 子系统）。 请访问&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E4%BD%BF%E7%94%A8WSL2%EF%BC%88Windows-Subsystem-for-Linux-%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%89%E9%83%A8%E7%BD%B2&#34;&gt;部署wiki-2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如何在二级网址（如&lt;code&gt;http://localhost/subpath&lt;/code&gt;）下运行。 请访问&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/WithFastapi.md&#34;&gt;FastAPI运行说明&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Advanced Usage&lt;/h1&gt; &#xA;&lt;h2&gt;自定义新的便捷按钮 / 自定义函数插件&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;自定义新的便捷按钮（学术快捷键） 任意文本编辑器打开&lt;code&gt;core_functional.py&lt;/code&gt;，添加条目如下，然后重启程序即可。（如果按钮已经添加成功并可见，那么前缀、后缀都支持热修改，无需重启程序即可生效。） 例如&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;超级英译中&#34;: {&#xA;    # 前缀，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等&#xA;    &#34;Prefix&#34;: &#34;请翻译把下面一段内容成中文，然后用一个markdown表格逐一解释文中出现的专有名词：\n\n&#34;, &#xA;    &#xA;    # 后缀，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来。&#xA;    &#34;Suffix&#34;: &#34;&#34;,&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226899272-477c2134-ed71-4326-810c-29891fe4a508.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;自定义函数插件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;编写强大的函数插件来执行任何你想得到的和想不到的任务。 本项目的插件编写、调试难度很低，只要您具备一定的python基础知识，就可以仿照我们提供的模板实现自己的插件功能。 详情请参考&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97&#34;&gt;函数插件指南&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Latest Update&lt;/h1&gt; &#xA;&lt;h2&gt;新功能动态&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;对话保存功能。在函数插件区调用 &lt;code&gt;保存当前的对话&lt;/code&gt; 即可将当前对话保存为可读+可复原的html文件， 另外在函数插件区（下拉菜单）调用 &lt;code&gt;载入对话历史存档&lt;/code&gt; ，即可还原之前的会话。 Tip：不指定文件直接点击 &lt;code&gt;载入对话历史存档&lt;/code&gt; 可以查看历史html存档缓存。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/235222390-24a9acc0-680f-49f5-bc81-2f3161f1e049.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;⭐Latex/Arxiv论文翻译功能⭐&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/002a1a75-ace0-4e6a-94e2-ec1406a746f1&#34; height=&#34;250&#34;&gt; ===&amp;gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/9fdcc391-f823-464f-9322-f8719677043b&#34; height=&#34;250&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;生成报告。大部分插件都会在执行结束后，生成工作报告&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227503770-fe29ce2c-53fd-47b0-b0ff-93805f0c2ff4.png&#34; height=&#34;250&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504617-7a497bb3-0a2a-4b50-9a8a-95ae60ea7afd.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;模块化功能设计，简单的接口却能支持强大的功能&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/229288270-093643c1-0018-487a-81e6-1d7809b6e90f.png&#34; height=&#34;400&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504931-19955f78-45cd-4d1c-adac-e71e50957915.png&#34; height=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;译解其他开源项目&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; height=&#34;250&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226969067-968a27c1-1b9c-486b-8b81-ab2de8d3f88a.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;装饰&lt;a href=&#34;https://github.com/fghrsh/live2d_demo&#34;&gt;live2d&lt;/a&gt;的小功能（默认关闭，需要修改&lt;code&gt;config.py&lt;/code&gt;）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/236432361-67739153-73e8-43fe-8111-b61296edabd9.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;新增MOSS大语言模型支持&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/236639178-92836f37-13af-4fdd-984d-b4450fe30336.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;OpenAI图像生成&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/bc7ab234-ad90-48a0-8d62-f703d9e74665&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;OpenAI音频解析与总结&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/709ccf95-3aee-498a-934a-e1c22d3d5d5b&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;Latex全文校对纠错&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/651ccd98-02c9-4464-91e1-77a6b7d1b033&#34; height=&#34;200&#34;&gt; ===&amp;gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/476f66d9-7716-4537-b5c1-735372c25adb&#34; height=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;版本:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;version 3.5(Todo): 使用自然语言调用本项目的所有函数插件（高优先级）&lt;/li&gt; &#xA; &lt;li&gt;version 3.4: +arxiv论文翻译、latex论文批改功能&lt;/li&gt; &#xA; &lt;li&gt;version 3.3: +互联网信息综合功能&lt;/li&gt; &#xA; &lt;li&gt;version 3.2: 函数插件支持更多参数接口 (保存对话功能, 解读任意语言代码+同时询问任意的LLM组合)&lt;/li&gt; &#xA; &lt;li&gt;version 3.1: 支持同时问询多个gpt模型！支持api2d，支持多个apikey负载均衡&lt;/li&gt; &#xA; &lt;li&gt;version 3.0: 对chatglm和其他小型llm的支持&lt;/li&gt; &#xA; &lt;li&gt;version 2.6: 重构了插件结构，提高了交互性，加入更多插件&lt;/li&gt; &#xA; &lt;li&gt;version 2.5: 自更新，解决总结大工程源代码时文本过长、token溢出的问题&lt;/li&gt; &#xA; &lt;li&gt;version 2.4: (1)新增PDF全文翻译功能; (2)新增输入区切换位置的功能; (3)新增垂直布局选项; (4)多线程函数插件优化。&lt;/li&gt; &#xA; &lt;li&gt;version 2.3: 增强多线程交互性&lt;/li&gt; &#xA; &lt;li&gt;version 2.2: 函数插件支持热重载&lt;/li&gt; &#xA; &lt;li&gt;version 2.1: 可折叠式布局&lt;/li&gt; &#xA; &lt;li&gt;version 2.0: 引入模块化函数插件&lt;/li&gt; &#xA; &lt;li&gt;version 1.0: 基础功能&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;gpt_academic开发者QQ群-2：610599535&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;已知问题 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;某些浏览器翻译插件干扰此软件前端的运行&lt;/li&gt; &#xA;   &lt;li&gt;官方Gradio目前有很多兼容性Bug，请务必使用&lt;code&gt;requirement.txt&lt;/code&gt;安装Gradio&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参考与学习&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;代码中参考了很多其他优秀项目中的设计，顺序不分先后：&#xA;&#xA;# 清华ChatGLM-6B:&#xA;https://github.com/THUDM/ChatGLM-6B&#xA;&#xA;# 清华JittorLLMs:&#xA;https://github.com/Jittor/JittorLLMs&#xA;&#xA;# ChatPaper:&#xA;https://github.com/kaixindelele/ChatPaper&#xA;&#xA;# Edge-GPT:&#xA;https://github.com/acheong08/EdgeGPT&#xA;&#xA;# ChuanhuChatGPT:&#xA;https://github.com/GaiZhenbiao/ChuanhuChatGPT&#xA;&#xA;# Oobabooga one-click installer:&#xA;https://github.com/oobabooga/one-click-installers&#xA;&#xA;# More：&#xA;https://github.com/gradio-app/gradio&#xA;https://github.com/fghrsh/live2d_demo&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/streamlit-agent</title>
    <updated>2023-07-05T01:43:33Z</updated>
    <id>tag:github.com,2023-07-05:/langchain-ai/streamlit-agent</id>
    <link href="https://github.com/langchain-ai/streamlit-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reference implementations of several LangChain agents as Streamlit apps&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦜️🔗 LangChain 🤝 Streamlit agent examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1&#34;&gt;&lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains reference implementations of various LangChain agents as Streamlit apps including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;basic_streaming.py&lt;/code&gt;: How to do streaming with a simple app using &lt;code&gt;langchain.chat_models.ChatOpenAI&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mrkl_demo.py&lt;/code&gt;: An agent that replicates the &lt;a href=&#34;https://python.langchain.com/docs/modules/agents/how_to/mrkl&#34;&gt;MRKL demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;minimal_agent.py&lt;/code&gt;: A minimal agent with search (requires setting &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env to run)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;search_and_chat.py&lt;/code&gt;: A search-enabled chatbot that remembers chat history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Apps feature LangChain 🤝 Streamlit integrations such as the &lt;a href=&#34;https://python.langchain.com/docs/modules/callbacks/integrations/streamlit&#34;&gt;Callback integration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://python-poetry.org/&#34;&gt;Poetry&lt;/a&gt; for dependency management.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Create Python environment&#xA;$ poetry install&#xA;&#xA;# Install git pre-commit hooks&#xA;$ poetry shell&#xA;$ pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Run mrkl_demo.py or another app the same way&#xA;$ streamlit run streamlit_agent/mrkl_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We plan to add more agent examples over time - PRs welcome&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Chat QA over docs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SQL agent&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>PlayVoice/so-vits-svc-5.0</title>
    <updated>2023-07-05T01:43:33Z</updated>
    <id>tag:github.com,2023-07-05:/PlayVoice/so-vits-svc-5.0</id>
    <link href="https://github.com/PlayVoice/so-vits-svc-5.0" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Core Engine of Singing Voice Conversion &amp; Singing Voice Clone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; Variational Inference with adversarial learning for end-to-end Singing Voice Conversion based on VITS &lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/maxmax20160403/sovits5.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1PY1E4bDAeHbAD4r99D_oYXB46fG8nIA5?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/PlayVoice/so-vits-svc-5.0&#34;&gt; &lt;img alt=&#34;GitHub forks&#34; src=&#34;https://img.shields.io/github/forks/PlayVoice/so-vits-svc-5.0&#34;&gt; &lt;img alt=&#34;GitHub issues&#34; src=&#34;https://img.shields.io/github/issues/PlayVoice/so-vits-svc-5.0&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/PlayVoice/so-vits-svc-5.0&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PlayVoice/so-vits-svc-5.0/bigvgan-mix-v2/README_ZH.md&#34;&gt;中文文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project is target for: beginners in deep learning, the basic operation of Python and PyTorch is the prerequisite for using this project;&lt;/li&gt; &#xA; &lt;li&gt;This project aims to help deep learning beginners get rid of boring pure theoretical learning, and master the basic knowledge of deep learning by combining it with practice;&lt;/li&gt; &#xA; &lt;li&gt;This project does not support real-time voice change; (support needs to replace whisper)&lt;/li&gt; &#xA; &lt;li&gt;This project will not develop one-click packages for other purposes；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PlayVoice/so-vits-svc-5.0/assets/16432329/3854b281-8f97-4016-875b-6eb663c92466&#34; alt=&#34;vits-5.0-frame&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;6G memory GPU can be used to trained&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;support for multiple speakers&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;create unique speakers through speaker mixing&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;even with light accompaniment can also be converted&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;F0 can be edited using Excel&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model properties&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;From&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Function&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;strong noise immunity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bigvgan&lt;/td&gt; &#xA;   &lt;td&gt;NVIDA&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;alias and snake&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;natural speech&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;reduce mispronunciation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;neural source-filter&lt;/td&gt; &#xA;   &lt;td&gt;NII&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;solve the problem of audio F0 discontinuity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speaker encoder&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Timbre Encoding and Clustering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRL for speaker&lt;/td&gt; &#xA;   &lt;td&gt;Ubisoft&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Preventing Encoder Leakage Timbre&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;one shot vits&lt;/td&gt; &#xA;   &lt;td&gt;Samsung&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Voice Clone&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SCLN&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Improve Clone&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPG perturbation&lt;/td&gt; &#xA;   &lt;td&gt;this project&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Improved noise immunity and de-timbre&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuBERT perturbation&lt;/td&gt; &#xA;   &lt;td&gt;this project&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Improved noise immunity and de-timbre&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VAE perturbation&lt;/td&gt; &#xA;   &lt;td&gt;this project&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Improve sound quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;due to the use of data perturbation, it takes longer to train than other projects.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset preparation&lt;/h2&gt; &#xA;&lt;p&gt;Necessary pre-processing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1 accompaniment separation, &lt;a href=&#34;https://github.com/Anjok07/ultimatevocalremovergui&#34;&gt;UVR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2 cut audio, less than 30 seconds for whisper, &lt;a href=&#34;https://github.com/flutydeer/audio-slicer&#34;&gt;slicer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;then put the dataset into the dataset_raw directory according to the following file structure&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset_raw&#xA;├───speaker0&#xA;│   ├───000001.wav&#xA;│   ├───...&#xA;│   └───000xxx.wav&#xA;└───speaker1&#xA;    ├───000001.wav&#xA;    ├───...&#xA;    └───000xxx.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;1 software dependency&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;pip install -r requirements.txt&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2 download the Timbre Encoder: &lt;a href=&#34;https://drive.google.com/drive/folders/15oeBYf6Qn1edONkVLXe82MzdIi3O_9m3&#34;&gt;Speaker-Encoder by @mueller91&lt;/a&gt;, put &lt;code&gt;best_model.pth.tar&lt;/code&gt; into &lt;code&gt;speaker_pretrain/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3 download whisper model &lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt&#34;&gt;whisper-large-v2&lt;/a&gt;, Make sure to download &lt;code&gt;large-v2.pt&lt;/code&gt;，put it into &lt;code&gt;whisper_pretrain/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4 whisper is built-in, do not install it additionally, it will conflict and report an error&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;5 download &lt;a href=&#34;https://github.com/bshall/hubert/releases/tag/v0.1&#34;&gt;hubert_soft model&lt;/a&gt;，put &lt;code&gt;hubert-soft-0d54a1f4.pt&lt;/code&gt; into &lt;code&gt;hubert_pretrain/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data preprocessing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;1， re-sampling&lt;/p&gt; &lt;p&gt;generate audio with a sampling rate of 16000Hz：./data_svc/waves-16k&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_a.py -w ./dataset_raw -o ./data_svc/waves-16k -s 16000&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;generate audio with a sampling rate of 32000Hz：./data_svc/waves-32k&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_a.py -w ./dataset_raw -o ./data_svc/waves-32k -s 32000&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2， use 16K audio to extract pitch：&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_crepe.py -w data_svc/waves-16k/ -p data_svc/pitch&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3， use 16K audio to extract ppg&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_ppg.py -w data_svc/waves-16k/ -p data_svc/whisper&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4， use 16K audio to extract hubert&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_hubert.py -w data_svc/waves-16k/ -v data_svc/hubert&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;5， use 16k audio to extract timbre code&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_speaker.py data_svc/waves-16k/ data_svc/speaker&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;6， extract the average value of the timbre code for inference; it can also replace a single audio timbre in generating the training index, and use it as the unified timbre of the speaker for training&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_speaker_ave.py data_svc/speaker/ data_svc/singer&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;7， use 32k audio to extract the linear spectrum&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_spec.py -w data_svc/waves-32k/ -s data_svc/specs&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;8， use 32k audio to generate training index&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_train.py&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;9， training file debugging&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python prepare/preprocess_zzz.py&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;data_svc/&#xA;└── waves-16k&#xA;│    └── speaker0&#xA;│    │      ├── 000001.wav&#xA;│    │      └── 000xxx.wav&#xA;│    └── speaker1&#xA;│           ├── 000001.wav&#xA;│           └── 000xxx.wav&#xA;└── waves-32k&#xA;│    └── speaker0&#xA;│    │      ├── 000001.wav&#xA;│    │      └── 000xxx.wav&#xA;│    └── speaker1&#xA;│           ├── 000001.wav&#xA;│           └── 000xxx.wav&#xA;└── pitch&#xA;│    └── speaker0&#xA;│    │      ├── 000001.pit.npy&#xA;│    │      └── 000xxx.pit.npy&#xA;│    └── speaker1&#xA;│           ├── 000001.pit.npy&#xA;│           └── 000xxx.pit.npy&#xA;└── hubert&#xA;│    └── speaker0&#xA;│    │      ├── 000001.vec.npy&#xA;│    │      └── 000xxx.vec.npy&#xA;│    └── speaker1&#xA;│           ├── 000001.vec.npy&#xA;│           └── 000xxx.vec.npy&#xA;└── whisper&#xA;│    └── speaker0&#xA;│    │      ├── 000001.ppg.npy&#xA;│    │      └── 000xxx.ppg.npy&#xA;│    └── speaker1&#xA;│           ├── 000001.ppg.npy&#xA;│           └── 000xxx.ppg.npy&#xA;└── speaker&#xA;│    └── speaker0&#xA;│    │      ├── 000001.spk.npy&#xA;│    │      └── 000xxx.spk.npy&#xA;│    └── speaker1&#xA;│           ├── 000001.spk.npy&#xA;│           └── 000xxx.spk.npy&#xA;└── singer&#xA;    ├── speaker0.spk.npy&#xA;    └── speaker1.spk.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;1， if fine-tuning based on the pre-trained model, you need to download the pre-trained model: &lt;a href=&#34;https://github.com/PlayVoice/so-vits-svc-5.0/releases/tag/bigvgan_release&#34;&gt;sovits5.0_bigvgan_mix_v2.pth&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;set pretrain: &#34;./sovits5.0_bigvgan_mix_v2.pth&#34; in configs/base.yaml，and adjust the learning rate appropriately, eg 5e-5&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2， start training&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python svc_trainer.py -c configs/base.yaml -n sovits5.0&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3， resume training&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python svc_trainer.py -c configs/base.yaml -n sovits5.0 -p chkpt/sovits5.0/***.pth&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4， view log&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;tensorboard --logdir logs/&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PlayVoice/so-vits-svc-5.0/assets/16432329/1628e775-5888-4eac-b173-a28dca978faa&#34; alt=&#34;sovits5 0_base&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;1， export inference model: text encoder, Flow network, Decoder network&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python svc_export.py --config configs/base.yaml --checkpoint_path chkpt/sovits5.0/***.pt&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2， use whisper to extract content encoding, without using one-click reasoning, in order to reduce GPU memory usage&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python whisper/inference.py -w test.wav -p test.ppg.npy&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3， use hubert to extract content vector, without using one-click reasoning, in order to reduce GPU memory usage&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python hubert/inference.py -w test.wav -v test.vec.npy&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4， extract the F0 parameter to the csv text format, open the csv file in Excel, and manually modify the wrong F0 according to Audition or SonicVisualiser&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python pitch/inference.py -w test.wav -p test.csv&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;5，specify parameters and infer&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;python svc_inference.py --config configs/base.yaml --model sovits5.0.pth --spk ./configs/singers/singer0001.npy --wave test.wav --ppg test.ppg.npy --vec test.vec.npy --pit test.csv&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;when --ppg is specified, when the same audio is reasoned multiple times, it can avoid repeated extraction of audio content codes; if it is not specified, it will be automatically extracted;&lt;/p&gt; &lt;p&gt;when --vec is specified, when the same audio is reasoned multiple times, it can avoid repeated extraction of audio content codes; if it is not specified, it will be automatically extracted;&lt;/p&gt; &lt;p&gt;when --pit is specified, the manually tuned F0 parameter can be loaded; if not specified, it will be automatically extracted;&lt;/p&gt; &lt;p&gt;generate files in the current directory:svc_out.wav&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;args&lt;/th&gt; &#xA;     &lt;th&gt;--config&lt;/th&gt; &#xA;     &lt;th&gt;--model&lt;/th&gt; &#xA;     &lt;th&gt;--spk&lt;/th&gt; &#xA;     &lt;th&gt;--wave&lt;/th&gt; &#xA;     &lt;th&gt;--ppg&lt;/th&gt; &#xA;     &lt;th&gt;--vec&lt;/th&gt; &#xA;     &lt;th&gt;--pit&lt;/th&gt; &#xA;     &lt;th&gt;--shift&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;name&lt;/td&gt; &#xA;     &lt;td&gt;config path&lt;/td&gt; &#xA;     &lt;td&gt;model path&lt;/td&gt; &#xA;     &lt;td&gt;speaker&lt;/td&gt; &#xA;     &lt;td&gt;wave input&lt;/td&gt; &#xA;     &lt;td&gt;wave ppg&lt;/td&gt; &#xA;     &lt;td&gt;wave hubert&lt;/td&gt; &#xA;     &lt;td&gt;wave pitch&lt;/td&gt; &#xA;     &lt;td&gt;pitch shift&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Creat singer&lt;/h2&gt; &#xA;&lt;p&gt;named by pure coincidence：average -&amp;gt; ave -&amp;gt; eva，eve(eva) represents conception and reproduction&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python svc_eva.py&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eva_conf = {&#xA;    &#39;./configs/singers/singer0022.npy&#39;: 0,&#xA;    &#39;./configs/singers/singer0030.npy&#39;: 0,&#xA;    &#39;./configs/singers/singer0047.npy&#39;: 0.5,&#xA;    &#39;./configs/singers/singer0051.npy&#39;: 0.5,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the generated singer file is：eva.spk.npy&lt;/p&gt; &#xA;&lt;h2&gt;Data set&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KiSing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://shijt.site/index.php/2021/05/16/kising-the-first-open-source-mandarin-singing-voice-synthesis-corpus/&#34;&gt;http://shijt.site/index.php/2021/05/16/kising-the-first-open-source-mandarin-singing-voice-synthesis-corpus/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PopCS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MoonInTheRiver/DiffSinger/raw/master/resources/apply_form.md&#34;&gt;https://github.com/MoonInTheRiver/DiffSinger/blob/master/resources/apply_form.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;opencpop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wenet.org.cn/opencpop/download/&#34;&gt;https://wenet.org.cn/opencpop/download/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-Singer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Multi-Singer/Multi-Singer.github.io&#34;&gt;https://github.com/Multi-Singer/Multi-Singer.github.io&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;M4Singer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/M4Singer/M4Singer/raw/master/apply_form.md&#34;&gt;https://github.com/M4Singer/M4Singer/blob/master/apply_form.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CSD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/record/4785016#.YxqrTbaOMU4&#34;&gt;https://zenodo.org/record/4785016#.YxqrTbaOMU4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KSS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/bryanpark/korean-single-speaker-speech-dataset&#34;&gt;https://www.kaggle.com/datasets/bryanpark/korean-single-speaker-speech-dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JVS MuSic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_music&#34;&gt;https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_music&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PJS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/research-topics/pjs_corpus&#34;&gt;https://sites.google.com/site/shinnosuketakamichi/research-topics/pjs_corpus&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JUST Song&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/publication/jsut-song&#34;&gt;https://sites.google.com/site/shinnosuketakamichi/publication/jsut-song&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MUSDB18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems&#34;&gt;https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DSD100&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sigsep.github.io/datasets/dsd100.html&#34;&gt;https://sigsep.github.io/datasets/dsd100.html&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aishell-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.aishelltech.com/aishell_3&#34;&gt;http://www.aishelltech.com/aishell_3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VCTK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2651&#34;&gt;https://datashare.ed.ac.uk/handle/10283/2651&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Code sources and references&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/speech-resynthesis&#34;&gt;https://github.com/facebookresearch/speech-resynthesis&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.00355&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;https://github.com/jaywalnut310/vits&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openai/whisper/&#34;&gt;https://github.com/openai/whisper/&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/BigVGAN&#34;&gt;https://github.com/NVIDIA/BigVGAN&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.04658&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mindslab-ai/univnet&#34;&gt;https://github.com/mindslab-ai/univnet&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.07889&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf&#34;&gt;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/brentspell/hifi-gan-bwe&#34;&gt;https://github.com/brentspell/hifi-gan-bwe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mozilla/TTS&#34;&gt;https://github.com/mozilla/TTS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bshall/soft-vc&#34;&gt;https://github.com/bshall/soft-vc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/maxrmorrison/torchcrepe&#34;&gt;https://github.com/maxrmorrison/torchcrepe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OlaWod/FreeVC&#34;&gt;https://github.com/OlaWod/FreeVC&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.15418&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hcy71o/SNAC&#34;&gt;SNAC : Speaker-normalized Affine Coupling Layer in Flow-based Architecture for Zero-Shot Multi-Speaker Text-to-Speech&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.00585&#34;&gt;Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.00993.pdf&#34;&gt;AdaSpeech: Adaptive Text to Speech for Custom Voice&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ubisoft/ubisoft-laforge-daft-exprt&#34;&gt;Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.05401&#34;&gt;Learn to Sing by Listening: Building Controllable Virtual Singer by Unsupervised Learning from Voice Recordings&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.09167.pdf&#34;&gt;Adversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation Based Voice Conversion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.01252&#34;&gt;Speaker normalization (GRL) for self-supervised speech emotion recognition&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Method of Preventing Timbre Leakage Based on Data Perturbation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/auspicious3000/contentvec/raw/main/contentvec/data/audio/audio_utils_1.py&#34;&gt;https://github.com/auspicious3000/contentvec/blob/main/contentvec/data/audio/audio_utils_1.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/revsic/torch-nansy/raw/main/utils/augment/praat.py&#34;&gt;https://github.com/revsic/torch-nansy/blob/main/utils/augment/praat.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/revsic/torch-nansy/raw/main/utils/augment/peq.py&#34;&gt;https://github.com/revsic/torch-nansy/blob/main/utils/augment/peq.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/biggytruck/SpeechSplit2/raw/main/utils.py&#34;&gt;https://github.com/biggytruck/SpeechSplit2/blob/main/utils.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OlaWod/FreeVC/raw/main/preprocess_sr.py&#34;&gt;https://github.com/OlaWod/FreeVC/blob/main/preprocess_sr.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PlayVoice/so-vits-svc/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PlayVoice/so-vits-svc&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>