<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-24T01:34:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Igang-Fan-Jing/HDIP-YOLO</title>
    <updated>2022-12-24T01:34:45Z</updated>
    <id>tag:github.com,2022-12-24:/Igang-Fan-Jing/HDIP-YOLO</id>
    <link href="https://github.com/Igang-Fan-Jing/HDIP-YOLO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Documentation&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.ultralytics.com&#34;&gt;YOLOv5 Docs&lt;/a&gt; for full documentation on training, testing and deployment.&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Quick Start Examples&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Install&lt;/summary&gt; &#xA; &lt;p&gt;Clone repo and install &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.7.0&lt;/strong&gt;&lt;/a&gt; environment, including &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;strong&gt;PyTorch&amp;gt;=1.7&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ultralytics/yolov5  # clone&#xA;cd yolov5&#xA;pip install -r requirements.txt  # install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Inference&lt;/summary&gt; &#xA; &lt;p&gt;Inference with YOLOv5 and &lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/36&#34;&gt;PyTorch Hub&lt;/a&gt; . &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;Models&lt;/a&gt; download automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# Model&#xA;model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)  # or yolov5m, yolov5l, yolov5x, custom&#xA;&#xA;# Images&#xA;img = &#39;https://ultralytics.com/images/zidane.jpg&#39;  # or file, Path, PIL, OpenCV, numpy, list&#xA;&#xA;# Inference&#xA;results = model(img)&#xA;&#xA;# Results&#xA;results.print()  # or .show(), .save(), .crop(), .pandas(), etc.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Inference with detect.py&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on a variety of sources, downloading &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;models&lt;/a&gt; automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt; and saving results to &lt;code&gt;runs/detect&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python detect.py --source 0  # webcam&#xA;                          img.jpg  # image&#xA;                          vid.mp4  # video&#xA;                          path/  # directory&#xA;                          path/*.jpg  # glob&#xA;                          &#39;https://youtu.be/Zgi9g1ksQHc&#39;  # YouTube&#xA;                          &#39;rtsp://example.com/media.mp4&#39;  # RTSP, RTMP, HTTP stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Training&lt;/summary&gt; &#xA; &lt;p&gt;The commands below reproduce YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/data/scripts/get_coco.sh&#34;&gt;COCO&lt;/a&gt; results. &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;Models&lt;/a&gt; and &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/data&#34;&gt;datasets&lt;/a&gt; download automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt;. Training times for YOLOv5n/s/m/l/x are 1/2/4/6/8 days on a V100 GPU (&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/475&#34;&gt;Multi-GPU&lt;/a&gt; times faster). Use the largest &lt;code&gt;--batch-size&lt;/code&gt; possible, or pass &lt;code&gt;--batch-size -1&lt;/code&gt; for YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/pull/5092&#34;&gt;AutoBatch&lt;/a&gt;. Batch sizes shown for V100-16GB.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py --data coco.yaml --cfg yolov5n.yaml --weights &#39;&#39; --batch-size 128&#xA;                                       yolov5s                                64&#xA;                                       yolov5m                                40&#xA;                                       yolov5l                                24&#xA;                                       yolov5x                                16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;img width=&#34;800&#34; src=&#34;https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Improvement&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;In the HDIP.py, each filter and some other modules are included.&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Future work&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;⭐ This code will be completely released after our article is received！！！！⭐ ⭐ Please wait patiently！！！Thank you！！！⭐&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Acknowledgements&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This code is built on &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5 (PyTorch)&lt;/a&gt;. We thank the authors for sharing the codes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ThioJoe/Auto-Synced-Translated-Dubs</title>
    <updated>2022-12-24T01:34:45Z</updated>
    <id>tag:github.com,2022-12-24:/ThioJoe/Auto-Synced-Translated-Dubs</id>
    <link href="https://github.com/ThioJoe/Auto-Synced-Translated-Dubs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatically translates the text of a video based on a subtitle file, and also uses AI voice to dub the video, and synced using the subtitle&#39;s timings&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Auto Synced &amp;amp; Translated Dubs&lt;/h1&gt; &#xA;&lt;p&gt;Automatically translates the text of a video into chosen languages based on a subtitle file, and also uses AI voice to dub the video, while keeping it properly synced to the original video using the subtitle&#39;s timings.&lt;/p&gt; &#xA;&lt;h3&gt;How It Works&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a human-made SRT subtitles file for a video, this will:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use Google Cloud to automatically translate the text, and create new translated SRT files&lt;/li&gt; &#xA; &lt;li&gt;Create text-to-speech audio clips of the translated text (using more realistic neural voices)&lt;/li&gt; &#xA; &lt;li&gt;Use the timings of the subtitle lines to calculate the correct duration of each spoken audio clip&lt;/li&gt; &#xA; &lt;li&gt;Stretch or shrink the translated audio clip to be exactly the same length as the original speech, and inserted at the same point in the audio. Therefore the translated speech will remain perfectly in sync with the original video. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optional (On by Default): Instead of stretching the audio clips, you can instead do a second pass at synthesizing each clip through the API using the proper speaking speed calculated during the first pass. This drastically improves audio quality.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Additional Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creates translated versions of the SRT subtitle file&lt;/li&gt; &#xA; &lt;li&gt;Batch processing of multiple languages in sequence&lt;/li&gt; &#xA; &lt;li&gt;Config files to save translation, synthesis, and language settings for re-use&lt;/li&gt; &#xA; &lt;li&gt;Included script for adding all language audio tracks to a video file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;With ability to merge a sound effects track into each language track&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Included script for translating a YouTube video Title and Description to multiple languages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;h3&gt;External Requirements:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ffmpeg must be installed (&lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;https://ffmpeg.org/download.html&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;You&#39;ll need the binaries for a program called &#39;rubberband&#39; ( &lt;a href=&#34;https://breakfastquay.com/rubberband/&#34;&gt;https://breakfastquay.com/rubberband/&lt;/a&gt; ) . Doesn&#39;t need to be installed, just put both exe&#39;s and the dll file in the same directory as the scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup &amp;amp; Configuration&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download or clone the repo and install the requirements using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I wrote this using Python 3.9 but it will probably work with earlier versions too&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install the programs mentioned in the &#39;External Requirements&#39; above.&lt;/li&gt; &#xA; &lt;li&gt;Setup your Google Cloud (See Wiki) and/or Microsoft Azure API access, and set the variables in &lt;code&gt;cloud_service_settings.ini&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I recommend Azure for the TTS voice synthesizing because they have newer and better voices in my opinion, and in higher quality (Azure supports sample rate up to 48KHz vs 24KHz with Google).&lt;/li&gt; &#xA;   &lt;li&gt;Google Cloud is currently the only option for the text translation part.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set up your configuration settings in &lt;code&gt;config.ini&lt;/code&gt;. The default settings should work in most cases, but read through them especially if you are using Azure for TTS because there are more applicable options you may want to customize. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This config includes options such as the ability to skip text translation, setting formats and sample rate, and using two-pass voice synthesizing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Finally open &lt;code&gt;batch.ini&lt;/code&gt; to set the language and voice settings that will be used for each run. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In the top &lt;code&gt;[SETTINGS]&lt;/code&gt; section you will enter the path to the original video file (used to get the correct audio length), and the original subtitle file path&lt;/li&gt; &#xA;   &lt;li&gt;Also you can use the &lt;code&gt;enabled_languages&lt;/code&gt; variable to list all the languages that will be translated and synthesized at once. The numbers will correspond to the &lt;code&gt;[LANGUAGE-#]&lt;/code&gt; sections in the same config file. The program will process only the languages listed in this variable.&lt;/li&gt; &#xA;   &lt;li&gt;This lets you add as many language presets as you want (such as the preferred voice per language), and can choose which languages you want to use (or not use) for any given run.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to Run:&lt;/strong&gt; After configuring the config files, simply run the main.py script using &lt;code&gt;python main.py&lt;/code&gt; and let it run to completion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Resulting translated subtitle files and dubbed audio tracks will be placed in a folder called &#39;output&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optional:&lt;/strong&gt; You can use the separate &lt;code&gt;TrackAdder.py&lt;/code&gt; script to automatically add the resulting language tracks to an mp4 video file. Requires ffmpeg to be installed. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open the script file with a text editor and change the values in the &#34;User Settings&#34; section at the top.&lt;/li&gt; &#xA;   &lt;li&gt;This will label the tracks so the video file is ready to be uploaded to YouTube. HOWEVER, the multiple audio tracks feature is only available to a limited number of channels. You will most likely need to contact YouTube creator support to ask for access, but there is no guarantee they will grant it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optional:&lt;/strong&gt; You can use the separate &lt;code&gt;TitleTranslator.py&lt;/code&gt; script if uploading to YouTube, which lets you enter a video&#39;s Title and Description, and the text will be translated into all the languages enabled in &lt;code&gt;batch.ini&lt;/code&gt;. They wil be placed together in a single text file in the &#34;output&#34; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Additional Notes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This works best with subtitles that do not remove gaps between sentences and lines.&lt;/li&gt; &#xA; &lt;li&gt;For now the process only assumes there is one speaker. However, if you can make separate SRT files for each speaker, you could generate each TTS track separately using different voices, then combine them afterwards.&lt;/li&gt; &#xA; &lt;li&gt;It only supports Google Translate API for text translation, but it supports both Google and Azure for Text-To-Speech with neural voices.&lt;/li&gt; &#xA; &lt;li&gt;This script was written with my own personal workflow in mind. That is: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I use &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;&lt;strong&gt;OpenAI Whisper&lt;/strong&gt;&lt;/a&gt; to transcribe the videos locally, then use &lt;a href=&#34;https://www.descript.com/&#34;&gt;&lt;strong&gt;Descript&lt;/strong&gt;&lt;/a&gt; to sync that transcription and touch it up with corrections.&lt;/li&gt; &#xA;   &lt;li&gt;Then I export the SRT file with Descript, which is ideal because it does not just butt the start and end times of each subtitle line next to each other. This means the resulting dub will preserve the pauses between sentences from the original speech. If you use subtitles from another program, you might find the pauses between lines are too short.&lt;/li&gt; &#xA;   &lt;li&gt;The SRT export settings in Descript that seem to work decently for dubbing are &lt;em&gt;150 max characters per line&lt;/em&gt;, and &lt;em&gt;1 max line per card&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The &#34;Two Pass&#34; synthesizing feature (can be enabled in the config) will drastically improve the quality of the final result, but will require synthesizing each clip twice, therefore doubling any API costs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;For Result Examples See: &lt;a href=&#34;https://github.com/ThioJoe/Auto-Synced-Translated-Dubs/wiki/Examples&#34;&gt;Examples Wiki Page&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;For Planned Features See: &lt;a href=&#34;https://github.com/ThioJoe/Auto-Synced-Translated-Dubs/wiki/Planned-Features&#34;&gt;Planned Features Wiki Page&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;For Google Cloud Project Setup Instructions See: &lt;a href=&#34;https://github.com/ThioJoe/Auto-Synced-Translated-Dubs/wiki/Instructions:-Obtaining-an-API-Key&#34;&gt;Instructions Wiki Page&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;For Microsoft Azure Setup Instructions See: &lt;a href=&#34;https://github.com/ThioJoe/Auto-Synced-Translated-Dubs/wiki/Instructions:-Microsoft-Azure-Setup&#34;&gt;Azure Instructions Wiki Page&lt;/a&gt;&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/metaseq</title>
    <updated>2022-12-24T01:34:45Z</updated>
    <id>tag:github.com,2022-12-24:/facebookresearch/metaseq</id>
    <link href="https://github.com/facebookresearch/metaseq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repo for external large-scale work&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Metaseq&lt;/h1&gt; &#xA;&lt;p&gt;A codebase for working with &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT&#34;&gt;Open Pre-trained Transformers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Using OPT with 🤗 Transformers&lt;/h3&gt; &#xA;&lt;p&gt;The OPT 125M--66B models are now available in &lt;a href=&#34;https://github.com/huggingface/transformers/releases/tag/v4.19.0&#34;&gt;HuggingFace Transformers&lt;/a&gt;. You can access them under the &lt;code&gt;facebook&lt;/code&gt; organization on the &lt;a href=&#34;https://huggingface.co/facebook&#34;&gt;Hugging Face Hub&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using OPT-175B with Alpa&lt;/h3&gt; &#xA;&lt;p&gt;The OPT 125M--175B models are now supported in the &lt;a href=&#34;https://alpa-projects.github.io/tutorials/opt_serving.html&#34;&gt;Alpa project&lt;/a&gt;, which enables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Using OPT with Colossal-AI&lt;/h3&gt; &#xA;&lt;p&gt;The OPT models are now supported in the &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI#OPT&#34;&gt;Colossal-AI&lt;/a&gt;, which helps users to efficiently and quickly deploy OPT models training and inference, reducing large AI model budgets and scaling down the labor cost of learning and deployment.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started in Metaseq&lt;/h2&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/docs/setup.md&#34;&gt;setup instructions here&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h3&gt;Documentation on workflows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/docs/training.md&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/docs/api.md&#34;&gt;API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Background Info&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/docs/history.md&#34;&gt;Background &amp;amp; relationship to fairseq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/README.md&#34;&gt;Chronicles of training OPT-175B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, bug reports, or feature requests regarding either the codebase or the models released in the projects section, please don&#39;t hesitate to post on our &lt;a href=&#34;https://github.com/facebookresearch/metaseq/issues&#34;&gt;Github Issues page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please remember to follow our &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome PRs from the community!&lt;/p&gt; &#xA;&lt;p&gt;You can find information about contributing to metaseq in our &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/metaseq/main/docs/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;Metaseq is currently maintained by the CODEOWNERS: &lt;a href=&#34;https://github.com/suchenzang&#34;&gt;Susan Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/stephenroller&#34;&gt;Stephen Roller&lt;/a&gt;, &lt;a href=&#34;https://github.com/ngoyal2707&#34;&gt;Naman Goyal&lt;/a&gt;, &lt;a href=&#34;https://github.com/punitkoura&#34;&gt;Punit Singh Koura&lt;/a&gt;, &lt;a href=&#34;https://github.com/moyapchen&#34;&gt;Moya Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/klshuster&#34;&gt;Kurt Shuster&lt;/a&gt;, &lt;a href=&#34;https://github.com/ruanslv&#34;&gt;Ruan Silva&lt;/a&gt;, &lt;a href=&#34;https://github.com/davides&#34;&gt;David Esiobu&lt;/a&gt;, &lt;a href=&#34;https://github.com/dgrnbrg-meta&#34;&gt;David Greenberg&lt;/a&gt;, &lt;a href=&#34;https://github.com/igormolybogFB&#34;&gt;Igor Molybog&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Xirider&#34;&gt;Peter Albert&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Previous maintainers include: &lt;a href=&#34;https://github.com/anj-s&#34;&gt;Anjali Sridhar&lt;/a&gt;, &lt;a href=&#34;https://github.com/m3rlin45&#34;&gt;Christopher Dewan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of metaseq is licensed under the MIT license, however portions of the project are available under separate license terms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Megatron-LM is licensed under the &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM/raw/main/LICENSE&#34;&gt;Megatron-LM license&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>