<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-06T01:38:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/Swin-Transformer</title>
    <updated>2024-01-06T01:38:45Z</updated>
    <id>tag:github.com,2024-01-06:/microsoft/Swin-Transformer</id>
    <link href="https://github.com/microsoft/Swin-Transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an official implementation for &#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Swin Transformer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco?p=swin-transformer-v2-scaling-up-capacity-and&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/object-detection-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-coco?p=swin-transformer-v2-scaling-up-capacity-and&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/instance-segmentation-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=swin-transformer-v2-scaling-up-capacity-and&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/semantic-segmentation-on-ade20k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/action-classification-on-kinetics-400?p=swin-transformer-v2-scaling-up-capacity-and&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/action-classification-on-kinetics-400&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo is the official implementation of &lt;a href=&#34;https://arxiv.org/pdf/2103.14030.pdf&#34;&gt;&#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&#34;&lt;/a&gt; as well as the follow-ups. It currently includes code and models for the following tasks:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Image Classification&lt;/strong&gt;: Included in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/get_started.md&#34;&gt;get_started.md&lt;/a&gt; for a quick start.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Object Detection and Instance Segmentation&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/SwinTransformer/Swin-Transformer-Object-Detection&#34;&gt;Swin Transformer for Object Detection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Semantic Segmentation&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation&#34;&gt;Swin Transformer for Semantic Segmentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Video Action Recognition&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/SwinTransformer/Video-Swin-Transformer&#34;&gt;Video Swin Transformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Semi-Supervised Object Detection&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/microsoft/SoftTeacher&#34;&gt;Soft Teacher&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;SSL: Contrasitive Learning&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/SwinTransformer/Transformer-SSL&#34;&gt;Transformer-SSL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;SSL: Masked Image Modeling&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer/raw/main/get_started.md#simmim-support&#34;&gt;get_started.md#simmim-support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Mixture-of-Experts&lt;/strong&gt;: See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/get_started.md#mixture-of-experts-support&#34;&gt;get_started&lt;/a&gt; for more instructions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Feature-Distillation&lt;/strong&gt;: See &lt;a href=&#34;https://github.com/SwinTransformer/Feature-Distillation&#34;&gt;Feature-Distillation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;12/29/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nvidia&lt;/strong&gt;&#39;s &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer/raw/main/docs/swin_guide.md&#34;&gt;FasterTransformer&lt;/a&gt; now supports Swin Transformer V2 inference, which have significant speed improvements on &lt;code&gt;T4 and A100 GPUs&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;11/30/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Models and codes of &lt;strong&gt;Feature Distillation&lt;/strong&gt; are released. Please refer to &lt;a href=&#34;https://github.com/SwinTransformer/Feature-Distillation&#34;&gt;Feature-Distillation&lt;/a&gt; for details, and the checkpoints (FD-EsViT-Swin-B, FD-DeiT-ViT-B, FD-DINO-ViT-B, FD-CLIP-ViT-B, FD-CLIP-ViT-L).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;09/24/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Merged &lt;a href=&#34;https://github.com/microsoft/SimMIM&#34;&gt;SimMIM&lt;/a&gt;, which is a &lt;strong&gt;Masked Image Modeling&lt;/strong&gt; based pre-training approach applicable to Swin and SwinV2 (and also applicable for ViT and ResNet). Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/get_started.md#simmim-support&#34;&gt;get started with SimMIM&lt;/a&gt; to play with SimMIM pre-training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Released a series of Swin and SwinV2 models pre-trained using the SimMIM approach (see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/MODELHUB.md#simmim-pretrained-swin-v2-models&#34;&gt;MODELHUB for SimMIM&lt;/a&gt;), with model size ranging from SwinV2-Small-50M to SwinV2-giant-1B, data size ranging from ImageNet-1K-10% to ImageNet-22K, and iterations from 125k to 500k. You may leverage these models to study the properties of MIM methods. Please look into the &lt;a href=&#34;https://arxiv.org/abs/2206.04664&#34;&gt;data scaling&lt;/a&gt; paper for more details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;07/09/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;News&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;SwinV2-G achieves &lt;code&gt;61.4 mIoU&lt;/code&gt; on ADE20K semantic segmentation (+1.5 mIoU over the previous SwinV2-G model), using an additional &lt;a href=&#34;https://github.com/SwinTransformer/Feature-Distillation&#34;&gt;feature distillation (FD)&lt;/a&gt; approach, &lt;strong&gt;setting a new recrod&lt;/strong&gt; on this benchmark. FD is an approach that can generally improve the fine-tuning performance of various pre-trained models, including DeiT, DINO, and CLIP. Particularly, it improves CLIP pre-trained ViT-L by +1.6% to reach &lt;code&gt;89.0%&lt;/code&gt; on ImageNet-1K image classification, which is &lt;strong&gt;the most accurate ViT-L model&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Merged a PR from &lt;strong&gt;Nvidia&lt;/strong&gt; that links to faster Swin Transformer inference that have significant speed improvements on &lt;code&gt;T4 and A100 GPUs&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Merged a PR from &lt;strong&gt;Nvidia&lt;/strong&gt; that enables an option to use &lt;code&gt;pure FP16 (Apex O2)&lt;/code&gt; in training, while almost maintaining the accuracy.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;06/03/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Added &lt;strong&gt;Swin-MoE&lt;/strong&gt;, the Mixture-of-Experts variant of Swin Transformer implemented using &lt;a href=&#34;https://github.com/microsoft/tutel&#34;&gt;Tutel&lt;/a&gt; (an optimized Mixture-of-Experts implementation). &lt;strong&gt;Swin-MoE&lt;/strong&gt; is introduced in the &lt;a href=&#34;https://arxiv.org/abs/2206.03382&#34;&gt;TuTel&lt;/a&gt; paper.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;05/12/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretrained models of &lt;a href=&#34;https://arxiv.org/abs/2111.09883&#34;&gt;Swin Transformer V2&lt;/a&gt; on ImageNet-1K and ImageNet-22K are released.&lt;/li&gt; &#xA; &lt;li&gt;ImageNet-22K pretrained models for Swin-V1-Tiny and Swin-V2-Small are released.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;03/02/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Swin Transformer V2 and SimMIM got accepted by CVPR 2022. &lt;a href=&#34;https://github.com/microsoft/SimMIM&#34;&gt;SimMIM&lt;/a&gt; is a self-supervised pre-training approach based on masked image modeling, a key technique that works out the 3-billion-parameter Swin V2 model using &lt;code&gt;40x less labelled data&lt;/code&gt; than that of previous billion-scale models based on JFT-3B.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;02/09/2022&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ðŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Swin-Transformer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;10/12/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Swin Transformer received ICCV 2021 best paper award (Marr Prize).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;08/09/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.09018v2.pdf&#34;&gt;Soft Teacher&lt;/a&gt; will appear at ICCV2021. The code will be released at &lt;a href=&#34;https://github.com/microsoft/SoftTeacher&#34;&gt;GitHub Repo&lt;/a&gt;. &lt;code&gt;Soft Teacher&lt;/code&gt; is an end-to-end semi-supervisd object detection method, achieving a new record on the COCO test-dev: &lt;code&gt;61.3 box AP&lt;/code&gt; and &lt;code&gt;53.0 mask AP&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;07/03/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;strong&gt;Swin MLP&lt;/strong&gt;, which is an adaption of &lt;code&gt;Swin Transformer&lt;/code&gt; by replacing all multi-head self-attention (MHSA) blocks by MLP layers (more precisely it is a group linear layer). The shifted window configuration can also significantly improve the performance of vanilla MLP architectures.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;06/25/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13230&#34;&gt;Video Swin Transformer&lt;/a&gt; is released at &lt;a href=&#34;https://github.com/SwinTransformer/Video-Swin-Transformer&#34;&gt;Video-Swin-Transformer&lt;/a&gt;. &lt;code&gt;Video Swin Transformer&lt;/code&gt; achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including action recognition (&lt;code&gt;84.9&lt;/code&gt; top-1 accuracy on Kinetics-400 and &lt;code&gt;86.1&lt;/code&gt; top-1 accuracy on Kinetics-600 with &lt;code&gt;~20x&lt;/code&gt; less pre-training data and &lt;code&gt;~3x&lt;/code&gt; smaller model size) and temporal modeling (&lt;code&gt;69.6&lt;/code&gt; top-1 accuracy on Something-Something v2).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;05/12/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Used as a backbone for &lt;code&gt;Self-Supervised Learning&lt;/code&gt;: &lt;a href=&#34;https://github.com/SwinTransformer/Transformer-SSL&#34;&gt;Transformer-SSL&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Using Swin-Transformer as the backbone for self-supervised learning enables us to evaluate the transferring performance of the learnt representations on down-stream tasks, which is missing in previous works due to the use of ViT/DeiT, which has not been well tamed for down-stream tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;04/12/2021&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Initial commits:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretrained models on ImageNet-1K (&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth&#34;&gt;Swin-T-IN1K&lt;/a&gt;, &lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth&#34;&gt;Swin-S-IN1K&lt;/a&gt;, &lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth&#34;&gt;Swin-B-IN1K&lt;/a&gt;) and ImageNet-22K (&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth&#34;&gt;Swin-B-IN22K&lt;/a&gt;, &lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth&#34;&gt;Swin-L-IN22K&lt;/a&gt;) are provided.&lt;/li&gt; &#xA; &lt;li&gt;The supported code and models for ImageNet-1K image classification, COCO object detection and ADE20K semantic segmentation are provided.&lt;/li&gt; &#xA; &lt;li&gt;The cuda kernel implementation for the &lt;a href=&#34;https://arxiv.org/pdf/1904.11491.pdf&#34;&gt;local relation layer&lt;/a&gt; is provided in branch &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer/tree/LR-Net&#34;&gt;LR-Net&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Swin Transformer&lt;/strong&gt; (the name &lt;code&gt;Swin&lt;/code&gt; stands for &lt;strong&gt;S&lt;/strong&gt;hifted &lt;strong&gt;win&lt;/strong&gt;dow) is initially described in &lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34;&gt;arxiv&lt;/a&gt;, which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.&lt;/p&gt; &#xA;&lt;p&gt;Swin Transformer achieves strong performance on COCO object detection (&lt;code&gt;58.7 box AP&lt;/code&gt; and &lt;code&gt;51.1 mask AP&lt;/code&gt; on test-dev) and ADE20K semantic segmentation (&lt;code&gt;53.5 mIoU&lt;/code&gt; on val), surpassing previous models by a large margin.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/figures/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Main Results on ImageNet with Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageNet-1K and ImageNet-22K Pretrained Swin-V1 Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@5&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;22K model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1K model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;755&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/156nWJy4Q28rDlrX-rRbI3w&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_tiny_patch4_window7_224.yaml&#34;&gt;config&lt;/a&gt;/&lt;a href=&#34;https://github.com/SwinTransformer/storage/files/7745562/log_swin_tiny_patch4_window7_224.txt&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;437&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1KFjpj3Efey3LmtE1QqPeQg&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_small_patch4_window7_224.yaml&#34;&gt;config&lt;/a&gt;/&lt;a href=&#34;https://github.com/SwinTransformer/storage/files/7745563/log_swin_small_patch4_window7_224.txt&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;278&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/16bqCTEc70nC_isSsgBSaqQ&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_base_patch4_window7_224.yaml&#34;&gt;config&lt;/a&gt;/&lt;a href=&#34;https://github.com/SwinTransformer/storage/files/7745564/log_swin_base_patch4_window7_224.txt&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1xT1cu740-ejW7htUdVLnmw&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_base_patch4_window12_384_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;755&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1vct0VYwwQQ8PYkBjwSSBZQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_tiny_patch4_window7_224_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22kto1k_finetune.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1K0OO-nGZDPkR8fm_r83e8Q?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_tiny_patch4_window7_224_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;437&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/11NC1xdT5BAGBgazdTme5Sg?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_small_patch4_window7_224_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22kto1k_finetune.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/10RFVfjQJhwPfeHrmxQUaLw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_small_patch4_window7_224_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;278&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1y1Ec3UlrKSI8IMtEs-oBXA&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_base_patch4_window7_224_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1n_wNkcbRxVXit8r_KrfAVg&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_base_patch4_window7_224_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1vwJxnJcVqcLZAw9HaqiR6g&#34;&gt;baidu&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1caKTSdoLJYoi4WBcnmWuWg&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_base_patch4_window12_384_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;197M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;141&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1pws3rOTFuOebBYP3h6Kx8w&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_large_patch4_window7_224_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1NkQApMWUhxBGjk1ne6VqBQ&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_large_patch4_window7_224_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;197M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;103.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1sl7o_bJA143OD7UqSLAMoA&#34;&gt;baidu&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1X0FLHQyPOC6Kmv2CmgxJvA&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_large_patch4_window12_384_22kto1k_finetune.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageNet-1K and ImageNet-22K Pretrained Swin-V2 Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;window&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@5&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;22K model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1K model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;572&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1RzLkAH_5OtfRCJe6Vlg6rg?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_tiny_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;327&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/195PdA41szEduW3jEtRSa4Q?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_small_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.3G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;217&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/18AfMSz3dPyzIvP1dKuERvQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;437&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1dyK3cK9Xipmv6RnTtrPocw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_tiny_patch4_window16_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;257&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1ZIPiSfWNKTPp821Ka-Mifw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_small_patch4_window16_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.8G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;174&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1dlDQGn8BXCmnh7wQSM5Nhw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window16_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-B&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.8G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;174&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1sgstld4MgGsZxhUAW7MlmQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-B&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24x24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1Xc2rsSsRQz_sy5mjgfxrMQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window12_192_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/17u3sEQaUYlvfL195rrORzQ?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-L&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;197M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1pqp31N80qIWjFPbudzB6Bw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinV2-L&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24x24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;197M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;115.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/11PhCV7qAGXtZ8dXNgyiGOw?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_large_patch4_window12_192_22k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/13URdNkygr3Xn0N3e6IwjgA?pwd=swin&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swinv2/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SwinV2-B&lt;sup&gt;*&lt;/sup&gt; (SwinV2-L&lt;sup&gt;*&lt;/sup&gt;) with input resolution of 256x256 and 384x384 both fine-tuned from the same pre-training model using a smaller input resolution of 192x192.&lt;/li&gt; &#xA; &lt;li&gt;SwinV2-B&lt;sup&gt;*&lt;/sup&gt; (384x384) achieves 78.08 acc@1 on ImageNet-1K-V2 while SwinV2-L&lt;sup&gt;*&lt;/sup&gt; (384x384) achieves 78.31.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageNet-1K Pretrained Swin MLP Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@5&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1K model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.01601.pdf&#34;&gt;Mixer-B/16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/vision_transformer&#34;&gt;official repo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.03404&#34;&gt;ResMLP-S24&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;715&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;timm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.03404&#34;&gt;ResMLP-B24&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;116M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;231&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;timm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T/C24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;563&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_tiny_c24_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/17k-7l6Sxt7uZ7IV0f26GNQ&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_tiny_c24_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinMLP-T/C24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;807&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c24_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1Sa4vP5R0M2RjfIe9HIga-Q&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_mlp_tiny_c24_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinMLP-T/C12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;792&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c12_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1mM9J2_DEVZHUB5ASIpFl0w&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_mlp_tiny_c12_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinMLP-T/C6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;766&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_tiny_c6_patch4_window8_256.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1hUTYVT2W1CsjICw-3W-Vjg&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_mlp_tiny_c6_patch4_window8_256.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinMLP-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;409&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.5/swin_mlp_base_patch4_window7_224.pth&#34;&gt;github&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1zww3dnbX3GxNiGfb-GwyUg&#34;&gt;baidu&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/configs/swin/swin_mlp_base_patch4_window7_224.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: access code for &lt;code&gt;baidu&lt;/code&gt; is &lt;code&gt;swin&lt;/code&gt;. C24 means each head has 24 channels.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageNet-22K Pretrained Swin-MoE Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/get_started.md#mixture-of-experts-support&#34;&gt;get_started&lt;/a&gt; for instructions on running Swin-MoE.&lt;/li&gt; &#xA; &lt;li&gt;Pretrained models for Swin-MoE can be found in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/MODELHUB.md#imagenet-22k-pretrained-swin-moe-models&#34;&gt;MODEL HUB&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main Results on Downstream Tasks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;COCO Object Detection (2017 val)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lr Schd&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;box mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mask mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mask R-CNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;267G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mask R-CNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;359G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cascade Mask R-CNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;745G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cascade Mask R-CNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;107M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;838G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cascade Mask R-CNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;145M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;982G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RepPoints V2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;283G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mask RepPoints V2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;292G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HTC++&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1043G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HTC++&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;284M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1470G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HTC++&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;284M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: &lt;sup&gt;*&lt;/sup&gt; indicates multi-scale testing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADE20K Semantic Segmentation (val)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Crop Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lr Schd&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU (ms+flip)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UPerNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;945G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1038G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;121M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1188G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UPerNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;121M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1841G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3230G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citing Swin Transformer&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{liu2021Swin,&#xA;  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},&#xA;  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},&#xA;  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing Local Relation Networks (the first full-attention visual backbone)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{hu2019local,&#xA;  title={Local Relation Networks for Image Recognition},&#xA;  author={Hu, Han and Zhang, Zheng and Xie, Zhenda and Lin, Stephen},&#xA;  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},&#xA;  pages={3464--3473},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing Swin Transformer V2&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{liu2021swinv2,&#xA;  title={Swin Transformer V2: Scaling Up Capacity and Resolution}, &#xA;  author={Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},&#xA;  booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing SimMIM (a self-supervised approach that enables SwinV2-G)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{xie2021simmim,&#xA;  title={SimMIM: A Simple Framework for Masked Image Modeling},&#xA;  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},&#xA;  booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing SimMIM-data-scaling&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{xie2022data,&#xA;  title={On Data Scaling in Masked Image Modeling},&#xA;  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Wei, Yixuan and Dai, Qi and Hu, Han},&#xA;  journal={arXiv preprint arXiv:2206.04664},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing Swin-MoE&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{hwang2022tutel,&#xA;      title={Tutel: Adaptive Mixture-of-Experts at Scale}, &#xA;      author={Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},&#xA;      year={2022},&#xA;      eprint={2206.03382},&#xA;      archivePrefix={arXiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For &lt;strong&gt;Image Classification&lt;/strong&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/get_started.md&#34;&gt;get_started.md&lt;/a&gt; for detailed instructions.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;Object Detection and Instance Segmentation&lt;/strong&gt;, please see &lt;a href=&#34;https://github.com/SwinTransformer/Swin-Transformer-Object-Detection&#34;&gt;Swin Transformer for Object Detection&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;Semantic Segmentation&lt;/strong&gt;, please see &lt;a href=&#34;https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation&#34;&gt;Swin Transformer for Semantic Segmentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;, please see &lt;a href=&#34;https://github.com/SwinTransformer/Transformer-SSL&#34;&gt;Transformer-SSL&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;Video Recognition&lt;/strong&gt;, please see &lt;a href=&#34;https://github.com/SwinTransformer/Video-Swin-Transformer&#34;&gt;Video Swin Transformer&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Third-party Usage and Experiments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;In this pargraph, we cross link third-party repositories which use Swin and report results. You can let us know by raising an issue&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;(&lt;code&gt;Note please report accuracy numbers and provide trained models in your new repository to facilitate others to get sense of correctness and model behavior&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;[12/29/2022] Swin Transformers (V2) inference implemented in FasterTransformer: &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer/raw/main/docs/swin_guide.md&#34;&gt;FasterTransformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[06/30/2022] Swin Transformers (V1) inference implemented in FasterTransformer: &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer/raw/main/docs/swin_guide.md&#34;&gt;FasterTransformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[05/12/2022] Swin Transformers (V1) implemented in TensorFlow with the pre-trained parameters ported into them. Find the implementation, TensorFlow weights, code example here in &lt;a href=&#34;https://github.com/sayakpaul/swin-transformers-tf/&#34;&gt;this repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[04/06/2022] Swin Transformer for Audio Classification: &lt;a href=&#34;https://github.com/RetroCirce/HTS-Audio-Transformer&#34;&gt;Hierarchical Token Semantic Audio Transformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[12/21/2021] Swin Transformer for StyleGAN: &lt;a href=&#34;https://github.com/microsoft/StyleSwin&#34;&gt;StyleSwin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[12/13/2021] Swin Transformer for Face Recognition: &lt;a href=&#34;https://github.com/JDAI-CV/FaceX-Zoo&#34;&gt;FaceX-Zoo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[08/29/2021] Swin Transformer for Image Restoration: &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;SwinIR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[08/12/2021] Swin Transformer for person reID: &lt;a href=&#34;https://github.com/layumi/Person_reID_baseline_pytorch&#34;&gt;https://github.com/layumi/Person_reID_baseline_pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[06/29/2021] Swin-Transformer in PaddleClas and inference based on whl package: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleClas&#34;&gt;https://github.com/PaddlePaddle/PaddleClas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[04/14/2021] Swin for RetinaNet in Detectron: &lt;a href=&#34;https://github.com/xiaohu2015/SwinT_detectron2&#34;&gt;https://github.com/xiaohu2015/SwinT_detectron2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[04/16/2021] Included in a famous model zoo: &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[04/20/2021] Swin-Transformer classifier inference using TorchServe: &lt;a href=&#34;https://github.com/kamalkraj/Swin-Transformer-Serve&#34;&gt;https://github.com/kamalkraj/Swin-Transformer-Serve&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InternLM/tutorial</title>
    <updated>2024-01-06T01:38:45Z</updated>
    <id>tag:github.com,2024-01-06:/InternLM/tutorial</id>
    <link href="https://github.com/InternLM/tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡åž‹å®žæˆ˜è¥&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/InternLM/tutorial/main/asset/camp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸ºäº†æŽ¨åŠ¨å¤§æ¨¡åž‹åœ¨æ›´å¤šè¡Œä¸šè½åœ°å¼€èŠ±ï¼Œè®©å¼€å‘è€…ä»¬æ›´é«˜æ•ˆçš„å­¦ä¹ å¤§æ¨¡åž‹çš„å¼€å‘ä¸Žåº”ç”¨ï¼Œä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤é‡ç£…æŽ¨å‡ºä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡åž‹å®žæˆ˜è¥ï¼Œä¸ºå¹¿å¤§å¼€å‘è€…æ­å»ºå¤§æ¨¡åž‹å­¦ä¹ å’Œå®žè·µå¼€å‘çš„å¹³å°ï¼Œä¸¤å‘¨æ—¶é—´å¸¦ä½ çŽ©è½¬å¤§æ¨¡åž‹å¾®è°ƒã€éƒ¨ç½²ä¸Žè¯„æµ‹å…¨é“¾è·¯ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å®žæˆ˜è¥ç¬¬äºŒæ‰¹ç«çƒ­æŠ¥åä¸­ï¼Œæ¬¢è¿Žå¡«å†™&lt;a href=&#34;https://www.wjx.top/vm/Yzzz2mi.aspx?udsid=876275&#34;&gt;è¡¨å•&lt;/a&gt;æŠ¥åï¼&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ˜Š ä½ å°†èŽ·å¾—&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ‘¨â€ðŸ« å®žåŠ›è®²å¸ˆï¼šæ¥è‡ªå‰æ²¿ç§‘ç ”æœºæž„ã€ä¸€çº¿å¤§åŽ‚å’Œ Github çƒ­é—¨å¼€æºé¡¹ç›®çš„è®²å¸ˆæ‰‹æŠŠæ‰‹æ•™å­¦&lt;br&gt; ðŸ’» ç®—åŠ›æ”¯æŒï¼šç®—åŠ›èµ„æºå…è´¹æä¾›ï¼ŒåŠ©åŠ›æ— å¿§è®­ç»ƒå¤§æ¨¡åž‹&lt;br&gt; ðŸ’¬ ä¸“å±žç¤¾ç¾¤ï¼šåŠ©æ•™ã€è®²å¸ˆå…¨ç¨‹é™ªä¼´ï¼Œæä¾›å½•æ’­å›žæ”¾ã€çº¿ä¸Šç­”ç–‘åŠå®žæˆ˜ä½œä¸šè¾…å¯¼&lt;br&gt; ðŸ“œ å®˜æ–¹è®¤è¯ï¼šä¼˜ç§€å­¦å‘˜å°†èŽ·å¾—è£èª‰è¯ä¹¦ï¼Œä¼˜ç§€é¡¹ç›®æœ‰æœºä¼šè¢«å®˜æ–¹æ”¶å½•ï¼ŒèŽ·å¾—æ›´å¤šå±•ç¤º&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ“… è¯¾ç¨‹å®‰æŽ’&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;è¯¾ç¨‹æ—¶é—´&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;è¯¾ç¨‹å†…å®¹&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;è®²å¸ˆ&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;èµ„æ–™&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 1 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡åž‹å…¨é“¾è·¯å¼€æºå¼€æ”¾ä½“ç³»&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é™ˆæº &lt;br&gt;ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤é’å¹´ç§‘å­¦å®¶&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Rc411b7ns/&#34;&gt;è§†é¢‘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 2 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è½»æ¾åˆ†é’ŸçŽ©è½¬ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡åž‹è¶£å‘³ Demo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å®‹å¿—å­¦&lt;br&gt;d2l-ai-solutions-manual å¼€æºé¡¹ç›®è´Ÿè´£äºº&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/tutorial/main/helloworld/hello_world.md&#34;&gt;æ–‡æ¡£&lt;/a&gt;ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 3 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åŸºäºŽ InternLM å’Œ Langchain æ­å»ºä½ çš„çŸ¥è¯†åº“&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é‚¹é›¨è¡¡&lt;br&gt;prompt-engineering-for-developers å¼€æºé¡¹ç›®è´Ÿè´£äºº&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/tutorial/main/langchain/readme.md&#34;&gt;æ–‡æ¡£&lt;/a&gt;ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 4 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;XTuner å¤§æ¨¡åž‹å•å¡ä½Žæˆæœ¬å¾®è°ƒå®žæˆ˜&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ±ªå‘¨è°¦&lt;br&gt;XTuner ç¤¾åŒºè´¡çŒ®è€…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/tutorial/main/xtuner/README.md&#34;&gt;æ–‡æ¡£&lt;/a&gt;ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 5 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LMDeploy å¤§æ¨¡åž‹é‡åŒ–éƒ¨ç½²å®žè·µ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é•¿ç´&lt;br&gt;HuggingLLMå¼€æºé¡¹ç›®è´Ÿè´£äºº&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/tutorial/main/lmdeploy/lmdeploy.md&#34;&gt;æ–‡æ¡£&lt;/a&gt;ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 6 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OpenCompass å¤§æ¨¡åž‹è¯„æµ‹è§£è¯»åŠå®žæˆ˜æŒ‡å—&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ›¹èŒ‚æ¾&lt;br&gt;OpenCompass æ ¸å¿ƒå¼€å‘è€…&lt;br&gt;&lt;br&gt;çŽ‹åŠª&lt;br&gt;OpenCompass ç¤¾åŒºè´¡çŒ®è€…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ–‡æ¡£ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ 7 èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å½©è›‹çŽ¯èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¥žç§˜å˜‰å®¾&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ–‡æ¡£ã€è§†é¢‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ðŸ“ ä½œä¸š&lt;/h2&gt; &#xA;&lt;p&gt;åŠ©æ•™è€å¸ˆå°†åœ¨ç¤¾ç¾¤ä¸­å…¬å¸ƒæ¯èŠ‚è¯¾çš„ä½œä¸šåŠæäº¤æ–¹å¼&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ–¥ï¸ ç®—åŠ›å¹³å°&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://studio.intern-ai.org.cn/&#34;&gt;https://studio.intern-ai.org.cn/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;é—®å·ä¸­çš„æ ¸é”€ç ä¸ºé‚€è¯·ç ï¼Œå¦‚é¢å¤–éœ€è¦å……å€¼ç®—åŠ›æˆ–è€…å…¶ä»–ä»»ä½•ç–‘é—®è¯·è”ç³»æµ¦è¯­å°åŠ©æ‰‹(å¾®ä¿¡å·æœç´¢ï¼šInternLM)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ali-vilab/dreamtalk</title>
    <updated>2024-01-06T01:38:45Z</updated>
    <id>tag:github.com,2024-01-06:/ali-vilab/dreamtalk</id>
    <link href="https://github.com/ali-vilab/dreamtalk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementations for paper: DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt;DreamTalk: When Expressive Talking Head Generation &lt;br&gt; Meets Diffusion Probabilistic Models&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dreamtalk-project.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.09767&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/VF4vlE6ZqWQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ali-vilab/dreamtalk/main/media/teaser.gif&#34; alt=&#34;teaser&#34; title=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DreamTalk is a diffusion-based audio-driven expressive talking head generation framework that can produce high-quality talking head videos across diverse speaking styles. DreamTalk exhibits robust performance with a diverse array of inputs, including songs, speech in multiple languages, noisy audio, and out-of-domain portraits.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01]&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/fffiloni&#34;&gt;fffiloni&lt;/a&gt; provides a HuggingFace Space &lt;a href=&#34;https://huggingface.co/spaces/fffiloni/dreamtalk&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt;. Thanks~&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; Release inference code and pretrained checkpoint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n dreamtalk python=3.7.0&#xA;conda activate dreamtalk&#xA;pip install -r requirements.txt&#xA;conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;conda update ffmpeg&#xA;&#xA;pip install urllib3==1.26.6&#xA;pip install transformers==4.28.1&#xA;pip install dlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;Download the checkpoint of the denoising network and the renderer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/damo-vilab/dreamtalk&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/dreamtalk/files&#34;&gt;ModelScope&lt;/a&gt; (in &lt;code&gt;checkpoints&lt;/code&gt; folder)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Put the downloaded checkpoints into &lt;code&gt;checkpoints&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Run the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_for_demo_video.py \&#xA;--wav_path data/audio/acknowledgement_english.m4a \&#xA;--style_clip_path data/style_clip/3DMM/M030_front_neutral_level1_001.mat \&#xA;--pose_path data/pose/RichardShelby_front_neutral_level1_001.mat \&#xA;--image_path data/src_img/uncropped/male_face.png \&#xA;--cfg_scale 1.0 \&#xA;--max_gen_len 30 \&#xA;--output_name acknowledgement_english@M030_front_neutral_level1_001@male_face&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;wav_path&lt;/code&gt; specifies the input audio. The input audio file extensions such as wav, mp3, m4a, and mp4 (video with sound) should all be compatible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;style_clip_path&lt;/code&gt; specifies the reference speaking style and &lt;code&gt;pose_path&lt;/code&gt; specifies head pose. They are 3DMM parameter sequences extracted from reference videos. You can follow &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRenderer&lt;/a&gt; to extract 3DMM parameters from your own videos. Note that the video frame rate should be 25 FPS. Besides, videos used for head pose reference should be first cropped to $256\times256$ using scripts in &lt;a href=&#34;https://github.com/AliaksandrSiarohin/video-preprocessing&#34;&gt;FOMM video preprocessing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;image_path&lt;/code&gt; specifies the input portrait. Its resolution should be larger than $256\times256$. Frontal portraits, with the face directly facing forward and not tilted to one side, usually achieve satisfactory results. The input portrait will be cropped to $256\times256$. If your portrait is already cropped to $256\times256$ and you want to disable cropping, use option &lt;code&gt;--disable_img_crop&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_for_demo_video.py \&#xA;--wav_path data/audio/acknowledgement_chinese.m4a \&#xA;--style_clip_path data/style_clip/3DMM/M030_front_surprised_level3_001.mat \&#xA;--pose_path data/pose/RichardShelby_front_neutral_level1_001.mat \&#xA;--image_path data/src_img/cropped/zp1.png \&#xA;--disable_img_crop \&#xA;--cfg_scale 1.0 \&#xA;--max_gen_len 30 \&#xA;--output_name acknowledgement_chinese@M030_front_surprised_level3_001@zp1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;cfg_scale&lt;/code&gt; controls the scale of classifer-free guidance. It can adjust the intensity of speaking styles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;max_gen_len&lt;/code&gt; is the maximum video generation duration, measured in seconds. If the input audio exceeds this length, it will be truncated.&lt;/p&gt; &#xA;&lt;p&gt;The generated video will be named &lt;code&gt;$(output_name).mp4&lt;/code&gt; and put in the output_video folder. Intermediate results, including the cropped portrait, will be in the &lt;code&gt;tmp/$(output_name)&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Sample inputs are presented in &lt;code&gt;data&lt;/code&gt; folder. Due to copyright issues, we are unable to include the songs we have used in this folder.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run this program on CPU, please add &lt;code&gt;--device=cpu&lt;/code&gt; to the command line arguments. (Thank &lt;a href=&#34;https://github.com/lukevs&#34;&gt;lukevs&lt;/a&gt; for adding CPU support.)&lt;/p&gt; &#xA;&lt;h2&gt;Ad-hoc solutions to improve resolution&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of this method is to achieve accurate lip-sync and produce vivid expressions across diverse speaking styles. The resolution was not considered in the initial design process. There are two ad-hoc solutions to improve resolution. The first option is to utilize &lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;CodeFormer&lt;/a&gt;, which can achieve a resolution of $1024\times1024$; however, it is relatively slow, processing only one frame per second on an A100 GPU, and suffers from issues with temporal inconsistency. The second option is to employ the Temporal Super-Resolution Model from &lt;a href=&#34;https://github.com/Meta-Portrait/MetaPortrait&#34;&gt;MetaPortrait&lt;/a&gt;, which attains a resolution of $512\times512$, offers a faster performance of 10 frames per second, and maintains temporal coherence. However, these super-resolution modules may reduce the intensity of facial emotions.&lt;/p&gt; &#xA;&lt;p&gt;The sample results after super-resolution processing are in the &lt;code&gt;output_video&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We extend our heartfelt thanks for the invaluable contributions made by preceding works to the development of DreamTalk. This includes, but is not limited to: &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRenderer&lt;/a&gt; ,&lt;a href=&#34;https://github.com/FuxiVirtualHuman/AAAI22-one-shot-talking-face&#34;&gt;AVCT&lt;/a&gt; ,&lt;a href=&#34;https://github.com/FuxiVirtualHuman/styletalk&#34;&gt;StyleTalk&lt;/a&gt; ,&lt;a href=&#34;https://github.com/sicxu/Deep3DFaceRecon_pytorch&#34;&gt;Deep3DFaceRecon_pytorch&lt;/a&gt; ,&lt;a href=&#34;https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english&#34;&gt;Wav2vec2.0&lt;/a&gt; ,&lt;a href=&#34;https://github.com/luost26/diffusion-point-cloud&#34;&gt;diffusion-point-cloud&lt;/a&gt; ,&lt;a href=&#34;https://github.com/AliaksandrSiarohin/video-preprocessing&#34;&gt;FOMM video preprocessing&lt;/a&gt;. We are dedicated to advancing upon these foundational works with the utmost respect for their original contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this codebase useful for your research, please use the following entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{ma2023dreamtalk,&#xA;  title={DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models},&#xA;  author={Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong},&#xA;  journal={arXiv preprint arXiv:2312.09767},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>