<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-28T01:34:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>willccbb/verifiers</title>
    <updated>2025-05-28T01:34:38Z</updated>
    <id>tag:github.com,2025-05-28:/willccbb/verifiers</id>
    <link href="https://github.com/willccbb/verifiers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Verifiers for LLM Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Verifiers: Reinforcement Learning with LLMs in Verifiable Environments&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains a set of tools for reinforcement learning with LLMs in verifiable environments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; This repository in its current state should be viewed as &lt;strong&gt;in-progress research code&lt;/strong&gt;, and is not guaranteed to yield stable or optimal training results. Best results will likely be found on reasonable timescales when using 7B+ models, and at least 8 GPUs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don&#39;t need multi-turn tool calling or agentic interactions, you should probably just use TRL (or Unsloth/Axolotl) for GRPO. This is mostly a multi-turn LLM RL repo with some other bells and whistles.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/verifiers/&#34;&gt;coming soon&lt;/a&gt;, for now just do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/willccbb/verifiers.git&#xA;cd verifiers&#xA;uv sync&#xA;uv pip install flash-attn --no-build-isolation&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If you encounter version issues, please confirm that you are able to run basic TRL training in your environment before opening an issue (see &lt;code&gt;verifiers/examples/trl_grpo.py&lt;/code&gt; as a reference).&lt;/p&gt; &#xA;&lt;h2&gt;Usage (Multi-GPU)&lt;/h2&gt; &#xA;&lt;h3&gt;Training with Multi-Turn GRPO&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;code&gt;verifiers/examples/math_train.py&lt;/code&gt; for an example with the ToolEnv environment + a Python tool.&lt;/p&gt; &#xA;&lt;p&gt;To run on a 8-GPU node with 4 inference GPUs and 4 training GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Launch vLLM inference server from verifiers/, with .venv active&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 python verifiers/inference/vllm_serve.py --model &#34;Qwen/Qwen2.5-7B-Instruct&#34; --tensor_parallel_size 4 --max_model_len 8192  --gpu_memory_utilization 0.9 --enable_prefix_caching True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run training script from verifiers/, with .venv active&#xA;CUDA_VISIBLE_DEVICES=4,5,6,7 accelerate launch --num-processes 4 --config-file configs/zero3.yaml verifiers/examples/math_train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multi-node training setups are supported as well; you can specify the host IP + port of your inference as an argument in the &lt;code&gt;GRPOConfig&lt;/code&gt; in your training script. See the TRL &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOTrainer&#34;&gt;docs&lt;/a&gt; for info on multi-node training via SLURM.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;You can also use environment classes to evaluate models with multi-turn tool use offline, i.e. without RL training. See &lt;code&gt;verifiers/examples/math_eval.py&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;h3&gt;Custom Environments&lt;/h3&gt; &#xA;&lt;p&gt;To create your own multi-turn environment, inherit from &lt;code&gt;MultiTurnEnv&lt;/code&gt; and implement:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_completed(self, messages: List[Dict[str, str]], **kwargs: Any) -&amp;gt; bool:&#xA;    pass&#xA;&#xA;def env_response(self, messages: List[Dict[str, str]], **kwargs: Any) -&amp;gt; Dict[str, str]:&#xA;    pass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Environments (&lt;code&gt;MultiTurnEnv&lt;/code&gt;): &lt;code&gt;DoubleCheckEnv&lt;/code&gt;, &lt;code&gt;CodeEnv&lt;/code&gt;, &lt;code&gt;ToolEnv&lt;/code&gt;, &lt;code&gt;SmolaToolEnv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-turn tool use in &lt;code&gt;ToolEnv&lt;/code&gt;, &lt;code&gt;SmolaToolEnv&lt;/code&gt;, &lt;code&gt;CodeEnv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset formatting + XML parsers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic rubrics for math/code correctness + formatting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Defaults for GRPO, model, tokenizer, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{brown2025verifiers,&#xA;  title={Verifiers: Reinforcement Learning with LLMs in Verifiable Environments},&#xA;  author={Brown, William},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>