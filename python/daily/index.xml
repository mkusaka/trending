<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-20T01:38:43Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZHO-ZHO-ZHO/ComfyUI-PhotoMaker</title>
    <updated>2024-01-20T01:38:43Z</updated>
    <id>tag:github.com,2024-01-20:/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker</id>
    <link href="https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of PhotoMaker for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/15f9ebaf-b205-4cbd-928e-eca1a0cacb7f&#34; alt=&#34;PNSTYLE_23png&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ComfyUI PhotoMaker&lt;/h1&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;PhotoMaker&lt;/a&gt; for ComfyUI&lt;/p&gt; &#xA;&lt;!--&#xA;![Dingtalk_20240117150313](https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/da664c2b-cb30-44e2-85ec-d6070fcfa8f0)&#xA;&#xA;&#xA;![Dingtalk_20240117161736](https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/07c924ab-3ee5-4919-87bc-ac49c28914f1)&#xA;---&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/0292bf55-21b7-4025-bc27-7e3e7ccc2af3&#34; alt=&#34;Dingtalk_20240118163802&#34;&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;![Dingtalk_20240118163953](https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/9b8a665f-6c9c-441c-aa81-fc56423de89e)&#xA;---&gt; &#xA;&lt;p&gt;单张参考与多张参考的对比：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/e7bccd61-7855-46c2-a6bc-31b34e742927&#34; alt=&#34;Dingtalk_20240117201650&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/6bbcfcf9-9027-4c6f-9be1-750971b7848c&#34; alt=&#34;Dingtalk_20240117201201&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目介绍 | Info&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;来自对&lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;PhotoMaker&lt;/a&gt;的非官方实现&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;版本：V2.5 支持lora、支持多批次、支持通用的styler&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;节点拆分 + 支持本地模型 + 支持自定义尺寸 +提速3倍 + 支持多图直接输入&#xA;---&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/d067fc21-3b51-44bc-b76e-9351a7f6966a&#34; alt=&#34;Dingtalk_20240119194547&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;视频演示&lt;/h2&gt; &#xA;&lt;!--&#xA;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/8718a70e-a5d7-463b-b36e-de1ffefad9ed&#xA;---&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/d58af6e7-d0f3-41ff-ab33-195cb6d66e9e&#34;&gt;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/d58af6e7-d0f3-41ff-ab33-195cb6d66e9e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;节点说明 | Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;基础模型加载 | base model loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📷Base Model Loader from hub🤗：支持从 huggingface hub 自动下载模型，输入模型名称（如：SG161222/RealVisXL_V3.0）即可&lt;/li&gt; &#xA;   &lt;li&gt;📷Base Model Loader locally：支持加载本地模型（需 SDXL 系列模型）&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;PhotoMaker Adapter 模型加载 | PhotoMaker Adapter Loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📷PhotoMaker Adapter Loader from hub🤗：支持从 huggingface hub 自动下载模型&lt;/li&gt; &#xA;   &lt;li&gt;📷PhotoMaker Adapter Loader locally：支持加载本地模型，输入 photomaker-v1.bin 模型所在路径即可&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;参考图预处理 | 📷Ref Image Preprocessing&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;直接模式 | Direct_input：接入单/多张图像（非必要项）&lt;/li&gt; &#xA;   &lt;li&gt;路径模式 | Path_input：自动读取路径中的所有图像&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lora模型加载 | 📷LoRALoader 🆕&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持加载本地 lora 模型&lt;/li&gt; &#xA;   &lt;li&gt;支持权重调节&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;提示词 + 风格 | 📷Prompt_Styler 🆕&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;与各种提示词（文本）输入（如肖像大师等）、styler兼容&lt;/li&gt; &#xA;   &lt;li&gt;prompt、negative：正负提示词&lt;/li&gt; &#xA;   &lt;li&gt;支持权重调节&lt;/li&gt; &#xA;   &lt;li&gt;style_name：支持官方提供的10种风格 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;(No style)&lt;/li&gt; &#xA;     &lt;li&gt;Cinematic&lt;/li&gt; &#xA;     &lt;li&gt;Disney Charactor&lt;/li&gt; &#xA;     &lt;li&gt;Digital Art&lt;/li&gt; &#xA;     &lt;li&gt;Photographic (Default)&lt;/li&gt; &#xA;     &lt;li&gt;Fantasy art&lt;/li&gt; &#xA;     &lt;li&gt;Neonpunk&lt;/li&gt; &#xA;     &lt;li&gt;Enhance&lt;/li&gt; &#xA;     &lt;li&gt;Comic book&lt;/li&gt; &#xA;     &lt;li&gt;Lowpoly&lt;/li&gt; &#xA;     &lt;li&gt;Line art&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;PhotoMaker 生成 | 📷PhotoMaker Generation 🆕&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;pipe：接入模型&lt;/li&gt; &#xA;   &lt;li&gt;pil_image：接入预处理图像&lt;/li&gt; &#xA;   &lt;li&gt;positivet、negative：正负提示词&lt;/li&gt; &#xA;   &lt;li&gt;batch_size：生成数量&lt;/li&gt; &#xA;   &lt;li&gt;style_strength_ratio：风格混合强度（高于30按30计算）&lt;/li&gt; &#xA;   &lt;li&gt;step：步数，官方默认50步，但毕竟是基于SDXL模型，我实测下来30步足够了&lt;/li&gt; &#xA;   &lt;li&gt;guidance_scale：提示词相关度，一般默认为5&lt;/li&gt; &#xA;   &lt;li&gt;width、height：尺寸设置（需1024维度）&lt;/li&gt; &#xA;   &lt;li&gt;seed：种子&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;- base_model_path：支持输入huggingface模型名称自动下载模型（如：SG161222/RealVisXL_V3.0）&#xA;- ref_images_path：支持批量读取参考图像，放入文件夹中即可&#xA;- ptompt、negative：正负提示词&#xA;- style_name：支持官方提供的10种风格&#xA;    - (No style)&#xA;    - Cinematic&#xA;    - Disney Charactor&#xA;    - Digital Art&#xA;    - Photographic (Default)&#xA;    - Fantasy art&#xA;    - Neonpunk&#xA;    - Enhance&#xA;    - Comic book&#xA;    - Lowpoly&#xA;    - Line art &#xA;- style_strength_ratio：风格混合强度（高于30按30计算）&#xA;- step：步数，官方默认50步，但毕竟是基于SDXL模型，我实测下来30步足够了&#xA;- guidance_scale：提示词相关度，一般默认为5&#xA;- seed：种子&#xA;---&gt; &#xA;&lt;h2&gt;风格 | Styles&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/dc675478-47a0-456d-946b-0cf781aa4c28&#34; alt=&#34;PNSTYLE_2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;安装 | Install&lt;/h2&gt; &#xA;&lt;!--&#xA;- 推荐使用管理器 ComfyUI Manager 安装&#xA;---&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;手动安装： &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd custom_nodes&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;git clone https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker.git&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd custom_nodes/ComfyUI-PhotoMaker&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;重启 ComfyUI&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;工作流 | Workflows&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/raw/main/PhotoMaker%20Workflows/PhotoMaker_lora_batch%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.5 lora + batch&lt;/a&gt; 🆕&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/b862b89f-1609-43d9-84a1-5f11a2d1ab2d&#34; alt=&#34;Dingtalk_20240119202403&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/raw/main/PhotoMaker%20Workflows/PhotoMaker_lora_portrait_styler%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.5 portraitmaster + styler + lora&lt;/a&gt; 🆕&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/38e01035-139e-4a89-8982-6f7168684045&#34; alt=&#34;Dingtalk_20240119201125&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/raw/main/PhotoMaker%20Workflows/PhotoMaker_locally%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.0 本地模型 locally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/bf6a55ae-767e-4aaf-9f75-6f752bb5b530&#34; alt=&#34;QQ截图20240118163432&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/raw/main/PhotoMaker%20Workflows/PhotoMaker_fromhub%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.0 自动下载 huggingface hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/f645c1b7-2548-45fc-b388-0ebe62e2724d&#34; alt=&#34;QQ截图20240118163252&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;20240119&lt;/p&gt; &lt;p&gt;更新为 V2.5：支持lora、支持自定义生成数量、支持通用提示词输入（文本）如：styler、portraitmater等&lt;/p&gt; &lt;p&gt;新增 lora + batch、portraitmaster + styler + lora 两个工作流&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240119&lt;/p&gt; &lt;p&gt;更新为 V2.1：参考图改为直接输入/路径输入两种新模式，其中直接输入支持多图&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/78595f2c-7f87-477a-9896-007dd24fe8c9&#34; alt=&#34;Dingtalk_20240119022341&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240118&lt;/p&gt; &lt;p&gt;更新为 V2.0：节点拆分 + 支持本地模型 + 支持自定义尺寸 +提速3倍&lt;/p&gt; &lt;p&gt;新增本地、hub加载工作流&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240117&lt;/p&gt; &lt;p&gt;新增单张图输入，并给出对比图&lt;/p&gt; &lt;p&gt;修复bug，初版上线&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240116&lt;/p&gt; &lt;p&gt;创建项目&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;速度实测 | Speed&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;V2.0 提速 3 倍&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A100 50步 7s&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/4ae13ffc-c770-4551-bcb2-ce0b0ddc1367&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;V1.5&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A100 50步 23s&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/df6eacda-2640-425b-b5ca-1ab5a8a61a66&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;v100 50步 90s&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker/assets/140084057/973b8b6b-9195-4044-b75d-bd833bd6421e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stars&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#ZHO-ZHO-ZHO/ComfyUI-PhotoMaker&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=ZHO-ZHO-ZHO/ComfyUI-PhotoMaker&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;感谢&lt;a href=&#34;https://twitter.com/eviljer&#34;&gt;@erLin&lt;/a&gt;对ComfyUI 的图像张量 Shape (N, H, W, C)的提醒，帮助我成功修复了bug！&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;PhotoMaker&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>msd0pe-1/cve-maker</title>
    <updated>2024-01-20T01:38:43Z</updated>
    <id>tag:github.com,2024-01-20:/msd0pe-1/cve-maker</id>
    <link href="https://github.com/msd0pe-1/cve-maker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tool to find CVEs and Exploits.&lt;/p&gt;&lt;hr&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://img.shields.io/badge/platform-linux-%23309874?style=flat&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/platform-linux-%23309874?style=flat&#34;&gt; &lt;/a&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://img.shields.io/badge/version-2.5.1-%2325c2a0?style=flat&amp;amp;color=%2325c2a0&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/version-2.5.1-%2325c2a0?style=flat&amp;amp;color=%2325c2a0&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://www.python.org/&#34; rel=&#34;nofollow&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.11.2-%23ab6cd6?style=flat&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/msd0pe-1/cve-maker-master/raw/master/LICENSE&#34; rel=&#34;nofollow&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-GPLv3-%231ac0c6?style=flat&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;CVE-MAKER&lt;/h1&gt; &#xA;&lt;p&gt;Use this software &lt;strong&gt;only for legal purposes&lt;/strong&gt;.&lt;br&gt; I am in no way responsible for your actions.&lt;br&gt; Use python 3.11.2&lt;br&gt; &lt;strong&gt;Made by msd0pe&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DESCRIPTION&lt;/h2&gt; &#xA;&lt;p&gt;cve-maker is a hub for finding CVEs and exploits. It is based on the official NIST, ExploitDB and Github databases. The tool makes it quick and easy to search for CVEs and their associated exploits. It is able to detect exploit compilation options. It can also be used to list the latest critical vulnerabilities.&lt;/p&gt; &#xA;&lt;h2&gt;USAGE&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/msd0pe-1/cve-maker/assets/47142249/931e2ac2-948f-4f88-a22a-03f751bf7273&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;INSTALLATION&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;From PIP:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get install python3 python3-pip python3-dev git libssl-dev libffi-dev build-essential curl&#xA;pip3 install --upgrade pip&#xA;pip3 install cve-maker&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;em&gt;Or download the project:&lt;/em&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apt-get install python3 python3-pip python3-dev git libssl-dev libffi-dev build-essential virtualenv curl&#xA;git clone https://github.com/msd0pe-1/cve-maker/&#xA;cd cve-maker&#xA;virtualenv -p python3 venv&#xA;source venv/bin/activate&#xA;pip3 install --upgrade pip&#xA;pip3 install .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;# Launch:&#xA;python3 -m cve-maker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CONTRIBUTING&lt;/h2&gt; &#xA;&lt;p&gt;This project is in active development. Feel free to suggest a new feature or open a pull request !&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apple/ml-aim</title>
    <updated>2024-01-20T01:38:43Z</updated>
    <id>tag:github.com,2024-01-20:/apple/ml-aim</id>
    <link href="https://github.com/apple/ml-aim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository provides the code and model checkpoints of the research paper: Scalable Pre-training of Large Autoregressive Image Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIM: Autoregressive Image Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, and Armand Joulin&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2401.08541&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-aim/main/#citation&#34;&gt;&lt;code&gt;BibTex&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;This software project accompanies the research paper, &lt;a href=&#34;https://arxiv.org/abs/2401.08541&#34;&gt;Scalable Pre-training of Large Autoregressive Image Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We introduce &lt;strong&gt;AIM&lt;/strong&gt; a collection of vision models pre-trained with an autoregressive generative objective. We show that autoregressive pre-training of image features exhibits similar scaling properties to their textual counterpart (i.e. Large Language Models). Specifically, we highlight two findings:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;the model capacity can be trivially scaled to billions of parameters, and&lt;/li&gt; &#xA; &lt;li&gt;AIM effectively leverages large collections of uncurated image data.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please install PyTorch using the official &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installation instructions&lt;/a&gt;. Afterward, install the package as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install git+https://git@github.com/apple/ml-aim.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also offer &lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;MLX&lt;/a&gt; backend support for research and experimentation on Apple silicon. To enable MLX support, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install mlx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Below we provide an example of usage in &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;&#xA;from aim.utils import load_pretrained&#xA;from aim.torch.data import val_transforms&#xA;&#xA;img = Image.open(...)&#xA;model = load_pretrained(&#34;aim-600M-2B-imgs&#34;, backend=&#34;torch&#34;)&#xA;transform = val_transforms()&#xA;&#xA;inp = transform(img).unsqueeze(0)&#xA;logits, features = model(inp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;and in both &lt;a href=&#34;https://ml-explore.github.io/mlx/&#34;&gt;MLX&lt;/a&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;import mlx.core as mx&#xA;&#xA;from aim.utils import load_pretrained&#xA;from aim.torch.data import val_transforms&#xA;&#xA;img = Image.open(...)&#xA;model = load_pretrained(&#34;aim-600M-2B-imgs&#34;, backend=&#34;mlx&#34;)&#xA;transform = val_transforms()&#xA;&#xA;inp = transform(img).unsqueeze(0)&#xA;inp = mx.array(inp.numpy())&#xA;logits, features = model(inp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;and &lt;a href=&#34;https://jax.readthedocs.io/&#34;&gt;JAX&lt;/a&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;import jax.numpy as jnp&#xA;&#xA;from aim.utils import load_pretrained&#xA;from aim.torch.data import val_transforms&#xA;&#xA;img = Image.open(...)&#xA;model, params = load_pretrained(&#34;aim-600M-2B-imgs&#34;, backend=&#34;jax&#34;)&#xA;transform = val_transforms()&#xA;&#xA;inp = transform(img).unsqueeze(0)&#xA;inp = jnp.array(inp)&#xA;(logits, features), _ = model.apply(params, inp, mutable=[&#39;batch_stats&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pre-trained checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;The pre-trained models can be accessed via &lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Hub&lt;/a&gt; as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;aim_600m = torch.hub.load(&#34;apple/ml-aim&#34;, &#34;aim_600M&#34;)&#xA;aim_1b   = torch.hub.load(&#34;apple/ml-aim&#34;, &#34;aim_1B&#34;)&#xA;aim_3b   = torch.hub.load(&#34;apple/ml-aim&#34;, &#34;aim_3B&#34;)&#xA;aim_7b   = torch.hub.load(&#34;apple/ml-aim&#34;, &#34;aim_7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or via &lt;a href=&#34;https://huggingface.co/docs/hub/&#34;&gt;HuggingFace Hub&lt;/a&gt; as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from aim.torch.models import AIMForImageClassification&#xA;&#xA;aim_600m = AIMForImageClassification.from_pretrained(&#34;apple/aim-600M&#34;)&#xA;aim_1b   = AIMForImageClassification.from_pretrained(&#34;apple/aim-1B&#34;)&#xA;aim_3b   = AIMForImageClassification.from_pretrained(&#34;apple/aim-3B&#34;)&#xA;aim_7b   = AIMForImageClassification.from_pretrained(&#34;apple/aim-7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-trained backbones&lt;/h3&gt; &#xA;&lt;p&gt;The following table contains pre-trained backbones used in our paper.&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;attn (best layer)&lt;/th&gt; &#xA;   &lt;th&gt;backbone, SHA256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-0.6B&lt;/td&gt; &#xA;   &lt;td&gt;0.6B&lt;/td&gt; &#xA;   &lt;td&gt;79.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_600m_2bimgs_attnprobe_backbone.pth&#34;&gt;link&lt;/a&gt;, 0d6f6b8f&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-1B&lt;/td&gt; &#xA;   &lt;td&gt;1B&lt;/td&gt; &#xA;   &lt;td&gt;82.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_1b_5bimgs_attnprobe_backbone.pth&#34;&gt;link&lt;/a&gt;, d254ecd3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-3B&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;83.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_3b_5bimgs_attnprobe_backbone.pth&#34;&gt;link&lt;/a&gt;, 8475ce4e&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-7B&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;84.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_7b_5bimgs_attnprobe_backbone.pth&#34;&gt;link&lt;/a&gt;, 184ed94c&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pre-trained attention heads&lt;/h3&gt; &#xA;&lt;p&gt;The table below contains the classification results on ImageNet-1k validation set.&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;top-1 IN-1k&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;attention head, SHA256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;last layer&lt;/th&gt; &#xA;   &lt;th&gt;best layer&lt;/th&gt; &#xA;   &lt;th&gt;last layer&lt;/th&gt; &#xA;   &lt;th&gt;best layer&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-0.6B&lt;/td&gt; &#xA;   &lt;td&gt;78.5%&lt;/td&gt; &#xA;   &lt;td&gt;79.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_600m_2bimgs_attnprobe_head_last_layers.pth&#34;&gt;link&lt;/a&gt;, 5ce5a341&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_600m_2bimgs_attnprobe_head_best_layers.pth&#34;&gt;link&lt;/a&gt;, ebd45c05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-1B&lt;/td&gt; &#xA;   &lt;td&gt;80.6%&lt;/td&gt; &#xA;   &lt;td&gt;82.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_1b_5bimgs_attnprobe_head_last_layers.pth&#34;&gt;link&lt;/a&gt;, db3be2ad&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_1b_5bimgs_attnprobe_head_best_layers.pth&#34;&gt;link&lt;/a&gt;, f1ed7852&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-3B&lt;/td&gt; &#xA;   &lt;td&gt;82.2%&lt;/td&gt; &#xA;   &lt;td&gt;83.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_3b_5bimgs_attnprobe_head_last_layers.pth&#34;&gt;link&lt;/a&gt;, 5c057b30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_3b_5bimgs_attnprobe_head_best_layers.pth&#34;&gt;link&lt;/a&gt;, ad380e16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AIM-7B&lt;/td&gt; &#xA;   &lt;td&gt;82.4%&lt;/td&gt; &#xA;   &lt;td&gt;84.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_7b_5bimgs_attnprobe_head_last_layers.pth&#34;&gt;link&lt;/a&gt;, 1e5c99ba&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/apple/AIM/resolve/main/aim_7b_5bimgs_attnprobe_head_best_layers.pth&#34;&gt;link&lt;/a&gt;, 73ecd732&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Reproducing the IN-1k classification results&lt;/h2&gt; &#xA;&lt;p&gt;The commands below reproduce the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-aim/main/#pre-trained-attention-heads&#34;&gt;attention probe results&lt;/a&gt; on ImageNet-1k validation set. We run the evaluation using 1 node with 8 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;torchrun --standalone --nnodes=1 --nproc-per-node=8 main_attnprobe.py \&#xA;  --model=aim-7B \&#xA;  --batch-size=64 \&#xA;  --data-path=/path/to/imagenet \&#xA;  --probe-layers=best \&#xA;  --backbone-ckpt-path=/path/to/backbone_ckpt.pth \&#xA;  --head-ckpt-path=/path/to/head_ckpt.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, we probe features from the intermediate 6 layers that provide the best performance. To change this, simply pass &lt;code&gt;--probe-layers=last&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing us as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{elnouby2024scalable,&#xA;      title={Scalable Pre-training of Large Autoregressive Image Models},&#xA;      author={Alaaeldin El-Nouby and Michal Klein and Shuangfei Zhai and Miguel Angel Bautista and Alexander Toshev and Vaishaal Shankar and Joshua M Susskind and Armand Joulin},&#xA;      year={2024},&#xA;      eprint={2401.08541},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>