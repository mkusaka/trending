<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-16T01:46:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Victorwz/LongMem</title>
    <updated>2023-06-16T01:46:12Z</updated>
    <id>tag:github.com,2023-06-16:/Victorwz/LongMem</id>
    <link href="https://github.com/Victorwz/LongMem" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LongMem&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of our paper &#34;&lt;a href=&#34;https://arxiv.org/abs//2306.07174&#34;&gt;Augmenting Language Models with Long-Term Memory&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;torch: Please follow &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;torch official installation guide&lt;/a&gt;. We recommend torch&amp;gt;=1.8.0. Please select the torch-gpu version which is consistent with your cuda driver version.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Faiss-GPU: For Nvidia V100 GPUs, simply install via &lt;code&gt;pip install faiss-gpu&lt;/code&gt;. For Nvidia A100 GPUs, please run &lt;code&gt;conda install faiss-gpu cudatoolkit=11.0 -c pytorch&lt;/code&gt;. The A100 GPU is not officially supported by faiss-gpu, sometimes it will lead to errors, you can refer to this git &lt;a href=&#34;https://github.com/facebookresearch/faiss/issues/2064&#34;&gt;issue&lt;/a&gt; of faiss for help.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;fairseq: &lt;code&gt;pip install --editable ./fairseq&lt;/code&gt; Then the revised &lt;code&gt;fairseq&lt;/code&gt; and ohter packages will be installed. We strongly recommend you to use python 3.8 for stability.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;Pre-trained LLM Class (L24, E1024, Alibi POS_ENCODING): &lt;a href=&#34;https://raw.githubusercontent.com/Victorwz/LongMem/main/fairseq/fairseq/models/newgpt.py&#34;&gt;&lt;code&gt;fairseq/fairseq/models/newgpt.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Transformer Decoder with SideNetwork (L12, E1024, Alibi POS_ENCODING): &lt;a href=&#34;https://raw.githubusercontent.com/Victorwz/LongMem/main/fairseq/fairseq/models/sidenet/transformer_decoder_sidenet.py&#34;&gt;&lt;code&gt;fairseq/fairseq/models/sidenet/transformer_decoder_sidenet.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Transformer Language Model with SideNetwork Class: &lt;a href=&#34;https://raw.githubusercontent.com/Victorwz/LongMem/main/fairseq/fairseq/models/transformer_lm_sidenet.py&#34;&gt;&lt;code&gt;fairseq/fairseq/models/transformer_lm_sidenet.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Memory Bank and Retrieval: &lt;a href=&#34;https://raw.githubusercontent.com/Victorwz/LongMem/main/fairseq/fairseq/modules/dynamic_memory_with_chunk.py&#34;&gt;&lt;code&gt;fairseq/fairseq/modules/dynamic_memory_with_chunk.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Joint Attention for Memory Fusion: &lt;a href=&#34;https://raw.githubusercontent.com/Victorwz/LongMem/main/fairseq/fairseq/modules/joint_multihead_attention_sum.py&#34;&gt;&lt;code&gt;fairseq/fairseq/modules/joint_multihead_attention_sum.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you find this repository helpful in your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{LongMem,&#xA;  title={Augmenting Language Models with Long-Term Memory},&#xA;  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},&#xA;  journal={arXiv preprint arXiv:2306.07174},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/ijepa</title>
    <updated>2023-06-16T01:46:12Z</updated>
    <id>tag:github.com,2023-06-16:/facebookresearch/ijepa</id>
    <link href="https://github.com/facebookresearch/ijepa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official codebase for I-JEPA, the Image-based Joint-Embedding Predictive Architecture. First outlined in the CVPR paper, &#34;Self-supervised learning from images with a joint-embedding predictive architecture.&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;I-JEPA&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch codebase for I-JEPA (the &lt;strong&gt;Image-based Joint-Embedding Predictive Architecture&lt;/strong&gt;) published @ CVPR-23. &lt;a href=&#34;https://arxiv.org/pdf/2301.08243.pdf&#34;&gt;[arXiv]&lt;/a&gt; &lt;a href=&#34;https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/&#34;&gt;[JEPAs]&lt;/a&gt; &lt;a href=&#34;https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/&#34;&gt;[blogpost]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;I-JEPA is a method for self-supervised learning. At a high level, I-JEPA predicts the representations of part of an image from the representations of other parts of the same image. Notably, this approach learns semantic image features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;without relying on pre-specified invariances to hand-crafted data transformations, which tend to be biased for particular downstream tasks,&lt;/li&gt; &#xA; &lt;li&gt;and without having the model fill in pixel-level details, which tend to result in learning less semantically meaningful representations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/ijepa/assets/7530871/dbad94ab-ac35-433b-8b4c-ca227886d311&#34; alt=&#34;ijepa&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Visualizations&lt;/h2&gt; &#xA;&lt;p&gt;As opposed to generative methods that have a pixel decoder, I-JEPA has a predictor that makes predictions in latent space. The predictor in I-JEPA can be seen as a primitive (and restricted) world-model that is able to model spatial uncertainty in a static image from a partially observable context. This world model is semantic in the sense that it predicts high level information about unseen regions in the image, rather than pixel-level details.&lt;/p&gt; &#xA;&lt;p&gt;We trained a stochastic decoder that maps the I-JEPA predicted representations back in pixel space as sketches. The model correctly captures positional uncertainty and produces high-level object parts with the correct pose (e.g., dog’s head, wolf’s front legs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/ijepa/assets/7530871/9b66e461-fc8b-4b12-9f06-63ec4dfc1452&#34; alt=&#34;ijepa-predictor-sketch&#34;&gt; &lt;sub&gt; Caption: Illustrating how the predictor learns to model the semantics of the world. For each image, the portion outside of the blue box is encoded and given to the predictor as context. The predictor outputs a representation for what it expects to be in the region within the blue box. To visualize the prediction, we train a generative model that produces a sketch of the contents represented by the predictor output, and we show a sample output within the blue box. The predictor recognizes the semantics of what parts should be filled in (the top of the dog’s head, the bird’s leg, the wolf’s legs, the other side of the building). &lt;/sub&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;I-JEPA pretraining is also computationally efficient. It does not involve any overhead associated with applying more computationally intensive data augmentations to produce multiple views. Only one view of the image needs to be processed by the target encoder, and only the context blocks need to be processed by the context encoder. Empirically, I-JEPA learns strong off-the-shelf semantic representations without the use of hand-crafted view augmentations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/ijepa/assets/7530871/e6e5291f-ca51-43a4-a6cf-069811094ece&#34; alt=&#34;1percenteval&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/ijepa/assets/7530871/d8cffa73-5350-444e-987a-7e131a86d767&#34; alt=&#34;lineareval&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;arch.&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;patch size&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;epochs&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;data&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;14x14&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-300e.pth.tar&#34;&gt;full checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-logs-rank.0.csv&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ijepa/raw/main/configs/in1k_vith14_ep300.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;16x16&lt;/td&gt; &#xA;   &lt;td&gt;448x448&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.16-448px-300e.pth.tar&#34;&gt;full checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.16.448-logs-rank.0.csv&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ijepa/raw/main/configs/in1k_vith16-448_ep300.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;14x14&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;66&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.h.14-900e.pth.tar&#34;&gt;full checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.h.14-logs-rank.0.csv&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ijepa/raw/main/configs/in22k_vith14_ep66.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g&lt;/td&gt; &#xA;   &lt;td&gt;16x16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;44&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet-22K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.g.16-600e.pth.tar&#34;&gt;full checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.g.16-logs-rank.0.csv&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ijepa/raw/main/configs/in22k_vitg16_ep44.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── configs                   # directory in which all experiment &#39;.yaml&#39; configs are stored&#xA;├── src                       # the package&#xA;│   ├── train.py              #   the I-JEPA training loop&#xA;│   ├── helper.py             #   helper functions for init of models &amp;amp; opt/loading checkpoint&#xA;│   ├── transforms.py         #   pre-train data transforms&#xA;│   ├── datasets              #   datasets, data loaders, ...&#xA;│   ├── models                #   model definitions&#xA;│   ├── masks                 #   mask collators, masking utilities, ...&#xA;│   └── utils                 #   shared utilities&#xA;├── main_distributed.py       # entrypoint for launch distributed I-JEPA pretraining on SLURM cluster&#xA;└── main.py                   # entrypoint for launch I-JEPA pretraining locally on your machine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Config files:&lt;/strong&gt; Note that all experiment parameters are specified in config files (as opposed to command-line-arguments). See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/configs/&#34;&gt;configs/&lt;/a&gt; directory for example config files.&lt;/p&gt; &#xA;&lt;h2&gt;Launching I-JEPA pretraining&lt;/h2&gt; &#xA;&lt;h3&gt;Single-GPU training&lt;/h3&gt; &#xA;&lt;p&gt;This implementation starts from the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/main.py&#34;&gt;main.py&lt;/a&gt;, which parses the experiment config file and runs the pre-training locally on a multi-GPU (or single-GPU) machine. For example, to run I-JEPA pretraining on GPUs &#34;0&#34;,&#34;1&#34;, and &#34;2&#34; on a local machine using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/configs/in1k_vith14_ep300.yaml&#34;&gt;configs/in1k_vith14_ep300.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py \&#xA;  --fname configs/in1k_vith14_ep300.yaml \&#xA;  --devices cuda:0 cuda:1 cuda:2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: This example is just used for illustrative purposes, as the ViT-H/14 config should be run on 16 A100 80G GPUs for an effective batch-size of 2048, in order to reproduce our results.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multi-GPU training&lt;/h3&gt; &#xA;&lt;p&gt;In the multi-GPU setting, the implementation starts from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/main_distributed.py&#34;&gt;main_distributed.py&lt;/a&gt;, which, in addition to parsing the config file, also allows for specifying details about distributed training. For distributed training, we use the popular open-source &lt;a href=&#34;https://github.com/facebookincubator/submitit&#34;&gt;submitit&lt;/a&gt; tool and provide examples for a SLURM cluster.&lt;/p&gt; &#xA;&lt;p&gt;For example, to pre-train on 16 A100 80G GPUs using the pre-training experiment configs specificed inside &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/configs/in1k_vith14_ep300.yaml&#34;&gt;configs/in1k_vith14_ep300.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main_distributed.py \&#xA;  --fname configs/in1k_vith14_ep300.yaml \&#xA;  --folder $path_to_save_submitit_logs \&#xA;  --partition $slurm_partition \&#xA;  --nodes 2 --tasks-per-node 8 \&#xA;  --time 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 (or newer)&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 2.0&lt;/li&gt; &#xA; &lt;li&gt;torchvision&lt;/li&gt; &#xA; &lt;li&gt;Other dependencies: pyyaml, numpy, opencv, submitit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ijepa/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details about the license under which this code is made available.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and a citation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{assran2023self,&#xA;  title={Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture},&#xA;  author={Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},&#xA;  journal={arXiv preprint arXiv:2301.08243},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>openai/whisper</title>
    <updated>2023-06-16T01:46:12Z</updated>
    <id>tag:github.com,2023-06-16:/openai/whisper</id>
    <link href="https://github.com/openai/whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Robust Speech Recognition via Large-Scale Weak Supervision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Whisper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/whisper&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/whisper/raw/main/model-card.md&#34;&gt;[Model card]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb&#34;&gt;[Colab example]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/whisper/main/approach.png&#34; alt=&#34;Approach&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We used Python 3.9.9 and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;OpenAI&#39;s tiktoken&lt;/a&gt; for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U openai-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/openai/whisper.git &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the package to the latest version of this repository, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also requires the command-line tool &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;&lt;code&gt;ffmpeg&lt;/code&gt;&lt;/a&gt; to be installed on your system, which is available from most package managers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# on Ubuntu or Debian&#xA;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&#xA;# on Arch Linux&#xA;sudo pacman -S ffmpeg&#xA;&#xA;# on MacOS using Homebrew (https://brew.sh/)&#xA;brew install ffmpeg&#xA;&#xA;# on Windows using Chocolatey (https://chocolatey.org/)&#xA;choco install ffmpeg&#xA;&#xA;# on Windows using Scoop (https://scoop.sh/)&#xA;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need &lt;a href=&#34;http://rust-lang.org&#34;&gt;&lt;code&gt;rust&lt;/code&gt;&lt;/a&gt; installed as well, in case &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; does not provide a pre-built wheel for your platform. If you see installation errors during the &lt;code&gt;pip install&lt;/code&gt; command above, please follow the &lt;a href=&#34;https://www.rust-lang.org/learn/get-started&#34;&gt;Getting started page&lt;/a&gt; to install Rust development environment. Additionally, you may need to configure the &lt;code&gt;PATH&lt;/code&gt; environment variable, e.g. &lt;code&gt;export PATH=&#34;$HOME/.cargo/bin:$PATH&#34;&lt;/code&gt;. If the installation fails with &lt;code&gt;No module named &#39;setuptools_rust&#39;&lt;/code&gt;, you need to install &lt;code&gt;setuptools_rust&lt;/code&gt;, e.g. by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install setuptools-rust&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available models and languages&lt;/h2&gt; &#xA;&lt;p&gt;There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;English-only model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Multilingual model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Required VRAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Relative speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tiny&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~32x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~16x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;244 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~6x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;medium&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;769 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1550 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;large&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~10 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;code&gt;.en&lt;/code&gt; models for English-only applications tend to perform better, especially for the &lt;code&gt;tiny.en&lt;/code&gt; and &lt;code&gt;base.en&lt;/code&gt; models. We observed that the difference becomes less significant for the &lt;code&gt;small.en&lt;/code&gt; and &lt;code&gt;medium.en&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;Whisper&#39;s performance varies widely depending on the language. The figure below shows a WER (Word Error Rate) breakdown by languages of the Fleurs dataset using the &lt;code&gt;large-v2&lt;/code&gt; model (The smaller the numbers, the better the performance). Additional WER scores corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4. Meanwhile, more BLEU (Bilingual Evaluation Understudy) scores can be found in Appendix D.3. Both are found in &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;the paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg?sanitize=true&#34; alt=&#34;WER breakdown by language&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Command-line usage&lt;/h2&gt; &#xA;&lt;p&gt;The following command will transcribe speech in audio files, using the &lt;code&gt;medium&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper audio.flac audio.mp3 audio.wav --model medium&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default setting (which selects the &lt;code&gt;small&lt;/code&gt; model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the &lt;code&gt;--language&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adding &lt;code&gt;--task translate&lt;/code&gt; will translate the speech into English:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese --task translate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following to view all available options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/openai/whisper/raw/main/whisper/tokenizer.py&#34;&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/p&gt; &#xA;&lt;h2&gt;Python usage&lt;/h2&gt; &#xA;&lt;p&gt;Transcription can also be performed within Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;result = model.transcribe(&#34;audio.mp3&#34;)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Internally, the &lt;code&gt;transcribe()&lt;/code&gt; method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.&lt;/p&gt; &#xA;&lt;p&gt;Below is an example usage of &lt;code&gt;whisper.detect_language()&lt;/code&gt; and &lt;code&gt;whisper.decode()&lt;/code&gt; which provide lower-level access to the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;&#xA;# load audio and pad/trim it to fit 30 seconds&#xA;audio = whisper.load_audio(&#34;audio.mp3&#34;)&#xA;audio = whisper.pad_or_trim(audio)&#xA;&#xA;# make log-Mel spectrogram and move to the same device as the model&#xA;mel = whisper.log_mel_spectrogram(audio).to(model.device)&#xA;&#xA;# detect the spoken language&#xA;_, probs = model.detect_language(mel)&#xA;print(f&#34;Detected language: {max(probs, key=probs.get)}&#34;)&#xA;&#xA;# decode the audio&#xA;options = whisper.DecodingOptions()&#xA;result = whisper.decode(model, mel, options)&#xA;&#xA;# print the recognized text&#xA;print(result.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/openai/whisper/discussions/categories/show-and-tell&#34;&gt;🙌 Show and tell&lt;/a&gt; category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Whisper&#39;s code and model weights are released under the MIT License. See &lt;a href=&#34;https://github.com/openai/whisper/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for further details.&lt;/p&gt;</summary>
  </entry>
</feed>