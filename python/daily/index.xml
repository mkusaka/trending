<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-20T01:36:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bytedance/LatentSync</title>
    <updated>2025-03-20T01:36:24Z</updated>
    <id>tag:github.com,2025-03-20:/bytedance/LatentSync</id>
    <link href="https://github.com/bytedance/LatentSync" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Taming Stable Diffusion for Lip Sync!&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;LatentSync&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.09262&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-b31b1b&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ByteDance/LatentSync-1.5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Model-yellow&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/fffiloni/LatentSync&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Space-yellow&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/lucataco/latentsync&#34;&gt;&lt;img src=&#34;https://replicate.com/lucataco/latentsync/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üî• Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2025/03/14&lt;/code&gt;: We released &lt;strong&gt;LatentSync 1.5&lt;/strong&gt;, which &lt;strong&gt;(1)&lt;/strong&gt; improves temporal consistency via adding temporal layer, &lt;strong&gt;(2)&lt;/strong&gt; improves performance on Chinese videos and &lt;strong&gt;(3)&lt;/strong&gt; reduces the VRAM requirement of the stage2 training to &lt;strong&gt;20 GB&lt;/strong&gt; through a series of optimizations. Learn more details &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/LatentSync/main/docs/changelog_v1.5.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìñ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We present &lt;em&gt;LatentSync&lt;/em&gt;, an end-to-end lip-sync method based on audio-conditioned latent diffusion models without any intermediate motion representation, diverging from previous diffusion-based lip-sync methods based on pixel-space diffusion or two-stage generation. Our framework can leverage the powerful capabilities of Stable Diffusion to directly model complex audio-visual correlations.&lt;/p&gt; &#xA;&lt;h2&gt;üèóÔ∏è Framework&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/LatentSync/main/docs/framework.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;LatentSync uses the &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; to convert melspectrogram into audio embeddings, which are then integrated into the U-Net via cross-attention layers. The reference and masked frames are channel-wise concatenated with noised latents as the input of U-Net. In the training process, we use a one-step method to get estimated clean latents from predicted noises, which are then decoded to obtain the estimated clean frames. The TREPA, &lt;a href=&#34;https://arxiv.org/abs/1801.03924&#34;&gt;LPIPS&lt;/a&gt; and &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf&#34;&gt;SyncNet&lt;/a&gt; losses are added in the pixel space.&lt;/p&gt; &#xA;&lt;h2&gt;üé¨ Demo&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;&lt;b&gt;Original video&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;&lt;b&gt;Lip-synced video&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/ff3a84da-dc9b-498a-950f-5c54f58dd5c5&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/150e00fd-381e-4421-a478-a9ea3d1212a8&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/32c830a9-4d7d-4044-9b33-b184d8e11010&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/84e4fe9d-b108-44a4-8712-13a012348145&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/7510a448-255a-44ee-b093-a1b98bd3961d&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/6150c453-c559-4ae0-bb00-c565f135ff41&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;300px&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/0f7f9845-68b2-4165-bd08-c7bbe01a0e52&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;300px&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/c34fe89d-0c09-4de3-8601-3d01229a69e3&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/7ce04d50-d39f-4154-932a-ec3a590a8f64&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/70bde520-42fa-4a0e-b66c-d3040ae5e065&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;(Photorealistic videos are filmed by contracted models, and anime videos are from &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vasa-1/&#34;&gt;VASA-1&lt;/a&gt; and &lt;a href=&#34;https://humanaigc.github.io/emote-portrait-alive/&#34;&gt;EMO&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;üìë Open-source Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code and checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Data processing pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîß Setting up the Environment&lt;/h2&gt; &#xA;&lt;p&gt;Install the required packages and download the checkpoints via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source setup_env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the download is successful, the checkpoints should appear as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./checkpoints/&#xA;|-- latentsync_unet.pt&#xA;|-- stable_syncnet.pt&#xA;|-- whisper&#xA;|   `-- tiny.pt&#xA;|-- auxiliary&#xA;|   |-- 2DFAN4-cd938726ad.zip&#xA;|   |-- i3d_torchscript.pt&#xA;|   |-- koniq_pretrained.pkl&#xA;|   |-- s3fd-619a316812.pth&#xA;|   |-- sfd_face.pth&#xA;|   |-- syncnet_v2.model&#xA;|   |-- vgg16-397923af.pth&#xA;|   `-- vit_g_hybrid_pt_1200e_ssv2_ft.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These already include all the checkpoints required for latentsync training and inference. If you just want to try inference, you only need to download &lt;code&gt;latentsync_unet.pt&lt;/code&gt; and &lt;code&gt;tiny.pt&lt;/code&gt; from our &lt;a href=&#34;https://huggingface.co/ByteDance/LatentSync-1.5&#34;&gt;HuggingFace repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Inference&lt;/h2&gt; &#xA;&lt;p&gt;There are two ways to perform inference, and both require &lt;strong&gt;6.8 GB&lt;/strong&gt; of VRAM.&lt;/p&gt; &#xA;&lt;h3&gt;1. Gradio App&lt;/h3&gt; &#xA;&lt;p&gt;Run the Gradio app for inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Command Line Interface&lt;/h3&gt; &#xA;&lt;p&gt;Run the script for inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./inference.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change the parameters &lt;code&gt;inference_steps&lt;/code&gt; and &lt;code&gt;guidance_scale&lt;/code&gt; to see more results.&lt;/p&gt; &#xA;&lt;h2&gt;üîÑ Data Processing Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;The complete data processing pipeline includes the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Remove the broken video files.&lt;/li&gt; &#xA; &lt;li&gt;Resample the video FPS to 25, and resample the audio to 16000 Hz.&lt;/li&gt; &#xA; &lt;li&gt;Scene detect via &lt;a href=&#34;https://github.com/Breakthrough/PySceneDetect&#34;&gt;PySceneDetect&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Split each video into 5-10 second segments.&lt;/li&gt; &#xA; &lt;li&gt;Affine transform the faces according to the landmarks detected by &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face-alignment&lt;/a&gt;, then resize to 256 $\times$ 256.&lt;/li&gt; &#xA; &lt;li&gt;Remove videos with &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf&#34;&gt;sync confidence score&lt;/a&gt; lower than 3, and adjust the audio-visual offset to 0.&lt;/li&gt; &#xA; &lt;li&gt;Calculate &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf&#34;&gt;hyperIQA&lt;/a&gt; score, and remove videos with scores lower than 40.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Run the script to execute the data processing pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./data_processing_pipeline.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should change the parameter &lt;code&gt;input_dir&lt;/code&gt; in the script to specify the data directory to be processed. The processed videos will be saved in the &lt;code&gt;high_visual_quality&lt;/code&gt; directory. Each step will generate a new directory to prevent the need to redo the entire pipeline in case the process is interrupted by an unexpected error.&lt;/p&gt; &#xA;&lt;h2&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è Training U-Net&lt;/h2&gt; &#xA;&lt;p&gt;Before training, you must process the data as described above and download all the checkpoints. We released a pretrained SyncNet with 94% accuracy on both VoxCeleb2 and HDTF datasets for the supervision of U-Net training. Note that this SyncNet is trained on affine transformed videos, so when using or evaluating this SyncNet, you need to perform affine transformation on the video first (the code of affine transformation is included in the data processing pipeline).&lt;/p&gt; &#xA;&lt;p&gt;If all the preparations are complete, you can train the U-Net with the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./train_unet.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We prepared three UNet configuration files in the &lt;code&gt;configs/unet&lt;/code&gt; directory, each corresponding to a different training setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;stage1.yaml&lt;/code&gt;: Stage1 training, requires &lt;strong&gt;23 GB&lt;/strong&gt; VRAM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stage2.yaml&lt;/code&gt;: Stage2 training with optimal performance, requires &lt;strong&gt;30 GB&lt;/strong&gt; VRAM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stage2_efficient.yaml&lt;/code&gt;: Efficient Stage 2 training, requires &lt;strong&gt;20 GB&lt;/strong&gt; VRAM. It may lead to slight degradation in visual quality and temporal consistency compared with &lt;code&gt;stage2.yaml&lt;/code&gt;, suitable for users with consumer-grade GPUs, such as the RTX 3090.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also remember to change the parameters in U-Net config file to specify the data directory, checkpoint save path, and other training hyperparameters.&lt;/p&gt; &#xA;&lt;h2&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è Training SyncNet&lt;/h2&gt; &#xA;&lt;p&gt;In case you want to train SyncNet on your own datasets, you can run the following script. The data processing pipeline for SyncNet is the same as U-Net.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./train_syncnet.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After &lt;code&gt;validations_steps&lt;/code&gt; training, the loss charts will be saved in &lt;code&gt;train_output_dir&lt;/code&gt;. They contain both the training and validation loss. If you want to customize the architecture of SyncNet for different image resolutions and input frame lengths, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/LatentSync/main/docs/syncnet_arch.md&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìä Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can evaluate the &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf&#34;&gt;sync confidence score&lt;/a&gt; of a generated video by running the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./eval/eval_sync_conf.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can evaluate the accuracy of SyncNet on a dataset by running the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./eval/eval_syncnet_acc.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our code is built on &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Some code are borrowed from &lt;a href=&#34;https://github.com/TMElyralab/MuseTalk&#34;&gt;MuseTalk&lt;/a&gt;, &lt;a href=&#34;https://github.com/guanjz20/StyleSync&#34;&gt;StyleSync&lt;/a&gt;, &lt;a href=&#34;https://github.com/joonson/syncnet_python&#34;&gt;SyncNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2Lip&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks for their generous contributions to the open-source community.&lt;/p&gt; &#xA;&lt;!-- ## Citation&#xA;If you find our repo useful for your research, please consider citing our paper:&#xA;```&#xA;&#xA;``` --&gt;</summary>
  </entry>
  <entry>
    <title>predibase/lorax</title>
    <updated>2025-03-20T01:36:24Z</updated>
    <id>tag:github.com,2025-03-20:/predibase/lorax</id>
    <link href="https://github.com/predibase/lorax" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/lorax/main/docs/LoRAX_Main_Logo-Orange.png&#34; alt=&#34;LoRAX Logo&#34; style=&#34;width:200px;&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;em&gt;LoRAX: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/CBgdrGnZjy&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&amp;amp;theme=discord-inverted&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/predibase/lorax/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/search?repo=lorax&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/lorax&#34; alt=&#34;Artifact Hub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;LoRAX (LoRA eXchange) is a framework that allows users to serve thousands of fine-tuned models on a single GPU, dramatically reducing the cost of serving without compromising on throughput or latency.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#-table-of-contents&#34;&gt;üìñ Table of contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#-features&#34;&gt;üå≥ Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#-models&#34;&gt;üè† Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#%EF%B8%8F-getting-started&#34;&gt;üèÉ‚Äç‚ôÇÔ∏è Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#launch-lorax-server&#34;&gt;Launch LoRAX Server&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#prompt-via-rest-api&#34;&gt;Prompt via REST API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#prompt-via-python-client&#34;&gt;Prompt via Python Client&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#chat-via-openai-api&#34;&gt;Chat via OpenAI API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#next-steps&#34;&gt;Next steps&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#-acknowledgements&#34;&gt;üôá Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/lorax/main/#%EF%B8%8F-roadmap&#34;&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üå≥ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÖ &lt;strong&gt;Dynamic Adapter Loading:&lt;/strong&gt; include any fine-tuned LoRA adapter from &lt;a href=&#34;https://predibase.github.io/lorax/models/adapters/#huggingface-hub&#34;&gt;HuggingFace&lt;/a&gt;, &lt;a href=&#34;https://predibase.github.io/lorax/models/adapters/#predibase&#34;&gt;Predibase&lt;/a&gt;, or &lt;a href=&#34;https://predibase.github.io/lorax/models/adapters/#local&#34;&gt;any filesystem&lt;/a&gt; in your request, it will be loaded just-in-time without blocking concurrent requests. &lt;a href=&#34;https://predibase.github.io/lorax/guides/merging_adapters/&#34;&gt;Merge adapters&lt;/a&gt; per request to instantly create powerful ensembles.&lt;/li&gt; &#xA; &lt;li&gt;üèãÔ∏è‚Äç‚ôÄÔ∏è &lt;strong&gt;Heterogeneous Continuous Batching:&lt;/strong&gt; packs requests for different adapters together into the same batch, keeping latency and throughput nearly constant with the number of concurrent adapters.&lt;/li&gt; &#xA; &lt;li&gt;üßÅ &lt;strong&gt;Adapter Exchange Scheduling:&lt;/strong&gt; asynchronously prefetches and offloads adapters between GPU and CPU memory, schedules request batching to optimize the aggregate throughput of the system.&lt;/li&gt; &#xA; &lt;li&gt;üë¨ &lt;strong&gt;Optimized Inference:&lt;/strong&gt; high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels (&lt;a href=&#34;https://arxiv.org/abs/2307.08691&#34;&gt;flash-attention&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2309.06180&#34;&gt;paged attention&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2310.18547&#34;&gt;SGMV&lt;/a&gt;), quantization, token streaming.&lt;/li&gt; &#xA; &lt;li&gt;üö¢ &lt;strong&gt;Ready for Production&lt;/strong&gt; prebuilt Docker images, Helm charts for Kubernetes, Prometheus metrics, and distributed tracing with Open Telemetry. OpenAI compatible API supporting multi-turn chat conversations. Private adapters through per-request tenant isolation. &lt;a href=&#34;https://predibase.github.io/lorax/guides/structured_output&#34;&gt;Structured Output&lt;/a&gt; (JSON mode).&lt;/li&gt; &#xA; &lt;li&gt;ü§Ø &lt;strong&gt;Free for Commercial Use:&lt;/strong&gt; Apache 2.0 License. Enough said üòé.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/predibase/lorax/assets/29719151/f88aa16c-66de-45ad-ad40-01a7874ed8a9&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üè† Models&lt;/h2&gt; &#xA;&lt;p&gt;Serving a fine-tuned model with LoRAX consists of two components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://predibase.github.io/lorax/models/base_models&#34;&gt;Base Model&lt;/a&gt;: pretrained large model shared across all adapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://predibase.github.io/lorax/models/adapters&#34;&gt;Adapter&lt;/a&gt;: task-specific adapter weights dynamically loaded per request.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;LoRAX supports a number of Large Language Models as the base model including &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama&lt;/a&gt; (including &lt;a href=&#34;https://huggingface.co/codellama&#34;&gt;CodeLlama&lt;/a&gt;), &lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral&lt;/a&gt; (including &lt;a href=&#34;https://huggingface.co/HuggingFaceH4/zephyr-7b-beta&#34;&gt;Zephyr&lt;/a&gt;), and &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen&lt;/a&gt;. See &lt;a href=&#34;https://predibase.github.io/lorax/models/base_models/#supported-architectures&#34;&gt;Supported Architectures&lt;/a&gt; for a complete list of supported base models.&lt;/p&gt; &#xA;&lt;p&gt;Base models can be loaded in fp16 or quantized with &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPT-Q&lt;/a&gt;, or &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Supported adapters include LoRA adapters trained using the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; and &lt;a href=&#34;https://ludwig.ai/&#34;&gt;Ludwig&lt;/a&gt; libraries. Any of the linear layers in the model can be adapted via LoRA and loaded in LoRAX.&lt;/p&gt; &#xA;&lt;h2&gt;üèÉ‚Äç‚ôÇÔ∏è Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;The minimum system requirements need to run LoRAX include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nvidia GPU (Ampere generation or above)&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.8 compatible device drivers and above&lt;/li&gt; &#xA; &lt;li&gt;Linux OS&lt;/li&gt; &#xA; &lt;li&gt;Docker (for this guide)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Launch LoRAX Server&lt;/h3&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;nvidia-container-toolkit&lt;/a&gt; Then&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sudo systemctl restart docker&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=mistralai/Mistral-7B-Instruct-v0.1&#xA;volume=$PWD/data&#xA;&#xA;docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \&#xA;    ghcr.io/predibase/lorax:main --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a full tutorial including token streaming and the Python client, see &lt;a href=&#34;https://predibase.github.io/lorax/getting_started/docker&#34;&gt;Getting Started - Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt via REST API&lt;/h3&gt; &#xA;&lt;p&gt;Prompt base LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl 127.0.0.1:8080/generate \&#xA;    -X POST \&#xA;    -d &#39;{&#xA;        &#34;inputs&#34;: &#34;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&#34;,&#xA;        &#34;parameters&#34;: {&#xA;            &#34;max_new_tokens&#34;: 64&#xA;        }&#xA;    }&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prompt a LoRA adapter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl 127.0.0.1:8080/generate \&#xA;    -X POST \&#xA;    -d &#39;{&#xA;        &#34;inputs&#34;: &#34;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&#34;,&#xA;        &#34;parameters&#34;: {&#xA;            &#34;max_new_tokens&#34;: 64,&#xA;            &#34;adapter_id&#34;: &#34;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&#34;&#xA;        }&#xA;    }&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://predibase.github.io/lorax/reference/rest_api&#34;&gt;Reference - REST API&lt;/a&gt; for full details.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt via Python Client&lt;/h3&gt; &#xA;&lt;p&gt;Install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install lorax-client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lorax import Client&#xA;&#xA;client = Client(&#34;http://127.0.0.1:8080&#34;)&#xA;&#xA;# Prompt the base LLM&#xA;prompt = &#34;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&#34;&#xA;print(client.generate(prompt, max_new_tokens=64).generated_text)&#xA;&#xA;# Prompt a LoRA adapter&#xA;adapter_id = &#34;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&#34;&#xA;print(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://predibase.github.io/lorax/reference/python_client&#34;&gt;Reference - Python Client&lt;/a&gt; for full details.&lt;/p&gt; &#xA;&lt;p&gt;For other ways to run LoRAX, see &lt;a href=&#34;https://predibase.github.io/lorax/getting_started/kubernetes&#34;&gt;Getting Started - Kubernetes&lt;/a&gt;, &lt;a href=&#34;https://predibase.github.io/lorax/getting_started/skypilot&#34;&gt;Getting Started - SkyPilot&lt;/a&gt;, and &lt;a href=&#34;https://predibase.github.io/lorax/getting_started/local&#34;&gt;Getting Started - Local&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chat via OpenAI API&lt;/h3&gt; &#xA;&lt;p&gt;LoRAX supports multi-turn chat conversations combined with dynamic adapter loading through an OpenAI compatible API. Just specify any adapter as the &lt;code&gt;model&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    api_key=&#34;EMPTY&#34;,&#xA;    base_url=&#34;http://127.0.0.1:8080/v1&#34;,&#xA;)&#xA;&#xA;resp = client.chat.completions.create(&#xA;    model=&#34;alignment-handbook/zephyr-7b-dpo-lora&#34;,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;system&#34;,&#xA;            &#34;content&#34;: &#34;You are a friendly chatbot who always responds in the style of a pirate&#34;,&#xA;        },&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How many helicopters can a human eat in one sitting?&#34;},&#xA;    ],&#xA;    max_tokens=100,&#xA;)&#xA;print(&#34;Response:&#34;, resp.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://predibase.github.io/lorax/reference/openai_api&#34;&gt;OpenAI Compatible API&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Next steps&lt;/h3&gt; &#xA;&lt;p&gt;Here are some other interesting Mistral-7B fine-tuned models to try out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora&#34;&gt;alignment-handbook/zephyr-7b-dpo-lora&lt;/a&gt;: Mistral-7b fine-tuned on Zephyr-7B dataset with DPO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora&#34;&gt;IlyaGusev/saiga_mistral_7b_lora&lt;/a&gt;: Russian chatbot based on &lt;code&gt;Open-Orca/Mistral-7B-OpenOrca&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Undi95/Mistral-7B-roleplay_alpaca-lora&#34;&gt;Undi95/Mistral-7B-roleplay_alpaca-lora&lt;/a&gt;: Fine-tuned using role-play prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find more LoRA adapters &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=trending&amp;amp;search=-lora&#34;&gt;here&lt;/a&gt;, or try fine-tuning your own with &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; or &lt;a href=&#34;https://ludwig.ai&#34;&gt;Ludwig&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üôá Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;LoRAX is built on top of HuggingFace&#39;s &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;text-generation-inference&lt;/a&gt;, forked from v0.9.4 (Apache 2.0).&lt;/p&gt; &#xA;&lt;p&gt;We&#39;d also like to acknowledge &lt;a href=&#34;https://github.com/punica-ai/punica&#34;&gt;Punica&lt;/a&gt; for their work on the SGMV kernel, which is used to speed up multi-adapter inference under heavy load.&lt;/p&gt; &#xA;&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Our roadmap is tracked &lt;a href=&#34;https://github.com/predibase/lorax/issues/57&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>