<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-08T01:34:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>evroon/bracket</title>
    <updated>2025-05-08T01:34:47Z</updated>
    <id>tag:github.com,2025-05-08:/evroon/bracket</id>
    <link href="https://github.com/evroon/bracket" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Selfhosted tournament system&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/frontend/public/favicon-wide.svg?sanitize=true&#34; alt=&#34;Bracket - Tournament System&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/evroon/bracket/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/evroon/bracket/backend.yml&#34; alt=&#34;build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crowdin.com/project/bracket&#34;&gt;&lt;img src=&#34;https://badges.crowdin.net/bracket/localized.svg?sanitize=true&#34; alt=&#34;translations&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/evroon/bracket/commits/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/evroon/bracket&#34; alt=&#34;last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/evroon/bracket/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/evroon/bracket&#34; alt=&#34;release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/evroon/bracket&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/evroon/bracket/branch/master/graph/badge.svg?token=YJL0DVPFFG&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.bracketapp.nl/demo&#34;&gt;Demo&lt;/a&gt; ¬∑ &lt;a href=&#34;https://docs.bracketapp.nl&#34;&gt;Documentation&lt;/a&gt; ¬∑ &lt;a href=&#34;https://docs.bracketapp.nl/docs/running-bracket/quickstart&#34;&gt;Quickstart&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/evroon/bracket&#34;&gt;GitHub&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/evroon/bracket/releases&#34;&gt;Releases&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Tournament system meant to be easy to use. Bracket is written in async Python (with &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;FastAPI&lt;/a&gt;) and &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; as frontend using the &lt;a href=&#34;https://mantine.dev/&#34;&gt;Mantine&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;It has the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;single elimination, round-robin and swiss&lt;/strong&gt; formats.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build your tournament structure&lt;/strong&gt; with multiple stages that can have multiple groups/brackets in them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Drag-and-drop matches&lt;/strong&gt; to different courts or reschedule them to another start time.&lt;/li&gt; &#xA; &lt;li&gt;Various &lt;strong&gt;dashboard pages&lt;/strong&gt; are available that can be presented to the public, customized with a logo.&lt;/li&gt; &#xA; &lt;li&gt;Create/update &lt;strong&gt;teams&lt;/strong&gt;, and add players to &lt;strong&gt;teams&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;strong&gt;multiple clubs&lt;/strong&gt;, with &lt;strong&gt;multiple tournaments&lt;/strong&gt; per club.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Swiss tournaments&lt;/strong&gt; can be handled dynamically, with automatic scheduling of matches.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/docs/content/img/bracket-screenshot-design.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.bracketapp.nl&#34;&gt;&lt;strong&gt;Explore the Bracket docs&amp;nbsp;&amp;nbsp;‚ñ∂&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Live Demo&lt;/h1&gt; &#xA;&lt;p&gt;A demo is available for free at &lt;a href=&#34;https://www.bracketapp.nl/demo&#34;&gt;https://www.bracketapp.nl/demo&lt;/a&gt;. The demo lasts for 30 minutes, after which your data will de deleted.&lt;/p&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;To quickly run bracket to see how it works, clone it and run &lt;code&gt;docker compose up&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:evroon/bracket.git&#xA;cd bracket&#xA;sudo docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the backend and frontend of Bracket, as well as a postgres instance. You should now be able to view bracket at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;. You can log in with the following credentials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Username: &lt;code&gt;test@example.org&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Password: &lt;code&gt;aeGhoe1ahng2Aezai0Dei6Aih6dieHoo&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To insert dummy rows into the database, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker exec bracket-backend pipenv run ./cli.py create-dev-db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See also the &lt;a href=&#34;https://docs.bracketapp.nl/docs/running-bracket/quickstart&#34;&gt;quickstart docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://docs.bracketapp.nl/docs/usage/guide&#34;&gt;usage guide&lt;/a&gt; for how to organize a tournament in Bracket from start to finish.&lt;/p&gt; &#xA;&lt;h1&gt;Configuration&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://docs.bracketapp.nl/docs/running-bracket/configuration&#34;&gt;configuration docs&lt;/a&gt; for how to configure Bracket.&lt;/p&gt; &#xA;&lt;p&gt;Bracket&#39;s backend is configured using &lt;code&gt;.env&lt;/code&gt; files (&lt;code&gt;prod.env&lt;/code&gt; for production, &lt;code&gt;dev.env&lt;/code&gt; for development etc.). But you can also configure Bracket using environment variables directly, for example by specifying them in the &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The frontend doesn&#39;t can be configured by environment variables as well, as well as &lt;code&gt;.env&lt;/code&gt; files using Next.js&#39; way of loading environment variables.&lt;/p&gt; &#xA;&lt;h1&gt;Running Bracket in production&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://docs.bracketapp.nl/docs/deployment&#34;&gt;deployment docs&lt;/a&gt; for how to deploy Bracket and run it in production.&lt;/p&gt; &#xA;&lt;p&gt;Bracket can be run in Docker or by itself (using &lt;code&gt;pipenv&lt;/code&gt; and &lt;code&gt;yarn&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;Development setup&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://docs.bracketapp.nl/docs/community/development&#34;&gt;development docs&lt;/a&gt; for how to run Bracket for development.&lt;/p&gt; &#xA;&lt;p&gt;Prerequisites are &lt;code&gt;yarn&lt;/code&gt;, &lt;code&gt;postgresql&lt;/code&gt; and &lt;code&gt;pipenv&lt;/code&gt; to run the frontend, database and backend.&lt;/p&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;p&gt;Based on your browser settings, your language should be automatically detected and loaded. For now, there&#39;s no manual way of choosing a different language.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Languages&lt;/h2&gt; &#xA;&lt;p&gt;To add/refine translations, &lt;a href=&#34;https://crowdin.com/project/bracket&#34;&gt;Crowdin&lt;/a&gt; is used. See the &lt;a href=&#34;https://docs.bracketapp.nl/docs/community/contributing/#translating&#34;&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;More screenshots&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/docs/content/img/schedule_preview.png&#34; width=&#34;50%&#34;&gt;&lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/docs/content/img/planning_preview.png&#34; width=&#34;50%&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/docs/content/img/builder_preview.png&#34; width=&#34;50%&#34;&gt;&lt;img alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/evroon/bracket/master/docs/content/img/standings_preview.png&#34; width=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Help&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re having trouble getting Bracket up and running, or have a question about usage or configuration, feel free to ask. The best place to do this is by creating a &lt;a href=&#34;https://github.com/evroon/bracket/discussions&#34;&gt;Discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Supporting Bracket&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re using Bracket and would like to help support its development, that would be greatly appreciated!&lt;/p&gt; &#xA;&lt;p&gt;Several areas that we need a bit of help with at the moment are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚≠ê &lt;strong&gt;Star Bracket&lt;/strong&gt; on GitHub&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Translating&lt;/strong&gt;: Help make Bracket available to non-native English speakers by adding your language (via &lt;a href=&#34;https://crowdin.com/project/bracket&#34;&gt;crowdin&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;strong&gt;Spread the word&lt;/strong&gt; by sharing Bracket to help new users discover it&lt;/li&gt; &#xA; &lt;li&gt;üñ•Ô∏è &lt;strong&gt;Submit a PR&lt;/strong&gt; to add a new feature, fix a bug, extend/update the docs or something else&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.bracketapp.nl/docs/community/contributing&#34;&gt;contribution docs&lt;/a&gt; for more information on how to contribute&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;!-- readme: collaborators,contributors,dependabot/- -start --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/evroon&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/11857441?v=4&#34; width=&#34;100;&#34; alt=&#34;evroon&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Erik Vroon&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/robigan&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/35210888?v=4&#34; width=&#34;100;&#34; alt=&#34;robigan&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Null&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/BachErik&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/75324423?v=4&#34; width=&#34;100;&#34; alt=&#34;BachErik&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;BachErik&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/djpiper28&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/13609136?v=4&#34; width=&#34;100;&#34; alt=&#34;djpiper28&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Danny Piper&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Sevichecc&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/91365763?v=4&#34; width=&#34;100;&#34; alt=&#34;Sevichecc&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;SevicheCC&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/nvanheuverzwijn&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/943226?v=4&#34; width=&#34;100;&#34; alt=&#34;nvanheuverzwijn&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Nicolas Vanheuverzwijn&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/IzStriker&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/44909896?v=4&#34; width=&#34;100;&#34; alt=&#34;IzStriker&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;IzStriker&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/MaxRickettsUy&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/22103252?v=4&#34; width=&#34;100;&#34; alt=&#34;MaxRickettsUy&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Max Ricketts-Uy&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/babeuh&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/60193302?v=4&#34; width=&#34;100;&#34; alt=&#34;babeuh&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Raphael Le Goaller&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!-- readme: collaborators,contributors,dependabot/- -end --&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Bracket is licensed under &lt;a href=&#34;https://choosealicense.com/licenses/agpl-3.0/&#34;&gt;AGPL-v3.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please note that any contributions also fall under this license.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/evroon/bracket/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/agent-squad</title>
    <updated>2025-05-08T01:34:47Z</updated>
    <id>tag:github.com,2025-05-08:/awslabs/agent-squad</id>
    <link href="https://github.com/awslabs/agent-squad" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Flexible and powerful framework for managing multiple AI agents and handling complex conversations&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt;Agent Squad&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;üì¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; üéâ&lt;br&gt; Same powerful functionalities, new catchy name. Embrace the squad! &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/awslabs/agent-squad&#34;&gt;&lt;img alt=&#34;GitHub Repo&#34; src=&#34;https://img.shields.io/badge/GitHub-Repo-green.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/agent-squad&#34;&gt;&lt;img alt=&#34;npm&#34; src=&#34;https://img.shields.io/npm/v/agent-squad.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/agent-squad/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- GitHub Stats --&gt; &lt;img src=&#34;https://img.shields.io/github/stars/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub watchers&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Repository Info --&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/awslabs/agent-squad&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/awslabs/agent-squad&#34; alt=&#34;Issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/awslabs/agent-squad&#34; alt=&#34;Pull Requests&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://awslabs.github.io/agent-squad/&#34; style=&#34;display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;&#34;&gt; üìö Explore Full Documentation &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üîñ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Intelligent intent classification&lt;/strong&gt; ‚Äî Dynamically route queries to the most suitable agent based on context and content.&lt;/li&gt; &#xA; &lt;li&gt;üî§ &lt;strong&gt;Dual language support&lt;/strong&gt; ‚Äî Fully implemented in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üåä &lt;strong&gt;Flexible agent responses&lt;/strong&gt; ‚Äî Support for both streaming and non-streaming responses from different agents.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Context management&lt;/strong&gt; ‚Äî Maintain and utilize conversation context across multiple agents for coherent interactions.&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Extensible architecture&lt;/strong&gt; ‚Äî Easily integrate new agents or customize existing ones to fit your specific needs.&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Universal deployment&lt;/strong&gt; ‚Äî Run anywhere - from AWS Lambda to your local environment or any cloud platform.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Pre-built agents and classifiers&lt;/strong&gt; ‚Äî A variety of ready-to-use agents and multiple classifier implementations available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s the Agent Squad ‚ùì&lt;/h2&gt; &#xA;&lt;p&gt;The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.&lt;/p&gt; &#xA;&lt;p&gt;The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.&lt;/p&gt; &#xA;&lt;p&gt;This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üèóÔ∏è High-level architecture flow diagram&lt;/h2&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg&#34; alt=&#34;High-level architecture flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The process begins with user input, which is analyzed by a Classifier.&lt;/li&gt; &#xA; &lt;li&gt;The Classifier leverages both Agents&#39; Characteristics and Agents&#39; Conversation history to select the most appropriate agent for the task.&lt;/li&gt; &#xA; &lt;li&gt;Once an agent is selected, it processes the user input.&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator then saves the conversation, updating the Agents&#39; Conversation history, before delivering the response back to the user.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png&#34; alt=&#34;&#34;&gt; Introducing SupervisorAgent: Agents Coordination&lt;/h2&gt; &#xA;&lt;p&gt;The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a &#34;agent-as-tools&#34; architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg&#34; alt=&#34;SupervisorAgent flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Key capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Team Coordination&lt;/strong&gt; - Coordonate multiple specialized agents working together on complex tasks&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;strong&gt;Parallel Processing&lt;/strong&gt; - Execute multiple agent queries simultaneously&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Smart Context Management&lt;/strong&gt; - Maintain conversation history across all team members&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Dynamic Delegation&lt;/strong&gt; - Intelligently distribute subtasks to appropriate team members&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Agent Compatibility&lt;/strong&gt; - Works with all agent types (Bedrock, Anthropic, Lex, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The SupervisorAgent can be used in two powerful ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct Usage&lt;/strong&gt; - Call it directly when you need dedicated team coordination for specific tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classifier Integration&lt;/strong&gt; - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here are just a few examples where this agent can be used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Customer Support Teams with specialized sub-teams&lt;/li&gt; &#xA; &lt;li&gt;AI Movie Production Studios&lt;/li&gt; &#xA; &lt;li&gt;Travel Planning Services&lt;/li&gt; &#xA; &lt;li&gt;Product Development Teams&lt;/li&gt; &#xA; &lt;li&gt;Healthcare Coordination Systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent&#34;&gt;Learn more about SupervisorAgent ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Demo App&lt;/h2&gt; &#xA;&lt;p&gt;In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Travel Agent&lt;/strong&gt;: Powered by an Amazon Lex Bot&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weather Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restaurant Agent&lt;/strong&gt;: Implemented as an Amazon Bedrock Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Math Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tech Agent&lt;/strong&gt;: A Bedrock LLM Agent designed to answer questions on technical topics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Health Agent&lt;/strong&gt;: A Bedrock LLM Agent focused on addressing health-related queries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information. Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.&lt;/p&gt; &#xA;&lt;p&gt;The demo highlights the system&#39;s ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Examples &amp;amp; Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Get hands-on experience with the Agent Squad through our diverse set of examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo Applications&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/python&#34;&gt;Streamlit Global Demo&lt;/a&gt;: A single Streamlit application showcasing multiple demos, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;AI Movie Production Studio&lt;/li&gt; &#xA;     &lt;li&gt;AI Travel Planner&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/&#34;&gt;Chat Demo App&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Explore multiple specialized agents handling various domains like travel, weather, math, and health&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/&#34;&gt;E-commerce Support Simulator&lt;/a&gt;: Experience AI-powered customer support with: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Automated response generation for common queries&lt;/li&gt; &#xA;     &lt;li&gt;Intelligent routing of complex issues to human support&lt;/li&gt; &#xA;     &lt;li&gt;Real-time chat and email-style communication&lt;/li&gt; &#xA;     &lt;li&gt;Human-in-the-loop interactions for complex cases&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sample Projects&lt;/strong&gt;: Explore our example implementations in the &lt;code&gt;examples&lt;/code&gt; folder: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app&#34;&gt;&lt;code&gt;chat-demo-app&lt;/code&gt;&lt;/a&gt;: Web-based chat interface with multiple specialized agents&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator&#34;&gt;&lt;code&gt;ecommerce-support-simulator&lt;/code&gt;&lt;/a&gt;: AI-powered customer support system&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app&#34;&gt;&lt;code&gt;chat-chainlit-app&lt;/code&gt;&lt;/a&gt;: Chat application built with Chainlit&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming&#34;&gt;&lt;code&gt;fast-api-streaming&lt;/code&gt;&lt;/a&gt;: FastAPI implementation with streaming support&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output&#34;&gt;&lt;code&gt;text-2-structured-output&lt;/code&gt;&lt;/a&gt;: Natural Language to Structured Data&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents&#34;&gt;&lt;code&gt;bedrock-inline-agents&lt;/code&gt;&lt;/a&gt;: Bedrock Inline Agents sample&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing&#34;&gt;&lt;code&gt;bedrock-prompt-routing&lt;/code&gt;&lt;/a&gt;: Bedrock Prompt Routing sample code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples are available in both Python and TypeScript. Check out our &lt;a href=&#34;https://awslabs.github.io/agent-squad/&#34;&gt;documentation&lt;/a&gt; for comprehensive guides on setting up and using the Agent Squad framework!&lt;/p&gt; &#xA;&lt;h2&gt;üìö Deep Dives: Stories, Blogs &amp;amp; Podcasts&lt;/h2&gt; &#xA;&lt;p&gt;Discover creative implementations and diverse applications of the Agent Squad:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations&#34;&gt;From &#39;Bonjour&#39; to &#39;Boarding Pass&#39;: Multilingual AI Chatbot for Flight Reservations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an &lt;strong&gt;Amazon Lex&lt;/strong&gt; bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system&#34;&gt;Beyond Auto-Replies: Building an AI-Powered E-commerce Support system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock&#34;&gt;Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via &lt;strong&gt;Amazon Connect&lt;/strong&gt; and &lt;strong&gt;Amazon Lex&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad&#34;&gt;Unlock Bedrock InvokeInlineAgent API&#39;s Hidden Potential&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to scale &lt;strong&gt;Amazon Bedrock Agents&lt;/strong&gt; beyond knowledge base limitations using the Agent Squad framework and &lt;strong&gt;InvokeInlineAgent API&lt;/strong&gt;. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad&#34;&gt;Supercharging Amazon Bedrock Flows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to enhance &lt;strong&gt;Amazon Bedrock Flows&lt;/strong&gt; with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows&#39; limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üéôÔ∏è Podcast Discussions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá´üá∑ Podcast (French)&lt;/strong&gt;: L&#39;orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá¨üáß Podcast (English)&lt;/strong&gt;: An Orchestrator for Your AI Agents&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TypeScript Version&lt;/h3&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîÑ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install agent-squad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;p&gt;The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { AgentSquad, BedrockLLMAgent, LexBotAgent } from &#34;agent-squad&#34;;&#xA;&#xA;const orchestrator = new AgentSquad();&#xA;&#xA;// Add a Bedrock LLM Agent with Converse API support&#xA;orchestrator.addAgent(&#xA;  new BedrockLLMAgent({&#xA;      name: &#34;Tech Agent&#34;,&#xA;      description:&#xA;        &#34;Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.&#34;,&#xA;      streaming: true&#xA;  })&#xA;);&#xA;&#xA;// Add a Lex Bot Agent for handling travel-related queries&#xA;orchestrator.addAgent(&#xA;  new LexBotAgent({&#xA;    name: &#34;Travel Agent&#34;,&#xA;    description: &#34;Helps users book and manage their flight reservations&#34;,&#xA;    botId: process.env.LEX_BOT_ID,&#xA;    botAliasId: process.env.LEX_BOT_ALIAS_ID,&#xA;    localeId: &#34;en_US&#34;,&#xA;  })&#xA;);&#xA;&#xA;// Example usage&#xA;const response = await orchestrator.routeRequest(&#xA;  &#34;I want to book a flight&#34;,&#xA;  &#39;user123&#39;,&#xA;  &#39;session456&#39;&#xA;);&#xA;&#xA;// Handle the response (streaming or non-streaming)&#xA;if (response.streaming == true) {&#xA;    console.log(&#34;\n** RESPONSE STREAMING ** \n&#34;);&#xA;    // Send metadata immediately&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: `);&#xA;&#xA;    // Stream the content&#xA;    for await (const chunk of response.output) {&#xA;      if (typeof chunk === &#34;string&#34;) {&#xA;        process.stdout.write(chunk);&#xA;      } else {&#xA;        console.error(&#34;Received unexpected chunk type:&#34;, typeof chunk);&#xA;      }&#xA;    }&#xA;&#xA;} else {&#xA;    // Handle non-streaming response (AgentProcessingResult)&#xA;    console.log(&#34;\n** RESPONSE ** \n&#34;);&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: ${response.output}`);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python Version&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîÑ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Optional: Set up a virtual environment&#xA;python -m venv venv&#xA;source venv/bin/activate  # On Windows use `venv\Scripts\activate`&#xA;pip install agent-squad[aws]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Default Usage&lt;/h4&gt; &#xA;&lt;p&gt;Here&#39;s an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;import asyncio&#xA;from agent_squad.orchestrator import AgentSquad&#xA;from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse&#xA;&#xA;orchestrator = AgentSquad()&#xA;&#xA;tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(&#xA;  name=&#34;Tech Agent&#34;,&#xA;  streaming=True,&#xA;  description=&#34;Specializes in technology areas including software development, hardware, AI, \&#xA;  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \&#xA;  related to technology products and services.&#34;,&#xA;  model_id=&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;,&#xA;))&#xA;orchestrator.add_agent(tech_agent)&#xA;&#xA;&#xA;health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(&#xA;  name=&#34;Health Agent&#34;,&#xA;  streaming=True,&#xA;  description=&#34;Specializes in health and well being&#34;,&#xA;))&#xA;orchestrator.add_agent(health_agent)&#xA;&#xA;async def main():&#xA;    # Example usage&#xA;    response = await orchestrator.route_request(&#xA;        &#34;What is AWS Lambda?&#34;,&#xA;        &#39;user123&#39;,&#xA;        &#39;session456&#39;,&#xA;        {},&#xA;        True&#xA;    )&#xA;&#xA;    # Handle the response (streaming or non-streaming)&#xA;    if response.streaming:&#xA;        print(&#34;\n** RESPONSE STREAMING ** \n&#34;)&#xA;        # Send metadata immediately&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(&#34;\n&amp;gt; Response: &#34;)&#xA;&#xA;        # Stream the content&#xA;        async for chunk in response.output:&#xA;            async for chunk in response.output:&#xA;              if isinstance(chunk, AgentStreamResponse):&#xA;                  print(chunk.text, end=&#39;&#39;, flush=True)&#xA;              else:&#xA;                  print(f&#34;Received unexpected chunk type: {type(chunk)}&#34;, file=sys.stderr)&#xA;&#xA;    else:&#xA;        # Handle non-streaming response (AgentProcessingResult)&#xA;        print(&#34;\n** RESPONSE ** \n&#34;)&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(f&#34;\n&amp;gt; Response: {response.output.content}&#34;)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;  asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These examples showcase:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.&lt;/li&gt; &#xA; &lt;li&gt;Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator&#39;s ability to route requests to the most appropriate agent based on the input.&lt;/li&gt; &#xA; &lt;li&gt;Handling of both streaming and non-streaming responses from different types of agents.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Modular Installation Options&lt;/h3&gt; &#xA;&lt;p&gt;The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.&lt;/p&gt; &#xA;&lt;h4&gt;Installation Options&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. AWS Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; pip install &#34;agent-squad[aws]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes core orchestration functionality with comprehensive AWS service integrations (&lt;code&gt;BedrockLLMAgent&lt;/code&gt;, &lt;code&gt;AmazonBedrockAgent&lt;/code&gt;, &lt;code&gt;LambdaAgent&lt;/code&gt;, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Anthropic Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[anthropic]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. OpenAI Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[openai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adds OpenAI&#39;s GPT models for agents and classification, along with core packages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Full Installation&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes all optional dependencies for maximum flexibility.&lt;/p&gt; &#xA;&lt;h3&gt;üôå &lt;strong&gt;We Want to Hear From You!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Have something to share, discuss, or brainstorm? We‚Äôd love to connect with you and hear about your journey with the &lt;strong&gt;Agent Squad framework&lt;/strong&gt;. Here‚Äôs how you can get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üôå Show &amp;amp; Tell&lt;/strong&gt;: Got a success story, cool project, or creative implementation? Share it with us in the &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell&#34;&gt;&lt;strong&gt;Show and Tell&lt;/strong&gt;&lt;/a&gt; section. Your work might inspire the entire community! üéâ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí¨ General Discussion&lt;/strong&gt;: Have questions, feedback, or suggestions? Join the conversation in our &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/general&#34;&gt;&lt;strong&gt;General Discussions&lt;/strong&gt;&lt;/a&gt; section. It‚Äôs the perfect place to connect with other users and contributors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí° Ideas&lt;/strong&gt;: Thinking of a new feature or improvement? Share your thoughts in the &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/ideas&#34;&gt;&lt;strong&gt;Ideas&lt;/strong&gt;&lt;/a&gt; section. We‚Äôre always open to exploring innovative ways to make the orchestrator even better!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let‚Äôs collaborate, learn from each other, and build something incredible together! üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;üìù Pull Request Guidelines&lt;/h2&gt; &#xA;&lt;h3&gt;Issue-First Policy&lt;/h3&gt; &#xA;&lt;p&gt;This repository follows an &lt;strong&gt;Issue-First&lt;/strong&gt; policy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Every pull request must be linked to an existing issue&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;If there isn&#39;t an issue for the changes you want to make, please create one first&lt;/li&gt; &#xA; &lt;li&gt;Use the issue to discuss proposed changes before investing time in implementation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to Link Pull Requests to Issues&lt;/h3&gt; &#xA;&lt;p&gt;When creating a pull request, you must link it to an issue using one of these methods:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Include a reference in the PR description using keywords:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Fixes #123&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Resolves #123&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Closes #123&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Manually link the PR to an issue through GitHub&#39;s UI:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On the right sidebar of your PR, click &#34;Development&#34; and then &#34;Link an issue&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Automated Enforcement&lt;/h3&gt; &#xA;&lt;p&gt;We use GitHub Actions to automatically verify that each PR is linked to an issue. PRs without linked issues will not pass required checks and cannot be merged.&lt;/p&gt; &#xA;&lt;p&gt;This policy helps us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Maintain clear documentation of changes and their purposes&lt;/li&gt; &#xA; &lt;li&gt;Ensure community discussion before implementation&lt;/li&gt; &#xA; &lt;li&gt;Keep a structured development process&lt;/li&gt; &#xA; &lt;li&gt;Make project history more traceable and understandable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è Note: Our project has been renamed from &lt;strong&gt;Multi-Agent Orchestrator&lt;/strong&gt; to &lt;strong&gt;Agent Squad&lt;/strong&gt;. Please use the new name in your contributions and discussions.&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è We value your contributions! Before submitting changes, please start a discussion by opening an issue to share your proposal.&lt;/p&gt; &#xA;&lt;p&gt;Once your proposal is approved, here are the next steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üìö Review our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí° Create a &lt;a href=&#34;https://github.com/awslabs/agent-squad/issues&#34;&gt;GitHub Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî® Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;‚úÖ Follow existing project structure and include documentation for new features.&lt;/p&gt; &#xA;&lt;p&gt;üåü &lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repository to be notified about new features, improvements, and exciting developments in the Agent Squad framework!&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/corneliucroitoru/&#34;&gt;Corneliu Croitoru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/anthonybernabeu/&#34;&gt;Anthony Bernabeu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üë• Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Big shout out to our awesome contributors! Thank you for making this project better! üåü ‚≠ê üöÄ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=awslabs/agent-squad&amp;amp;max=2000&#34; alt=&#34;contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for guidelines on how to propose bugfixes and improvements.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 licence - see the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Font License&lt;/h2&gt; &#xA;&lt;p&gt;This project uses the JetBrainsMono NF font, licensed under the SIL Open Font License 1.1. For full license details, see &lt;a href=&#34;https://github.com/JetBrains/JetBrainsMono/raw/master/OFL.txt&#34;&gt;FONT-LICENSE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lightricks/LTX-Video</title>
    <updated>2025-05-08T01:34:47Z</updated>
    <id>tag:github.com,2025-05-08:/Lightricks/LTX-Video</id>
    <link href="https://github.com/Lightricks/LTX-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;LTX-Video&lt;/h1&gt; &#xA; &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.lightricks.com/ltxv&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/Mn8BRgUKKy&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news&#34;&gt;What&#39;s new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide&#34;&gt;Quick Start Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-demo&#34;&gt;Online demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally&#34;&gt;Run locally&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration&#34;&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide&#34;&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution&#34;&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#trining&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us&#34;&gt;Join Us!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &#xA;&lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#39;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00002.gif&#34; alt=&#34;example2&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman walks away from a white Jeep parked on a city street at night...&lt;/summary&gt;A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00003.gif&#34; alt=&#34;example3&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with blonde hair styled up, wearing a black dress...&lt;/summary&gt;A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman&#39;s face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00004.gif&#34; alt=&#34;example4&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans over a snow-covered mountain range...&lt;/summary&gt;The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00008.gif&#34; alt=&#34;example8&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with blood on her face and a white tank top...&lt;/summary&gt;A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman&#39;s face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00009.gif&#34; alt=&#34;example9&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man with graying hair, a beard, and a gray shirt...&lt;/summary&gt;A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man&#39;s face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00010.gif&#34; alt=&#34;example10&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00011.gif&#34; alt=&#34;example11&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00012.gif&#34; alt=&#34;example12&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The waves crash against the jagged rocks of the shoreline...&lt;/summary&gt;The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00013.gif&#34; alt=&#34;example13&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00014.gif&#34; alt=&#34;example14&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00015.gif&#34; alt=&#34;example15&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#39; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00016.gif&#34; alt=&#34;example16&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with short brown hair, wearing a maroon sleeveless top...&lt;/summary&gt;A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman&#39;s face. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors&#34;&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official CompfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new upscalers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors&#34;&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors&#34;&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; &#xA; &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Release a new distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), 4, 2 or 1 diffusion steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; &#xA; &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New license for commercial use (&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt&#34;&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support keyframes and video extension&lt;/li&gt; &#xA; &lt;li&gt;Support higher resolutions&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt understanding&lt;/li&gt; &#xA; &lt;li&gt;Improved VAE&lt;/li&gt; &#xA; &lt;li&gt;New online web app in &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; &#xA; &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; &#xA; &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release the &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;research paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support for STG / PAG&lt;/li&gt; &#xA; &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; &#xA; &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; &#xA; &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; &#xA; &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; &#xA; &lt;li&gt;Relax transformers dependency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Models&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;th&gt;inference.py config&lt;/th&gt; &#xA;   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;0.9.7&lt;/td&gt; &#xA;   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json&#34;&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-fp8&lt;/td&gt; &#xA;   &lt;td&gt;0.9.7&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json&#34;&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml&#34;&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json&#34;&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-distilled&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml&#34;&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json&#34;&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Quick Start Guide&lt;/h1&gt; &#xA;&lt;h2&gt;Online inference&lt;/h2&gt; &#xA;&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video&#34;&gt;Fal.ai text-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video/image-to-video&#34;&gt;Fal.ai image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/lightricks/ltx-video&#34;&gt;Replicate text-to-video and image-to-video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run locally&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightricks/LTX-Video.git&#xA;cd LTX-Video&#xA;&#xA;# create env&#xA;python -m venv env&#xA;source env/bin/activate&#xA;python -m pip install -e .\[inference-script\]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI&lt;/a&gt; workflow. We‚Äôre working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; &#xA;&lt;p&gt;To use our model, please follow the inference code in &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py&#34;&gt;inference.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h4&gt;For text-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extending a video:&lt;/h4&gt; &#xA;&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; &#xA;&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8&#34;&gt;see details below&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model User Guide&lt;/h1&gt; &#xA;&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; &#xA;&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; &#xA; &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; &#xA; &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; &#xA; &lt;li&gt;Include background and environment details&lt;/li&gt; &#xA; &lt;li&gt;Specify camera angles and movements&lt;/li&gt; &#xA; &lt;li&gt;Describe lighting and colors&lt;/li&gt; &#xA; &lt;li&gt;Note any changes or sudden events&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;When using &lt;code&gt;inference.py&lt;/code&gt;, shorts prompts (below &lt;code&gt;prompt_enhancement_words_threshold&lt;/code&gt; words) are automatically enhanced by a language model. This is supported with text-to-video and image-to-video (first-frame conditioning).&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; &#xA; &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; &#xA; &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; &#xA; &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contribution&lt;/h2&gt; &#xA;&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; &#xA;&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href=&#34;https://rf-inversion.github.io/&#34;&gt;RF-Inversion&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/wangjiangshan0725/RF-Solver-Edit&#34;&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/fallenshock/FlowEdit&#34;&gt;FlowEdit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href=&#34;https://junhahyung.github.io/STGuidance/&#34;&gt;STGuidance&lt;/a&gt;. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id=&#34;ltx-videoq8&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href=&#34;https://github.com/Lightricks/LTX-Video&#34;&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/KONAKONA666/LTX-Video&#34;&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; &#xA;   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; &#xA;   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/&#34;&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href=&#34;https://github.com/sayakpaul/q8-ltx-video&#34;&gt;Details here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id=&#34;TeaCache&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video&#34;&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; &#xA;   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; &#xA;   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Your Contribution&lt;/h3&gt; &#xA;&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;h2&gt;Diffusers&lt;/h2&gt; &#xA;&lt;p&gt;Diffusers implemented &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10228&#34;&gt;LoRA support&lt;/a&gt;, with a training script for fine-tuning. More information and training script in &lt;a href=&#34;https://github.com/a-r-r-o-w/finetrainers?tab=readme-ov-file#training&#34;&gt;finetrainers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusion-Pipe&lt;/h2&gt; &#xA;&lt;p&gt;An experimental training framework with pipeline parallelism, enabling fine-tuning of large models like &lt;strong&gt;LTX-Video&lt;/strong&gt; across multiple GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/tdrussell/diffusion-pipe&#34;&gt;Diffusion-Pipe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üõ†Ô∏è Full fine-tune support for LTX-Video using LoRA&lt;/li&gt; &#xA;   &lt;li&gt;üìä Useful metrics logged to Tensorboard&lt;/li&gt; &#xA;   &lt;li&gt;üîÑ Training state checkpointing and resumption&lt;/li&gt; &#xA;   &lt;li&gt;‚ö° Efficient pre-caching of latents and text embeddings for multi-GPU setups&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Join Us üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; &#xA;&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we&#39;re revolutionizing how visual content is created.&lt;/p&gt; &#xA;&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D&#34;&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,&#xA;  title={LTX-Video: Realtime Video Latent Diffusion},&#xA;  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},&#xA;  journal={arXiv preprint arXiv:2501.00103},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>