<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-13T01:37:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>netease-youdao/EmotiVoice</title>
    <updated>2023-11-13T01:37:11Z</updated>
    <id>tag:github.com,2023-11-13:/netease-youdao/EmotiVoice</id>
    <link href="https://github.com/netease-youdao/EmotiVoice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EmotiVoice ðŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;font size=&#34;4&#34;&gt; README: EN | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/EmotiVoice/main/README.zh.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; &lt;/font&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;EmotiVoice ðŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/EmotiVoice/main/README.zh.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-ä¸­æ–‡ç‰ˆæœ¬-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/EmotiVoice/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;EmotiVoice&lt;/strong&gt; is a powerful and modern open-source text-to-speech engine. EmotiVoice speaks both English and Chinese, and with over 2000 different voices. The most prominent feature is &lt;strong&gt;emotional synthesis&lt;/strong&gt;, allowing you to create speech with a wide range of emotions, including happy, excited, sad, angry and others.&lt;/p&gt; &#xA;&lt;p&gt;An easy-to-use web interface is provided. There is also a scripting interface for batch generation of results.&lt;/p&gt; &#xA;&lt;p&gt;Here are a few samples that EmotiVoice generates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/EmotiVoice/assets/3909232/6426d7c1-d620-4bfc-ba03-cd7fc046a4fb&#34;&gt;Chinese audio sample&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/EmotiVoice/assets/3909232/8f272eba-49db-493b-b479-2d9e5a419e26&#34;&gt;English audio sample&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/EmotiVoice/assets/3909232/a0709012-c3ef-4182-bb0e-b7a2ba386f1c&#34;&gt;Fun Chinese English audio sample&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;EmotiVoice Docker image&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to try EmotiVoice is by running the docker image. You need a machine with a NVidia GPU. If you have not done so, set up NVidia container toolkit by following the instructions for &lt;a href=&#34;https://www.server-world.info/en/note?os=Ubuntu_22.04&amp;amp;p=nvidia&amp;amp;f=2&#34;&gt;Linux&lt;/a&gt; or &lt;a href=&#34;https://github.com/nyp-sit/it3103/raw/main/nvidia-docker-wsl2.md&#34;&gt;Windows WSL2&lt;/a&gt;. Then EmotiVoice can be run with,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now open your browser and navigate to &lt;a href=&#34;http://localhost:8501&#34;&gt;http://localhost:8501&lt;/a&gt; to start using EmotiVoice&#39;s powerful TTS capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Full installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n EmotiVoice python=3.8 -y&#xA;conda activate EmotiVoice&#xA;pip install torch torchaudio&#xA;pip install numpy numba scipy transformers==4.26.1 soundfile yacs g2p_en jieba pypinyin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare model files&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git lfs install&#xA;git lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir -p WangZeJun/simbert-base-chinese&#xA;wget https://huggingface.co/WangZeJun/simbert-base-chinese/resolve/main/config.json -P WangZeJun/simbert-base-chinese&#xA;wget https://huggingface.co/WangZeJun/simbert-base-chinese/resolve/main/pytorch_model.bin -P WangZeJun/simbert-base-chinese&#xA;wget https://huggingface.co/WangZeJun/simbert-base-chinese/resolve/main/vocab.txt -P WangZeJun/simbert-base-chinese&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You have to download the &lt;a href=&#34;https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing&#34;&gt;pretrained models&lt;/a&gt;, and run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir -p outputs/style_encoder/ckpt&#xA;mkdir -p outputs/prompt_tts_open_source_joint/ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;And place &lt;code&gt;g_*&lt;/code&gt;, &lt;code&gt;do_*&lt;/code&gt; under &lt;code&gt;outputs/prompt_tts_open_source_joint/ckpt&lt;/code&gt; and put &lt;code&gt;checkpoint_*&lt;/code&gt; in &lt;code&gt;outputs/style_encoder/ckpt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The inference text format is &lt;code&gt;&amp;lt;speaker&amp;gt;|&amp;lt;style_prompt/emotion_prompt/content&amp;gt;|&amp;lt;phoneme&amp;gt;|&amp;lt;content&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;inference text example: &lt;code&gt;Maria_Kasper|Happy|&amp;lt;sos/eos&amp;gt; [IH0] [M] [AA1] [T] engsp4 [V] [OY1] [S] engsp4 [AH0] engsp1 [M] [AH1] [L] [T] [IY0] engsp4 [V] [OY1] [S] engsp1 [AE1] [N] [D] engsp1 [P] [R] [AA1] [M] [P] [T] engsp4 [K] [AH0] [N] [T] [R] [OW1] [L] [D] engsp1 [T] [IY1] engsp4 [T] [IY1] engsp4 [EH1] [S] engsp1 [EH1] [N] [JH] [AH0] [N] . &amp;lt;sos/eos&amp;gt;|Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;You can get phonemes by &lt;code&gt;python frontend_en.py data/my_text.txt &amp;gt; data/my_text_for_tts.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;TEXT=data/inference/text&#xA;python inference_am_vocoder_joint.py \&#xA;--logdir prompt_tts_open_source_joint \&#xA;--config_folder config/joint \&#xA;--checkpoint g_00140000 \&#xA;--test_file $TEXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the synthesized speech is under &lt;code&gt;outputs/prompt_tts_open_source_joint/test_audio&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Or if you just want to use the interactive TTS demo page, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install streamlit&#xA;streamlit run demo_page.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To be released.&lt;/p&gt; &#xA;&lt;h2&gt;Future work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The current implementation focuses on emotion/style control by prompts. It uses only pitch, speed, energy, and emotion as style factors, and does not use gender. But it is not complicated to change it to style/timbre control, similar to the original close-source implementation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WeChat group&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to scan the personal QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/netease-youdao/EmotiVoice/assets/3909232/94ee0824-0304-4487-8682-664fafd09cdf&#34; alt=&#34;qr&#34; width=&#34;150&#34;&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speechresearch.github.io/prompttts/&#34;&gt;PromptTTS&lt;/a&gt;. The PromptTTS paper is a key basis of this project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openslr.org/60/&#34;&gt;LibriTTS&lt;/a&gt;. The LibriTTS dataset is used in training of EmotiVoice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openslr.org/109/&#34;&gt;HiFiTTS&lt;/a&gt;. The HiFi TTS dataset is used in training of EmotiVoice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wetts&#34;&gt;WeTTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;HiFi-GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34;&gt;tacotron&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba-damo-academy/KAN-TTS&#34;&gt;KAN-TTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS&#34;&gt;StyleTTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZhuiyiTechnology/simbert&#34;&gt;Simbert&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;EmotiVoice is provided under the Apache-2.0 License - see the &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/EmotiVoice/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;p&gt;The interactive page is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/EmotiVoice/main/EmotiVoice_UserAgreement_%E6%98%93%E9%AD%94%E5%A3%B0%E7%94%A8%E6%88%B7%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;User Agreement&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shiqiangw/iclr2024-scores</title>
    <updated>2023-11-13T01:37:11Z</updated>
    <id>tag:github.com,2023-11-13:/shiqiangw/iclr2024-scores</id>
    <link href="https://github.com/shiqiangw/iclr2024-scores" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ICLR 2024 Scores from OpenReview&lt;/h1&gt; &#xA;&lt;p&gt;A simple script to extract scores of publicly available ICLR submissions from OpenReview.&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; for installing basic requirements.&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;python3 main.py&lt;/code&gt; to get a spreadsheet including paper titles and scores, in CSV format. The result is saved in &lt;code&gt;output.csv&lt;/code&gt;, sorted according to average score.&lt;/p&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Withdrawn papers are excluded from the output.&lt;/li&gt; &#xA; &lt;li&gt;If there is any issue with pip-installing &lt;code&gt;openreview-py&lt;/code&gt;, you can install it manually following Step 1 in &lt;a href=&#34;https://docs.openreview.net/getting-started/using-the-api/installing-and-instantiating-the-python-client&#34;&gt;https://docs.openreview.net/getting-started/using-the-api/installing-and-instantiating-the-python-client&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Let&#39;s not overload the OpenReview server, so don&#39;t run this too often :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>YORG-AI/Open-Assistant</title>
    <updated>2023-11-13T01:37:11Z</updated>
    <id>tag:github.com,2023-11-13:/YORG-AI/Open-Assistant</id>
    <link href="https://github.com/YORG-AI/Open-Assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YORG Open Source Version&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Assistant&lt;/h1&gt; &#xA;&lt;p&gt;Y&#39;ORG AI provides a local implementation of OpenAI&#39;s assistant.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI&#39;s Assistant API is awesome: channeling the power of &lt;strong&gt;Code interpreter&lt;/strong&gt; and &lt;strong&gt;Retrieval&lt;/strong&gt; and thereby helping developers build powerful AI assistants capable of performing various tasks. However, it executes codes within an online sandbox and requires us to upload our files to OpenAI&#39;s platform -- which does not sound that awesome...&lt;/p&gt; &#xA;&lt;p&gt;Y&#39;ORG AI thus introduces the Open Assistant, which allows you to run your codes locally, retrieve knowledge from local files (without sendding them to OpenAI), and access more developer-friendly tools!&lt;/p&gt; &#xA;&lt;h2&gt;Key Advantages&lt;/h2&gt; &#xA;&lt;p&gt;Our platform is designed with the developer and data analyst in mind, offering unparalleled advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fortified Data Privacy&lt;/strong&gt;: Your sensitive information never leaves your own secure servers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Boundless Document Handling&lt;/strong&gt;: Wave goodbye to restrictions on file size or quantity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: Eliminate session and retrieval costs associated with cloud-based services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local LLM Flexibility&lt;/strong&gt;: Opt for the Large Language Model of your choice and maintain all operations in-house.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tools and pre-built assistants&lt;/h2&gt; &#xA;&lt;p&gt;Y&#39;ORG provide additional tools for developers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understand codebases.&lt;/li&gt; &#xA; &lt;li&gt;Draft development specification.&lt;/li&gt; &#xA; &lt;li&gt;Introduce new features into existing projects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And for data analyst:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Conduct data analysis.&lt;/li&gt; &#xA; &lt;li&gt;Create reports for diverse audiences.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also provide pre-built assistants for our users:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDE assistant&lt;/strong&gt;: Transform feature requests into executable code and documentation by harnessing the collective intelligence of your entire code repositoryâ€”no IDE required.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data assistant&lt;/strong&gt;: Analyze datasets, distill insights, and convert them into comprehensive reports for varied stakeholders.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;TBD (Your thoughts and suggestions are highly welcomed! Create an issue or email &lt;a href=&#34;mailto:contact@yorg.ai&#34;&gt;contact@yorg.ai&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Add your API keys&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your API keys here:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backend/.env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation by Docker&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have Docker running and Run Command in terminal&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build frontend and backend at the same time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build frontend only&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build frontend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build backend only&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build backend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you see: &lt;code&gt;exec ./entrypoint.sh: no such file or directory&lt;/code&gt;, try&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;delete&lt;/strong&gt; the line &lt;code&gt;entrypoint: ./entrypoint.sh in docker-compose.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;add&lt;/strong&gt; &lt;code&gt;CMD [&#34;/bin/sh&#34;, &#34;/app/entrypoint.sh&#34;]&lt;/code&gt; to the end of file &lt;code&gt;/backend/Dockerfile&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;rerun &lt;code&gt;docker-compose up --build&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you see: &lt;code&gt;/app/entrypoint.sh: 3: set: Illegal option -&lt;/code&gt;, try in console&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cd backend&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sed -i &#39;s/\r$//&#39; entrypoint.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;rerun &lt;code&gt;docker-compose up --build&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;http://localhost:8888/&#34;&gt;http://localhost:8888/&lt;/a&gt; and you will see a Jupyter Notebook.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new notebook with YKernel&lt;/li&gt; &#xA; &lt;li&gt;Wait until Ykernel is ready&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;+&lt;/code&gt; next to the keyboard button to create a new cell to start &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Software Engineer&lt;/li&gt; &#xA;   &lt;li&gt;OpenAI Chat&lt;/li&gt; &#xA;   &lt;li&gt;File Upload&lt;/li&gt; &#xA;   &lt;li&gt;Data Analysis&lt;/li&gt; &#xA;   &lt;li&gt;Python Code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions / feedback / comment, do not hesitate to contact us.&lt;/p&gt; &#xA;&lt;p&gt;Email: &lt;a href=&#34;mailto:contact@yorg.ai&#34;&gt;contact@yorg.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub Issues: For more technical inquiries, you can also create a new issue in our GitHub repository.&lt;/p&gt;</summary>
  </entry>
</feed>