<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-19T01:36:34Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vastsa/FileCodeBox</title>
    <updated>2022-12-19T01:36:34Z</updated>
    <id>tag:github.com,2022-12-19:/vastsa/FileCodeBox</id>
    <link href="https://github.com/vastsa/FileCodeBox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;文件快递柜-匿名口令分享文本，文件，像拿快递一样取文件（File Express Cabinet - Anonymous Passcode Sharing Text, Files, Like Taking Express Delivery for Files）&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;h1&gt;文件快递柜-轻量&lt;/h1&gt; &#xA; &lt;h2&gt;FileCoxBox-Lite&lt;/h2&gt; &#xA; &lt;p&gt;&lt;em&gt;匿名口令分享文本，文件，像拿快递一样取文件&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/readme.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/readme_en.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;主要特色&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 轻量简洁：Fastapi+Sqlite3+Vue2+ElementUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 轻松上传：复制粘贴，拖拽选择&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 多种类型：文本，文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 防止爆破：错误次数限制&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 防止滥用：IP限制上传次数&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 口令分享：随机口令，存取文件，自定义次数以及有效期&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 匿名分享：无需注册，无需登录&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 管理面板：查看所有文件，删除文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 一键部署：docker一键部署&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;未来规划&lt;/h2&gt; &#xA;&lt;p&gt;2022年12月14日&lt;/p&gt; &#xA;&lt;p&gt;这个项目的灵感来源于丁丁快传，然后写了这么一个基于本机存储的快传系统，本系统主要是以轻量，单用户，离线环境（&lt;code&gt;私有化&lt;/code&gt; ）为主，因此也不需要加太多东西，所以其实这个项目到这基本功能已经完成了，剩下的就是维护和完善现有功能。&lt;/p&gt; &#xA;&lt;p&gt;也不会再加入新的大功能了，如果有新的功能的话，那就是我们的Pro版本了，当然也是继续开源的，能和@veoco一起开源挺荣幸的，在他的代码中我学到了许多，此前我基本上是使用Django那一套，对Fastapi仅限于使用，他的许多写法让我受益匪浅，也让我对Fastapi有了更深的了解，所以我也会在Pro版本中使用Fastapi。&lt;/p&gt; &#xA;&lt;p&gt;根据目前一些使用反馈来说，希望加入登录功能，还有多存储引擎等，欢迎各位继续提意见，加入我们共同开发。&lt;/p&gt; &#xA;&lt;p&gt;如果你有更好的想法和建议欢迎提issue。&lt;/p&gt; &#xA;&lt;h2&gt;预览&lt;/h2&gt; &#xA;&lt;h3&gt;例站&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://share.lanol.cn&#34;&gt;https://share.lanol.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;寄件&lt;/h3&gt; &#xA;&lt;table style=&#34;width: 100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;width: 100%&#34;&gt; &#xA;   &lt;td style=&#34;width: 50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_1.png&#34; alt=&#34;寄文件&#34;&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;width: 50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_2.png&#34; alt=&#34;寄文本&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;width: 100%;&#34;&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;width: 100%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_3.png&#34; alt=&#34;寄文本&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;取件&lt;/h3&gt; &#xA;&lt;table style=&#34;width: 100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;width: 100%&#34;&gt; &#xA;   &lt;td style=&#34;width: 50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_6.png&#34; alt=&#34;取件&#34;&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;width: 50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_5.png&#34; alt=&#34;取件码错误&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;width: 100%;&#34;&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;width: 100%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_4.png&#34; alt=&#34;取文件&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;管理&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_7.png&#34; alt=&#34;管理&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;部署方式&lt;/h2&gt; &#xA;&lt;h3&gt;Docker一键部署&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --restart=always -p 12345:12345 -v /opt/FileCodeBox/:/app/data --name filecodebox lanol/filecodebox:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;其他方式&lt;/h3&gt; &#xA;&lt;p&gt;仅供参考，历史版本-&amp;gt;&lt;a href=&#34;https://www.yuque.com/lxyo/work/zd0kvzy7fofx6w7v&#34;&gt;部署文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;配置文件&lt;/h2&gt; &#xA;&lt;p&gt;如果需要修改配置，可以将该文件放在&lt;code&gt;/opt/FileCodeBox/&lt;/code&gt;目录下，并命名为&lt;code&gt;.env&lt;/code&gt;，然后重启容器即可。 如果不是Docker，则需要在项目同目录下新建一个&lt;code&gt;data&lt;/code&gt;文件夹，然后在创建&lt;code&gt;.env&lt;/code&gt;文件&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dotenv&#34;&gt;# 端口&#xA;PORT=12345&#xA;# Sqlite数据库文件&#xA;DATABASE_URL=sqlite+aiosqlite:///database.db&#xA;# 静态文件夹&#xA;DATA_ROOT=./static&#xA;# 静态文件夹URL&#xA;STATIC_URL=/static&#xA;# 开启上传&#xA;ENABLE_UPLOAD=True&#xA;# 错误次数&#xA;ERROR_COUNT=5&#xA;# 错误限制分钟数&#xA;ERROR_MINUTE=10&#xA;# 上传次数&#xA;UPLOAD_COUNT=60&#xA;# 上传限制分钟数&#xA;UPLOAD_MINUTE=1&#xA;# 删除过期文件的间隔（分钟）&#xA;DELETE_EXPIRE_FILES_INTERVAL=10&#xA;# 管理地址&#xA;ADMIN_ADDRESS=admin&#xA;# 管理密码&#xA;ADMIN_PASSWORD=admin&#xA;# 文件大小限制，默认10MB&#xA;FILE_SIZE_LIMIT=10&#xA;# 网站标题&#xA;TITLE=文件快递柜&#xA;# 网站描述&#xA;DESCRIPTION=FileCodeBox，文件快递柜，口令传送箱，匿名口令分享文本，文件，图片，视频，音频，压缩包等文件&#xA;# 网站关键词&#xA;KEYWORDS=FileCodeBox，文件快递柜，口令传送箱，匿名口令分享文本，文件，图片，视频，音频，压缩包等文件&#xA;# 存储引擎&#xA;STORAGE_ENGINE=filesystem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;状态&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://repobeats.axiom.co/api/embed/7a6c92f1d96ee57e6fb67f0df371528397b0c9ac.svg?sanitize=true&#34; alt=&#34;Alt&#34; title=&#34;Repobeats analytics image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;赞赏&lt;/h2&gt; &#xA;&lt;table style=&#34;width: 100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;width: 100%&#34;&gt; &#xA;   &lt;td style=&#34;width: 50%;text-align: center;&#34;&gt; 支付宝 &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_9.png&#34; alt=&#34;支付宝&#34;&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;width: 50%;text-align: center&#34;&gt; 微信 &lt;img src=&#34;https://raw.githubusercontent.com/vastsa/FileCodeBox/master/images/img_8.png&#34; alt=&#34;微信&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目开源仅供学习使用，不得用于任何违法用途，否则后果自负，与本人无关。使用请保留项目地址谢谢。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/openai-python</title>
    <updated>2022-12-19T01:36:34Z</updated>
    <id>tag:github.com,2022-12-19:/openai/openai-python</id>
    <link href="https://github.com/openai/openai-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python Library&lt;/h1&gt; &#xA;&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. It includes a pre-defined set of classes for API resources that initialize themselves dynamically from API responses which makes it compatible with a wide range of versions of the OpenAI API.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/api-reference?lang=python&#34;&gt;OpenAI API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You don&#39;t need this source code unless you want to modify the package. If you just want to use the package, just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install --upgrade openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install from source with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The library needs to be configured with your account&#39;s secret key which is available on the &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;website&lt;/a&gt;. Either set it as the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable before using the library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;sk-...&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or set &lt;code&gt;openai.api_key&lt;/code&gt; to its value:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;&#xA;&#xA;# list engines&#xA;engines = openai.Engine.list()&#xA;&#xA;# print the first engine&#39;s id&#xA;print(engines.data[0].id)&#xA;&#xA;# create a completion&#xA;completion = openai.Completion.create(engine=&#34;ada&#34;, prompt=&#34;Hello world&#34;)&#xA;&#xA;# print the completion&#xA;print(completion.choices[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Params&lt;/h3&gt; &#xA;&lt;p&gt;All endpoints have a &lt;code&gt;.create&lt;/code&gt; method that support a &lt;code&gt;request_timeout&lt;/code&gt; param. This param takes a &lt;code&gt;Union[float, Tuple[float, float]]&lt;/code&gt; and will raise a &lt;code&gt;openai.error.TimeoutError&lt;/code&gt; error if the request exceeds that time in seconds (See: &lt;a href=&#34;https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts&#34;&gt;https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Microsoft Azure Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;In order to use the library with Microsoft Azure endpoints, you need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to &#39;azure&#39; and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_type = &#34;azure&#34;&#xA;openai.api_key = &#34;...&#34;&#xA;openai.api_base = &#34;https://example-endpoint.openai.azure.com&#34;&#xA;openai.api_version = &#34;2021-11-01-preview&#34;&#xA;&#xA;# create a completion&#xA;completion = openai.Completion.create(engine=&#34;deployment-name&#34;, prompt=&#34;Hello world&#34;)&#xA;&#xA;# print the completion&#xA;print(completion.choices[0].text)&#xA;&#xA;# create a search and pass the deployment-name as the engine Id.&#xA;search = openai.Engine(id=&#34;deployment-name&#34;).search(documents=[&#34;White House&#34;, &#34;hospital&#34;, &#34;school&#34;], query =&#34;the president&#34;)&#xA;&#xA;# print the search&#xA;print(search)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that for the moment, the Microsoft Azure endpoints can only be used for completion, search and fine-tuning operations. For a detailed example on how to use fine-tuning and other operations using Azure endpoints, please check out the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples/azure/finetuning.ipynb&#34;&gt;Using Azure fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/azure/embeddings.ipynb&#34;&gt;Using Azure embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Microsoft Azure Active Directory Authentication&lt;/h3&gt; &#xA;&lt;p&gt;In order to use Microsoft Active Directory to authenticate to your Azure endpoint, you need to set the api_type to &#34;azure_ad&#34; and pass the acquired credential token to api_key. The rest of the parameters need to be set as specified in the previous section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from azure.identity import DefaultAzureCredential&#xA;import openai&#xA;&#xA;# Request credential&#xA;default_credential = DefaultAzureCredential()&#xA;token = default_credential.get_token(&#34;https://cognitiveservices.azure.com&#34;)&#xA;&#xA;# Setup parameters&#xA;openai.api_type = &#34;azure_ad&#34;&#xA;openai.api_key = token.token&#xA;openai.api_base = &#34;https://example-endpoint.openai.azure.com/&#34;&#xA;openai.api_version = &#34;2022-03-01-preview&#34;&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command-line interface&lt;/h3&gt; &#xA;&lt;p&gt;This library additionally provides an &lt;code&gt;openai&lt;/code&gt; command-line utility which makes it easy to interact with the API from your terminal. Run &lt;code&gt;openai api -h&lt;/code&gt; for usage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# list engines&#xA;openai api engines.list&#xA;&#xA;# create a completion&#xA;openai api completions.create -e ada -p &#34;Hello world&#34;&#xA;&#xA;# generate images via DALL·E API&#xA;openai api image.create -p &#34;two dogs playing chess, cartoon&#34; -n 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example code&lt;/h2&gt; &#xA;&lt;p&gt;Examples of how to use this Python library to accomplish various tasks can be found in the &lt;a href=&#34;https://github.com/openai/openai-cookbook/&#34;&gt;OpenAI Cookbook&lt;/a&gt;. It contains code examples for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Classification using fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Clustering&lt;/li&gt; &#xA; &lt;li&gt;Code search&lt;/li&gt; &#xA; &lt;li&gt;Customizing embeddings&lt;/li&gt; &#xA; &lt;li&gt;Question answering from a corpus of documents&lt;/li&gt; &#xA; &lt;li&gt;Recommendations&lt;/li&gt; &#xA; &lt;li&gt;Visualization of embeddings&lt;/li&gt; &#xA; &lt;li&gt;And more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prior to July 2022, this OpenAI Python library hosted code examples in its examples folder, but since then all examples have been migrated to the &lt;a href=&#34;https://github.com/openai/openai-cookbook/&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;In the OpenAI Python library, an embedding represents a text string as a fixed-length vector of floating point numbers. Embeddings are designed to measure the similarity or relevance between text strings.&lt;/p&gt; &#xA;&lt;p&gt;To get an embedding for a text string, you can use the embeddings method as follows in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;# choose text to embed&#xA;text_string = &#34;sample text&#34;&#xA;&#xA;# choose an embedding&#xA;model_id = &#34;text-similarity-davinci-001&#34;&#xA;&#xA;# compute the embedding of the text&#xA;embedding = openai.Embedding.create(input=text_string, engine=model_id)[&#39;data&#39;][0][&#39;embedding&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of how to call the embeddings method is shown in this &lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Get_embeddings.ipynb&#34;&gt;get embeddings notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Examples of how to use embeddings are shared in the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Classification_using_embeddings.ipynb&#34;&gt;Classification using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Clustering.ipynb&#34;&gt;Clustering using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Code_search.ipynb&#34;&gt;Code search using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Semantic_text_search_using_embeddings.ipynb&#34;&gt;Semantic text search using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/User_and_product_embeddings.ipynb&#34;&gt;User and product embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Zero-shot_classification_with_embeddings.ipynb&#34;&gt;Zero-shot classification using embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Recommendation_using_embeddings.ipynb&#34;&gt;Recommendation using embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information on embeddings and the types of embeddings OpenAI offers, read the &lt;a href=&#34;https://beta.openai.com/docs/guides/embeddings&#34;&gt;embeddings guide&lt;/a&gt; in the OpenAI documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Fine tuning&lt;/h3&gt; &#xA;&lt;p&gt;Fine tuning a model on training data can both improve the results (by giving the model more examples to learn from) and reduce the cost/latency of API calls (chiefly through reducing the need to include training examples in prompts).&lt;/p&gt; &#xA;&lt;p&gt;Examples of fine tuning are shared in the following Jupyter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/Fine-tuned_classification.ipynb&#34;&gt;Classification with fine tuning&lt;/a&gt; (a simple notebook that shows the steps required for fine tuning)&lt;/li&gt; &#xA; &lt;li&gt;Fine tuning a model that answers questions about the 2020 Olympics &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb&#34;&gt;Step 1: Collecting data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb&#34;&gt;Step 2: Creating a synthetic Q&amp;amp;A dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb&#34;&gt;Step 3: Train a fine-tuning model specialized for Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sync your fine-tunes to &lt;a href=&#34;https://wandb.me/openai-docs&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; to track experiments, models, and datasets in your central dashboard with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openai wandb sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on fine tuning, read the &lt;a href=&#34;https://beta.openai.com/docs/guides/fine-tuning&#34;&gt;fine-tuning guide&lt;/a&gt; in the OpenAI documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Moderation&lt;/h3&gt; &#xA;&lt;p&gt;OpenAI provides a Moderation endpoint that can be used to check whether content complies with the OpenAI &lt;a href=&#34;https://beta.openai.com/docs/usage-policies&#34;&gt;content policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;moderation_resp = openai.Moderation.create(input=&#34;Here is some perfectly innocuous text that follows all OpenAI content policies.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/guides/moderation&#34;&gt;moderation guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Image generation (DALL·E)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_key = &#34;sk-...&#34;  # supply your API key however you choose&#xA;&#xA;image_resp = openai.Image.create(prompt=&#34;two dogs playing chess, oil painting&#34;, n=4, size=&#34;512x512&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://beta.openai.com/docs/guides/images&#34;&gt;usage guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7.1+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, we want to support the versions of Python that our customers are using. If you run into problems with any version issues, please let us know at &lt;a href=&#34;mailto:support@openai.com&#34;&gt;support@openai.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;This library is forked from the &lt;a href=&#34;https://github.com/stripe/stripe-python&#34;&gt;Stripe Python Library&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chaiNNer-org/chaiNNer</title>
    <updated>2022-12-19T01:36:34Z</updated>
    <id>tag:github.com,2022-12-19:/chaiNNer-org/chaiNNer</id>
    <link href="https://github.com/chaiNNer-org/chaiNNer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A node-based image processing GUI aimed at making chaining image processing tasks (especially upscaling done by neural networks) easy, intuitive, and customizable.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chaiNNer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/chaiNNer-org/chaiNNer&#34; alt=&#34;GitHub Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/chaiNNer-org/chaiNNer/total&#34; alt=&#34;GitHub Total Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/chaiNNer-org/chaiNNer/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/chaiNNer-org/chaiNNer&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/pzvAKPKyHM&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/930865462852591648?label=Discord&amp;amp;logo=Discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ko-fi.com/T6T46KTTW&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Ko--fi-Support%20chaiNNer%20-hotpink?logo=kofi&amp;amp;logoColor=white&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/releases&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chaiNNer-org/chaiNNer/main/src/public/banner.png&#34; width=&#34;720&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;A flowchart/node-based image processing GUI aimed at making chaining image processing tasks (especially upscaling done by neural networks) easy, intuitive, and customizable.&lt;/p&gt; &#xA;&lt;p&gt;No existing upscaling GUI gives you the level of customization of your image processing workflow that chaiNNer does. Not only do you have full control over your processing pipeline, you can do incredibly complex tasks just by connecting a few nodes together.&lt;/p&gt; &#xA;&lt;p&gt;chaiNNer is also cross-platform, meaning you can run it on Windows, MacOS, and Linux.&lt;/p&gt; &#xA;&lt;p&gt;For help, suggestions, or just to hang out, you can join the &lt;a href=&#34;https://discord.gg/pzvAKPKyHM&#34;&gt;chaiNNer Discord server&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Remember: chaiNNer is still a work in progress and in alpha. While it is slowly getting more to where we want it, it is going to take quite some time to have every possible feature we want to add. If you&#39;re knowledgeable in TypeScript, React, or Python, feel free to contribute to this project and help us get closer to that goal.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest release from the &lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/releases&#34;&gt;Github releases page&lt;/a&gt; and run the installer best suited for your system. Simple as that.&lt;/p&gt; &#xA;&lt;p&gt;You don&#39;t even need to have Python installed, as chaiNNer will download an isolated integrated Python build on startup. From there, you can install all the other dependencies via the Dependency Manager.&lt;/p&gt; &#xA;&lt;p&gt;If you do wish to use your system Python installation still, you can turn the system Python setting on. However, it is much more recommended to use the integrated Python. If you do wish to use your system Python, make sure the Python version you are using is either 3.8 or 3.9. 3.10 also should work for the most part, but it is not fully supported at this time.&lt;/p&gt; &#xA;&lt;p&gt;If you are using the provided .zip portable version of chaiNNer, please be aware that the integrated Python it uses is not portable like the rest of it. It is located at &lt;code&gt;%appdata%/chaiNNer/python&lt;/code&gt; (windows), &lt;code&gt;.config/chaiNNer/python&lt;/code&gt; (linux), or &lt;code&gt;Application Support/chaiNNer/python&lt;/code&gt;. We plan on making this truly portable in the future.&lt;/p&gt; &#xA;&lt;h2&gt;How To Use&lt;/h2&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;p&gt;While it might seem intimidating at first due to all the possible options, chaiNNer is pretty simple to use. For example, this is all you need to do in order to perform an upscale:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chaiNNer-org/chaiNNer/main/src/public/simple_screenshot.png&#34; width=&#34;480&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Before you get to this point though, you&#39;ll need to install one of the neural network frameworks from the dependency manager. You can access this via the button in the upper-right-hand corner. ChaiNNer offers support for PyTorch (with select model architectures), NCNN, and ONNX. For Nvidia users, PyTorch will be the preferred way to upscale. For AMD users, NCNN will be the preferred way to upscale.&lt;/p&gt; &#xA;&lt;p&gt;All the other Python dependencies are automatically installed, and chaiNNer even carries its own integrated Python support so that you do not have to modify your existing Python configuration.&lt;/p&gt; &#xA;&lt;p&gt;Then, all you have to do is drag and drop (or double click) node names in the selection panel to bring them into the editor. Then, drag from one node handle to another to connect the nodes. Each handle is color-coded to its specific type, and while connecting will show you only the compatible connections. This makes it very easy to know what to connect where.&lt;/p&gt; &#xA;&lt;p&gt;Once you have a working chain set up in the editor, you can press the green &#34;run&#34; button in the top bar to run the chain you have made. You will see the connections between nodes become animated, and start to un-animate as they finish processing. You can stop or pause processing with the red &#34;stop&#34; and yellow &#34;pause&#34; buttons respectively. Note: pressing stop is usually unable to kill an in-progress upscale during the actual upscaling step. This is a known issue without a workaround at the moment, so just be patient and wait for it to finish or restart chaiNNer.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chaiNNer-org/chaiNNer/main/src/public/screenshot.png&#34; width=&#34;540&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget, there&#39;s plenty of non-upscaling tasks you can do with chaiNNer as well!&lt;/p&gt; &#xA;&lt;h3&gt;Tips &amp;amp; Tricks&lt;/h3&gt; &#xA;&lt;p&gt;To select multiple nodes, hold down shift and drag around all the nodes you want selected. You can also select an individual node by just clicking on it. When nodes are selected, you can press backspace or delete to delete them from the editor.&lt;/p&gt; &#xA;&lt;p&gt;To batch upscale, create an Image Iterator node and drag the nodes you want to use into the iterator&#39;s editor area. You can expand the iterator by clicking and dragging the bottom right corner outwards (like you would a UI window). Simply wire up a chain in an iterator the same as you would normally, and when you click run it will run on every image in the folder you chose. You also can select an entire existing chain, and drag it into the iterator&#39;s editor area to essentially convert the entire thing into an iterable chain.&lt;/p&gt; &#xA;&lt;p&gt;You can right-click in the editor viewport to show an inline nodes list to select from. You also can get this menu by dragging a connection out to the editor rather than making an actual connection, and it will show compatible nodes to automatically create a connection with.&lt;/p&gt; &#xA;&lt;h3&gt;Helpful Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kimberly990/kim-chaiNNer-Templates/&#34;&gt;Kim&#39;s chaiNNer Templates&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A collection of useful chain templates that can quickly get you started if you are still new to using chaiNNer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://upscale.wiki/wiki/Model_Database&#34;&gt;Upscale Wiki Model Database&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A very nice collection of mostly ESRGAN models that have been trained for various tasks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;MacOS versions older than 10.15 are not supported at this time. This is due to a major dependency (opencv) not yet having a build for this version. The next release of it should be compatible though, so stay tuned for an update that adds support for that (assuming no more compatibility issues are found).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows versions 7 and below are also not supported at this time. You can attempt troubleshooting steps mentioned below in the troubleshooting section, but at this time we do not officially support more Windows versions than Microsoft does.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Apple M1 laptops are mostly untested, though they should support almost everything. Although, ONNX is unable to be installed as it does not yet have an arm64 build, and NCNN sometimes does not work properly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some NCNN users with non-Nvidia GPUs might get all-black outputs. I am not sure what to do to fix this as it appears to be due to the graphics driver crashing as a result of going out of memory. If this happens to you, try manually setting a tiling amount.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Arch Linux users may need to manually install libxcrypt before chaiNner&#39;s integrated Python will correctly start up.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To use the Clipboard nodes, Linux users need to have xclip or, for wayland users, wl-copy installed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GPU Support&lt;/h2&gt; &#xA;&lt;p&gt;For PyTorch inference, only Nvidia GPUs are supported. If you do not have an Nvidia GPU, you will have to use PyTorch in CPU mode. This is because PyTorch only support&#39;s Nvidia&#39;s CUDA. MacOS also does not support CUDA at all, so PyTorch will only work in CPU mode on MacOS.&lt;/p&gt; &#xA;&lt;p&gt;If you have an AMD or Intel GPU that supports NCNN however, chaiNNer now supports NCNN inference. You can use any existing NCNN .bin/.param model files (only ESRGAN-related SR models have been tested), or use chaiNNer to convert a PyTorch or ONNX model to NCNN.&lt;/p&gt; &#xA;&lt;p&gt;For NCNN, make sure to select which GPU you want to use in the settings. It might be defaulting to your integrated graphics!&lt;/p&gt; &#xA;&lt;p&gt;For Nvidia GPUs, ONNX is also an option to be used. ONNX will use CPU mode on non-Nvidia GPUs, similar to PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;Model Architecture Support&lt;/h2&gt; &#xA;&lt;p&gt;chaiNNer currently supports a limited amount of neural network architectures. More architectures will be supported in the future.&lt;/p&gt; &#xA;&lt;h3&gt;Pytorch&lt;/h3&gt; &#xA;&lt;h4&gt;Single Image Super Resolution&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt; (RRDBNet) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This includes regular &lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/ncarraz/ESRGANplus&#34;&gt;ESRGAN+&lt;/a&gt;, &#34;new-arch ESRGAN&#34; (&lt;a href=&#34;https://github.com/jixiaozhong/RealSR&#34;&gt;RealSR&lt;/a&gt;, &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;BSRGAN&lt;/a&gt;), &lt;a href=&#34;https://github.com/Maclory/SPSR&#34;&gt;SPSR&lt;/a&gt;, and &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Models: &lt;a href=&#34;https://upscale.wiki/wiki/Model_Database&#34;&gt;Community ESRGAN&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/1lNky9afqEP-qdxrAwDFPJ1g0ui4x7Sin&#34;&gt;ESRGAN+&lt;/a&gt; | &lt;a href=&#34;https://github.com/cszn/BSRGAN/tree/main/model_zoo&#34;&gt;BSRGAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/jixiaozhong/RealSR#pre-trained-models&#34;&gt;RealSR&lt;/a&gt; | &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/docs/model_zoo.md&#34;&gt;Real-ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN Compact&lt;/a&gt; (SRVGGNet) | &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/docs/model_zoo.md&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Koushik0901/Swift-SRGAN&#34;&gt;Swift-SRGAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/Koushik0901/Swift-SRGAN/releases/tag/v0.1&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;SwinIR&lt;/a&gt; | &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases/tag/v0.0&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mv-lab/swin2sr&#34;&gt;Swin2SR&lt;/a&gt; | &lt;a href=&#34;https://github.com/mv-lab/swin2sr/releases/tag/v0.0.1&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XPixelGroup/HAT&#34;&gt;HAT&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/1HpmReFfoUqUbnAOQ7rvOeNU3uf_m69w0&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Face Restoration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.2.pth&#34;&gt;1.2&lt;/a&gt;, &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;1.3&lt;/a&gt;, &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth&#34;&gt;1.4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wzhouxiff/RestoreFormer&#34;&gt;RestoreFormer&lt;/a&gt; | &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth&#34;&gt;Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;CodeFormer&lt;/a&gt; | &lt;a href=&#34;https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth&#34;&gt;Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NCNN&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Technically, almost any SR model should work assuming they follow a typical CNN-based SR structure, however I have only tested with ESRGAN (and its variants) and with Waifu2x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ONNX&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Similarly to NCNN, technically almost any SR model should work assuming they follow a typical CNN-based SR structure, however I have only tested with ESRGAN.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;For troubleshooting information, view the &lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/raw/main/troubleshooting.md&#34;&gt;troubleshooting document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building chaiNNer Yourself&lt;/h2&gt; &#xA;&lt;p&gt;I provide pre-built versions of chaiNNer here on GitHub. However, if you would like to build chaiNNer yourself, simply run &lt;code&gt;npm install&lt;/code&gt; (make sure that you have at least npm v7 installed) to install all the nodejs dependencies, and &lt;code&gt;npm run make&lt;/code&gt; to build the application.&lt;/p&gt; &#xA;&lt;h2&gt;Planned Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the Discord server for a list of planned features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;For FAQ information, view the &lt;a href=&#34;https://github.com/chaiNNer-org/chaiNNer/raw/main/FAQ.md&#34;&gt;FAQ document&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>