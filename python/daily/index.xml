<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-02T01:43:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deep-floyd/IF</title>
    <updated>2023-05-02T01:43:49Z</updated>
    <id>tag:github.com,2023-05-02:/deep-floyd/IF</id>
    <link href="https://github.com/deep-floyd/IF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_License-Modified_MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/LICENSE-MODEL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Weights_License-DeepFloyd_IF-orange.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/deepfloyd_if&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/deepfloyd_if&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/umz62Mgr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%237289DA.svg?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/deepfloydai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-%231DA1F2.svg?logo=twitter&amp;amp;logoColor=white&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://linktr.ee/deepfloyd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Linktree-%2339E09B.svg?logo=linktree&amp;amp;logoColor=white&#34; alt=&#34;Linktree&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;IF by &lt;a href=&#34;https://deepfloyd.ai&#34;&gt;DeepFloyd Lab&lt;/a&gt; at &lt;a href=&#34;https://stability.ai/&#34;&gt;StabilityAI&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/nabla.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We introduce DeepFloyd IF, a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. DeepFloyd IF is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules: a base model that generates 64x64 px image based on text prompt and two super-resolution models, each designed to generate images of increasing resolution: 256x256 px and 1024x1024 px. All stages of the model utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/deepfloyd_if_scheme.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Inspired by&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/pdf/2205.11487.pdf&#34;&gt;&lt;em&gt;Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Minimum requirements to use all IF models:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;16GB vRAM for IF-I-XL (4.3B text to 64x64 base module) &amp;amp; IF-II-L (1.2B to 256x256 upscaler module)&lt;/li&gt; &#xA; &lt;li&gt;24GB vRAM for IF-I-XL (4.3B text to 64x64 base module) &amp;amp; IF-II-L (1.2B to 256x256 upscaler module) &amp;amp; Stable x4 (to 1024x1024 upscaler)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xformers&lt;/code&gt; and set env variable &lt;code&gt;FORCE_MEM_EFFICIENT_ATTN=1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/DeepFloyd/IF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install deepfloyd_if==1.0.2rc0&#xA;pip install xformers==0.0.16&#xA;pip install git+https://github.com/openai/CLIP.git --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Local notebooks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-notebooks/blob/main/pipes-DeepFloyd-IF-v1.0.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/jupyter_notebook-%23FF7A01.svg?logo=jupyter&amp;amp;logoColor=white&#34; alt=&#34;Jupyter Notebook&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Dream, Style Transfer, Super Resolution or Inpainting modes are avaliable in a Jupyter Notebook &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-notebooks/blob/main/pipes-DeepFloyd-IF-v1.0.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Integration with ü§ó Diffusers&lt;/h2&gt; &#xA;&lt;p&gt;IF is also integrated with the ü§ó Hugging Face &lt;a href=&#34;https://github.com/huggingface/diffusers/&#34;&gt;Diffusers library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diffusers runs each stage individually allowing the user to customize the image generation process as well as allowing to inspect intermediate results easily.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;Before you can use IF, you need to accept its usage conditions. To do so:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure to have a &lt;a href=&#34;https://huggingface.co/join&#34;&gt;Hugging Face account&lt;/a&gt; and be loggin in&lt;/li&gt; &#xA; &lt;li&gt;Accept the license on the model card of &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;DeepFloyd/IF-I-XL-v1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make sure to login locally. Install &lt;code&gt;huggingface_hub&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install huggingface_hub --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;run the login function in a Python shell&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from huggingface_hub import login&#xA;&#xA;login()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and enter your &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens&#34;&gt;Hugging Face Hub access token&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next we install &lt;code&gt;diffusers&lt;/code&gt; and dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install diffusers accelerate transformers safetensors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And we can now run the model locally.&lt;/p&gt; &#xA;&lt;p&gt;By default &lt;code&gt;diffusers&lt;/code&gt; makes use of &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings&#34;&gt;model cpu offloading&lt;/a&gt; to run the whole IF pipeline with as little as 14 GB of VRAM.&lt;/p&gt; &#xA;&lt;p&gt;If you are using &lt;code&gt;torch&amp;gt;=2.0.0&lt;/code&gt;, make sure to &lt;strong&gt;delete all&lt;/strong&gt; &lt;code&gt;enable_xformers_memory_efficient_attention()&lt;/code&gt; functions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from diffusers import DiffusionPipeline&#xA;from diffusers.utils import pt_to_pil&#xA;import torch&#xA;&#xA;# stage 1&#xA;stage_1 = DiffusionPipeline.from_pretrained(&#34;DeepFloyd/IF-I-XL-v1.0&#34;, variant=&#34;fp16&#34;, torch_dtype=torch.float16)&#xA;stage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &amp;gt;= 2.0.0&#xA;stage_1.enable_model_cpu_offload()&#xA;&#xA;# stage 2&#xA;stage_2 = DiffusionPipeline.from_pretrained(&#xA;    &#34;DeepFloyd/IF-II-L-v1.0&#34;, text_encoder=None, variant=&#34;fp16&#34;, torch_dtype=torch.float16&#xA;)&#xA;stage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &amp;gt;= 2.0.0&#xA;stage_2.enable_model_cpu_offload()&#xA;&#xA;# stage 3&#xA;safety_modules = {&#34;feature_extractor&#34;: stage_1.feature_extractor, &#34;safety_checker&#34;: stage_1.safety_checker, &#34;watermarker&#34;: stage_1.watermarker}&#xA;stage_3 = DiffusionPipeline.from_pretrained(&#34;stabilityai/stable-diffusion-x4-upscaler&#34;, **safety_modules, torch_dtype=torch.float16)&#xA;stage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &amp;gt;= 2.0.0&#xA;stage_3.enable_model_cpu_offload()&#xA;&#xA;prompt = &#39;a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says &#34;very deep learning&#34;&#39;&#xA;&#xA;# text embeds&#xA;prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)&#xA;&#xA;generator = torch.manual_seed(0)&#xA;&#xA;# stage 1&#xA;image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=&#34;pt&#34;).images&#xA;pt_to_pil(image)[0].save(&#34;./if_stage_I.png&#34;)&#xA;&#xA;# stage 2&#xA;image = stage_2(&#xA;    image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=&#34;pt&#34;&#xA;).images&#xA;pt_to_pil(image)[0].save(&#34;./if_stage_II.png&#34;)&#xA;&#xA;# stage 3&#xA;image = stage_3(prompt=prompt, image=image, generator=generator, noise_level=100).images&#xA;image[0].save(&#34;./if_stage_III.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are multiple ways to speed up the inference time and lower the memory consumption even more with &lt;code&gt;diffusers&lt;/code&gt;. To do so, please have a look at the Diffusers docs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-speed&#34;&gt;Optimizing for inference time&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è &lt;a href=&#34;https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-memory&#34;&gt;Optimizing for low memory during inference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more in-detail information about how to use IF, please have a look at &lt;a href=&#34;https://huggingface.co/blog/if&#34;&gt;the IF blog post&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/if&#34;&gt;the documentation&lt;/a&gt; üìñ.&lt;/p&gt; &#xA;&lt;h2&gt;Run the code locally&lt;/h2&gt; &#xA;&lt;h3&gt;Loading the models into VRAM&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII&#xA;from deepfloyd_if.modules.t5 import T5Embedder&#xA;&#xA;device = &#39;cuda:0&#39;&#xA;if_I = IFStageI(&#39;IF-I-XL-v1.0&#39;, device=device)&#xA;if_II = IFStageII(&#39;IF-II-L-v1.0&#39;, device=device)&#xA;if_III = StableStageIII(&#39;stable-diffusion-x4-upscaler&#39;, device=device)&#xA;t5 = T5Embedder(device=&#34;cpu&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;I. Dream&lt;/h3&gt; &#xA;&lt;p&gt;Dream is the text-to-image mode of the IF model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepfloyd_if.pipelines import dream&#xA;&#xA;prompt = &#39;ultra close-up color photo portrait of rainbow owl with deer horns in the woods&#39;&#xA;count = 4&#xA;&#xA;result = dream(&#xA;    t5=t5, if_I=if_I, if_II=if_II, if_III=if_III,&#xA;    prompt=[prompt]*count,&#xA;    seed=42,&#xA;    if_I_kwargs={&#xA;        &#34;guidance_scale&#34;: 7.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;smart100&#34;,&#xA;    },&#xA;    if_II_kwargs={&#xA;        &#34;guidance_scale&#34;: 4.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;smart50&#34;,&#xA;    },&#xA;    if_III_kwargs={&#xA;        &#34;guidance_scale&#34;: 9.0,&#xA;        &#34;noise_level&#34;: 20,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,&#xA;    },&#xA;)&#xA;&#xA;if_III.show(result[&#39;III&#39;], size=14)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/dream-III.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;II. Zero-shot Image-to-Image Translation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/img_to_img_scheme.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In Style Transfer mode, the output of your prompt comes out at the style of the &lt;code&gt;support_pil_img&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepfloyd_if.pipelines import style_transfer&#xA;&#xA;result = style_transfer(&#xA;    t5=t5, if_I=if_I, if_II=if_II,&#xA;    support_pil_img=raw_pil_image,&#xA;    style_prompt=[&#xA;        &#39;in style of professional origami&#39;,&#xA;        &#39;in style of oil art, Tate modern&#39;,&#xA;        &#39;in style of plastic building bricks&#39;,&#xA;        &#39;in style of classic anime from 1990&#39;,&#xA;    ],&#xA;    seed=42,&#xA;    if_I_kwargs={&#xA;        &#34;guidance_scale&#34;: 10.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;10,10,10,10,10,10,10,10,0,0&#34;,&#xA;        &#39;support_noise_less_qsample_steps&#39;: 5,&#xA;    },&#xA;    if_II_kwargs={&#xA;        &#34;guidance_scale&#34;: 4.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#39;smart50&#39;,&#xA;        &#34;support_noise_less_qsample_steps&#34;: 5,&#xA;    },&#xA;)&#xA;if_I.show(result[&#39;II&#39;], 1, 20)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/deep_floyd_if_image_2_image.gif&#34; alt=&#34;Alternative Text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;III. Super Resolution&lt;/h2&gt; &#xA;&lt;p&gt;For super-resolution, users can run &lt;code&gt;IF-II&lt;/code&gt; and &lt;code&gt;IF-III&lt;/code&gt; or &#39;Stable x4&#39; on an image that was not necessarely generated by IF (two cascades):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepfloyd_if.pipelines import super_resolution&#xA;&#xA;middle_res = super_resolution(&#xA;    t5,&#xA;    if_III=if_II,&#xA;    prompt=[&#39;woman with a blue headscarf and a blue sweaterp, detailed picture, 4k dslr, best quality&#39;],&#xA;    support_pil_img=raw_pil_image,&#xA;    img_scale=4.,&#xA;    img_size=64,&#xA;    if_III_kwargs={&#xA;        &#39;sample_timestep_respacing&#39;: &#39;smart100&#39;,&#xA;        &#39;aug_level&#39;: 0.5,&#xA;        &#39;guidance_scale&#39;: 6.0,&#xA;    },&#xA;)&#xA;high_res = super_resolution(&#xA;    t5,&#xA;    if_III=if_III,&#xA;    prompt=[&#39;&#39;],&#xA;    support_pil_img=middle_res[&#39;III&#39;][0],&#xA;    img_scale=4.,&#xA;    img_size=256,&#xA;    if_III_kwargs={&#xA;        &#34;guidance_scale&#34;: 9.0,&#xA;        &#34;noise_level&#34;: 20,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,&#xA;    },&#xA;)&#xA;show_superres(raw_pil_image, high_res[&#39;III&#39;][0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/if_as_upscaler.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;IV. Zero-shot Inpainting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepfloyd_if.pipelines import inpainting&#xA;&#xA;result = inpainting(&#xA;    t5=t5, if_I=if_I,&#xA;    if_II=if_II,&#xA;    if_III=if_III,&#xA;    support_pil_img=raw_pil_image,&#xA;    inpainting_mask=inpainting_mask,&#xA;    prompt=[&#xA;        &#39;oil art, a man in a hat&#39;,&#xA;    ],&#xA;    seed=42,&#xA;    if_I_kwargs={&#xA;        &#34;guidance_scale&#34;: 7.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;10,10,10,10,10,0,0,0,0,0&#34;,&#xA;        &#39;support_noise_less_qsample_steps&#39;: 0,&#xA;    },&#xA;    if_II_kwargs={&#xA;        &#34;guidance_scale&#34;: 4.0,&#xA;        &#39;aug_level&#39;: 0.0,&#xA;        &#34;sample_timestep_respacing&#34;: &#39;100&#39;,&#xA;    },&#xA;    if_III_kwargs={&#xA;        &#34;guidance_scale&#34;: 9.0,&#xA;        &#34;noise_level&#34;: 20,&#xA;        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,&#xA;    },&#xA;)&#xA;if_I.show(result[&#39;I&#39;], 2, 3)&#xA;if_I.show(result[&#39;II&#39;], 2, 6)&#xA;if_I.show(result[&#39;III&#39;], 2, 14)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/deep_floyd_if_inpainting.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ü§ó Model Zoo ü§ó&lt;/h3&gt; &#xA;&lt;p&gt;The link to download the weights as well as the model cards will be available soon on each model of the model zoo&lt;/p&gt; &#xA;&lt;h4&gt;Original&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cascade&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Batch size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Steps&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-M-v1.0&#34;&gt;IF-I-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;I&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;400M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3072&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.5M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-L-v1.0&#34;&gt;IF-I-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;I&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;900M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3200&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.0M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;IF-I-XL&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;I&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3072&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.42M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-II-M-v1.0&#34;&gt;IF-II-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;II&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;450M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1536&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.5M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-II-L-v1.0&#34;&gt;IF-II-L&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;II&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1536&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.5M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;IF-III-L* &lt;em&gt;(soon)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;III&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;700M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3072&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.25M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*best modules&lt;/p&gt; &#xA;&lt;h3&gt;Quantitative Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;FID = 6.66&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/fid30k_if.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is released under the bespoke license (see added &lt;a href=&#34;https://github.com/deep-floyd/IF/raw/main/LICENSE#L13&#34;&gt;point two&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The weights will be available soon via &lt;a href=&#34;https://huggingface.co/DeepFloyd&#34;&gt;the DeepFloyd organization at Hugging Face&lt;/a&gt; and have their own LICENSE.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; &lt;em&gt;The initial release of the IF model is under a restricted research-purposes-only license temporarily to gather feedback, and after that we intend to release a fully open-source model in line with other Stability AI models.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Limitations and Biases&lt;/h2&gt; &#xA;&lt;p&gt;The models available in this codebase have known limitations and biases. Please refer to &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-L-v1.0&#34;&gt;the model card&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;üéì DeepFloyd IF creators:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alex Shonenkov &lt;a href=&#34;https://github.com/shonenkov&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://linktr.ee/shonenkovAI&#34;&gt;Linktr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Misha Konstantinov &lt;a href=&#34;https://github.com/zeroshot-ai&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/_bra_ket&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Daria Bakshandaeva &lt;a href=&#34;https://github.com/Gugutse&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/_gugutse_&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Christoph Schuhmann &lt;a href=&#34;https://github.com/christophschuhmann&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/laion_ai&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ksenia Ivanova &lt;a href=&#34;https://github.com/ivksu&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/susiaiv&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nadiia Klokova &lt;a href=&#34;https://github.com/vauimpuls&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/vauimpuls&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÑ Research Paper (Soon)&lt;/h2&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;http://stability.ai&#34;&gt;StabilityAI&lt;/a&gt; and its CEO &lt;a href=&#34;https://twitter.com/emostaque&#34;&gt;Emad Mostaque&lt;/a&gt; for invaluable support, providing GPU compute and infrastructure to train the models (our gratitude goes to &lt;a href=&#34;https://github.com/rvencu&#34;&gt;Richard Vencu&lt;/a&gt;); thanks to &lt;a href=&#34;https://laion.ai&#34;&gt;LAION&lt;/a&gt; and &lt;a href=&#34;https://github.com/christophschuhmann&#34;&gt;Christoph Schuhmann&lt;/a&gt; in particular for contribution to the project and well-prepared datasets; thanks to &lt;a href=&#34;https://huggingface.co&#34;&gt;Huggingface&lt;/a&gt; teams for optimizing models&#39; speed and memory consumption during inference, creating demos and giving cool advice!&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ External Contributors üöÄ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Biggest Thanks &lt;a href=&#34;https://github.com/apolinario&#34;&gt;@Apolin√°rio&lt;/a&gt;, for ideas, consultations, help and support on all stages to make IF available in open-source; for writing a lot of documentation and instructions; for creating a friendly atmosphere in difficult moments ü¶â;&lt;/li&gt; &#xA; &lt;li&gt;Thanks, &lt;a href=&#34;https://github.com/patrickvonplaten&#34;&gt;@patrickvonplaten&lt;/a&gt;, for improving loading time of unet models by 80%; for integration Stable-Diffusion-x4 as native pipeline üí™;&lt;/li&gt; &#xA; &lt;li&gt;Thanks, &lt;a href=&#34;https://github.com/williamberman&#34;&gt;@williamberman&lt;/a&gt; and &lt;a href=&#34;https://github.com/patrickvonplaten&#34;&gt;@patrickvonplaten&lt;/a&gt; for diffusers integration üôå;&lt;/li&gt; &#xA; &lt;li&gt;Thanks, &lt;a href=&#34;https://github.com/hysts&#34;&gt;@hysts&lt;/a&gt; and &lt;a href=&#34;https://github.com/apolinario&#34;&gt;@Apolin√°rio&lt;/a&gt; for creating &lt;a href=&#34;https://huggingface.co/spaces/DeepFloyd/IF&#34;&gt;the best gradio demo with IF&lt;/a&gt; üöÄ;&lt;/li&gt; &#xA; &lt;li&gt;Thanks, &lt;a href=&#34;https://github.com/Dango233&#34;&gt;@Dango233&lt;/a&gt;, for adapting IF with xformers memory efficient attention üí™;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jostmey/NakedAttention</title>
    <updated>2023-05-02T01:43:49Z</updated>
    <id>tag:github.com,2023-05-02:/jostmey/NakedAttention</id>
    <link href="https://github.com/jostmey/NakedAttention" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Revealing example of self-attention, the building block of transformer AI models&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Revealing example of a self-attention model, the building block of a transformer model&lt;/h2&gt; &#xA;&lt;p&gt;Have you heard of transformer models and attempted to study their code but struggled to locate the essential components that make them tick? Look no further, you&#39;ve arrived at the right place!&lt;/p&gt; &#xA;&lt;p&gt;This GitHub repository presents a lucid, minimalistic example of an attention mechanism, which forms the backbone of a transformer model. By simplifying the code, the self-attention model becomes easier to grasp. To examine the code, open &lt;code&gt;model_and_script.py&lt;/code&gt; and navigate to line 20, where you&#39;ll find a straightforward self-attention model implementation.&lt;/p&gt; &#xA;&lt;p&gt;Inside &lt;code&gt;SelfAttentionModel&lt;/code&gt;, the code proceeds to demonstrate the model&#39;s execution process starting at line 36. Employing a loop, each sample in the batch is sequentially processed through self-attention. Within this loop, queries, keys, and values are computed, and attention weights are calculated using queries, keys, and the softmax function. Following this, attention is applied to the values, yielding the self-attention mechanism&#39;s output. At line 43, the self-attention results from individual samples are aggregated into a batch. Finally, the self-attention output is passed through an additional layer to produce the model&#39;s final output. Tensor shapes are provided after each line of code to clarify data manipulation.&lt;/p&gt; &#xA;&lt;p&gt;The model is tested on the widely-recognized MNIST dataset. To maintain simplicity, the 28x28 pixels of each image are flattened into 784 inputs. Although the results may not be as remarkable as those of other deep learning models, the primary goal is to offer the most straightforward, concise example of self-attention. The model&#39;s speed is also limited due to the use of a for-loop for processing samples sequentially. Employing built-in functions would enable parallel computation of self-attention for each sample, enhancing performance.&lt;/p&gt; &#xA;&lt;p&gt;To run the code, simply execute python3 model_and_script.py, or you can refer to results.txt.&lt;/p&gt; &#xA;&lt;p&gt;With this foundational understanding of self-attention, you&#39;re now equipped with the essential knowledge to comprehend how transformer models function. Your next step is to explore further resources on constructing transformer models based on self-attention mechanisms. Good luck!&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please feel free to reach out if you encounter any issues or even have suspicions about potential problems within this repository. Your feedback is greatly appreciated and will help ensure the accuracy and quality of the content presented.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://news.ycombinator.com/user?id=GaggiX&#34;&gt;GaggiX&lt;/a&gt; for helping identify issues with this repository.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/jminuse&#34;&gt;James Stevenson&lt;/a&gt; for identifying and fixing a memory leak issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://torchmetrics.readthedocs.io/&#34;&gt;TorchMetrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linux Environment (Recommended)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download: &lt;a href=&#34;https://github.com/jostmey/NakedAttention/zipball/master&#34;&gt;zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Git: &lt;code&gt;git clone https://github.com/jostmey/NakedAttention&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mlc-ai/mlc-llm</title>
    <updated>2023-05-02T01:43:49Z</updated>
    <id>tag:github.com,2023-05-02:/mlc-ai/mlc-llm</id>
    <link href="https://github.com/mlc-ai/mlc-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enable everyone to develop, optimize and deploy AI models natively on everyone&#39;s devices.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLC LLM&lt;/h1&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://mlc.ai/mlc-llm/&#34;&gt;Project&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/blog/blog/2023/05/01/bringing-accelerated-llm-to-consumer-hardware&#34;&gt;Blog&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/mlc-llm/#iphone&#34;&gt;Demo: iOS&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/mlc-llm/#windows-linux-mac&#34;&gt;Demo: CLI&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/web-llm/&#34;&gt;WebLLM&lt;/a&gt; | &lt;a href=&#34;https://mlc.ai/web-stable-diffusion/&#34;&gt;WebStableDiffusion&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;MLC LLM is a &lt;strong&gt;universal solution&lt;/strong&gt; that allows &lt;strong&gt;any language models&lt;/strong&gt; to be &lt;strong&gt;deployed natively&lt;/strong&gt; on a diverse set of hardware backends and native applications, plus a &lt;strong&gt;productive framework&lt;/strong&gt; for everyone to further optimize model performance for their own use cases.&lt;/p&gt; &#xA;&lt;p&gt;Our mission is to &lt;strong&gt;enable everyone to develop, optimize and deploy AI models natively on everyone&#39;s devices&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Everything runs locally with no server support and accelerated with local GPUs on your phone and laptops. &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm/issues/15&#34;&gt;Supported platforms&lt;/a&gt; include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iPhone, iPad&lt;/li&gt; &#xA; &lt;li&gt;Metal GPUs and Intel/ARM MacBooks;&lt;/li&gt; &#xA; &lt;li&gt;AMD and NVIDIA GPUs via Vulkan on Windows and Linux;&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPUs via CUDA on Windows and Linux;&lt;/li&gt; &#xA; &lt;li&gt;WebGPU on browsers (through companion project &lt;a href=&#34;https://github.com/mlc-ai/web-llm/tree/main&#34;&gt;WebLLM&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://mlc.ai/mlc-llm/&#34;&gt;Check out our instruction page to try out!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/site/demo.gif&#34; height=&#34;700&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;What is MLC LLM?&lt;/h2&gt; &#xA;&lt;p&gt;In recent years, there has been remarkable progress in generative artificial intelligence (AI) and large language models (LLMs), which are becoming increasingly prevalent. Thanks to open-source initiatives, it is now possible to develop personal AI assistants using open-sourced models. However, LLMs tend to be resource-intensive and computationally demanding. To create a scalable service, developers may need to rely on powerful clusters and expensive hardware to run model inference. Additionally, deploying LLMs presents several challenges, such as their ever-evolving model innovation, memory constraints, and the need for potential optimization techniques.&lt;/p&gt; &#xA;&lt;p&gt;The goal of this project is to enable the development, optimization, and deployment of AI models for inference across a range of devices, including not just server-class hardware, but also users&#39; browsers, laptops, and mobile apps. To achieve this, we need to address the diverse nature of compute devices and deployment environments. Some of the key challenges include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supporting different models of CPUs, GPUs, and potentially other co-processors and accelerators.&lt;/li&gt; &#xA; &lt;li&gt;Deploying on the native environment of user devices, which may not have python or other necessary dependencies readily available.&lt;/li&gt; &#xA; &lt;li&gt;Addressing memory constraints by carefully planning allocation and aggressively compressing model parameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MLC LLM offers a repeatable, systematic, and customizable workflow that empowers developers and AI system researchers to implement models and optimizations in a productivity-focused, Python-first approach. This methodology enables quick experimentation with new models, new ideas and new compiler passes, followed by native deployment to the desired targets. Furthermore, we are continuously expanding LLM acceleration by broadening TVM backends to make model compilation more transparent and efficient.&lt;/p&gt; &#xA;&lt;h2&gt;How does MLC Enable Universal Native Deployment?&lt;/h2&gt; &#xA;&lt;p&gt;The cornerstone of our solution is machine learning compilation (&lt;a href=&#34;https://mlc.ai/&#34;&gt;MLC&lt;/a&gt;), which we leverage to efficiently deploy AI models. We build on the shoulders of open-source ecosystems, including tokenizers from HuggingFace and Google, as well as open-source LLMs like Llama, Vicuna, Dolly, MOSS and more. Our primary workflow is based on &lt;a href=&#34;https://github.com/apache/tvm/tree/unity&#34;&gt;Apache TVM Unity&lt;/a&gt;, an exciting ongoing development in the Apache TVM Community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dynamic shape: we bake a language model as a TVM IRModule with native dynamic shape support, avoiding the need for extra padding to the maximum length and reducing both computation amount and memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Composable ML compilation optimizations: we perform many model deployment optimizations, such as better compilation code transformation, fusion, memory planning, library offloading and manual code optimization can be easily incorporated as TVM&#39;s IRModule transformations exposed as Python APIs.&lt;/li&gt; &#xA; &lt;li&gt;Quantization: we utilize low-bit quantizations to compress the model weights and leverage TVM&#39;s loop-level TensorIR to quickly customize code generations for different compression encoding schemes.&lt;/li&gt; &#xA; &lt;li&gt;Runtime: The final generated libraries run on the native environment, with TVM runtime that comes with minimal dependencies, which supports various GPU driver APIs and native language bindings (C, JavaScript, etc).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/site/img/diag.svg?sanitize=true&#34; alt=&#34;Architecture Diagram&#34; height=&#34;&#34;&gt; &#xA;&lt;p&gt;Additionally, we also provide a lightweight C++-based example CLI app that showcases how to wrap up the compiled artifacts and necessary pre/post-processing, which will hopefully clarify the workflow to embed them into native applications.&lt;/p&gt; &#xA;&lt;p&gt;As a starting point, MLC generates GPU shaders for CUDA, Vulkan and Metal. It is possible to add more support, such as OpenCL, sycl, webgpu-native, through improvements to TVM compiler and runtime. MLC also supports various CPU targets including ARM and x86 via LLVM.&lt;/p&gt; &#xA;&lt;p&gt;We heavily rely on open-source ecosystem, more specifically, &lt;a href=&#34;https://discuss.tvm.apache.org/t/establish-tvm-unity-connection-a-technical-strategy/13344&#34;&gt;TVM Unity&lt;/a&gt;, an exciting latest development in the TVM project that enables python-first interactive MLC development experiences that allows us to easily compose new optimizations all in Python, and incrementally bring our app to the environment of interest. We also leveraged optimizations such as fused quantization kernels, first class dynamic shape support and diverse GPU backends.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You might also be interested in &lt;a href=&#34;https://github.com/mlc-ai/web-llm/tree/main&#34;&gt;WebLLM&lt;/a&gt;, our companion derived project that focus on bringing LLM to browsers.&lt;/li&gt; &#xA; &lt;li&gt;Project page for &lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/site/index.md&#34;&gt;instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/ios/README.md&#34;&gt;Local build Instructions for ios App&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You might want to check out our online public &lt;a href=&#34;https://mlc.ai&#34;&gt;Machine Learning Compilation course&lt;/a&gt; for a systematic walkthrough of our approaches.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is initiated by members from CMU catalyst, UW SAMPL, SJTU, OctoML and the MLC community. We would love to continue developing and supporting the open-source ML community.&lt;/p&gt; &#xA;&lt;p&gt;This project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities that make these models accessible. We would like to thank the teams behind Vicuna, SentencePiece, LLaMA, Alpaca and MOSS. We also would like to thank the Vulkan, Swift, C++, python Rust communities that enables this project.&lt;/p&gt;</summary>
  </entry>
</feed>