<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-29T01:40:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xxlong0/Wonder3D</title>
    <updated>2023-10-29T01:40:18Z</updated>
    <id>tag:github.com,2023-10-29:/xxlong0/Wonder3D</id>
    <link href="https://github.com/xxlong0/Wonder3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A cross-domain diffusion model for 3D reconstruction from a single image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Wonder3D&lt;/h1&gt; &#xA;&lt;p&gt;Single Image to 3D using Cross-Domain Diffusion&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.15008&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.xxlong.site/Wonder3D/&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/flamehaze1115/Wonder3D-demo&#34;&gt;Hugging Face Demo&lt;/a&gt; | &lt;a href=&#34;https://github.com/camenduru/Wonder3D-colab&#34;&gt;Colab from @camenduru&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xxlong0/Wonder3D/main/assets/fig_teaser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Wonder3D reconstructs highly-detailed textured meshes from a single-view image in only 2 ∼ 3 minutes. Wonder3D first generates consistent multi-view normal maps with corresponding color images via a cross-domain diffusion model, and then leverages a novel normal fusion method to achieve fast and high-quality reconstruction.&lt;/p&gt; &#xA;&lt;h2&gt;Schedule&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code and pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Huggingface demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; New model trained on the whole Objaverse dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preparation for inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install packages in &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;conda create -n wonder3d&#xA;conda activate wonder3d&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install tiny-cuda-nn PyTorch extension for mesh extraction: &lt;code&gt;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Download the &lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:f:/g/personal/xxlong_connect_hku_hk/EgSHPyJAtaJFpV_BjXM3zXwB-UMIrT4v-sQwGgw-coPtIA&#34;&gt;checkpoints&lt;/a&gt; and into the root folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have the following models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Wonder3D&#xA;|-- ckpts&#xA;    |-- unet&#xA;    |-- scheduler.bin&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Predict foreground mask as the alpha channel. We use &lt;a href=&#34;https://clipdrop.co/remove-background&#34;&gt;Clipdrop&lt;/a&gt; to segment the foreground object interactively. You may also use &lt;code&gt;rembg&lt;/code&gt; to remove the backgrounds.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# !pip install rembg&#xA;import rembg&#xA;result = rembg.remove(result)&#xA;result.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run Wonder3d to produce multiview-consistent normal maps and color images. Then you can check the results in the folder &lt;code&gt;./outputs&lt;/code&gt;. (we use rembg to remove backgrounds of the results, but the segmemtations are not always perfect.)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file 1gpu.yaml test_mvdiffusion_seq.py \&#xA;            --config mvdiffusion-joint-ortho-6views.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash run_test.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Mesh Extraction&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./instant-nsr-pl&#xA;bash run.sh output_folder_path scene_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our generated normals and color images are defined in orthographic views, so the reconstructed mesh is also in orthographic camera space. If you use MeshLab to view the meshes, you can click &lt;code&gt;Toggle Orthographic Camera&lt;/code&gt; in &lt;code&gt;View&lt;/code&gt; tab.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful in your project, please cite the following work. :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{long2023wonder3d,&#xA;      title={Wonder3D: Single Image to 3D using Cross-Domain Diffusion}, &#xA;      author={Xiaoxiao Long and Yuan-Chen Guo and Cheng Lin and Yuan Liu and Zhiyang Dou and Lingjie Liu and Yuexin Ma and Song-Hai Zhang and Marc Habermann and Christian Theobalt and Wenping Wang},&#xA;      year={2023},&#xA;      eprint={2310.15008},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>codefuse-ai/Test-Agent</title>
    <updated>2023-10-29T01:40:18Z</updated>
    <id>tag:github.com,2023-10-29:/codefuse-ai/Test-Agent</id>
    <link href="https://github.com/codefuse-ai/Test-Agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;国内首个测试行业大模型工具，体验AIGC为测试领域带来的变革！&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Test-Agent: 您的智能测试助理&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/5737b652-1549-4242-bcb2-69e76603c5e6&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/codefuse-ai/Test-Agent&#34;&gt; &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/codefuse-ai/Test-Agent?style=social&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/codefuse-ai/Test-Agent&#34;&gt; &lt;img alt=&#34;forks&#34; src=&#34;https://img.shields.io/github/forks/codefuse-ai/Test-Agent?style=social&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/codefuse-ai/Test-Agent/LICENCE&#34;&gt; &lt;img alt=&#34;License: MIT&#34; src=&#34;https://badgen.net/badge/license/apache2.0/blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/codefuse-ai/Test-Agent/issues&#34;&gt; &lt;img alt=&#34;Open Issues&#34; src=&#34;https://img.shields.io/github/issues-raw/codefuse-ai/Test-Agent&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;本地Mac M1体验效果&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/8dba860f-c1bb-49d5-b9dd-a58e541562a6&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;魔搭体验效果&lt;/h3&gt; &#xA;&lt;p&gt;魔搭模型访问链接：&lt;a href=&#34;https://modelscope.cn/models/codefuse-ai/TestGPT-7B/summary&#34;&gt;ModelScope TestGPT-7B&lt;/a&gt; &lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/0e50b258-44f9-4dc6-8e30-0a01cf62d02b&#34; alt=&#34;MS&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;什么是Test Agent？（Introduction）&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Test Agent&lt;/strong&gt; 旨在构建测试领域的“智能体”，融合大模型和质量领域工程化技术，促进质量技术代系升级。我们期望和社区成员一起合作，打造创新的测试领域解决方案，构建24小时在线的测试助理服务，让测试如丝般顺滑。&lt;/p&gt; &#xA;&lt;h2&gt;本期特性（Features）&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;模型&lt;/strong&gt; 本期我们开源了测试领域模型TestGPT-7B。模型以CodeLlama-7B为基座，进行了相关下游任务的微调：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;多语言测试用例生成（Java/Python/Javascript）&lt;/strong&gt; 一直以来都是学术界和工业界非常关注的领域，近年来不断有新产品或工具孵化出来，如EvoSuite、Randoop、SmartUnit等。然而传统的用例生成存在其难以解决的痛点问题，基于大模型的测试用例生成在测试用例可读性、测试场景完整度、多语言支持方面都优于传统用例生成工具。本次重点支持了多语言测试用例生成，在我们本次开源的版本中首先包含了Java、Python、Javascript的测试用例生成能力，下一版本中逐步开放Go、C++等语言。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;测试用例Assert补全&lt;/strong&gt; 对当前测试用例现状的分析与探查时，我们发现代码仓库中存在一定比例的存量测试用例中未包含Assert。没有Assert的测试用例虽然能够在回归过程中执行通过，却无法发现问题。因此我们拓展了测试用例Assert自动补全这一场景。通过该模型能力，结合一定的工程化配套，可以实现对全库测试用例的批量自动补全，智能提升项目质量水位。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;工程框架&lt;/strong&gt; 本地模型快速发布和体验工程化框架&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ChatBot页面&lt;/li&gt; &#xA;   &lt;li&gt;模型快速启动&lt;/li&gt; &#xA;   &lt;li&gt;私有化部署，本地化的GPT大模型与您的数据和环境进行交互，无数据泄露风险，100%安全&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;后续我们会持续迭代模型和工程化能力：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;不断加入更多令人激动的测试域应用场景，如领域知识问答、测试场景分析等&lt;/li&gt; &#xA; &lt;li&gt;支撑面向测试场景的copilot 工程框架开放，如测试领域知识智能embedding、测试通用工具API体系、智能测试Agent等，敬请期待！&lt;/li&gt; &#xA; &lt;li&gt;以7B为基础，逐步扩展至13B、34B模型。欢迎关注！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;性能最强的7B测试领域大模型（Model）&lt;/h2&gt; &#xA;&lt;p&gt;目前在TestAgent中，我们默认使用了TestGPT-7B模型。与当前已有开源模型相比，&lt;strong&gt;TestGPT-7B模型在用例执行通过率（pass@1）、用例场景覆盖（平均测试场景数）上都处于业界领先水平。&lt;/strong&gt; TestGPT-7B模型核心能力的评测结果如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;多语言测试用例生成 针对模型支持的三种语言：Java、Python、Javascript，Pass@1评测结果如下：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Java pass@1&lt;/th&gt; &#xA;   &lt;th&gt;Java Average number of test scenarios&lt;/th&gt; &#xA;   &lt;th&gt;Python pass@1&lt;/th&gt; &#xA;   &lt;th&gt;Python Average number of test scenarios&lt;/th&gt; &#xA;   &lt;th&gt;Javascript pass@1&lt;/th&gt; &#xA;   &lt;th&gt;Javascript Average number of test scenarios&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TestGPT-7B&lt;/td&gt; &#xA;   &lt;td&gt;48.6%&lt;/td&gt; &#xA;   &lt;td&gt;4.37&lt;/td&gt; &#xA;   &lt;td&gt;35.67%&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;36%&lt;/td&gt; &#xA;   &lt;td&gt;2.76&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLlama-13B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;40.54%&lt;/td&gt; &#xA;   &lt;td&gt;1.08&lt;/td&gt; &#xA;   &lt;td&gt;30.57%&lt;/td&gt; &#xA;   &lt;td&gt;1.65&lt;/td&gt; &#xA;   &lt;td&gt;31.7%&lt;/td&gt; &#xA;   &lt;td&gt;3.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;10.81%&lt;/td&gt; &#xA;   &lt;td&gt;2.78&lt;/td&gt; &#xA;   &lt;td&gt;15.9%&lt;/td&gt; &#xA;   &lt;td&gt;1.32&lt;/td&gt; &#xA;   &lt;td&gt;9.15%&lt;/td&gt; &#xA;   &lt;td&gt;4.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2-13B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;13.5%&lt;/td&gt; &#xA;   &lt;td&gt;2.24&lt;/td&gt; &#xA;   &lt;td&gt;12.7%&lt;/td&gt; &#xA;   &lt;td&gt;2.12&lt;/td&gt; &#xA;   &lt;td&gt;6.1%&lt;/td&gt; &#xA;   &lt;td&gt;3.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;测试用例Assert补全 目前模型支持Java用例的Assert补全，Pass@1评测结果如下：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;pass@1&lt;/th&gt; &#xA;   &lt;th&gt;Percentage of strong validation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Codefuse-TestGPT-7B&lt;/td&gt; &#xA;   &lt;td&gt;71.1%&lt;/td&gt; &#xA;   &lt;td&gt;100%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;工程架构（Engineering Architecture）&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/1b61beff-df59-4ab3-843c-266413c8dbc4&#34; alt=&#34;JG&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;大模型的号角已经吹响，测试领域大模型也在不断进化中，通过预训练过程中积累的丰富世界知识，在复杂交互环境中展现出了非凡的推理与决策能力。&lt;/p&gt; &#xA;&lt;p&gt;尽管在测试领域中基础模型取得了显著的成果，但仍然存在一些局限性，特定领域的测试任务通常需要专业化的工具或领域知识来解决。例如，基础模型可以通过预训练知识完成单次测试代码生成和测试文本生成等任务，但处理复杂的集成用例生成、特定领域用例生成和测试流程pipeline交互等问题时，需要更专业的工具和领域知识。因此将专用工具与基础模型整合在一起，可以充分发挥它们各自的优势。专用工具可以解决模型时效性不足、增强专业知识、提高可解释性和鲁棒性的问题。而基础模型则具备类人的推理规划能力，可以理解复杂的数据和场景，并与现实世界进行交互。&lt;/p&gt; &#xA;&lt;p&gt;在本期开放模型工程化部署和ChatBot基础上，我们将继续在测试开源领域深耕投入。协同社区志趣相投开发者们，一起打造测试领域最领先的Tools工程体系、智能测试助理和测试开源工程！&lt;/p&gt; &#xA;&lt;h2&gt;快速使用（QuickStart）&lt;/h2&gt; &#xA;&lt;h3&gt;前置准备&lt;/h3&gt; &#xA;&lt;h4&gt;模型下载&lt;/h4&gt; &#xA;&lt;p&gt;您可在&lt;a href=&#34;https://modelscope.cn/models/codefuse-ai/TestGPT-7B&#34;&gt;modelscope&lt;/a&gt;或&lt;a href=&#34;https://huggingface.co/codefuse-ai/TestGPT-7B&#34;&gt;huggingface&lt;/a&gt;上获取到模型的详细信息并下载模型文件。 需要注意的是： 1）如果您通过modelscope下载模型，下载方式可参考：&lt;a href=&#34;https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD#%E4%BD%BF%E7%94%A8Git%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B&#34;&gt;下载说明&lt;/a&gt;； 2）如果您通过huggingface下载模型，请确保您可以正常访问huggingface。&lt;/p&gt; &#xA;&lt;h4&gt;环境安装&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python&amp;gt;=3.8&lt;/li&gt; &#xA; &lt;li&gt;transformers==4.33.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;git clone https://github.com/codefuse-ai/Test-Agent&#xA;cd Test-Agent&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;在开始运行TestGPT-7B模型之前，请确保你的执行环境拥有大约14GB的显存。&lt;/p&gt; &#xA;&lt;h3&gt;启动服务&lt;/h3&gt; &#xA;&lt;p&gt;项目提供了网页端快速搭建UI的能力能够更直观的展示模型交互和效果，我们可以使用简单的几个命令把前端页面唤醒并实时调用模型能力。在项目目录下，依次启动以下服务：&lt;/p&gt; &#xA;&lt;p&gt;1.&lt;strong&gt;启动controller&lt;/strong&gt; &lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/e68ce187-c9f1-4ce8-9d59-ff9d8348d0ac&#34; alt=&#34;controller&#34;&gt; python3 -m chat.server.controller&lt;/p&gt; &#xA;&lt;p&gt;2.&lt;strong&gt;启动模型worker&lt;/strong&gt; &lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/073e4e79-4005-4c98-87f7-0eaa0b2b1e22&#34; alt=&#34;work&#34;&gt; python3 -m chat.server.model_worker --model-path models/TestGPT-7B --device mps&lt;/p&gt; &#xA;&lt;p&gt;（models/TestGPT-7B 为实际模型文件路径）&lt;/p&gt; &#xA;&lt;p&gt;对于启动方式，可以按需选择以下几种配置选项：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;--device mps 用于在Mac电脑上开启GPU加速的选项（Apple Silicon或AMD GPUs）；&lt;/li&gt; &#xA; &lt;li&gt;--device xpu 用于在Intel XPU上开启加速的选项（Intel Data Center and Arc A-Series GPUs）； &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;需安装&lt;a href=&#34;https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/installation.html&#34;&gt;Intel Extension for PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;设置OneAPI环境变量：source /opt/intel/oneapi/setvars.sh&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;--device npu 用于在华为AI处理器上开启加速的选项； &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;需安装&lt;a href=&#34;https://github.com/Ascend/pytorch&#34;&gt;Ascend PyTorch Adapter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;设置CANN环境变量：source /usr/local/Ascend/ascend-toolkit/set_env.sh&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;--device cpu 单独使用CPU运行的选项，不需要GPU；&lt;/li&gt; &#xA; &lt;li&gt;--num-gpus 2 指定并发gpu运行的选项。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;启动web服务&lt;/strong&gt; python3 -m chat.server.gradio_testgpt &lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/340dae35-573b-4046-a3e8-e87a91453601&#34; alt=&#34;web&#34;&gt; 待服务准备就绪后，我们可以打开本地启动的web服务地址 &lt;a href=&#34;http://0.0.0.0:7860&#34;&gt;http://0.0.0.0:7860&lt;/a&gt; ，就能看到完整的前端页面了。在页面下方包含了【单测生成】和【Assert补全】的两个例子，点击按钮后会自动生成一段样例文本到输入框中，点击Send按钮就会触发模型运行，之后耐心等待一段时间后（运行时间视本机性能而定）即可看到完整的回答了。 &lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/103973989/fd24274c-729b-4ce7-8763-a083b39300fb&#34; alt=&#34;demo&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🤗 致谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 构建，在此深深感谢他们的开源贡献！&lt;/p&gt; &#xA;&lt;h2&gt;联系我们&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/codefuse-ai/Test-Agent/assets/106229399/074aac97-3486-4062-8942-68495c7e4dc4&#34; alt=&#34;wexin_2&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>serengil/deepface</title>
    <updated>2023-10-29T01:40:18Z</updated>
    <id>tag:github.com,2023-10-29:/serengil/deepface</id>
    <link href="https://github.com/serengil/deepface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;deepface&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/deepface&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/deepface?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=pypi%20downloads&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/deepface&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/dn/conda-forge/deepface?color=green&amp;amp;label=conda%20downloads&#34; alt=&#34;Conda Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/serengil/deepface/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/serengil/deepface?color=yellow&amp;amp;style=flat&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;http://img.shields.io/:license-MIT-green.svg?style=flat&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/serengil?repo=deepface&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fshieldsio-patreon.vercel.app%2Fapi%3Fusername%3Dserengil%26type%3Dpatrons&amp;amp;style=flat&#34; alt=&#34;Support me on Patreon&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/serengil&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/serengil?logo=GitHub&amp;amp;color=lightgray&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/ASYU50717.2020.9259802&#34;&gt;&lt;img src=&#34;http://img.shields.io/:DOI-10.1109/ASYU50717.2020.9259802-blue.svg?style=flat&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICEET53442.2021.9659697&#34;&gt;&lt;img src=&#34;http://img.shields.io/:DOI-10.1109/ICEET53442.2021.9659697-blue.svg?style=flat&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://sefiks.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&amp;amp;logo=wordpress&#34; alt=&#34;Blog&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/@sefiks?sub_confirmation=1&#34;&gt;&lt;img src=&#34;https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&amp;amp;logo=youtube&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/user?screen_name=serengil&#34;&gt;&lt;img src=&#34;https://img.shields.io/:follow-@serengil-blue.svg?style=flat&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png&#34; width=&#34;200&#34; height=&#34;240&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deepface is a lightweight &lt;a href=&#34;https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/&#34;&gt;face recognition&lt;/a&gt; and facial attribute analysis (&lt;a href=&#34;https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/&#34;&gt;age&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/&#34;&gt;gender&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/&#34;&gt;emotion&lt;/a&gt; and &lt;a href=&#34;https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/&#34;&gt;race&lt;/a&gt;) framework for python. It is a hybrid face recognition framework wrapping &lt;strong&gt;state-of-the-art&lt;/strong&gt; models: &lt;a href=&#34;https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/&#34;&gt;&lt;code&gt;VGG-Face&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/&#34;&gt;&lt;code&gt;Google FaceNet&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/&#34;&gt;&lt;code&gt;OpenFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/&#34;&gt;&lt;code&gt;Facebook DeepFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/&#34;&gt;&lt;code&gt;DeepID&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/&#34;&gt;&lt;code&gt;ArcFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/&#34;&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SFace&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Experiments show that human beings have 97.53% accuracy on facial recognition tasks whereas those models already reached and passed that accuracy level.&lt;/p&gt; &#xA;&lt;h2&gt;Installation &lt;a href=&#34;https://pypi.org/project/deepface/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/deepface.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/deepface&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/deepface.svg?sanitize=true&#34; alt=&#34;Conda&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to install deepface is to download it from &lt;a href=&#34;https://pypi.org/project/deepface/&#34;&gt;&lt;code&gt;PyPI&lt;/code&gt;&lt;/a&gt;. It&#39;s going to install the library itself and its prerequisites as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install deepface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Secondly, DeepFace is also available at &lt;a href=&#34;https://anaconda.org/conda-forge/deepface&#34;&gt;&lt;code&gt;Conda&lt;/code&gt;&lt;/a&gt;. You can alternatively install the package via conda.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ conda install -c conda-forge deepface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Thirdly, you can install deepface from its source code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/serengil/deepface.git&#xA;$ cd deepface&#xA;$ pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you will be able to import the library and use its functionalities.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepface import DeepFace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Facial Recognition&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/WnUVYQP4h44&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A modern &lt;a href=&#34;https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/&#34;&gt;&lt;strong&gt;face recognition pipeline&lt;/strong&gt;&lt;/a&gt; consists of 5 common stages: &lt;a href=&#34;https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/&#34;&gt;detect&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/&#34;&gt;align&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/&#34;&gt;normalize&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/&#34;&gt;represent&lt;/a&gt; and &lt;a href=&#34;https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/&#34;&gt;verify&lt;/a&gt;. While Deepface handles all these common stages in the background, you don’t need to acquire in-depth knowledge about all the processes behind it. You can just call its verification, find or analysis function with a single line of code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Verification&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/KRCvkNCOphE&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This function verifies face pairs as same person or different persons. It expects exact image paths as inputs. Passing numpy or base64 encoded images is also welcome. Then, it is going to return a dictionary and you should check just its verified key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = DeepFace.verify(img1_path = &#34;img1.jpg&#34;, img2_path = &#34;img2.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Verification function can also handle many faces in the face pairs. In this case, the most similar faces will be compared.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/verify-many-faces.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face recognition&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/Hrjp-EStM_s&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/&#34;&gt;Face recognition&lt;/a&gt; requires applying face verification many times. Herein, deepface has an out-of-the-box find function to handle this action. It&#39;s going to look for the identity of input image in the database path and it will return list of pandas data frame as output. Meanwhile, facial embeddings of the facial database are stored in a pickle file to be searched faster in next time. Result is going to be the size of faces appearing in the source image. Besides, target images in the database can have many faces as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfs = DeepFace.find(img_path = &#34;img1.jpg&#34;, db_path = &#34;C:/workspace/my_db&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Face recognition models basically represent facial images as multi-dimensional vectors. Sometimes, you need those embedding vectors directly. DeepFace comes with a dedicated representation function. Represent function returns a list of embeddings. Result is going to be the size of faces appearing in the image path.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_objs = DeepFace.represent(img_path = &#34;img.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This function returns an array as embedding. The size of the embedding array would be different based on the model name. For instance, VGG-Face is the default model and it represents facial images as 2622 dimensional vectors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding = embedding_objs[0][&#34;embedding&#34;]&#xA;assert isinstance(embedding, list)&#xA;assert model_name = &#34;VGG-Face&#34; and len(embedding) == 2622&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, embedding is also &lt;a href=&#34;https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/&#34;&gt;plotted&lt;/a&gt; with 2622 slots horizontally. Each slot is corresponding to a dimension value in the embedding vector and dimension value is explained in the colorbar on the right. Similar to 2D barcodes, vertical dimension stores no information in the illustration.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face recognition models&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/i_MOwvhbLdI&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deepface is a &lt;strong&gt;hybrid&lt;/strong&gt; face recognition package. It currently wraps many &lt;strong&gt;state-of-the-art&lt;/strong&gt; face recognition models: &lt;a href=&#34;https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/&#34;&gt;&lt;code&gt;VGG-Face&lt;/code&gt;&lt;/a&gt; , &lt;a href=&#34;https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/&#34;&gt;&lt;code&gt;Google FaceNet&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/&#34;&gt;&lt;code&gt;OpenFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/&#34;&gt;&lt;code&gt;Facebook DeepFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/&#34;&gt;&lt;code&gt;DeepID&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/&#34;&gt;&lt;code&gt;ArcFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/&#34;&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SFace&lt;/code&gt;. The default configuration uses VGG-Face model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;models = [&#xA;  &#34;VGG-Face&#34;, &#xA;  &#34;Facenet&#34;, &#xA;  &#34;Facenet512&#34;, &#xA;  &#34;OpenFace&#34;, &#xA;  &#34;DeepFace&#34;, &#xA;  &#34;DeepID&#34;, &#xA;  &#34;ArcFace&#34;, &#xA;  &#34;Dlib&#34;, &#xA;  &#34;SFace&#34;,&#xA;]&#xA;&#xA;#face verification&#xA;result = DeepFace.verify(img1_path = &#34;img1.jpg&#34;, &#xA;      img2_path = &#34;img2.jpg&#34;, &#xA;      model_name = models[0]&#xA;)&#xA;&#xA;#face recognition&#xA;dfs = DeepFace.find(img_path = &#34;img1.jpg&#34;,&#xA;      db_path = &#34;C:/workspace/my_db&#34;, &#xA;      model_name = models[1]&#xA;)&#xA;&#xA;#embeddings&#xA;embedding_objs = DeepFace.represent(img_path = &#34;img.jpg&#34;, &#xA;      model_name = models[2]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-v8.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;FaceNet, VGG-Face, ArcFace and Dlib are &lt;a href=&#34;https://youtu.be/i_MOwvhbLdI&#34;&gt;overperforming&lt;/a&gt; ones based on experiments. You can find out the scores of those models below on both &lt;a href=&#34;https://sefiks.com/2020/08/27/labeled-faces-in-the-wild-for-face-recognition/&#34;&gt;Labeled Faces in the Wild&lt;/a&gt; and YouTube Faces in the Wild data sets declared by its creators.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LFW Score&lt;/th&gt; &#xA;   &lt;th&gt;YTF Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Facenet512&lt;/td&gt; &#xA;   &lt;td&gt;99.65%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SFace&lt;/td&gt; &#xA;   &lt;td&gt;99.60%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ArcFace&lt;/td&gt; &#xA;   &lt;td&gt;99.41%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dlib&lt;/td&gt; &#xA;   &lt;td&gt;99.38 %&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Facenet&lt;/td&gt; &#xA;   &lt;td&gt;99.20%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VGG-Face&lt;/td&gt; &#xA;   &lt;td&gt;98.78%&lt;/td&gt; &#xA;   &lt;td&gt;97.40%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;Human-beings&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;97.53%&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenFace&lt;/td&gt; &#xA;   &lt;td&gt;93.80%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepID&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;97.05%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Similarity&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Face recognition models are regular &lt;a href=&#34;https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/&#34;&gt;convolutional neural networks&lt;/a&gt; and they are responsible to represent faces as vectors. We expect that a face pair of same person should be &lt;a href=&#34;https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/&#34;&gt;more similar&lt;/a&gt; than a face pair of different persons.&lt;/p&gt; &#xA;&lt;p&gt;Similarity could be calculated by different metrics such as &lt;a href=&#34;https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/&#34;&gt;Cosine Similarity&lt;/a&gt;, Euclidean Distance and L2 form. The default configuration uses cosine similarity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;metrics = [&#34;cosine&#34;, &#34;euclidean&#34;, &#34;euclidean_l2&#34;]&#xA;&#xA;#face verification&#xA;result = DeepFace.verify(img1_path = &#34;img1.jpg&#34;, &#xA;          img2_path = &#34;img2.jpg&#34;, &#xA;          distance_metric = metrics[1]&#xA;)&#xA;&#xA;#face recognition&#xA;dfs = DeepFace.find(img_path = &#34;img1.jpg&#34;, &#xA;          db_path = &#34;C:/workspace/my_db&#34;, &#xA;          distance_metric = metrics[2]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Euclidean L2 form &lt;a href=&#34;https://youtu.be/i_MOwvhbLdI&#34;&gt;seems&lt;/a&gt; to be more stable than cosine and regular Euclidean distance based on experiments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Facial Attribute Analysis&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/GT2UeN85BdA&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deepface also comes with a strong facial attribute analysis module including &lt;a href=&#34;https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/&#34;&gt;&lt;code&gt;age&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/&#34;&gt;&lt;code&gt;gender&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/&#34;&gt;&lt;code&gt;facial expression&lt;/code&gt;&lt;/a&gt; (including angry, fear, neutral, sad, disgust, happy and surprise) and &lt;a href=&#34;https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/&#34;&gt;&lt;code&gt;race&lt;/code&gt;&lt;/a&gt; (including asian, white, middle eastern, indian, latino and black) predictions. Result is going to be the size of faces appearing in the source image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;objs = DeepFace.analyze(img_path = &#34;img4.jpg&#34;, &#xA;        actions = [&#39;age&#39;, &#39;gender&#39;, &#39;race&#39;, &#39;emotion&#39;]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Age model got ± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its &lt;a href=&#34;https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Detectors&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/GZ2p2hj2H5k&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Face detection and alignment are important early stages of a modern face recognition pipeline. Experiments show that just alignment increases the face recognition accuracy almost 1%. &lt;a href=&#34;https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/&#34;&gt;&lt;code&gt;OpenCV&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/&#34;&gt;&lt;code&gt;SSD&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/&#34;&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/&#34;&gt;&lt;code&gt;MTCNN&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/timesler/facenet-pytorch&#34;&gt;&lt;code&gt;Faster MTCNN&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/&#34;&gt;&lt;code&gt;RetinaFace&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/&#34;&gt;&lt;code&gt;MediaPipe&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/derronqi/yolov8-face&#34;&gt;&lt;code&gt;YOLOv8 Face&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ShiqiYu/libfacedetection&#34;&gt;&lt;code&gt;YuNet&lt;/code&gt;&lt;/a&gt; detectors are wrapped in deepface.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v5.jpg&#34; width=&#34;95%&#34; height=&#34;95%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All deepface functions accept an optional detector backend input argument. You can switch among those detectors with this argument. OpenCV is the default detector.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;backends = [&#xA;  &#39;opencv&#39;, &#xA;  &#39;ssd&#39;, &#xA;  &#39;dlib&#39;, &#xA;  &#39;mtcnn&#39;, &#xA;  &#39;retinaface&#39;, &#xA;  &#39;mediapipe&#39;,&#xA;  &#39;yolov8&#39;,&#xA;  &#39;yunet&#39;,&#xA;  &#39;fastmtcnn&#39;,&#xA;]&#xA;&#xA;#face verification&#xA;obj = DeepFace.verify(img1_path = &#34;img1.jpg&#34;, &#xA;        img2_path = &#34;img2.jpg&#34;, &#xA;        detector_backend = backends[0]&#xA;)&#xA;&#xA;#face recognition&#xA;dfs = DeepFace.find(img_path = &#34;img.jpg&#34;, &#xA;        db_path = &#34;my_db&#34;, &#xA;        detector_backend = backends[1]&#xA;)&#xA;&#xA;#embeddings&#xA;embedding_objs = DeepFace.represent(img_path = &#34;img.jpg&#34;, &#xA;        detector_backend = backends[2]&#xA;)&#xA;&#xA;#facial analysis&#xA;demographies = DeepFace.analyze(img_path = &#34;img4.jpg&#34;, &#xA;        detector_backend = backends[3]&#xA;)&#xA;&#xA;#face detection and alignment&#xA;face_objs = DeepFace.extract_faces(img_path = &#34;img.jpg&#34;, &#xA;        target_size = (224, 224), &#xA;        detector_backend = backends[4]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Face recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-detectors-v3.jpg&#34; width=&#34;90%&#34; height=&#34;90%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/&#34;&gt;RetinaFace&lt;/a&gt; and &lt;a href=&#34;https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/&#34;&gt;MTCNN&lt;/a&gt; seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.&lt;/p&gt; &#xA;&lt;p&gt;The performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That&#39;s why, alignment score of RetinaFace is high as well.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg&#34; width=&#34;90%&#34; height=&#34;90%&#34;&gt; &lt;br&gt;&lt;em&gt;The Yellow Angels - Fenerbahce Women&#39;s Volleyball Team&lt;/em&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can find out more about RetinaFace on this &lt;a href=&#34;https://github.com/serengil/retinaface&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real Time Analysis&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/-c9sSJcx6wI&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DeepFace.stream(db_path = &#34;C:/User/Sefik/Desktop/database&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg&#34; width=&#34;90%&#34; height=&#34;90%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Even though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;user&#xA;├── database&#xA;│   ├── Alice&#xA;│   │   ├── Alice1.jpg&#xA;│   │   ├── Alice2.jpg&#xA;│   ├── Bob&#xA;│   │   ├── Bob.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;API&lt;/strong&gt; - &lt;a href=&#34;https://youtu.be/HeKCQ6U9XmI&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DeepFace serves an API as well. You can clone &lt;a href=&#34;https://github.com/serengil/deepface/tree/master/api&#34;&gt;&lt;code&gt;/api&lt;/code&gt;&lt;/a&gt; folder and run the api via gunicorn server. This will get a rest service up. In this way, you can call deepface from an external system such as mobile app or web.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd scripts&#xA;./service.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg&#34; width=&#34;90%&#34; height=&#34;90%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Face recognition, facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Default service endpoints will be &lt;code&gt;http://localhost:5000/verify&lt;/code&gt; for face recognition, &lt;code&gt;http://localhost:5000/analyze&lt;/code&gt; for facial attribute analysis, and &lt;code&gt;http://localhost:5000/represent&lt;/code&gt; for vector representation. You can pass input images as exact image paths on your environment, base64 encoded strings or images on web. &lt;a href=&#34;https://github.com/serengil/deepface/tree/master/api&#34;&gt;Here&lt;/a&gt;, you can find a postman project to find out how these methods should be called.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dockerized Service&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can deploy the deepface api on a kubernetes cluster with docker. The following &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/scripts/dockerize.sh&#34;&gt;shell script&lt;/a&gt; will serve deepface on &lt;code&gt;localhost:5000&lt;/code&gt;. You need to re-configure the &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; if you want to change the port. Then, even if you do not have a development environment, you will be able to consume deepface services such as verify and analyze. You can also access the inside of the docker image to run deepface related commands. Please follow the instructions in the &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/scripts/dockerize.sh&#34;&gt;shell script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd scripts&#xA;./dockerize.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-dockerized-v2.jpg&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;DeepFace comes with a command line interface as well. You are able to access its functions in command line as shown below. The command deepface expects the function name as 1st argument and function arguments thereafter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#face verification&#xA;$ deepface verify -img1_path tests/dataset/img1.jpg -img2_path tests/dataset/img2.jpg&#xA;&#xA;#facial analysis&#xA;$ deepface analyze -img_path tests/dataset/img1.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run these commands if you are running deepface with docker. Please follow the instructions in the &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/scripts/dockerize.sh#L17&#34;&gt;shell script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution &lt;a href=&#34;https://github.com/serengil/deepface/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are more than welcome! You should run the unit tests locally by running &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/tests/unit_tests.py&#34;&gt;&lt;code&gt;test/unit_tests.py&lt;/code&gt;&lt;/a&gt; before creating a PR. Once a PR sent, GitHub test workflow will be run automatically and unit test results will be available in &lt;a href=&#34;https://github.com/serengil/deepface/actions&#34;&gt;GitHub actions&lt;/a&gt; before approval. Besides, workflow will evaluate the code with pylint as well.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to support a project - starring⭐️ the GitHub repo is just one 🙏&lt;/p&gt; &#xA;&lt;p&gt;You can also support this work on &lt;a href=&#34;https://www.patreon.com/serengil?repo=deepface&#34;&gt;Patreon&lt;/a&gt; or &lt;a href=&#34;https://github.com/sponsors/serengil&#34;&gt;GitHub Sponsors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.patreon.com/serengil?repo=deepface&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png&#34; width=&#34;30%&#34; height=&#34;30%&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite deepface in your publications if it helps your research. Here are its BibTex entries:&lt;/p&gt; &#xA;&lt;p&gt;If you use deepface for facial recogntion purposes, please cite the this publication.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@inproceedings{serengil2020lightface,&#xA;  title        = {LightFace: A Hybrid Deep Face Recognition Framework},&#xA;  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},&#xA;  booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},&#xA;  pages        = {23-27},&#xA;  year         = {2020},&#xA;  doi          = {10.1109/ASYU50717.2020.9259802},&#xA;  url          = {https://doi.org/10.1109/ASYU50717.2020.9259802},&#xA;  organization = {IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use deepface for facial attribute analysis purposes such as age, gender, emotion or ethnicity prediction or face detection purposes, please cite the this publication.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@inproceedings{serengil2021lightface,&#xA;  title        = {HyperExtended LightFace: A Facial Attribute Analysis Framework},&#xA;  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},&#xA;  booktitle    = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},&#xA;  pages        = {1-4},&#xA;  year         = {2021},&#xA;  doi          = {10.1109/ICEET53442.2021.9659697},&#xA;  url          = {https://doi.org/10.1109/ICEET53442.2021.9659697},&#xA;  organization = {IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, if you use deepface in your GitHub projects, please add &lt;code&gt;deepface&lt;/code&gt; in the &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;Deepface is licensed under the MIT License - see &lt;a href=&#34;https://github.com/serengil/deepface/raw/master/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt;</summary>
  </entry>
</feed>