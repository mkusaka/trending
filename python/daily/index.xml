<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-08T01:34:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>joeyism/linkedin_scraper</title>
    <updated>2025-07-08T01:34:37Z</updated>
    <id>tag:github.com,2025-07-08:/joeyism/linkedin_scraper</id>
    <link href="https://github.com/joeyism/linkedin_scraper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that scrapes Linkedin for user data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Linkedin Scraper&lt;/h1&gt; &#xA;&lt;p&gt;Scrapes Linkedin User Data&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#linkedin-scraper&#34;&gt;Linkedin Scraper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#sample-usage&#34;&gt;Sample Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#user-scraping&#34;&gt;User Scraping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#company-scraping&#34;&gt;Company Scraping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#job-scraping&#34;&gt;Job Scraping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#job-search-scraping&#34;&gt;Job Search Scraping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#scraping-sites-where-login-is-required-first&#34;&gt;Scraping sites where login is required first&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#scraping-sites-and-login-automatically&#34;&gt;Scraping sites and login automatically&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#api&#34;&gt;API&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#person&#34;&gt;Person&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#linkedin_url&#34;&gt;&lt;code&gt;linkedin_url&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#name&#34;&gt;&lt;code&gt;name&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#about&#34;&gt;&lt;code&gt;about&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#experiences&#34;&gt;&lt;code&gt;experiences&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#educations&#34;&gt;&lt;code&gt;educations&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#interests&#34;&gt;&lt;code&gt;interests&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#accomplishment&#34;&gt;&lt;code&gt;accomplishment&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#company&#34;&gt;&lt;code&gt;company&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#job_title&#34;&gt;&lt;code&gt;job_title&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#driver&#34;&gt;&lt;code&gt;driver&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#scrape&#34;&gt;&lt;code&gt;scrape&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#scrapeclose_on_completetrue&#34;&gt;&lt;code&gt;scrape(close_on_complete=True)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#company&#34;&gt;Company&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#linkedin_url-1&#34;&gt;&lt;code&gt;linkedin_url&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#name-1&#34;&gt;&lt;code&gt;name&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#about_us&#34;&gt;&lt;code&gt;about_us&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#website&#34;&gt;&lt;code&gt;website&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#headquarters&#34;&gt;&lt;code&gt;headquarters&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#founded&#34;&gt;&lt;code&gt;founded&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#company_type&#34;&gt;&lt;code&gt;company_type&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#company_size&#34;&gt;&lt;code&gt;company_size&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#specialties&#34;&gt;&lt;code&gt;specialties&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#showcase_pages&#34;&gt;&lt;code&gt;showcase_pages&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#affiliated_companies&#34;&gt;&lt;code&gt;affiliated_companies&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#driver-1&#34;&gt;&lt;code&gt;driver&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#get_employees&#34;&gt;&lt;code&gt;get_employees&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#scrapeclose_on_completetrue-1&#34;&gt;&lt;code&gt;scrape(close_on_complete=True)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joeyism/linkedin_scraper/master/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install --user linkedin_scraper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Version &lt;strong&gt;2.0.0&lt;/strong&gt; and before is called &lt;code&gt;linkedin_user_scraper&lt;/code&gt; and can be installed via &lt;code&gt;pip3 install --user linkedin_user_scraper&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;First, you must set your chromedriver location by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CHROMEDRIVER=~/chromedriver&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sponsor&lt;/h2&gt; &#xA;&lt;p&gt;Message me if you&#39;d like to sponsor me&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use it, just create the class.&lt;/p&gt; &#xA;&lt;h3&gt;Sample Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Person, actions&#xA;from selenium import webdriver&#xA;driver = webdriver.Chrome()&#xA;&#xA;email = &#34;some-email@email.address&#34;&#xA;password = &#34;password123&#34;&#xA;actions.login(driver, email, password) # if email and password isnt given, it&#39;ll prompt in terminal&#xA;person = Person(&#34;https://www.linkedin.com/in/joey-sham-aa2a50122&#34;, driver=driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The account used to log-in should have it&#39;s language set English to make sure everything works as expected.&lt;/p&gt; &#xA;&lt;h3&gt;User Scraping&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Person&#xA;person = Person(&#34;https://www.linkedin.com/in/andre-iguodala-65b48ab5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Company Scraping&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Company&#xA;company = Company(&#34;https://ca.linkedin.com/company/google&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Job Scraping&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Job, actions&#xA;from selenium import webdriver&#xA;&#xA;driver = webdriver.Chrome()&#xA;email = &#34;some-email@email.address&#34;&#xA;password = &#34;password123&#34;&#xA;actions.login(driver, email, password) # if email and password isnt given, it&#39;ll prompt in terminal&#xA;input(&#34;Press Enter&#34;)&#xA;job = Job(&#34;https://www.linkedin.com/jobs/collections/recommended/?currentJobId=3456898261&#34;, driver=driver, close_on_complete=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Job Search Scraping&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import JobSearch, actions&#xA;from selenium import webdriver&#xA;&#xA;driver = webdriver.Chrome()&#xA;email = &#34;some-email@email.address&#34;&#xA;password = &#34;password123&#34;&#xA;actions.login(driver, email, password) # if email and password isnt given, it&#39;ll prompt in terminal&#xA;input(&#34;Press Enter&#34;)&#xA;job_search = JobSearch(driver=driver, close_on_complete=False, scrape=False)&#xA;# job_search contains jobs from your logged in front page:&#xA;# - job_search.recommended_jobs&#xA;# - job_search.still_hiring&#xA;# - job_search.more_jobs&#xA;&#xA;job_listings = job_search.search(&#34;Machine Learning Engineer&#34;) # returns the list of `Job` from the first page&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scraping sites where login is required first&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run &lt;code&gt;ipython&lt;/code&gt; or &lt;code&gt;python&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;ipython&lt;/code&gt;/&lt;code&gt;python&lt;/code&gt;, run the following code (you can modify it if you need to specify your driver)&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Person&#xA;from selenium import webdriver&#xA;driver = webdriver.Chrome()&#xA;person = Person(&#34;https://www.linkedin.com/in/andre-iguodala-65b48ab5&#34;, driver = driver, scrape=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Login to Linkedin&lt;/li&gt; &#xA; &lt;li&gt;[OPTIONAL] Logout of Linkedin&lt;/li&gt; &#xA; &lt;li&gt;In the same &lt;code&gt;ipython&lt;/code&gt;/&lt;code&gt;python&lt;/code&gt; code, run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;person.scrape()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The reason is that LinkedIn has recently blocked people from viewing certain profiles without having previously signed in. So by setting &lt;code&gt;scrape=False&lt;/code&gt;, it doesn&#39;t automatically scrape the profile, but Chrome will open the linkedin page anyways. You can login and logout, and the cookie will stay in the browser and it won&#39;t affect your profile views. Then when you run &lt;code&gt;person.scrape()&lt;/code&gt;, it&#39;ll scrape and close the browser. If you want to keep the browser on so you can scrape others, run it as&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: For version &amp;gt;= &lt;code&gt;2.1.0&lt;/code&gt;, scraping can also occur while logged in. Beware that users will be able to see that you viewed their profile.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;person.scrape(close_on_complete=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;so it doesn&#39;t close.&lt;/p&gt; &#xA;&lt;h3&gt;Scraping sites and login automatically&lt;/h3&gt; &#xA;&lt;p&gt;From verison &lt;strong&gt;2.4.0&lt;/strong&gt; on, &lt;code&gt;actions&lt;/code&gt; is a part of the library that allows signing into Linkedin first. The email and password can be provided as a variable into the function. If not provided, both will be prompted in terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linkedin_scraper import Person, actions&#xA;from selenium import webdriver&#xA;driver = webdriver.Chrome()&#xA;email = &#34;some-email@email.address&#34;&#xA;password = &#34;password123&#34;&#xA;actions.login(driver, email, password) # if email and password isnt given, it&#39;ll prompt in terminal&#xA;person = Person(&#34;https://www.linkedin.com/in/andre-iguodala-65b48ab5&#34;, driver=driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;h3&gt;Person&lt;/h3&gt; &#xA;&lt;p&gt;A Person object can be created with the following inputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Person(linkedin_url=None, name=None, about=[], experiences=[], educations=[], interests=[], accomplishments=[], company=None, job_title=None, driver=None, scrape=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;linkedin_url&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the linkedin url of their profile&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;name&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the name of the person&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;about&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the small paragraph about the person&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;experiences&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the past experiences they have. A list of &lt;code&gt;linkedin_scraper.scraper.Experience&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;educations&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the past educations they have. A list of &lt;code&gt;linkedin_scraper.scraper.Education&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;interests&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the interests they have. A list of &lt;code&gt;linkedin_scraper.scraper.Interest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;accomplishment&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the accomplishments they have. A list of &lt;code&gt;linkedin_scraper.scraper.Accomplishment&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;company&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This the most recent company or institution they have worked at.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;job_title&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This the most recent job title they have.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;driver&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.&lt;/p&gt; &#xA;&lt;p&gt;For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver = webdriver.Chrome()&#xA;person = Person(&#34;https://www.linkedin.com/in/andre-iguodala-65b48ab5&#34;, driver = driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;scrape&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;When this is &lt;strong&gt;True&lt;/strong&gt;, the scraping happens automatically. To scrape afterwards, that can be run by the &lt;code&gt;scrape()&lt;/code&gt; function from the &lt;code&gt;Person&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;scrape(close_on_complete=True)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the meat of the code, where execution of this function scrapes the profile. If &lt;em&gt;close_on_complete&lt;/em&gt; is True (which it is by default), then the browser will close upon completion. If scraping of other profiles are desired, then you might want to set that to false so you can keep using the same driver.&lt;/p&gt; &#xA;&lt;h3&gt;Company&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Company(linkedin_url=None, name=None, about_us=None, website=None, phone=None, headquarters=None, founded=None, company_type=None, company_size=None, specialties=None, showcase_pages=[], affiliated_companies=[], driver=None, scrape=True, get_employees=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;linkedin_url&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the linkedin url of their profile&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;name&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the name of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;about_us&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The description of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;website&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The website of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;phone&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The phone of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;headquarters&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The headquarters location of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;founded&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;When the company was founded&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;company_type&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The type of the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;company_size&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;How many people are employeed at the company&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;specialties&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;What the company specializes in&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;showcase_pages&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Pages that the company owns to showcase their products&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;affiliated_companies&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Other companies that are affiliated with this one&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;driver&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;get_employees&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Whether to get all the employees of company&lt;/p&gt; &#xA;&lt;p&gt;For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver = webdriver.Chrome()&#xA;company = Company(&#34;https://ca.linkedin.com/company/google&#34;, driver=driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;scrape(close_on_complete=True)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is the meat of the code, where execution of this function scrapes the company. If &lt;em&gt;close_on_complete&lt;/em&gt; is True (which it is by default), then the browser will close upon completion. If scraping of other companies are desired, then you might want to set that to false so you can keep using the same driver.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/joeyism&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Alibaba-NLP/WebAgent</title>
    <updated>2025-07-08T01:34:37Z</updated>
    <id>tag:github.com,2025-07-08:/Alibaba-NLP/WebAgent</id>
    <link href="https://github.com/Alibaba-NLP/WebAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker &amp; WebDancer &amp; WebSailor https://arxiv.org/pdf/2507.02592&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;WebAgent for Information Seeking bulit by Tongyi Lab, Alibaba Group &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;30px&#34; style=&#34;display:inline;&#34;&gt;&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/14217&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/14217&#34; alt=&#34;Alibaba-NLP%2FWebAgent | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebSailor&#34; target=&#34;_blank&#34;&gt;WebSailor&lt;/a&gt; ÔΩú ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34; target=&#34;_blank&#34;&gt;WebDancer-QwQ-32B&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/datasets/callanwu/WebWalkerQA&#34; target=&#34;_blank&#34;&gt;WebWalkerQA&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/roadmap.png&#34; width=&#34;100%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can check the paper of &lt;a href=&#34;https://arxiv.org/pdf/2505.22648&#34;&gt;WebDancer&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2501.07572&#34;&gt;WebWalker&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2507.02592&#34;&gt;WebSailor&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí• üí• üí• Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor&#34;&gt;&lt;strong&gt;WebSailor&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;strong&gt;WebDancer&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebWalker&#34;&gt;&lt;strong&gt;WebWalker&lt;/strong&gt;&lt;/a&gt; (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∞ News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.07.03&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebSailor&lt;/strong&gt;, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. &lt;strong&gt;WebSailor&lt;/strong&gt; topped the HuggingFace &lt;a href=&#34;https://huggingface.co/papers/2507.02592&#34;&gt;daily papers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.06.23&lt;/code&gt; üî•üî•üî•The model, interactive demo, and some of the data of &lt;strong&gt;WebDancer&lt;/strong&gt; have been open-sourced. You&#39;re welcome to try them out!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.29&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebDancer&lt;/strong&gt;, a native agentic search model towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.15&lt;/code&gt; &lt;strong&gt;WebWalker&lt;/strong&gt; is accepted by ACL 2025 main conference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.01.14&lt;/code&gt; We release &lt;strong&gt;WebWalker&lt;/strong&gt;, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíé Results Showcase&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/result.png&#34; width=&#34;800%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚õµÔ∏è Features for WebSailor&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.&lt;/li&gt; &#xA; &lt;li&gt;Introduces &lt;strong&gt;SailorFog-QA&lt;/strong&gt;, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor/dataset/sailorfog-QA.jsonl&#34;&gt;&lt;code&gt;WebSailor/dataset/sailorfog-QA.jsonl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by &lt;strong&gt;Duplicating Sampling Policy Optimization (DUPO)&lt;/strong&gt;, an efficient agentic RL algorithm excelling in effectiveness and efficiency.&lt;/li&gt; &#xA; &lt;li&gt;WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of &lt;strong&gt;12.0%&lt;/strong&gt; on BrowseComp-en, &lt;strong&gt;30.1%&lt;/strong&gt; on BrowseComp-zh, and &lt;strong&gt;55.4%&lt;/strong&gt; on GAIA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The checkpoint is coming soon.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåê Features for WebDancer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;We introduce a four-stage training paradigm comprising &lt;strong&gt;browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization&lt;/strong&gt;, enabling the agent to autonomously acquire autonomous search and reasoning skills.&lt;/li&gt; &#xA; &lt;li&gt;Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for &lt;strong&gt;training agentic systems&lt;/strong&gt; via SFT or RL.&lt;/li&gt; &#xA; &lt;li&gt;WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You need to enter the &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;code&gt;WebDancer&lt;/code&gt;&lt;/a&gt; folder for the following commands.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Set Up the Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n webdancer python=3.12&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Deploy the Model&lt;/h3&gt; &#xA;&lt;p&gt;Download the WebDancer model from &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34;&gt;ü§ó HuggingFace&lt;/a&gt; and deploy it using the provided scripts with &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash deploy_model.sh WebDancer_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Replace &lt;code&gt;WebDancer_PATH&lt;/code&gt; with the actual path to the downloaded model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 2: Run the Demo&lt;/h3&gt; &#xA;&lt;p&gt;Edit the following keys in &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer/scripts/run_demo.sh&#34;&gt;&lt;code&gt;WebDancer/scripts/run_demo.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GOOGLE_SEARCH_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://serpapi.com/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://jina.ai/api-dashboard/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://dashscope.aliyun.com/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, launch the demo with Gradio to interact with the WebDancer model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üé• WebSailor Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-en&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-zh&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé• WebDancer Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for WebWalkerQA, GAIA and Daily Use. Our model can execute the long-horizon tasks with &lt;strong&gt;multiple steps&lt;/strong&gt; and &lt;strong&gt;complex reasoning&lt;/strong&gt;, such as web traversal, information seeking and question answering.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;WebWalkerQA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;GAIA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìÉ License&lt;/h2&gt; &#xA;&lt;p&gt;The content of this project itself is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üö© Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bigquery&#34;&gt;@misc{li2025websailor,&#xA;      title={WebSailor: Navigating Super-human Reasoning for Web Agent},&#xA;      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2507.02592},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2507.02592},&#xA;}&#xA;@misc{wu2025webdancer,&#xA;      title={WebDancer: Towards Autonomous Information Seeking Agency},&#xA;      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2505.22648},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2505.22648},&#xA;}&#xA;@misc{wu2025webwalker,&#xA;      title={WebWalker: Benchmarking LLMs in Web Traversal},&#xA;      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},&#xA;      year={2025},&#xA;      eprint={2501.07572},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2501.07572},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The repo is contributed by &lt;a href=&#34;https://callanwu.github.io/&#34;&gt;Jialong Wu&lt;/a&gt;. If you have any questions, please feel free to contact via &lt;a href=&#34;mailto:wujialongml@gmail.com&#34;&gt;wujialongml@gmail.com&lt;/a&gt; or create an issue.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Misc&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; &#xA;&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; &#xA;&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=&#34;&#34;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>