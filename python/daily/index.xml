<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-08T01:41:34Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>roboflow/maestro</title>
    <updated>2025-02-08T01:41:34Z</updated>
    <id>tag:github.com,2025-02-08:/roboflow/maestro</id>
    <link href="https://github.com/roboflow/maestro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;streamline the fine-tuning process for multimodal models: PaliGemma, Florence-2, and Qwen2-VL&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;maestro&lt;/h1&gt; &#xA; &lt;h3&gt;VLM fine-tuning for everyone&lt;/h3&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/c9416f1f-a2bf-4590-86da-d2fc89ba559b&#34; width=&#34;80&#34; height=&#34;40&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/75dc7214-e82a-498d-950e-c64d90218e49&#34; width=&#34;80&#34; height=&#34;40&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/5d265473-b938-4501-b894-6a44a6a28a8c&#34; width=&#34;80&#34; height=&#34;40&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/b7ccdf39-ac77-4dbd-8608-0fa2d9dadf0a&#34; width=&#34;80&#34; height=&#34;40&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/maestro&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/maestro.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/maestro/blob/develop/cookbooks/maestro_qwen2_5_vl_json_extraction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;maestro&lt;/strong&gt; is a streamlined tool to accelerate the fine-tuning of multimodal models. By encapsulating best practices from our core modules, maestro handles configuration, data loading, reproducibility, and training loop setup. It currently offers ready-to-use recipes for popular vision-language models such as &lt;strong&gt;Florence-2&lt;/strong&gt;, &lt;strong&gt;PaliGemma 2&lt;/strong&gt;, and &lt;strong&gt;Qwen2.5-VL&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/3bb9ccba-b0ee-4964-bcd6-f71124a08bc2&#34; alt=&#34;maestro&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;p&gt;To begin, install the model-specific dependencies. Since some models may have clashing requirements, we recommend creating a dedicated Python environment for each model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;maestro[paligemma_2]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;Kick off fine-tuning with our command-line interface, which leverages the configuration and training routines defined in each modelâ€™s core module. Simply specify key parameters such as the dataset location, number of epochs, batch size, optimization strategy, and metrics.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;maestro paligemma_2 train \&#xA;  --dataset &#34;dataset/location&#34; \&#xA;  --epochs 10 \&#xA;  --batch-size 4 \&#xA;  --optimization_strategy &#34;qlora&#34; \&#xA;  --metrics &#34;edit_distance&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;For greater control, use the Python API to fine-tune your models. Import the train function from the corresponding module and define your configuration in a dictionary. The core modules take care of reproducibility, data preparation, and training setup.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from maestro.trainer.models.paligemma_2.core import train&#xA;&#xA;config = {&#xA;    &#34;dataset&#34;: &#34;dataset/location&#34;,&#xA;    &#34;epochs&#34;: 10,&#xA;    &#34;batch_size&#34;: 4,&#xA;    &#34;optimization_strategy&#34;: &#34;qlora&#34;,&#xA;    &#34;metrics&#34;: [&#34;edit_distance&#34;]&#xA;}&#xA;&#xA;train(config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cookbooks&lt;/h2&gt; &#xA;&lt;p&gt;Looking for a place to start? Try our cookbooks to learn how to fine-tune different VLMs on various vision tasks with &lt;strong&gt;maestro&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;open in colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Finetune Florence-2 for object detection with LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow/maestro/blob/develop/cookbooks/maestro_florence_2_object_detection.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Finetune PaliGemma 2 for JSON data extraction with LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow/maestro/blob/develop/cookbooks/maestro_paligemma_2_json_extraction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Finetune Qwen2.5-VL for JSON data extraction with QLoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow/maestro/blob/develop/cookbooks/maestro_qwen2_5_vl_json_extraction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We would love your help in making this repository even better! We are especially looking for contributors with experience in fine-tuning vision-language models (VLMs). If you notice any bugs or have suggestions for improvement, feel free to open an &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro/issues&#34;&gt;issue&lt;/a&gt; or submit a &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro/pulls&#34;&gt;pull request&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>