<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-05T01:40:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jianchang512/pyvideotrans</title>
    <updated>2023-11-05T01:40:04Z</updated>
    <id>tag:github.com,2023-11-05:/jianchang512/pyvideotrans</id>
    <link href="https://github.com/jianchang512/pyvideotrans" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Translate the video from one language to another and add dubbing. 将视频从一种语言翻译为另一种语言，并添加配音&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/pyvideotrans/main/README_ENG.md&#34;&gt;English&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;这是一个视频翻译工具，可将一种语言的视频翻译为另一种语言和配音的视频。 语音识别基于 &lt;code&gt;openai-whisper&lt;/code&gt; 离线模型、文字翻译使用&lt;code&gt;google&lt;/code&gt;翻译接口，文字合成语音使用 &lt;code&gt;Microsoft Edge tts&lt;/code&gt;，背景音乐去除使用 &lt;code&gt;Spleeter&lt;/code&gt;,无需购买任何商业接口，也无需付费&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/27af2313-1717-43c2-9853-6d4b1d3ecdf4&#34;&gt;https://github.com/jianchang512/pyvideotrans/assets/3378335/27af2313-1717-43c2-9853-6d4b1d3ecdf4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/753d76c0-e9b9-44fd-9b53-477986aea5e6&#34;&gt;https://github.com/jianchang512/pyvideotrans/assets/3378335/753d76c0-e9b9-44fd-9b53-477986aea5e6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;使用预编译版本方法&lt;/h1&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;只可用于 win10 win11 系统 (编译版非最新，建议源码部署)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;从 release 中下载最新版，解压，双击 sp.exe&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;原始视频目录：选择mp4视频；&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;输出视频目录：如果不选择，则默认生成在同目录下的 &lt;code&gt;_video_out&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;网络代理地址：如果你所在地区无法直接访问 google，需要在软件界面 网络代理 中设置代理，比如若使用 v2ray ，则填写 &lt;code&gt;http://127.0.0.1:10809&lt;/code&gt;,若clash，则填写 &lt;code&gt;http://127.0.0.1:7890&lt;/code&gt;. 如果你修改了默认端口或使用的其他代理软件，则按需填写&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;视频原始语言：选择待翻译视频里的语言种类&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;翻译目标语言：选择希望翻译到的语言种类&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;选择配音：选择翻译目标语言后，可从配音选项中，选择配音角色；&lt;/p&gt; &lt;p&gt;硬字幕: 是指始终显示字幕，不可隐藏，如果希望网页中播放时也有字幕，请选择硬字幕嵌入&lt;/p&gt; &lt;p&gt;软字幕: 如果播放器支持字幕管理，可显示或者隐藏字幕，该方式网页中播放时不会显示字幕，某些国产播放器可能不支持,需要将生成的视频同名srt文件和视频放在一个目录下才会显示&lt;/p&gt; &lt;p&gt;&lt;strong&gt;不能“既不嵌入字幕又不选择配音角色”&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;文字识别模型: 选择 base/small/medium/large, 识别效果越来越好，但识别速度越来越慢，第一次将需要下载模型，默认 base,可以预先单独下载模型后，放到 &lt;code&gt;当前软件目录/models&lt;/code&gt;目录下.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;模型单独下载地址&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt&#34;&gt;tiny模型&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt&#34;&gt;base模型&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt&#34;&gt;small模型&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt&#34;&gt;medium模型&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large.pt&#34;&gt;large模型&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;配音语速：填写 -90到+90 之间的数字，同样一句话在不同语言语音下，所需时间是不同的，因此配音后可能声画字幕不同步，可以调整此处语速，负数代表降速，正数代表加速播放。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;自动加速: 如果翻译后的语音时长大于原时长，并且这里确认选中，那么将强制加速播放该片段，以缩小时长&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;去除背景音：若选中可尝试删掉背景音乐，以使结果更准确&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;静音片段: 填写100到2000的数字，代表毫秒，默认 500，即以大于等于 500ms 的静音片段为区间分割语音&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;点击 开始按钮 底部会显示当前进度和日志，右侧文本框内显示字幕&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;原始视频统一使用mp4格式，处理速度快，网络兼容性好&lt;/p&gt; &#xA; &lt;p&gt;采用软合成字幕：字幕作为单独文件嵌入视频，可再次提取出，如果播放器支持，可在播放器字幕管理中启用或禁用字幕；&lt;/p&gt; &#xA; &lt;p&gt;默认会在 原始视频目录 下生成同名的字幕文件 视频名.srt&lt;/p&gt; &#xA; &lt;p&gt;对于无法识别的语音将直接复制原语音&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;源码部署&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;配置好 python 3.9+ 环境&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/jianchang512/pyvideotrans&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd pyvideotrans&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;解压 ffmpeg.zip 到根目录下 (ffmpeg.exe文件)&lt;/li&gt; &#xA; &lt;li&gt;解压 pretrained_models.zip 在根目录下(Spleeter模型文件)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python sp.py&lt;/code&gt; 打开软件界面, &lt;code&gt;python cli.py&lt;/code&gt; 命令行执行&lt;/li&gt; &#xA; &lt;li&gt;如果使用去除背景音功能，第一次需要下载模型，会比较耗时。你可以解压pretrained_models.zip 到当前项目根下&lt;/li&gt; &#xA; &lt;li&gt;如果希望打包为exe的话，请使用命令 &lt;code&gt;pyinstaller sp.py&lt;/code&gt;,不要添加 &lt;code&gt;-w -F&lt;/code&gt; 参数，否则可能闪退(tensorflow缘故)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;CLI 命令行方式使用&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;按照上述源码部署方式部署好后，执行 &lt;code&gt;python cli.py&lt;/code&gt;，可在命令行下执行&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;支持的参数&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;--source_mp4&lt;/strong&gt;： 【必填】待翻译视频路径，以.mp4结尾&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--target_dir&lt;/strong&gt;： 翻译后视频存放位置，默认存放源视频目录下的 _video_out 文件夹&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--source_language&lt;/strong&gt;：视频语言代码,默认&lt;code&gt;en&lt;/code&gt; ( zh-cn | zh-tw | en | fr | de | ja | ko | ru | es | th | it | pt | vi | ar )&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--target_language&lt;/strong&gt;：目标语言代码,默认&lt;code&gt;zh-cn&lt;/code&gt; ( zh-cn | zh-tw | en | fr | de | ja | ko | ru | es | th | it | pt | vi | ar )&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;zh-cn: Simplified_Chinese&#xA;zh-tw: Traditional_Chinese&#xA;en: English&#xA;fr: French&#xA;de: German&#xA;ja: Japanese&#xA;ko: Korean&#xA;ru: Russian&#xA;es: Spanish&#xA;th: Thai&#xA;it: Italian&#xA;pt: Portuguese&#xA;vi: Vietnamese&#xA;ar: Arabic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;--proxy&lt;/strong&gt;：填写 http 代理地址，默认 None,如果所在地区无法访问google，需要填写，例如: &lt;code&gt;http://127.0.0.1:10809&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--subtitle_type&lt;/strong&gt;：1 嵌入硬字幕，2 嵌入软字幕。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;硬字幕: 是指始终显示字幕，不可隐藏，如果希望网页中播放时也有字幕，请选择硬字幕嵌入&#xA;&#xA;软字幕: 如果播放器支持字幕管理，可显示或者隐藏字幕，该方式网页中播放时不会显示字幕，某些国产播放器可能不支持&#xA;&#xA;**该参数和 --voice_role 必须至少设置其中一个,也就是不能“既不嵌入字幕又不选择配音角色**&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;--voice_role&lt;/strong&gt;：根据所选目标语言代码，填写对应的角色名，注意角色名的前2个字母需要和目标语言代码的前2个字母一致，如果不知道该怎么填写，执行&lt;code&gt;python cli.py show_vioce&lt;/code&gt; 将显示每种语言对应可用的角色名称&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;zh: zh-HK-HiuGaaiNeural, zh-HK-HiuMaanNeural, zh-HK-WanLungNeural, zh-CN-XiaoxiaoNeural, zh-CN-XiaoyiNeural, zh-CN-YunjianNeural, zh-CN-YunxiNeural&#xA;, zh-CN-YunxiaNeural, zh-CN-YunyangNeural, zh-CN-liaoning-XiaobeiNeural, zh-TW-HsiaoChenNeural, zh-TW-YunJheNeural, zh-TW-HsiaoYuNeural, zh-CN-shaa&#xA;nxi-XiaoniNeural&#xA;en: en-AU-NatashaNeural, en-AU-WilliamNeural, en-CA-ClaraNeural, en-CA-LiamNeural, en-HK-SamNeural, en-HK-YanNeural, en-IN-NeerjaExpressiveNeural,&#xA;en-IN-NeerjaNeural, en-IN-PrabhatNeural, en-IE-ConnorNeural, en-IE-EmilyNeural, en-KE-AsiliaNeural, en-KE-ChilembaNeural, en-NZ-MitchellNeural, en-&#xA;NZ-MollyNeural, en-NG-AbeoNeural, en-NG-EzinneNeural, en-PH-JamesNeural, en-PH-RosaNeural, en-SG-LunaNeural, en-SG-WayneNeural, en-ZA-LeahNeural, e&#xA;n-ZA-LukeNeural, en-TZ-ElimuNeural, en-TZ-ImaniNeural, en-GB-LibbyNeural, en-GB-MaisieNeural, en-GB-RyanNeural, en-GB-SoniaNeural, en-GB-ThomasNeur&#xA;al, en-US-AriaNeural, en-US-AnaNeural, en-US-ChristopherNeural, en-US-EricNeural, en-US-GuyNeural, en-US-JennyNeural, en-US-MichelleNeural, en-US-R&#xA;ogerNeural, en-US-SteffanNeural&#xA;fr: fr-BE-CharlineNeural, fr-BE-GerardNeural, fr-CA-AntoineNeural, fr-CA-JeanNeural, fr-CA-SylvieNeural, fr-FR-DeniseNeural, fr-FR-EloiseNeural, fr&#xA;-FR-HenriNeural, fr-CH-ArianeNeural, fr-CH-FabriceNeural&#xA;de: de-AT-IngridNeural, de-AT-JonasNeural, de-DE-AmalaNeural, de-DE-ConradNeural, de-DE-KatjaNeural, de-DE-KillianNeural, de-CH-JanNeural, de-CH-Le&#xA;niNeural    &#xA;ja: ja-JP-KeitaNeural, ja-JP-NanamiNeural&#xA;ko: ko-KR-InJoonNeural, ko-KR-SunHiNeural    &#xA;ru: ru-RU-DmitryNeural, ru-RU-SvetlanaNeural&#xA;es: es-AR-ElenaNeural, es-AR-TomasNeural, es-BO-MarceloNeural, es-BO-SofiaNeural, es-CL-CatalinaNeural, es-CL-LorenzoNeural, es-CO-GonzaloNeural, e&#xA;s-CO-SalomeNeural, es-CR-JuanNeural, es-CR-MariaNeural, es-CU-BelkysNeural, es-CU-ManuelNeural, es-DO-EmilioNeural, es-DO-RamonaNeural, es-EC-Andre&#xA;aNeural, es-EC-LuisNeural, es-SV-LorenaNeural, es-SV-RodrigoNeural, es-GQ-JavierNeural, es-GQ-TeresaNeural, es-GT-AndresNeural, es-GT-MartaNeural,&#xA;es-HN-CarlosNeural, es-HN-KarlaNeural, es-MX-DaliaNeural, es-MX-JorgeNeural, es-NI-FedericoNeural, es-NI-YolandaNeural, es-PA-MargaritaNeural, es-P&#xA;A-RobertoNeural, es-PY-MarioNeural, es-PY-TaniaNeural, es-PE-AlexNeural, es-PE-CamilaNeural, es-PR-KarinaNeural, es-PR-VictorNeural, es-ES-AlvaroNe&#xA;ural, es-ES-ElviraNeural, es-US-AlonsoNeural, es-US-PalomaNeural, es-UY-MateoNeural, es-UY-ValentinaNeural, es-VE-PaolaNeural, es-VE-SebastianNeura&#xA;l&#xA;th: th-TH-NiwatNeural, th-TH-PremwadeeNeural&#xA;it: it-IT-DiegoNeural, it-IT-ElsaNeural, it-IT-IsabellaNeural&#xA;pt: pt-BR-AntonioNeural, pt-BR-FranciscaNeural, pt-PT-DuarteNeural, pt-PT-RaquelNeural&#xA;vi: vi-VN-HoaiMyNeural, vi-VN-NamMinhNeural&#xA;ar: ar-DZ-AminaNeural, ar-DZ-IsmaelNeural, ar-BH-AliNeural, ar-BH-LailaNeural, ar-EG-SalmaNeural, ar-EG-ShakirNeural, ar-IQ-BasselNeural, ar-IQ-Ran&#xA;aNeural, ar-JO-SanaNeural, ar-JO-TaimNeural, ar-KW-FahedNeural, ar-KW-NouraNeural, ar-LB-LaylaNeural, ar-LB-RamiNeural, ar-LY-ImanNeural, ar-LY-Oma&#xA;rNeural, ar-MA-JamalNeural, ar-MA-MounaNeural, ar-OM-AbdullahNeural, ar-OM-AyshaNeural, ar-QA-AmalNeural, ar-QA-MoazNeural, ar-SA-HamedNeural, ar-S&#xA;A-ZariyahNeural, ar-SY-AmanyNeural, ar-SY-LaithNeural, ar-TN-HediNeural, ar-TN-ReemNeural, ar-AE-FatimaNeural, ar-AE-HamdanNeural, ar-YE-MaryamNeural, ar-YE-SalehNeural&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;--voice_rate&lt;/strong&gt;：负数降低配音语速，正数加快配音语速，默认&lt;code&gt;0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--voice_silence&lt;/strong&gt;: 输入100-2000之间的数字，表示静音段的最小毫秒，默认为 500。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--voice_autorate&lt;/strong&gt;: 如果翻译后的音频时长超过原时长，是否强制加速播放翻译后的音频，以便对齐时长&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--whisper_model&lt;/strong&gt;: 默认为base，可选 base / small / medium / large，效果越来好，速度越来越慢。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--remove_background&lt;/strong&gt;：是否移除背景音，如果传入该参数即代表去除背景音&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;cli示例&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python cli.py --source_mp4 &#34;D:/video/ex.mp4&#34; --source_language en --target_language zh-cn --proxy &#34;http://127.0.0.1:10809&#34; --voice_replace zh-CN-XiaoxiaoNeural&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;上述意思是，将源语言为英文的 D:/video/ex.mp4 视频，翻译为中文视频，设置代理 &lt;a href=&#34;http://127.0.0.1:10809&#34;&gt;http://127.0.0.1:10809&lt;/a&gt; 使用配音角色为 zh-CN-XiaoxiaoNeural&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python cli.py --source_mp4 &#34;D:/video/ex.mp4&#34; --source_language zh-cn --target_language en --proxy &#34;http://127.0.0.1&#34;1080 9&#34; --voice_replace en-US-AriaNeural --voice_autorate --whisper_model small&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;上述意思是，将源语言为中文的 D:/video/ex.mp4 视频，翻译为英文视频，设置代理 &lt;a href=&#34;http://127.0.0.1:10809&#34;&gt;http://127.0.0.1:10809&lt;/a&gt; 使用配音角色为 en-US-AriaNeural，如果翻译后的语音时长大于原语音，则自动加速，文字识别模型采用 small 模型&lt;/p&gt; &#xA;&lt;h1&gt;软件预览截图&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jianchang512/pyvideotrans/main/images/p1.png?a&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/jianchang512/pyvideotrans/main/images/cli.png?b&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;视频前后对比&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.wonyes.org/demo.html&#34;&gt;Demo 原视频和翻译后视频&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/-WAyWjJPSEk&#34;&gt;Youtube demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;可能的问题&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;翻译使用 requests 请求 google api，然后提取，过于频繁可能会被限制。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;致谢&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;本程序依赖这些开源项目&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;pydub&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg&lt;/li&gt; &#xA; &lt;li&gt;PyQt5&lt;/li&gt; &#xA; &lt;li&gt;SpeechRecognition&lt;/li&gt; &#xA; &lt;li&gt;edge-tts&lt;/li&gt; &#xA; &lt;li&gt;Spleeter&lt;/li&gt; &#xA; &lt;li&gt;openai-whisper&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>WilliamStar007/ClashX-V2Ray-TopFreeProxy</title>
    <updated>2023-11-05T01:40:04Z</updated>
    <id>tag:github.com,2023-11-05:/WilliamStar007/ClashX-V2Ray-TopFreeProxy</id>
    <link href="https://github.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Top free VPN (ClashX &amp; V2Ray proxy) with subscription links. [免费VPN、免费梯子、免费科学上网、免费订阅链接、免费节点、精选、ClashX &amp; V2Ray 教程]&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Dreamacro/clash/raw/master/docs/logo.png&#34; alt=&#34;Clash&#34; width=&#34;200&#34;&gt; &lt;br&gt; ClashX Setup Tutorial &lt;br&gt; &lt;/h1&gt; &#xA;&lt;!-- newly added! --&gt; &#xA;&lt;!-- ![Vistors](https://visitor-badge.laobi.icu/badge?page_id=WilliamStar007.ClashX-V2Ray-TopFreeProxy) --&gt; &#xA;&lt;!-- ![LICENSE](https://img.shields.io/badge/license-MIT-green.svg) --&gt; &#xA;&lt;p&gt;Tutorial for setting up a &lt;a href=&#34;https://github.com/yichengchen/clashX&#34;&gt;ClashX&lt;/a&gt; proxy with free subscription links.&lt;br&gt; Feel free to submit an &lt;a href=&#34;https://github.com/WilliamStar007/ClashX-TopFreeProxy/issues&#34;&gt;Issue&lt;/a&gt; or make a &lt;a href=&#34;https://github.com/WilliamStar007/ClashX-TopFreeProxy/pulls&#34;&gt;Pull Request&lt;/a&gt;!&lt;br&gt; For V2Ray users, see &lt;a href=&#34;https://github.com/WilliamStar007/ClashX-TopFreeProxy/raw/main/v2ray.md&#34;&gt;V2Ray Setup Tutorial&lt;/a&gt;.&lt;br&gt; For a tutorial in Chinese, see &lt;a href=&#34;https://github.com/WilliamStar007/ClashX-TopFreeProxy/raw/main/clash%E4%B8%AD%E6%96%87%E7%89%88.md&#34;&gt;中文版教程&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#subscription-links&#34;&gt;Subscription Links&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clash&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;Clash&lt;/strong&gt; from &lt;a href=&#34;https://github.com/Dreamacro/clash/releases&#34;&gt;Clash Release&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;Clash for Windows&lt;/strong&gt; from &lt;a href=&#34;https://github.com/Fndroid/clash_for_windows_pkg/releases&#34;&gt;CFW Release&lt;/a&gt; page.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ClashX ★&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;ClashX&lt;/strong&gt; from &lt;a href=&#34;https://github.com/yichengchen/clashX/releases&#34;&gt;ClashX Release&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;ClashX Pro&lt;/strong&gt; with enhanced mode and native Apple Silicon support at &lt;a href=&#34;https://install.appcenter.ms/users/clashx/apps/clashx-pro/distribution_groups/public&#34;&gt;AppCenter&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Subscription Links&lt;/h2&gt; &#xA;&lt;!-- **Modify the date (if any) in the links to the current date!!** --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NodeFree: &lt;a href=&#34;https://nodefree.org/dy/2023/11/20231104.yaml&#34;&gt;https://nodefree.org/dy/2023/11/20231104.yaml&lt;/a&gt; ★&lt;/li&gt; &#xA; &lt;li&gt;ClashNode: &lt;a href=&#34;https://clashnode.com/wp-content/uploads/2023/11/20231105.yaml&#34;&gt;https://clashnode.com/wp-content/uploads/2023/11/20231105.yaml&lt;/a&gt; ★&lt;/li&gt; &#xA; &lt;li&gt;Mfuu: &lt;a href=&#34;https://raw.githubusercontent.com/mfuu/v2ray/master/clash.yaml&#34;&gt;https://raw.githubusercontent.com/mfuu/v2ray/master/clash.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anaer: &lt;a href=&#34;https://raw.githubusercontent.com/anaer/Sub/main/clash.yaml&#34;&gt;https://raw.githubusercontent.com/anaer/Sub/main/clash.yaml&lt;/a&gt; ★&lt;/li&gt; &#xA; &lt;li&gt;Pojiezhiyuanjun: &lt;a href=&#34;https://raw.githubusercontent.com/pojiezhiyuanjun/2023/master/1103clash.yml&#34;&gt;https://raw.githubusercontent.com/pojiezhiyuanjun/2023/master/1103clash.yml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Oslook: &lt;a href=&#34;https://raw.githubusercontent.com/oslook/clash-freenode/main/clash.yaml&#34;&gt;https://raw.githubusercontent.com/oslook/clash-freenode/main/clash.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ermaozi: &lt;a href=&#34;https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml&#34;&gt;https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml&lt;/a&gt; ★&lt;/li&gt; &#xA; &lt;li&gt;Learnhard-cn: &lt;a href=&#34;https://cdn.jsdelivr.net/gh/learnhard-cn/free_proxy_ss@main/clash/clash.provider.yaml&#34;&gt;https://cdn.jsdelivr.net/gh/learnhard-cn/free_proxy_ss@main/clash/clash.provider.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tbbatbb: &lt;a href=&#34;https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/clash.config.yaml&#34;&gt;https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/clash.config.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vveg26: &lt;a href=&#34;https://raw.githubusercontent.com/vveg26/get_proxy/main/dist/clash.config.yaml&#34;&gt;https://raw.githubusercontent.com/vveg26/get_proxy/main/dist/clash.config.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenRunner: &lt;a href=&#34;https://raw.githubusercontent.com/openrunner/clash-freenode/main/clash.yaml&#34;&gt;https://raw.githubusercontent.com/openrunner/clash-freenode/main/clash.yaml&lt;/a&gt; ★&lt;/li&gt; &#xA; &lt;li&gt;Xrayfree: &lt;a href=&#34;https://tt.vg/freeclash&#34;&gt;https://tt.vg/freeclash&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Free Node Pool&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Zu1k: &lt;a href=&#34;https://github.com/zu1k/proxypool/releases&#34;&gt;https://github.com/zu1k/proxypool/releases&lt;/a&gt; ★&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;For a detailed guide, see: &lt;a href=&#34;https://lancellc.gitbook.io/clash&#34;&gt;https://lancellc.gitbook.io/clash&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open &lt;strong&gt;ClashX&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;ClashX icon&lt;/strong&gt; in the status bar&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Config&lt;/strong&gt; and then &lt;strong&gt;Remote Config&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Manage&lt;/strong&gt; and then &lt;strong&gt;Add&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paste a &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WilliamStar007/ClashX-V2Ray-TopFreeProxy/main/#subscription-links&#34;&gt;Subscription Link&lt;/a&gt;&lt;/strong&gt; to the &lt;strong&gt;url&lt;/strong&gt; field&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OK!!&lt;/strong&gt; (manually select a node if necessary)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/89805831/179545223-69177f8e-5f2d-4bd3-ba27-b68018418e5a.mp4&#34;&gt;https://user-images.githubusercontent.com/89805831/179545223-69177f8e-5f2d-4bd3-ba27-b68018418e5a.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NodeFree (&lt;a href=&#34;https://nodefree.org&#34;&gt;https://nodefree.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ClashNode (&lt;a href=&#34;https://clashnode.com&#34;&gt;https://clashnode.com&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mfuu (&lt;a href=&#34;https://github.com/mfuu/v2ray&#34;&gt;https://github.com/mfuu/v2ray&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Anaer (&lt;a href=&#34;https://github.com/anaer/Sub&#34;&gt;https://github.com/anaer/Sub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Pojiezhiyuanjun (&lt;a href=&#34;https://github.com/pojiezhiyuanjun/2023&#34;&gt;https://github.com/pojiezhiyuanjun/2023&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Oslook (&lt;a href=&#34;https://github.com/oslook/clash-freenode&#34;&gt;https://github.com/oslook/clash-freenode&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Ermaozi (&lt;a href=&#34;https://github.com/ermaozi/get_subscribe&#34;&gt;https://github.com/ermaozi/get_subscribe&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Learnhard-cn (&lt;a href=&#34;https://github.com/learnhard-cn/free_proxy_ss&#34;&gt;https://github.com/learnhard-cn/free_proxy_ss&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tbbatbb (&lt;a href=&#34;https://github.com/tbbatbb/Proxy&#34;&gt;https://github.com/tbbatbb/Proxy&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Vveg26 (&lt;a href=&#34;https://github.com/vveg26/getProxy&#34;&gt;https://github.com/vveg26/getProxy&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;OpenRunner (&lt;a href=&#34;https://github.com/openRunner/clash-freenode&#34;&gt;https://github.com/openRunner/clash-freenode&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Xrayfree (&lt;a href=&#34;https://github.com/xrayfree/free-ssr-ss-v2ray-vpn-clash&#34;&gt;https://github.com/xrayfree/free-ssr-ss-v2ray-vpn-clash&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Zu1k (&lt;a href=&#34;https://github.com/zu1k/proxypool&#34;&gt;https://github.com/zu1k/proxypool&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Lancellc (&lt;a href=&#34;https://lancellc.gitbook.io/clash&#34;&gt;https://lancellc.gitbook.io/clash&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Archived Reference:&#xA;https://github.com/gooooooooooooogle/Clash-Config &#xA;https://proxies.bihai.cf &#xA;https://fq.lonxin.net &#xA;https://github.com/kxswa/k &#xA;https://github.com/NiceVPN123/NiceVPN  --&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project is meant for personal and educational uses only.&lt;/li&gt; &#xA; &lt;li&gt;Please follow relevant laws and regulations when using this project.&lt;/li&gt; &#xA; &lt;li&gt;Project owner is not responsible or liable in any manner for the use of the content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;[![Star History Chart](https://api.star-history.com/svg?repos=WilliamStar007/ClashX-TopFreeProxy&amp;type=Date)](https://star-history.com/#WilliamStar007/ClashX-TopFreeProxy&amp;Date)&#xA;--&gt;</summary>
  </entry>
  <entry>
    <title>tigerlab-ai/tiger</title>
    <updated>2023-11-05T01:40:04Z</updated>
    <id>tag:github.com,2023-11-05:/tigerlab-ai/tiger</id>
    <link href="https://github.com/tigerlab-ai/tiger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source LLM toolkit to build LLM applications. TigerRag (embedding, RAG), TigerTune (fine-tuning), TigerArmor (AI safety)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://lilinwang.github.io/image/tigerLabLogo.png&#34; height=&#34;24px&#34; style=&#34;padding-top:4px&#34;&gt;TigerLab - Open Source LLM Toolkit&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://lilinwang.github.io/image/TigerLab.png&#34; width=&#34;80%&#34; style=&#34;padding: 40px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🐅🚀&lt;em&gt;LLM Toolkit: RAG + FineTune + ?&lt;/em&gt;🚀🐅 &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/GnwH2STv&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?style=for-the-badge&#34; alt=&#34;Join our Discord&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/TigerLabAI&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/twitter/follow/TigerLabAI?style=for-the-badge&#34; height=&#34;20&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/tigerlab-ai/tiger&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/stars/tigerlab-ai/tiger?style=for-the-badge&amp;amp;color=gold&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/tigerlab-ai/tiger/commits/main&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/last-commit/tigerlab-ai/tiger/main?style=for-the-badge&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/tigerlab-ai/tiger/raw/main/README.md&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=Apache License 2.0&amp;amp;color=green&amp;amp;style=for-the-badge&#34; alt=&#34;License&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;A significant gap has arisen between general Large Language Models (LLMs) and the data stores that provide them with contextual information. Bridging this gap is a crucial step towards grounding AI systems in efficient and factual domains, where their value lies not only in their generality but also in their specificity and uniqueness.&lt;/p&gt; &#xA;&lt;p&gt;In pursuit of this goal, we are thrilled to introduce the Tiger toolkit (TigerRag, TigerTune, TigerDA, TigerArmor) as an open-source resource for developers to create AI models and language applications tailored to their specific needs.&lt;/p&gt; &#xA;&lt;p&gt;We believe that our efforts will play a pivotal role in shaping the next phase of language modeling. This phase involves organizations customizing AI systems to align with their unique intellectual property and safety requirements, ushering in a new era of AI customization and precision.&lt;/p&gt; &#xA;&lt;h2&gt;✨ Demo&lt;/h2&gt; &#xA;&lt;p&gt;Find more demos at &lt;a href=&#34;https://www.tigerlab.ai/&#34;&gt;TigerLab.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo 1 - Enhanced Retrieval Capabilities w/ EBR, RAG and GAR&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/gi8P1i0hm70&#34;&gt;Demo 1 - Youtube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tigerlab-ai/tiger/assets/4805931/e7c35117-269a-437d-99ab-10407a901cc5&#34;&gt;https://github.com/tigerlab-ai/tiger/assets/4805931/e7c35117-269a-437d-99ab-10407a901cc5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo 2 - Fine-tuning Llama2 and DistilBERT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/0v0Qe-cbvRs&#34;&gt;Demo 2 - Youtube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tigerlab-ai/tiger/assets/148816206/4835b876-77e2-4483-9773-ea0b1d625f6c&#34;&gt;https://github.com/tigerlab-ai/tiger/assets/148816206/4835b876-77e2-4483-9773-ea0b1d625f6c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔬 Tech stack&lt;/h2&gt; &#xA;&lt;img width=&#34;2046&#34; alt=&#34;Untitled-2&#34; src=&#34;https://github.com/tigerlab-ai/tiger/assets/148816206/6616f960-1dc0-4e70-b44e-b34e20730152&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TigerRag&lt;/strong&gt;: Use embeddings-based retrieval (EBR), retrieval-augmented generation (RAG), and generation-augmented retrieval (GAR) to fulfill queries. The demo used &lt;code&gt;BERT&lt;/code&gt; for embedding, &lt;code&gt;FAISS&lt;/code&gt; for indexing, &lt;code&gt;text-davinci-003&lt;/code&gt; for generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TigerTune&lt;/strong&gt;: Python SDK to finetune, make inference, and evaluate Text Generation models and Text Classification models. The notebook demo finetuned &lt;code&gt;Llama2&lt;/code&gt; and &lt;code&gt;DistilBERT&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TigerDA&lt;/strong&gt;: Data Augmentation Toolkit. Coming soon!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TigerArmor&lt;/strong&gt; AI safety Toolkit. Coming soon!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👨‍🚀 Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before you begin setting up this project, please ensure you have completed the following tasks:&lt;/p&gt; &#xA;&lt;h3&gt;0. Setup Tutorial&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/fztXswkYz7c&#34;&gt;Tutorial - YouTuBe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. LLM - OpenAI API Token&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; This application utilizes the OpenAI API to access its powerful language model capabilities. In order to use the OpenAI API, you will need to obtain an API token. &#xA; &lt;p&gt;To get your OpenAI API token, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Go to the &lt;a href=&#34;https://beta.openai.com/signup/&#34;&gt;OpenAI website&lt;/a&gt; and sign up for an account if you haven&#39;t already.&lt;/li&gt; &#xA;  &lt;li&gt;Once you&#39;re logged in, navigate to the &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;API keys page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Generate a new API key by clicking on the &#34;Create API Key&#34; button.&lt;/li&gt; &#xA;  &lt;li&gt;Copy the API key and store it safely.&lt;/li&gt; &#xA;  &lt;li&gt;Add the API key to your environment variable, e.g. &lt;code&gt;export OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;💿 Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;. Clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/tigerlab-ai/tiger.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;. Install TigerRag&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install all Python requirements&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tiger/TigerRag &#xA;pip install -r tigerrag/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Demo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd tigerrag/demo/movie_recs&#xA;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;. Install TigerTune&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install all Python requirements&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tiger/TigerTune&#xA;pip install -r tigertune/requirements.txt&#xA;pip install --upgrade -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Demo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python tigertune/examples/classification_example.py &#xA;python tigertune/examples/generation_example.py &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CUDA GPU is needed to run generation_example.py&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📍 Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Launch v0.0.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release TigerDA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release TigerArmor&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add additional model support in TigerTune&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; VectorDB for TigerRag&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; WebApp&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🫶 Contribute to TigerLab&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://raw.githubusercontent.com/tigerlab-ai/tiger/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;For bug fixes and feature requests, please file a Github issue.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the mentioned roadmap, we also maintain a backlog at &lt;a href=&#34;https://github.com/tigerlab-ai/tiger/issues&#34;&gt;https://github.com/tigerlab-ai/tiger/issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💪 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/tigerlab-ai/tiger/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=tigerlab-ai/tiger&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;🎲 Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join us on &lt;a href=&#34;https://discord.gg/GnwH2STv&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>