<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-13T01:37:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/Llama-2-Onnx</title>
    <updated>2023-08-13T01:37:07Z</updated>
    <id>tag:github.com,2023-08-13:/microsoft/Llama-2-Onnx</id>
    <link href="https://github.com/microsoft/Llama-2-Onnx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;Llama 2 Powered By ONNX&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is an optimized version of the Llama 2 model, available from Meta under the Llama Community License Agreement found on this repository. Microsoft permits you to use, modify, redistribute and create derivatives of Microsoft&#39;s contributions to the optimized version subject to the restrictions and disclaimers of warranty and liability in the Llama Community License agreement.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;Before You Start&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;The sub-modules that contain the ONNX files in this repository are access controlled. To get access permissions to the Llama 2 model, please fill out the &lt;a href=&#34;https://forms.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR1sq8AbaR35DlqQqW8HAxY1UQlU4UThHTlFWVUUwMzBXV1gxWENRTjRHRi4u&#34;&gt;Llama 2 access request form&lt;/a&gt;. If allowable, you will receive GitHub access in the next 48 hours, but usually much sooner.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Cloning This Repository And The Submodules&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Chose from the following sub-modules:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B_FT_float16&lt;/li&gt; &#xA; &lt;li&gt;7B_FT_float32&lt;/li&gt; &#xA; &lt;li&gt;7B_float16&lt;/li&gt; &#xA; &lt;li&gt;7B_float32&lt;/li&gt; &#xA; &lt;li&gt;13B_FT_float16&lt;/li&gt; &#xA; &lt;li&gt;13B_FT_float32&lt;/li&gt; &#xA; &lt;li&gt;13B_float16&lt;/li&gt; &#xA; &lt;li&gt;13B_float32&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/Llama-2-Onnx.git&#xA;cd Llama-2-Onnx&#xA;git submodule init &amp;lt;chosen_submodule&amp;gt; &#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can repeate the init command with a different submodule name to initialize multiple submodules. Be careful, the contained files are very large! (7B Float16 models are about 10GB)&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;What is Llama 2?&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Llama 2 is a collection of pretrained and fine-tuned generative text models. To learn more about Llama 2, review the &lt;a href=&#34;https://github.com/microsoft/Llama-2-Onnx/raw/main/MODEL-CARD-META-LLAMA-2.md&#34;&gt;Llama 2 model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;What Is The Structure Of Llama 2?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Llama 2 model consists of a stack of decoder layers. Each decoder layer (or transformer block) is constructed from one self-attention layer and one feed-forward multi-layer perceptron. Llama models use different projection sizes compared with classic transformers in the feed-forward layer, for instance, both Llama 1 and Llama 2 projection use 2.7x hidden size rather than the standard 4x hidden size. A key difference between Llama 1 and Llama 2 is the architectural change of attention layer, in which Llama 2 takes advantage of Grouped Query Attention (GQA) mechanism to improve efficiency.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Llama-2-Onnx/main/Images/Llama2Model.png&#34; alt=&#34;Llama 2 Model&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Is There A Simple Code Example Running Llama 2 With ONNX?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;There are two examples provided in this repository. There is a minimum working example shown in &lt;a href=&#34;https://github.com/microsoft/Llama-2-Onnx/raw/main/MinimumExample/Example.md&#34;&gt;Llama-2-Onnx/MinimumExample&lt;/a&gt;. This is simply a command line program that will complete some text with the chosen version of Llama 2.&lt;/p&gt; &#xA;&lt;p&gt;Given the following input:&lt;/p&gt; &#xA;&lt;!-- Section of bash code --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python MinimumExample/Example_ONNX_LlamaV2.py --onnx_file 7B_FT_float16/ONNX/LlamaV2_7B_FT_float16.onnx --embedding_file 7B_FT_float16/embeddings.pth --tokenizer_path tokenizer.model --prompt &#34;What is the lightest element?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;The lightest element is hydrogen. Hydrogen is the lightest element on the periodic table, with an atomic mass of 1.00794 u (unified atomic mass units).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Is There A More Complete Code Example Running Llama 2 With ONNX?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;There is a more complete chat bot interface that is available in &lt;a href=&#34;https://github.com/microsoft/Llama-2-Onnx/raw/main/ChatApp/ChatApp.md&#34;&gt;Llama-2-Onnx/ChatApp&lt;/a&gt;. This is a python program based on the popular Gradio web interface. It will allow you to interact with the chosen version of Llama 2 in a chat bot interface.&lt;/p&gt; &#xA;&lt;p&gt;An example interaction can be seen here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Llama-2-Onnx/main/Images/ChatAppExample.png&#34; alt=&#34;Chat App&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;How Do I Use The Fine-tuned Models?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The fine-tuned models were trained for dialogue applications.&lt;/p&gt; &#xA;&lt;p&gt;To get the expected features and performance for them, a specific formatting needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; tag, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and breaklines in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces).&lt;/p&gt; &#xA;&lt;p&gt;This enables models in chat mode as well as additional safeguards to reduce potentially undesirable output.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Why Is The First Inference Session Slow?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;ONNX runtime execution provider might need to generate JIT binaries for the underlying hardware, typically the binary is cache and will be loaded directly in the subsequent runs to reduce the overhead.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Why Is FP16 ONNX Slower Than ONNX FP32 On My Device?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;It is possible that your device does not support native FP16 math, therefore weights will be cast to FP32 at runtime. Using the FP32 version of the model will avoid the cast overhead.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;How Do I Get Better Inference Speed?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;It is recommended that inputs/outputs are put on target device to avoid expensive data copies, please refer to the following document for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://onnxruntime.ai/docs/performance/tune-performance/iobinding.html&#34;&gt;I/O Binding | onnxruntime&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;What Parameters Should I Test With?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Users can perform temperature and top-p sampling using the model‚Äôs output logits. Please refer to Meta‚Äôs guidance for the best parameters combination; an example is located &lt;a href=&#34;https://github.com/facebookresearch/llama/&#34;&gt;here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;How Can I Develop With Llama 2 Responsibly?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;In order to help developers innovate responsibly, Meta encourages you to review the &lt;a href=&#34;https://ai.meta.com/llama/responsible-use-guide/&#34;&gt;Responsible Use Guide&lt;/a&gt; for the Llama 2 models.&lt;/p&gt; &#xA;&lt;p&gt;Microsoft encourages you to learn more about its &lt;a href=&#34;https://aka.ms/rai&#34;&gt;Responsible AI approach&lt;/a&gt;, including many publicly available resources and tools for developers.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/trl</title>
    <updated>2023-08-13T01:37:07Z</updated>
    <id>tag:github.com,2023-08-13:/huggingface/trl</id>
    <link href="https://github.com/huggingface/trl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Train transformer language models with reinforcement learning.&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_banner_dark.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;TRL - Transformer Reinforcement Learning&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Full stack transformer language models with reinforcement learning.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/huggingface/trl/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/huggingface/trl.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/trl/index&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/website/http/huggingface.co/docs/trl/index.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/trl/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/huggingface/trl.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;code&gt;trl&lt;/code&gt; is a full stack library where we provide a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. The library is built on top of the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; library by ü§ó Hugging Face. Therefore, pre-trained language models can be directly loaded via &lt;code&gt;transformers&lt;/code&gt;. At this point most of decoder architectures and encoder-decoder architectures are supported. Refer to the documentation or the &lt;code&gt;examples/&lt;/code&gt; folder for example code snippets and how to run these tools.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/trl/sft_trainer&#34;&gt;&lt;code&gt;SFTTrainer&lt;/code&gt;&lt;/a&gt;: A light and friendly wrapper around &lt;code&gt;transformers&lt;/code&gt; Trainer to easily fine-tune language models or adapters on a custom dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/trl/reward_trainer&#34;&gt;&lt;code&gt;RewardTrainer&lt;/code&gt;&lt;/a&gt;: A light wrapper around &lt;code&gt;transformers&lt;/code&gt; Trainer to easily fine-tune language models for human preferences (Reward Modeling).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/trl/trainer#trl.PPOTrainer&#34;&gt;&lt;code&gt;PPOTrainer&lt;/code&gt;&lt;/a&gt;: A PPO trainer for language models that just needs (query, response, reward) triplets to optimise the language model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/trl/models#trl.AutoModelForCausalLMWithValueHead&#34;&gt;&lt;code&gt;AutoModelForCausalLMWithValueHead&lt;/code&gt;&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/docs/trl/models#trl.AutoModelForSeq2SeqLMWithValueHead&#34;&gt;&lt;code&gt;AutoModelForSeq2SeqLMWithValueHead&lt;/code&gt;&lt;/a&gt;: A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/trl/tree/main/examples&#34;&gt;Examples&lt;/a&gt;: Train GPT2 to generate positive movie reviews with a BERT sentiment classifier, full RLHF using adapters only, train GPT-j to be less toxic, &lt;a href=&#34;https://huggingface.co/blog/stackllama&#34;&gt;Stack-Llama example&lt;/a&gt;, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How PPO works&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tuning a language model via PPO consists of roughly three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rollout&lt;/strong&gt;: The language model generates a response or continuation based on query which could be the start of a sentence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don&#39;t deviate to far from the reference language model. The active language model is then trained with PPO.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This process is illustrated in the sketch below:&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_overview.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;b&gt;Figure:&lt;/b&gt; Sketch of the workflow. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Python package&lt;/h3&gt; &#xA;&lt;p&gt;Install the library with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install trl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From source&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run the examples in the repository a few additional libraries are required. Clone the repository and install it with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/trl.git&#xA;cd trl/&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you wish to develop TRL, you should install in editable mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;SFTTrainer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is a basic example on how to use the &lt;code&gt;SFTTrainer&lt;/code&gt; from the library. The &lt;code&gt;SFTTrainer&lt;/code&gt; is a light wrapper around the &lt;code&gt;transformers&lt;/code&gt; Trainer to easily fine-tune language models or adapters on a custom dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# imports&#xA;from datasets import load_dataset&#xA;from trl import SFTTrainer&#xA;&#xA;# get dataset&#xA;dataset = load_dataset(&#34;imdb&#34;, split=&#34;train&#34;)&#xA;&#xA;# get trainer&#xA;trainer = SFTTrainer(&#xA;    &#34;facebook/opt-350m&#34;,&#xA;    train_dataset=dataset,&#xA;    dataset_text_field=&#34;text&#34;,&#xA;    max_seq_length=512,&#xA;)&#xA;&#xA;# train&#xA;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;RewardTrainer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is a basic example on how to use the &lt;code&gt;RewardTrainer&lt;/code&gt; from the library. The &lt;code&gt;RewardTrainer&lt;/code&gt; is a wrapper around the &lt;code&gt;transformers&lt;/code&gt; Trainer to easily fine-tune reward models or adapters on a custom preference dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# imports&#xA;from transformers import AutoModelForSequenceClassification, AutoTokenizer&#xA;from trl import RewardTrainer&#xA;&#xA;# load model and dataset - dataset needs to be in a specific format&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#34;gpt2&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)&#xA;&#xA;...&#xA;&#xA;# load trainer&#xA;trainer = RewardTrainer(&#xA;    model=model,&#xA;    tokenizer=tokenizer,&#xA;    train_dataset=dataset,&#xA;)&#xA;&#xA;# train&#xA;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;PPOTrainer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is a basic example on how to use the &lt;code&gt;PPOTrainer&lt;/code&gt; from the library. Based on a query the language model creates a response which is then evaluated. The evaluation could be a human in the loop or another model&#39;s output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# imports&#xA;import torch&#xA;from transformers import AutoTokenizer&#xA;from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model&#xA;from trl.core import respond_to_batch&#xA;&#xA;# get models&#xA;model = AutoModelForCausalLMWithValueHead.from_pretrained(&#39;gpt2&#39;)&#xA;model_ref = create_reference_model(model)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;gpt2&#39;)&#xA;&#xA;# initialize trainer&#xA;ppo_config = PPOConfig(&#xA;    batch_size=1,&#xA;)&#xA;&#xA;# encode a query&#xA;query_txt = &#34;This morning I went to the &#34;&#xA;query_tensor = tokenizer.encode(query_txt, return_tensors=&#34;pt&#34;)&#xA;&#xA;# get model response&#xA;response_tensor  = respond_to_batch(model, query_tensor)&#xA;&#xA;# create a ppo trainer&#xA;ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)&#xA;&#xA;# define a reward for response&#xA;# (this could be any reward such as human feedback or output from another model)&#xA;reward = [torch.tensor(1.0)]&#xA;&#xA;# train model for one step with ppo&#xA;train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;h3&gt;Proximal Policy Optimisation&lt;/h3&gt; &#xA;&lt;p&gt;The PPO implementation largely follows the structure introduced in the paper &lt;strong&gt;&#34;Fine-Tuning Language Models from Human Preferences&#34;&lt;/strong&gt; by D. Ziegler et al. [&lt;a href=&#34;https://arxiv.org/pdf/1909.08593.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/lm-human-preferences&#34;&gt;code&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;h3&gt;Language models&lt;/h3&gt; &#xA;&lt;p&gt;The language models utilize the &lt;code&gt;transformers&lt;/code&gt; library by ü§ó Hugging Face.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{vonwerra2022trl,&#xA;  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert},&#xA;  title = {TRL: Transformer Reinforcement Learning},&#xA;  year = {2020},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/trl}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>infobyte/emploleaks</title>
    <updated>2023-08-13T01:37:07Z</updated>
    <id>tag:github.com,2023-08-13:/infobyte/emploleaks</id>
    <link href="https://github.com/infobyte/emploleaks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An OSINT tool that helps detect members of a company with leaked credentials&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üîé EmploLeaks&lt;/h1&gt; &#xA;&lt;p&gt;This is a tool designed for Open Source Intelligence (OSINT) purposes, which helps to gather information about employees of a company.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ How it Works&lt;/h2&gt; &#xA;&lt;p&gt;The tool starts by searching through LinkedIn to obtain a list of employees of the company. Then, it looks for their social network profiles to find their personal email addresses. Finally, it uses those email addresses to search through a custom COMB database to retrieve leaked passwords. You an easily add yours and connect to through the tool.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use this tool, you&#39;ll need to have Python 3.10 installed on your machine. Clone this repository to your local machine and install the required dependencies using pip in the &lt;em&gt;cli&lt;/em&gt; folder:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;OSX&lt;/h3&gt; &#xA;&lt;p&gt;We know that there is a problem when installing the tool due to the &lt;em&gt;psycopg2&lt;/em&gt; binary. If you run into this problem, you can solve it running:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 -m pip install psycopg2-binary&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìà Basic Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use the tool, simply run the following command:&lt;/p&gt; &#xA;&lt;p&gt;python3 cli/emploleaks.py&lt;/p&gt; &#xA;&lt;p&gt;If everything went well during the installation, you will be able to start using EmploLeaks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;___________              .__         .__                 __&#xA;\_   _____/ _____ ______ |  |   ____ |  |   ____ _____  |  | __  ______&#xA; |    __)_ /     \____  \|  |  /  _ \|  | _/ __ \__   \ |  |/ / /  ___/&#xA; |        \  Y Y  \  |_&amp;gt; &amp;gt;  |_(  &amp;lt;_&amp;gt; )  |_\  ___/ / __ \|    &amp;lt;  \___ \&#xA;/_______  /__|_|  /   __/|____/\____/|____/\___  &amp;gt;____  /__|_ \/____  &amp;gt;&#xA;        \/      \/|__|                         \/     \/     \/     \/&#xA;&#xA;OSINT tool üïµ  to chain multiple apis&#xA;emploleaks&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Right now, the tool supports two functionalities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linkedin, for searching all employees from a company and get their personal emails. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A GitLab extension, which is capable of finding personal code repositories from the employees.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If defined and connected, when the tool is gathering employees profiles, a search to a COMB database will be made in order to retrieve leaked passwords.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Retrieving Linkedin Profiles&lt;/h3&gt; &#xA;&lt;p&gt;First, you must set the plugin to use, which in this case is &lt;em&gt;linkedin&lt;/em&gt;. After, you should set your credentials and the run the &lt;em&gt;login&lt;/em&gt; proccess:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emploleaks&amp;gt; use --plugin linkedin&#xA;emploleaks(linkedin)&amp;gt; setopt USER myemail@domain.com&#xA;[+] Updating value successfull&#xA;emploleaks(linkedin)&amp;gt; setopt PASS mypass1234&#xA;[+] Updating value successfull&#xA;emploleaks(linkedin)&amp;gt; show options&#xA;Module options:&#xA;&#xA;Name    Current Setting     Required    Description&#xA;------  ------------------  ----------  ---------------------------&#xA;USER    myemail@domain.com  yes         linkedin account&#39;s username&#xA;PASS    ********            yes         linkedin accout&#39;s password&#xA;hide    yes                 no          hide the password field&#xA;emploleaks(linkedin)&amp;gt; run login&#xA;[+] Session established.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now that the module is configured, you can run it and start gathering information from the company:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emploleaks(linkedin)&amp;gt; run find EvilCorp&#xA;[+] Added 25 new names.&#xA;[+] Added 22 new names.&#xA;ü¶Ñ Listing profiles:&#xA;üîë Getting and processing contact info of &#34;XYZ ZYX&#34;&#xA;&#x9;Contact info:&#xA;&#x9;email: xya@gmail.com&#xA;&#x9;full name: XYZ XYZ&#xA;&#x9;profile name: xyz-xyz&#xA;&#x9;occupation: Sr XYZ&#xA;&#x9;public identifier: xyz-zyx&#xA;&#x9;urn: urn:li:member:67241221&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get Linkedin accounts + Leaked Passwords&lt;/h3&gt; &#xA;&lt;p&gt;We created a custom &lt;em&gt;workflow&lt;/em&gt;, where with the information retrieved by Linkedin, we try to match employees&#39; personal emails to potential leaked passwords. In this case, you can connect to a database (in our case we have a custom indexed COMB database) using the &lt;em&gt;connect&lt;/em&gt; command, as it is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emploleaks(linkedin)&amp;gt; connect --user myuser --passwd mypass123 --dbname mydbname --host 1.2.3.4&#xA;[+] Connecting to the Leak Database...&#xA;[*] version: PostgreSQL 12.15&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once it&#39;s connected, you can run the &lt;em&gt;workflow&lt;/em&gt;. With all the users gathered, the tool will try to search in the database if a leaked credential is affecting someone:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emploleaks(linkedin)&amp;gt; run_pyscript workflows/check_leaked_passwords.py EvilCorp&#xA;[-] Failing login... trying again!&#xA;[-] Failing login... trying again!&#xA;[+] Connected to the LinkedIn api successfull&#xA;The following command could take a couple of minutes, be patient&#xA;ü¶Ñ Listing profiles:&#xA;üîë Getting and processing contact info of &#34;XYZ ZXY&#34;&#xA;üîë Getting and processing contact info of &#34;YZX XZY&#34;&#xA;üîë Getting and processing contact info of &#34;ZYX ZXY&#34;&#xA;[...]&#xA;[+] Password for &#34;XYZ ZXY&#34; exists&#xA;[*] Email: xyzzxy@gmail.com&#xA;+------------------+&#xA;| passwords leaked |&#xA;+------------------+&#xA;| laFQqAOSL6t      |&#xA;+------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a conclusion, the tool will generate a console output with the following information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A list of employees of the company (obtained from LinkedIn)&lt;/li&gt; &#xA; &lt;li&gt;The social network profiles associated with each employee (obtained from email address)&lt;/li&gt; &#xA; &lt;li&gt;A list of leaked passwords associated with each email address.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìå Next Steps&lt;/h2&gt; &#xA;&lt;p&gt;We are integrating other public sites and applications that may offer about a leaked credential. We may not be able to see the plaintext password, but it will give an insight if the user has any compromised credential:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration with Have I Been Pwned?&lt;/li&gt; &#xA; &lt;li&gt;Integration with Firefox Monitor&lt;/li&gt; &#xA; &lt;li&gt;Integration with Leak Check&lt;/li&gt; &#xA; &lt;li&gt;Integration with BreachAlarm&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, we will be focusing on gathering even more information from public sources of every employee. Do you have any idea in mind? Don&#39;t hesitate to reach us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javi Aguinaga: &lt;a href=&#34;mailto:jaguinaga@faradaysec.com&#34;&gt;jaguinaga@faradaysec.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gabi Franco: &lt;a href=&#34;mailto:gabrielf@faradaysec.com&#34;&gt;gabrielf@faradaysec.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Or you con DM at &lt;a href=&#34;https://twitter.com/pastacls&#34;&gt;@pastacls&lt;/a&gt; or &lt;a href=&#34;https://twitter.com/gaaabifranco&#34;&gt;@gaaabifranco&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;h2&gt;üìù License&lt;/h2&gt; &#xA;&lt;p&gt;This tool is licensed under the MIT License. See the &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;</summary>
  </entry>
</feed>