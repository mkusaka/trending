<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-29T01:35:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Blaizzy/mlx-audio</title>
    <updated>2025-04-29T01:35:16Z</updated>
    <id>tag:github.com,2025-04-29:/Blaizzy/mlx-audio</id>
    <link href="https://github.com/Blaizzy/mlx-audio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech synthesis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; &#xA;&lt;p&gt;A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech synthesis on Apple Silicon.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast inference on Apple Silicon (M series chips)&lt;/li&gt; &#xA; &lt;li&gt;Multiple language support&lt;/li&gt; &#xA; &lt;li&gt;Voice customization options&lt;/li&gt; &#xA; &lt;li&gt;Adjustable speech speed control (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; &#xA; &lt;li&gt;REST API for TTS generation&lt;/li&gt; &#xA; &lt;li&gt;Quantization support for optimized performance&lt;/li&gt; &#xA; &lt;li&gt;Direct access to output files via Finder/Explorer integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install the package&#xA;pip install mlx-audio&#xA;&#xA;# For web interface and API dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic usage&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34;&#xA;&#xA;# Specify prefix for output file&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --file_prefix hello&#xA;&#xA;# Adjust speaking speed (0.5-2.0)&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --speed 1.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to call from python&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.generate import generate_audio&#xA;&#xA;# Example: Generate an audiobook chapter as mp3 audio&#xA;generate_audio(&#xA;    text=(&#34;In the beginning, the universe was created...\n&#34;&#xA;        &#34;...or the simulation was booted up.&#34;),&#xA;    model_path=&#34;prince-canuma/Kokoro-82M&#34;,&#xA;    voice=&#34;af_heart&#34;,&#xA;    speed=1.2,&#xA;    lang_code=&#34;a&#34;, # Kokoro: (a)f_heart, or comment out for auto&#xA;    file_prefix=&#34;audiobook_chapter1&#34;,&#xA;    audio_format=&#34;wav&#34;,&#xA;    sample_rate=24000,&#xA;    join_audio=True,&#xA;    verbose=True  # Set to False to disable print messages&#xA;)&#xA;&#xA;print(&#34;Audiobook chapter successfully generated!&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Interface &amp;amp; API Server&lt;/h3&gt; &#xA;&lt;p&gt;MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate TTS with different voices and speed settings&lt;/li&gt; &#xA; &lt;li&gt;Upload and play your own audio files&lt;/li&gt; &#xA; &lt;li&gt;Visualize audio with an interactive 3D orb&lt;/li&gt; &#xA; &lt;li&gt;Automatically saves generated audio files to the outputs directory in the current working folder&lt;/li&gt; &#xA; &lt;li&gt;Open the output folder directly from the interface (when running locally)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Voice Options&lt;/strong&gt;: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adjustable Speech Speed&lt;/strong&gt;: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time 3D Visualization&lt;/strong&gt;: A responsive 3D orb that reacts to audio frequencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Audio Upload&lt;/strong&gt;: Play and visualize your own audio files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-play Option&lt;/strong&gt;: Automatically play generated audio&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output Folder Access&lt;/strong&gt;: Convenient button to open the output folder in your system&#39;s file explorer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start the web interface and API server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Using the command-line interface&#xA;mlx_audio.server&#xA;&#xA;# With custom host and port&#xA;mlx_audio.server --host 0.0.0.0 --port 9000&#xA;&#xA;# With verbose logging&#xA;mlx_audio.server --verbose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available command line arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;: Host address to bind the server to (default: 127.0.0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port to bind the server to (default: 8000)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then open your browser and navigate to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;API Endpoints&lt;/h4&gt; &#xA;&lt;p&gt;The server provides the following REST API endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /tts&lt;/code&gt;: Generate TTS audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The text to convert to speech (required)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;voice&lt;/code&gt;: Voice to use (default: &#34;af_heart&#34;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;speed&lt;/code&gt;: Speech speed from 0.5 to 2.0 (default: 1.0)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with filename of generated audio&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GET /audio/{filename}&lt;/code&gt;: Retrieve generated audio file&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /play&lt;/code&gt;: Play audio directly from the server&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;filename&lt;/code&gt;: The filename of the audio to play (required)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with status and filename&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /stop&lt;/code&gt;: Stop any currently playing audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /open_output_folder&lt;/code&gt;: Open the output folder in the system&#39;s file explorer&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status and path&lt;/li&gt; &#xA;   &lt;li&gt;Note: This feature only works when running the server locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Generated audio files are stored in &lt;code&gt;~/.mlx_audio/outputs&lt;/code&gt; by default, or in a fallback directory if that location is not writable.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Kokoro&lt;/h3&gt; &#xA;&lt;p&gt;Kokoro is a multilingual TTS model that supports various languages and voice styles.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.models.kokoro import KokoroPipeline&#xA;from mlx_audio.tts.utils import load_model&#xA;from IPython.display import Audio&#xA;import soundfile as sf&#xA;&#xA;# Initialize the model&#xA;model_id = &#39;prince-canuma/Kokoro-82M&#39;&#xA;model = load_model(model_id)&#xA;&#xA;# Create a pipeline with American English&#xA;pipeline = KokoroPipeline(lang_code=&#39;a&#39;, model=model, repo_id=model_id)&#xA;&#xA;# Generate audio&#xA;text = &#34;The MLX King lives. Let him cook!&#34;&#xA;for _, _, audio in pipeline(text, voice=&#39;af_heart&#39;, speed=1, split_pattern=r&#39;\n+&#39;):&#xA;    # Display audio in notebook (if applicable)&#xA;    display(Audio(data=audio, rate=24000, autoplay=0))&#xA;&#xA;    # Save audio to file&#xA;    sf.write(&#39;audio.wav&#39;, audio[0], 24000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Language Options&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ðŸ‡ºðŸ‡¸ &lt;code&gt;&#39;a&#39;&lt;/code&gt; - American English&lt;/li&gt; &#xA; &lt;li&gt;ðŸ‡¬ðŸ‡§ &lt;code&gt;&#39;b&#39;&lt;/code&gt; - British English&lt;/li&gt; &#xA; &lt;li&gt;ðŸ‡¯ðŸ‡µ &lt;code&gt;&#39;j&#39;&lt;/code&gt; - Japanese (requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ðŸ‡¨ðŸ‡³ &lt;code&gt;&#39;z&#39;&lt;/code&gt; - Mandarin Chinese (requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CSM (Conversational Speech Model)&lt;/h3&gt; &#xA;&lt;p&gt;CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Generate speech using CSM-1B model with reference audio&#xA;python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text &#34;Hello from Sesame.&#34; --play --ref_audio ./conversational_a.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass any audio to clone the voice from or download sample audio file from &lt;a href=&#34;https://huggingface.co/mlx-community/csm-1b/tree/main/prompts&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can quantize models for improved performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.utils import quantize_model, load_model&#xA;import json&#xA;import mlx.core as mx&#xA;&#xA;model = load_model(repo_id=&#39;prince-canuma/Kokoro-82M&#39;)&#xA;config = model.config&#xA;&#xA;# Quantize to 8-bit&#xA;group_size = 64&#xA;bits = 8&#xA;weights, config = quantize_model(model, config, group_size, bits)&#xA;&#xA;# Save quantized model&#xA;with open(&#39;./8bit/config.json&#39;, &#39;w&#39;) as f:&#xA;    json.dump(config, f)&#xA;&#xA;mx.save_safetensors(&#34;./8bit/kokoro-v1_0.safetensors&#34;, weights, metadata={&#34;format&#34;: &#34;mlx&#34;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MLX&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;Apple Silicon Mac (for optimal performance)&lt;/li&gt; &#xA; &lt;li&gt;For the web interface and API: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FastAPI&lt;/li&gt; &#xA;   &lt;li&gt;Uvicorn&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.&lt;/li&gt; &#xA; &lt;li&gt;This project uses the Kokoro model architecture for text-to-speech synthesis.&lt;/li&gt; &#xA; &lt;li&gt;The 3D visualization uses Three.js for rendering.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>