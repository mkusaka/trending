<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-31T01:36:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nebuly-ai/nebullvm</title>
    <updated>2022-12-31T01:36:15Z</updated>
    <id>tag:github.com,2022-12-31:/nebuly-ai/nebullvm</id>
    <link href="https://github.com/nebuly-ai/nebullvm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Accelerate AI models leveraging best-of-breed optimization techniques üöÄ&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;a https: docs.nebuly.com welcome quick-start&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/83510798/208247207-861541f0-b968-484c-8a0c-0fb110399c16.png&#34; width=&#34;400px&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;AI Optimization AppStore to boost the performances of your AI systems&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/nebullvm/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/nebullvm.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/nebullvm&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nebullvm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/77d5kGSa8e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-1.1k-blueviolet?logo=discord&amp;amp;logoColor=white&amp;amp;style=round&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://twitter.com/nebuly_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url.svg?label=Follow%20%40nebuly_ai&amp;amp;style=social&amp;amp;url=https%3A%2F%2Ftwitter.com-nebuly_ai&#34;&gt;&lt;/a&gt;  &lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;Nebullvm&lt;/code&gt; is an ecosystem of open-source Apps to boost the performances of your AI systems. The optimization Apps are stack-agnostic and work with any library.&lt;/p&gt; &#xA;&lt;p&gt;Data. Models. Hardware. These are not independent factors, and making optimal choices on all fronts is hard. Our open source Apps help you to combine these 3 factors seamlessly, thus bringing incredibly fast and efficient AI systems to your fingertips. Four Apps categories to push the boundaries of AI efficiency. Dozens of Apps.&lt;/p&gt; &#xA;&lt;p&gt;If you like the idea, give us a star to show your support for the project ‚≠ê&lt;/p&gt; &#xA;&lt;h2&gt;Accelerate Apps&lt;/h2&gt; &#xA;&lt;p&gt;Achieve sub-10ms response time for any AI application, including generative and language models. Improve customer experience by providing near real-time inferences.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/accelerate/speedster&#34;&gt;Speedster&lt;/a&gt;: Automatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward&#34;&gt;Forward-Forward&lt;/a&gt;: Test the performance of the Forward-Forward algorithm in PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor&#34;&gt;OpenAlphaTensor&lt;/a&gt;: Boost your DL model&#39;s performance with OpenAlphaTensor&#39;s custom-generated matrix multiplication algorithms (AlphaTensor open-source).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/accelerate/large_speedster&#34;&gt;LargeSpeedster&lt;/a&gt;: Automatically apply SOTA optimization techniques on large AI models to achieve the maximum acceleration on your hardware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/accelerate/cloud_surfer&#34;&gt;CloudSurfer&lt;/a&gt;: Discover the optimal inference hardware and cloud platform to run an optimized version of your AI model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/accelerate/optimate&#34;&gt;OptiMate&lt;/a&gt;: Interactive tool guiding savvy users in achieving the best inference performance out of a given model / hardware setup.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Maximize Apps&lt;/h2&gt; &#xA;&lt;p&gt;Make your Kubernetes GPU infrastructure efficient. Simplify cluster management, maximize hardware utilization and minimize costs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/maximize/gpu_partitioner&#34;&gt;GPU Partitioner&lt;/a&gt;: Effortlessly maximize the utilization of GPU resources in a Kubernetes cluster through real-time dynamic partitioning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/maximize/gpus_elasticity&#34;&gt;GPUs Elasticity&lt;/a&gt;: Maximize your GPUs Kubernetes resource utilization with flexible and efficient elastic quotas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Extract Apps&lt;/h2&gt; &#xA;&lt;p&gt;Don‚Äôt settle on generic AI-models. Extract domain-specific knowledge from large foundational models to create portable, super efficient AI models tailored for your use case.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/extract/promptify&#34;&gt;Promptify&lt;/a&gt;: Effortlessly fine-tune large language and multi-modal models with minimal data and hardware requirements using p-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/extract/large_oracle_distillation&#34;&gt;LargeOracle Distillation&lt;/a&gt;: Leverage advanced knowledge distillation to extract a small and efficient model out of a larger model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Simulate Apps&lt;/h2&gt; &#xA;&lt;p&gt;The time for trial and error is over. Simulate the performances of large models on different computing architectures to reduce time-to-market, maximize accuracy and minimize costs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/simulate/simulinf&#34;&gt;Simulinf&lt;/a&gt;: Simulate inference performances of your AI model on different hardware and cloud platforms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/apps/simulate/training_sim&#34;&gt;TrainingSim&lt;/a&gt;: Easily simulate and optimize the training of large AI models on a distributed infrastructure.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Couldn&#39;t find the optimization app you were looking for? Please open an issue or contact us at &lt;a href=&#34;mailto:info@nebuly.ai&#34;&gt;info@nebuly.ai&lt;/a&gt; and we will be happy to develop it together.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/RbeQMu886J&#34;&gt;Join the community&lt;/a&gt; | &lt;a href=&#34;https://docs.nebuly.com/welcome/questions-and-contributions&#34;&gt;Contribute to the library&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrh0wl/Cloudmare</title>
    <updated>2022-12-31T01:36:15Z</updated>
    <id>tag:github.com,2022-12-31:/mrh0wl/Cloudmare</id>
    <link href="https://github.com/mrh0wl/Cloudmare" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloudflare, Sucuri, Incapsula real IP tracker.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cloudmare&lt;/h1&gt; &#xA;&lt;p&gt;Cloudmare is a simple tool to find the origin servers of websites protected by Cloudflare, Sucuri, or Incapsula with a misconfiguration DNS.&lt;/p&gt; &#xA;&lt;p&gt;For more detail about this common misconfiguration and how Cloudmare works, send me a private message.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s what Cloudmare looks like in action.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/pSzOXFG.png&#34; alt=&#34;Example usage&#34; title=&#34;Example usage&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(&lt;em&gt;The websites and the IP addresses in this example have been obfuscated&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/MrH0wl/Cloudmare.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Go to the folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Cloudmare&#xA;python Cloudmare.py -h or python Cloudmare.py -hh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run Cloudmare (see &lt;a href=&#34;https://raw.githubusercontent.com/mrh0wl/Cloudmare/master/#usage&#34;&gt;Usage&lt;/a&gt; below for more detail)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python Cloudmare.py -u target.site --bruter -sC -sSh -sSt --host verified.site&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Remember to view -hh for more info about the arguments)&lt;/p&gt; &#xA;&lt;h2&gt;Termux users&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;pkg upgrade &amp;amp;&amp;amp; pkg update&lt;/li&gt; &#xA; &lt;li&gt;pkg install git python libxml2 libxslt dnsutils&lt;/li&gt; &#xA; &lt;li&gt;git clone &lt;a href=&#34;https://github.com/MrH0wl/Cloudmare.git&#34;&gt;https://github.com/MrH0wl/Cloudmare.git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cd Cloudmare&lt;/li&gt; &#xA; &lt;li&gt;python Cloudmare.py -h or python Cloudmare.py -hh&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Note: Be patient if the script requires to install modules.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/9pmF1ol.png&#34; alt=&#34;Help options&#34; title=&#34;Help options&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Tested on Python=&amp;lt;3.7 (don&#39;t use Python 2 more), working on Linux and Windows. Feel free to &lt;a href=&#34;https://github.com/MrH0wl/Cloudmare/issues/new&#34;&gt;open an issue&lt;/a&gt; if you have bug reports or questions. If you want to collaborate, you&#39;re welcome.&lt;/p&gt; &#xA;&lt;h2&gt;Donate BTC&lt;/h2&gt; &#xA;&lt;p&gt;If you want Cloudmare to be updated more frequently with many more features, you can donate to help make this happen.&lt;/p&gt; &#xA;&lt;p&gt;BTC:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;15y7CTsrJeLmJuKzWXT8BVRoxEPXbK4Zp5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ETH:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0x9665C32ba7dD6e7C8278ff788303B937aA9b2f41&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;a href=&#34;https://paypal.me/mrh0wl&#34;&gt; &lt;img src=&#34;https://i.imgur.com/BtQVHbH.png&#34; alt=&#34;Donate with PayPal&#34; width=&#34;250&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://buymeacoffee.com/mrh0wl&#34;&gt; &lt;img src=&#34;https://miro.medium.com/max/720/1*VJdus0nKuy1uNoByh5BN3w.png&#34; alt=&#34;Buy me a coffee&#34; width=&#34;260&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Contact Info&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚úâÔ∏èEmail: secmare@protonmail.com&#xA;üê¶Twitter: @mrh0wl&#xA;üì∑Instagram: @mrh0wl&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/ConvNeXt</title>
    <updated>2022-12-31T01:36:15Z</updated>
    <id>tag:github.com,2022-12-31:/facebookresearch/ConvNeXt</id>
    <link href="https://github.com/facebookresearch/ConvNeXt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for ConvNeXt model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;A ConvNet for the 2020s&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch implementation of &lt;strong&gt;ConvNeXt&lt;/strong&gt;, from the following paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;A ConvNet for the 2020s&lt;/a&gt;. CVPR 2022.&lt;br&gt; &lt;a href=&#34;https://liuzhuang13.github.io&#34;&gt;Zhuang Liu&lt;/a&gt;, &lt;a href=&#34;https://hanzimao.me/&#34;&gt;Hanzi Mao&lt;/a&gt;, &lt;a href=&#34;https://chaoyuan.org/&#34;&gt;Chao-Yuan Wu&lt;/a&gt;, &lt;a href=&#34;https://feichtenhofer.github.io/&#34;&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~trevor/&#34;&gt;Trevor Darrell&lt;/a&gt; and &lt;a href=&#34;https://sainingxie.com&#34;&gt;Saining Xie&lt;/a&gt;&lt;br&gt; Facebook AI Research, UC Berkeley&lt;br&gt; [&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;&lt;code&gt;arXiv&lt;/code&gt;&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=QzCjXqFnWPE&#34;&gt;&lt;code&gt;video&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/8370623/180626875-fe958128-6102-4f01-9ca4-e3a30c3148f9.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; class=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We propose &lt;strong&gt;ConvNeXt&lt;/strong&gt;, a pure ConvNet model constructed entirely from standard ConvNet modules. ConvNeXt is accurate, efficient, scalable and very simple in design.&lt;/p&gt; &#xA;&lt;h2&gt;Catalog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet-1K Training Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet-22K Pre-training Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet-1K Fine-tuning Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Downstream Transfer (Detection, Segmentation) Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image Classification &lt;a href=&#34;https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO?usp=sharing&#34;&gt;[Colab]&lt;/a&gt; and Web Demo &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/convnext&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fine-tune on CIFAR with Weights &amp;amp; Biases logging &lt;a href=&#34;https://colab.research.google.com/drive/1ijAxGthE9RENJJQRO17v9A7PTd1Tei9F?usp=sharing&#34;&gt;[Colab]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ‚úÖ ‚¨úÔ∏è  --&gt; &#xA;&lt;h2&gt;Results and Pre-trained Models&lt;/h2&gt; &#xA;&lt;h3&gt;ImageNet-1K trained models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;198M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;198M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;101.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ImageNet-22K trained models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;22k model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1k model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;198M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;198M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;101.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_384.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-XL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;350M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-XL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;350M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;179.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_384_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ImageNet-1K trained models (isotropic)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.3G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_iso_small_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_iso_base_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;306M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_iso_large_1k_224_ema.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ConvNeXt/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; for installation instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We give an example evaluation command for a ImageNet-22K pre-trained, then ImageNet-1K fine-tuned ConvNeXt-B:&lt;/p&gt; &#xA;&lt;p&gt;Single-GPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --model convnext_base --eval true \&#xA;--resume https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth \&#xA;--input_size 224 --drop_path 0.2 \&#xA;--data_path /path/to/imagenet-1k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multi-GPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 main.py \&#xA;--model convnext_base --eval true \&#xA;--resume https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth \&#xA;--input_size 224 --drop_path 0.2 \&#xA;--data_path /path/to/imagenet-1k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should give&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* Acc@1 85.820 Acc@5 97.868 loss 0.563&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For evaluating other model variants, change &lt;code&gt;--model&lt;/code&gt;, &lt;code&gt;--resume&lt;/code&gt;, &lt;code&gt;--input_size&lt;/code&gt; accordingly. You can get the url to pre-trained models from the tables above.&lt;/li&gt; &#xA; &lt;li&gt;Setting model-specific &lt;code&gt;--drop_path&lt;/code&gt; is not strictly required in evaluation, as the &lt;code&gt;DropPath&lt;/code&gt; module in timm behaves the same during evaluation; but it is required in training. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ConvNeXt/main/TRAINING.md&#34;&gt;TRAINING.md&lt;/a&gt; or our paper for the values used for different models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ConvNeXt/main/TRAINING.md&#34;&gt;TRAINING.md&lt;/a&gt; for training and fine-tuning instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repository is built using the &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;timm&lt;/a&gt; library, &lt;a href=&#34;https://github.com/facebookresearch/deit&#34;&gt;DeiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;BEiT&lt;/a&gt; repositories.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the MIT license. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ConvNeXt/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{liu2022convnet,&#xA;  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},&#xA;  title   = {A ConvNet for the 2020s},&#xA;  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year    = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>