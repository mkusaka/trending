<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-22T01:39:29Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>spyboy-productions/CloakQuest3r</title>
    <updated>2023-12-22T01:39:29Z</updated>
    <id>tag:github.com,2023-12-22:/spyboy-productions/CloakQuest3r</id>
    <link href="https://github.com/spyboy-productions/CloakQuest3r" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Uncover the true IP address of websites safeguarded by Cloudflare &amp; Others&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://spyboy.in/twitter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-TWITTER-black?logo=twitter&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://spyboy.in/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-spyboy.in-black?logo=google&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://spyboy.blog/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-spyboy.blog-black?logo=wordpress&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://spyboy.in/Discord&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Discord-black?logo=discord&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;img width=&#34;100%&#34; align=&#34;centre&#34; src=&#34;https://cdn.discordapp.com/attachments/1141162711464550430/1185865562627252274/dwsda.png&#34;&gt; &#xA;&lt;be&gt; &#xA; &lt;p&gt;CloakQuest3r is a powerful Python tool meticulously crafted to uncover the true IP address of websites safeguarded by Cloudflare and other alternatives, a widely adopted web security and performance enhancement service. Its core mission is to accurately discern the actual IP address of web servers that are concealed behind Cloudflare&#39;s protective shield. Subdomain scanning is employed as a key technique in this pursuit. This tool is an invaluable resource for penetration testers, security professionals, and web administrators seeking to perform comprehensive security assessments and identify vulnerabilities that may be obscured by Cloudflare&#39;s security measures.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real IP Detection:&lt;/strong&gt; CloakQuest3r excels in the art of discovering the real IP address of web servers employing Cloudflare&#39;s services. This crucial information is paramount for conducting comprehensive penetration tests and ensuring the security of web assets.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Subdomain Scanning:&lt;/strong&gt; Subdomain scanning is harnessed as a fundamental component in the process of finding the real IP address. It aids in the identification of the actual server responsible for hosting the website and its associated subdomains.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;IP address History:&lt;/strong&gt; Retrieve historical IP address information for a given domain. It uses the ViewDNS service to fetch and display details such as IP address, location, owner, and last seen date.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SSL Certificate Analysis:&lt;/strong&gt; Extract and analyze SSL certificates associated with the target domain. This could provide additional information about the hosting infrastructure and potentially reveal the true IP address.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Threaded Scanning:&lt;/strong&gt; To enhance efficiency and expedite the real IP detection process, CloakQuest3r utilizes threading. This feature enables scanning of a substantial list of subdomains without significantly extending the execution time.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Detailed Reporting:&lt;/strong&gt; The tool provides comprehensive output, including the total number of subdomains scanned, the total number of subdomains found, and the time taken for the scan. Any real IP addresses unveiled during the process are also presented, facilitating in-depth analysis and penetration testing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;With CloakQuest3r, you can confidently evaluate website security, unveil hidden vulnerabilities, and secure your web assets by disclosing the true IP address concealed behind Cloudflare&#39;s protective layers.&lt;/p&gt; &#xA; &lt;h4&gt;Limitation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;&#xA;- Sometimes it can&#39;t detect the real Ip.&#xA;&#xA;- CloakQuest3r combines multiple indicators to uncover real IP addresses behind Cloudflare. While subdomain scanning is a part of the process, we do not assume that all subdomains&#39; A records point to the target host. The tool is designed to provide valuable insights but may not work in every scenario. We welcome any specific suggestions for improvement. &#xA;&#xA;1. False Negatives: CloakReveal3r may not always accurately identify the real IP address behind Cloudflare, particularly for websites with complex network configurations or strict security measures.&#xA;&#xA;2. Dynamic Environments: Websites&#39; infrastructure and configurations can change over time. The tool may not capture these changes, potentially leading to outdated information.&#xA;&#xA;3. Subdomain Variation: While the tool scans subdomains, it doesn&#39;t guarantee that all subdomains&#39; A records will point to the primary host. Some subdomains may also be protected by Cloudflare.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; This tool is a Proof of Concept and is for Educational Purposes Only. &lt;/h4&gt; &#xA; &lt;hr&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; OS compatibility : &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/Windows-05122A?style=for-the-badge&amp;amp;logo=windows&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Linux-05122A?style=for-the-badge&amp;amp;logo=linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Android-05122A?style=for-the-badge&amp;amp;logo=android&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/macOS-05122A?style=for-the-badge&amp;amp;logo=macos&#34;&gt; &lt;/h4&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; Requirements: &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/Python-05122A?style=for-the-badge&amp;amp;logo=python&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Git-05122A?style=for-the-badge&amp;amp;logo=git&#34;&gt; &lt;/h4&gt; &#xA; &lt;p&gt;&lt;strong&gt;How to Use:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Run CloudScan with a single command-line argument: the target domain you want to analyze.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone https://github.com/spyboy-productions/CloakQuest3r.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;cd CloakQuest3r&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;For Termux(android) User&lt;/code&gt; use the command given below if having trouble installing &lt;code&gt;cryptography&lt;/code&gt; using requirements.txt&lt;/p&gt; &lt;p&gt;&lt;code&gt;pkg install python-cryptography&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python cloakquest3r.py example.com&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;The tool will check if the website is using Cloudflare. If not, it will inform you and ask if you still want to proceed.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;If Cloudflare is detected, CloudScan will scan for subdomains and identify their real IP addresses.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;You will receive detailed output, including the number of subdomains scanned, the total number of subdomains found, and the time taken for the scan.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Any real IP addresses found will be displayed, allowing you to conduct further analysis and penetration testing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;CloudScan simplifies the process of assessing website security by providing a clear, organized, and informative report. Use it to enhance your security assessments, identify potential vulnerabilities, and secure your web assets.&lt;/p&gt; &#xA; &lt;h4&gt;Run It Online on replit.com&lt;/h4&gt; &#xA; &lt;p&gt;It is just a demo and not all functionality is available. Please install the tool to access its full potential.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://replit.com/@spyb0y/CloakQuest3r&#34;&gt;&lt;img src=&#34;https://repl.it/badge/github/spyboy-productions/CloakQuest3r&#34; alt=&#34;Run on Repl.it&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;h4&gt;Contribution:&lt;/h4&gt; &#xA; &lt;p&gt;Contributions and feature requests are welcome! If you encounter any issues or have ideas for improvement, feel free to open an issue or submit a pull request.&lt;/p&gt; &#xA; &lt;h4&gt;üò¥ü•±üò™üí§ ToDo:&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Add free API (ex: securitytrails)&lt;/li&gt; &#xA;  &lt;li&gt;Discover IP through website API calls (POC)&lt;/li&gt; &#xA;  &lt;li&gt;Save all info on a Txt/CSV file.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;üí¨ If having an issue &lt;a href=&#34;https://discord.gg/ZChEmMwE8d&#34;&gt;Chat here&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/ZChEmMwE8d&#34;&gt;&lt;img src=&#34;https://discord.com/api/guilds/726495265330298973/embed.png&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;‚≠î Snapshots:&lt;/h3&gt; &#xA; &lt;hr&gt; &#xA; &lt;img width=&#34;100%&#34; align=&#34;centre&#34; src=&#34;https://cdn.discordapp.com/attachments/1141162711464550430/1185878687388807238/Screenshot_2023-12-16_at_4.51.00_PM.png&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; align=&#34;centre&#34; src=&#34;https://cdn.discordapp.com/attachments/1141162711464550430/1185878687820828742/Screenshot_2023-12-16_at_4.51.45_PM.png&#34;&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; If you find this GitHub repo useful, please consider giving it a star! ‚≠êÔ∏è &lt;/h4&gt; &#xA;&lt;/be&gt;</summary>
  </entry>
  <entry>
    <title>andrew-codechimp/HA-Battery-Notes</title>
    <updated>2023-12-22T01:39:29Z</updated>
    <id>tag:github.com,2023-12-22:/andrew-codechimp/HA-Battery-Notes</id>
    <link href="https://github.com/andrew-codechimp/HA-Battery-Notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Home Assistant integration to provide battery notes of devices&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Home Assistant Battery Notes&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/andrew-codechimp/HA-Battery-Notes/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/andrew-codechimp/HA-Battery-Notes.svg?style=for-the-badge&#34; alt=&#34;GitHub Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/andrew-codechimp/HA-Battery-Notes/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/y/andrew-codechimp/HA-Battery-Notes.svg?style=for-the-badge&#34; alt=&#34;GitHub Activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/andrew-codechimp/HA-Battery-Notes/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/andrew-codechimp/HA-Battery-Notes.svg?style=for-the-badge&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hacs/integration&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HACS-Default-41BDF5.svg?style=for-the-badge&#34; alt=&#34;hacs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://community.home-assistant.io/t/custom-component-battery-notes/613821&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/community-forum-brightgreen.svg?style=for-the-badge&#34; alt=&#34;Community Forum&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Integration to add battery notes to a device, with automatic discovery via a growing battery library for devices&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please &lt;span&gt;‚≠ê&lt;/span&gt; this repo if you find it useful&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/andrew-codechimp/HA-Battery-Notes/raw/main/images/screenshot-device.png&#34; alt=&#34;Battery Notes&#34; title=&#34;Battery Notes&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This integration will set up the following platforms.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Platform&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sensor&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Show battery type.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;HACS&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://my.home-assistant.io/redirect/hacs_repository/?owner=andrew-codechimp&amp;amp;repository=HA-Battery-Notes&amp;amp;category=Integration&#34;&gt;&lt;img src=&#34;https://my.home-assistant.io/badges/hacs_repository.svg?sanitize=true&#34; alt=&#34;Open your Home Assistant instance and open a repository inside the Home Assistant Community Store.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or Search for &lt;code&gt;Battery Notes&lt;/code&gt; in HACS and install it under the &#34;Integrations&#34; category.&lt;/p&gt; &#xA;&lt;p&gt;Restart Home Assistant&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Add the following entry to your &lt;code&gt;configuration.yaml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;battery_notes:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Restart Home Assistant a final time In the HA UI go to &#34;Configuration&#34; -&amp;gt; &#34;Integrations&#34; click &#34;+&#34; and search for &#34;Battery Notes&#34;&lt;/p&gt; &#xA;&lt;h3&gt;Manual Installation&lt;/h3&gt; &#xA;&lt;p&gt;Using the tool of choice open the directory (folder) for your HA configuration (where you find &lt;code&gt;configuration.yaml&lt;/code&gt;). If you do not have a &lt;code&gt;custom_components&lt;/code&gt; directory (folder) there, you need to create it. In the &lt;code&gt;custom_components&lt;/code&gt; directory (folder) create a new folder called &lt;code&gt;battery_notes&lt;/code&gt;. Download &lt;em&gt;all&lt;/em&gt; the files from the &lt;code&gt;custom_components/battery_notes/&lt;/code&gt; directory (folder) in this repository. Place the files you downloaded in the new directory (folder) you created. Restart Home Assistant Add the following entry to your &lt;code&gt;configuration.yaml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;battery_notes:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Restart Home Assistant a final time In the HA UI go to &#34;Configuration&#34; -&amp;gt; &#34;Integrations&#34; click &#34;+&#34; and search for &#34;Battery Notes&#34;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration is done in the UI&lt;/h2&gt; &#xA;&lt;p&gt;On the &#34;Configuration&#34; -&amp;gt; &#34;Integrations&#34; -&amp;gt; &#34;Battery Notes&#34; screen add a new device, pick your device with a battery and add the battery type. The battery type will then be displayed as a diagnostic sensor on the device.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&#39;s&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When is the library updated?&lt;br&gt; It updates when Home Assistant is restarted and approximately every 24 hours after that.&lt;br&gt; It will pull the latest devices that have been merged into the main branch, if you have recently submitted a pull request for a new device it will not appear until it has been manually reviewed and merged.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How do I remove a battery note on a device?&lt;br&gt; Go into the Settings -&amp;gt; Integrations -&amp;gt; Battery Notes, use the menu on the right of a device and select Delete, this will only delete the battery note, not the whole device.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Why does the device icon change?&lt;br&gt; Unfortunately where there are multiple integrations associated with a device Home Assistant seems to choose an icon at random, I have no control over this.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Can I edit a battery note?&lt;br&gt; Go into Settings -&amp;gt; Integrations -&amp;gt; Battery Notes and click Configure on the device you want to edit.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Automatic discovery&lt;/h2&gt; &#xA;&lt;p&gt;Battery Notes will automatically discover devices (as long as you have followed the installation instructions above) that it has in its library and create a notification to add a battery note. &lt;img src=&#34;https://github.com/andrew-codechimp/HA-Battery-Notes/raw/main/images/screenshot-discovery.png&#34; alt=&#34;Discovery&#34; title=&#34;Device Discovery&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you wish to disable this functionality then change your &lt;code&gt;configuration.yaml&lt;/code&gt; to set enable_autodiscovery to false&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;battery_notes:&#xA;  enable_autodiscovery: false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing to the Battery Library&lt;/h2&gt; &#xA;&lt;!-- To add a device definition to the battery library so that it will be automatically configured there are two options:&#xA;&#xA;### Submit Definition via GitHub Issues Form&#xA;&#xA;To add a new device via GitHub Issues, fill out [this form (BETA)](https://github.com/andrew-codechimp/HA-Battery-Notes/issues/new?template=new_device_request.yml&amp;title=[Device]%3A+).&#xA;Upon submission of the issue, GitHub will attempt to make the required code changes automatically.&#xA;&#xA;### Submit Definition via Pull Request&#xA;&#xA;If you have issues with the form, or if you feel more comfortable editing JSON data, you can directly add definitions to [the library.json file](custom_components/battery_notes/data/library.json). --&gt; &#xA;&lt;p&gt;Fork the repository, add your device details to the JSON document &lt;code&gt;custom_components/battery_notes/data/library.json&lt;/code&gt;, and then submit a pull request.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The manufacturer and model should be exactly what is displayed on the Device screen within Home Assistant.&lt;/li&gt; &#xA; &lt;li&gt;The make &amp;amp; model names may be different between integrations such as Zigbee2MQTT and ZHA, if you see a similar device please duplicate the entry rather than changing it.&lt;/li&gt; &#xA; &lt;li&gt;Please keep devices in alphabetical order by manufacturer/model.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;battery_quantity&lt;/code&gt; data is numeric (no quotes) and optional. If a device only requires a single battery, it should be omitted.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;battery_type&lt;/code&gt; data should follow the most common naming for general batteries (ex. AAA, D) and the IEC naming for battery cells according to &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_battery_sizes&#34;&gt;Wikipedia&lt;/a&gt; (ex. CR2032, 18650)&lt;/li&gt; &#xA; &lt;li&gt;If a device has a bespoke rechargeable battery you can use &lt;code&gt;&#34;battery_type&#34;: &#34;Rechargeable&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For devices like smoke alarms where the battery is not replaceable you can use &lt;code&gt;&#34;battery_type&#34;: &#34;Irreplaceable&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For the example image below, your JSON entry will look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;manufacturer&#34;: &#34;Philips&#34;,&#xA;    &#34;model&#34;: &#34;Hue motion sensor (9290012607)&#34;,&#xA;    &#34;battery_type&#34;: &#34;AAA&#34;,&#xA;    &#34;battery_quantity&#34;: 2&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/andrew-codechimp/HA-Battery-Notes/raw/main/images/screenshot-device-info.png&#34; alt=&#34;Device Details&#34; title=&#34;Device Details&#34;&gt;&lt;/p&gt; &#xA;&lt;!----&gt; &#xA;&lt;h2&gt;Contributions are welcome!&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute to this please read the &lt;a href=&#34;https://raw.githubusercontent.com/andrew-codechimp/HA-Battery-Notes/main/CONTRIBUTING.md&#34;&gt;Contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;A lot of the inspiration for this integration came from the excellent &lt;a href=&#34;https://github.com/bramstroker/homeassistant-powercalc&#34;&gt;PowerCalc by bramstroker&lt;/a&gt;, without adapting code from PowerCalc I&#39;d never have worked out how to add additional sensors to a device.&lt;/p&gt; &#xA;&lt;!-- Huge thanks to @bmos for creating the issue form &amp; automations for adding new devices. COMING SOON --&gt; &#xA;&lt;p&gt;Thanks to everyone who has submitted devices to the library.&lt;/p&gt; &#xA;&lt;!----&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/distil-whisper</title>
    <updated>2023-12-22T01:39:29Z</updated>
    <id>tag:github.com,2023-12-22:/huggingface/distil-whisper</id>
    <link href="https://github.com/huggingface/distil-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Distilled variant of Whisper for speech recognition. 6x faster, 50% smaller, within 1% word error rate.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Distil-Whisper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.00430&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6&#34;&gt;[Models]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/Distil_Whisper_Benchmark.ipynb&#34;&gt;[Colab]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/training&#34;&gt;[Training Code]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Distil-Whisper is a distilled version of Whisper that is &lt;strong&gt;6 times faster&lt;/strong&gt;, 49% smaller, and performs &lt;strong&gt;within 1% word error rate (WER)&lt;/strong&gt; on out-of-distribution evaluation sets:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Params / M&lt;/th&gt; &#xA;   &lt;th&gt;Rel. Latency ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;Short-Form WER ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;Long-Form WER ‚Üì&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;large-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1550&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2&#34;&gt;distil-large-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;756&lt;/td&gt; &#xA;   &lt;td&gt;5.8&lt;/td&gt; &#xA;   &lt;td&gt;10.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;11.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en&#34;&gt;distil-medium.en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;394&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11.1&lt;/td&gt; &#xA;   &lt;td&gt;12.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en&#34;&gt;distil-small.en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;166&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;5.6&lt;/td&gt; &#xA;   &lt;td&gt;12.1&lt;/td&gt; &#xA;   &lt;td&gt;12.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For applications where latency and accuracy are important, we recommend the &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en&#34;&gt;distil-medium.en&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2&#34;&gt;distil-large-v2&lt;/a&gt; checkpoints. For resource-constrained applications, such as on-device or mobile applications, the &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en&#34;&gt;distil-small.en&lt;/a&gt; is a great choice, since it is only 166M parameters, while performing within 3% WER of Whisper large-v2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Distil-Whisper is currently only available for English speech recognition. We are working with the community to distill Whisper on other languages. If you are interested in distilling Whisper in your language, check out the provided &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/training&#34;&gt;training code&lt;/a&gt;. We will soon update the repository with multilingual checkpoints when ready!&lt;/p&gt; &#xA;&lt;h2&gt;1. Usage&lt;/h2&gt; &#xA;&lt;p&gt;Distil-Whisper is supported in Hugging Face ü§ó Transformers from version 4.35 onwards. To run the model, first install the latest version of the Transformers library. For this example, we&#39;ll also install ü§ó Datasets to load a toy audio dataset from the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install --upgrade transformers accelerate datasets[audio]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Short-Form Transcription&lt;/h3&gt; &#xA;&lt;p&gt;Short-form transcription is the process of transcribing audio samples that are less than 30-seconds long, which is the maximum receptive field of the Whisper models. This means the entire audio clip can be processed in one go without the need for chunking.&lt;/p&gt; &#xA;&lt;p&gt;First, we load Distil-Whisper via the convenient &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSpeechSeq2Seq&#34;&gt;&lt;code&gt;AutoModelForSpeechSeq2Seq&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoProcessor&#34;&gt;&lt;code&gt;AutoProcessor&lt;/code&gt;&lt;/a&gt; classes.&lt;/p&gt; &#xA;&lt;p&gt;We load the model in &lt;code&gt;float16&lt;/code&gt; precision and make sure that loading time takes as little time as possible by passing &lt;code&gt;low_cpu_mem_usage=True&lt;/code&gt;. In addition, we want to make sure that the model is loaded in &lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;&lt;code&gt;safetensors&lt;/code&gt;&lt;/a&gt; format by passing &lt;code&gt;use_safetensors=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32&#xA;&#xA;model_id = &#34;distil-whisper/distil-large-v2&#34;&#xA;&#xA;model = AutoModelForSpeechSeq2Seq.from_pretrained(&#xA;    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True&#xA;)&#xA;model.to(device)&#xA;&#xA;processor = AutoProcessor.from_pretrained(model_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model and processor can then be passed to the &lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline&#34;&gt;&lt;code&gt;pipeline&lt;/code&gt;&lt;/a&gt;. Note that if you would like to have more control over the generation process, you can directly make use of &lt;code&gt;model.generate(...)&lt;/code&gt; as shown &lt;a href=&#34;https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = pipeline(&#xA;    &#34;automatic-speech-recognition&#34;,&#xA;    model=model,&#xA;    tokenizer=processor.tokenizer,&#xA;    feature_extractor=processor.feature_extractor,&#xA;    max_new_tokens=128,&#xA;    torch_dtype=torch_dtype,&#xA;    device=device,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we load an example short-form audio from the LibriSpeech corpus:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#34;hf-internal-testing/librispeech_asr_dummy&#34;, &#34;clean&#34;, split=&#34;validation&#34;)&#xA;sample = dataset[0][&#34;audio&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we can call the pipeline to transcribe the audio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = pipe(sample)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = pipe(&#34;audio.mp3&#34;)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on how to customize the automatic speech recognition pipeline, please refer to the ASR pipeline &lt;a href=&#34;https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline&#34;&gt;docs&lt;/a&gt;. We also provide an end-to-end &lt;a href=&#34;https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/Distil_Whisper_Benchmark.ipynb&#34;&gt;Google Colab&lt;/a&gt; that benchmarks Whisper against Distil-Whisper.&lt;/p&gt; &#xA;&lt;h3&gt;Long-Form Transcription&lt;/h3&gt; &#xA;&lt;p&gt;Distil-Whisper uses a chunked algorithm to transcribe long-form audio files longer than 30-seconds. In practice, this chunked long-form algorithm is 9x faster than the sequential algorithm proposed by OpenAI in the Whisper paper (see Table 7 of the &lt;a href=&#34;https://arxiv.org/abs/2311.00430&#34;&gt;Distil-Whisper paper&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We can load the model and processor as before:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32&#xA;&#xA;model_id = &#34;distil-whisper/distil-large-v2&#34;&#xA;&#xA;model = AutoModelForSpeechSeq2Seq.from_pretrained(&#xA;    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True&#xA;)&#xA;model.to(device)&#xA;&#xA;processor = AutoProcessor.from_pretrained(model_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable chunking, pass the &lt;code&gt;chunk_length_s&lt;/code&gt; parameter to the &lt;code&gt;pipeline&lt;/code&gt;. For Distil-Whisper, a chunk length of 15-seconds is optimal. To activate batching, pass the argument &lt;code&gt;batch_size&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = pipeline(&#xA;    &#34;automatic-speech-recognition&#34;,&#xA;    model=model,&#xA;    tokenizer=processor.tokenizer,&#xA;    feature_extractor=processor.feature_extractor,&#xA;    max_new_tokens=128,&#xA;    chunk_length_s=15,&#xA;    batch_size=16,&#xA;    torch_dtype=torch_dtype,&#xA;    device=device,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The argument &lt;code&gt;max_new_tokens&lt;/code&gt; controls the maximum number of generated tokens &lt;em&gt;per-chunk&lt;/em&gt;. In the typical speech setting, we have no more than 3 words spoken per-second. Therefore, for a 15-second input, we have at most 45 words (approx 60 tokens). We set the maximum number of generated tokens per-chunk to 128 to truncate any possible hallucinations that occur at the end of the segment. These tokens get removed at the chunk borders using the long-form chunking transcription algorithm, so it is more efficient to truncate them directly during generation to avoid redundant generation steps in the decoder.&lt;/p&gt; &#xA;&lt;p&gt;Now, let&#39;s load a long-form audio sample. Here, we use an example of concatenated samples from the LibriSpeech corpus:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#34;distil-whisper/librispeech_long&#34;, &#34;clean&#34;, split=&#34;validation&#34;)&#xA;sample = dataset[0][&#34;audio&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we can call the pipeline to transcribe the audio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = pipe(sample)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;**Tip:** The pipeline can also be used to transcribe an audio file from a remote URL, for example:&#xA;&#xA;```python&#xA;result = pipe(&#34;https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav&#34;)&#xA;```&#xA;---&gt; &#xA;&lt;p&gt;For more information on how to customize the automatic speech recognition pipeline, please refer to the ASR pipeline &lt;a href=&#34;https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Speculative Decoding&lt;/h3&gt; &#xA;&lt;p&gt;Distil-Whisper can be used as an assistant model to Whisper for &lt;a href=&#34;https://huggingface.co/blog/whisper-speculative-decoding&#34;&gt;speculative decoding&lt;/a&gt;. Speculative decoding mathematically ensures the exact same outputs as Whisper are obtained while being 2 times faster. This makes it the perfect drop-in replacement for existing Whisper pipelines, since the same outputs are guaranteed.&lt;/p&gt; &#xA;&lt;p&gt;For speculative decoding, we need to load both the teacher: &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;&lt;code&gt;openai/whisper-large-v2&lt;/code&gt;&lt;/a&gt;. As well as the assistant (&lt;em&gt;a.k.a&lt;/em&gt; student) &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2&#34;&gt;&lt;code&gt;distil-whisper/distil-large-v2&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s start by loading the teacher model and processor. We do this in much the same way we loaded the Distil-Whisper model in the previous examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor&#xA;import torch&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32&#xA;&#xA;model_id = &#34;openai/whisper-large-v2&#34;&#xA;&#xA;model = AutoModelForSpeechSeq2Seq.from_pretrained(&#xA;    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True&#xA;)&#xA;model.to(device)&#xA;&#xA;processor = AutoProcessor.from_pretrained(model_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s load the assistant. Since Distil-Whisper shares exactly same encoder as the teacher model, we only need to load the 2-layer decoder as a &#34;Decoder-only&#34; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM&#xA;assistant_model_id = &#34;distil-whisper/distil-large-v2&#34;&#xA;&#xA;assistant_model = AutoModelForCausalLM.from_pretrained(&#xA;    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True&#xA;)&#xA;assistant_model.to(device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The assistant model shares the same processor as the teacher, so there&#39;s no need to load a student processor.&lt;/p&gt; &#xA;&lt;p&gt;We can now pass the assistant model to the pipeline to be used for speculative decoding. We pass it as a &lt;code&gt;generate_kwarg&lt;/code&gt; with the key &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate.assistant_model&#34;&gt;&lt;code&gt;&#34;assistant_model&#34;&lt;/code&gt;&lt;/a&gt; so that speculative decoding is enabled:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = pipeline(&#xA;    &#34;automatic-speech-recognition&#34;,&#xA;    model=model,&#xA;    tokenizer=processor.tokenizer,&#xA;    feature_extractor=processor.feature_extractor,&#xA;    max_new_tokens=128,&#xA;    generate_kwargs={&#34;assistant_model&#34;: assistant_model},&#xA;    torch_dtype=torch_dtype,&#xA;    device=device,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As before, we can pass any sample to the pipeline to be transcribed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#34;hf-internal-testing/librispeech_asr_dummy&#34;, &#34;clean&#34;, split=&#34;validation&#34;)&#xA;sample = dataset[0][&#34;audio&#34;]&#xA;&#xA;result = pipe(sample)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; speculative decoding should be on average 2x faster than using &#34;only&#34; Whisper large-v2 at a mere 8% increase in VRAM memory usage while mathematically ensuring the same results. This makes it the perfect replacement for Whisper large-v2 in existing speech recognition pipelines.&lt;/p&gt; &#xA;&lt;p&gt;For more details on speculative decoding, refer to the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/whisper-speculative-decoding&#34;&gt;Speculative decoding for 2x faster Whisper inference&lt;/a&gt; blog post by Sanchit Gandhi&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/assisted-generation&#34;&gt;Assisted Generation: a new direction toward low-latency text generation&lt;/a&gt; blog post by Joao Gante&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.17192&#34;&gt;Fast Inference from Transformers via Speculative Decoding&lt;/a&gt; paper by Leviathan et. al.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Additional Speed &amp;amp; Memory Improvements&lt;/h3&gt; &#xA;&lt;p&gt;You can apply additional speed and memory improvements to Distil-Whisper which we cover in the following.&lt;/p&gt; &#xA;&lt;h4&gt;Flash Attention&lt;/h4&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2&#34;&gt;Flash Attention 2&lt;/a&gt; if your GPU allows for it. To do so, you first need to install &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then pass &lt;code&gt;use_flash_attention_2=True&lt;/code&gt; to &lt;code&gt;from_pretrained&lt;/code&gt; to enable Flash Attention 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)&#xA;+ model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, use_flash_attention_2=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Torch Scale-Product-Attention (SDPA)&lt;/h4&gt; &#xA;&lt;p&gt;If your GPU does not support Flash Attention, we recommend making use of &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#bettertransformer&#34;&gt;BetterTransformers&lt;/a&gt;. To do so, you first need to install optimum:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade optimum&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then convert your model to a &#34;BetterTransformer&#34; model before using it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)&#xA;+ model = model.to_bettertransformer()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Exporting to Other Libraries&lt;/h3&gt; &#xA;&lt;p&gt;Distil-Whisper has support in the following libraries with the original &#34;sequential&#34; long-form transcription algorithm. Click the links in the table to see the relevant code-snippets for each:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;distil-small.en&lt;/th&gt; &#xA;   &lt;th&gt;distil-medium.en&lt;/th&gt; &#xA;   &lt;th&gt;distil-large-v2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI Whisper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en#running-whisper-in-openai-whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en#running-whisper-in-openai-whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2#running-whisper-in-openai-whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Whisper cpp&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en#whispercpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en#whispercpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2#whispercpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers js&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en#transformersjs&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en#transformersjs&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2#transformersjs&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Candle (Rust)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-small.en#candle&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en#candle&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2#candle&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Updates will be posted here with the integration of the &#34;chunked&#34; long-form transcription algorithm into the respective libraries.&lt;/p&gt; &#xA;&lt;p&gt;For the ü§ó Transformers code-examples, refer to the sections &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/#short-form-transcription&#34;&gt;Short-Form&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/#long-form-transcription&#34;&gt;Long-Form&lt;/a&gt; Transcription.&lt;/p&gt; &#xA;&lt;h2&gt;2. Why use Distil-Whisper? ‚ÅâÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Distil-Whisper is designed to be a drop-in replacement for Whisper on English speech recognition. Here are 5 reasons for making the switch to Distil-Whisper:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster inference:&lt;/strong&gt; 6 times faster inference speed, while performing to within 1% WER of Whisper on out-of-distribution audio:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/distil-whisper/figures/resolve/main/main_table.png?raw=true&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robustness to noise:&lt;/strong&gt; demonstrated by strong WER performance at low signal-to-noise ratios:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/distil-whisper/figures/resolve/main/noise.png?raw=true&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robustness to hallucinations:&lt;/strong&gt; quantified by 1.3 times fewer repeated 5-gram word duplicates (5-Dup.) and 2.1% lower insertion error rate (IER) than Whisper:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/distil-whisper/figures/resolve/main/hallucination.png?raw=true&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Designed for speculative decoding:&lt;/strong&gt; Distil-Whisper can be used as an assistant model to Whisper, giving 2 times faster inference speed while mathematically ensuring the same outputs as the Whisper model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Permissive license:&lt;/strong&gt; Distil-Whisper is &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/LICENSE&#34;&gt;MIT licensed&lt;/a&gt;, meaning it can be used for commercial applications.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;3. Approach ‚úçÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;To distill Whisper, we copy the entire encoder module and freeze it during training. We copy only two decoder layers, which are initialised from the first and last decoder layers from Whisper. All other decoder layers from Whisper are discarded:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/distil-whisper/figures/resolve/main/architecture.png?raw=true&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Distil-Whisper is trained on a &lt;em&gt;knowledge distillation&lt;/em&gt; objective. Specifically, it is trained to minimise the KL divergence between the distilled model and the Whisper model, as well as the cross-entropy loss on pseudo-labelled audio data.&lt;/p&gt; &#xA;&lt;p&gt;We train Distil-Whisper on a total of 22k hours of pseudo-labelled audio data, spanning 10 domains with over 18k speakers:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/distil-whisper/figures/resolve/main/datasets.png?raw=true&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This diverse audio dataset is paramount to ensuring robustness of Distil-Whisper to different datasets and domains.&lt;/p&gt; &#xA;&lt;p&gt;In addition, we use a WER filter to discard pseudo-labels where Whisper mis-transcribes or hallucinates. This greatly improves WER performance of the downstream distilled model.&lt;/p&gt; &#xA;&lt;p&gt;For full details on the distillation set-up and evaluation results, refer to the &lt;a href=&#34;https://arxiv.org/abs/2311.00430&#34;&gt;Distil-Whisper paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Training Code&lt;/h2&gt; &#xA;&lt;p&gt;Training code to reproduce Distil-Whisper can be found in the directory &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/distil-whisper/main/training&#34;&gt;training&lt;/a&gt;. This code has been adapted be general enough to distill Whisper for multilingual speech recognition, facilitating anyone in the community to distill Whisper on their choice of language.&lt;/p&gt; &#xA;&lt;h2&gt;5. Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI for the Whisper &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;model&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;original codebase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face ü§ó &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt; for the model integration&lt;/li&gt; &#xA; &lt;li&gt;Google&#39;s &lt;a href=&#34;https://sites.research.google/trc/about/&#34;&gt;TPU Research Cloud (TRC)&lt;/a&gt; program for Cloud TPU v4s&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;6. Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this model, please consider citing the Distil-Whisper paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gandhi2023distilwhisper,&#xA;      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling}, &#xA;      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},&#xA;      year={2023},&#xA;      eprint={2311.00430},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And also the Whisper paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{radford2022robust,&#xA;      title={Robust Speech Recognition via Large-Scale Weak Supervision}, &#xA;      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},&#xA;      year={2022},&#xA;      eprint={2212.04356},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={eess.AS}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>