<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-07T01:44:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mpoon/gpt-repository-loader</title>
    <updated>2023-05-07T01:44:33Z</updated>
    <id>tag:github.com,2023-05-07:/mpoon/gpt-repository-loader</id>
    <link href="https://github.com/mpoon/gpt-repository-loader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert code repos into an LLM prompt-friendly format. Mostly built by GPT-4.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-repository-loader&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;gpt-repository-loader&lt;/code&gt; is a command-line tool that converts the contents of a Git repository into a text format, preserving the structure of the files and file contents. The generated output can be interpreted by AI language models, allowing them to process the repository&#39;s contents for various tasks, such as code review or documentation generation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Some context around building this is &lt;a href=&#34;https://github.com/mpoon/gpt-repository-loader/discussions/18&#34;&gt;located here&lt;/a&gt;. Appreciate any issues and pull requests in the spirit of having mostly GPT build out this tool. Using &lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT Plus&lt;/a&gt; is recommended for quick access to GPT-4.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with &lt;code&gt;gpt-repository-loader&lt;/code&gt;, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have Python 3 installed on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone or download the &lt;code&gt;gpt-repository-loader&lt;/code&gt; repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the repository&#39;s root directory in your terminal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;gpt-repository-loader&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt_repository_loader.py /path/to/git/repository [-p /path/to/preamble.txt] [-o /path/to/output_file.txt]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &lt;code&gt;/path/to/git/repository&lt;/code&gt; with the path to the Git repository you want to process. Optionally, you can specify a preamble file with -p or an output file with -o. If not specified, the default output file will be named output.txt in the current directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The tool will generate an output.txt file containing the text representation of the repository. You can now use this file as input for AI language models or other text-based processing tasks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests for &lt;code&gt;gpt-repository-loader&lt;/code&gt;, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have Python 3 installed on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the repository&#39;s root directory in your terminal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the tests with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m unittest test_gpt_repository_loader.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, the test harness is added to the &lt;code&gt;gpt-repository-loader&lt;/code&gt; project. You can run the tests by executing the command &lt;code&gt;python -m unittest test_gpt_repository_loader.py&lt;/code&gt; in your terminal.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dandelionsllm/pandallm</title>
    <updated>2023-05-07T01:44:33Z</updated>
    <id>tag:github.com,2023-05-07:/dandelionsllm/pandallm</id>
    <link href="https://github.com/dandelionsllm/pandallm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Panda: 海外中文开源大语言模型，基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练。&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/panda_logo.PNG&#34; alt=&#34;Panda&#34; style=&#34;width: 60%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Panda: 海外中文开源大语言模型&lt;/h1&gt; &#xA;&lt;p&gt;欢迎来到我们的海外中文大语言模型开源项目—— Panda！Panda 系列语言模型目前基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练, 使用了接近 15M 条数据, 并针对推理能力在中文 benchmark 上进行了评测, 希望能够为中文自然语言处理领域提供具有泛用性的通用基础工具.&lt;/p&gt; &#xA;&lt;!-- 该项目旨在提供一款开源、高质量的中文大语言模型，能够支持各种自然语言处理任务, 并且特别注重海外华人使用体验。--&gt; &#xA;&lt;p&gt;我们的 Panda 模型以及训练涉及的中文数据集将以开源形式发布，任何人都可以免费使用并参与开发。我们欢迎来自全球的开发者一起参与到该项目中，共同推动中文自然语言处理技术的发展。我们后续会进一步完善针对中文语言模型基础能力的评测，同时开放更大规模的模型。&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/#news&#34;&gt;最近更新&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/#model&#34;&gt;项目内容&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/#evaluation&#34;&gt;实验测评&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/#contribute&#34;&gt;如何参与&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dandelionsllm/pandallm/main/#acknowledge&#34;&gt;鸣谢&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 id=&#34;news&#34;&gt;最近更新&lt;/h2&gt; &#xA;&lt;p&gt;发布了大模型 &lt;strong&gt;Panda&lt;/strong&gt; 和 &lt;strong&gt;Flan-LLaMA&lt;/strong&gt; 的 technical report！&lt;/p&gt; &#xA;&lt;p&gt;论文链接： &lt;a href=&#34;https://arxiv.org/pdf/2305.03025&#34;&gt;https://arxiv.org/pdf/2305.03025&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如何引用我们：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{latex}&#34;&gt;@article{jiao2023panda,&#xA;  title={Panda {LLM}: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models},&#xA;  author={Jiao, Fangkai and Ding, Bosheng and Luo, Tianze and Mo, Zhanfeng},&#xA;  journal={arXiv preprint arXiv:2305.03025},&#xA;  year={2023},&#xA;  url={https://arxiv.org/pdf/2305.03025.pdf}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2 id=&#34;model&#34;&gt;项目内容&lt;/h2&gt; &#xA;&lt;h3&gt;Panda 模型&lt;/h3&gt; &#xA;&lt;p&gt;详见 Panda/train，我们集成了 Deepspeed 加速框架，支持模型 pretrain，finetune，lora 以及 distillation (后续推出).&lt;/p&gt; &#xA;&lt;p&gt;我们目前开放基于中英文语料库的训练与调优模型：Panda-7B 和 Panda-13B。&lt;/p&gt; &#xA;&lt;p&gt;模型版本：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;模型大小&lt;/th&gt; &#xA;   &lt;th&gt;下载链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-7B&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/chitanda/llama-panda-zh-7b-delta&#34;&gt;https://huggingface.co/chitanda/llama-panda-zh-7b-delta&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-Instruct-7B&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/chitanda/llama-panda-zh-coig-7b-delta&#34;&gt;https://huggingface.co/chitanda/llama-panda-zh-coig-7b-delta&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-Instruct-13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-LLaMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;因为 LLaMA 权重 License 的存在，我们无法直接发布完整的模型权重，因此我们放出了训练后模型的权重与原始 LLaMA 权重的差，保证能够获得 LLaMA 权重的用户能够同样使用这些模型。我们提供了一个&lt;a href=&#34;https://github.com/dandelionsllm/pandallm/raw/main/apply_delta.py&#34;&gt;脚本&lt;/a&gt;来帮助转换。&lt;/li&gt; &#xA; &lt;li&gt;由于模型训练期间使用了 &lt;code&gt;bfloat16&lt;/code&gt;，在非安培架构的显卡上直接使用 &lt;code&gt;fp16&lt;/code&gt; 格式进行微调时可能会出现无法收敛的情况，需要额外注意。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;数据&lt;/h2&gt; &#xA;&lt;p&gt;模型数据现阶段均采用开源的公开中英文语料数据集：&lt;/p&gt; &#xA;&lt;h3&gt;中文 instruction-tuning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;维基百科(wiki2019zh)，100万个结构良好的中文词条&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;新闻语料(news2016zh)，250万篇新闻，含关键词、描述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;百科问答(baike2018qa)，150万个带问题类型的问答&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;社区问答json版(webtext2019zh)，410万个高质量社区问答，适合训练超大模型&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;翻译语料(translation2019zh)，520万个中英文句子对&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BAAI/COIG&#34;&gt;Chinese Open Instruction Generalist (COIG)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;对于除维基百科和新闻语料外的其他语料，用 Conditional Generation 的方式优化，即 instruction 部分与输入部分不计算损失，只计算输出部分的损失。除 COIG 外的语料中的 instruction 为固定模板。&lt;/li&gt; &#xA; &lt;li&gt;一开始我们将以上所有语料混合在一起进行训练，但发现最终的模型在 instruction following 方面的能力并不好，因此我们决定单独在 COIG 数据集上进行指令微调，并得到最终模型。推测原因可能是 COIG 在整体训练数据中的占比过小，可选的解决方案是对 COIG 加大采样的概率。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;英文 instruction-tuning&lt;/h3&gt; &#xA;&lt;p&gt;为了提升模型的基础能力，我们选择使用 FLAN Collection 进行训练。由于 FLAN collection 语料规模过于庞大，我们按比例抽取了 7M 的语料用于训练，且最终性能仍远落后于 FLAN-T5-3B，因此目前我们决定暂时停止该方向的训练，并思考其他可能的构建较小的同时具有较强基础能力的语言模型的方向。&lt;/p&gt; &#xA;&lt;h2&gt;训练框架&lt;/h2&gt; &#xA;&lt;p&gt;Deepspeed Zero-1 + Gradient Checkpointing&lt;/p&gt; &#xA;&lt;h3&gt;模型训练&lt;/h3&gt; &#xA;&lt;p&gt;对应模型的训练时超参数见：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# LLaMA-7b pretrain on general Chinese Corpus&#xA;conf/llama/zh/llama_7b_zh_instruct_v1_0_ds.yaml&#xA;&#xA;# LLaMA-7b instruction tuning on COIG&#xA;conf/llama/zh/llama_7b_zh_instruct_coig_sft_v1_0_ds.yaml&#xA;&#xA;# LLaMA-13b pretrain on general Chinese Corpus (Ongoing)&#xA;conf/llama/zh/llama_13b_zh_instruct_v1_0_ds.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 deepspeed --include localhost:0,1,2,3,4,5,6,7 trainer_base_ds_mul.py -cp conf/llama/zh -cn &amp;lt;file name of yaml config&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;我们的训练使用了 2 * 8 * A100 80G GPU。如使用更少的显卡，请相应的调整 &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; 和 &lt;code&gt;per_gpu_train_batch_size&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2 id=&#34;evaluation&#34;&gt;实验测评&lt;/h2&gt; &#xA;&lt;h3&gt;基础能力测评&lt;/h3&gt; &#xA;&lt;h4&gt;测评数据集&lt;/h4&gt; &#xA;&lt;h5&gt;复杂推理&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/csitfun/LogiQA2.0&#34;&gt;LogiQA-v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dataset.org/c3/&#34;&gt;C3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;其他能力&lt;/h3&gt; &#xA;&lt;p&gt;测试进行中（欢迎数据集PR）&lt;/p&gt; &#xA;&lt;h3&gt;Baseline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE-LLaMA-Ext-7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CVI-SZU/Linly&#34;&gt;Linly-Chinese-LLaMA-7b-hf&lt;/a&gt; (Huggingface weights of chat-based model in 7B size are not released now. Corresponding results will be updated after weights are released)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;​&lt;/th&gt; &#xA;   &lt;th&gt;LogiQA-v2​&lt;/th&gt; &#xA;   &lt;th&gt;C3-d​&lt;/th&gt; &#xA;   &lt;th&gt;C3-m​&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BELLE-LLaMA-Ext-7B​&lt;/td&gt; &#xA;   &lt;td&gt;26.41​&lt;/td&gt; &#xA;   &lt;td&gt;29.52​&lt;/td&gt; &#xA;   &lt;td&gt;​28.87​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linly-Chinese-LLaMA-7b-hf​&lt;/td&gt; &#xA;   &lt;td&gt;25.91​&lt;/td&gt; &#xA;   &lt;td&gt;32.28​&lt;/td&gt; &#xA;   &lt;td&gt;34.52​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-7B​ (ours)&lt;/td&gt; &#xA;   &lt;td&gt;27.41​&lt;/td&gt; &#xA;   &lt;td&gt;43.02​&lt;/td&gt; &#xA;   &lt;td&gt;43.66​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-Instruct-7B 3k steps​ (ours)&lt;/td&gt; &#xA;   &lt;td&gt;26.22​&lt;/td&gt; &#xA;   &lt;td&gt;39.05​&lt;/td&gt; &#xA;   &lt;td&gt;42.11​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-Instruct-7B 6k steps​ (ours)&lt;/td&gt; &#xA;   &lt;td&gt;30.30​&lt;/td&gt; &#xA;   &lt;td&gt;47.14​&lt;/td&gt; &#xA;   &lt;td&gt;​56.94​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda-Instruct-7B 9k steps (ours)​&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;31.93&lt;/strong&gt;​&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;47.30&lt;/strong&gt;​&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57.04&lt;/strong&gt;​&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;由于模型对 instruction 的敏感性不同测评结果可能会有较大波动，测评结果仅供参考，并且可能无法完全反应模型之间的优劣。我们对于所有模型采用了最简单的 instruction（可以在对应数据集配置文件中找到）。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Linly-Chinese 在训练时可能在 instruction 中添加了额外的前缀（如用 assistant 和 user 去区分对话中的角色），在测试时对齐这一点可能会进一步提升性能，但我们目前为了统一instruction 并没有对齐。后续我们考虑收集多样化的 instruction 进行评测并汇报平均值。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 id=&#34;contribute&#34;&gt;如何参与&lt;/h2&gt; &#xA;&lt;p&gt;开发者可以通过贡献有用的代码、数据、论文和计算资源等方式成为贡献者。&lt;/p&gt; &#xA;&lt;p&gt;代码：包括算法实现、训练优化、推理优化和模型部署。&lt;/p&gt; &#xA;&lt;p&gt;数据：每个研究领域和版本迭代都需要高质量的数据，包括指令-答案、预训练、多模态、多语言和用户反馈等数据。&lt;/p&gt; &#xA;&lt;p&gt;论文：我们将维护一个 Panda 论文列表，并使用 Panda 作为优化、完全测试和显著改进的学术论文的基础模型。&lt;/p&gt; &#xA;&lt;p&gt;计算资源：我们希望通过协调一些开发者的冗余计算能力或从大学/企业获得非营利性赞助来帮助加速模型迭代速度。&lt;/p&gt; &#xA;&lt;h2 id=&#34;acknowledge&#34;&gt;鸣谢&lt;/h2&gt; &#xA;&lt;p&gt;我们非常感谢国内的一些大企业支持，为我们提供大量 GPU 来支持我们的模型训练。这些 GPU 的高性能计算能力为我们在 Panda 模型的研究和开发工作提供了强大的支持。&lt;/p&gt; &#xA;&lt;h2 id=&#34;acknowledge&#34;&gt; 开发者&lt;/h2&gt; &#xA;&lt;p&gt;Fangkai Jiao&lt;br&gt; Bosheng Ding&lt;br&gt; Tianze Luo&lt;br&gt; Zhanfeng Mo&lt;br&gt; Chengwei Qin&lt;/p&gt; &#xA;&lt;h2 id=&#34;acknowledge&#34;&gt; 项目总负责人&lt;/h2&gt; &#xA;&lt;p&gt;Bosheng Ding&lt;/p&gt; &#xA;&lt;h2 id=&#34;acknowledge&#34;&gt; 联系我们&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:dandelionsllm@gmail.com&#34;&gt;dandelionsllm@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 id=&#34;acknowledge&#34;&gt; 项目总部&lt;/h2&gt; &#xA;&lt;p&gt;新加坡&lt;/p&gt; &#xA;&lt;h3&gt;免责声明&lt;/h3&gt; &#xA;&lt;p&gt;我们要求开发者不得将我们开源的代码、数据、模型及后续用此项目生成的衍生物用于任何商业以及为社会带来危害的用途。由 Panda 和 Flan-LLaMA 任何模型生成的内容均受随机性和不可控因素的影响，本项目无法保证其准确性。本项目不承担任何关于模型输出内容的法律责任，也不对使用相关资源和输出结果可能导致的任何损失承担责任。&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dandelionsllm/pandallm&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dandelionsllm/pandallm&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/LoRA</title>
    <updated>2023-05-07T01:44:33Z</updated>
    <id>tag:github.com,2023-05-07:/microsoft/LoRA</id>
    <link href="https://github.com/microsoft/LoRA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for loralib, an implementation of &#34;LoRA: Low-Rank Adaptation of Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;(For the radio communication technique, see &lt;a href=&#34;https://lora-alliance.org/&#34;&gt;LoRa&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the source code of the Python package &lt;code&gt;loralib&lt;/code&gt; and several examples of how to integrate it with PyTorch models, such as those in HuggingFace. We only support PyTorch for now. See our paper for a detailed description of LoRA.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Edward J. Hu*, Yelong Shen*, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen&lt;/em&gt; &lt;br&gt; Paper: &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;https://arxiv.org/abs/2106.09685&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Update 2/2023: LoRA is now supported by the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;State-of-the-art Parameter-Efficient Fine-Tuning (PEFT)&lt;/a&gt; library by HuggingFace.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;We obtain result comparable or superior to full finetuning on the GLUE benchmark using &lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;RoBERTa (Liu et al., 2019)&lt;/a&gt; base and large and &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;DeBERTa (He et al., 2020)&lt;/a&gt; XXL 1.5B, while only training and storing a fraction of the parameters. Click the numbers below to download the RoBERTa and DeBERTa LoRA checkpoints.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;RoBERTa base &lt;br&gt; Fine-tune&lt;/th&gt; &#xA;   &lt;th&gt;RoBERTa base &lt;br&gt; LoRA&lt;/th&gt; &#xA;   &lt;th&gt;DeBERTa XXL &lt;br&gt; Fine-tune&lt;/th&gt; &#xA;   &lt;th&gt;DeBERTa XXL &lt;br&gt; LoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;# of Trainable Params.&lt;/td&gt; &#xA;   &lt;td&gt;125M&lt;/td&gt; &#xA;   &lt;td&gt;0.8M&lt;/td&gt; &#xA;   &lt;td&gt;1.5B&lt;/td&gt; &#xA;   &lt;td&gt;4.7M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MNLI (m-Acc/mm-Acc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;87.6&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_mnli.bin&#34;&gt;&lt;b&gt;87.5&lt;/b&gt;±.3/86.9±.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91.7/&lt;b&gt;91.9&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_mnli.bin&#34;&gt;&lt;b&gt;91.9&lt;/b&gt;±.1/&lt;b&gt;91.9&lt;/b&gt;±.2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SST2 (Acc)&lt;/td&gt; &#xA;   &lt;td&gt;94.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_sst2.bin&#34;&gt;&lt;b&gt;95.1&lt;/b&gt;±.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;97.2&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_sst2.bin&#34;&gt;96.9±.2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MRPC (Acc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;90.2&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_mrpc.bin&#34;&gt;&lt;b&gt;89.7&lt;/b&gt;±.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;92.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_mrpc.bin&#34;&gt;&lt;b&gt;92.6&lt;/b&gt;±.6&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CoLA (Matthew&#39;s Corr)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;63.6&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_cola.bin&#34;&gt;&lt;b&gt;63.4&lt;/b&gt;±1.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;72.0&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_cola.bin&#34;&gt;&lt;b&gt;72.4&lt;/b&gt;±1.1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;QNLI (Acc)&lt;/td&gt; &#xA;   &lt;td&gt;92.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_qnli.bin&#34;&gt;&lt;b&gt;93.3&lt;/b&gt;±.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;96.0&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_qnli.bin&#34;&gt;&lt;b&gt;96.0&lt;/b&gt;±.1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;QQP (Acc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;91.9&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_qqp.bin&#34;&gt;90.8±.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;92.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_qqp.bin&#34;&gt;&lt;b&gt;92.9&lt;/b&gt;±.1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RTE (Acc)&lt;/td&gt; &#xA;   &lt;td&gt;78.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_rte.bin&#34;&gt;&lt;b&gt;86.6&lt;/b&gt;±.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;93.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_rte.bin&#34;&gt;&lt;b&gt;94.9&lt;/b&gt;±.4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;STSB (Pearson/Spearman Corr)&lt;/td&gt; &#xA;   &lt;td&gt;91.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/RoBERTa-base/roberta_base_lora_stsb.bin&#34;&gt;&lt;b&gt;91.5&lt;/b&gt;±.2/&lt;b&gt;91.3&lt;/b&gt;±.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;92.9&lt;/b&gt;/92.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/DeBERTa/deberta_v2_xxlarge_lora_stsb.bin&#34;&gt;&lt;b&gt;93.0&lt;/b&gt;±.2/&lt;b&gt;92.9&lt;/b&gt;±.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Average&lt;/td&gt; &#xA;   &lt;td&gt;86.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;87.24&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91.06&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;91.32&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;i&gt;Note: You still need the original pre-trained checkpoint from &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt; to use the LoRA checkpoints.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning numbers are taken from &lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;Liu et al. (2019)&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;He et al. (2020)&lt;/a&gt;. We include confidence intervals on results from our experiments. Please follow the instructions in &lt;code&gt;examples/NLU/&lt;/code&gt; to reproduce our results.&lt;/p&gt; &#xA;&lt;p&gt;On GPT-2, LoRA compares favorably to both full finetuning and other efficient tuning methods, such as &lt;a href=&#34;https://arxiv.org/abs/1902.00751&#34;&gt;adapter (Houlsby et al., 2019)&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2101.00190&#34;&gt;prefix tuning (Li and Liang, 2021)&lt;/a&gt;. We evaluated on E2E NLG Challenge, DART, and WebNLG:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;# of Trainable Params&lt;/th&gt; &#xA;   &lt;th&gt;E2E (BLEU)&lt;/th&gt; &#xA;   &lt;th&gt;DART (BLEU)&lt;/th&gt; &#xA;   &lt;th&gt;WebNLG (BLEU-U/S/A)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 M (Fine-Tune)&lt;/td&gt; &#xA;   &lt;td&gt;354.92M&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;30.4/&lt;b&gt;63.2&lt;/b&gt;/47.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 M (Adapter)&lt;/td&gt; &#xA;   &lt;td&gt;0.37M&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;42.4&lt;/td&gt; &#xA;   &lt;td&gt;45.1/54.5/50.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 M (Prefix)&lt;/td&gt; &#xA;   &lt;td&gt;0.35M&lt;/td&gt; &#xA;   &lt;td&gt;69.7&lt;/td&gt; &#xA;   &lt;td&gt;45.7&lt;/td&gt; &#xA;   &lt;td&gt;44.1/63.1/54.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 M (LoRA)&lt;/td&gt; &#xA;   &lt;td&gt;0.35M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;70.4&lt;/b&gt;±.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;47.1&lt;/b&gt;±.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;46.7&lt;/b&gt;±.4/62.1±.2/&lt;b&gt;55.3&lt;/b&gt;±.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 L (Fine-Tune)&lt;/td&gt; &#xA;   &lt;td&gt;774.03M&lt;/td&gt; &#xA;   &lt;td&gt;68.5&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;41.7/&lt;b&gt;64.6&lt;/b&gt;/54.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 L (Adapter)&lt;/td&gt; &#xA;   &lt;td&gt;0.88M&lt;/td&gt; &#xA;   &lt;td&gt;69.1±.1&lt;/td&gt; &#xA;   &lt;td&gt;45.7±.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;49.8&lt;/b&gt;±.0/61.1±.0/56.0±.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 L (Prefix)&lt;/td&gt; &#xA;   &lt;td&gt;0.77M&lt;/td&gt; &#xA;   &lt;td&gt;70.3&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;47.0/64.2/56.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2 L (LoRA)&lt;/td&gt; &#xA;   &lt;td&gt;0.77M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;70.4&lt;/b&gt;±.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;47.5&lt;/b&gt;±.1&lt;/td&gt; &#xA;   &lt;td&gt;48.4±.3/&lt;b&gt;64.0&lt;/b&gt;±.3/&lt;b&gt;57.0&lt;/b&gt;±.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Non-LoRA baselines, except for adapter on GPT-2 large, are taken from &lt;a href=&#34;https://arxiv.org/abs/2101.00190&#34;&gt;Li and Liang (2021)&lt;/a&gt;. We include confidence intervals on results from our experiments.&lt;/p&gt; &#xA;&lt;p&gt;Download the GPT-2 LoRA checkpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_md_lora_e2e.pt&#34;&gt;GPT-2 Medium E2E&lt;/a&gt; (1.5 MB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_md_lora_dart.pt&#34;&gt;GPT-2 Medium DART&lt;/a&gt; (1.5 MB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_md_lora_webnlg.pt&#34;&gt;GPT-2 Medium WebNLG&lt;/a&gt; (1.5 MB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_lg_lora_e2e.pt&#34;&gt;GPT-2 Large E2E&lt;/a&gt; (2.3 MB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_lg_lora_dart.pt&#34;&gt;GPT-2 Large DART&lt;/a&gt; (2.3 MB)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/releases/download/GPT-2/gpt2_lg_lora_webnlg.pt&#34;&gt;GPT-2 Large WebNLG&lt;/a&gt; (2.3 MB)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please follow the instructions in &lt;code&gt;examples/NLG/&lt;/code&gt; to reproduce our result.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;i&gt;(The initial release of this repo has been archived in the branch &#34;snapshot-9-15-2021&#34;)&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are several directories in this repo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/loralib&#34;&gt;loralib/&lt;/a&gt; contains the source code for the package &lt;code&gt;loralib&lt;/code&gt;, which needs to be installed to run the examples we provide;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG&#34;&gt;examples/NLG/&lt;/a&gt; contains an example implementation of LoRA in GPT-2 using our package, which can be used to reproduce the result in our paper;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLU&#34;&gt;examples/NLU/&lt;/a&gt; contains an example implementation of LoRA in RoBERTa and DeBERTa using our package, which produces competitive results on the GLUE benchmark;&lt;/li&gt; &#xA; &lt;li&gt;See how we use &lt;code&gt;loralib&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG/src/model.py&#34;&gt;GPT-2&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLU/src/transformers/models/roberta/modeling_roberta.py&#34;&gt;RoBERTa&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLU/src/transformers/models/deberta_v2/modeling_deberta_v2.py&#34;&gt;DeBERTa v2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Installing &lt;code&gt;loralib&lt;/code&gt; is simply&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install loralib&#xA;# Alternatively&#xA;# pip install git+https://github.com/microsoft/LoRA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;You can choose to adapt some layers by replacing them with counterparts implemented in &lt;code&gt;loralib&lt;/code&gt;. We only support &lt;code&gt;nn.Linear&lt;/code&gt;, &lt;code&gt;nn.Embedding&lt;/code&gt;, and &lt;code&gt;nn.Conv2d&lt;/code&gt; for now. We also support a &lt;code&gt;MergedLinear&lt;/code&gt; for cases where a single &lt;code&gt;nn.Linear&lt;/code&gt; represents more than one layers, such as in some implementations of the attention &lt;code&gt;qkv&lt;/code&gt; projection (see Additional Notes for more).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ===== Before =====&#xA;# layer = nn.Linear(in_features, out_features)&#xA;&#xA;# ===== After ======&#xA;import loralib as lora&#xA;# Add a pair of low-rank adaptation matrices with rank r=16&#xA;layer = lora.Linear(in_features, out_features, r=16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Before the training loop begins, mark only LoRA parameters as trainable.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;import loralib as lora&#xA;model = BigModel()&#xA;# This sets requires_grad to False for all parameters without the string &#34;lora_&#34; in their names&#xA;lora.mark_only_lora_as_trainable(model)&#xA;# Training loop&#xA;for batch in dataloader:&#xA;   ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;When saving a checkpoint, generate a &lt;code&gt;state_dict&lt;/code&gt; that only contains LoRA parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ===== Before =====&#xA;# torch.save(model.state_dict(), checkpoint_path)&#xA;# ===== After =====&#xA;torch.save(lora.lora_state_dict(model), checkpoint_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;When loading a checkpoint using &lt;code&gt;load_state_dict&lt;/code&gt;, be sure to set &lt;code&gt;strict=False&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Load the pretrained checkpoint first&#xA;model.load_state_dict(torch.load(&#39;ckpt_pretrained.pt&#39;), strict=False)&#xA;# Then load the LoRA checkpoint&#xA;model.load_state_dict(torch.load(&#39;ckpt_lora.pt&#39;), strict=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Now training can proceed as usual.&lt;/h4&gt; &#xA;&lt;h2&gt;Additional Notes&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;While we focus on a simple yet effect setup, namely adapting only the &lt;code&gt;q&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; projection in a Transformer, in our examples, LoRA can be apply to any subsets of pre-trained weights. We encourage you to explore different configurations, such as adapting the embedding layer by replacing &lt;code&gt;nn.Embedding&lt;/code&gt; with &lt;code&gt;lora.Embedding&lt;/code&gt; and/or adapting the MLP layers. It&#39;s very likely that the optimal configuration varies for different model architectures and tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some Transformer implementation uses a single &lt;code&gt;nn.Linear&lt;/code&gt; for the projection matrices for query, key, and value. If one wishes to constrain the rank of the updates to the individual matrices, one has to either break it up into three separate matrices or use &lt;code&gt;lora.MergedLinear&lt;/code&gt;. Make sure to modify the checkpoint accordingly if you choose to break up the layer.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ===== Before =====&#xA;# qkv_proj = nn.Linear(d_model, 3*d_model)&#xA;# ===== After =====&#xA;# Break it up (remember to modify the pretrained checkpoint accordingly)&#xA;q_proj = lora.Linear(d_model, d_model, r=8)&#xA;k_proj = nn.Linear(d_model, d_model)&#xA;v_proj = lora.Linear(d_model, d_model, r=8)&#xA;# Alternatively, use lora.MergedLinear (recommended)&#xA;qkv_proj = lora.MergedLinear(d_model, 3*d_model, r=8, enable_lora=[True, False, True])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Training bias vectors in tandem with LoRA might be a cost-efficient way to squeeze out extra task performance (if you tune the learning rate carefully). While we did not study its effect thoroughly in our paper, we make it easy to try in &lt;code&gt;lora&lt;/code&gt;. You can mark some biases as trainable by passing &#34;all&#34; or &#34;lora_only&#34; to &lt;code&gt;bias=&lt;/code&gt; when calling &lt;code&gt;mark_only_lora_as_trainable&lt;/code&gt;. Remember to pass the corresponding &lt;code&gt;bias=&lt;/code&gt; argument to &lt;code&gt;lora_state_dict&lt;/code&gt; when saving a checkpoint.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ===== Before =====&#xA;# lora.mark_only_lora_as_trainable(model) # Not training any bias vectors&#xA;# ===== After =====&#xA;# Training all bias vectors associated with modules we apply LoRA to &#xA;lora.mark_only_lora_as_trainable(model, bias=&#39;lora_only&#39;)&#xA;# Alternatively, we can train *all* bias vectors in the model, including LayerNorm biases&#xA;lora.mark_only_lora_as_trainable(model, bias=&#39;all&#39;)&#xA;# When saving a checkpoint, use the same bias= (&#39;all&#39; or &#39;lora_only&#39;)&#xA;torch.save(lora.lora_state_dict(model, bias=&#39;all&#39;), checkpoint_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Calling &lt;code&gt;model.eval()&lt;/code&gt; will trigger the merging of LoRA parameters with the corresponding pretrained ones, which eliminates additional latency for subsequent forward passes. Calling &lt;code&gt;model.train()&lt;/code&gt; again will undo the merge. This can be disabled by passing &lt;code&gt;merge_weights=False&lt;/code&gt; to LoRA layers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Please contact us or post an issue if you have any questions.&lt;/p&gt; &#xA;&lt;p&gt;For questions related to the package &lt;code&gt;loralib&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edward Hu (&lt;a href=&#34;mailto:edward@edwardjhu.com&#34;&gt;edward@edwardjhu.com&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Phillip Wallis (&lt;a href=&#34;mailto:phwallis@microsoft.com&#34;&gt;phwallis@microsoft.com&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Weizhu Chen (&lt;a href=&#34;mailto:wzchen@microsoft.com&#34;&gt;wzchen@microsoft.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The GPT-2 example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Phillip Wallis (&lt;a href=&#34;mailto:phwallis@microsoft.com&#34;&gt;phwallis@microsoft.com&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yelong Shen (&lt;a href=&#34;mailto:yeshe@microsoft.com&#34;&gt;yeshe@microsoft.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The RoBERTa/DeBERTa example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lu Wang (&lt;a href=&#34;mailto:luw@microsoft.com&#34;&gt;luw@microsoft.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank in alphabetical order Jianfeng Gao, Jade Huang, Jiayuan Huang, Lisa Xiang Li, Xiaodong Liu, Yabin Liu, Benjamin Van Durme, Luis Vargas, Haoran Wei, Peter Welinder, and Greg Yang for providing valuable feedback.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{hu2021lora,&#xA;    title={LoRA: Low-Rank Adaptation of Large Language Models},&#xA;    author={Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},&#xA;    year={2021},&#xA;    eprint={2106.09685},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
</feed>