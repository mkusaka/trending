<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-23T01:31:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GaParmar/img2img-turbo</title>
    <updated>2024-03-23T01:31:42Z</updated>
    <id>tag:github.com,2024-03-23:/GaParmar/img2img-turbo</id>
    <link href="https://github.com/GaParmar/img2img-turbo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;One-step image-to-image with Stable Diffusion turbo: sketch2image, day2night, and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;img2img-turbo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12036&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/gparmar/img2img-turbo-sketch&#34;&gt;&lt;strong&gt;Sketch2Image Demo&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Quick start:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/#getting-started&#34;&gt;&lt;strong&gt;Running Locally&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/#gradio-demo&#34;&gt;&lt;strong&gt;Gradio (locally hosted)&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h3&gt;Cat Sketching&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/cat_2x.gif&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Fish Sketching&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/fish_2x.gif&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We propose a general method for adapting a single-step diffusion model, such as SD-Turbo, to new tasks and domains through adversarial learning. This enables us to leverage the internal knowledge of pre-trained diffusion models while achieving efficient inference (e.g., for 512x512 images, 0.29 seconds on A6000 and 0.11 seconds on A100).&lt;/p&gt; &#xA;&lt;p&gt;Our one-step conditional models &lt;strong&gt;CycleGAN-Turbo&lt;/strong&gt; and &lt;strong&gt;pix2pix-turbo&lt;/strong&gt; can perform various image-to-image translation tasks for both unpaired and paired settings. CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods, while pix2pix-turbo is on par with recent works such as ControlNet for Sketch2Photo and Edge2Image, but with one-step inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12036&#34;&gt;One-Step Image Translation with Text-to-Image Models&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://gauravparmar.com/&#34;&gt;Gaurav Parmar&lt;/a&gt;, &lt;a href=&#34;https://taesung.me/&#34;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~srinivas/&#34;&gt;Srinivasa Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://github.com/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CMU and Adobe, arXiv 2403.12036&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/teaser_results.jpg&#34; align=&#34;center&#34; width=&#34;1000px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Paired Translation with pix2pix-turbo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edge to Image&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/edge_to_image_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- **Sketch to Image**&#xA;TODO --&gt; &#xA;&lt;h3&gt;Generating Diverse Outputs&lt;/h3&gt; &#xA;&lt;p&gt;By varying the input noise map, our method can generate diverse outputs from the same input conditioning. The output style can be controlled by changing the text prompt.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/gen_variations.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Unpaired Translation with CycleGAN-Turbo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Day to Night&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/day2night_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Night to Day&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt;&#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/night2day_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clear to Rainy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/clear2rainy_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rainy to Clear&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/rainy2clear.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our Generator Architecture:&lt;/strong&gt; We tightly integrate three separate modules in the original latent diffusion models into a single end-to-end network with small trainable weights. This architecture allows us to translate the input image x to the output y, while retaining the input scene structure. We use LoRA adapters in each module, introduce skip connections and Zero-Convs between input and output, and retrain the first layer of the U-Net. Blue boxes indicate trainable layers. Semi-transparent layers are frozen. The same generator can be used for various GAN objectives.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/method.jpg&#34; align=&#34;center&#34; width=&#34;900px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Environment Setup&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/environment.yml&#34;&gt;conda env file&lt;/a&gt; that contains all the required dependencies. &lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Following this, you can activate the conda environment with the command below. &lt;pre&gt;&lt;code&gt;conda activate img2img-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paired Image Translation (pix2pix-turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes an image file and a prompt as inputs, extracts the canny edges, and saves the results in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/inference_paired.py --model &#34;edge_to_image&#34; \&#xA;    --input_image &#34;assets/bird.png&#34; \&#xA;    --prompt &#34;a blue bird&#34; \&#xA;    --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a sketch and a prompt as inputs, and saves the results in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/inference_paired.py --model &#34;sketch_to_image_stochastic&#34; \&#xA;--input_image &#34;assets/sketch.png&#34; --gamma 0.4 \&#xA;--prompt &#34;ethereal fantasy concept art of an asteroid. magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy&#34; \&#xA;--output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unpaired Image Translation (CycleGAN-Turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The following command takes an image file as input, and saves the results in the directory specified. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/inference_unpaired.py --model &#34;day_to_night&#34; \&#xA;    --input_image &#34;assets/day.png&#34; --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a Gradio demo for the paired image translation tasks.&lt;/li&gt; &#xA; &lt;li&gt;The following command will launch the sketch to image locally using gradio. &lt;pre&gt;&lt;code&gt;gradio gradio_sketch2image.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Our work uses the Stable Diffusion-Turbo as the base model with the following &lt;a href=&#34;https://huggingface.co/stabilityai/sd-turbo/blob/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lightning-AI/lightning-thunder</title>
    <updated>2024-03-23T01:31:42Z</updated>
    <id>tag:github.com,2024-03-23:/Lightning-AI/lightning-thunder</id>
    <link href="https://github.com/Lightning-AI/lightning-thunder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Source to source compiler for PyTorch. It makes PyTorch programs faster on single accelerators and distributed.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;Thunder&#34; src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/LightningThunderLightModewByline.png#gh-light-mode-only&#34; width=&#34;400px&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;img alt=&#34;Thunder&#34; src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/LightningThunderDarkModewByline.png#gh-dark-mode-only&#34; width=&#34;400px&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Make PyTorch models Lightning fast.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://lightning.ai/&#34;&gt;Lightning.ai&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#performance&#34;&gt;Performance&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#get-started&#34;&gt;Get started&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#install-thunder&#34;&gt;Install&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#hello-world&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#features&#34;&gt;Features&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/#documentation&#34;&gt;Documentation&lt;/a&gt; • &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lightning-thunder/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lightning-thunder/actions/workflows/ci-testing.yml&#34;&gt;&lt;img src=&#34;https://github.com/Lightning-AI/lightning-thunder/actions/workflows/ci-testing.yml/badge.svg?event=push&#34; alt=&#34;CI testing&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lightning-thunder/actions/workflows/ci-checks.yml&#34;&gt;&lt;img src=&#34;https://github.com/Lightning-AI/lightning-thunder/actions/workflows/ci-checks.yml/badge.svg?event=push&#34; alt=&#34;General checks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lightning-thunder.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/lightning-thunder/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/Lightning-AI/lightning-thunder/main&#34;&gt;&lt;img src=&#34;https://results.pre-commit.ci/badge/github/Lightning-AI/lightning-thunder/main.svg?sanitize=true&#34; alt=&#34;pre-commit.ci status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Welcome to ⚡ Lightning Thunder&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thunder makes PyTorch models Lightning fast.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thunder is a source-to-source compiler for PyTorch. It makes PyTorch programs faster by combining and using different hardware executors at once (ie: nvFuser, torch.compile, cuDNN, and TransformerEngine FP8).&lt;/p&gt; &#xA;&lt;p&gt;Works on single accelerators and in multi-GPU settings. Thunder aims to be usable, understandable, and extensible.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Thunder can achieve significant speedups over standard PyTorch eager code, through the compounding effects of optimizations and the use of best-in-class executors. Here is an example of the pretraining throughput for Llama 2 7B as implemented in &lt;a href=&#34;https://github.com/Lightning-AI/litgpt&#34;&gt;LitGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;Thunder&#34; src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/training_throughput_single.png&#34; width=&#34;800px&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Thunder achieves a 40% speedup in training throughput compared to eager code on H100 using a combination of executors including nvFuser, torch.compile, cuDNN, and TransformerEngine FP8.&lt;/p&gt; &#xA;&lt;p&gt;Thunder supports distributed strategies like DDP and FSDP (ZeRO2 and ZeRO3). Here is the normalized throughput measured for Llama 2 7B (this time without FP8 mixed precision, support for FSDP is underway).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;Thunder&#34; src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/normalized_training_throughput_zero2.png&#34; width=&#34;800px&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: Lightning Thunder is alpha.&lt;/strong&gt; Feel free to get involved, expect a few bumps along the way.&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;p&gt;Try Thunder without installing by using our &lt;a href=&#34;https://lightning.ai/lightning-ai/studios/zero-to-thunder-tutorial&#34;&gt;Zero to Thunder Tutorial Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install Thunder&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/NVIDIA/Fuser&#34;&gt;nvFuser&lt;/a&gt; nightly, and Thunder together&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install nvFuser which installs the matching nightly PyTorch&#xA;pip install --pre &#39;nvfuser-cu121[torch]&#39; --extra-index-url https://pypi.nvidia.com&#xA;&#xA;# install thunder&#xA;pip install lightning-thunder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Advanced install options&lt;/summary&gt; &#xA; &lt;!-- following section will be skipped from PyPI description --&gt; &#xA; &lt;h3&gt;Install from main&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/Lightning-AI/lightning-thunder.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Install to tinker and contribute&lt;/h3&gt; &#xA; &lt;p&gt;Install this way to tinker with the internals and contribute:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- end skipping PyPI description --&gt; &#xA;&lt;h2&gt;Hello World&lt;/h2&gt; &#xA;&lt;p&gt;Here is a simple example of how Thunder lets you compile and run PyTorch code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import thunder&#xA;&#xA;&#xA;def foo(a, b):&#xA;    return a + b&#xA;&#xA;&#xA;jfoo = thunder.jit(foo)&#xA;&#xA;a = torch.full((2, 2), 1)&#xA;b = torch.full((2, 2), 3)&#xA;&#xA;result = jfoo(a, b)&#xA;&#xA;print(result)&#xA;&#xA;# prints&#xA;# tensor(&#xA;#  [[4, 4]&#xA;#   [4, 4]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The compiled function &lt;code&gt;jfoo&lt;/code&gt; takes and returns PyTorch tensors, just like the original function, so modules and functions compiled by Thunder can be used as part of larger PyTorch programs.&lt;/p&gt; &#xA;&lt;h2&gt;Train models&lt;/h2&gt; &#xA;&lt;p&gt;Thunder is in its early stages and should not be used for production runs yet.&lt;/p&gt; &#xA;&lt;p&gt;However, it can already deliver outstanding performance on LLM model supported by &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt&#34;&gt;LitGPT&lt;/a&gt;, such as Mistral, Llama 2, Gemma, Falcon, and others.&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/tree/main/extensions/thunder&#34;&gt;the LitGPT integration&lt;/a&gt; to learn about running LitGPT and Thunder together.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;Given a Python callable or PyTorch module, Thunder can generate an optimized program that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Computes its forward and backward passes&lt;/li&gt; &#xA; &lt;li&gt;Coalesces operations into efficient fusion regions&lt;/li&gt; &#xA; &lt;li&gt;Dispatches computations to optimized kernels&lt;/li&gt; &#xA; &lt;li&gt;Distributes computations optimally across machines&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To do so, Thunder ships with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A JIT for acquiring Python programs targeting PyTorch and custom operations&lt;/li&gt; &#xA; &lt;li&gt;A multi-level IR to represent operations as a trace of a reduced op-set&lt;/li&gt; &#xA; &lt;li&gt;An extensible set of transformations on the trace, such as &lt;code&gt;grad&lt;/code&gt;, fusions, distributed (like &lt;code&gt;ddp&lt;/code&gt;, &lt;code&gt;fsdp&lt;/code&gt;), functional (like &lt;code&gt;vmap&lt;/code&gt;, &lt;code&gt;vjp&lt;/code&gt;, &lt;code&gt;jvp&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;A way to dispatch operations to an extensible collection of executors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thunder is written entirely in Python. Even its trace is represented as valid Python at all stages of transformation. This allows unprecedented levels of introspection and extensibility.&lt;/p&gt; &#xA;&lt;p&gt;Thunder doesn&#39;t generate code for accelerators directly. It acquires and transforms user programs so that it&#39;s possible to optimally select or generate device code using fast executors like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/pytorch-2.0/&#34;&gt;torch.compile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/Fuser&#34;&gt;nvFuser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/apex&#34;&gt;Apex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TransformerEngine&#34;&gt;TransformerEngine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch eager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;custom kernels, including those written with &lt;a href=&#34;https://github.com/openai/triton&#34;&gt;OpenAI Triton&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Modules and functions compiled with Thunder fully interoperate with vanilla PyTorch and support PyTorch&#39;s autograd. Also, Thunder works alongside torch.compile to leverage its state-of-the-art optimizations.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Docs are currently not hosted publicly. However you can build them locally really quickly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and point your browser to the generated docs at &lt;code&gt;docs/build/index.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Develop and run tests&lt;/h2&gt; &#xA;&lt;p&gt;You can set up your environment for developing Thunder by installing the development requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements/devel.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Thunder as an editable package (optional):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you run tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest thunder/tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Thunder is very thoroughly tested, so expect this to take a while.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Lightning Thunder is released under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2.0&lt;/a&gt; license. See the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>harry0703/MoneyPrinterTurbo</title>
    <updated>2024-03-23T01:31:42Z</updated>
    <id>tag:github.com,2024-03-23:/harry0703/MoneyPrinterTurbo</id>
    <link href="https://github.com/harry0703/MoneyPrinterTurbo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;利用大模型，一键生成短视频&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoneyPrinterTurbo 💸&lt;/h1&gt; &#xA;&lt;p&gt;只需提供一个视频 &lt;strong&gt;主题&lt;/strong&gt; 或 &lt;strong&gt;关键词&lt;/strong&gt; ，就可以全自动生成视频文案、视频素材、视频字幕、视频背景音乐，然后合成一个高清的短视频。&lt;/p&gt; &#xA;&lt;h2&gt;功能特性 🎯&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 完整的 &lt;strong&gt;MVC架构&lt;/strong&gt;，代码 &lt;strong&gt;结构清晰&lt;/strong&gt;，易于维护，支持API和Web界面&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持多种 &lt;strong&gt;高清视频&lt;/strong&gt; 尺寸 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 竖屏 9:16，&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 横屏 16:9，&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;中文&lt;/strong&gt; 和 &lt;strong&gt;英文&lt;/strong&gt; 视频文案&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;多种语音&lt;/strong&gt; 合成&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;字幕生成&lt;/strong&gt;，可以调整字体、颜色、大小，同时支持字幕描边设置&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持 &lt;strong&gt;背景音乐&lt;/strong&gt;，随机或者指定音乐文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 视频素材来源 &lt;strong&gt;无版权&lt;/strong&gt; 问题&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;后期计划 🚀&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 完善异步API接口，进度显示&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 优化语音合成，利用大模型，使其合成的声音，更加自然，情绪更加丰富&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加视频转场效果，使其看起来更加的流畅&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 优化字幕效果&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 优化视频素材的匹配度&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;视频演示 📺&lt;/h2&gt; &#xA;&lt;h3&gt;竖屏 9:16&lt;/h3&gt; &#xA;&lt;p&gt;▶️ 《如何增加生活的乐趣》&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&#34;&gt;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;▶️ 《生命的意义是什么》&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&#34;&gt;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;横屏 16:9&lt;/h3&gt; &#xA;&lt;p&gt;▶️《生命的意义是什么》&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&#34;&gt;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;▶️《为什么要运动》&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&#34;&gt;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;安装部署 📥&lt;/h2&gt; &#xA;&lt;p&gt;建议使用 &lt;a href=&#34;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&#34;&gt;conda&lt;/a&gt; 创建 python 虚拟环境&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git&#xA;cd MoneyPrinterTurbo&#xA;conda create -n MoneyPrinterTurbo python=3.10&#xA;conda activate MoneyPrinterTurbo&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;快速使用 🚀&lt;/h2&gt; &#xA;&lt;h3&gt;视频教程&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;完整的使用演示：&lt;a href=&#34;https://v.douyin.com/iFhnwsKY/&#34;&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;如何在Windows上部署：&lt;a href=&#34;https://v.douyin.com/iFyjoW3M&#34;&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;前提&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注意，尽量不要使用 &lt;strong&gt;中文路径&lt;/strong&gt;，避免出现一些无法预料的问题&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装好 ImageMagick &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;下载 &lt;a href=&#34;https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-29-Q16-x64-static.exe&#34;&gt;https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-29-Q16-x64-static.exe&lt;/a&gt; 并安装（不要修改安装路径）&lt;/li&gt; &#xA;     &lt;li&gt;修改配置文件 &lt;code&gt;config.toml&lt;/code&gt; 中的 &lt;code&gt;imagemagick_path&lt;/code&gt; 为你的实际安装路径（如果安装的时候没有修改路径，直接取消注释即可）&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;MacOS: &lt;code&gt;brew install imagemagick&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;将 &lt;code&gt;config.example.toml&lt;/code&gt; 文件重命名为 &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;按照 &lt;code&gt;config.toml&lt;/code&gt; 文件中的说明，配置好 &lt;code&gt;pexels_api_keys&lt;/code&gt; 和 llm 相关的 api key&lt;/li&gt; &#xA; &lt;li&gt;如果没有OpenAI的API Key，可以使用到 &lt;a href=&#34;https://platform.moonshot.cn/console/api-keys&#34;&gt;月之暗面&lt;/a&gt; 申请。注册就送 15元体验金，可以对话1500次左右。然后设置 &lt;code&gt;llm_provider=&#34;moonshot&#34;&lt;/code&gt; 和 &lt;code&gt;moonshot_api_key&lt;/code&gt;。感谢 &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues/8&#34;&gt;@jerryblues&lt;/a&gt; 的建议&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;启动Web界面&lt;/h3&gt; &#xA;&lt;p&gt;注意需要到 MoneyPrinterTurbo 项目 &lt;code&gt;根目录&lt;/code&gt; 下执行以下命令&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;webui.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;MacOS or Linux&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sh webui.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动后，会自动打开浏览器，效果如下图： &lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;启动API服务&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动后，可以查看 &lt;code&gt;API文档&lt;/code&gt; &lt;a href=&#34;http://127.0.0.1:8080/docs&#34;&gt;http://127.0.0.1:8080/docs&lt;/a&gt; 直接在线调试接口，快速体验。 &lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;语音合成 🗣&lt;/h2&gt; &#xA;&lt;p&gt;所有支持的声音列表，可以查看：&lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt&#34;&gt;声音列表&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;字幕生成 📜&lt;/h2&gt; &#xA;&lt;p&gt;当前支持2种字幕生成方式：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;edge&lt;/li&gt; &#xA; &lt;li&gt;whisper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;可以修改 &lt;code&gt;config.toml&lt;/code&gt; 配置文件中的 &lt;code&gt;subtitle_provider&lt;/code&gt; 进行切换，如果留空，表示不生成字幕。&lt;/p&gt; &#xA;&lt;h2&gt;背景音乐 🎵&lt;/h2&gt; &#xA;&lt;p&gt;用于视频的背景音乐，位于项目的 &lt;code&gt;resource/songs&lt;/code&gt; 目录下。当前项目里面放了一些默认的音乐，来自于 YouTube 视频，如有侵权，请删除。&lt;/p&gt; &#xA;&lt;h2&gt;字幕字体 🅰&lt;/h2&gt; &#xA;&lt;p&gt;用于视频字幕的渲染，位于项目的 &lt;code&gt;resource/fonts&lt;/code&gt; 目录下，你也可以放进去自己的字体。&lt;/p&gt; &#xA;&lt;h2&gt;反馈建议 📢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;可以提交 &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/issues&#34;&gt;issue&lt;/a&gt; 或者 &lt;a href=&#34;https://github.com/harry0703/MoneyPrinterTurbo/pulls&#34;&gt;pull request&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;也可以关注我的 &lt;strong&gt;抖音&lt;/strong&gt; 或 &lt;strong&gt;视频号&lt;/strong&gt;：&lt;code&gt;网旭哈瑞.AI&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;我会在上面发布一些 &lt;strong&gt;使用教程&lt;/strong&gt; 和 &lt;strong&gt;纯技术&lt;/strong&gt; 分享。&lt;/li&gt; &#xA;   &lt;li&gt;如果有更新和优化，我也会在上面 &lt;strong&gt;及时通知&lt;/strong&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;有问题也可以在上面 &lt;strong&gt;留言&lt;/strong&gt;，我会 &lt;strong&gt;尽快回复&lt;/strong&gt;。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;抖音&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;视频号&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/douyin.jpg&#34; width=&#34;180&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/shipinghao.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;特别感谢 🙏&lt;/h2&gt; &#xA;&lt;p&gt;该项目基于 &lt;a href=&#34;https://github.com/FujiwaraChoki/MoneyPrinter&#34;&gt;https://github.com/FujiwaraChoki/MoneyPrinter&lt;/a&gt; 重构而来，做了大量的优化，增加了更多的功能。 感谢原作者的开源精神。&lt;/p&gt; &#xA;&lt;h2&gt;许可证 📝&lt;/h2&gt; &#xA;&lt;p&gt;点击查看 &lt;a href=&#34;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; 文件&lt;/p&gt;</summary>
  </entry>
</feed>