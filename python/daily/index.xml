<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-10T01:40:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>laixintao/flameshow</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/laixintao/flameshow</id>
    <link href="https://github.com/laixintao/flameshow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A terminal Flamegraph viewer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Flameshow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/laixintao/flameshow/actions/workflows/pytest.yaml&#34;&gt;&lt;img src=&#34;https://github.com/laixintao/flameshow/actions/workflows/pytest.yaml/badge.svg?branch=main&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/laixintao/flameshow&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/laixintao/flameshow/graph/badge.svg?token=XQCGN9GBL4&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/flameshow/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/flameshow.svg?logo=pypi&amp;amp;label=PyPI&amp;amp;logoColor=gold&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/flameshow?logo=python&amp;amp;logoColor=gold&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/flameshow&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Flameshow is a terminal Flamegraph viewer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/laixintao/flameshow/main/docs/flameshow.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Renders Flamegraphs in your terminal&lt;/li&gt; &#xA; &lt;li&gt;Supports zooming in and displaying percentages&lt;/li&gt; &#xA; &lt;li&gt;Keyboard input is prioritized&lt;/li&gt; &#xA; &lt;li&gt;All operations can also be performed using the mouse.&lt;/li&gt; &#xA; &lt;li&gt;Can switch to different sample types&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Flameshow is written in pure Python, so you can install via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install flameshow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But you can also run it through &lt;a href=&#34;https://nixos.org/&#34;&gt;nix&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nix run github:laixintao/flameshow&#xA;# Or if you want to install it imperatively:&#xA;nix profile install github:laixintao/flameshow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;View golang&#39;s goroutine dump:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl http://localhost:9100/debug/pprof/goroutine -o goroutine.out&#xA;$ flameshow goroutine.out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After entering the TUI, the available actions are listed on Footer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;kbd&gt;q&lt;/kbd&gt; for quit&lt;/li&gt; &#xA; &lt;li&gt;&lt;kbd&gt;j&lt;/kbd&gt; &lt;kbd&gt;i&lt;/kbd&gt; &lt;kbd&gt;j&lt;/kbd&gt; &lt;kbd&gt;k&lt;/kbd&gt; or &lt;kbd&gt;â†&lt;/kbd&gt; &lt;kbd&gt;â†“&lt;/kbd&gt; &lt;kbd&gt;â†‘&lt;/kbd&gt; &lt;kbd&gt;â†’&lt;/kbd&gt; for moving around, and &lt;kbd&gt;Enter&lt;/kbd&gt; for zoom in, then &lt;kbd&gt;Esc&lt;/kbd&gt; for zoom out.&lt;/li&gt; &#xA; &lt;li&gt;You can also use a mouse, hover on a span will show it details, and click will zoom it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Formats&lt;/h2&gt; &#xA;&lt;p&gt;As far as I know, there is no standard specification for profiles. Different languages or tools might generate varying profile formats. I&#39;m actively working on supporting more formats. Admittedly, I might not be familiar with every tool and its specific format. So, if you&#39;d like Flameshow to integrate with a tool you love, please feel free to reach out and submit an issue.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Golang pprof&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.brendangregg.com/flamegraphs.html&#34;&gt;Brendan Gregg&#39;s Flamegraph&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to dive into the code and make some changes, start with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:laixintao/flameshow.git&#xA;cd flameshow&#xA;pip install poetry&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This project is proudly powered by &lt;a href=&#34;https://github.com/Textualize/textual&#34;&gt;textual&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jianchang512/stt</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/jianchang512/stt</id>
    <link href="https://github.com/jianchang512/stt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Voice Recognition to Text Tool / ä¸€ä¸ªç¦»çº¿è¿è¡Œçš„æœ¬åœ°è¯­éŸ³è¯†åˆ«è½¬æ–‡å­—æœåŠ¡ï¼Œè¾“å‡ºjsonã€srtå­—å¹•å¸¦æ—¶é—´æˆ³ã€çº¯æ–‡å­—æ ¼å¼&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/stt/main/README_EN.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://discord.gg/TMCM2PfHzQ&#34;&gt;Discord&lt;/a&gt; / Qç¾¤ 902124277&lt;/p&gt; &#xA;&lt;h1&gt;è¯­éŸ³è¯†åˆ«è½¬æ–‡å­—å·¥å…·&lt;/h1&gt; &#xA;&lt;p&gt;è¿™æ˜¯ä¸€ä¸ªç¦»çº¿è¿è¡Œçš„æœ¬åœ°è¯­éŸ³è¯†åˆ«è½¬æ–‡å­—å·¥å…·ï¼ŒåŸºäº fast-whipser å¼€æºæ¨¡å‹ï¼Œå¯å°†è§†é¢‘/éŸ³é¢‘ä¸­çš„äººç±»å£°éŸ³è¯†åˆ«å¹¶è½¬ä¸ºæ–‡å­—ï¼Œå¯è¾“å‡ºjsonæ ¼å¼ã€srtå­—å¹•å¸¦æ—¶é—´æˆ³æ ¼å¼ã€çº¯æ–‡å­—æ ¼å¼ã€‚å¯ç”¨äºè‡ªè¡Œéƒ¨ç½²åæ›¿ä»£ openai çš„è¯­éŸ³è¯†åˆ«æ¥å£æˆ–ç™¾åº¦è¯­éŸ³è¯†åˆ«ç­‰ï¼Œå‡†ç¡®ç‡åŸºæœ¬ç­‰åŒopenaiå®˜æ–¹apiæ¥å£ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;éƒ¨ç½²æˆ–ä¸‹è½½åï¼ŒåŒå‡» start.exe è‡ªåŠ¨è°ƒç”¨æœ¬åœ°æµè§ˆå™¨æ‰“å¼€æœ¬åœ°ç½‘é¡µã€‚&lt;/p&gt; &#xA; &lt;p&gt;æ‹–æ‹½æˆ–ç‚¹å‡»é€‰æ‹©è¦è¯†åˆ«çš„éŸ³é¢‘è§†é¢‘æ–‡ä»¶ï¼Œç„¶åé€‰æ‹©å‘å£°è¯­è¨€ã€è¾“å‡ºæ–‡å­—æ ¼å¼ã€æ‰€ç”¨æ¨¡å‹(å·²å†…ç½®baseæ¨¡å‹),ç‚¹å‡»å¼€å§‹è¯†åˆ«ï¼Œè¯†åˆ«å®Œæˆåä»¥æ‰€é€‰æ ¼å¼è¾“å‡ºåœ¨å½“å‰ç½‘é¡µã€‚&lt;/p&gt; &#xA; &lt;p&gt;å…¨è¿‡ç¨‹æ— éœ€è”ç½‘ï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œï¼Œå¯éƒ¨ç½²äºå†…ç½‘&lt;/p&gt; &#xA; &lt;p&gt;fast-whisper å¼€æºæ¨¡å‹æœ‰ base/small/medium/large-v3, å†…ç½®baseæ¨¡å‹ï¼Œbase-&amp;gt;large-v3è¯†åˆ«æ•ˆæœè¶Šæ¥è¶Šå¥½ï¼Œä½†æ‰€éœ€è®¡ç®—æœºèµ„æºä¹Ÿæ›´å¤šï¼Œæ ¹æ®éœ€è¦å¯è‡ªè¡Œä¸‹è½½åè§£å‹åˆ° models ç›®å½•ä¸‹å³å¯ã€‚&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/tag/0.0&#34;&gt;å…¨éƒ¨æ¨¡å‹ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;è§†é¢‘æ¼”ç¤º&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/assets/3378335/d716acb6-c20c-4174-9620-f574a7ff095d&#34;&gt;https://github.com/jianchang512/stt/assets/3378335/d716acb6-c20c-4174-9620-f574a7ff095d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/stt/assets/3378335/0f724ff1-21b3-4960-b6ba-5aa994ea414c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;é¢„ç¼–è¯‘Winç‰ˆä½¿ç”¨æ–¹æ³•/Linuxå’ŒMacæºç éƒ¨ç½²&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases&#34;&gt;ç‚¹å‡»æ­¤å¤„æ‰“å¼€Releasesé¡µé¢ä¸‹è½½&lt;/a&gt;é¢„ç¼–è¯‘æ–‡ä»¶&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ä¸‹è½½åè§£å‹åˆ°æŸå¤„ï¼Œæ¯”å¦‚ E:/stt&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åŒå‡» start.exe ï¼Œç­‰å¾…è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨çª—å£å³å¯&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç‚¹å‡»é¡µé¢ä¸­çš„ä¸Šä¼ åŒºåŸŸï¼Œåœ¨å¼¹çª—ä¸­æ‰¾åˆ°æƒ³è¯†åˆ«çš„éŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶ï¼Œæˆ–ç›´æ¥æ‹–æ‹½éŸ³é¢‘è§†é¢‘æ–‡ä»¶åˆ°ä¸Šä¼ åŒºåŸŸï¼Œç„¶åé€‰æ‹©å‘ç”Ÿè¯­è¨€ã€æ–‡æœ¬è¾“å‡ºæ ¼å¼ã€æ‰€ç”¨æ¨¡å‹ï¼Œç‚¹å‡»â€œç«‹å³å¼€å§‹è¯†åˆ«â€ï¼Œç¨ç­‰ç‰‡åˆ»ï¼Œåº•éƒ¨æ–‡æœ¬æ¡†ä¸­ä¼šä»¥æ‰€é€‰æ ¼å¼æ˜¾ç¤ºè¯†åˆ«ç»“æœ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¦‚æœæœºå™¨æ‹¥æœ‰è‹±ä¼Ÿè¾¾GPUï¼Œå¹¶æ­£ç¡®é…ç½®äº†CUDAç¯å¢ƒï¼Œå°†è‡ªåŠ¨ä½¿ç”¨CUDAåŠ é€Ÿ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;æºç éƒ¨ç½²(Linux/Mac/Window)&lt;/h1&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;è¦æ±‚ python 3.9-&amp;gt;3.11&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºç©ºç›®å½•ï¼Œæ¯”å¦‚ E:/stt, åœ¨è¿™ä¸ªç›®å½•ä¸‹æ‰“å¼€ cmd çª—å£ï¼Œæ–¹æ³•æ˜¯åœ°å€æ ä¸­è¾“å…¥ &lt;code&gt;cmd&lt;/code&gt;, ç„¶åå›è½¦ã€‚&lt;/p&gt; &lt;p&gt;ä½¿ç”¨gitæ‹‰å–æºç åˆ°å½“å‰ç›®å½• &lt;code&gt;git clone git@github.com:jianchang512/stt.git .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ¿€æ´»ç¯å¢ƒï¼Œwinä¸‹å‘½ä»¤ &lt;code&gt;%cd%/venv/scripts/activate&lt;/code&gt;ï¼Œlinuxå’ŒMacä¸‹å‘½ä»¤ &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…ä¾èµ–: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;,å¦‚æœæŠ¥ç‰ˆæœ¬å†²çªé”™è¯¯ï¼Œè¯·æ‰§è¡Œ &lt;code&gt;pip install -r requirements.txt --no-deps&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;winä¸‹è§£å‹ ffmpeg.7zï¼Œå°†å…¶ä¸­çš„&lt;code&gt;ffmpeg.exe&lt;/code&gt;å’Œ&lt;code&gt;ffprobe.exe&lt;/code&gt;æ”¾åœ¨é¡¹ç›®ç›®å½•ä¸‹, linuxå’Œmac åˆ° &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpegå®˜ç½‘&lt;/a&gt;ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ffmpegï¼Œè§£å‹å…¶ä¸­çš„&lt;code&gt;ffmpeg&lt;/code&gt;å’Œ&lt;code&gt;ffprobe&lt;/code&gt;äºŒè¿›åˆ¶ç¨‹åºæ”¾åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/tag/0.0&#34;&gt;ä¸‹è½½æ¨¡å‹å‹ç¼©åŒ…&lt;/a&gt;ï¼Œæ ¹æ®éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œä¸‹è½½åå°†å‹ç¼©åŒ…é‡Œçš„æ–‡ä»¶å¤¹æ”¾åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ models æ–‡ä»¶å¤¹å†…&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ‰§è¡Œ &lt;code&gt;python start.py &lt;/code&gt;ï¼Œç­‰å¾…è‡ªåŠ¨æ‰“å¼€æœ¬åœ°æµè§ˆå™¨çª—å£ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;æ³¨æ„ï¼š&lt;/h1&gt; &#xA;&lt;p&gt;æœ‰æ—¶ä¼šé‡åˆ°â€œcublasxx.dllä¸å­˜åœ¨â€çš„é”™è¯¯ï¼Œæ­¤æ—¶éœ€è¦ä¸‹è½½ cuBLASï¼Œç„¶åå°†dllæ–‡ä»¶å¤åˆ¶åˆ°ç³»ç»Ÿç›®å½•ä¸‹&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/download/0.0/cuBLAS_win.7z&#34;&gt;ç‚¹å‡»ä¸‹è½½ cuBLAS&lt;/a&gt;ï¼Œè§£å‹åå°†é‡Œé¢çš„dllæ–‡ä»¶å¤åˆ¶åˆ° C:/Windows/System32ä¸‹&lt;/p&gt; &#xA;&lt;h1&gt;apiæ¥å£&lt;/h1&gt; &#xA;&lt;p&gt;æ¥å£åœ°å€: &lt;a href=&#34;http://127.0.0.1:9977/api&#34;&gt;http://127.0.0.1:9977/api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;è¯·æ±‚æ–¹æ³•: POST&lt;/p&gt; &#xA;&lt;p&gt;è¯·æ±‚å‚æ•°:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;language: è¯­è¨€ä»£ç :å¯é€‰å¦‚ä¸‹&#xA;&#xA;&amp;gt;&#xA;&amp;gt; ä¸­æ–‡ï¼šzh&#xA;&amp;gt; è‹±è¯­ï¼šen&#xA;&amp;gt; æ³•è¯­ï¼šfr&#xA;&amp;gt; å¾·è¯­ï¼šde&#xA;&amp;gt; æ—¥è¯­ï¼šja&#xA;&amp;gt; éŸ©è¯­ï¼ško&#xA;&amp;gt; ä¿„è¯­ï¼šru&#xA;&amp;gt; è¥¿ç­ç‰™è¯­ï¼šes&#xA;&amp;gt; æ³°å›½è¯­ï¼šth&#xA;&amp;gt; æ„å¤§åˆ©è¯­ï¼šit&#xA;&amp;gt; è‘¡è„ç‰™è¯­ï¼špt&#xA;&amp;gt; è¶Šå—è¯­ï¼švi&#xA;&amp;gt; é˜¿æ‹‰ä¼¯è¯­ï¼šar&#xA;&amp;gt; åœŸè€³å…¶è¯­ï¼štr&#xA;&amp;gt;&#xA;&#xA;model: æ¨¡å‹åç§°ï¼Œå¯é€‰å¦‚ä¸‹&#xA;&amp;gt;&#xA;&amp;gt; base å¯¹åº”äº models/models--Systran--faster-whisper-base&#xA;&amp;gt; small å¯¹åº”äº models/models--Systran--faster-whisper-small&#xA;&amp;gt; medium å¯¹åº”äº models/models--Systran--faster-whisper-medium&#xA;&amp;gt; large-v3 å¯¹åº”äº models/models--Systran--faster-whisper-large-v3&#xA;&amp;gt;&#xA;&#xA;response_format: è¿”å›çš„å­—å¹•æ ¼å¼ï¼Œå¯é€‰ text|json|srt&#xA;&#xA;file: éŸ³è§†é¢‘æ–‡ä»¶ï¼ŒäºŒè¿›åˆ¶ä¸Šä¼ &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Api è¯·æ±‚ç¤ºä¾‹&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    import requests&#xA;    # è¯·æ±‚åœ°å€&#xA;    url = &#34;http://127.0.0.1:9977/api&#34;&#xA;    # è¯·æ±‚å‚æ•°  file:éŸ³è§†é¢‘æ–‡ä»¶ï¼Œlanguageï¼šè¯­è¨€ä»£ç ï¼Œmodelï¼šæ¨¡å‹ï¼Œresponse_format:text|json|srt&#xA;    # è¿”å› code==0 æˆåŠŸï¼Œå…¶ä»–å¤±è´¥ï¼Œmsg==æˆåŠŸä¸ºokï¼Œå…¶ä»–å¤±è´¥åŸå› ï¼Œdata=è¯†åˆ«åè¿”å›æ–‡å­—&#xA;    files = {&#34;file&#34;: open(&#34;C:\\Users\\c1\\Videos\\2.wav&#34;, &#34;rb&#34;)}&#xA;    data={&#34;language&#34;:&#34;zh&#34;,&#34;model&#34;:&#34;base&#34;,&#34;response_format&#34;:&#34;json&#34;}&#xA;    response = requests.request(&#34;POST&#34;, url, timeout=600, data=data,files=files)&#xA;    print(response.json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;CUDA åŠ é€Ÿæ”¯æŒ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;å®‰è£…CUDAå·¥å…·&lt;/strong&gt; &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;è¯¦ç»†å®‰è£…æ–¹æ³•&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœä½ çš„ç”µè„‘æ‹¥æœ‰ Nvidia æ˜¾å¡ï¼Œå…ˆå‡çº§æ˜¾å¡é©±åŠ¨åˆ°æœ€æ–°ï¼Œç„¶åå»å®‰è£…å¯¹åº”çš„ &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA Toolkit 11.8&lt;/a&gt; å’Œ &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-archive&#34;&gt;cudnn for CUDA11.X&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å®‰è£…å®Œæˆæˆï¼ŒæŒ‰&lt;code&gt;Win + R&lt;/code&gt;,è¾“å…¥ &lt;code&gt;cmd&lt;/code&gt;ç„¶åå›è½¦ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­è¾“å…¥&lt;code&gt;nvcc --version&lt;/code&gt;,ç¡®è®¤æœ‰ç‰ˆæœ¬ä¿¡æ¯æ˜¾ç¤ºï¼Œç±»ä¼¼è¯¥å›¾ &lt;img src=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/e68de07f-4bb1-4fc9-bccd-8f841825915a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ç„¶åç»§ç»­è¾“å…¥&lt;code&gt;nvidia-smi&lt;/code&gt;,ç¡®è®¤æœ‰è¾“å‡ºä¿¡æ¯ï¼Œå¹¶ä¸”èƒ½çœ‹åˆ°cudaç‰ˆæœ¬å·ï¼Œç±»ä¼¼è¯¥å›¾ &lt;img src=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/71f1d7d3-07f9-4579-b310-39284734006b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ç„¶åæ‰§è¡Œ `python testcuda.py`ï¼Œå¦‚æœæç¤ºæˆåŠŸï¼Œè¯´æ˜å®‰è£…æ­£ç¡®ï¼Œå¦åˆ™è¯·ä»”ç»†æ£€æŸ¥é‡æ–°å®‰è£…&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;æ³¨æ„äº‹é¡¹&lt;/h1&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;å¦‚æœæ²¡æœ‰è‹±ä¼Ÿè¾¾æ˜¾å¡æˆ–æœªé…ç½®å¥½CUDAç¯å¢ƒï¼Œä¸è¦ä½¿ç”¨ large/large-v3 æ¨¡å‹ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜è€—å°½æ­»æœº&lt;/li&gt; &#xA; &lt;li&gt;ä¸­æ–‡åœ¨æŸäº›æƒ…å†µä¸‹ä¼šè¾“å‡ºç¹ä½“å­—&lt;/li&gt; &#xA; &lt;li&gt;æœ‰æ—¶ä¼šé‡åˆ°â€œcublasxx.dllä¸å­˜åœ¨â€çš„é”™è¯¯ï¼Œæ­¤æ—¶éœ€è¦ä¸‹è½½ cuBLASï¼Œç„¶åå°†dllæ–‡ä»¶å¤åˆ¶åˆ°ç³»ç»Ÿç›®å½•ä¸‹ï¼Œ&lt;a href=&#34;https://github.com/jianchang512/stt/releases/download/0.0/cuBLAS_win.7z&#34;&gt;ç‚¹å‡»ä¸‹è½½ cuBLAS&lt;/a&gt;ï¼Œè§£å‹åå°†é‡Œé¢çš„dllæ–‡ä»¶å¤åˆ¶åˆ° C:/Windows/System32ä¸‹&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;ç›¸å…³è”é¡¹ç›®&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/pyvideotrans&#34;&gt;è§†é¢‘ç¿»è¯‘é…éŸ³å·¥å…·:ç¿»è¯‘å­—å¹•å¹¶é…éŸ³&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/clone-voice&#34;&gt;å£°éŸ³å…‹éš†å·¥å…·:ç”¨ä»»æ„éŸ³è‰²åˆæˆè¯­éŸ³&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt&#34;&gt;äººå£°èƒŒæ™¯ä¹åˆ†ç¦»:æç®€çš„äººå£°å’ŒèƒŒæ™¯éŸ³ä¹åˆ†ç¦»å·¥å…·ï¼Œæœ¬åœ°åŒ–ç½‘é¡µæ“ä½œ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;è‡´è°¢&lt;/h1&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä¸»è¦ä¾èµ–çš„å…¶ä»–é¡¹ç›®&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whipser&#34;&gt;https://github.com/openai/whipser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pallets/flask&#34;&gt;https://github.com/pallets/flask&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://layui.dev&#34;&gt;https://layui.dev&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>netease-youdao/BCEmbedding</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/netease-youdao/BCEmbedding</id>
    <link href="https://github.com/netease-youdao/BCEmbedding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Netease Youdao&#39;s open-source embedding and reranker models for RAG products.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;BCEmbedding: Bilingual and Crosslingual Embedding for RAG&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong style=&#34;background-color: green;&#34;&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/README_zh.md&#34; target=&#34;_Self&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Click to Open Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-bilingual-and-crosslingual-superiority&#34; target=&#34;_Self&#34;&gt;ğŸŒ Bilingual and Crosslingual Superiority&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-key-features&#34; target=&#34;_Self&#34;&gt;ğŸ’¡ Key Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-latest-updates&#34; target=&#34;_Self&#34;&gt;ğŸš€ Latest Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-model-list&#34; target=&#34;_Self&#34;&gt;ğŸ Model List&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-manual&#34; target=&#34;_Self&#34;&gt;ğŸ“– Manual&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#installation&#34; target=&#34;_Self&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#quick-start&#34; target=&#34;_Self&#34;&gt;Quick Start (&lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;sentence-transformers&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#integrations-for-rag-frameworks&#34; target=&#34;_Self&#34;&gt;Integrations for RAG Frameworks (&lt;code&gt;langchain&lt;/code&gt;, &lt;code&gt;llama_index&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#%EF%B8%8F-evaluation&#34; target=&#34;_Self&#34;&gt;âš™ï¸ Evaluation&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#evaluate-semantic-representation-by-mteb&#34; target=&#34;_Self&#34;&gt;Evaluate Semantic Representation by MTEB&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#evaluate-rag-by-llamaindex&#34; target=&#34;_Self&#34;&gt;Evaluate RAG by LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-leaderboard&#34; target=&#34;_Self&#34;&gt;ğŸ“ˆ Leaderboard&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#semantic-representation-evaluations-in-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-youdaos-bcembedding-api&#34; target=&#34;_Self&#34;&gt;ğŸ›  Youdao&#39;s BCEmbedding API&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-wechat-group&#34; target=&#34;_Self&#34;&gt;ğŸ§² WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#%EF%B8%8F-citation&#34; target=&#34;_Self&#34;&gt;âœï¸ Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-license&#34; target=&#34;_Self&#34;&gt;ğŸ” License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-related-links&#34; target=&#34;_Self&#34;&gt;ğŸ”— Related Links&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;ilingual and &lt;strong&gt;C&lt;/strong&gt;rosslingual &lt;strong&gt;Embedding&lt;/strong&gt; (&lt;code&gt;BCEmbedding&lt;/code&gt;), developed by NetEase Youdao, encompasses &lt;code&gt;EmbeddingModel&lt;/code&gt; and &lt;code&gt;RerankerModel&lt;/code&gt;. The &lt;code&gt;EmbeddingModel&lt;/code&gt; specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the &lt;code&gt;RerankerModel&lt;/code&gt; excels at refining search results and ranking tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;BCEmbedding&lt;/code&gt; serves as the cornerstone of Youdao&#39;s Retrieval Augmented Generation (RAG) implmentation, notably &lt;a href=&#34;http://qanything.ai&#34;&gt;QAnything&lt;/a&gt; [&lt;a href=&#34;https://github.com/netease-youdao/qanything&#34;&gt;github&lt;/a&gt;], an open-source implementation widely integrated in various Youdao products like &lt;a href=&#34;https://read.youdao.com/#/home&#34;&gt;Youdao Speed Reading&lt;/a&gt; and &lt;a href=&#34;https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation&#34;&gt;Youdao Translation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Distinguished for its bilingual and crosslingual proficiency, &lt;code&gt;BCEmbedding&lt;/code&gt; excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A high performence on &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#semantic-representation-evaluations-in-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸŒ Bilingual and Crosslingual Superiority&lt;/h2&gt; &#xA;&lt;p&gt;Existing embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. &lt;code&gt;BCEmbedding&lt;/code&gt;, leveraging the strength of Youdao&#39;s translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;EmbeddingModel&lt;/code&gt; supports &lt;em&gt;&lt;strong&gt;Chinese (ch) and English (en)&lt;/strong&gt;&lt;/em&gt; (more languages support will come soon), while &lt;code&gt;RerankerModel&lt;/code&gt; supports &lt;em&gt;&lt;strong&gt;Chinese (ch), English (en), Japanese (ja) and Korean (ko)&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’¡ Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bilingual and Crosslingual Proficiency&lt;/strong&gt;: Powered by Youdao&#39;s translation engine, excelling in Chinese, English and their crosslingual retrieval task, with upcoming support for additional languages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RAG-Optimized&lt;/strong&gt;: Tailored for diverse RAG tasks including &lt;strong&gt;translation, summarization, and question answering&lt;/strong&gt;, ensuring accurate &lt;strong&gt;query understanding&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient and Precise Retrieval&lt;/strong&gt;: Dual-encoder for efficient retrieval of &lt;code&gt;EmbeddingModel&lt;/code&gt; in first stage, and cross-encoder of &lt;code&gt;RerankerModel&lt;/code&gt; for enhanced precision and deeper semantic analysis in second stage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad Domain Adaptability&lt;/strong&gt;: Trained on diverse datasets for superior performance across various fields.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-Friendly Design&lt;/strong&gt;: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meaningful Reranking Scores&lt;/strong&gt;: &lt;code&gt;RerankerModel&lt;/code&gt; provides relevant scores to improve result quality and optimize large language model performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Proven in Production&lt;/strong&gt;: Successfully implemented and validated in Youdao&#39;s products.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸš€ Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Model Releases&lt;/strong&gt; - &lt;a href=&#34;https://huggingface.co/maidalun1020/bce-embedding-base_v1&#34;&gt;bce-embedding-base_v1&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/maidalun1020/bce-reranker-base_v1&#34;&gt;bce-reranker-base_v1&lt;/a&gt; are available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Eval Datasets&lt;/strong&gt; [&lt;a href=&#34;https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset&#34;&gt;CrosslingualMultiDomainsDataset&lt;/a&gt;] - Evaluate the performence of RAG, using &lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Eval Datasets&lt;/strong&gt; [&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/evaluation/c_mteb/Retrieval.py&#34;&gt;Details&lt;/a&gt;] - Evaluate the performence of crosslingual semantic representation, using &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ Model List&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Languages&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bce-embedding-base_v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;EmbeddingModel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ch, en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;279M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/maidalun1020/bce-embedding-base_v1&#34;&gt;Huggingface&lt;/a&gt;, &lt;a href=&#34;https://www.modelscope.cn/models/maidalun/bce-embedding-base_v1/summary&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bce-reranker-base_v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;RerankerModel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ch, en, ja, ko&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;279M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/maidalun1020/bce-reranker-base_v1&#34;&gt;Huggingface&lt;/a&gt;, &lt;a href=&#34;https://www.modelscope.cn/models/maidalun/bce-reranker-base_v1/summary&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ“– Manual&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;First, create a conda environment and activate it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name bce python=3.10 -y&#xA;conda activate bce&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install &lt;code&gt;BCEmbedding&lt;/code&gt; for minimal installation (To avoid cuda version conflicting, you should install &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/a&gt; that is compatible to your system cuda version manually first):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install BCEmbedding==0.1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:netease-youdao/BCEmbedding.git&#xA;cd BCEmbedding&#xA;pip install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;h4&gt;1. Based on &lt;code&gt;BCEmbedding&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Use &lt;code&gt;EmbeddingModel&lt;/code&gt;, and &lt;code&gt;cls&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/models/embedding.py#L24&#34;&gt;pooler&lt;/a&gt; is default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from BCEmbedding import EmbeddingModel&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init embedding model&#xA;model = EmbeddingModel(model_name_or_path=&#34;maidalun1020/bce-embedding-base_v1&#34;)&#xA;&#xA;# extract embeddings&#xA;embeddings = model.encode(sentences)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;RerankerModel&lt;/code&gt; to calculate relevant scores and rerank:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from BCEmbedding import RerankerModel&#xA;&#xA;# your query and corresponding passages&#xA;query = &#39;input_query&#39;&#xA;passages = [&#39;passage_0&#39;, &#39;passage_1&#39;, ...]&#xA;&#xA;# construct sentence pairs&#xA;sentence_pairs = [[query, passage] for passage in passages]&#xA;&#xA;# init reranker model&#xA;model = RerankerModel(model_name_or_path=&#34;maidalun1020/bce-reranker-base_v1&#34;)&#xA;&#xA;# method 0: calculate scores of sentence pairs&#xA;scores = model.compute_score(sentence_pairs)&#xA;&#xA;# method 1: rerank passages&#xA;rerank_results = model.rerank(query, passages)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/models/reranker.py#L137&#34;&gt;&lt;code&gt;RerankerModel.rerank&lt;/code&gt;&lt;/a&gt; method, we provide an advanced preproccess that we use in production for making &lt;code&gt;sentence_pairs&lt;/code&gt;, when &#34;passages&#34; are very long.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Based on &lt;code&gt;transformers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;EmbeddingModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init model and tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;maidalun1020/bce-embedding-base_v1&#39;)&#xA;model = AutoModel.from_pretrained(&#39;maidalun1020/bce-embedding-base_v1&#39;)&#xA;&#xA;device = &#39;cuda&#39;  # if no GPU, set &#34;cpu&#34;&#xA;model.to(device)&#xA;&#xA;# get inputs&#xA;inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=&#34;pt&#34;)&#xA;inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}&#xA;&#xA;# get embeddings&#xA;outputs = model(**inputs_on_device, return_dict=True)&#xA;embeddings = outputs.last_hidden_state[:, 0]  # cls pooler&#xA;embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;RerankerModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForSequenceClassification&#xA;&#xA;# init model and tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;maidalun1020/bce-reranker-base_v1&#39;)&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#39;maidalun1020/bce-reranker-base_v1&#39;)&#xA;&#xA;device = &#39;cuda&#39;  # if no GPU, set &#34;cpu&#34;&#xA;model.to(device)&#xA;&#xA;# get inputs&#xA;inputs = tokenizer(sentence_pairs, padding=True, truncation=True, max_length=512, return_tensors=&#34;pt&#34;)&#xA;inputs_on_device = {k: v.to(device) for k, v in inputs.items()}&#xA;&#xA;# calculate scores&#xA;scores = model(**inputs_on_device, return_dict=True).logits.view(-1,).float()&#xA;scores = torch.sigmoid(scores)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Based on &lt;code&gt;sentence_transformers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;EmbeddingModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import SentenceTransformer&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init embedding model&#xA;## New update for sentence-trnasformers. So clean up your &#34;`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1&#34; or &#34;ï½/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1&#34; first for downloading new version.&#xA;model = SentenceTransformer(&#34;maidalun1020/bce-embedding-base_v1&#34;)&#xA;&#xA;# extract embeddings&#xA;embeddings = model.encode(sentences, normalize_embeddings=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;RerankerModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import CrossEncoder&#xA;&#xA;# init reranker model&#xA;model = CrossEncoder(&#39;maidalun1020/bce-reranker-base_v1&#39;, max_length=512)&#xA;&#xA;# calculate scores of sentence pairs&#xA;scores = model.predict(sentence_pairs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Integrations for RAG Frameworks&lt;/h3&gt; &#xA;&lt;h4&gt;1. Used in &lt;code&gt;langchain&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.embeddings import HuggingFaceEmbeddings&#xA;from langchain_community.vectorstores import FAISS&#xA;from langchain_community.vectorstores.utils import DistanceStrategy&#xA;&#xA;query = &#39;apples&#39;&#xA;passages = [&#xA;        &#39;I like apples&#39;, &#xA;        &#39;I like oranges&#39;, &#xA;        &#39;Apples and oranges are fruits&#39;&#xA;    ]&#xA;  &#xA;# init embedding model&#xA;model_name = &#39;maidalun1020/bce-embedding-base_v1&#39;&#xA;model_kwargs = {&#39;device&#39;: &#39;cuda&#39;}&#xA;encode_kwargs = {&#39;batch_size&#39;: 64, &#39;normalize_embeddings&#39;: True, &#39;show_progress_bar&#39;: False}&#xA;&#xA;embed_model = HuggingFaceEmbeddings(&#xA;    model_name=model_name,&#xA;    model_kwargs=model_kwargs,&#xA;    encode_kwargs=encode_kwargs&#xA;  )&#xA;&#xA;# example #1. extract embeddings&#xA;query_embedding = embed_model.embed_query(query)&#xA;passages_embeddings = embed_model.embed_documents(passages)&#xA;&#xA;# example #2. langchain retriever example&#xA;faiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)&#xA;&#xA;retriever = faiss_vectorstore.as_retriever(search_type=&#34;similarity&#34;, search_kwargs={&#34;score_threshold&#34;: 0.5, &#34;k&#34;: 3})&#xA;&#xA;related_passages = retriever.get_relevant_documents(query)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Used in &lt;code&gt;llama_index&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.embeddings import HuggingFaceEmbedding&#xA;from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader&#xA;from llama_index.node_parser import SimpleNodeParser&#xA;from llama_index.llms import OpenAI&#xA;&#xA;query = &#39;apples&#39;&#xA;passages = [&#xA;        &#39;I like apples&#39;, &#xA;        &#39;I like oranges&#39;, &#xA;        &#39;Apples and oranges are fruits&#39;&#xA;    ]&#xA;&#xA;# init embedding model&#xA;model_args = {&#39;model_name&#39;: &#39;maidalun1020/bce-embedding-base_v1&#39;, &#39;max_length&#39;: 512, &#39;embed_batch_size&#39;: 64, &#39;device&#39;: &#39;cuda&#39;}&#xA;embed_model = HuggingFaceEmbedding(**model_args)&#xA;&#xA;# example #1. extract embeddings&#xA;query_embedding = embed_model.get_query_embedding(query)&#xA;passages_embeddings = embed_model.get_text_embedding_batch(passages)&#xA;&#xA;# example #2. rag example&#xA;llm = OpenAI(model=&#39;gpt-3.5-turbo-0613&#39;, api_key=os.environ.get(&#39;OPENAI_API_KEY&#39;), api_base=os.environ.get(&#39;OPENAI_BASE_URL&#39;))&#xA;service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)&#xA;&#xA;documents = SimpleDirectoryReader(input_files=[&#34;BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf&#34;]).load_data()&#xA;node_parser = SimpleNodeParser.from_defaults(chunk_size=512)&#xA;nodes = node_parser.get_nodes_from_documents(documents[0:36])&#xA;index = VectorStoreIndex(nodes, service_context=service_context)&#xA;query_engine = index.as_query_engine()&#xA;response = query_engine.query(&#34;What is llama?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;âš™ï¸ Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluate Semantic Representation by MTEB&lt;/h3&gt; &#xA;&lt;p&gt;We provide evaluation tools for &lt;code&gt;embedding&lt;/code&gt; and &lt;code&gt;reranker&lt;/code&gt; models, based on &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C_MTEB&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, install &lt;code&gt;MTEB&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install mteb==1.1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;1. Embedding Models&lt;/h4&gt; &#xA;&lt;p&gt;Just run following cmd to evaluate &lt;code&gt;your_embedding_model&lt;/code&gt; (e.g. &lt;code&gt;maidalun1020/bce-embedding-base_v1&lt;/code&gt;) in &lt;strong&gt;bilingual and crosslingual settings&lt;/strong&gt; (e.g. &lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The total evaluation tasks contain &lt;em&gt;&lt;strong&gt;114 datastes&lt;/strong&gt;&lt;/em&gt; of &lt;strong&gt;&#34;Retrieval&#34;, &#34;STS&#34;, &#34;PairClassification&#34;, &#34;Classification&#34;, &#34;Reranking&#34; and &#34;Clustering&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;All models are evaluated in their recommended pooling method (&lt;code&gt;pooler&lt;/code&gt;)&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;mean&lt;/code&gt; pooler: &#34;jina-embeddings-v2-base-en&#34;, &#34;m3e-base&#34;, &#34;m3e-large&#34;, &#34;e5-large-v2&#34;, &#34;multilingual-e5-base&#34;, &#34;multilingual-e5-large&#34; and &#34;gte-large&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cls&lt;/code&gt; pooler: Other models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&#34;jina-embeddings-v2-base-en&#34; model should be loaded with &lt;code&gt;trust_remote_code&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path {mean_pooler_models} --pooler mean&#xA;&#xA;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Reranker Models&lt;/h4&gt; &#xA;&lt;p&gt;Run following cmd to evaluate &lt;code&gt;your_reranker_model&lt;/code&gt; (e.g. &#34;maidalun1020/bce-reranker-base_v1&#34;) in &lt;strong&gt;bilingual and crosslingual settings&lt;/strong&gt; (e.g. &lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The evaluation tasks contain &lt;em&gt;&lt;strong&gt;12 datastes&lt;/strong&gt;&lt;/em&gt; of &lt;strong&gt;&#34;Reranking&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;3. Metrics Visualization Tool&lt;/h4&gt; &#xA;&lt;p&gt;We proveide a one-click script to sumarize evaluation results of &lt;code&gt;embedding&lt;/code&gt; and &lt;code&gt;reranker&lt;/code&gt; models as &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluation Summary&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluation Summary&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluate RAG by LlamaIndex&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; is a famous data framework for LLM-based applications, particularly in RAG. Recently, a &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt; has evaluated the popular embedding and reranker models in RAG pipeline and attracts great attention. Now, we follow its pipeline to evaluate our &lt;code&gt;BCEmbedding&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, install LlamaIndex, and upgrade &lt;code&gt;transformers&lt;/code&gt; to 4.36.0:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers==4.36.0&#xA;&#xA;pip install llama-index==0.9.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Export your &#34;openai&#34; and &#34;cohere&#34; app keys, and openai base url (e.g. &#34;&lt;a href=&#34;https://api.openai.com/v1&#34;&gt;https://api.openai.com/v1&lt;/a&gt;&#34;) to env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_BASE_URL={openai_base_url}  # https://api.openai.com/v1&#xA;export OPENAI_API_KEY={your_openai_api_key}&#xA;export COHERE_APPKEY={your_cohere_api_key}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;1. Metrics Definition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Hit Rate:&lt;/p&gt; &lt;p&gt;Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it&#39;s about how often our system gets it right within the top few guesses. &lt;em&gt;&lt;strong&gt;The larger, the better.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mean Reciprocal Rank (MRR):&lt;/p&gt; &lt;p&gt;For each query, MRR evaluates the system&#39;s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it&#39;s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it&#39;s second, the reciprocal rank is 1/2, and so on. &lt;em&gt;&lt;strong&gt;The larger, the better.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Reproduce &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;In order to compare our &lt;code&gt;BCEmbedding&lt;/code&gt; with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our &lt;code&gt;BCEmbedding&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# There should be two GPUs available at least.&#xA;CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, sumarize the evaluation results by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_reproduce_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results Reproduced from the LlamaIndex Blog can be checked in &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md&#34;&gt;Reproduced Summary of RAG Evaluation&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;, with some obvious &lt;em&gt;&lt;strong&gt;conclusions&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. Broad Domain Adaptability&lt;/h4&gt; &#xA;&lt;p&gt;The evaluation of &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt; is &lt;strong&gt;monolingual, small amount of data, and specific domain&lt;/strong&gt; (just including &#34;llama2&#34; paper). In order to evaluate the &lt;strong&gt;broad domain adaptability, bilingual and crosslingual capability&lt;/strong&gt;, we follow the blog to build a multiple domains evaluation dataset (includding &#34;Computer Science&#34;, &#34;Physics&#34;, &#34;Biology&#34;, &#34;Economics&#34;, &#34;Math&#34;, and &#34;Quantitative Finance&#34;. &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/tools/eval_rag/eval_pdfs/&#34;&gt;Details&lt;/a&gt;), named &lt;a href=&#34;https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset&#34;&gt;CrosslingualMultiDomainsDataset&lt;/a&gt;, &lt;strong&gt;by OpenAI &lt;code&gt;gpt-4-1106-preview&lt;/code&gt; for high quality&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, run following cmd to evaluate the most popular and powerful embedding and reranker models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# There should be two GPUs available at least.&#xA;CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the following script to sumarize the evaluation results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The summary of multiple domains evaluations can be seen in &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#1-multiple-domains-scenarios&#34; target=&#34;_Self&#34;&gt;Multiple Domains Scenarios&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“ˆ Leaderboard&lt;/h2&gt; &#xA;&lt;h3&gt;Semantic Representation Evaluations in MTEB&lt;/h3&gt; &#xA;&lt;h4&gt;1. Embedding Models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dimensions&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pooler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Instructions&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval (47)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS (19)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification (5)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification (21)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking (12)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering (15)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;AVG&lt;/strong&gt;&lt;/em&gt; (119)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gte-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gte-large-zh&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.51&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;jina-embeddings-v2-base-en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;e5-large-v2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;multilingual-e5-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;multilingual-e5-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.79&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.61&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.09&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.50&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our &lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt; outperforms other opensource embedding models with comparable model sizes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;114 datastes including 119 eval results&lt;/strong&gt;&lt;/em&gt; (some dataset contain multiple languages) of &#34;Retrieval&#34;, &#34;STS&#34;, &#34;PairClassification&#34;, &#34;Classification&#34;, &#34;Reranking&#34; and &#34;Clustering&#34; in &lt;em&gt;&lt;strong&gt;&lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt; setting&lt;/strong&gt;&lt;/em&gt;, including &lt;strong&gt;MTEB and CMTEB&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/evaluation/c_mteb/Retrieval.py&#34;&gt;crosslingual evaluation datasets&lt;/a&gt; we released belong to &lt;code&gt;Retrieval&lt;/code&gt; task.&lt;/li&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluations&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Reranker Models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking (12)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;AVG&lt;/strong&gt;&lt;/em&gt; (12)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our &lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt; outperforms other opensource reranker models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;12 datastes&lt;/strong&gt;&lt;/em&gt; of &#34;Reranking&#34; in &lt;em&gt;&lt;strong&gt;&lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt; setting&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluations&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG Evaluations in LlamaIndex&lt;/h3&gt; &#xA;&lt;h4&gt;1. Multiple Domains Scenarios&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/assets/rag_eval_multiple_domains_summary.jpg&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consistent with our &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md&#34;&gt;Reproduced Results&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; of &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ›  Youdao&#39;s BCEmbedding API&lt;/h2&gt; &#xA;&lt;p&gt;For users who prefer a hassle-free experience without the need to download and configure the model on their own systems, &lt;code&gt;BCEmbedding&lt;/code&gt; is readily accessible through Youdao&#39;s API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at &lt;a href=&#34;https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html&#34;&gt;Youdao BCEmbedding API&lt;/a&gt;. Here, you&#39;ll find all the necessary guidance to easily implement &lt;code&gt;BCEmbedding&lt;/code&gt; across a variety of use cases, ensuring a smooth and effective integration for optimal results.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ§² WeChat Group&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to scan the QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/assets/Wechat.jpg&#34; width=&#34;20%&#34; height=&#34;auto&#34;&gt; &#xA;&lt;h2&gt;âœï¸ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;BCEmbedding&lt;/code&gt; in your research or project, please feel free to cite and star it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{youdao_bcembedding_2023,&#xA;    title={BCEmbedding: Bilingual and Crosslingual Embedding for RAG},&#xA;    author={NetEase Youdao, Inc.},&#xA;    year={2023},&#xA;    howpublished={\url{https://github.com/netease-youdao/BCEmbedding}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ” License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;BCEmbedding&lt;/code&gt; is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”— Related Links&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/qanything&#34;&gt;Netease Youdao - QAnything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding&#34;&gt;FlagEmbedding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C_MTEB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LLama Index&lt;/a&gt; | &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>