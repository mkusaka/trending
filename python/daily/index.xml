<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-18T01:40:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>missuo/FuckSheepGame</title>
    <updated>2022-09-18T01:40:36Z</updated>
    <id>tag:github.com,2022-09-18:/missuo/FuckSheepGame</id>
    <link href="https://github.com/missuo/FuckSheepGame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;羊了个羊刷通关助手 支持QuanX和HTTP Catcher&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FuckSheepGame&lt;/h1&gt; &#xA;&lt;p&gt;羊了个羊刷通关次数&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;h3&gt;2022年9月17日&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修改接口为最新的 &lt;code&gt;map_info_new&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;写在前面&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本 Repo 仅供学习使用，禁止用于任何盈利为目的的用途。如有任何后果，作者概不负责。&lt;/li&gt; &#xA; &lt;li&gt;如果您没有开发的基础，可能对您来说会相对困难一些，当然也鼓励您尝试一下本通关方案。&lt;/li&gt; &#xA; &lt;li&gt;由于需要篡改 HTTPS 的请求，所以必须要在你的手机上安装证书，并且在关于本机 - 证书信任设置里面信任证书。&lt;/li&gt; &#xA; &lt;li&gt;QuanX 需要安装并且信任证书后，开启MITM和Rewrite。&lt;/li&gt; &#xA; &lt;li&gt;HTTP Catcher 需要开启解密HTTPS流量，并且开启重写列表。&lt;/li&gt; &#xA; &lt;li&gt;截止到2022年9月17日21点43分依然可用，如果遇到无法进入游戏，可能是服务器崩溃。（我会每天更新可用情况）&lt;/li&gt; &#xA; &lt;li&gt;如有任何疑问，请在 &lt;a href=&#34;https://github.com/missuo/FuckSheepGame/issues/new&#34;&gt;Issues&lt;/a&gt; 中提问，我会尽快为您解答。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第二关过关方式&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;code&gt;MITM&lt;/code&gt; 篡改请求，将 &lt;code&gt;map_id&lt;/code&gt; 的 &lt;code&gt;90014&lt;/code&gt; 修改为 &lt;code&gt;80001&lt;/code&gt; 即可。这样子你的第二关地图也会变成第一关的地图。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;iOS上可以使用 &lt;code&gt;QuanX&lt;/code&gt;、&lt;code&gt;Surge&lt;/code&gt;、&lt;code&gt;HTTP Catcher&lt;/code&gt;，Android上 &lt;strong&gt;没试过&lt;/strong&gt; 。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;code&gt;Surge&lt;/code&gt; 模块&lt;/h3&gt; &#xA;&lt;p&gt;开发中...&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;QuanX&lt;/code&gt; 脚本(&lt;strong&gt;强烈推荐&lt;/strong&gt;)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 羊了个羊 通关&#xA;https://raw.githubusercontent.com/missuo/FuckSheepGame/main/sheep.js&#xA;, tag=Sheep, update-interval=172800, opt-parser=true, enabled=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;复制上面的代码，粘贴到&lt;code&gt;QuanX&lt;/code&gt;配置文件的&lt;code&gt;[rewrite_remote]&lt;/code&gt;下。如果没有添加过资源解析器，请在&lt;code&gt;[general]&lt;/code&gt;下添加以下代码。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 资源解析器，自定义各类远程资源的转换&#xA;resource_parser_url=https://cdn.jsdelivr.net/gh/KOP-XIAO/QuantumultX@master/Scripts/resource-parser.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;HTTP Catcher&lt;/code&gt; 重写规则&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;rules&#34; : [&#xA;    {&#xA;      &#34;action&#34; : &#34;modify-query&#34;,&#xA;      &#34;matchField&#34; : &#34;map_id&#34;,&#xA;      &#34;field&#34; : &#34;&#34;,&#xA;      &#34;value&#34; : &#34;80001&#34;,&#xA;      &#34;matchValue&#34; : &#34;&#34;,&#xA;      &#34;destiontion&#34; : &#34;request&#34;,&#xA;      &#34;isRegex&#34; : false&#xA;    }&#xA;  ],&#xA;  &#34;enabled&#34; : true,&#xA;  &#34;name&#34; : &#34;羊羊羊&#34;,&#xA;  &#34;description&#34; : &#34;羊羊羊&#34;,&#xA;  &#34;locations&#34; : [&#xA;    {&#xA;      &#34;method&#34; : &#34;GET&#34;,&#xA;      &#34;scheme&#34; : &#34;https&#34;,&#xA;      &#34;enabled&#34; : true,&#xA;      &#34;port&#34; : 0,&#xA;      &#34;query&#34; : &#34;&#34;,&#xA;      &#34;host&#34; : &#34;cat-match.easygame2021.com&#34;,&#xA;      &#34;path&#34; : &#34;\/sheep\/v1\/game\/map_info_new&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;刷通关次数&lt;/h2&gt; &#xA;&lt;p&gt;修改 &lt;code&gt;t&lt;/code&gt; 为你自己的 &lt;code&gt;cookies&lt;/code&gt; ，运行脚本，运行一次通关一次。 &lt;code&gt;Cookies&lt;/code&gt; 可以使用 &lt;code&gt;Stream&lt;/code&gt; &lt;code&gt;HTTP Catcher&lt;/code&gt; &lt;code&gt;Proxyman&lt;/code&gt; 等工具安装信任证书开启解密HTTPS后打开游戏抓取。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip3 install requests&#xA;python3 sheep.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ansible/ansible</title>
    <updated>2022-09-18T01:40:36Z</updated>
    <id>tag:github.com,2022-09-18:/ansible/ansible</id>
    <link href="https://github.com/ansible/ansible" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|PyPI version| |Docs badge| |Chat badge| |Build Status| |Code Of Conduct| |Mailing Lists| |License| |CII Best Practices|&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Ansible&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;code&gt;website &amp;lt;https://ansible.com/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;Design Principles&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; &#xA; &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; &#xA; &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; &#xA; &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; &#xA; &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; &#xA; &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; &#xA; &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; &#xA; &lt;li&gt;Be usable as non-root.&lt;/li&gt; &#xA; &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Use Ansible&lt;/h1&gt; &#xA;&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;code&gt;installation guide &amp;lt;https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html&amp;gt;&lt;/code&gt;_ for details on installing Ansible on a variety of platforms.&lt;/p&gt; &#xA;&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;h1&gt;Get Involved&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read &lt;code&gt;Community Information &amp;lt;https://docs.ansible.com/ansible/latest/community&amp;gt;&lt;/code&gt;_ for all kinds of ways to contribute to and interact with the project, including mailing list information and how to submit bug reports and code to Ansible.&lt;/li&gt; &#xA; &lt;li&gt;Join a &lt;code&gt;Working Group &amp;lt;https://github.com/ansible/community/wiki&amp;gt;&lt;/code&gt;_, an organized community devoted to a specific technology domain or platform.&lt;/li&gt; &#xA; &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; &#xA; &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; &#xA; &lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the &lt;code&gt;Communication page &amp;lt;https://docs.ansible.com/ansible/latest/community/communication.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Coding Guidelines&lt;/h1&gt; &#xA;&lt;p&gt;We document our Coding Guidelines in the &lt;code&gt;Developer Guide &amp;lt;https://docs.ansible.com/ansible/devel/dev_guide/&amp;gt;&lt;/code&gt;_. We particularly suggest you review:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Contributing your module to Ansible &amp;lt;https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Conventions, tips, and pitfalls &amp;lt;https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Branch Info&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; &#xA; &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;code&gt;dev environment &amp;lt;https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup&amp;gt;&lt;/code&gt;_ if you want to open a PR.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;code&gt;Ansible release and maintenance &amp;lt;https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html&amp;gt;&lt;/code&gt;_ page for information about active branches.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;code&gt;Ansible Roadmap page &amp;lt;https://docs.ansible.com/ansible/devel/roadmap/&amp;gt;&lt;/code&gt;_ details what is planned and how to influence the roadmap.&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;p&gt;Ansible was created by &lt;code&gt;Michael DeHaan &amp;lt;https://github.com/mpdehaan&amp;gt;&lt;/code&gt;_ and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Ansible &amp;lt;https://www.ansible.com&amp;gt;&lt;/code&gt;_ is sponsored by &lt;code&gt;Red Hat, Inc. &amp;lt;https://www.redhat.com&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;COPYING &amp;lt;COPYING&amp;gt;&lt;/code&gt;_ to see the full text.&lt;/p&gt; &#xA;&lt;p&gt;.. |PyPI version| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/ansible-core.svg&#34;&gt;https://img.shields.io/pypi/v/ansible-core.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/ansible-core&#34;&gt;https://pypi.org/project/ansible-core&lt;/a&gt; .. |Docs badge| image:: &lt;a href=&#34;https://img.shields.io/badge/docs-latest-brightgreen.svg&#34;&gt;https://img.shields.io/badge/docs-latest-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://docs.ansible.com/ansible/latest/&#34;&gt;https://docs.ansible.com/ansible/latest/&lt;/a&gt; .. |Build Status| image:: &lt;a href=&#34;https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel&#34;&gt;https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel&lt;/a&gt; :target: &lt;a href=&#34;https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel&#34;&gt;https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel&lt;/a&gt; .. |Chat badge| image:: &lt;a href=&#34;https://img.shields.io/badge/chat-IRC-brightgreen.svg&#34;&gt;https://img.shields.io/badge/chat-IRC-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://docs.ansible.com/ansible/latest/community/communication.html&#34;&gt;https://docs.ansible.com/ansible/latest/community/communication.html&lt;/a&gt; .. |Code Of Conduct| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg&#34;&gt;https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg&lt;/a&gt; :target: &lt;a href=&#34;https://docs.ansible.com/ansible/latest/community/code_of_conduct.html&#34;&gt;https://docs.ansible.com/ansible/latest/community/code_of_conduct.html&lt;/a&gt; :alt: Ansible Code of Conduct .. |Mailing Lists| image:: &lt;a href=&#34;https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg&#34;&gt;https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg&lt;/a&gt; :target: &lt;a href=&#34;https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information&#34;&gt;https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information&lt;/a&gt; :alt: Ansible mailing lists .. |License| image:: &lt;a href=&#34;https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg&lt;/a&gt; :target: COPYING :alt: Repository License .. |CII Best Practices| image:: &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/2372/badge&#34;&gt;https://bestpractices.coreinfrastructure.org/projects/2372/badge&lt;/a&gt; :target: &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/2372&#34;&gt;https://bestpractices.coreinfrastructure.org/projects/2372&lt;/a&gt; :alt: Ansible CII Best Practices certification&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlfoundations/open_clip</title>
    <updated>2022-09-18T01:40:36Z</updated>
    <id>tag:github.com,2022-09-18:/mlfoundations/open_clip</id>
    <link href="https://github.com/mlfoundations/open_clip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source implementation of CLIP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenCLIP&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.01903&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb&#34;&gt;[Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to an open source implementation of OpenAI&#39;s &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;CLIP&lt;/a&gt; (Contrastive Language-Image Pre-training).&lt;/p&gt; &#xA;&lt;p&gt;The goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. Our starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. Specifically, a ResNet-50 model trained with our codebase on OpenAI&#39;s &lt;a href=&#34;https://github.com/openai/CLIP/raw/main/data/yfcc100m.md&#34;&gt;15 million image subset of YFCC&lt;/a&gt; achieves &lt;strong&gt;32.7%&lt;/strong&gt; top-1 accuracy on ImageNet. OpenAI&#39;s CLIP model reaches &lt;strong&gt;31.3%&lt;/strong&gt; when trained on the same subset of YFCC. For ease of experimentation, we also provide code for training on the 3 million images in the &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/download&#34;&gt;Conceptual Captions&lt;/a&gt; dataset, where a ResNet-50x4 trained with our codebase reaches 22.2% top-1 ImageNet accuracy.&lt;/p&gt; &#xA;&lt;p&gt;We further this with a replication study on a dataset of comparable size to OpenAI&#39;s, &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt;, and with the larger &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-2B&lt;/a&gt; superset.&lt;/p&gt; &#xA;&lt;p&gt;We have trained:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ViT-B/32 on LAION-400M with a accuracy of &lt;strong&gt;62.9%&lt;/strong&gt;, comparable to OpenAI&#39;s &lt;strong&gt;63.2%&lt;/strong&gt;, zero-shot top-1 on ImageNet1k&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/32 on LAION-2B with a accuracy of &lt;strong&gt;66.6%&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/16 on LAION-400M achieving an accuracy of &lt;strong&gt;67.1%&lt;/strong&gt;, lower than OpenAI&#39;s &lt;strong&gt;68.3%&lt;/strong&gt; (as measured here, 68.6% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-B/16+ 240x240 (~50% more FLOPS than B/16 224x224) on LAION-400M achieving an accuracy of &lt;strong&gt;69.2%&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;ViT-L/14 on LAION-400M with an accuracy of &lt;strong&gt;72.77%&lt;/strong&gt;, vs OpenAI&#39;s &lt;strong&gt;75.5%&lt;/strong&gt; (as measured here, 75.3% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-L/14 on LAION-2B with an accuracy of &lt;strong&gt;75.3%&lt;/strong&gt;, vs OpenAI&#39;s &lt;strong&gt;75.5%&lt;/strong&gt; (as measured here, 75.3% in paper)&lt;/li&gt; &#xA; &lt;li&gt;ViT-H/14 on LAION-2B with an accuracy of &lt;strong&gt;78.0&lt;/strong&gt;. The best in1k zero-shot for released, open-source weights thus far.&lt;/li&gt; &#xA; &lt;li&gt;ViT-g/14 on LAION-2B with an accuracy of &lt;strong&gt;76.6&lt;/strong&gt;. This was trained on reduced schedule, same samples seen as 400M models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As we describe in more detail &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/#why-are-low-accuracy-clip-models-interesting&#34;&gt;below&lt;/a&gt;, CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow &lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;reliable scaling laws&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This codebase is work in progress, and we invite all to contribute in making it more acessible and useful. In the future, we plan to add support for TPU training and release larger models. We hope this codebase facilitates and promotes further research in contrastive image-text learning. Please submit an issue or send an email if you have any other requests or suggestions.&lt;/p&gt; &#xA;&lt;p&gt;Note that portions of &lt;code&gt;src/open_clip/&lt;/code&gt; modelling and tokenizer code are adaptations of OpenAI&#39;s official &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png&#34; alt=&#34;CLIP&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Credit: &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install open_clip_torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;import open_clip&#xA;&#xA;model, _, preprocess = open_clip.create_model_and_transforms(&#39;ViT-B-32-quickgelu&#39;, pretrained=&#39;laion400m_e32&#39;)&#xA;&#xA;image = preprocess(Image.open(&#34;CLIP.png&#34;)).unsqueeze(0)&#xA;text = open_clip.tokenize([&#34;a diagram&#34;, &#34;a dog&#34;, &#34;a cat&#34;])&#xA;&#xA;with torch.no_grad(), torch.cuda.amp.autocast():&#xA;    image_features = model.encode_image(image)&#xA;    text_features = model.encode_text(text)&#xA;    image_features /= image_features.norm(dim=-1, keepdim=True)&#xA;    text_features /= text_features.norm(dim=-1, keepdim=True)&#xA;&#xA;    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)&#xA;&#xA;print(&#34;Label probs:&#34;, text_probs)  # prints: [[1., 0., 0.]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compute billions of embeddings efficiently, you can use &lt;a href=&#34;https://github.com/rom1504/clip-retrieval&#34;&gt;clip-retrieval&lt;/a&gt; which has openclip support.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning on classification tasks&lt;/h2&gt; &#xA;&lt;p&gt;This repository is focused on training CLIP models. To fine-tune a &lt;em&gt;trained&lt;/em&gt; zero-shot model on a downstream classification task such as ImageNet, please see &lt;a href=&#34;https://github.com/mlfoundations/wise-ft&#34;&gt;our other repository: WiSE-FT&lt;/a&gt;. The &lt;a href=&#34;https://github.com/mlfoundations/wise-ft&#34;&gt;WiSE-FT repository&lt;/a&gt; contains code for our paper on &lt;a href=&#34;https://arxiv.org/abs/2109.01903&#34;&gt;Robust Fine-tuning of Zero-shot Models&lt;/a&gt;, in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;h3&gt;Conceptual Captions&lt;/h3&gt; &#xA;&lt;p&gt;OpenCLIP reads a CSV file with two columns: a path to an image, and a text caption. The names of the columns are passed as an argument to &lt;code&gt;main.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script &lt;code&gt;src/data/gather_cc.py&lt;/code&gt; will collect the Conceptual Captions images. First, download the &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/download&#34;&gt;Conceptual Captions URLs&lt;/a&gt; and then run the script from our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 src/data/gather_cc.py path/to/Train_GCC-training.tsv path/to/Validation_GCC-1.1.0-Validation.tsv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our training set contains 2.89M images, and our validation set contains 13K images.&lt;/p&gt; &#xA;&lt;h3&gt;YFCC and other datasets&lt;/h3&gt; &#xA;&lt;p&gt;In addition to specifying the training data via CSV files as mentioned above, our codebase also supports &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset&lt;/a&gt;, which is recommended for larger scale datasets. The expected format is a series of &lt;code&gt;.tar&lt;/code&gt; files. Each of these &lt;code&gt;.tar&lt;/code&gt; files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, &lt;code&gt;shard_001.tar&lt;/code&gt; could contain files such as &lt;code&gt;abc.jpg&lt;/code&gt; and &lt;code&gt;abc.txt&lt;/code&gt;. You can learn more about &lt;code&gt;webdataset&lt;/code&gt; at &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;https://github.com/webdataset/webdataset&lt;/a&gt;. We use &lt;code&gt;.tar&lt;/code&gt; files with 1,000 data points each, which we create using &lt;a href=&#34;https://github.com/webdataset/tarp&#34;&gt;tarp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download the YFCC dataset from &lt;a href=&#34;http://mmcommons.org/&#34;&gt;Multimedia Commons&lt;/a&gt;. Similar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers. The indices of images in this subset are in &lt;a href=&#34;https://github.com/openai/CLIP/raw/main/data/yfcc100m.md&#34;&gt;OpenAI&#39;s CLIP repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training CLIP&lt;/h2&gt; &#xA;&lt;h3&gt;Setup Environment and Install dependencies&lt;/h3&gt; &#xA;&lt;h4&gt;Conda&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a conda environment (heavily recommended)&#xA;conda create -n open_clip python=3.10&#xA;conda activate open_clip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install conda PyTorch as per &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Virtualenv&lt;/h4&gt; &#xA;&lt;p&gt;openclip also can be used with virtualenv with these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .env&#xA;source .env/bin/activate&#xA;pip install -U pip&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install pip PyTorch as per &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Test can be run with &lt;code&gt;make install-dev&lt;/code&gt; then &lt;code&gt;make test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Other dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Install open_clip pacakge and remaining dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd open_clip&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to train models, you will also need to install the packages from &lt;code&gt;requirements-training.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Sample single-process running code:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --save-frequency 1 \&#xA;    --zeroshot-frequency 1 \&#xA;    --report-to tensorboard \&#xA;    --train-data=&#34;/path/to/train_data.csv&#34;  \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --csv-img-key filepath \&#xA;    --csv-caption-key title \&#xA;    --imagenet-val=/path/to/imagenet/root/val/ \&#xA;    --warmup 10000 \&#xA;    --batch-size=128 \&#xA;    --lr=1e-3 \&#xA;    --wd=0.1 \&#xA;    --epochs=30 \&#xA;    --workers=8 \&#xA;    --model RN50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;imagenet-val&lt;/code&gt; is the path to the &lt;em&gt;validation&lt;/em&gt; set of ImageNet for zero-shot evaluation, not the training set! You can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the &lt;code&gt;val&lt;/code&gt; folder should contain subfolders. If it doest not, please use &lt;a href=&#34;https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh&#34;&gt;this script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-GPU and Beyond&lt;/h3&gt; &#xA;&lt;p&gt;This code has been battle tested up to 1024 A100s and offers a variety of solutions for distributed training. We include native support for SLURM clusters.&lt;/p&gt; &#xA;&lt;p&gt;As the number of devices used to train increases, so does the space complexity of the the logit matrix. Using a naïve all-gather scheme, space complexity will be &lt;code&gt;O(n^2)&lt;/code&gt;. Instead, complexity may become effectively linear if the flags &lt;code&gt;--gather-with-grad&lt;/code&gt; and &lt;code&gt;--local-loss&lt;/code&gt; are used. This alteration results in one-to-one numerical results as the naïve method.&lt;/p&gt; &#xA;&lt;h4&gt;Single-Node&lt;/h4&gt; &#xA;&lt;p&gt;We make use of &lt;code&gt;torchrun&lt;/code&gt; to launch distributed jobs. The following launches a a job on a node of 4 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd open_clip/src&#xA;torchrun --nproc_per_node 4 -m training.main \&#xA;    --train-data &#39;/data/cc12m/cc12m-train-{0000..2175}.tar&#39; \&#xA;    --train-num-samples 10968539 \&#xA;    --dataset-type webdataset \&#xA;    --batch-size 320 \&#xA;    --precision amp \&#xA;    --workers 4 \&#xA;    --imagenet-val /data/imagenet/validation/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-Node&lt;/h4&gt; &#xA;&lt;p&gt;The same script above works, so long as users include information about the number of nodes and host node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd open_clip/src&#xA;torchrun --nproc_per_node=4 \&#xA;    --rdzv_endpoint=$HOSTE_NODE_ADDR \&#xA;    -m training.main \&#xA;    --train-data &#39;/data/cc12m/cc12m-train-{0000..2175}.tar&#39; \&#xA;    --train-num-samples 10968539 \&#xA;    --dataset-type webdataset \&#xA;    --batch-size 320 \&#xA;    --precision amp \&#xA;    --workers 4 \&#xA;    --imagenet-val /data/imagenet/validation/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SLURM&lt;/h4&gt; &#xA;&lt;p&gt;This is likely the easiest solution to utilize. The following script was used to train our largest models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash -x&#xA;#SBATCH --nodes=32&#xA;#SBATCH --gres=gpu:4&#xA;#SBATCH --ntasks-per-node=4&#xA;#SBATCH --cpus-per-task=6&#xA;#SBATCH --wait-all-nodes=1&#xA;#SBATCH --job-name=open_clip&#xA;#SBATCH --account=ACCOUNT_NAME&#xA;#SBATCH --partition PARTITION_NAME&#xA;&#xA;eval &#34;$(/path/to/conda/bin/conda shell.bash hook)&#34; # init conda&#xA;conda activate open_clip&#xA;export CUDA_VISIBLE_DEVICES=0,1,2,3&#xA;export MASTER_PORT=12802&#xA;&#xA;master_addr=$(scontrol show hostnames &#34;$SLURM_JOB_NODELIST&#34; | head -n 1)&#xA;export MASTER_ADDR=$master_addr&#xA;&#xA;cd /shared/open_clip&#xA;export PYTHONPATH=&#34;$PYTHONPATH:$PWD/src&#34;&#xA;srun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \&#xA;    --save-frequency 1 \&#xA;    --report-to tensorboard \&#xA;    --train-data=&#34;/data/LAION-400M/{00000..41455}.tar&#34; \&#xA;    --warmup 2000 \&#xA;    --batch-size=256 \&#xA;    --epochs=32 \&#xA;    --workers=8 \&#xA;    --model ViT-B-32 \&#xA;    --name &#34;ViT-B-32-Vanilla&#34; \&#xA;    --seed 0 \&#xA;    --local-loss \&#xA;    --gather-with-grad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resuming from a checkpoint:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --train-data=&#34;/path/to/train_data.csv&#34; \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --resume /path/to/checkpoints/epoch_K.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loss Curves&lt;/h3&gt; &#xA;&lt;p&gt;When run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png&#34; alt=&#34;CLIP zero shot training curve&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More detailed curves for Conceptual Captions are given at &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_conceptual_captions.md&#34;&gt;/docs/clip_conceptual_captions.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When training a RN50 on YFCC the same hyperparameters as above are used, with the exception of &lt;code&gt;lr=5e-4&lt;/code&gt; and &lt;code&gt;epochs=32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that to use another model, like &lt;code&gt;ViT-B/32&lt;/code&gt; or &lt;code&gt;RN50x4&lt;/code&gt; or &lt;code&gt;RN50x16&lt;/code&gt; or &lt;code&gt;ViT-B/16&lt;/code&gt;, specify with &lt;code&gt;--model RN50x4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Launch tensorboard:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tensorboard --logdir=logs/tensorboard/ --port=7777&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation / Zero-Shot&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluating local checkpoint:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --val-data=&#34;/path/to/validation_data.csv&#34;  \&#xA;    --model RN101 \&#xA;    --pretrained /path/to/checkpoints/epoch_K.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m training.main \&#xA;    --imagenet-val /path/to/imagenet/validation \&#xA;    --model ViT-B-32-quickgelu \&#xA;    --pretrained laion400m_e32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrained model details&lt;/h2&gt; &#xA;&lt;h3&gt;LAION-400M - &lt;a href=&#34;https://laion.ai/laion-400-open-dataset&#34;&gt;https://laion.ai/laion-400-open-dataset&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We are working on reproducing OpenAI&#39;s ViT results with the comparably sized (and open) LAION-400M dataset. Trained weights may be found in release &lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/tag/v0.2-weights&#34;&gt;v0.2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The LAION400M weights have been trained on the JUWELS supercomputer (see acknowledgements section below).&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 224x224&lt;/h4&gt; &#xA;&lt;p&gt;We replicate OpenAI&#39;s results on ViT-B/32, reaching a top-1 ImageNet-1k zero-shot accuracy of 62.96%.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot comparison (courtesy of Andreas Fürst)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_openai_compare_b32.jpg&#34; width=&#34;700&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/32 was trained with 128 A100 (40 GB) GPUs for ~36 hours, 4600 GPU-hours. The per-GPU batch size was 256 for a global batch size of 32768. 256 is much lower than it could have been (~320-384) due to being sized initially before moving to &#39;local&#39; contrastive loss.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/16 224x224&lt;/h4&gt; &#xA;&lt;p&gt;The B/16 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 67.07.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;This was the first major train session using the updated webdataset 0.2.x code. A bug was found that prevented shards from being shuffled properly between nodes/workers each epoch. This was fixed part way through training (epoch 26) but likely had an impact.&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/16 was trained with 176 A100 (40 GB) GPUS for ~61 hours, 10700 GPU-hours. Batch size per GPU was 192 for a global batch size of 33792.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/16+ 240x240&lt;/h4&gt; &#xA;&lt;p&gt;The B/16+ 240x240 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 69.21.&lt;/p&gt; &#xA;&lt;p&gt;This model is the same depth as the B/16, but increases the&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vision width from 768 -&amp;gt; 896&lt;/li&gt; &#xA; &lt;li&gt;text width from 512 -&amp;gt; 640&lt;/li&gt; &#xA; &lt;li&gt;the resolution 224x224 -&amp;gt; 240x240 (196 -&amp;gt; 225 tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16_plus_240.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;Unlike the B/16 run above, this model was a clean run with no dataset shuffling issues.&lt;/p&gt; &#xA;&lt;p&gt;ViT-B/16+ was trained with 224 A100 (40 GB) GPUS for ~61 hours, 13620 GPU-hours. Batch size per GPU was 160 for a global batch size of 35840.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-L/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;The L/14 LAION-400M training reached a top-1 ImageNet-1k zero-shot validation score of 72.77.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_l14.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;ViT-L/14 was trained with 400 A100 (40 GB) GPUS for ~127 hours, 50800 GPU-hours. Batch size per GPU was 96 for a global batch size of 38400. Grad checkpointing was enabled.&lt;/p&gt; &#xA;&lt;h3&gt;LAION-2B (en) - &lt;a href=&#34;https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/&#34;&gt;https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A ~2B sample subset of LAION-5B with english captions (&lt;a href=&#34;https://huggingface.co/datasets/laion/laion2B-en&#34;&gt;https://huggingface.co/datasets/laion/laion2B-en&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h4&gt;ViT-B/32 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-B/32 trained on LAION-2B, reaching a top-1 ImageNet-1k zero-shot accuracy of 65.62%.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion2b_clip_zeroshot_b32.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;ViT-B/32 was trained with 112 A100 (40 GB) GPUs. The per-GPU batch size was 416 for a global batch size of 46592. Compute generously provided by &lt;a href=&#34;https://stability.ai/&#34;&gt;stability.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A second iteration of B/32 was trained on stability.ai cluster with a larger global batch size and learning rate, hitting 66.6% top-1. See &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ViT-L/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-L/14 with a 75.3% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These weights use a different dataset mean and std than others. Instead of using the OpenAI mean &amp;amp; std, inception style normalization &lt;code&gt;[-1, 1]&lt;/code&gt; is used via a mean and std of &lt;code&gt;[0.5, 0.5, 0.5]&lt;/code&gt;. This is handled automatically if using &lt;code&gt;open_clip.create_model_and_transforms&lt;/code&gt; from pretrained weights.&lt;/p&gt; &#xA;&lt;h4&gt;ViT-H/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-H/14 with a 78.0% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ViT-g/14 224x224&lt;/h4&gt; &#xA;&lt;p&gt;A ViT-H/14 with a 76.6% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K&#34;&gt;https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This model was trained with a shorted schedule than other LAION-2B models with 12B samples seen instead of 32+B. It matches LAION-400M training in samples seen. Many zero-shot results are lower as a result, but despite this it performs very well in some OOD zero-shot and retrieval tasks.&lt;/p&gt; &#xA;&lt;h4&gt;YFCC-15M&lt;/h4&gt; &#xA;&lt;p&gt;Below are checkpoints of models trained on YFCC-15M, along with their zero-shot top-1 accuracies on ImageNet and ImageNetV2. These models were trained using 8 GPUs and the same hyperparameters described in the &#34;Sample running code&#34; section, with the exception of &lt;code&gt;lr=5e-4&lt;/code&gt; and &lt;code&gt;epochs=32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt&#34;&gt;ResNet-50&lt;/a&gt; (32.7% / 27.9%)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt&#34;&gt;ResNet-101&lt;/a&gt; (34.8% / 30.0%)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CC12M - &lt;a href=&#34;https://github.com/google-research-datasets/conceptual-12m&#34;&gt;https://github.com/google-research-datasets/conceptual-12m&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt&#34;&gt;ResNet-50&lt;/a&gt; (36.45%)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pretrained Model Interface&lt;/h3&gt; &#xA;&lt;p&gt;We offer a simple model interface to instantiate both pre-trained and untrained models.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient that native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with &lt;code&gt;-quickgelu&lt;/code&gt; postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non &lt;code&gt;-quickgelu&lt;/code&gt; model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.&lt;/p&gt; &#xA;&lt;p&gt;Future trained models will use nn.GELU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import open_clip&#xA;&amp;gt;&amp;gt;&amp;gt; open_clip.list_pretrained()&#xA;[(&#39;RN50&#39;, &#39;openai&#39;),&#xA; (&#39;RN50&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50&#39;, &#39;cc12m&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50-quickgelu&#39;, &#39;cc12m&#39;),&#xA; (&#39;RN101&#39;, &#39;openai&#39;),&#xA; (&#39;RN101&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN101-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;RN101-quickgelu&#39;, &#39;yfcc15m&#39;),&#xA; (&#39;RN50x4&#39;, &#39;openai&#39;),&#xA; (&#39;RN50x16&#39;, &#39;openai&#39;),&#xA; (&#39;RN50x64&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion2b_e16&#39;),&#xA; (&#39;ViT-B-32&#39;, &#39;laion2b_s34b_b79k&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-32-quickgelu&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-16&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-B-16-plus-240&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-B-16-plus-240&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion400m_e31&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion400m_e32&#39;),&#xA; (&#39;ViT-L-14&#39;, &#39;laion2b_s32b_b82k&#39;),&#xA; (&#39;ViT-L-14-336&#39;, &#39;openai&#39;),&#xA; (&#39;ViT-H-14&#39;, &#39;laion2b_s32b_b79k&#39;),&#xA; (&#39;ViT-g-14&#39;, &#39;laion2b_s12b_b42k&#39;)]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; model, train_transform, eval_transform = open_clip.create_model_and_transforms(&#39;ViT-B-32&#39;, pretrained=&#39;laion2b_s34b_b79k&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scaling trends&lt;/h2&gt; &#xA;&lt;p&gt;The plot below shows how zero-shot performance of CLIP models varies as we scale the number of samples used for training. Zero-shot performance increases steadily for both ImageNet and &lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;ImageNetV2&lt;/a&gt;, and is far from saturated at ~15M samples.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/scaling.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;h2&gt;Why are low-accuracy CLIP models interesting?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; CLIP models have high effective robustness, even at small scales.&lt;/p&gt; &#xA;&lt;p&gt;CLIP models are particularly intriguing because they are more robust to natural distribution shifts (see Section 3.3 in the &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;CLIP paper&lt;/a&gt;). This phenomena is illustrated by the figure below, with ImageNet accuracy on the x-axis and &lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;ImageNetV2&lt;/a&gt; (a reproduction of the ImageNet validation set with distribution shift) accuracy on the y-axis. Standard training denotes training on the ImageNet train set and the CLIP zero-shot models are shown as stars.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/effective_robustness.png&#34; alt=&#34;CLIP scatter plot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As observed by &lt;a href=&#34;https://arxiv.org/abs/2007.00644&#34;&gt;Taori et al., 2020&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;Miller et al., 2021&lt;/a&gt;, the in-distribution and out-of-distribution accuracies of models trained on ImageNet follow a predictable linear trend (the red line in the above plot). &lt;em&gt;Effective robustness&lt;/em&gt; quantifies robustness as accuracy beyond this baseline, i.e., how far a model lies above the red line. Ideally a model would not suffer from distribution shift and fall on the y = x line (&lt;a href=&#34;http://proceedings.mlr.press/v119/shankar20c.html&#34;&gt;trained human labelers are within a percentage point of the y = x line&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Even though the CLIP models trained with this codebase achieve much lower accuracy than those trained by OpenAI, our models still lie on the same trend of improved effective robustness (the purple line). Therefore, we can study what makes CLIP robust without requiring industrial-scale compute.&lt;/p&gt; &#xA;&lt;p&gt;For more information on effective robustness, please see:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.10811&#34;&gt;Recht et al., 2019&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00644&#34;&gt;Taori et al., 2020&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.04649&#34;&gt;Miller et al., 2021&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To know more about the factors that contribute to CLIP&#39;s robustness refer to &lt;a href=&#34;https://arxiv.org/abs/2205.01397&#34;&gt;Fang et al., 2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (&lt;a href=&#34;http://www.gauss-centre.eu&#34;&gt;www.gauss-centre.eu&lt;/a&gt;) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC).&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;Current development of this repository is led by &lt;a href=&#34;https://rwightman.com/&#34;&gt;Ross Wightman&lt;/a&gt;, &lt;a href=&#34;http://cadegordon.io/&#34;&gt;Cade Gordon&lt;/a&gt;, and &lt;a href=&#34;http://vaishaal.com/&#34;&gt;Vaishaal Shankar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://gabrielilharco.com/&#34;&gt;Gabriel Ilharco*&lt;/a&gt;, &lt;a href=&#34;https://mitchellnw.github.io/&#34;&gt;Mitchell Wortsman*&lt;/a&gt;, &lt;a href=&#34;https://nicholas.carlini.com/&#34;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&#34;http://www.achaldave.com/&#34;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&#34;http://vaishaal.com/&#34;&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~miller_john/&#34;&gt;John Miller&lt;/a&gt;, &lt;a href=&#34;https://hsnamkoong.github.io/&#34;&gt;Hongseok Namkoong&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~hannaneh/&#34;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~ali/&#34;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/ludwigs/&#34;&gt;Ludwig Schmidt&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://jongwook.kim/&#34;&gt;Jong Wook Kim&lt;/a&gt; and &lt;a href=&#34;https://github.com/Newmu&#34;&gt;Alec Radford&lt;/a&gt; for help with reproducing CLIP!&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{ilharco_gabriel_2021_5143773,&#xA;  author       = {Ilharco, Gabriel and&#xA;                  Wortsman, Mitchell and&#xA;                  Wightman, Ross and&#xA;                  Gordon, Cade and&#xA;                  Carlini, Nicholas and&#xA;                  Taori, Rohan and&#xA;                  Dave, Achal and&#xA;                  Shankar, Vaishaal and&#xA;                  Namkoong, Hongseok and&#xA;                  Miller, John and&#xA;                  Hajishirzi, Hannaneh and&#xA;                  Farhadi, Ali and&#xA;                  Schmidt, Ludwig},&#xA;  title        = {OpenCLIP},&#xA;  month        = jul,&#xA;  year         = 2021,&#xA;  note         = {If you use this software, please cite it as below.},&#xA;  publisher    = {Zenodo},&#xA;  version      = {0.1},&#xA;  doi          = {10.5281/zenodo.5143773},&#xA;  url          = {https://doi.org/10.5281/zenodo.5143773}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Radford2021LearningTV,&#xA;  title={Learning Transferable Visual Models From Natural Language Supervision},&#xA;  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},&#xA;  booktitle={ICML},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/390536799&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/390536799.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>