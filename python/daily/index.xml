<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-27T01:41:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZHO-ZHO-ZHO/ComfyUI-InstantID</title>
    <updated>2024-01-27T01:41:39Z</updated>
    <id>tag:github.com,2024-01-27:/ZHO-ZHO-ZHO/ComfyUI-InstantID</id>
    <link href="https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of InstantID for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/01393483-3145-4691-9daa-7ce9035c9bd0&#34; alt=&#34;ISID_&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ComfyUI InstantID&lt;/h1&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;InstantID&lt;/a&gt; for ComfyUI&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/7a99b32c-b4a2-4c46-acb0-f796fc46f9ee&#34; alt=&#34;Dingtalk_20240123182131&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pose_ref&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/caa60456-f2d8-4315-864b-659a9e7cea89&#34; alt=&#34;Dingtalk_20240124232946&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目介绍 | Info&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;来自对&lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;InstantID&lt;/a&gt;的非官方实现&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;版本：V2.0 支持姿势参考图&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;  同时支持本地、huggingface hub模型，支持通用styler（也与 PhotoMaker Styler 通用）&#xA;---&gt; &#xA;&lt;h2&gt;视频演示&lt;/h2&gt; &#xA;&lt;p&gt;V2.0&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/083c9e5e-06a0-4623-b5ac-05f7e85a74f2&#34;&gt;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/083c9e5e-06a0-4623-b5ac-05f7e85a74f2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;V1.0&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/7295c0d7-1d1b-4044-aea3-8efa67047362&#34;&gt;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/7295c0d7-1d1b-4044-aea3-8efa67047362&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;节点说明 | Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;基础模型加载 | base model loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📷ID Base Model Loader from hub 🤗：支持从 huggingface hub 自动下载模型，输入模型名称（如：wangqixun/YamerMIX_v8）即可&lt;/li&gt; &#xA;   &lt;li&gt;📷ID Base Model Loader locally：支持加载本地模型（需 SDXL 系列模型）&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;InsightFace 模型加载 | 📷InsightFace Loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;：支持 CUDA 和 CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ID ControlNet 模型加载 | 📷ID ControlNet Loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;controlnet_path：ID ControlNet 模型地址&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ipadapter_instantid 模型加载 | 📷Ipadapter_instantid Loader&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ipadapter_instantid_path：模型路径&lt;/li&gt; &#xA;   &lt;li&gt;filename：模型名称&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;提示词 + 风格 | 📷ID Prompt_Styler&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;与各种提示词（文本）输入（如肖像大师等）、styler、 Photomaker Prompt_Styler 兼容&lt;/li&gt; &#xA;   &lt;li&gt;prompt、negative：正负提示词&lt;/li&gt; &#xA;   &lt;li&gt;style_name：支持官方提供的8种风格 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;(No style)&lt;/li&gt; &#xA;     &lt;li&gt;Watercolor&lt;/li&gt; &#xA;     &lt;li&gt;Film Noir&lt;/li&gt; &#xA;     &lt;li&gt;Neon&lt;/li&gt; &#xA;     &lt;li&gt;Jungle&lt;/li&gt; &#xA;     &lt;li&gt;Mars&lt;/li&gt; &#xA;     &lt;li&gt;Vibrant Color&lt;/li&gt; &#xA;     &lt;li&gt;Snow&lt;/li&gt; &#xA;     &lt;li&gt;Line art&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;InstantID 生成 | 📷InstantID Generation 🆕&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;face_image：接入脸部参考图像&lt;/li&gt; &#xA;   &lt;li&gt;pipe：接入模型&lt;/li&gt; &#xA;   &lt;li&gt;insightface：接入 insightface 模型 🆕&lt;/li&gt; &#xA;   &lt;li&gt;pose_image_optional（非必要）：接入姿势参考图像（注意：仅对面部周围姿势起效，与通常的 openpose 不同）&lt;/li&gt; &#xA;   &lt;li&gt;positivet、negative：正负提示词&lt;/li&gt; &#xA;   &lt;li&gt;ip_adapter_scale：IPA 强度&lt;/li&gt; &#xA;   &lt;li&gt;controlnet_conditioning_scale：ID Controlnet 强度&lt;/li&gt; &#xA;   &lt;li&gt;step：步数，官方默认30步&lt;/li&gt; &#xA;   &lt;li&gt;guidance_scale：提示词相关度，一般默认为5&lt;/li&gt; &#xA;   &lt;li&gt;enhance_face_region：脸部增强选项 🆕&lt;/li&gt; &#xA;   &lt;li&gt;seed：种子&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;风格 | Styles&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/142bda7a-798b-46b3-aa69-1b88701c8311&#34; alt=&#34;ISID_STYLE&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;安装 | Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;推荐使用管理器 ComfyUI Manager 安装（On the Way）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;手动安装：&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd custom_nodes&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;git clone https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID.git&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd custom_nodes/ComfyUI-InstantID&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;重启 ComfyUI&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用方法 | How to Use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;下载 &lt;a href=&#34;https://huggingface.co/InstantX/InstantID/tree/main/ControlNetModel&#34;&gt;InstantID/ControlNetModel&lt;/a&gt; 中的 config.json 和 diffusion_pytorch_model.safetensors ，将模型地址填入 📷ID ControlNet Loader 节点中（例如：ComfyUI/custom_nodes/ComfyUI-InstantID/checkpoints/controlnet）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;下载 &lt;a href=&#34;https://huggingface.co/InstantX/InstantID/tree/main&#34;&gt;InstantID/ip-adapter&lt;/a&gt; 中的 ip-adapter.bin ，将其地址填入 📷Ipadapter_instantid Loader 节点中（例如：ComfyUI/custom_nodes/ComfyUI-InstantID/checkpoints）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;下载 &lt;a href=&#34;https://huggingface.co/DIAMONIK7777/antelopev2/tree/main&#34;&gt;DIAMONIK7777/antelopev2&lt;/a&gt; 中的所有模型，将其放入 ComfyUI//custom_nodes/ComfyUI-InstantID/models/antelopev2 中&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;兼容性： CUDA11 支持默认安装的 onnxruntime-gpu（1.16.0），如果是 CUDA12 则需手动安装 onnxruntime-gpu==1.17.0 &lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_artifacts/feed/onnxruntime-cuda-12/PyPI/onnxruntime-gpu/overview/1.17.0&#34;&gt;地址&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;工作流 | Workflows&lt;/h2&gt; &#xA;&lt;p&gt;V2.0&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V2.0%20InstantID_pose_ref%20%2B%20ArtGallery%20%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.0 InstantID_pose_ref + ArtGallery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/99be9592-775d-4c33-bafc-5bd5c95a7222&#34; alt=&#34;Dingtalk_20240124232833&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V2.0%20InstantID_fromhub_pose_ref%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.0 自动下载 huggingface hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/95c4a1dd-864d-4a46-8c45-a48866aef29f&#34; alt=&#34;Dingtalk_20240124230145&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V2.0%20InstantID_locally_pose_ref%E3%80%90Zho%E3%80%91.json&#34;&gt;V2.0 InstantID_locally_pose_ref&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/d4c22389-f853-44bd-9ea2-568b2ac7ed06&#34; alt=&#34;Dingtalk_20240124230609&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;V1.0 工作流仅适用于V1.0 版本&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V1.0%20InstantID%20%2B%20ArtGallery%E3%80%90Zho%E3%80%91.json&#34;&gt;V1.0 InstantID + ArtGallery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/c6ee25bf-a528-4d78-9b35-f5b0d0303601&#34; alt=&#34;Dingtalk_20240123182440&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V1.0%20InstantID_locally%E3%80%90Zho%E3%80%91.json&#34;&gt;V1.0 本地模型 locally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/459bfede-59e8-4d8d-941c-a950c4827c49&#34; alt=&#34;Dingtalk_20240123175624&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/raw/main/INSTANTID%20WORKFLOWS/V1.0%20InstantID_fromhub%E3%80%90Zho%E3%80%91.json&#34;&gt;V1.0 自动下载 huggingface hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/50133961-1752-4ec8-ac0b-068d998b8534&#34; alt=&#34;Dingtalk_20240123174950&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;20240124&lt;/p&gt; &lt;p&gt;更新为 V2.0 ：新增姿势参考图、优化代码&lt;/p&gt; &lt;p&gt;修复 insightfaceloader 冲突问题&lt;/p&gt; &lt;p&gt;修复 onnxruntime-gpu 版本兼容性的问题&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240123&lt;/p&gt; &lt;p&gt;V1.0 上线：同时支持本地、huggingface hub托管模型，支持8种风格&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20240122&lt;/p&gt; &lt;p&gt;创建项目&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;速度实测 | Speed&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;V1.0&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A100 50步 14s&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID/assets/140084057/dc535e67-3f56-4faf-be81-621b84bb6ee2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stars&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#ZHO-ZHO-ZHO/ComfyUI-PhotoMaker&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=ZHO-ZHO-ZHO/ComfyUI-InstantID&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;InstantID&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📷InsightFace Loader 代码修改自 &lt;a href=&#34;https://github.com/cubiq/ComfyUI_IPAdapter_plus&#34;&gt;ComfyUI_IPAdapter_plus&lt;/a&gt;，感谢 &lt;a href=&#34;https://github.com/cubiq&#34;&gt;@cubiq&lt;/a&gt;！&lt;/p&gt; &#xA;&lt;p&gt;感谢 &lt;a href=&#34;https://twitter.com/hidecloud&#34;&gt;@hidecloud&lt;/a&gt; 对 onnxruntime 版本兼容性的测试与反馈！&lt;/p&gt; &#xA;&lt;p&gt;感谢 &lt;a href=&#34;https://www.esheep.com/&#34;&gt;esheep&lt;/a&gt; 技术人员对节点冲突问题的反馈！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/GenerativeAIExamples</title>
    <updated>2024-01-27T01:41:39Z</updated>
    <id>tag:github.com,2024-01-27:/NVIDIA/GenerativeAIExamples</id>
    <link href="https://github.com/NVIDIA/GenerativeAIExamples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative AI reference workflows optimized for accelerated infrastructure and microservice architecture.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA Generative AI Examples&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;State-of-the-art Generative AI examples that are easy to deploy, test, and extend. All examples run on the high performance NVIDIA CUDA-X software stack and NVIDIA GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;NVIDIA NGC&lt;/h2&gt; &#xA;&lt;p&gt;Generative AI Examples uses resources from the &lt;a href=&#34;https://ngc.nvidia.com&#34;&gt;NVIDIA NGC AI Development Catalog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Sign up for a &lt;a href=&#34;https://ngc.nvidia.com/signin&#34;&gt;free NGC developer account&lt;/a&gt; to access:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU-optimized containers used in these examples&lt;/li&gt; &#xA; &lt;li&gt;Release notes and developer documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Retrieval Augmented Generation (RAG)&lt;/h2&gt; &#xA;&lt;p&gt;A RAG pipeline embeds multimodal data -- such as documents, images, and video -- into a database connected to a LLM. RAG lets users chat with their data!&lt;/p&gt; &#xA;&lt;h3&gt;Developer RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The developer RAG examples run on a single VM. They demonstrate how to combine NVIDIA GPU acceleration with popular LLM programming frameworks using NVIDIA&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#open-source-integrations&#34;&gt;open source connectors&lt;/a&gt;. The examples are easy to deploy via &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Examples support local and remote inference endpoints. If you have a GPU, you can inference locally via &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;. If you don&#39;t have a GPU, you can inference and embed remotely via &lt;a href=&#34;https://www.nvidia.com/en-us/ai-data-science/foundation-models/&#34;&gt;NVIDIA AI Foundations endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA AI Foundation&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2&lt;/td&gt; &#xA;   &lt;td&gt;e5-large-v2&lt;/td&gt; &#xA;   &lt;td&gt;Llamaindex&lt;/td&gt; &#xA;   &lt;td&gt;Canonical QA Chatbot&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/README.md#3-qa-chatbot-multi-gpu----a100h100l40s&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/README.md#2-qa-chatbot----a100h100l40s-gpu&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;Milvus/&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/(RetrievalAugmentedGeneration/README.md#2-qa-chatbot----a100h100l40s-gpu)&#34;&gt;PGVector&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mixtral_8x7b&lt;/td&gt; &#xA;   &lt;td&gt;nvolveqa_40k&lt;/td&gt; &#xA;   &lt;td&gt;Langchain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/README.md#1-qa-chatbot----nvidia-ai-foundation-inference-endpoint&#34;&gt;Nvidia AI foundation based QA Chatbot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2&lt;/td&gt; &#xA;   &lt;td&gt;all-MiniLM-L6-v2&lt;/td&gt; &#xA;   &lt;td&gt;Llama Index&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NVIDIA/trt-llm-rag-windows/tree/release/1.0&#34;&gt;QA Chatbot, GeForce, Windows&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2&lt;/td&gt; &#xA;   &lt;td&gt;nvolveqa_40k&lt;/td&gt; &#xA;   &lt;td&gt;Langchain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/README.md#5-qa-chatbot-with-task-decomposition-example----a100h100l40s&#34;&gt;QA Chatbot, Task Decomposition Agent&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mixtral_8x7b&lt;/td&gt; &#xA;   &lt;td&gt;nvolveqa_40k&lt;/td&gt; &#xA;   &lt;td&gt;Langchain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/examples/README.md#rag-in-5-minutes-example&#34;&gt;Minimilastic example showcasing RAG using Nvidia AI foundation models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Enterprise RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The enterprise RAG examples run as microservies distributed across multiple VMs and GPUs. They show how RAG pipelines can be orchestrated with &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; and deployed with &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples include a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Kubernetes operator&lt;/a&gt; for LLM lifecycle management. It is compatible with the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/gpu-operator&#34;&gt;NVIDIA GPU operator&lt;/a&gt; that automates GPU discovery and lifecycle management in a Kubernetes cluster.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples also support local and remote inference via &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt; and &lt;a href=&#34;https://www.nvidia.com/en-us/ai-data-science/foundation-models/&#34;&gt;NVIDIA AI Foundations endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;Multi-node&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA AI Foundation&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2&lt;/td&gt; &#xA;   &lt;td&gt;NV-Embed-QA-003&lt;/td&gt; &#xA;   &lt;td&gt;Llamaindex&lt;/td&gt; &#xA;   &lt;td&gt;QA Chatbot, Helm, k8s&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/developer-llm-operator/&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;Milvus&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;Example tools and tutorials to enhance LLM development and productivity when using NVIDIA RAG pipelines.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Deployment&lt;/th&gt; &#xA;   &lt;th&gt;Tutorial&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;Example open source RAG eval tool that uses synthetic data generation and LLM-as-a-judge&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/deploy/compose/docker-compose-evaluation.yaml&#34;&gt;Docker compose file&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/rag/evaluation.md&#34;&gt;README&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Observability&lt;/td&gt; &#xA;   &lt;td&gt;Observability serves as an efficient mechanism for both monitoring and debugging RAG pipelines.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/deploy/compose/docker-compose-observability.yaml&#34;&gt;Docker compose file&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/rag/observability.md&#34;&gt;README&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open Source Integrations&lt;/h2&gt; &#xA;&lt;p&gt;These are open source connectors for NVIDIA-hosted and self-hosted API endpoints. These open source connectors are maintained and tested by NVIDIA engineers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Chat&lt;/th&gt; &#xA;   &lt;th&gt;Text Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/providers/nvidia&#34;&gt;NVIDIA AI Foundation Endpoints&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-ai-endpoints/&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Easy access to NVIDIA hosted models. Supports chat, embedding, code generation, steerLM, multimodal, and RAG.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/tree/master/libs/partners/nvidia-trt&#34;&gt;NVIDIA Triton + TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/raw/master/libs/partners/nvidia-trt/docs/llms.ipynb&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/raw/master/libs/partners/nvidia-trt/docs/llms.ipynb&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-trt/&#34;&gt;YES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This connector allows Langchain to remotely interact with a Triton inference server over GRPC or HTTP tfor optimized LLM inference.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_triton.html&#34;&gt;NVIDIA Triton Inference Server&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;Triton inference server provides API access to hosted LLM models over gRPC.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html&#34;&gt;NVIDIA TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT-LLM provides a Python API to build TensorRT engines with state-of-the-art optimizations for LLM inference on NVIDIA GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;NVIDIA support&lt;/h2&gt; &#xA;&lt;p&gt;In each example README we indicate the level of support provided.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback / Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re posting these examples on GitHub to support the NVIDIA LLM community, facilitate feedback. We invite contributions via GitHub Issues or pull requests!&lt;/p&gt; &#xA;&lt;h2&gt;Known issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In each of the READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/li&gt; &#xA; &lt;li&gt;The datasets provided as part of this project is under a different license for research and evaluation purposes.&lt;/li&gt; &#xA; &lt;li&gt;This project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>YangLing0818/RPG-DiffusionMaster</title>
    <updated>2024-01-27T01:41:39Z</updated>
    <id>tag:github.com,2024-01-27:/YangLing0818/RPG-DiffusionMaster</id>
    <link href="https://github.com/YangLing0818/RPG-DiffusionMaster" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs (PRG)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs&lt;/h2&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2401.11708&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Arxiv:RPG&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt;   &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repository contains the official implementation of our &lt;a href=&#34;https://arxiv.org/abs/2401.11708&#34;&gt;RPG&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.11708&#34;&gt;&lt;strong&gt;Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://yangling0818.github.io/&#34;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&#34;https://github.com/BitCodingWalkin&#34;&gt;Zhaochen Yu&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~chenlin/&#34;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&#34;https://minkaixu.com/&#34;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~ermon/&#34;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&#34;https://cuibinpku.github.io/&#34;&gt;Bin Cui&lt;/a&gt; &lt;br&gt;&lt;strong&gt;Peking University, Stanford University, Pika Labs&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;100%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/method.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;100%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;Overview of our RPG &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: RPG is a powerful training-free paradigm that can utilize proprietary MLLMs (e.g., GPT-4, Gemini-Pro) or open-source local MLLMs (e.g., miniGPT-4) as the &lt;strong&gt;prompt recaptioner and region planner&lt;/strong&gt; with our &lt;strong&gt;complementary regional diffusion&lt;/strong&gt; to achieve SOTA text-to-image generation and editing. Our framework is very flexible and can generalize to arbitrary MLLM architectures and diffusion backbones. RPG is also capable of generating image with super high resolutions, here is an example:&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;100%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/icefire.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;100%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;Text prompt: A beautiful landscape with a river in the middle the left of the river is in the evening and in the winter with a big iceberg and a small village while some people are skating on the river and some people are skiing, the right of the river is in the summer with a volcano in the morning and a small village while some people are playing. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🚩 New Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.1]&lt;/strong&gt; Our main code along with the demo release, supporting different diffusion backbones (&lt;strong&gt;SDXL&lt;/strong&gt;, &lt;strong&gt;SD v2.0/2.1&lt;/strong&gt; &lt;strong&gt;SD v1.4/1.5&lt;/strong&gt;), and one can reproduce our good results utilizing GPT-4 and Gemini-Pro. Our RPG is also compatible with local MLLMs, and we will continue to improve the results in the future.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Gradio demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release RPG for image editing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release RPG v2 with ControlNet&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release RPG v1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;h3&gt;1. Multi-people with complex attribute binding&lt;/h3&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;1024*1024 Examples&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/people/cafe.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/people/cowboy.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/people/couple.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/people/tea.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;A girl with white ponytail and black dress are chatting with a blonde curly hair girl in a white dress in a cafe.&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;A twin-tail girl wearing a brwon cowboy hat and white shirt printed with apples, and blue denim jeans with knee boots,full body shot.&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;A couple, the beautiful girl on the left, silver hair, braided ponytail, happy, dynamic, energetic, peaceful, the handsome young man on the right detailed gorgeous face, grin, blonde hair, enchanting&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt; Two beautiful Chinese girls wearing cheongsams are drinking tea in the tea room, and a Chinese Landscape Painting is hanging on the wall, the girl on the left is black ponytail in red cheongsam, the girl on the right is white ponytail in orange cheongsam&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; 2048*1024 Example&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;100%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/people/three.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;100%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;From left to right, a blonde ponytail Europe girl in white shirt, a brown curly hair African girl in blue shirt printed with a bird, an Asian young man with black short hair in suit are walking in the campus happily.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;2. Multi-object with complex relationship&lt;/h3&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; 1024*1024 Examples&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/apple.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/mug.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/watermelon.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/ragdoll.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;From left to right, two red apples and an apple printed shirt and an ipad on the wooden floor &lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;Seven white ceramic mugs with different geometric patterns on the marble table while a bunch of rose on the left &lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;Five watermelons arranged in X shape on a wooden table, with the one in the middle being cut, realistic style, top down view. &lt;/td&gt; &#xA;    &lt;td width=&#34;25%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt; From left to right ,bathed in soft morning light,a cozy nook features a steaming Starbucks latte on a rustic table beside an elegant vase of blooming roses,while a plush ragdoll cat purrs contentedly nearby,its eyes half-closed in blissful serenity.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; 2048*1024 Example&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;100%&#34; style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/object/girl.png&#34; style=&#34;width:100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td width=&#34;100%&#34; style=&#34;border: none; text-align: center; word-wrap: break-word&#34;&gt;A green twintail girl in orange dress is sitting on the sofa while a messy desk under a big window on the left, a lively aquarium is on the top right of the sofa, realistic style &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;3. RPG With ControlNet&lt;/h3&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Open Pose Example&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr style=&#34;line-height: 0&#34;&gt; &#xA;    &lt;td colspan=&#34;2&#34; style=&#34;border: none; text-align: center&#34;&gt;Open Pose&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/Pose.png&#34;&gt;&lt;/td&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/Pose_girl.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; Text prompt: A beautiful black hair girl with her eyes closed in champagne long sleeved formal dress standing in her bright room with delicate blue vases with pink roses on the left and some white roses, filled with upgraded growth all around on the right. &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Depth Map Example&lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr style=&#34;line-height: 0&#34;&gt; &#xA;    &lt;td colspan=&#34;2&#34; style=&#34;border: none; text-align: center&#34;&gt;Depth Map&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/depth.jpg&#34; , style=&#34;width: 256px,height: 448px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/Depth_valley.png&#34; , style=&#34;width: 1024px, height:1792px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; Text prompt: Under the clear starry sky, clear river water flows in the mountains, and the lavender flower sea dances with the wind, a peaceful, beautiful, and harmonious atmosphere. &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Canny Edge Example &lt;/summary&gt; &#xA; &lt;table class=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr style=&#34;line-height: 0&#34;&gt; &#xA;    &lt;td colspan=&#34;2&#34; style=&#34;border: none; text-align: center&#34;&gt;Canny Edge&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/Canny.png&#34; , style=&#34;width: 768px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td style=&#34;border: none&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/__asset__/demo/Controlnet/Canny_town.png&#34; ,style=&#34;width: 2048px, height: 1024px&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; Text prompt: From left to right, an acient Chinese city in spring, summer, autumn and winter in four different regions &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Preparations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Set Environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/YangLing0818/RPG-DiffusionMaster&#xA;cd RPG-DiffusionMaster&#xA;conda create -n RPG python==3.9&#xA;conda activate RPG&#xA;pip install -r requirements.txt&#xA;mkdir repositories&#xA;mkdir -p generated_imgs/demo_imgs&#xA;mkdir models/Stable-diffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Download Libraries&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd repositories&#xA;git clone https://github.com/Stability-AI/generative-models&#xA;git clone https://github.com/Stability-AI/stablediffusion&#xA;git clone https://github.com/sczhou/CodeFormer&#xA;git clone https://github.com/crowsonkb/k-diffusion&#xA;git clone https://github.com/salesforce/BLIP&#xA;mv stablediffusion stable-diffusion-stability-ai&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Download Diffusion Models and MLLMs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In our experiments designed to attain state-of-the-art generative capabilities, we predominantly employ &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;SDXL&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/sdxl-turbo&#34;&gt;SDXL-Turbo&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic&#34;&gt;Playground v2&lt;/a&gt;. To generate images of high fidelity across various styles, such as photorealism, cartoons, and anime, we judiciously incorporate certain models from &lt;a href=&#34;https://civitai.com/&#34;&gt;CIVITA&lt;/a&gt;. For images aspiring to photorealism, we advocate the use of &lt;a href=&#34;https://civitai.com/models/140737/albedobase-xl?modelVersionId=281176&#34;&gt;AlbedoBase XL&lt;/a&gt; , and &lt;a href=&#34;https://civitai.com/models/112902/dreamshaper-xl?modelVersionId=251662&#34;&gt;DreamShaper XL&lt;/a&gt;. Moreover, we generalized our paradigm to SD v1.5 and SD v2.1 to accommodate a spectrum of requisites. All pertinent checkpoints are accessible within our &lt;a href=&#34;https://huggingface.co/BitStarWalkin/RPG_models&#34;&gt;Hugging Face spaces&lt;/a&gt;, with detailed descriptions found on the accompanying model cards. Then we need move the downloaded diffusion model weights into the folder &lt;strong&gt;models/Stable-diffusion/&lt;/strong&gt;, and please note that the generated images in generated_imgs/.&lt;/p&gt; &#xA;&lt;p&gt;We recommend the utilization of GPT-4 or Gemini-Pro for users of Multilingual Large Language Models (MLLMs), as they not only exhibit superior performance but also reduce local memory. According to our experiments, the minimum requirements of VRAM is 10GB with GPT-4, if you want to use local LLM, it would need more VRAM. For those interested in using MLLMs locally, we suggest deploying &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;miniGPT-4&lt;/a&gt; or directly engaging with substantial Local LLMs such as &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;Llama2-13b-chat&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&#34;&gt;Llama2-70b-chat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-Image Generation&lt;/h2&gt; &#xA;&lt;h4&gt;1. Quick Start&lt;/h4&gt; &#xA;&lt;p&gt;For individuals equipped with constrained computational resources, we here provide a simple notebook demonstration that partitions the image into two equal-sized subregions. By making minor alterations to select functions within the diffusers library, one may achieve commendable outcomes utilizing base diffusion models such as SD v1.4, v1.5, v2.0, and v2.1, as mentioned in our paper. Additionally, you can apply your customized configurations to experiment with a graphics card possessing 8GB of VRAM. For an in-depth exposition, kindly refer to our &lt;a href=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/RegionalDiffusion_playground.ipynb&#34;&gt;Example_Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;2. Demo&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Note that we have uploaded detailed parameters of some examples in our paper, to make perfect reproduction, the only thing is to download the models we specify in &lt;a href=&#34;https://raw.githubusercontent.com/YangLing0818/RPG-DiffusionMaster/main/template/demo.py&#34;&gt;demo.py&lt;/a&gt; and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python RPG.py --demo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the results in outputs/txt2img-images which caches the generated history, or directly in generated_imgs/demo_imgs/&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;3. Regional Diffusion with GPT-4&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Our approach can automatically generates output without pre-storing MLLM responses, leveraging Chain-of-Thought reasoning and high-quality in-context examples to obtain satisfactory results. Users only need to understand specific parameters. For example, to use GPT-4 as the planner, we can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python RPG.py --user_prompt &#39;A blonde hair girl with black suit and white skirt&#39; --model_name &#39;input your model name here&#39; --version_number 0 --api_key &#39;put your api_key here&#39; --use_gpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;--user_prompt&lt;/strong&gt; is the original prompt that roughly summarize the content contained in the image&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--model_name&lt;/strong&gt; is the name of the model in the directory models/Stable-diffusion/&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--version_number&lt;/strong&gt; is the class of our in-context examples used in generation. Our experiments suggest that in various scenarios, by employing proper in-context exemplars as few-shot samples, the planning capabilities of MLLMs can be substantially enhanced. For this case, we aim to synthesize a character bearing multiple attributes. We elect option 0, which is apt for a plan that binds multiple attributes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--api_key&lt;/strong&gt; is needed if you use GPT-4.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;4. Regional Diffusion with local LLMs&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;p&gt;We recommend to use base models with over 13 billion parameters for high-quality results, but it will increase load times and graphical memory use at the same time. We have conducted experiments on three different sized models, Here we take llama2-13b-chat as an example, we can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python RPG.py --user_prompt &#39;A blonde hair girl with black suit and white skirt&#39; --model_name &#39;input your model name here&#39; --version_number 0 --use_local --llm_path &#39;local_llms/llama2-13b-chat&#39; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In local version, we only need to clarify the local llm_path to use llm locally.&lt;/p&gt; &#xA;&lt;p&gt;Here we can also specify other usual parameters in diffusion model like:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--cfg&lt;/strong&gt; which is the context-free guidance scale&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--steps&lt;/strong&gt; the steps to generate an image&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--seed&lt;/strong&gt; control the seed to make the generation reproducible&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;It should be noted that we also introduce some new parameters into diffusion generation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--use_base&lt;/strong&gt; the function of this boolean variable is to activate the base prompt in diffusion process. Utilizing the base prompt signifies that we avoid the direct amalgamation of subregions as the latent representation. Instead, we use a foundational prompt that summarizes the image&#39;s key components and obatin the overall structure latent of the image. We then compute the weighted aggregate of these latents to yield the conclusive output. This method is instrumental in addressing the problems like omission of entities in complicated prompt generation tasks, and it also contributes to refining the edges of each subregion, ensuring they are seamlessly integrated and resonate harmony.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;--base_ratio&lt;/strong&gt; the weight of the base prompt latent, if too small, it is difficult to work, if too big, it will confuse the composition and properties of subregions. We conduct ablation experiment in our paper, see our paper for more detailed information and analysis.&lt;/p&gt; &#xA;&lt;h1&gt;📖BibTeX&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yang2024mastering,&#xA;  title={Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs},&#xA;  author={Yang, Ling and Yu, Zhaochen and Meng, Chenlin and Xu, Minkai and Ermon, Stefano and Cui, Bin},&#xA;  journal={arXiv preprint arXiv:2401.11708},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Our RPG is a general MLLM-controlled text-to-image generation/editing framework, which is builded upon several solid works. Thanks to &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111&lt;/a&gt;, &lt;a href=&#34;https://github.com/hako-mikan/sd-webui-regional-prompter&#34;&gt;regional-prompter&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34;&gt;IA&lt;/a&gt; for their wonderful work and codebase! We also thank Hugging Face for sharing our &lt;a href=&#34;https://huggingface.co/papers/2401.11708&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>