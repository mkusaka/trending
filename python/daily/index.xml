<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-18T01:42:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dsdanielpark/Bard-API</title>
    <updated>2023-05-18T01:42:58Z</updated>
    <id>tag:github.com,2023-05-18:/dsdanielpark/Bard-API</id>
    <link href="https://github.com/dsdanielpark/Bard-API" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The python package that returns response of Google Bard through API.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Development Status :: 5 - Production/Stable&lt;/p&gt; &#xA;&lt;h1&gt;Google &lt;a href=&#34;https://bard.google.com/&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/adb54264fe2ad5067d07d0752fc32600b4e6250073b01ce8c386575b431e3f06/68747470733a2f2f7777772e677374617469632e636f6d2f6c616d64612f696d616765732f66617669636f6e5f76315f31353031363063646466663766323934636533302e737667&#34; height=&#34;20px&#34;&gt;&lt;/a&gt; Bard API&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a&gt;&lt;img alt=&#34;PyPI package&#34; src=&#34;https://img.shields.io/badge/pypi-BardAPI-black&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://pepy.tech/project/bardapi&#34;&gt;&lt;img alt=&#34;Downloads&#34; src=&#34;https://pepy.tech/badge/bardapi&#34;&gt;&lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a&gt;&lt;img alt=&#34;commit update&#34; src=&#34;https://img.shields.io/github/last-commit/dsdanielpark/Bard-API?color=black&#34;&gt;&lt;/a&gt; --&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img alt=&#34;Code style: black&#34; src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dsdanielpark/Bard-API&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fdsdanielpark%2FBARD_API&amp;amp;count_bg=%23000000&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=BardAPI&amp;amp;edge_flat=false&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/bardapi/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/bardapi&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The python package that returns response of &lt;a href=&#34;https://bard.google.com/&#34;&gt;Google Bard&lt;/a&gt; through API.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dsdanielpark/Bard-API/main/assets/bard_api.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;I referred to &lt;a href=&#34;https://github.com/acheong08/Bard&#34;&gt;this github repository(github.com/acheong08/Bard)&lt;/a&gt; where inference process of Bard was reverse engineered. Using &lt;code&gt;__Secure-1PSID&lt;/code&gt;, you can ask questions and get answers from Google Bard. This package is designed for application to the Python package &lt;a href=&#34;https://github.com/dsdanielpark/ExceptNotifier&#34;&gt;ExceptNotifier&lt;/a&gt; and &lt;a href=&#34;https://github.com/dsdanielpark/Co-Coder&#34;&gt;Co-Coder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Do not expose the &lt;code&gt;__Secure-1PSID&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that while I referred to &lt;code&gt;__Secure-1PSID&lt;/code&gt; value as an API KEY for convenience, it is not an officially provided API KEY.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/dsdanielpark/amazing-bard-prompts&#34;&gt;Amazing Bard Prompts&lt;/a&gt; Is All You Need!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helpful prompts for Google Bard&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;The latest stable release (and required dependencies) can be installed from PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install bardapi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may instead want to use the development version from Github:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/dsdanielpark/Bard-API.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Authentication&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Visit &lt;a href=&#34;https://bard.google.com/&#34;&gt;https://bard.google.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;F12 for console&lt;/li&gt; &#xA; &lt;li&gt;Session: Application → Cookies → Copy the value of &lt;code&gt;__Secure-1PSID&lt;/code&gt; cookie.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1YIMA8aBmEQSSk90bB0Q9tznaLLQcluGA?usp=share_link&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simple Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bardapi import Bard&#xA;import os&#xA;&#xA;os.environ[&#39;_BARD_API_KEY&#39;]=&#34;xxxxxxxx&#34;&#xA;Bard().get_answer(&#34;나와 내 동년배들이 좋아하는 뉴진스에 대해서 알려줘&#34;)[&#39;content&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can use this&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bardapi&#xA;import os&#xA;&#xA;# set your __Secure-1PSID value to key&#xA;os.environ[&#39;_BARD_API_KEY&#39;]=&#34;xxxxxxxx&#34;&#xA;&#xA;# set your input text&#xA;input_text = &#34;나와 내 동년배들이 좋아하는 뉴진스에 대해서 알려줘&#34;&#xA;&#xA;# Send an API request and get a response.&#xA;response = bardapi.core.Bard().get_answer(input_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Addressing errors caused by delayed responses in environments like Google Colab and containers. If an error occurs despite following the proper procedure, utilize the timeout argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bardapi import Bard&#xA;import os&#xA;os.environ[&#39;_BARD_API_KEY&#39;]=&#34;xxxxxxxx&#34;&#xA;&#xA;bard = Bard(timeout=10) # Set timeout in seconds&#xA;bard.get_answer(&#34;나와 내 동년배들이 좋아하는 뉴진스에 대해서 알려줘&#34;)[&#39;content&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Futher&lt;/h2&gt; &#xA;&lt;h3&gt;Behind a proxy&lt;/h3&gt; &#xA;&lt;p&gt;If you are working behind a proxy, use the following.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bardapi import Bard&#xA;import os&#xA;&#xA;os.environ[&#39;_BARD_API_KEY&#39;]=&#34;xxxxxxxx&#34;&#xA;# Change &#39;http://127.0.0.1:1080&#39; to your http proxy&#xA;# timeout in seconds&#xA;bard = Bard(proxies={&#39;http&#39;:&#39;http://127.0.0.1:1080&#39;, &#39;https&#39;:&#39;http://127.0.0.1:1080&#39;}, timeout=10)&#xA;bard.get_answer(&#34;나와 내 동년배들이 좋아하는 뉴진스에 대해서 알려줘&#34;)[&#39;content&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reusable session object&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bardapi import Bard&#xA;import os&#xA;import requests&#xA;&#xA;os.environ[&#39;_BARD_API_KEY&#39;] = &#39;xxxxxxxxxxx&#39;&#xA;session = requests.Session()&#xA;session.headers = {&#xA;            &#34;Host&#34;: &#34;bard.google.com&#34;,&#xA;            &#34;X-Same-Domain&#34;: &#34;1&#34;,&#xA;            &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36&#34;,&#xA;            &#34;Content-Type&#34;: &#34;application/x-www-form-urlencoded;charset=UTF-8&#34;,&#xA;            &#34;Origin&#34;: &#34;https://bard.google.com&#34;,&#xA;            &#34;Referer&#34;: &#34;https://bard.google.com/&#34;,&#xA;        }&#xA;session.cookies.set(&#34;__Secure-1PSID&#34;, os.environ[&#34;_BARD_API_KEY&#34;])&#xA;&#xA;bard = Bard(session=session)&#xA;bard.get_answer(&#34;나와 내 동년배들이 좋아하는 뉴진스에 대해서 알려줘&#34;)[&#39;content&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Simple Example &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bard.google.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dsdanielpark/Bard-API/main/assets/bardimg.png&#34; height=&#34;600px&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;https://bard.google.com/&#34;&gt; &lt;br&gt; &lt;h2&gt;Scripts&lt;/h2&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://bard.google.com/&#34;&gt;In the scripts &lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dsdanielpark/Bard-API/main/scripts/&#34;&gt;folder&lt;/a&gt;, I have released a script to help you compare &lt;a href=&#34;https://raw.githubusercontent.com/dsdanielpark/Bard-API/main/scripts/openai_api.ipynb&#34;&gt;OpenAI-ChatGPT&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/dsdanielpark/Bard-API/main/scripts/google_api.ipynb&#34;&gt;Google-Bard&lt;/a&gt;. I hope they will help more developers.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/license/mit/&#34;&gt;MIT&lt;/a&gt; &lt;br&gt; I hold no legal responsibility; for more information, please refer to the bottom of the readme file. I just want you to give me and &lt;a href=&#34;https://github.com/acheong08/Bard&#34;&gt;them&lt;/a&gt; a star.&lt;/p&gt; &#xA;&lt;h2&gt;Bugs and Issues&lt;/h2&gt; &#xA;&lt;p&gt;Sincerely grateful for any reports on new features or bugs. Your valuable feedback on the code is highly appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Contacts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Core maintainer: &lt;a href=&#34;https://github.com/DSDanielPark&#34;&gt;Daniel Park, South Korea&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;E-mail: &lt;a href=&#34;mailto:parkminwoo1991@gmail.com&#34;&gt;parkminwoo1991@gmail.com&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://github.com/acheong08/Bard&#34;&gt;https://github.com/acheong08/Bard&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Important Notice&lt;/h3&gt; &#xA;&lt;p&gt;The user assumes all legal responsibilities associated with using the BardAPI package. This Python package merely facilitates easy access to Google Bard for developers. Users are solely responsible for managing data and using the package appropriately. For further information, please consult the Google Bard Official Document.&lt;/p&gt; &#xA;&lt;h4&gt;Could you kindly add this badge to your repository?&lt;/h4&gt; &#xA;&lt;p&gt;markdown&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;![BardAPI](https://img.shields.io/badge/pypi-BardAPI-black)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;html&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;a href=&#34;https://github.com/dsdanielpark/Bard-API&#34;&amp;gt;&amp;lt;img alt=&#34;PyPI package&#34; src=&#34;https://img.shields.io/badge/pypi-BardAPI-black&#34;&amp;gt;&amp;lt;/a&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Thank you for your interest.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Copyright (c) 2023 MinWoo Park, South Korea&lt;/em&gt;&lt;br&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>e-johnstonn/BriefGPT</title>
    <updated>2023-05-18T01:42:58Z</updated>
    <id>tag:github.com,2023-05-18:/e-johnstonn/BriefGPT</id>
    <link href="https://github.com/e-johnstonn/BriefGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Locally hosted tool that connects documents to LLMs for summarization and querying, with a simple GUI.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BriefGPT&lt;/h1&gt; &#xA;&lt;p&gt;BriefGPT is a powerful, locally-run tool for document summarization and querying using OpenAI&#39;s models. You retain control over your documents and API keys, ensuring privacy and security.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;Added support for fully local use! Instructor is used to embed documents, and the LLM can be either LlamaCpp or GPT4ALL, ggml formatted. Put your model in the &#39;models&#39; folder, set up your environmental variables (model type and path), and run &lt;code&gt;streamlit run local_app.py&lt;/code&gt; to get started. Tested with the following models: &lt;a href=&#34;https://huggingface.co/eachadea/ggml-vicuna-13b-1.1/blob/main/ggml-vic13b-q5_1.bin&#34;&gt;Llama&lt;/a&gt;, &lt;a href=&#34;https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin&#34;&gt;GPT4ALL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please note this is experimental - it will be significantly slower and the quality may vary. PR&#39;s welcome!&lt;/p&gt; &#xA;&lt;h1&gt;Example (using the &#34;Sparks of AGI&#34; paper, sped up)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ipgvsgb.gif&#34; alt=&#34;chat&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Setup&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA; &lt;li&gt;Download all requirements &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set your API key in test.env&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the project directory and run &lt;code&gt;streamlit run main.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add your PDF&#39;s or .txt&#39;s to the documents folder in the project directory&lt;/li&gt; &#xA; &lt;li&gt;If using epubs, ensure you have pandoc installed and added to PATH&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How it works&lt;/h1&gt; &#xA;&lt;h2&gt;Chat&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Creating and saving embeddings - once you load a file, it is broken into chunks and stored as a FAISS index in the &#39;embeddings&#39; folder. These embeddings will be used if you load the document into the chat again.&lt;/li&gt; &#xA; &lt;li&gt;Retrieving, ranking, and processing results - a similarity search is performed on the index to get the top n results. These results are then re-ranked by a function that strips the original query of stopwords and uses fuzzy matching to find the similarity in exact words between the query and the retrieved results. This gets better results than solely doing a similarity search.&lt;/li&gt; &#xA; &lt;li&gt;Output - the re-ranked results and the user query are passed to the llm, and the response is displayed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Summarization&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Input - can handle both documents and YouTube URL&#39;s - will find the transcript and generate a summary based off of that.&lt;/li&gt; &#xA; &lt;li&gt;Processing and embedding - before embedding, documents are stripped of any special tokens that might cause errors. Documents are embedded in chunks of varying size, depending on the overall document&#39;s size.&lt;/li&gt; &#xA; &lt;li&gt;Clustering - once the documents are embedded, they are grouped into clusters using the K-means algorithm. The number of clusters can be predetermined (10) or variable (finds optimal number based on the elbow method). The embedding closest to each cluster centroid is retrieved - each cluster might represent a different theme or idea, and the retrieved embeddings are those that best encapsulate that theme or idea - that&#39;s the goal, at least.&lt;/li&gt; &#xA; &lt;li&gt;Summarization - summarization is performed in two steps. First, each retrieved embedding is matched with its corresponding text chunk. Each chunk is passed to GPT-3.5 in an individual call to the API - these calls are made in parallel. Once we have accumulated a summary for each chunk, the summaries are passed to GPT-3.5 or GPT-4 for the final summary.&lt;/li&gt; &#xA; &lt;li&gt;Output - the summary is displayed on the page and saved as a text file. &lt;img src=&#34;https://i.imgur.com/sUcay6a.gif&#34; alt=&#34;summary&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Improved support for locally run LLM&#39;s is coming.&lt;/p&gt; &#xA;&lt;p&gt;Built using Langchain! This is project was made for fun, and is likely full of bugs. It is not fully optimized. Contributions or bug reports are welcomed!&lt;/p&gt; &#xA;&lt;p&gt;todo: keep summary in session state, save transcripts when loaded to summarize&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>s-JoL/Open-Llama</title>
    <updated>2023-05-18T01:42:58Z</updated>
    <id>tag:github.com,2023-05-18:/s-JoL/Open-Llama</id>
    <link href="https://github.com/s-JoL/Open-Llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The complete training code of the open-source high-performance Llama model, including the full process from pre-training to RLHF.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/README_zh.md&#34;&gt;&lt;strong&gt;中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/assets/logo.png&#34; alt=&#34;camel&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Open-Llama&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/s-JoL/Open-Llama.svg?color=blue&amp;amp;style=flat-square&#34;&gt; &lt;img alt=&#34;GitHub release (latest by date)&#34; src=&#34;https://img.shields.io/github/v/release/s-JoL/Open-Llama&#34;&gt; &lt;img alt=&#34;GitHub top language&#34; src=&#34;https://img.shields.io/github/languages/top/s-JoL/Open-Llama&#34;&gt; &lt;img alt=&#34;GitHub last commit&#34; src=&#34;https://img.shields.io/github/last-commit/s-JoL/Open-Llama&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Open-Llama is an open-source project that offers a complete training pipeline for building large language models, ranging from dataset preparation to tokenization, pre-training, prompt tuning, lora, and the reinforcement learning technique RLHF.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can try this model directly from the &lt;a href=&#34;http://home.ustc.edu.cn/~sl9292/&#34;&gt;Demo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://discord.gg/TrKxrTpnab&#34;&gt;discord&lt;/a&gt; to discuss the development of large language models.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Main contents&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support Transformers/HuggingFace.&lt;/strong&gt; The CheckPoint after Instruct-tuning is open-source on &lt;a href=&#34;https://huggingface.co/s-JoL/Open-Llama-V2&#34;&gt;Hugging Face: s-JoL/Open-Llama-V2&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;By adopting the same evaluation method as the FastChat project, Open-Llama&#39;s performance is compared to GPT3.5’s. After testing, it can reach 89% of GPT3.5&#39;s performance on Chinese questions.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The training speed reaches 3620 tokens/s, faster than the 3370 tokens/s reported in the original Llama paper, reaching the current state-of-the-art level.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;s-JoL/Open-Llama-V2&#34;, use_fast=False)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;s-JoL/Open-Llama-V2&#34;, device_map=&#34;auto&#34;)&#xA;&#xA;inputs = tokenizer(&#39;user:implement quick sort in python\nsystem:&#39;, return_tensors=&#39;pt&#39;, return_attention_mask=False, add_special_tokens=False)&#xA;for k, v in inputs.items():&#xA;   inputs[k] = v.cuda()&#xA;pred = model.generate(**inputs, max_new_tokens=512, do_sample=True)&#xA;print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CheckPoint after pre-training only is also uploaded to &lt;a href=&#34;https://huggingface.co/s-JoL/Open-Llama-V2-pretrain&#34;&gt;s-JoL/Open-Llama-V2-pretrain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have completed 330B token pre-training, training a total of 80 K steps. The Global Batch Size is consistent with Llama at 4M. Using a total of 7 parts of data to constitute the Instruction-tuning data, the model has certain programming abilities, mathematical abilities, and multi-turn dialogue abilities. Specific data can be found in the Instruction-Tuning section.&lt;/p&gt; &#xA;&lt;p&gt;Below is a display of the model&#39;s multi-turn dialogue ability regarding code:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/assets/multiturn_chat_en.jpg&#34; alt=&#34;image4&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Updates&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023.5.8] Release v2.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This update adds support for larger model training. Using DeepSpeed stage3 + offload + activation checkpoint, you can &lt;strong&gt;train a 65B model with A100-80G&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The peft library is introduced to &lt;strong&gt;support training such as lora&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following table compares the training speed of Open-Llama and the original Llama, and the performance data of Llama is quoted from the original Llama paper.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed Stage&lt;/th&gt; &#xA;   &lt;th&gt;Offload&lt;/th&gt; &#xA;   &lt;th&gt;Activation Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Total Token&lt;/th&gt; &#xA;   &lt;th&gt;GPU hours&lt;/th&gt; &#xA;   &lt;th&gt;Speed token/s/gpu&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Llama 7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;173.7B&lt;/td&gt; &#xA;   &lt;td&gt;13412&lt;/td&gt; &#xA;   &lt;td&gt;3620&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Llama 13B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1856&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Llama 33B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;708&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Llama 65B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;369&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1T&lt;/td&gt; &#xA;   &lt;td&gt;82432&lt;/td&gt; &#xA;   &lt;td&gt;3370&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 13B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1T&lt;/td&gt; &#xA;   &lt;td&gt;135168&lt;/td&gt; &#xA;   &lt;td&gt;2055&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 33B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1.4T&lt;/td&gt; &#xA;   &lt;td&gt;530432&lt;/td&gt; &#xA;   &lt;td&gt;733&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 65B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1.4T&lt;/td&gt; &#xA;   &lt;td&gt;1022362&lt;/td&gt; &#xA;   &lt;td&gt;380&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023.4.28] Release v2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This update mainly includes the following aspects, increasing the effective training speed by &lt;strong&gt;50%&lt;/strong&gt; compared to the v1 version, reducing padding from &lt;strong&gt;30%&lt;/strong&gt; to &lt;strong&gt;5%&lt;/strong&gt;, and improving training speed from &lt;strong&gt;3200 tokens/s&lt;/strong&gt; to &lt;strong&gt;3620 tokens/s&lt;/strong&gt;. 0.95 * 3620 / (0.7 * 3200) = 1.521&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use Hugging Face&#39;s datasets library for data reading, with the process as follows: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Use the transform function to unify data formats from different datasets to {&#39;text&#39;: &#39;xxx&#39;}&lt;/li&gt; &#xA;   &lt;li&gt;Tokenize using Tokenizer&lt;/li&gt; &#xA;   &lt;li&gt;Sample long sequences; currently, three modes are provided: truncation, sampling (refer to the &lt;a href=&#34;https://arxiv.org/abs/2112.11446&#34;&gt;Gopher paper&lt;/a&gt;), and splitting&lt;/li&gt; &#xA;   &lt;li&gt;Optional: concatenate texts from different docs, reducing padding in the data and accelerating training. In the v1 version, padding accounted for &lt;strong&gt;30%&lt;/strong&gt;; after concatenation, padding is reduced to &lt;strong&gt;5%&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Add Trainer, which can be reused for both pre-training and instruction fine-tuning, see solver/trainer.py&lt;/li&gt; &#xA; &lt;li&gt;Unify the pre-training and instruction fine-tuning training entry to train_lm.py&lt;/li&gt; &#xA; &lt;li&gt;Provide more convenient configuration, see configs/pretrain_config.yaml&lt;/li&gt; &#xA; &lt;li&gt;Provide functionality to continue pre-training based on other pre-trained models and supplementing vocabulary&lt;/li&gt; &#xA; &lt;li&gt;Resuming training from a checkpoint is supported, including loading optimizer parameters/learning rate and skipping duplicate data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;[2023.4.16] Release v1.0&lt;/p&gt; &#xA;&lt;p&gt;Basic pre-training and instruction fine-tuning codes are provided, with a training speed comparable to that of the original Llama. The pre-trained and fine-tuned models are already open-sourced on Hugging Face.&lt;/p&gt; &#xA;&lt;p&gt;v1 version code can be seen at &lt;a href=&#34;https://github.com/s-JoL/Open-Llama/tree/v1.0&#34;&gt;https://github.com/s-JoL/Open-Llama/tree/v1.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Easy to use&lt;/h3&gt; &#xA;&lt;p&gt;We believe that ease of use is one of the most important features when building large language models. To make Open-LLAMA more accessible, we have focused on the following aspects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minimal implementation&lt;/strong&gt;: We have adopted the simplest implementation methods, lowering the entry threshold and allowing beginners to get started with ease.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Complete pipeline&lt;/strong&gt;: We have published the complete code from dataset construction to training, making every step in the process of building a large language model clear and visible.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;High performance&lt;/h3&gt; &#xA;&lt;p&gt;Due to the high cost of training large language models, high performance is also crucial when building them. To achieve high-performance training, we have employed the following techniques:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fused CUDA kernel&lt;/strong&gt;: Using the fused CUDA kernel provided in &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt; can fuse multiple operations, reducing data transfer between the GPU and CPU, thereby improving training efficiency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallelized training&lt;/strong&gt;: We employ the &lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;Accelerate&lt;/a&gt; library to support parallelized training on multiple GPUs to speed up the training process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a 7B model, the training speed with the native PyTorch Llama model in Transformers is &lt;strong&gt;1378 tokens/s/GPU&lt;/strong&gt;. Using this codebase, the training speed reaches &lt;strong&gt;3626 tokens/s/GPU&lt;/strong&gt;, exceeding &lt;strong&gt;3370 tokens/s/GPU&lt;/strong&gt; reported in the &lt;a href=&#34;https://arxiv.org/pdf/2302.13971.pdf&#34;&gt;original Llama paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If pre-training with 500B tokens, 38300 GPU hours are required. According to the hourly price for 8 A100-80G Spot GPUs on Google Cloud, which is 12.6 US dollars, the total cost is 60,300 US dollars. When using the unaccelerated version for training, the cost is 158,744 US dollars. The final training cost is reduced by 98,000 US dollars.&lt;/p&gt; &#xA;&lt;p&gt;For more testing, see &lt;a href=&#34;https://github.com/s-JoL/Open-Llama#%E5%92%8C%E5%85%B6%E4%BB%96%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94&#34;&gt;performance comparison with other open-source models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Versatility&lt;/h3&gt; &#xA;&lt;p&gt;When training language models, our goal is to build a versatile model that can handle different languages and domains. To achieve this, we have employed the following strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-language support&lt;/strong&gt;: We support multiple language corpora, including English, Chinese, Japanese, and many other languages, allowing users to choose according to their requirements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain versatility&lt;/strong&gt;: We hope that the model can not only help with everyday questions but also assist in professional domains such as science, law, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interaction with the world&lt;/strong&gt;: By incorporating reinforcement learning (RL), we hope to give the model the ability to interact with the world.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7 or higher&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.13&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;Accelerate library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.6 or higher (for GPU acceleration)&lt;/li&gt; &#xA; &lt;li&gt;Hardware configuration: currently using (64 CPU, 1000G Memory, 8xA100-80G) x N. There is a rather curious phenomenon that when more CPUs are used, the system runs slightly slower. I speculate this may have something to do with the multi-processing of dataloader.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Use the following command to install related dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Currently provided are the Wudao dataset open-sourced by Zhiyuan and the Pile dataset open-sourced by EleutherAI. Dataset download and processing scripts are located in the data directory. Due to the required agreement for downloading the Wudao dataset, you may need to modify the link in download_wudao. &lt;a href=&#34;https://data.baai.ac.cn/details/WuDaoCorporaText&#34;&gt;Wudao&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/skepsun&#34;&gt;@skepsun&lt;/a&gt;&#39;s suggestion, using scidb to download the wudao dataset does not require login, and the download is more stable. &lt;a href=&#34;https://github.com/s-JoL/Open-Llama/issues/42&#34;&gt;https://github.com/s-JoL/Open-Llama/issues/42&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that data download may fail. It is recommended to divide the download and processing in the script into two parts for multiple attempts, which will automatically resume downloads from breakpoints.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run the following commands to download the data and perform partitioning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash data/download_the_pile.sh&#xA;bash data/download_wudao.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The data will be stored as small files, with a maximum of 16384 lines per file, for easy reading during multi-process training. The storage format is jsonl.zst, compressed using zstd, with a final data size of 519.5 GB, consisting of 16,466 files in total.&lt;/p&gt; &#xA;&lt;p&gt;The Pile dataset contains 210,607,728 JSON lines, while the Wudao dataset contains 59,132,213 JSON lines.&lt;/p&gt; &#xA;&lt;p&gt;The specific data format is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WuDao&#xA;{&#39;id&#39;: 1, &#39;dataType&#39;: &#39;百科&#39;, &#39;title&#39;: &#39;some title&#39;, &#39;content&#39;: &#39;some content&#39;}&#xA;&#xA;The Pile&#xA;{&#39;text&#39;: &#39;some text&#39;, &#39;meta&#39;: {&#39;pile_set_name&#39;: &#39;Github&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check the data integrity in &lt;a href=&#34;https://github.com/s-JoL/Open-Llama/issues/5&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Related Tools&lt;/h3&gt; &#xA;&lt;p&gt;In the utils directory, training tokenizer/supplementing existing tokenizer models and conversion checkpoint code are provided.&lt;/p&gt; &#xA;&lt;p&gt;Use SentencePiece to train a tokenizer with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 utils/train_tokenizer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In configs, a tokenizer model with a 40k vocabulary, trained only using the Wudao dataset (4w_cn_vocab_wudao15.model), is provided.&lt;/p&gt; &#xA;&lt;p&gt;To supplement the vocabulary based on an existing tokenizer model, refer to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 utils/merge_tokenizer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A bilingual English and Chinese tokenizer model (llama_tokenizer_extended.model) is created by merging the META official tokenizer model with the 40k Chinese tokenizer mentioned above.&lt;/p&gt; &#xA;&lt;p&gt;To convert existing Llama model checkpoints, refer to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 utils/convert_ckpt.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Loading&lt;/h3&gt; &#xA;&lt;p&gt;Data loading-related code can be found in dataset/dataset.py, which includes pre-training and instruction fine-tuning data processing. To add other datasets, only the transform function needs to be modified.&lt;/p&gt; &#xA;&lt;p&gt;The data loading process is as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use the transform function to unify data formats from different datasets to {&#39;text&#39;: &#39;xxx&#39;}&lt;/li&gt; &#xA; &lt;li&gt;Tokenize using Tokenizer&lt;/li&gt; &#xA; &lt;li&gt;Sample long sequences; currently, three modes are provided: truncation, sampling (refer to the Gopher paper), and splitting&lt;/li&gt; &#xA; &lt;li&gt;Optional: concatenate texts from different docs, reducing padding in the data and accelerating training. In the v1 version, padding accounted for 30%; after concatenation, padding is reduced to 5%.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Use the following command to view the output of DataLoader and check the correctness of tokenization:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 dataset/dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Structure&lt;/h3&gt; &#xA;&lt;p&gt;We modified according to the section 2.4 Efficient implementation of the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama&lt;/a&gt; paper in the Transformers library, and also referenced other papers to introduce some optimizations. Specifically, we used the memory_efficient_attention operation from the &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers library&lt;/a&gt; open-sourced by META for Self Attention computation, which has a significant performance improvement of approximately 30%. Further details can be found in &lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/models/open_llama/modeling_open_llama.py#L229&#34;&gt;modeling_llama.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we referred to &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;Bloom&lt;/a&gt; and introduced Stable Embedding for Token Embedding to better stabilize training.&lt;/p&gt; &#xA;&lt;p&gt;Finally, we referenced &lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34;&gt;PALM&lt;/a&gt; and employed Shared Input-Output Embeddings.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;p&gt;We use multi-GPU parallel training based on the Accelerate library, with the following start command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file configs/accelerate_configs/ds_stage1.yaml train_lm.py --train_config configs/pretrain_config.yaml --model_config configs/model_configs/7B.json &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In some cases, you may need to specify the following parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--main_process_ip&#xA;--main_process_port&#xA;--num_processes&#xA;--num_machines&#xA;--machine_rank&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://wandb.ai/&#34;&gt;Wandb&lt;/a&gt; for visualizing training. You need to modify the WANDB_API_KEY environment variable yourself.&lt;/p&gt; &#xA;&lt;p&gt;Among them, we use DeepSpeed stage1 to reduce memory usage. For Accelerate-related configurations, see configs/accelerate_configs.&lt;/p&gt; &#xA;&lt;p&gt;Training related hyperparameters can be found in configs/pretrain_config.yaml.&lt;/p&gt; &#xA;&lt;p&gt;The default parameters use LlamaTokenizer with a supplemented 40k Chinese vocabulary tokenizer model, and the model size is 7B. The specific configuration is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;max_length&lt;/th&gt; &#xA;   &lt;th&gt;batch_size&lt;/th&gt; &#xA;   &lt;th&gt;learning_rate&lt;/th&gt; &#xA;   &lt;th&gt;weight_decay&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;dimension&lt;/th&gt; &#xA;   &lt;th&gt;n heads&lt;/th&gt; &#xA;   &lt;th&gt;n layer&lt;/th&gt; &#xA;   &lt;th&gt;vocab_size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;2e-4&lt;/td&gt; &#xA;   &lt;td&gt;1e-1&lt;/td&gt; &#xA;   &lt;td&gt;7.03B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;68762&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code&gt;==============================================================================================================&#xA;Layer (type:depth-idx)                                       Output Shape              Param #&#xA;==============================================================================================================&#xA;OpenLlamaForCausalLM                                         [1, 32, 64, 128]          --&#xA;├─OpenLlamaModel: 1-1                                        [1, 32, 64, 128]          --&#xA;│    └─Embedding: 2-1                                        [1, 64, 4096]             281,649,152&#xA;│    └─ModuleList: 2-2                                       --                        --&#xA;│    │    └─OpenLlamaDecoderLayer: 3x32                      [1, 64, 4096]             202,383,360&#xA;│    └─OpenLlamaRMSNorm: 2-3                                 [1, 64, 4096]             4,096&#xA;├─Linear: 1-2                                                [1, 64, 68762]            281,649,152&#xA;==============================================================================================================&#xA;Total params: 7,039,569,920&#xA;Trainable params: 7,039,569,920&#xA;Non-trainable params: 0&#xA;Total mult-adds (G): 7.04&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pre-training loss from scratch is shown below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/assets/pretrain_loss.png&#34; alt=&#34;loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instruction-Tuning&lt;/h3&gt; &#xA;&lt;p&gt;We use the currently available seven datasets for Instruction-tuning, and more tasks and our own datasets will be added later.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/yizhongw/self_instruct&#34;&gt;yizhongw/self_instruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BelleGroup/train_0.5M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BelleGroup/train_1M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BelleGroup/multiturn_chat_0.8M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BelleGroup/school_math_0.25M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered&#34;&gt;anon8231489123/ShareGPT_Vicuna_unfiltered&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Graverman/Instruct-to-Code&#34;&gt;Graverman/Instruct-to-Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The ShareGPT_Vicuna_unfiltered dataset has some issues in the datastes processing, so we directly downloaded the original data and reprocessed it. We performed some preprocessing on the original data, with the format as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;user: {prompt}\nsystem: {completion}&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The startup command is basically the same as pre-training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file configs/accelerate_configs/ds_stage1.yaml train_lm.py --train_config configs/instruct_config.yaml --model_config configs/model_configs/7B.json &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In some cases, you may need to specify the following parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--main_process_ip&#xA;--main_process_port&#xA;--num_processes&#xA;--num_machines&#xA;--machine_rank&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The loss during the process is shown below, with a total of 3 epochs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s-JoL/Open-Llama/main/assets/instruct_loss.png&#34; alt=&#34;loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;RLHF&lt;/h3&gt; &#xA;&lt;p&gt;Not available yet.&lt;/p&gt; &#xA;&lt;h3&gt;Server&lt;/h3&gt; &#xA;&lt;p&gt;For multi-turn dialogue, use chat_server.py.&lt;/p&gt; &#xA;&lt;p&gt;Developed based on Gradio.&lt;/p&gt; &#xA;&lt;h2&gt;Performance Comparison&lt;/h2&gt; &#xA;&lt;h3&gt;Training Framework&lt;/h3&gt; &#xA;&lt;p&gt;In terms of training frameworks, we tested Hugging Face&#39;s open-source Accelerate library, PyTorch Lightning, and HPC-AI&#39;s open-source ColossalAI. We found that their performance differences are relatively small when fully utilizing GPUs. Therefore, we chose the relatively simple-to-implement Accelerate library as the training framework.&lt;/p&gt; &#xA;&lt;p&gt;The test code can be found in utils/speed_test.py.&lt;/p&gt; &#xA;&lt;p&gt;The model structure used during the testing process is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;n gpu&lt;/th&gt; &#xA;   &lt;th&gt;n layer&lt;/th&gt; &#xA;   &lt;th&gt;n heads&lt;/th&gt; &#xA;   &lt;th&gt;hidden size&lt;/th&gt; &#xA;   &lt;th&gt;vocab size&lt;/th&gt; &#xA;   &lt;th&gt;seq length&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT2&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;heads&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;250100&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The test results are shown below, indicating that when the GPUs are fully utilized, the differences in speed and memory consumption are not significant.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;ColossalAI&lt;/th&gt; &#xA;   &lt;th&gt;ColossalAI&lt;/th&gt; &#xA;   &lt;th&gt;ColossalAI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;config&lt;/td&gt; &#xA;   &lt;td&gt;without activation ckpt, bs2&lt;/td&gt; &#xA;   &lt;td&gt;without activation ckpt, max_bs=12&lt;/td&gt; &#xA;   &lt;td&gt;with activation ckpt, bs2&lt;/td&gt; &#xA;   &lt;td&gt;without activation ckpt, bs2&lt;/td&gt; &#xA;   &lt;td&gt;without activation ckpt, max_bs=10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;second pre step&lt;/td&gt; &#xA;   &lt;td&gt;0.336, fw=0.033, bw=0.3, opt=5e-6&lt;/td&gt; &#xA;   &lt;td&gt;1.25&lt;/td&gt; &#xA;   &lt;td&gt;0.347&lt;/td&gt; &#xA;   &lt;td&gt;0.308, fw=0.067, bw=0.152, opt=0.088&lt;/td&gt; &#xA;   &lt;td&gt;1.055&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpu memory&lt;/td&gt; &#xA;   &lt;td&gt;nvidia-smi 45445&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;fw+bw+opt=21053.63+22064.12+17987.52, nvidia-smi 40961&lt;/td&gt; &#xA;   &lt;td&gt;fw+bw+opt=24684.74+21087.13+17987.52, nvidia-smi 46821&lt;/td&gt; &#xA;   &lt;td&gt;oom after 10 steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Performance Optimization&lt;/h3&gt; &#xA;&lt;p&gt;In the earliest version, we used the native Llama implementation from DeepSpeed stage2 + Transformers for training. However, the speed was significantly different from what was claimed in the paper. Therefore, we carried out a series of optimizations afterwards, and we list each step of the performance improvement below for reference.&lt;/p&gt; &#xA;&lt;p&gt;The paper mentioned that for the 6.7B model, 1T token was used for training and the final GPU time was 82432, from which the training speed was roughly calculated as 3370 token/s/gpu. After using the following optimizations, the speed is now basically consistent with what was claimed in the paper when tested on 20x8 A100-80G. It is expected that more fusion operators will be added in the future to achieve better performance.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;V1&lt;/th&gt; &#xA;   &lt;th&gt;V2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataset&lt;/td&gt; &#xA;   &lt;td&gt;self implemented&lt;/td&gt; &#xA;   &lt;td&gt;datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model&lt;/td&gt; &#xA;   &lt;td&gt;Transformers&lt;/td&gt; &#xA;   &lt;td&gt;Transformers+xformers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Optimizer&lt;/td&gt; &#xA;   &lt;td&gt;Pytorch Adam&lt;/td&gt; &#xA;   &lt;td&gt;Fused Adam&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSpeed&lt;/td&gt; &#xA;   &lt;td&gt;stage2&lt;/td&gt; &#xA;   &lt;td&gt;stage1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Grad Accumulation&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Return Padding Mask&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speed token/s/gpu&lt;/td&gt; &#xA;   &lt;td&gt;1378&lt;/td&gt; &#xA;   &lt;td&gt;3637&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Comparison with Other Open-source Models&lt;/h3&gt; &#xA;&lt;p&gt;The following table summarizes the performance of currently available open-source models. In all cases, the GPU device used is A100. Due to differences in the size and structure of the models, it is difficult to make accurate performance comparisons. As a rough estimate, it can be assumed that the speed is generally inversely proportional to the size of the model parameters, which is confirmed by the performance of Llama with models of different sizes. Based on this rough estimate, it can be seen that the performance using our project is significantly better than that of other projects.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Open-Llama&lt;/th&gt; &#xA;   &lt;th&gt;LLAMA&lt;/th&gt; &#xA;   &lt;th&gt;LLAMA&lt;/th&gt; &#xA;   &lt;th&gt;LLAMA&lt;/th&gt; &#xA;   &lt;th&gt;OPT&lt;/th&gt; &#xA;   &lt;th&gt;Bloom&lt;/th&gt; &#xA;   &lt;th&gt;GLM&lt;/th&gt; &#xA;   &lt;th&gt;GPT-NEOX&lt;/th&gt; &#xA;   &lt;th&gt;CPM-ANT&lt;/th&gt; &#xA;   &lt;th&gt;CodeGeeX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model size&lt;/td&gt; &#xA;   &lt;td&gt;7.0B&lt;/td&gt; &#xA;   &lt;td&gt;6.7B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;175B&lt;/td&gt; &#xA;   &lt;td&gt;175B&lt;/td&gt; &#xA;   &lt;td&gt;130B&lt;/td&gt; &#xA;   &lt;td&gt;20B&lt;/td&gt; &#xA;   &lt;td&gt;10B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1T&lt;/td&gt; &#xA;   &lt;td&gt;1T&lt;/td&gt; &#xA;   &lt;td&gt;1.4T&lt;/td&gt; &#xA;   &lt;td&gt;180B&lt;/td&gt; &#xA;   &lt;td&gt;366B&lt;/td&gt; &#xA;   &lt;td&gt;400B&lt;/td&gt; &#xA;   &lt;td&gt;402B&lt;/td&gt; &#xA;   &lt;td&gt;200B&lt;/td&gt; &#xA;   &lt;td&gt;13.9B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU Hour&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;82,432&lt;/td&gt; &#xA;   &lt;td&gt;135,168&lt;/td&gt; &#xA;   &lt;td&gt;1,022,362&lt;/td&gt; &#xA;   &lt;td&gt;809,472&lt;/td&gt; &#xA;   &lt;td&gt;1,082,990&lt;/td&gt; &#xA;   &lt;td&gt;43776&lt;/td&gt; &#xA;   &lt;td&gt;175680&lt;/td&gt; &#xA;   &lt;td&gt;47040&lt;/td&gt; &#xA;   &lt;td&gt;3072&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speed token/s/gpu&lt;/td&gt; &#xA;   &lt;td&gt;3637&lt;/td&gt; &#xA;   &lt;td&gt;3370&lt;/td&gt; &#xA;   &lt;td&gt;2055&lt;/td&gt; &#xA;   &lt;td&gt;380&lt;/td&gt; &#xA;   &lt;td&gt;61.8&lt;/td&gt; &#xA;   &lt;td&gt;93.9&lt;/td&gt; &#xA;   &lt;td&gt;105.7&lt;/td&gt; &#xA;   &lt;td&gt;635.6&lt;/td&gt; &#xA;   &lt;td&gt;1181&lt;/td&gt; &#xA;   &lt;td&gt;1257&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;相关依赖&lt;/td&gt; &#xA;   &lt;td&gt;xformers&lt;/td&gt; &#xA;   &lt;td&gt;xformers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;measeq&lt;/td&gt; &#xA;   &lt;td&gt;Megatron-DeepSpeed&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BMtrain&lt;/td&gt; &#xA;   &lt;td&gt;MindSpore&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speed token*params B/s/gpu&lt;/td&gt; &#xA;   &lt;td&gt;25728&lt;/td&gt; &#xA;   &lt;td&gt;22579&lt;/td&gt; &#xA;   &lt;td&gt;26715&lt;/td&gt; &#xA;   &lt;td&gt;24700&lt;/td&gt; &#xA;   &lt;td&gt;10815&lt;/td&gt; &#xA;   &lt;td&gt;16432&lt;/td&gt; &#xA;   &lt;td&gt;13741&lt;/td&gt; &#xA;   &lt;td&gt;12712&lt;/td&gt; &#xA;   &lt;td&gt;11810&lt;/td&gt; &#xA;   &lt;td&gt;16341&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Future Plans&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Integrate RLHF code.&lt;/li&gt; &#xA; &lt;li&gt;Use Triton to add more high-performance operators to further improve performance.&lt;/li&gt; &#xA; &lt;li&gt;Add code for building pre-training datasets based on Common Crawl and open related datasets.&lt;/li&gt; &#xA; &lt;li&gt;Add code for multimodal training.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{openllama,&#xA;  title={Open-Llama},&#xA;  author={s-JoL},&#xA;  year={2023},&#xA;  howpublished={\url{https://github.com/s-JoL/Open-Llama}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#s-JoL/Open-Llama&amp;amp;Date&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=s-JoL/Open-Llama&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>