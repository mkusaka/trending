<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-19T01:40:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yenchenlin/nerf-pytorch</title>
    <updated>2022-09-19T01:40:32Z</updated>
    <id>tag:github.com,2022-09-19:/yenchenlin/nerf-pytorch</id>
    <link href="https://github.com/yenchenlin/nerf-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A PyTorch implementation of NeRF (Neural Radiance Fields) that reproduces the results.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeRF-pytorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.matthewtancik.com/nerf&#34;&gt;NeRF&lt;/a&gt; (Neural Radiance Fields) is a method that achieves state-of-the-art results for synthesizing novel views of complex scenes. Here are some videos generated by this repository (pre-trained models are provided below):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is a faithful PyTorch implementation of &lt;a href=&#34;http://www.matthewtancik.com/nerf&#34;&gt;NeRF&lt;/a&gt; that &lt;strong&gt;reproduces&lt;/strong&gt; the results while running &lt;strong&gt;1.3 times faster&lt;/strong&gt;. The code is based on authors&#39; Tensorflow implementation &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;here&lt;/a&gt;, and has been tested to match it numerically.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/yenchenlin/nerf-pytorch.git&#xA;cd nerf-pytorch&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Dependencies (click to expand) &lt;/summary&gt; &#xA; &lt;h2&gt;Dependencies&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;PyTorch 1.4&lt;/li&gt; &#xA;  &lt;li&gt;matplotlib&lt;/li&gt; &#xA;  &lt;li&gt;numpy&lt;/li&gt; &#xA;  &lt;li&gt;imageio&lt;/li&gt; &#xA;  &lt;li&gt;imageio-ffmpeg&lt;/li&gt; &#xA;  &lt;li&gt;configargparse&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The LLFF data loader requires ImageMagick.&lt;/p&gt; &#xA; &lt;p&gt;You will also need the &lt;a href=&#34;http://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt; (and COLMAP) set up to compute poses if you want to run on your own real data.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;How To Run?&lt;/h2&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;Download data for two example datasets: &lt;code&gt;lego&lt;/code&gt; and &lt;code&gt;fern&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train a low-res &lt;code&gt;lego&lt;/code&gt; NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config configs/lego.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After training for 100k iterations (~4 hours on a single 2080 Ti), you can find the following video at &lt;code&gt;logs/lego_test/lego_test_spiral_100000_rgb.mp4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To train a low-res &lt;code&gt;fern&lt;/code&gt; NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config configs/fern.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After training for 200k iterations (~8 hours on a single 2080 Ti), you can find the following video at &lt;code&gt;logs/fern_test/fern_test_spiral_200000_rgb.mp4&lt;/code&gt; and &lt;code&gt;logs/fern_test/fern_test_spiral_200000_disp.mp4&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;More Datasets&lt;/h3&gt; &#xA;&lt;p&gt;To play with other scenes presented in the paper, download the data &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;. Place the downloaded dataset according to the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── configs                                                                                                       &#xA;│&amp;nbsp;&amp;nbsp; ├── ...                                                                                     &#xA;│&amp;nbsp;&amp;nbsp;                                                                                             &#xA;├── data                                                                                                                                                                                                       &#xA;│&amp;nbsp;&amp;nbsp; ├── nerf_llff_data                                                                                                  &#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── fern                                                                                                                             &#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;  └── flower  # downloaded llff dataset                                                                                  &#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;  └── horns   # downloaded llff dataset&#xA;|   |   └── ...&#xA;|   ├── nerf_synthetic&#xA;|   |   └── lego&#xA;|   |   └── ship    # downloaded synthetic dataset&#xA;|   |   └── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To train NeRF on different datasets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config configs/{DATASET}.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;replace &lt;code&gt;{DATASET}&lt;/code&gt; with &lt;code&gt;trex&lt;/code&gt; | &lt;code&gt;horns&lt;/code&gt; | &lt;code&gt;flower&lt;/code&gt; | &lt;code&gt;fortress&lt;/code&gt; | &lt;code&gt;lego&lt;/code&gt; | etc.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To test NeRF trained on different datasets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config configs/{DATASET}.txt --render_only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;replace &lt;code&gt;{DATASET}&lt;/code&gt; with &lt;code&gt;trex&lt;/code&gt; | &lt;code&gt;horns&lt;/code&gt; | &lt;code&gt;flower&lt;/code&gt; | &lt;code&gt;fortress&lt;/code&gt; | &lt;code&gt;lego&lt;/code&gt; | etc.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained Models&lt;/h3&gt; &#xA;&lt;p&gt;You can download the pre-trained models &lt;a href=&#34;https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv&#34;&gt;here&lt;/a&gt;. Place the downloaded directory in &lt;code&gt;./logs&lt;/code&gt; in order to test it later. See the following directory structure for an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── logs &#xA;│&amp;nbsp;&amp;nbsp; ├── fern_test&#xA;│&amp;nbsp;&amp;nbsp; ├── flower_test  # downloaded logs&#xA;│   ├── trex_test    # downloaded logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reproducibility&lt;/h3&gt; &#xA;&lt;p&gt;Tests that ensure the results of all functions and training loop match the official implentation are contained in a different branch &lt;code&gt;reproduce&lt;/code&gt;. One can check it out and run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout reproduce&#xA;py.test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://tancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~bmild/&#34;&gt;Ben Mildenhall&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/&#34;&gt;Pratul P. Srinivasan&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://tancik.com/&#34;&gt;Matthew Tancik&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://jonbarron.info/&#34;&gt;Jonathan T. Barron&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://cseweb.ucsd.edu/~ravir/&#34;&gt;Ravi Ramamoorthi&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html&#34;&gt;Ren Ng&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;UC Berkeley, &lt;sup&gt;2&lt;/sup&gt;Google Research, &lt;sup&gt;3&lt;/sup&gt;UC San Diego&lt;br&gt; *denotes equal contribution&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/yenchenlin/nerf-pytorch/master/imgs/pipeline.jpg&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the &#34;volume&#34; so we can use volume rendering to differentiably render new views&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Kudos to the authors for their amazing results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{mildenhall2020nerf,&#xA;    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},&#xA;    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},&#xA;    year={2020},&#xA;    eprint={2003.08934},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if you find this implementation or pre-trained models helpful, please consider to cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lin2020nerfpytorch,&#xA;  title={NeRF-pytorch},&#xA;  author={Yen-Chen, Lin},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished={\url{https://github.com/yenchenlin/nerf-pytorch/}},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lucidrains/denoising-diffusion-pytorch</title>
    <updated>2022-09-19T01:40:32Z</updated>
    <id>tag:github.com,2022-09-19:/lucidrains/denoising-diffusion-pytorch</id>
    <link href="https://github.com/lucidrains/denoising-diffusion-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Denoising Diffusion Probabilistic Model in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/denoising-diffusion-pytorch/main/images/denoising-diffusion.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Denoising Diffusion Probabilistic Model, in Pytorch&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;Denoising Diffusion Probabilistic Model&lt;/a&gt; in Pytorch. It is a new approach to generative modeling that may &lt;a href=&#34;https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/&#34;&gt;have the potential&lt;/a&gt; to rival GANs. It uses denoising score matching to estimate the gradient of the data distribution, followed by Langevin sampling to sample from the true distribution.&lt;/p&gt; &#xA;&lt;p&gt;This implementation was transcribed from the official Tensorflow version &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Youtube AI Educators - &lt;a href=&#34;https://www.youtube.com/watch?v=W-O7AZNzbzQ&#34;&gt;Yannic Kilcher&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=344w5h24-h8&#34;&gt;AI Coffeebreak with Letitia&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=HoKDTa5jHvg&#34;&gt;Outlier&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/annotated-diffusion&#34;&gt;Annotated code&lt;/a&gt; by Research Scientists / Engineers from &lt;a href=&#34;https://huggingface.co/&#34;&gt;🤗 Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update: Turns out none of the technicalities really matters at all | &lt;a href=&#34;https://arxiv.org/abs/2208.09392&#34;&gt;&#34;Cold Diffusion&#34; paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/denoising-diffusion-pytorch/main/images/sample.png&#34; width=&#34;500px&#34;&gt;&lt;img&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/denoising-diffusion-pytorch&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/denoising-diffusion-pytorch.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install denoising_diffusion_pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from denoising_diffusion_pytorch import Unet, GaussianDiffusion&#xA;&#xA;model = Unet(&#xA;    dim = 64,&#xA;    dim_mults = (1, 2, 4, 8)&#xA;)&#xA;&#xA;diffusion = GaussianDiffusion(&#xA;    model,&#xA;    image_size = 128,&#xA;    timesteps = 1000,   # number of steps&#xA;    loss_type = &#39;l1&#39;    # L1 or L2&#xA;)&#xA;&#xA;training_images = torch.randn(8, 3, 128, 128) # images are normalized from 0 to 1&#xA;loss = diffusion(training_images)&#xA;loss.backward()&#xA;# after a lot of training&#xA;&#xA;sampled_images = diffusion.sample(batch_size = 4)&#xA;sampled_images.shape # (4, 3, 128, 128)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, if you simply want to pass in a folder name and the desired image dimensions, you can use the &lt;code&gt;Trainer&lt;/code&gt; class to easily train a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer&#xA;&#xA;model = Unet(&#xA;    dim = 64,&#xA;    dim_mults = (1, 2, 4, 8)&#xA;).cuda()&#xA;&#xA;diffusion = GaussianDiffusion(&#xA;    model,&#xA;    image_size = 128,&#xA;    timesteps = 1000,           # number of steps&#xA;    sampling_timesteps = 250,   # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])&#xA;    loss_type = &#39;l1&#39;            # L1 or L2&#xA;).cuda()&#xA;&#xA;trainer = Trainer(&#xA;    diffusion,&#xA;    &#39;path/to/your/images&#39;,&#xA;    train_batch_size = 32,&#xA;    train_lr = 8e-5,&#xA;    train_num_steps = 700000,         # total training steps&#xA;    gradient_accumulate_every = 2,    # gradient accumulation steps&#xA;    ema_decay = 0.995,                # exponential moving average decay&#xA;    amp = True                        # turn on mixed precision&#xA;)&#xA;&#xA;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Samples and model checkpoints will be logged to &lt;code&gt;./results&lt;/code&gt; periodically&lt;/p&gt; &#xA;&lt;h2&gt;Multi-GPU Training&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;Trainer&lt;/code&gt; class is now equipped with &lt;a href=&#34;https://huggingface.co/docs/accelerate/accelerator&#34;&gt;🤗 Accelerator&lt;/a&gt;. You can easily do multi-gpu training in two steps using their &lt;code&gt;accelerate&lt;/code&gt; CLI&lt;/p&gt; &#xA;&lt;p&gt;At the project root directory, where the training script is, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, in the same directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ accelerate launch train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{NEURIPS2020_4c5bcfec,&#xA;    author      = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},&#xA;    booktitle   = {Advances in Neural Information Processing Systems},&#xA;    editor      = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},&#xA;    pages       = {6840--6851},&#xA;    publisher   = {Curran Associates, Inc.},&#xA;    title       = {Denoising Diffusion Probabilistic Models},&#xA;    url         = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},&#xA;    volume      = {33},&#xA;    year        = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{pmlr-v139-nichol21a,&#xA;    title       = {Improved Denoising Diffusion Probabilistic Models},&#xA;    author      = {Nichol, Alexander Quinn and Dhariwal, Prafulla},&#xA;    booktitle   = {Proceedings of the 38th International Conference on Machine Learning},&#xA;    pages       = {8162--8171},&#xA;    year        = {2021},&#xA;    editor      = {Meila, Marina and Zhang, Tong},&#xA;    volume      = {139},&#xA;    series      = {Proceedings of Machine Learning Research},&#xA;    month       = {18--24 Jul},&#xA;    publisher   = {PMLR},&#xA;    pdf         = {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},&#xA;    url         = {https://proceedings.mlr.press/v139/nichol21a.html},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{kingma2021on,&#xA;    title       = {On Density Estimation with Diffusion Models},&#xA;    author      = {Diederik P Kingma and Tim Salimans and Ben Poole and Jonathan Ho},&#xA;    booktitle   = {Advances in Neural Information Processing Systems},&#xA;    editor      = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},&#xA;    year        = {2021},&#xA;    url         = {https://openreview.net/forum?id=2LdBqxc1Yv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Choi2022PerceptionPT,&#xA;    title   = {Perception Prioritized Training of Diffusion Models},&#xA;    author  = {Jooyoung Choi and Jungbeom Lee and Chaehun Shin and Sungwon Kim and Hyunwoo J. Kim and Sung-Hoon Yoon},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2204.00227}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Karras2022ElucidatingTD,&#xA;    title   = {Elucidating the Design Space of Diffusion-Based Generative Models},&#xA;    author  = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2206.00364}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Song2021DenoisingDI,&#xA;    title   = {Denoising Diffusion Implicit Models},&#xA;    author  = {Jiaming Song and Chenlin Meng and Stefano Ermon},&#xA;    journal = {ArXiv},&#xA;    year    = {2021},&#xA;    volume  = {abs/2010.02502}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2022analog,&#xA;    title   = {Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},&#xA;    author  = {Ting Chen and Ruixiang Zhang and Geoffrey Hinton},&#xA;    year    = {2022},&#xA;    eprint  = {2208.04202},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Qiao2019WeightS,&#xA;    title   = {Weight Standardization},&#xA;    author  = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Loddon Yuille},&#xA;    journal = {ArXiv},&#xA;    year    = {2019},&#xA;    volume  = {abs/1903.10520}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>relogX/system-design-questions</title>
    <updated>2022-09-19T01:40:32Z</updated>
    <id>tag:github.com,2022-09-19:/relogX/system-design-questions</id>
    <link href="https://github.com/relogX/system-design-questions" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Problem statements on System Design and Software Architecture as part of Arpit&#39;s System Design Masterclass&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;System Design Questions&lt;/h1&gt; &#xA;&lt;p&gt;The repository contains a set of problem statements around Software Architecture and System Design as conducted by &lt;a href=&#34;https://arpitbhayani.me/masterclass&#34;&gt;Arpit&#39;s System Design Masterclass&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Questions&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/blogging-platform.md&#34;&gt;Design a Blogging Platform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/online-offline-indicator.md&#34;&gt;Design Online Offline Indicator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/airline-checkin.md&#34;&gt;Design Airline Check-in&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/sql-kv.md&#34;&gt;Design SQL backed KV Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Design Slack&#39;s Realtime Communication - NEW&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/load-balancer.md&#34;&gt;Design a Load Balancer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/queue-consumers.md&#34;&gt;Design Synchronized Queue Consumers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/image-service.md&#34;&gt;Design an Image Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/hashtag-service.md&#34;&gt;Design a HashTag Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/onepic.md&#34;&gt;Design OnePic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/tagging-photos-with-people.md&#34;&gt;Design Photo Tagging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/user-affinity.md&#34;&gt;Design User Affinity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/newly-unread-indicator.md&#34;&gt;Design Newly Unread Message Indicator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/distributed-cache.md&#34;&gt;Design a Distributed Cache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/word-dictionary.md&#34;&gt;Design a Word Dictionary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/superfast-kv.md&#34;&gt;Design a Superfast KV Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/s3.md&#34;&gt;Design S3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/faster-superfast-kv.md&#34;&gt;Design a Faster Superfast KV Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/video-pipeline.md&#34;&gt;Design a Video Processing Pipeline for Steaming Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/text-search-engine.md&#34;&gt;Design a Text-based Search Engine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/recent-searches.md&#34;&gt;Design a service that serves Recent Searches for a user&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/live-commentary.md&#34;&gt;Design a Text-based Cricket Commentary Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/sql-broker.md&#34;&gt;Design a SQL backed Message Broker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/task-scheduler.md&#34;&gt;Design a Distributed Task Scheduler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/flash-sale.md&#34;&gt;Design Flash Sale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/counting-impressions.md&#34;&gt;Design Counting Impressions at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/file-sync.md&#34;&gt;Designing a Remote File Sync Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/near-me.md&#34;&gt;Designing a &#34;who&#39;s near me&#34; Service&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Questions that I do not cover anymore&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/relogX/system-design-questions/master/realtime-db.md&#34;&gt;Designing a Realtime DB&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Arpit&#39;s System Design Masterclass&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A masterclass that helps you become great at designing &lt;em&gt;scalable&lt;/em&gt;, &lt;em&gt;fault-tolerant&lt;/em&gt;, and &lt;em&gt;highly available&lt;/em&gt; systems.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;The Program&lt;/h2&gt; &#xA;&lt;p&gt;This is a prime and intermediate-level cohort-based course aimed at providing an exclusive and crisp learning experience. The program will cover most of the topics under System Design and Software Architecture including but not limited to - &lt;em&gt;Architecting Social Networks&lt;/em&gt;, &lt;em&gt;Building Storage Engines&lt;/em&gt; and, &lt;em&gt;Designing High Throughput Systems&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The program will have a blend of &lt;em&gt;Live Classes happening on Weekends 4 to 6:30 pm IST&lt;/em&gt;, &lt;em&gt;1:1 Mentorship sessions happening on weekdays&lt;/em&gt;, and &lt;em&gt;assignments&lt;/em&gt;. The program is designed to be intense and crisp to accelerate learning.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The course has been taken up by &lt;strong&gt;200+&lt;/strong&gt; people, spanning &lt;strong&gt;7&lt;/strong&gt; countries.&lt;/li&gt; &#xA; &lt;li&gt;The NPS of the course is &lt;strong&gt;89&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;People from companies like Tesla, Amazon, Microsoft, Google, Yelp, Github, Flipkart, Practo, Grab, PayPal, and many more, have taken up this course.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hi, I&#39;m Arpit Bhayani 👋&lt;/h2&gt; &#xA;&lt;img width=&#34;256px&#34; src=&#34;https://arpitbhayani.me/static/img/arpit.jpg&#34;&gt; &#xA;&lt;p&gt;In my last &lt;strong&gt;~9&lt;/strong&gt; years of experience, I have worked at &lt;strong&gt;D. E. Shaw&lt;/strong&gt;, &lt;strong&gt;Practo&lt;/strong&gt;, &lt;strong&gt;Amazon&lt;/strong&gt;, and &lt;strong&gt;Unacademy&lt;/strong&gt;; and have built systems, services, and platforms that scaled to billions.&lt;/p&gt; &#xA;&lt;p&gt;Post my masters in CSE from &lt;strong&gt;IIIT Hyderabad&lt;/strong&gt; I joined D. E. Shaw for a short stint of 2 months, before moving to Practo and working there as a &lt;strong&gt;Platform Engineer&lt;/strong&gt;, building and owning close to 8 different microservices. Post Practo I worked at Amazon on their primary mission-critical E-Commerce Database and built &lt;strong&gt;Data Pipelines&lt;/strong&gt; that cold tiered the stale data.&lt;/p&gt; &#xA;&lt;p&gt;After quitting Amazon in 2018, I joined Unacademy as their first &lt;strong&gt;Technical Architect&lt;/strong&gt; and there I designed, built, managed, and scaled services like &lt;em&gt;Search&lt;/em&gt;, &lt;em&gt;Notification&lt;/em&gt;, &lt;em&gt;Logging&lt;/em&gt;, &lt;em&gt;Deployment Engine&lt;/em&gt;, and many more. I have now transitioned into the role of a Sr. Engineering Manager, leading the Site Reliability vertical.&lt;/p&gt; &#xA;&lt;p&gt;In January 2020, I started my &lt;a href=&#34;https://arpitbhayani.me/newsletter&#34;&gt;newsletter&lt;/a&gt; where I write and share an essay about programming languages internals, deep dives on some super-clever algorithms, and few tips on building scalable distributed systems. The newsletter currently has close to &lt;strong&gt;2000+&lt;/strong&gt; subscribers.&lt;/p&gt; &#xA;&lt;p&gt;Recently, I have started building &lt;a href=&#34;https://revine.arpitbhayani.me&#34;&gt;Revine&lt;/a&gt; - a programming langauge for kids helping them develop logic through &lt;strong&gt;animations&lt;/strong&gt; and spark their creativity through &lt;strong&gt;artwork&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://arpitbhayani.me/masterclass&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/4745789/137859181-d4499cf4-ce65-4466-8b88-a078ece0f081.PNG&#34; width=&#34;300px&#34;&gt; &lt;/a&gt; &#xA;&lt;/center&gt;</summary>
  </entry>
</feed>