<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-17T01:43:23Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiyouga/LLaMA-Efficient-Tuning</title>
    <updated>2023-06-17T01:43:23Z</updated>
    <id>tag:github.com,2023-06-17:/hiyouga/LLaMA-Efficient-Tuning</id>
    <link href="https://github.com/hiyouga/LLaMA-Efficient-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA Efficient Tuning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/LLaMA-Efficient-Tuning?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/hiyouga/LLaMA-Efficient-Tuning&#34; alt=&#34;GitHub Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Efficient-Tuning&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;üëã Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[23/06/15] Now we support training the baichuan-7B model in this repo. Try &lt;code&gt;--model_name_or_path baichuan-inc/baichuan-7B&lt;/code&gt; argument to use the baichuan-7B model.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/03] Now we support quantized training and inference (aka &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;). Try &lt;code&gt;--quantization_bit 4/8&lt;/code&gt; argument to work with quantized model. (experimental feature)&lt;/p&gt; &#xA;&lt;p&gt;[23/05/31] Now we support training the BLOOM &amp;amp; BLOOMZ models in this repo. Try &lt;code&gt;--model_name_or_path bigscience/bloomz-7b1-mt&lt;/code&gt; argument to use the BLOOMZ model.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; (7B/13B/33B/65B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BLOOMZ&lt;/a&gt; (560M/1.1B/1.7B/3B/7.1B/176B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan&lt;/a&gt; (7B)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;(Continually) pre-training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;Partial-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.01652&#34;&gt;Supervised fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;Partial-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;RLHF&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Provided Datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For pre-training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/data/wiki_demo.txt&#34;&gt;Wiki Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For supervised fine-tuning: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For reward model training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your HuggingFace account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+ and PyTorch 1.13.1+&lt;/li&gt; &#xA; &lt;li&gt;ü§óTransformers, Datasets, Accelerate, PEFT and TRL&lt;/li&gt; &#xA; &lt;li&gt;protobuf, cpm_kernels and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;jieba, rouge_chinese and nltk (used at evaluation)&lt;/li&gt; &#xA; &lt;li&gt;gradio and mdtex2html (used in web_demo.py)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And &lt;strong&gt;powerful GPUs&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;data/example_dataset&lt;/code&gt; for checking the details about the format of dataset files. You can either use a single &lt;code&gt;.json&lt;/code&gt; file or a &lt;a href=&#34;https://huggingface.co/docs/datasets/dataset_script&#34;&gt;dataset loading script&lt;/a&gt; with multiple files to create a custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note: please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset. About the format of this file, please refer to &lt;code&gt;data/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dependence Installation (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git&#xA;conda create -n llama_etuning python=3.10&#xA;conda activate llama_etuning&#xA;cd LLaMA-Efficient-Tuning&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaMA Weights Preparation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the weights of the LLaMA models.&lt;/li&gt; &#xA; &lt;li&gt;Convert them to HF format using the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m transformers.models.llama.convert_llama_weights_to_hf \&#xA;    --input_dir path_to_llama_weights --model_size 7B --output_dir path_to_llama_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(Continually) Pre-Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_pt.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset wiki_demo \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_pt_checkpoint \&#xA;    --overwrite_cache \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supervised Fine-Tuning&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_sft_checkpoint \&#xA;    --overwrite_cache \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reward Model Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_rm.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset comparison_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_rm_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PPO Training (RLHF)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_ppo.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --reward_model path_to_rm_checkpoint \&#xA;    --output_dir path_to_ppo_checkpoint \&#xA;    --per_device_train_batch_size 2 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --resume_lora_training False \&#xA;    --plot_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distributed Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config # configure the environment&#xA;accelerate launch src/train_XX.py # arguments (same as above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation (BLEU and ROUGE_CHINESE)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_eval \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_eval_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend using &lt;code&gt;--per_device_eval_batch_size=1&lt;/code&gt; and &lt;code&gt;--max_target_length 128&lt;/code&gt; at 4/8-bit evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/cli_demo.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/web_demo.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/export_model.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_export&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt; to use the LLaMA models.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://huggingface.co/spaces/bigscience/license&#34;&gt;RAIL License&lt;/a&gt; to use the BLOOM &amp;amp; BLOOMZ models.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;baichuan-7B License&lt;/a&gt; to use the baichuan-7B model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{llama-efficient-tuning,&#xA;  title = {LLaMA Efficient Tuning},&#xA;  author = {hiyouga},&#xA;  howpublished = {\url{https://github.com/hiyouga/LLaMA-Efficient-Tuning}},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo is a sibling of &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning&#34;&gt;ChatGLM-Efficient-Tuning&lt;/a&gt;. They share a similar code structure of efficient tuning on large language models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>h2oai/h2ogpt</title>
    <updated>2023-06-17T01:43:23Z</updated>
    <id>tag:github.com,2023-06-17:/h2oai/h2ogpt</id>
    <link href="https://github.com/h2oai/h2ogpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Join us at H2O.ai to make the world&#39;s best open-source GPT with document and image Q&amp;A, 100% private chat, no data leaks, Apache 2.0 https://arxiv.org/pdf/2306.08161.pdf&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;h2oGPT&lt;/h2&gt; &#xA;&lt;p&gt;Technical Paper: &lt;a href=&#34;https://arxiv.org/pdf/2306.08161.pdf&#34;&gt;https://arxiv.org/pdf/2306.08161.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. Documents help to &lt;strong&gt;ground&lt;/strong&gt; LLMs against hallucinations by providing them context relevant to the instruction. h2oGPT is fully permissive Apache V2 open-source project for 100% private and secure use of LLMs and document embeddings for document question-answer.&lt;/p&gt; &#xA;&lt;p&gt;Welcome! Join us and make an issue or a PR, and contribute to making the best fine-tuned LLMs, chatbot UI, and document question-answer framework!&lt;/p&gt; &#xA;&lt;p&gt;Turn ‚òÖ into ‚≠ê (top-right corner) if you like the project!&lt;/p&gt; &#xA;&lt;!--  cat README.md | ./gh-md-toc  -  But Help is heavily processed --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#try-h2ogpt-now&#34;&gt;Try h2oGPT now&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#supported-os-and-hardware&#34;&gt;Supported OS and Hardware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#apache-v2-chatbot-with-langchain-integration&#34;&gt;Apache V2 ChatBot with LangChain Integration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#apache-v2-data-preparation-code-training-code-and-models&#34;&gt;Apache V2 Data Preparation code, Training code, and Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#tldr&#34;&gt;TLDR Install &amp;amp; Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#gpu-cuda&#34;&gt;GPU (CUDA)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#cpu&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#macos&#34;&gt;MACOS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#windows-1011&#34;&gt;Windows 10/11&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#cli-chat&#34;&gt;CLI chat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#gradio-ui&#34;&gt;Gradio UI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#client-api&#34;&gt;Client API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#python-wheel&#34;&gt;Python Wheel&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#help&#34;&gt;Help&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#supported-datatypes&#34;&gt;LangChain file types supported&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#database-creation&#34;&gt;CLI Database control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like&#34;&gt;Why h2oGPT for Doc Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/LINKS.md&#34;&gt;Useful Links&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FINETUNE.md&#34;&gt;Fine-Tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL-DOCKER.md&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/TRITON.md&#34;&gt;Triton&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#why-h2oai&#34;&gt;Why H2O.ai?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Try h2oGPT now&lt;/h3&gt; &#xA;&lt;p&gt;Live hosted instances:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llama.h2o.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/img-small.png&#34; alt=&#34;img-small.png&#34;&gt; h2oGPT LLaMa 65B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt.h2o.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/img-small.png&#34; alt=&#34;img-small.png&#34;&gt; h2oGPT Falcon 40B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://falcon.h2o.ai&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/img-small.png&#34; alt=&#34;img-small.png&#34;&gt; h2oGPT Falcon 40B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/h2oai/h2ogpt-chatbot&#34;&gt;ü§ó h2oGPT 12B #1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/h2oai/h2ogpt-chatbot2&#34;&gt;ü§ó h2oGPT 12B #2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--  - [![img-small.png](docs/img-small.png) Latest LangChain-enabled h2oGPT (temporary link) 12B](https://0756a80f3de3f98413.gradio.live) --&gt; &#xA;&lt;!--  - [![img-small.png](docs/img-small.png) Latest LangChain-enabled h2oGPT (temporary link) 12B](https://0f3a3869de5fb3b6b5.gradio.live) --&gt; &#xA;&lt;!--  - [![img-small.png](docs/img-small.png) Latest LangChain-enabled h2oGPT (temporary link) 12B](https://32a0109ace8028ce1a.gradio.live) --&gt; &#xA;&lt;!--  - [![img-small.png](docs/img-small.png) Latest LangChain-enabled h2oGPT (temporary link) 12B](https://3ec823894d1e933650.gradio.live) --&gt; &#xA;&lt;p&gt;For questions, discussing, or just hanging out, come and join our &lt;a href=&#34;https://discord.gg/WKhYMWcVbq&#34;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h3&gt;Supported OS and Hardware&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/NVIDIA/nvidia-docker?style=flat-square&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=macos&amp;amp;logoColor=F0F0F0&#34; alt=&#34;macOS&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt; mode requires CUDA support via torch and transformers. A 6.9B (or 12GB) model in 8-bit uses 7GB (or 13GB) of GPU memory. 8-bit or 4-bit precision can further reduce memory requirements.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt; mode uses GPT4ALL and LLaMa.cpp, e.g. gpt4all-j, requiring about 14GB of system RAM in typical use.&lt;/p&gt; &#xA;&lt;p&gt;GPU and CPU mode tested on variety of NVIDIA GPUs in Ubuntu 18-22, but any modern Linux variant should work. MACOS support tested on Macbook Pro running Monterey v12.3.1 using CPU mode.&lt;/p&gt; &#xA;&lt;h3&gt;Apache V2 ChatBot with LangChain Integration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt; equipped Chatbot integration and streaming responses&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Persistent&lt;/strong&gt; database using Chroma or in-memory with FAISS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Original&lt;/strong&gt; content url links and scores to rank content against query&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt; offline database of any documents (&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#supported-datatypes&#34;&gt;PDFs, Images, and many more&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; documents via chatbot into shared space or only allow scratch space&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt; data sources and the context provided to LLM&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient&lt;/strong&gt; use of context using instruct-tuned LLMs (no need for many examples)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; for client-server control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CPU and GPU&lt;/strong&gt; support from variety of HF models, and CPU support using GPT4ALL and LLaMa cpp&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux, MAC, and Windows&lt;/strong&gt; support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Light mode with soft colors talking to cat image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/ui_talk_to_images.png&#34; alt=&#34;Talk to Cat&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dark mode with H2O.ai colors: &lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/langchain.png&#34; alt=&#34;VectorDB&#34; title=&#34;VectorDB via LangChain&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Apache V2 Data Preparation code, Training code, and Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Variety&lt;/strong&gt; of models (h2oGPT, WizardLM, Vicuna, OpenAssistant, etc.) supported&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fully Commercially&lt;/strong&gt; Apache V2 code, data and models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Quality&lt;/strong&gt; data cleaning of large open-source instruction datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt; and &lt;strong&gt;QLoRA&lt;/strong&gt; (low-rank approximation) efficient 4-bit, 8-bit and 16-bit fine-tuning and generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Large&lt;/strong&gt; (up to 65B parameters) models built on commodity or enterprise GPUs (single or multi node)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluate&lt;/strong&gt; performance using RLHF-based reward models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6147661/232924684-6c0e2dfb-2f24-4098-848a-c3e4396f29f6.mov&#34;&gt;https://user-images.githubusercontent.com/6147661/232924684-6c0e2dfb-2f24-4098-848a-c3e4396f29f6.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All open-source datasets and models are posted on &lt;a href=&#34;https://huggingface.co/h2oai/&#34;&gt;ü§ó H2O.ai&#39;s Hugging Face page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also check out &lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio&#34;&gt;H2O LLM Studio&lt;/a&gt; for our no-code LLM fine-tuning framework!&lt;/p&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration of code and resulting LLMs with downstream applications and low/no-code platforms&lt;/li&gt; &#xA; &lt;li&gt;Complement h2oGPT chatbot with search and other APIs&lt;/li&gt; &#xA; &lt;li&gt;High-performance distributed training of larger models on trillion tokens&lt;/li&gt; &#xA; &lt;li&gt;Enhance the model&#39;s code completion, reasoning, and mathematical capabilities, ensure factual correctness, minimize hallucinations, and avoid repetitive output&lt;/li&gt; &#xA; &lt;li&gt;Add other tools like search&lt;/li&gt; &#xA; &lt;li&gt;Add agents for SQL and CSV question/answer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;First one needs a Python 3.10 environment. For help installing a Python 3.10 environment, see &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md#install-python-environment&#34;&gt;Install Python 3.10 Environment&lt;/a&gt;. On newer Ubuntu systems and environment may be installed by just doing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install -y build-essential gcc python3.10-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check your installation by doing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version # should say 3.10.xx&#xA;pip --version  # should say pip 23.x.y ... (python 3.10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On some systems, &lt;code&gt;pip&lt;/code&gt; still refers back to the system one, then one can use &lt;code&gt;python -m pip&lt;/code&gt; or &lt;code&gt;pip3&lt;/code&gt; instead of &lt;code&gt;pip&lt;/code&gt; or try &lt;code&gt;python3&lt;/code&gt; instead of &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;TLDR&lt;/h4&gt; &#xA;&lt;p&gt;After Python 3.10 environment installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/h2oai/h2ogpt.git&#xA;cd h2ogpt&#xA;# broad support, but no training-time or data creation dependencies&#xA;for fil in requirements.txt reqs_optional/requirements_optional_langchain.txt reqs_optional/requirements_optional_gpt4all.txt reqs_optional/requirements_optional_langchain.gpllike.txt ; do pip install -r $fil ; done&#xA;# Optional: support docx, pptx, ArXiv, etc.&#xA;sudo apt-get install -y libmagic-dev poppler-utils tesseract-ocr libreoffice&#xA;# Optional: for supporting unstructured package&#xA;python -m nltk.downloader all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Place all documents in &lt;code&gt;user_path&lt;/code&gt; or upload in UI.&lt;/p&gt; &#xA;&lt;p&gt;UI using GPU with at least 24GB with streaming:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=h2oai/h2ogpt-oasst1-512-12b --load_8bit=True  --score_model=None --langchain_mode=&#39;UserData&#39; --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;UI using CPU (streaming not yet supported in UI):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=&#39;llama&#39; --prompt_type=wizard2 --score_model=None --langchain_mode=&#39;UserData&#39; --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;GPU (CUDA)&lt;/h4&gt; &#xA;&lt;p&gt;For help installing cuda toolkit, see &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md#installing-cuda-toolkit&#34;&gt;CUDA Toolkit&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/h2oai/h2ogpt.git&#xA;cd h2ogpt&#xA;pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu117&#xA;python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --load_8bit=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then point browser at &lt;a href=&#34;http://0.0.0.0:7860&#34;&gt;http://0.0.0.0:7860&lt;/a&gt; (linux) or &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt; (windows/mac) or the public live URL printed by the server (disable shared link with &lt;code&gt;--share=False&lt;/code&gt;). For 4-bit or 8-bit support, older GPUs may require older bitsandbytes installed as &lt;code&gt;pip uninstall bitsandbytes -y ; pip install bitsandbytes==0.38.1&lt;/code&gt;. For production uses, we recommend at least the 12B model, ran as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --base_model=h2oai/h2ogpt-oasst1-512-12b --load_8bit=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and one can use &lt;code&gt;--h2ocolors=False&lt;/code&gt; to get soft blue-gray colors instead of H2O.ai colors. &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#what-envs-can-i-pass-to-control-h2ogpt&#34;&gt;Here&lt;/a&gt; is a list of environment variables that can control some things in &lt;code&gt;generate.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note if you download the model yourself and point &lt;code&gt;--base_model&lt;/code&gt; to that location, you&#39;ll need to specify the prompt_type as well by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --base_model=&amp;lt;user path&amp;gt; --load_8bit=True --prompt_type=human_bot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for some user path &lt;code&gt;&amp;lt;user path&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For quickly using a private document collection for Q/A, place documents (PDFs, text, etc.) into a folder called &lt;code&gt;user_path&lt;/code&gt; and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r reqs_optional/requirements_optional_langchain.txt&#xA;python -m nltk.downloader all  # for supporting unstructured package&#xA;python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b  --load_8bit=True --langchain_mode=UserData --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more ways to ingest on CLI and control see &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;LangChain Readme&lt;/a&gt;. For example, for improved pdf handling via pymupdf (GPL) and support for docx, ppt, OCR, and ArXiV run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install -y libmagic-dev poppler-utils tesseract-ocr tesseract-ocr libreoffice&#xA;pip install -r reqs_optional/requirements_optional_langchain.gpllike.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For 4-bit support, the latest dev versions of transformers, accelerate, and peft are required, which can be installed by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip uninstall peft transformers accelerate -y&#xA;pip install -r reqs_optional/requirements_optional_4bit.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where uninstall is required in case, e.g., peft was installed from GitHub previously. Then when running generate pass &lt;code&gt;--load_4bit=True&lt;/code&gt;, which is only supported for certain &lt;a href=&#34;https://github.com/huggingface/peft#models-support-matrix&#34;&gt;architectures&lt;/a&gt; like GPT-NeoX-20B, GPT-J, LLaMa, etc.&lt;/p&gt; &#xA;&lt;p&gt;Any other instruct-tuned base models can be used, including non-h2oGPT ones. &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#larger-models-require-more-gpu-memory&#34;&gt;Larger models require more GPU memory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;CPU&lt;/h4&gt; &#xA;&lt;p&gt;CPU support is obtained after installing two optional requirements.txt files. This does not preclude GPU support, just adds CPU support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install base, langchain, and GPT4All, and python LLaMa dependencies:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/h2oai/h2ogpt.git&#xA;cd h2ogpt&#xA;pip install -r requirements.txt  # only do if didn&#39;t already do for GPU support, since windows needs --extra-index-url line&#xA;pip install -r reqs_optional/requirements_optional_langchain.txt&#xA;python -m nltk.downloader all  # for supporting unstructured package&#xA;pip install -r reqs_optional/requirements_optional_gpt4all.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt; for details on installation instructions if any issues encountered.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change &lt;code&gt;.env_gpt4all&lt;/code&gt; model name if desired.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.env_gpt4all&#34;&gt;model_path_llama=WizardLM-7B-uncensored.ggmlv3.q8_0.bin&#xA;model_path_gptj=ggml-gpt4all-j-v1.3-groovy.bin&#xA;model_name_gpt4all_llama=ggml-wizardLM-7B.q4_2.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;gptj&lt;/code&gt; and &lt;code&gt;gpt4all_llama&lt;/code&gt;, you can choose a different model than our default choice by going to GPT4All Model explorer &lt;a href=&#34;https://gpt4all.io/index.html&#34;&gt;GPT4All-J compatible model&lt;/a&gt;. One does not need to download manually, the gp4all package will download at runtime and put it into &lt;code&gt;.cache&lt;/code&gt; like huggingface would. However, &lt;code&gt;gpjt&lt;/code&gt; model often gives &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#gpt4all-not-producing-output&#34;&gt;no output&lt;/a&gt;, even outside h2oGPT.&lt;/p&gt; &#xA;&lt;p&gt;So, for chatting, a better instruct fine-tuned LLaMa-based model for llama.cpp can be downloaded from &lt;a href=&#34;https://huggingface.co/TheBloke&#34;&gt;TheBloke&lt;/a&gt;. For example, &lt;a href=&#34;https://huggingface.co/TheBloke/wizardLM-13B-1.0-GGML&#34;&gt;13B WizardLM Quantized&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML&#34;&gt;7B WizardLM Quantized&lt;/a&gt;. TheBloke has a variety of model types, quantization bit depths, and memory consumption. Choose what is best for your system&#39;s specs. However, be aware that LLaMa-based models are not &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#commercial-viability&#34;&gt;commercially viable&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For 7B case, download &lt;a href=&#34;https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML/blob/main/WizardLM-7B-uncensored.ggmlv3.q8_0.bin&#34;&gt;WizardLM-7B-uncensored.ggmlv3.q8_0.bin&lt;/a&gt; into local path. Then one sets &lt;code&gt;model_path_llama&lt;/code&gt; in &lt;code&gt;.env_gpt4all&lt;/code&gt;, which is currently the default.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run generate.py&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For LangChain support using documents in &lt;code&gt;user_path&lt;/code&gt; folder, run h2oGPT like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=&#39;llama&#39; --prompt_type=wizard2 --score_model=None --langchain_mode=&#39;UserData&#39; --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;LangChain Readme&lt;/a&gt; for more details. For no langchain support (still uses LangChain package as model wrapper), run as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=&#39;llama&#39; --prompt_type=wizard2 --score_model=None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;No streaming is currently supported for these CPU models in UI, but that will be fixed soon.&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;llama.cpp&lt;/code&gt; based CPU models, for computers with low system RAM or slow CPUs, we recommend adding to &lt;code&gt;.env_gpt4all&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.env_gpt4all&#34;&gt;use_mlock=False&#xA;n_ctx=1024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;use_mlock=True&lt;/code&gt; is default to avoid slowness and &lt;code&gt;n_ctx=2048&lt;/code&gt; is default for large context handling. For computers with plenty of system RAM, we recommend adding to &lt;code&gt;.env_gpt4all&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.env_gpt4all&#34;&gt;n_batch=1024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for faster handling. One some systems this has no strong effect, but on others may increase speed quite a bit.&lt;/p&gt; &#xA;&lt;p&gt;Also, for slow and low-memory systems, we recommend using a smaller embedding by using with &lt;code&gt;generrate.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py ... --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;...&lt;/code&gt; means any other options one should add like &lt;code&gt;--base_model&lt;/code&gt; etc. This simpler embedding is about half the size as default &lt;code&gt;instruct-large&lt;/code&gt; and so uses less disk, CPU memory, and GPU memory if using GPUs.&lt;/p&gt; &#xA;&lt;h4&gt;MACOS&lt;/h4&gt; &#xA;&lt;p&gt;First install &lt;a href=&#34;https://www.geeksforgeeks.org/how-to-install-rust-in-macos/&#34;&gt;Rust&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl ‚Äìproto ‚Äò=https‚Äô ‚Äìtlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enter new shell and test: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;When running a Mac with Intel hardware (not M1), you may run into &lt;code&gt;_clang: error: the clang compiler does not support &#39;-march=native&#39;_&lt;/code&gt; during pip install. If so, set your archflags during pip install. eg: &lt;code&gt;ARCHFLAGS=&#34;-arch x86_64&#34; pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you encounter an error while building a wheel during the &lt;code&gt;pip install&lt;/code&gt; process, you may need to install a C++ compiler on your computer.&lt;/p&gt; &#xA;&lt;p&gt;Now go back to normal &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md#cpu&#34;&gt;CPU&lt;/a&gt; installation.&lt;/p&gt; &#xA;&lt;h4&gt;Windows 10/11&lt;/h4&gt; &#xA;&lt;p&gt;Follow these steps, which includes the above GPU or CPU install step at one point:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Visual Studio 2022 (requires newer windows versions of 10/11) with following selected: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows 11 SDK&lt;/li&gt; &#xA;   &lt;li&gt;C++ Universal Windows Platform support for development&lt;/li&gt; &#xA;   &lt;li&gt;MSVC VS 2022 C++ x64/x86 build tools&lt;/li&gt; &#xA;   &lt;li&gt;C++ CMake tools for Windows&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Download the MinGW installer from the &lt;a href=&#34;https://sourceforge.net/projects/mingw/&#34;&gt;MinGW website&lt;/a&gt; and select, go to installation tab, then apply changes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;minigw32-base&lt;/li&gt; &#xA;   &lt;li&gt;mingw32-gcc-g++&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md#install-python-environment&#34;&gt;Setup Environment&lt;/a&gt; for Windows&lt;/li&gt; &#xA; &lt;li&gt;Run Miniconda shell (not power shell) as administrator&lt;/li&gt; &#xA; &lt;li&gt;Run: &lt;code&gt;set path=%path%;c:\MinGW\msys\1.0\bin\&lt;/code&gt; to get C++ in path&lt;/li&gt; &#xA; &lt;li&gt;Download latest nvidia driver for windows&lt;/li&gt; &#xA; &lt;li&gt;Confirm can run nvidia-smi and see driver version&lt;/li&gt; &#xA; &lt;li&gt;Install cuda toolkit from conda: &lt;code&gt;conda install cudatoolkit -c conda-forge&lt;/code&gt; as required easily make bitsandbytes work&lt;/li&gt; &#xA; &lt;li&gt;Run: &lt;code&gt;wsl --install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Now go back to normal &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md#gpu-cuda&#34;&gt;GPU&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md#cpu&#34;&gt;CPU&lt;/a&gt; (most general) installation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IMPORTANT: Run &lt;code&gt;pip install&lt;/code&gt; with &lt;code&gt;--extra-index-url https://download.pytorch.org/whl/cu117&lt;/code&gt; as in GPU section&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Upgrade to windows GPU version of bitsandbytes if using GPU:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For GPU support of 4-bit and 8-bit, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip uninstall bitsandbytes&#xA;pip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.39.0-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;unless you have compute capability &amp;lt;7.0, then your GPU only supports 8-bit (not 4-bit) and you should install older bitsandbytes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip uninstall bitsandbytes&#xA;pip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.38.1-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running windows on GPUs with bitsandbytes you should see something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(h2ogpt) c:\Users\pseud\h2ogpt&amp;gt;python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --load_8bit=True&#xA;bin C:\Users\pseud\.conda\envs\h2ogpt\lib\site-packages\bitsandbytes\libbitsandbytes_cuda118.dll&#xA;Using Model h2oai/h2ogpt-oig-oasst1-512-6_9b&#xA;device_map: {&#39;&#39;: 0}&#xA;Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06&amp;lt;00:00,  2.16s/it]&#xA;device_map: {&#39;&#39;: 1}&#xA;Running on local URL:  http://0.0.0.0:7860&#xA;Running on public URL: https://f8fa95f123416c72dc.gradio.live&#xA;&#xA;This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where bitsandbytes cuda118 was used because conda cuda toolkit is cuda 11.8. You can confirm GPU use via &lt;code&gt;nvidia-smi&lt;/code&gt; showing GPU memory consumed.&lt;/p&gt; &#xA;&lt;p&gt;Note 8-bit inference is about twice slower than 16-bit inference, and the only use of 8-bit is to keep memory profile low.&lt;/p&gt; &#xA;&lt;p&gt;Bitsandbytes can be uninstalled (&lt;code&gt;pip uninstall bitsandbytes&lt;/code&gt;) and still h2oGPT can be used if one does not pass &lt;code&gt;--load_8bit=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;CLI chat&lt;/h4&gt; &#xA;&lt;p&gt;The CLI can be used instead of gradio by running for some base model, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=gptj --cli=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and for LangChain run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python make_db.py --user_path=user_path --collection_name=UserData&#xA;python generate.py --base_model=gptj --cli=True --langchain_mode=UserData&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;with documents in &lt;code&gt;user_path&lt;/code&gt; folder, or directly run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=gptj --cli=True --langchain_mode=UserData --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will build the database first time. One can also use any other models, like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --cli=True --langchain_mode=UserData --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for WizardLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --base_model=&#39;llama&#39; --prompt_type=wizard2 --cli=True --langchain_mode=UserData --user_path=user_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;No streaming is currently supported for llama in CLI chat, but that will be fixed soon.&lt;/p&gt; &#xA;&lt;h4&gt;Gradio UI&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;generate.py&lt;/code&gt; by default runs a gradio server with a &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#explain-things-in-ui&#34;&gt;UI (click for help with UI)&lt;/a&gt;. Key benefits of the UI include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Save, export, import chat histories and undo or regenerate last query-response pair&lt;/li&gt; &#xA; &lt;li&gt;Upload and control documents of various kinds for document Q/A&lt;/li&gt; &#xA; &lt;li&gt;Choose which specific collection to query, or just chat with LLM&lt;/li&gt; &#xA; &lt;li&gt;Choose specific documents out of collection for asking questions&lt;/li&gt; &#xA; &lt;li&gt;Side-by-side 2-model comparison view&lt;/li&gt; &#xA; &lt;li&gt;RLHF response score evaluation for every query-response&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See how we compare to other tools like PrivateGPT, see our comparisons at &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like&#34;&gt;h2oGPT&#39;s LangChain Integration FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We disable background uploads by disabling telemetry for huggingface, gradio, and chroma, and one can additionally avoid downloads (of fonts) by running &lt;code&gt;generate.py&lt;/code&gt; with &lt;code&gt;--gradio_offline_level=2&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#offline-mode&#34;&gt;Offline Documentation&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h4&gt;Client API&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;generate.py&lt;/code&gt; by default runs a gradio server, which also gives access to client API using gradio client. See example &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/client_test.py&#34;&gt;test code&lt;/a&gt; or other tests in our &lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/tests/test_client_calls.py&#34;&gt;tests&lt;/a&gt;. Any element in &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/gradio_runner.py&#34;&gt;gradio_runner.py&lt;/a&gt; with &lt;code&gt;api_name&lt;/code&gt; defined can be accessed via the gradio client.&lt;/p&gt; &#xA;&lt;h4&gt;Python Client Library&lt;/h4&gt; &#xA;&lt;p&gt;An OpenAI compliant client is available. Refer the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/client/README.md&#34;&gt;README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h4&gt;Python Wheel&lt;/h4&gt; &#xA;&lt;p&gt;The wheel adds all dependencies including optional dependencies like 4-bit and flash-attention. To build do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python setup.py sdist bdist_wheel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the default dependencies do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dist/h2ogpt-0.1.0-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;replace &lt;code&gt;0.1.0&lt;/code&gt; with actual version built if more than one. To install additional dependencies, for instance for faiss on GPU, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dist/h2ogpt-0.1.0-py3-none-any.whl&#xA;pip install dist/h2ogpt-0.1.0-py3-none-any.whl[FAISS]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;once &lt;code&gt;whl&lt;/code&gt; file is installed, two new scripts will be added to the current environment: &lt;code&gt;h2ogpt_finetune&lt;/code&gt;, and &lt;code&gt;h2ogpt_generate&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The wheel is not required to use h2oGPT locally from repo, but makes it portable with all required dependencies.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/setup.py&#34;&gt;setup.py&lt;/a&gt; for controlling other options via &lt;code&gt;extras_require&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To create a development environment for training and generation, follow the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md&#34;&gt;installation instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To fine-tune any LLM models on your data, follow the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FINETUNE.md&#34;&gt;fine-tuning instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To create a container for deployment, follow the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL-DOCKER.md&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Help&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Flash attention support, see &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md#flash-attention&#34;&gt;Flash Attention&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL-DOCKER.md#containerized-installation-for-inference-on-linux-gpu-servers&#34;&gt;Docker&lt;/a&gt; for inference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;README for LangChain&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/LINKS.md&#34;&gt;Links&lt;/a&gt;, context, competitors, models, datasets&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some training code was based upon March 24 version of &lt;a href=&#34;https://github.com/tloen/alpaca-lora/&#34;&gt;Alpaca-LoRA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used high-quality created data by &lt;a href=&#34;https://open-assistant.io/&#34;&gt;OpenAssistant&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used base models by &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used OIG data created by &lt;a href=&#34;https://laion.ai/blog/oig-dataset/&#34;&gt;LAION&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why H2O.ai?&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://h2o.ai/company/team/&#34;&gt;Makers&lt;/a&gt; at &lt;a href=&#34;https://h2o.ai&#34;&gt;H2O.ai&lt;/a&gt; have built several world-class Machine Learning, Deep Learning and AI platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;#1 open-source machine learning platform for the enterprise &lt;a href=&#34;https://github.com/h2oai/h2o-3&#34;&gt;H2O-3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The world&#39;s best AutoML (Automatic Machine Learning) with &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/h2o-driverless-ai/&#34;&gt;H2O Driverless AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;No-Code Deep Learning with &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/hydrogen-torch/&#34;&gt;H2O Hydrogen Torch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Document Processing with Deep Learning in &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/document-ai/&#34;&gt;Document AI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also built platforms for deployment and monitoring, and for data wrangling and governance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/operate/h2o-mlops/&#34;&gt;H2O MLOps&lt;/a&gt; to deploy and monitor models at scale&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/feature-store/&#34;&gt;H2O Feature Store&lt;/a&gt; in collaboration with AT&amp;amp;T&lt;/li&gt; &#xA; &lt;li&gt;Open-source Low-Code AI App Development Frameworks &lt;a href=&#34;https://wave.h2o.ai/&#34;&gt;Wave&lt;/a&gt; and &lt;a href=&#34;https://nitro.h2o.ai/&#34;&gt;Nitro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-source Python &lt;a href=&#34;https://github.com/h2oai/datatable/&#34;&gt;datatable&lt;/a&gt; (the engine for H2O Driverless AI feature engineering)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Many of our customers are creating models and deploying them enterprise-wide and at scale in the &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/&#34;&gt;H2O AI Cloud&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-Cloud or on Premises&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/managed&#34;&gt;Managed Cloud (SaaS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/hybrid&#34;&gt;Hybrid Cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.h2o.ai/h2o-ai-cloud/&#34;&gt;AI Appstore&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are proud to have over 25 (of the world&#39;s 280) &lt;a href=&#34;https://h2o.ai/company/team/kaggle-grandmasters/&#34;&gt;Kaggle Grandmasters&lt;/a&gt; call H2O home, including three Kaggle Grandmasters who have made it to world #1.&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;Please read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.&lt;/li&gt; &#xA; &lt;li&gt;Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user&#39;s responsibility to critically evaluate the generated content and use it at their discretion.&lt;/li&gt; &#xA; &lt;li&gt;Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.&lt;/li&gt; &#xA; &lt;li&gt;Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.&lt;/li&gt; &#xA; &lt;li&gt;Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.&lt;/li&gt; &#xA; &lt;li&gt;Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user&#39;s responsibility to periodically review the disclaimer to stay informed about any changes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/WebGLM</title>
    <updated>2023-06-17T01:43:23Z</updated>
    <id>tag:github.com,2023-06-17:/THUDM/WebGLM</id>
    <link href="https://github.com/THUDM/WebGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WebGLM: Towards An Efficient Web-enhanced Question Answering System with Human Preference&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;üìÉ &lt;a href=&#34;https://arxiv.org/pdf/2306.07906.pdf&#34; target=&#34;_blank&#34;&gt;Paper (KDD 2023)&lt;/a&gt; ‚Ä¢ üåê &lt;a href=&#34;https://github.com/THUDM/WebGLM/raw/main/README_zh.md&#34; target=&#34;_blank&#34;&gt;‰∏≠Êñá README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of WebGLM. If you find our open-sourced efforts useful, please üåü the repo to encourage our following developement!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/WebGLM/assets/129033897/d2e1dd35-6340-4175-ac2d-fd585daa17cf&#34;&gt;https://github.com/THUDM/WebGLM/assets/129033897/d2e1dd35-6340-4175-ac2d-fd585daa17cf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Read this in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/README_zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#preparation&#34;&gt;Preparation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-code-and-environments&#34;&gt;Prepare Code and Environments&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-serpapi-key&#34;&gt;Prepare SerpAPI Key&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-retriever-checkpoint&#34;&gt;Prepare Retriever Checkpoint&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#try-webglm&#34;&gt;Try WebGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#export-environment-variables&#34;&gt;Export Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-command-line-interface&#34;&gt;Run as Command Line Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-web-service&#34;&gt;Run as Web Service&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-webglm&#34;&gt;Train WebGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-generator&#34;&gt;Train Generator&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-data&#34;&gt;Prepare Data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-retriever&#34;&gt;Train Retriever&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-data-1&#34;&gt;Prepare Data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#training-1&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#real-application-cases&#34;&gt;Real Application Cases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/main_process.png&#34; alt=&#34;paper&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;WebGLM aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 10-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM-augmented Retriever&lt;/strong&gt;: Enhances the retrieval of relevant web content to better aid in answering questions accurately.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bootstrapped Generator&lt;/strong&gt;: Generates human-like responses to questions, leveraging the power of the GLM to provide refined answers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human Preference-aware Scorer&lt;/strong&gt;: Estimates the quality of generated responses by prioritizing human preferences, ensuring the system produces useful and engaging content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Preparation&lt;/h1&gt; &#xA;&lt;h2&gt;Prepare Code and Environments&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repo, and install python requirements.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Nodejs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt install nodejs # If you use Ubuntu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install playwright dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If browsing environments are not installed in your host, you need to install them. Do not worry, playwright will give you instructions when you first execute it if so.&lt;/p&gt; &#xA;&lt;h2&gt;Prepare SerpAPI Key&lt;/h2&gt; &#xA;&lt;p&gt;In search process, we use SerpAPI to get search results. You need to get a SerpAPI key from &lt;a href=&#34;https://serpapi.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, set the environment variable &lt;code&gt;SERPAPI_KEY&lt;/code&gt; to your key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SERPAPI_KEY=&#34;YOUR KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prepare Retriever Checkpoint&lt;/h2&gt; &#xA;&lt;p&gt;Download the checkpoint on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/54056861b2f34bbfb3f9/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;p&gt;You can manually specify the path to save the checkpoint by &lt;code&gt;--save SAVE_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py retriever-pretrained-checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Try WebGLM&lt;/h1&gt; &#xA;&lt;p&gt;Before you run the code, make sure that the space of your device is enough.&lt;/p&gt; &#xA;&lt;h2&gt;Export Environment Variables&lt;/h2&gt; &#xA;&lt;p&gt;Export the environment variable &lt;code&gt;WEBGLM_RETRIEVER_CKPT&lt;/code&gt; to the path of the retriever checkpoint. If you have downloaded the retriever checkpoint in the default path, you can simply run the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export WEBGLM_RETRIEVER_CKPT=./download/retriever-pretrained-checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run as Command Line Interface&lt;/h2&gt; &#xA;&lt;p&gt;You can try WebGLM-2B model by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py -w THUDM/WebGLM-2B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or directly for WebGLM-10B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run as Web Service&lt;/h2&gt; &#xA;&lt;p&gt;You can try WebGLM-2B model by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py -w THUDM/WebGLM-2B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or directly for WebGLM-10B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train WebGLM&lt;/h1&gt; &#xA;&lt;h2&gt;Train Generator&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Data&lt;/h3&gt; &#xA;&lt;p&gt;Download the training data on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/ae204894f2e842f19a3f/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py generator-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will automatically download all the data and preprocess them into the seq2seq form that can be used immediately in &lt;code&gt;./download&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/THUDM/GLM#train-with-your-own-data&#34;&gt;GLM repo&lt;/a&gt; for seq2seq training.&lt;/p&gt; &#xA;&lt;h2&gt;Train Retriever&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Data&lt;/h3&gt; &#xA;&lt;p&gt;Download the training data on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/fa5e6eb1afac4f08a4c6/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py retriever-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command line to train the retriever. If you have downloaded the retriever training data in the default path, you can simply run the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_retriever.py --train_data_dir ./download/retriever-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;You can reproduce our results on TriviaQA, WebQuestions and NQ Open. Take TriviaQA for example, you can simply run the command line below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/triviaqa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and start running the experiment.&lt;/p&gt; &#xA;&lt;h1&gt;Real Application Cases&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases&#34;&gt;Here&lt;/a&gt; you can see some examples of WebGLM real application scenarios.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{liu2023webglm,&#xA;      title={WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},&#xA;      author={Xiao Liu and Hanyu Lai and Hao Yu and Yifan Xu and Aohan Zeng and Zhengxiao Du and Peng Zhang and Yuxiao Dong and Jie Tang},&#xA;      year={2023},&#xA;      eprint={2306.07906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repo is simplified for easier deployment.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>