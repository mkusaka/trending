<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-10T01:44:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>juncongmoo/chatllama</title>
    <updated>2023-03-10T01:44:18Z</updated>
    <id>tag:github.com,2023-03-10:/juncongmoo/chatllama</id>
    <link href="https://github.com/juncongmoo/chatllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatLLaMA 📢 Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatLLaMA&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;📢 Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than &lt;code&gt;ChatGPT&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 Please check &lt;a href=&#34;https://github.com/juncongmoo/pyllama&#34;&gt;&lt;code&gt;pyllama&lt;/code&gt;&lt;/a&gt; for &lt;code&gt;LLaMA&lt;/code&gt; installation and &lt;code&gt;single GPU inference&lt;/code&gt; setup.&lt;/li&gt; &#xA; &lt;li&gt;🔥 To train ChatGPT in 5 mins - &lt;a href=&#34;https://github.com/juncongmoo/minichatgpt&#34;&gt;minichatgpt&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meta has recently released LLaMA, a collection of foundational large language models ranging from 7 to 65 billion parameters. LLaMA is creating a lot of excitement because it is smaller than GPT-3 but has better performance. For example, LLaMA&#39;s 13B architecture outperforms GPT-3 despite being 10 times smaller. This new collection of fundamental models opens the door to faster inference performance and chatGPT-like real-time assistants, while being cost-effective and running on a single GPU.&lt;/p&gt; &#xA;&lt;p&gt;However, LLaMA was not fine-tuned for instruction task with a Reinforcement Learning from Human Feedback (RLHF) training process.&lt;/p&gt; &#xA;&lt;p&gt;The good news is that we introduce &lt;code&gt;ChatLLaMA&lt;/code&gt;, the first open source implementation of LLaMA based on RLHF:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complete open source implementation that enables you to build a ChatGPT-style service based on pre-trained LLaMA models.&lt;/li&gt; &#xA; &lt;li&gt;Compared to the original ChatGPT, the training process and single-GPU inference are much faster and cheaper by taking advantage of the smaller size of LLaMA architectures.&lt;/li&gt; &#xA; &lt;li&gt;ChatLLaMA has built-in support for DeepSpeed ZERO to speedup the fine-tuning process.&lt;/li&gt; &#xA; &lt;li&gt;The library also supports all LLaMA model architectures (7B, 13B, 33B, 65B), so that you can fine-tune the model according to your preferences for training time and inference performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;1032&#34; alt=&#34;Screen Shot 2023-02-26 at 10 56 13 PM&#34; src=&#34;https://user-images.githubusercontent.com/83510798/221439813-5972d029-dae5-4561-ab3d-5a55fa5cde09.png&#34;&gt; &#xA;&lt;p&gt;Image from &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;OpenAI’s blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install chatllama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Get started with ChatLLaMA&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Please note this code represents the algorithmic implementation for RLHF training process of LLaMA and does not contain the model weights. To access the model weights, you need to apply to Meta&#39;s &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;form&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;ChatLLaMA allows you to easily train LLaMA-based architectures in a similar way to ChatGPT, using RLHF. For example, below is the code to start the training in the case of ChatLLaMA 7B.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chatllama.rlhf.trainer import RLTrainer&#xA;from chatllama.rlhf.config import Config&#xA;&#xA;path = &#34;path_to_config_file.yaml&#34;&#xA;config = Config(path=path)&#xA;trainer = RLTrainer(config.trainer)&#xA;trainer.distillate()&#xA;trainer.train()&#xA;trainer.training_stats.plot()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you should provide Meta&#39;s original weights and your custom dataset before starting the fine-tuning process. Alternatively, you can generate your own dataset using LangChain&#39;s agents.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python generate_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The code is originally from &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama&#34;&gt;nebuly-ai&lt;/a&gt; with some changes. More changes will follow up soon. And the original license link is &lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/raw/main/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/prismer</title>
    <updated>2023-03-10T01:44:18Z</updated>
    <id>tag:github.com,2023-03-10:/NVlabs/prismer</id>
    <link href="https://github.com/NVlabs/prismer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The implementation of &#34;Prismer: A Vision-Language Model with An Ensemble of Experts&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prismer&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the source code of &lt;strong&gt;Prismer&lt;/strong&gt; and &lt;strong&gt;PrismerZ&lt;/strong&gt; from the paper, &lt;a href=&#34;https://arxiv.org/abs/2303.02506&#34;&gt;Prismer: A Vision-Language Model with An Ensemble of Experts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/prismer/main/helpers/intro.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;The implementation is based on &lt;code&gt;PyTorch 1.13&lt;/code&gt;, and highly integrated with Huggingface &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;&lt;code&gt;accelerate&lt;/code&gt;&lt;/a&gt; toolkit for readable and optimised multi-node multi-gpu training.&lt;/p&gt; &#xA;&lt;p&gt;First, let&#39;s install all package dependencies by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare Accelerator Config&lt;/h3&gt; &#xA;&lt;p&gt;Then we generate the corresponding &lt;code&gt;accelerate&lt;/code&gt; config based on your training server configuration. For both single-node multi-gpu and multi-node multi-gpu training, simply run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# to get your machine rank 0 IP address&#xA;hostname -i&#xA;&#xA;# and for each machine, run the following command, set --num_machines 1 in a single-node setting&#xA;python generate_config.py —-main_ip {MAIN_IP} -—rank {MACHINE_RANK} —-num_machines {TOTAL_MACHINES}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;p&gt;We pre-train Prismer/PrismerZ with a combination of five widely used image-alt/text datasets, with pre-organised data lists provided below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/6btr8hz5n1e1q4d/coco_karpathy_train.json?dl=0&#34;&gt;COCO 2014&lt;/a&gt;: the Karpathy training split (which will also be used for fine-tuning).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/kailbaay0sqraxc/vg_caption.json?dl=0&#34;&gt;Visual Genome&lt;/a&gt;: the official Visual Genome captioning dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/xp2nuhc88f1czxm/filtered_cc3m_sbu.json?dl=0&#34;&gt;CC3M + SGU&lt;/a&gt;: filtered and re-captioned by BLIP-Large.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/th358bb6wqkpwbz/filtered_cc12m.json?dl=0&#34;&gt;CC12M&lt;/a&gt;: filtered and re-captioned by BLIP-Large.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The web datasets (CC3M, SGU, CC12M) is composed with image urls. It is highly recommended to use &lt;a href=&#34;https://github.com/rom1504/img2dataset&#34;&gt;img2dataset&lt;/a&gt;, a highly optimised toolkit for large-scale web scraping to download these images. An example bash script of using &lt;code&gt;img2dataset&lt;/code&gt; to download &lt;code&gt;cc12m&lt;/code&gt; dataset is provided below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;img2dataset --url_list filtered_cc12m.json --input_format &#34;json&#34; --url_col &#34;url&#34; --caption_col &#34;caption&#34; --output_folder cc12m --processes_count 16 --thread_count 64 --image_size 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: It is expected that the number of downloaded images is less than the number of images in the json file, because some urls might not be valid or require long loading time.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Captioning / VQA&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate image captioning performance on two datasets, COCO 2014 and NoCaps; and VQA performance on VQAv2 dataset. In VQA tasks, we additionally augment the training data with Visual Genome QA, following BLIP. Again, we have prepared and organised the training and evaluation data lists provided below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/sh/quu6v5hzdetjcdz/AACze0_h6BO8LJmSsEq4MM8-a?dl=0&#34;&gt;Image Captioning&lt;/a&gt;: including COCO (Karpathy Split) and NoCaps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/sh/hqtxl1k8gkbhhoi/AACiax5qi7no3pJgO1E57Xefa?dl=0&#34;&gt;VQAv2&lt;/a&gt;: including VQAv2 and VG QA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generating Expert Labels&lt;/h2&gt; &#xA;&lt;p&gt;Before starting any experiments with Prismer, we need to first pre-generate the modality expert labels, so we may construct a multi-label dataset. In &lt;code&gt;experts&lt;/code&gt; folder, we have included all 6 experts we introduced in our paper. We have organised each expert&#39;s codebase with a shared and simple API.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Specifically for segmentation experts, please first install deformable convolution operations by &lt;code&gt;cd experts/segmentation/mask2former/modeling/pixel_decoder/ops&lt;/code&gt; and run &lt;code&gt;sh make.sh&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;To download pre-trained modality experts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download_checkpoints.py --download_experts=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate the expert labels, simply edit the &lt;code&gt;configs/experts.yaml&lt;/code&gt; with the corresponding data paths, and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=.&#xA;accelerate experts/generate_{EXPERT_NAME}.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Expert label generation is only required for Prismer models, not for PrismerZ models.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Experiments&lt;/h2&gt; &#xA;&lt;p&gt;We have provided both Prismer and PrismerZ for pre-trained checkpoints (for zero-shot image captioning), as well as fined-tuned checkpoints on VQAv2 and COCO datasets. With these checkpoints, it should be expected to reproduce the exact performance listed below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Pre-trained [Zero-shot]&lt;/th&gt; &#xA;   &lt;th&gt;COCO [Fine-tuned]&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2 [Fine-tuned]&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PrismerZ-BASE&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [109.6]&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [133.7]&lt;/td&gt; &#xA;   &lt;td&gt;test-dev [76.58]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prismer-BASE&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [122.6]&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [135.1]&lt;/td&gt; &#xA;   &lt;td&gt;test-dev [76.84]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PrismerZ-LARGE&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [124.8]&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [135.7]&lt;/td&gt; &#xA;   &lt;td&gt;test-dev [77.49]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prismer-LARGE&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [129.7]&lt;/td&gt; &#xA;   &lt;td&gt;COCO CIDEr [136.5]&lt;/td&gt; &#xA;   &lt;td&gt;test-dev [78.42]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To download pre-trained/fined-tuned checkpoints, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# to download all model checkpoints (12 models in total)&#xA;python download_checkpoints.py --download_models=True&#xA;&#xA;# to download specific checkpoints (Prismer-Base for fine-tuned VQA) in this example&#xA;python download_checkpoints.py --download_models=&#34;vqa_prismer_base&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Remember to install java via &lt;code&gt;sudo apt-get install default-jre&lt;/code&gt; which is required to run the official COCO caption evaluation scripts.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate the model checkpoints, please run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# zero-shot image captioning (remember to remove caption prefix in the config files)&#xA;python train_caption.py --exp_name {MODEL_NAME} --evaluate&#xA;&#xA;# fine-tuned image captioning&#xA;python train_caption.py --exp_name {MODEL_NAME} --from_checkpoint --evaluate&#xA;&#xA;# fine-tuned VQA&#xA;python train_vqa.py --exp_name {MODEL_NAME} --from_checkpoint --evaluate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training / Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;To pre-train or fine-tune any model with or without checkpoints, please run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# to train/fine-tuning from scratch&#xA;python train_{TASK}.py --exp_name {MODEL_NAME}&#xA;&#xA;# to train/fine-tuning from the latest checkpoints (saved every epoch)&#xA;python train_{TASK}.py --exp_name {MODEL_NAME} --from_checkpoint &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have also included model sharding in the current training script via PyTorch&#39;s official &lt;a href=&#34;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&#34;&gt;FSDP plugin&lt;/a&gt;. With the same training commands, additionally add &lt;code&gt;--shard_grad_op&lt;/code&gt; for ZeRO-2 Sharding (Gradients + Optimiser States), or &lt;code&gt;--full_shard&lt;/code&gt; for ZeRO-3 Sharding (ZeRO-2 + Network Parameters).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: You should expect the error range for VQAv2 Acc. to be less than 0.1; for COCO/NoCAPs CIDEr score to be less than 1.0.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Finally, we have offered a minimalist example to perform image captioning in a single GPU with our fine-tuned Prismer/PrismerZ checkpoint. Simply put your images under &lt;code&gt;helpers/images&lt;/code&gt; (&lt;code&gt;.jpg&lt;/code&gt; images), and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --exp_name {MODEL_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You then can see all generated modality expert labels in the &lt;code&gt;helpers/labels&lt;/code&gt; folder and the generated captions in the &lt;code&gt;helpers/images&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Particularly for the Prismer models, we have also offered a simple script to prettify the generated expert labels. To prettify and visualise the expert labels as well as its predicted captions, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo_vis.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Remember to set up the corresponding config in the &lt;code&gt;configs/caption.yaml&lt;/code&gt; demo section. The default demo model config is for Prismer-Base.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code/work to be useful in your own research, please considering citing the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023prismer,&#xA;    title={Prismer: A Vision-Language Model with An Ensemble of Experts},&#xA;    author={Liu, Shikun and Fan, Linxi and Johns, Edward and Yu, Zhiding and Xiao, Chaowei and Anandkumar, Anima},&#xA;    journal={arXiv preprint arXiv:2303.02506},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © 2023, NVIDIA Corporation. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;This work is made available under the Nvidia Source Code License-NC.&lt;/p&gt; &#xA;&lt;p&gt;The model checkpoints are shared under CC-BY-NC-SA-4.0. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank all the researchers who open source their works to make this project possible. &lt;a href=&#34;https://github.com/bjoernpl&#34;&gt;@bjoernpl&lt;/a&gt; for contributing an automated checkpoint download script.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please contact &lt;code&gt;sk.lorenmt@gmail.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>juncongmoo/pyllama</title>
    <updated>2023-03-10T01:44:18Z</updated>
    <id>tag:github.com,2023-03-10:/juncongmoo/pyllama</id>
    <link href="https://github.com/juncongmoo/pyllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙 LLaMA - Run LLM in A Single GPU&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;📢 &lt;code&gt;pyllama&lt;/code&gt; is a hacked version of &lt;code&gt;LLaMA&lt;/code&gt; based on original Facebook&#39;s implementation but more convenient to run in a Single consumer grade GPU.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🔥 In order to download the checkpoints and tokenizer, use this BitTorrent link: &#34;&lt;a href=&#34;magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;amp;dn=LLaMA&#34;&gt;magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;amp;dn=LLaMA&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda env with pytorch / cuda available, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pyllama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🐏 If you have installed llama library from other sources, please uninstall the previous llama library and use &lt;code&gt;pip install pyllama -U&lt;/code&gt; to install the latest version.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Single GPU Inference&lt;/h2&gt; &#xA;&lt;p&gt;Set the environment variables &lt;code&gt;CKPT_DIR&lt;/code&gt; as your llamm model folder, for example &lt;code&gt;/llama_data/7B&lt;/code&gt;, and &lt;code&gt;TOKENIZER_PATH&lt;/code&gt; as your tokenizer&#39;s path, such as &lt;code&gt;/llama_data/tokenizer.model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following is an example of LLaMA running in a 8GB single GPU.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_inference.png&#34; alt=&#34;LLaMA Inference&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To load KV cache in CPU, run &lt;code&gt;export KV_CAHCHE_IN_GPU=0&lt;/code&gt; in the shell.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To profile CPU/GPU/Latency, run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_driver.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample result is like:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_profiling.png&#34; alt=&#34;LLaMA Inference&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tune &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; to reduce memory consumption to be able to run in GPU. Refer to: &lt;a href=&#34;https://github.com/juncongmoo/pyllama/issues/9&#34;&gt;this post&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Start a gradio webui&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd apps/gradio&#xA;$ python webapp_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see something like this in your browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_webui.png&#34; alt=&#34;LLaMA Inference&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Start a web server&lt;/h3&gt; &#xA;&lt;p&gt;The following command will start a flask web server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd apps/flask&#xA;$ python web_server_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Multiple GPU Inference&lt;/h2&gt; &#xA;&lt;p&gt;The provided &lt;code&gt;example.py&lt;/code&gt; can be run on a single or multi-gpu node with &lt;code&gt;torchrun&lt;/code&gt; and will output completions for two pre-defined prompts. Using &lt;code&gt;TARGET_FOLDER&lt;/code&gt; as defined in &lt;code&gt;download.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node MP example.py --ckpt_dir $TARGET_FOLDER/model_size --tokenizer_path $TARGET_FOLDER/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Different models require different MP values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the checkpoints and tokenizer, fill this &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;google form&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive links to download the tokenizer and model files. Edit the &lt;code&gt;download.sh&lt;/code&gt; script with the signed url provided in the email to download the model weights and tokenizer.&lt;/p&gt; &#xA;&lt;h3&gt;Model Card&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/juncongmoo/pyllama/raw/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/juncongmoo/pyllama/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>