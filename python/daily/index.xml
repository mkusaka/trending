<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-02T01:34:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Agenta-AI/agenta</title>
    <updated>2024-12-02T01:34:35Z</updated>
    <id>tag:github.com,2024-12-02:/Agenta-AI/agenta</id>
    <link href="https://github.com/Agenta-AI/agenta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open-source LLMOps platform: prompt playground, prompt management, LLM evaluation, and LLM Observability all in one place.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source width=&#34;275&#34; media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/Agenta-AI/agenta/assets/4510758/cdddf5ad-2352-4920-b1d9-ae7f8d9d7735&#34;&gt; &#xA;   &lt;source width=&#34;275&#34; media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/Agenta-AI/agenta/assets/4510758/ab75cbac-b807-496f-aab3-57463a33f726&#34;&gt; &#xA;   &lt;img alt=&#34;Shows the logo of agenta&#34; src=&#34;https://github.com/Agenta-AI/agenta/assets/4510758/68e055d4-d7b8-4943-992f-761558c64253&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/agenta-hq/shared_invite/zt-1zsafop5i-Y7~ZySbhRZvKVPV5DO_7IA&#34;&gt;Slack&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;strong&gt; &lt;h2&gt; The Open source LLMOps Platform &lt;/h2&gt;&lt;/strong&gt; Prompt playground, prompt management, evaluation, and observability &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license.&#34;&gt; &lt;a href=&#34;https://docs.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Doc-online-green&#34; alt=&#34;Doc&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/raw/main/CONTRIBUTING.md&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-brightgreen&#34; alt=&#34;PRs welcome&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/Agenta-AI/agenta&#34; alt=&#34;Contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/Agenta-AI/agenta&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/m/agenta-ai/agenta&#34; alt=&#34;Commits per month&#34;&gt; &lt;a href=&#34;https://pypi.org/project/agenta/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/agenta&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://join.slack.com/t/agenta-hq/shared_invite/zt-1zsafop5i-Y7~ZySbhRZvKVPV5DO_7IA&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/JOIN%20US%20ON%20SLACK-4A154B?style=for-the-badge&amp;amp;logo=slack&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/agenta-ai/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/agenta_ai&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/agenta_ai?style=social&#34; height=&#34;28&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://cloud.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source width=&#34;275&#34; media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/user-attachments/assets/b8912ecb-c7a0-47bd-8507-29b12382fef6&#34;&gt; &#xA;   &lt;source width=&#34;275&#34; media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/user-attachments/assets/f133dd08-04a3-4b20-b047-22f8f841cfbb&#34;&gt; &#xA;   &lt;img alt=&#34;Try Agenta Live Demo&#34; src=&#34;https://github.com/Agenta-AI/agenta/assets/4510758/68e055d4-d7b8-4943-992f-761558c64253&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://cloud.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;img width=&#34;800&#34; alt=&#34;Screenshot Agenta&#34; src=&#34;https://github.com/user-attachments/assets/32e95ddb-e001-4462-b92e-72bf4cc78597&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt;  &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://docs.agenta.ai/changelog/main?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;&lt;b&gt;Changelog&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://cloud.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;&lt;b&gt;Agenta Cloud&lt;/b&gt;&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;What is Agenta?&lt;/h1&gt; &#xA;&lt;p&gt;Agenta is a platform for building production-grade LLM applications. It helps &lt;strong&gt;engineering and product teams&lt;/strong&gt; create reliable LLM apps faster.&lt;/p&gt; &#xA;&lt;p&gt;Agenta provides end-to-end tools for the entire LLMOps workflow: building (&lt;strong&gt;LLM playground&lt;/strong&gt;, &lt;strong&gt;evaluation&lt;/strong&gt;), deploying (&lt;strong&gt;prompt and configuration management&lt;/strong&gt;), and monitoring (&lt;strong&gt;LLM observability and tracing&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Playground&lt;/strong&gt;: Experiment, iterate on prompts, and compare outputs from over 50 LLM models side by side (&lt;a href=&#34;https://docs.agenta.ai/prompt-management/using-the-playground?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;docs&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Workflows&lt;/strong&gt;: Build a playground for any custom LLM workflow, such as RAG or agents. Enable all the team to easily iterate on its parameters and evaluate it from the web UI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM evaluation&lt;/strong&gt;: Run evaluation suite from the webUI using predefined evaluators like LLM-as-a-judge, RAG evaluators, or custom code evaluators. (&lt;a href=&#34;https://docs.agenta.ai/evaluation/overview?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;docs&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human evaluation&lt;/strong&gt;: Collaborate with subject matter experts for human annotation evaluation, including A/B testing and annotating golden test sets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Management&lt;/strong&gt;: Version your prompts and manage them across different environments (&lt;a href=&#34;https://docs.agenta.ai/prompt-management/overview?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;docs&lt;/a&gt;, &lt;a href=&#34;https://docs.agenta.ai/prompt-management/quick-start?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;quick start&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Tracing&lt;/strong&gt;: Observe and debug your apps with integrations to most providers and frameworks (&lt;a href=&#34;https://docs.agenta.ai/observability/overview?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;docs&lt;/a&gt;, &lt;a href=&#34;https://docs.agenta.ai/observability/quickstart?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;quick start&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Monitoring&lt;/strong&gt;: Track cost and latency and compare different deployments.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Agenta Cloud:&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get started is through Agenta Cloud. It is free to signup, and comes with a generous free-tier.&lt;/p&gt; &#xA;&lt;a href=&#34;https://cloud.agenta.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source width=&#34;160&#34; media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/user-attachments/assets/759422d8-01bc-4503-bf3c-b5871c99359a&#34;&gt; &#xA;  &lt;source width=&#34;160&#34; media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/user-attachments/assets/ffa9af5f-0981-4e95-9272-cb35eedb6780&#34;&gt; &#xA;  &lt;img alt=&#34;Get Started with Agenta Cloud&#34; src=&#34;https://github.com/user-attachments/assets/ffa9af5f-0981-4e95-9272-cb35eedb6780&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Self-host:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir agenta &amp;amp;&amp;amp; cd agenta&#xA;curl -L https://raw.githubusercontent.com/agenta-ai/agenta/main/docker-compose.gh.yml -o docker-compose.gh.yml&#xA;docker compose -f docker-compose.gh.yml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disabling Anonymized Tracking&lt;/h1&gt; &#xA;&lt;p&gt;By default, Agenta automatically reports anonymized basic usage statistics. This helps us understand how Agenta is used and track its overall usage and growth. This data does not include any sensitive information. To disable anonymized telemetry, follow these steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For web: Set &lt;code&gt;TELEMETRY_TRACKING_ENABLED&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in your &lt;code&gt;agenta-web/.env&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;For CLI: Set &lt;code&gt;telemetry_tracking_enabled&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in your &lt;code&gt;~/.agenta/config.toml&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We warmly welcome contributions to Agenta. Feel free to submit issues, fork the repository, and send pull requests.&lt;/p&gt; &#xA;&lt;p&gt;We are usually hanging in our Slack. Feel free to &lt;a href=&#34;https://join.slack.com/t/agenta-hq/shared_invite/zt-1zsafop5i-Y7~ZySbhRZvKVPV5DO_7IA&#34;&gt;join our Slack and ask us anything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://docs.agenta.ai/misc/contributing/getting-started?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme&#34;&gt;Contributing Guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors ✨&lt;/h2&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#contributors-&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/all_contributors-48-orange.svg?style=flat-square&#34; alt=&#34;All Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful people (&lt;a href=&#34;https://allcontributors.org/docs/en/emoji-key&#34;&gt;emoji key&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/SamMethnani&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/57623556?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Sameh Methnani&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Sameh Methnani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=SamMethnani&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=SamMethnani&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/suadsuljovic&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/8658374?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Suad Suljovic&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Suad Suljovic&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=suadsuljovic&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#design-suadsuljovic&#34; title=&#34;Design&#34;&gt;🎨&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#mentoring-suadsuljovic&#34; title=&#34;Mentoring&#34;&gt;🧑‍🏫&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/pulls?q=is%3Apr+reviewed-by%3Asuadsuljovic&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/burtenshaw&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/19620375?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;burtenshaw&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;burtenshaw&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=burtenshaw&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://abram.tech&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/55067204?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Abram&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Abram&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=aybruhm&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=aybruhm&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://israelabebe.com&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7479824?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Israel Abebe&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Israel Abebe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Avernu&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#design-vernu&#34; title=&#34;Design&#34;&gt;🎨&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=vernu&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/SohaibAnwaar&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/29427728?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Master X&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Master X&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=SohaibAnwaar&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://main-portfolio-26wv6oglp-witehound.vercel.app/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/26417477?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;corinthian&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;corinthian&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=witehound&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#design-witehound&#34; title=&#34;Design&#34;&gt;🎨&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Pajko97&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/25198892?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Pavle Janjusevic&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Pavle Janjusevic&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#infra-Pajko97&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;🚇&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://kaosiso-ezealigo.netlify.app&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/99529776?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Kaosi Ezealigo&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Kaosi Ezealigo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Abekossy&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=bekossy&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/albnunes&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/46302915?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Alberto Nunes&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Alberto Nunes&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Aalbnunes&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/mohammed-maaz-6290b0116/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/17180132?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Maaz Bin Khawar&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Maaz Bin Khawar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=MohammedMaaz&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/pulls?q=is%3Apr+reviewed-by%3AMohammedMaaz&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#mentoring-MohammedMaaz&#34; title=&#34;Mentoring&#34;&gt;🧑‍🏫&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/devgenix&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/56418363?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Nehemiah Onyekachukwu Emmanuel&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Nehemiah Onyekachukwu Emmanuel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=devgenix&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#example-devgenix&#34; title=&#34;Examples&#34;&gt;💡&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=devgenix&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/philipokiokio&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/55271518?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Philip Okiokio&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Philip Okiokio&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=philipokiokio&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://sweetdevil144.github.io/My-Website/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/117591942?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Abhinav Pandey&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Abhinav Pandey&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Sweetdevil144&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/RamchandraWarang9822&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92023869?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Ramchandra Warang&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ramchandra Warang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=RamchandraWarang9822&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3ARamchandraWarang9822&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/lazyfuhrer&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/64888892?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Biswarghya Biswas&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Biswarghya Biswas&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=lazyfuhrer&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/okieLoki&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/96105929?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Uddeepta Raaj Kashyap&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Uddeepta Raaj Kashyap&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=okieLoki&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://www.linkedin.com/in/nayeem-abdullah-317098141&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/32274108?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Nayeem Abdullah&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Nayeem Abdullah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=nayeem01&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/kangsuhyun-yanolja&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/124246127?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Kang Suhyun&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Kang Suhyun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=kangsuhyun-yanolja&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/yeokyeong-yanolja&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/128676129?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Yoon&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Yoon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=yeokyeong-yanolja&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://mrkirthi24.netlify.app/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/53830546?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Kirthi Bagrecha Jain&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Kirthi Bagrecha Jain&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=mrkirthi-24&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/navdeep1840&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/80774259?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Navdeep&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Navdeep&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=navdeep1840&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/rhythm-sharma-708a421a8/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/64489317?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Rhythm Sharma&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Rhythm Sharma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Rhythm-08&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://osinachi.me&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/40396070?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Osinachi Chukwujama &#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Osinachi Chukwujama &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=vicradon&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://liduos.com/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/47264881?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;莫尔索&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;莫尔索&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=morsoli&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://luccithedev.com&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22600781?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Agunbiade Adedeji&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Agunbiade Adedeji&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=dejongbaba&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://techemmy.github.io/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/43725109?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Emmanuel Oloyede&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Oloyede&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=techemmy&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=techemmy&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Dhaneshwarguiyan&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/116065351?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Dhaneshwarguiyan&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Dhaneshwarguiyan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Dhaneshwarguiyan&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/PentesterPriyanshu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/98478305?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Priyanshu Prajapati&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Priyanshu Prajapati&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=PentesterPriyanshu&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://venkataravitejagullapudi.github.io/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/70102577?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Raviteja&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Raviteja&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=VenkataRavitejaGullapudi&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/ArijitCloud&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81144422?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Arijit&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Arijit&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=ArijitCloud&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Yachika9925&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/147185379?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Yachika9925&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Yachika9925&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Yachika9925&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Dhoni77&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/53973174?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Aldrin&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Aldrin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Dhoni77&#34; title=&#34;Tests&#34;&gt;⚠️&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/seungduk-yanolja&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/115020208?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;seungduk.kim.2304&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;seungduk.kim.2304&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=seungduk-yanolja&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://dandrei.com/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/59015981?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Andrei Dragomir&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Andrei Dragomir&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=andreiwebdev&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://diegolikescode.me/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/57499868?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;diego&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;diego&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=diegolikescode&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/brockWith&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/105627491?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;brockWith&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;brockWith&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=brockWith&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://denniszelada.wordpress.com/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/219311?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Dennis Zelada&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Dennis Zelada&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=denniszelada&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/romainrbr&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10381609?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Romain Brucker&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Romain Brucker&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=romainrbr&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://heonheo.com&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/76820291?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Heon Heo&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Heon Heo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=HeonHeo23&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Drewski2222&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/39228951?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Drew Reisner&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Drew Reisner&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=Drewski2222&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://speakerdeck.com/eltociear&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22633385?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Ikko Eltociear Ashimine&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ikko Eltociear Ashimine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=eltociear&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/vishalvanpariya&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/27823328?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Vishal Vanpariya&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Vishal Vanpariya&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=vishalvanpariya&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/youcefs21&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/34604972?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Youcef Boumar&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Youcef Boumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=youcefs21&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/LucasTrg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/47852577?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;LucasTrg&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;LucasTrg&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=LucasTrg&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3ALucasTrg&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://ashrafchowdury.me&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87828904?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Ashraf Chowdury&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ashraf Chowdury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Aashrafchowdury&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=ashrafchowdury&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/jp-agenta&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/174311389?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;jp-agenta&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;jp-agenta&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/commits?author=jp-agenta&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Ajp-agenta&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://mrunhap.github.io&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24653356?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Mr Unhappy&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Mr Unhappy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/Agenta-AI/agenta/issues?q=author%3Amrunhap&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Agenta-AI/agenta/main/#infra-mrunhap&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;🚇&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;p&gt;This project follows the &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind are welcome!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gaogaotiantian/viztracer</title>
    <updated>2024-12-02T01:34:35Z</updated>
    <id>tag:github.com,2024-12-02:/gaogaotiantian/viztracer</id>
    <link href="https://github.com/gaogaotiantian/viztracer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A debugging and profiling tool that can trace and visualize python code execution&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VizTracer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/actions?query=workflow%3Abuild&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/actions?query=workflow%3ALint&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/workflows/lint/badge.svg?sanitize=true&#34; alt=&#34;flake8&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://img.shields.io/readthedocs/viztracer&#34; alt=&#34;readthedocs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/gaogaotiantian/viztracer&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/gaogaotiantian/viztracer&#34; alt=&#34;coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/viztracer/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/viztracer.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=gaogaotiantian.viztracer-vscode&#34;&gt;&lt;img src=&#34;https://img.shields.io/visual-studio-marketplace/v/gaogaotiantian.viztracer-vscode?logo=visual-studio&#34; alt=&#34;Visual Studio Marketplace Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/pypi/pyversions/viztracer&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/viztracer&#34; alt=&#34;support-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/gaogaotiantian/viztracer&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/commits/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/gaogaotiantian/viztracer&#34; alt=&#34;commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/gaogaotiantian&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E2%9D%A4-Sponsor%20me-%23c96198?style=flat&amp;amp;logo=GitHub&#34; alt=&#34;sponsor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VizTracer is a low-overhead logging/debugging/profiling tool that can trace and visualize your python code execution.&lt;/p&gt; &#xA;&lt;p&gt;The front-end UI is powered by &lt;a href=&#34;https://perfetto.dev/&#34;&gt;Perfetto&lt;/a&gt;. &lt;strong&gt;Use &#34;AWSD&#34; to zoom/navigate&lt;/strong&gt;. More help can be found in &#34;Support - Controls&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/example.png&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/example.png&#34; alt=&#34;example_img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detailed function entry/exit information on timeline with source code&lt;/li&gt; &#xA; &lt;li&gt;Super easy to use, no source code change for most features, no package dependency&lt;/li&gt; &#xA; &lt;li&gt;Low overhead, probably the fastest tracer in the market&lt;/li&gt; &#xA; &lt;li&gt;Supports threading, multiprocessing, subprocess, async and PyTorch&lt;/li&gt; &#xA; &lt;li&gt;Powerful front-end, able to render GB-level trace smoothly&lt;/li&gt; &#xA; &lt;li&gt;Works on Linux/MacOS/Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;The preferred way to install VizTracer is via pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install viztracer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Command Line&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Instead of &#34;python3 my_script.py arg1 arg2&#34;&#xA;viztracer my_script.py arg1 arg2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; A &lt;code&gt;result.json&lt;/code&gt; file will be generated, which you can open with &lt;code&gt;vizviewer&lt;/code&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# You can display all the files in a directory and open them in browser too&#xA;vizviewer ./&#xA;# For very large trace files, try external trace processor&#xA;vizviewer --use_external_processor result.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;vizviewer will host an HTTP server on &lt;code&gt;http://localhost:9001&lt;/code&gt;. You can also open your browser and use that address.&lt;/p&gt; &#xA; &lt;p&gt;If you do not want vizviewer to open the webbrowser automatically, you can use&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;vizviewer --server_only result.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you just need to bring up the trace report once, and do not want the persistent server, use&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;vizviewer --once result.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;vizviewer result.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=gaogaotiantian.viztracer-vscode&#34;&gt;VS Code Extension&lt;/a&gt; is available to make your life even easier.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/gaogaotiantian/viztracer-vscode/raw/master/assets/demo.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Add &lt;code&gt;--open&lt;/code&gt; to open the reports right after tracing &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;viztracer --open my_script.py arg1 arg2&#xA;viztracer -o result.html --open my_script.py arg1 arg2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; modules and console scripts(like &lt;code&gt;flask&lt;/code&gt;) are supported as well &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;viztracer -m your_module&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;viztracer flask run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Inline&lt;/h3&gt; &#xA;&lt;p&gt;You can also manually start/stop VizTracer in your script as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from viztracer import VizTracer&#xA;&#xA;tracer = VizTracer()&#xA;tracer.start()&#xA;# Something happens here&#xA;tracer.stop()&#xA;tracer.save() # also takes output_file as an optional argument&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, you can do it with &lt;code&gt;with&lt;/code&gt; statement&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with VizTracer(output_file=&#34;optional.json&#34;) as tracer:&#xA;    # Something happens here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Jupyter&lt;/h3&gt; &#xA;&lt;p&gt;If you are using Jupyter, you can use viztracer cell magics.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# You need to load the extension first&#xA;%load_ext viztracer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%viztracer&#xA;# Your code after&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A &lt;code&gt;VizTracer Report&lt;/code&gt; button will appear after the cell and you can click it to view the results&lt;/p&gt; &#xA;&lt;h3&gt;PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer can log native calls and GPU events of PyTorch (based on &lt;code&gt;torch.profiler&lt;/code&gt;) with &lt;code&gt;--log_torch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with VizTracer(log_torch=True) as tracer:&#xA;    # Your torch code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;viztracer --log_torch your_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Trace Filter&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer can filter out the data you don&#39;t want to reduce overhead and keep info of a longer time period before you dump the log.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#min-duration&#34;&gt;Min Duration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#max-stack-depth&#34;&gt;Max Stack Depth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#include-files&#34;&gt;Include Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#exclude-files&#34;&gt;Exclude Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#ignore-c-function&#34;&gt;Ignore C Function&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/filter.html#log-sparse&#34;&gt;Sparse Log&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extra Logs without Code Change&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer can log extra information without changing your source code&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-variable&#34;&gt;Any Variable/Attribute with RegEx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-function-entry&#34;&gt;Function Entry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-function-execution&#34;&gt;Variables in Specified Function&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-garbage-collector&#34;&gt;Garbage Collector Operation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-function-arguments&#34;&gt;Function Input Arguments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-function-return-value&#34;&gt;Function Return Value&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-audit&#34;&gt;Audit Events&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/extra_log.html#log-exception&#34;&gt;Raised Exceptions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Add Custom Event&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer supports inserting custom events while the program is running. This works like a print debug, but you can know when this print happens while looking at trace data.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/custom_event_intro.html#instant-event&#34;&gt;Instant Event&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/custom_event_intro.html#variable-event&#34;&gt;Variable Event&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/custom_event_intro.html#duration-event&#34;&gt;Duration Event&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Misc&lt;/h2&gt; &#xA;&lt;h3&gt;Multi Thread Support&lt;/h3&gt; &#xA;&lt;p&gt;For Python3.12+, VizTracer supports Python-level multi-thread tracing without the need to do any modification to your code.&lt;/p&gt; &#xA;&lt;p&gt;For versions before 3.12, VizTracer supports python native &lt;code&gt;threading&lt;/code&gt; module. Just start &lt;code&gt;VizTracer&lt;/code&gt; before you create threads and it will just work.&lt;/p&gt; &#xA;&lt;p&gt;For other multi-thread scenarios, you can use &lt;code&gt;enable_thread_tracing()&lt;/code&gt; to notice VizTracer about the thread to trace it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/multithread_example.png&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/multithread_example.png&#34; alt=&#34;example_img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/concurrency.html&#34;&gt;multi thread docs&lt;/a&gt; for details&lt;/p&gt; &#xA;&lt;h3&gt;Multi Process Support&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer supports &lt;code&gt;subprocess&lt;/code&gt;, &lt;code&gt;multiprocessing&lt;/code&gt;, &lt;code&gt;os.fork()&lt;/code&gt;, &lt;code&gt;concurrent.futures&lt;/code&gt;, and &lt;code&gt;loky&lt;/code&gt; out of the box.&lt;/p&gt; &#xA;&lt;p&gt;For more general multi-process cases, VizTracer can support with some extra steps.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/multiprocess_example.png&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/multiprocess_example.png&#34; alt=&#34;example_img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/concurrency.html&#34;&gt;multi process docs&lt;/a&gt; for details&lt;/p&gt; &#xA;&lt;h3&gt;Async Support&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer supports &lt;code&gt;asyncio&lt;/code&gt; natively, but could enhance the report by using &lt;code&gt;--log_async&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/async_example.png&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/async_example.png&#34; alt=&#34;example_img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/concurrency.html&#34;&gt;async docs&lt;/a&gt; for details&lt;/p&gt; &#xA;&lt;h3&gt;Flamegraph&lt;/h3&gt; &#xA;&lt;p&gt;Perfetto supports native flamegraph, just select slices on the UI and choose &#34;Slice Flamegraph&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/flamegraph.png&#34;&gt;&lt;img src=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/img/flamegraph.png&#34; alt=&#34;example_img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Remote attach&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer supports remote attach to an arbitrary Python process to trace it, as long as viztracer is importable&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable/remote_attach.html&#34;&gt;remote attach docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;JSON alternative&lt;/h3&gt; &#xA;&lt;p&gt;VizTracer needs to dump the internal data to json format. It is recommended for the users to install &lt;code&gt;orjson&lt;/code&gt;, which is much faster than the builtin &lt;code&gt;json&lt;/code&gt; library. VizTracer will try to import &lt;code&gt;orjson&lt;/code&gt; and fall back to the builtin &lt;code&gt;json&lt;/code&gt; library if &lt;code&gt;orjson&lt;/code&gt; does not exist.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;VizTracer puts in a lot of effort to achieve low overhead. The actual performance impact largely depends on your application. For typical codebases, the overhead is expected to be below 1x. If your code has infrequent function calls, the overhead could be minimal.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Detailed explanation &lt;/summary&gt; &#xA; &lt;p&gt;The overhead introduced by VizTracer is basically a fixed amount of time during function entry and exit, so the more time spent on function entries and exits, the more overhead will be observed. A pure recursive &lt;code&gt;fib&lt;/code&gt; function could suffer 3x-4x overhead on Python3.11+ (when the Python call is optimized, before that Python call was slower so the overhead ratio would be less).&lt;/p&gt; &#xA; &lt;p&gt;In the real life scenario, your code should not spend too much time on function calls (they don&#39;t really do anything useful), so the overhead would be much smaller.&lt;/p&gt; &#xA; &lt;p&gt;Many techniques are applied to minimize the overall overhead during code execution to reduce the inevitable skew introduced by VizTracer (the report saving part is not as critical). For example, VizTracer tries to use the CPU timestamp counter instead of a syscall to get the time when available. On Python 3.12+, VizTracer uses &lt;code&gt;sys.monitoring&lt;/code&gt; which has less overhead than &lt;code&gt;sys.setprofile&lt;/code&gt;. All of the efforts made it observably faster than &lt;code&gt;cProfile&lt;/code&gt;, the Python stdlib profiler.&lt;/p&gt; &#xA; &lt;p&gt;However, VizTracer is a tracer, which means it has to record every single function entry and exit, so it can&#39;t be as fast as the sampling profilers - they are not the same thing. With the extra overhead, VizTracer provides a lot more information than normal sampling profilers.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For full documentation, please see &lt;a href=&#34;https://viztracer.readthedocs.io/en/stable&#34;&gt;https://viztracer.readthedocs.io/en/stable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bugs/Requests&lt;/h2&gt; &#xA;&lt;p&gt;Please send bug reports and feature requests through &lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/issues&#34;&gt;github issue tracker&lt;/a&gt;. VizTracer is currently under development now and it&#39;s open to any constructive suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2020-2024 Tian Gao.&lt;/p&gt; &#xA;&lt;p&gt;Distributed under the terms of the &lt;a href=&#34;https://github.com/gaogaotiantian/viztracer/raw/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>t41372/Open-LLM-VTuber</title>
    <updated>2024-12-02T01:34:35Z</updated>
    <id>tag:github.com,2024-12-02:/t41372/Open-LLM-VTuber</id>
    <link href="https://github.com/t41372/Open-LLM-VTuber" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Talk to any LLM with hands-free voice interaction, voice interruption, Live2D taking face, and long-term memory running locally across platforms&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-LLM-VTuber&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/t41372/Open-LLM-VTuber/raw/main/README.CN.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/t41372/Open-LLM-VTuber/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/t41372/Open-LLM-VTuber&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/t41372/Open-LLM-VTuber/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/t41372/Open-LLM-VTuber&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/t41372/open-llm-vtuber&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/t41372%2FOpen--LLM--VTuber-%25230db7ed.svg?logo=docker&amp;amp;logoColor=blue&amp;amp;labelColor=white&amp;amp;color=blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/users/t41372/projects/1/views/1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/todo_list-GitHub_Project-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/yi.ting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;amp;logo=buy-me-a-coffee&amp;amp;logoColor=black&#34; alt=&#34;BuyMeACoffee&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/3UDA8YFDXx&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/3UDA8YFDXx&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &amp;lt;- (Clickable links)&lt;/p&gt; &#xA;&lt;p&gt;(QQ群: 792615362）&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;常见问题 Common Issues doc (Written in Chinese): &lt;a href=&#34;https://docs.qq.com/doc/DTHR6WkZ3aU9JcXpy&#34;&gt;https://docs.qq.com/doc/DTHR6WkZ3aU9JcXpy&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;User Survey: &lt;a href=&#34;https://forms.gle/w6Y6PiHTZr1nzbtWA&#34;&gt;https://forms.gle/w6Y6PiHTZr1nzbtWA&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;调查问卷(中文)(现在不用登入了): &lt;a href=&#34;https://wj.qq.com/s2/16150415/f50a/&#34;&gt;https://wj.qq.com/s2/16150415/f50a/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; This project is in its early stages and is currently under &lt;strong&gt;active development&lt;/strong&gt;. Features are unstable, code is messy, and breaking changes will occur. The main goal of this stage is to build a minimum viable prototype using technologies that are easy to integrate.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; This project is &lt;strong&gt;NOT&lt;/strong&gt; easy to install. Join the Discord server or QQ group if you need help or to get updates about this project.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; If you want to run this program on a server and access it remotely on your laptop, the microphone on the front end will only launch in a secure context (a.k.a. https or localhost). See &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia&#34;&gt;MDN Web Doc&lt;/a&gt;. Therefore, you might want to configure https with a reverse proxy if you want to access the page on a remote machine (non-localhost).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;What is this project?&lt;/h3&gt; &#xA;&lt;p&gt;Open-LLM-VTuber allows you to talk to (and interrupt!) any LLM locally by voice (hands-free) with a Live2D talking face. The LLM inference backend, speech recognition, and speech synthesizer are all designed to be swappable. This project can be configured to run offline on macOS, Linux, and Windows. Online LLM/ASR/TTS options are also supported.&lt;/p&gt; &#xA;&lt;p&gt;Long-term memory with MemGPT can be configured to achieve perpetual chat, infinite* context length, and external data source.&lt;/p&gt; &#xA;&lt;p&gt;This project started as an attempt to recreate the closed-source AI VTuber &lt;code&gt;neuro-sama&lt;/code&gt; with open-source alternatives that can run offline on platforms other than Windows.&lt;/p&gt; &#xA;&lt;img width=&#34;500&#34; alt=&#34;demo-image&#34; src=&#34;https://github.com/t41372/Open-LLM-VTuber/assets/36402030/fa363492-7c01-47d8-915f-f12a3a95942c&#34;&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;English demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/f13b2f8e-160c-4e59-9bdb-9cfb6e57aca9&#34;&gt;https://github.com/user-attachments/assets/f13b2f8e-160c-4e59-9bdb-9cfb6e57aca9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;English Demo: &lt;a href=&#34;https://youtu.be/gJuPM_2qEZc&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;中文 demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1krHUeRE98/&#34;&gt;BiliBili&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/cb5anPTNklw&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;中文安装教学:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1GYSrYKE8i/&#34;&gt;BiliBili&lt;/a&gt; (v0.2.4)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;English Installation Tutorial:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We don&#39;t have it yet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why this project and not other similar projects on GitHub?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It works on macOS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Many existing solutions display Live2D models with VTube Studio and achieve lip sync by routing desktop internal audio into VTube Studio and controlling the lips with that. On macOS, however, there is no easy way to let VTuber Studio listen to internal audio on the desktop.&lt;/li&gt; &#xA;   &lt;li&gt;Many existing solutions lack support for GPU acceleration on macOS, which makes them run slow on Mac.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;This project supports &lt;a href=&#34;https://github.com/cpacker/MemGPT&#34;&gt;MemGPT&lt;/a&gt; for perpetual chat. The chatbot remembers what you&#39;ve said.&lt;/li&gt; &#xA; &lt;li&gt;No data leaves your computer if you wish to &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can choose local LLM/voice recognition/speech synthesis solutions; everything works offline. Tested on macOS.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You can interrupt the LLM anytime with your voice without wearing headphones.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Basic Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Chat with any LLM by voice&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Interrupt LLM with voice at any time&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Choose your own LLM backend&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Choose your own Speech Recognition &amp;amp; Text to Speech provider&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Long-term memory&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Live2D frontend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target Platform&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS&lt;/li&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Feature Updates&lt;/h3&gt; &#xA;&lt;p&gt;... A lot more. Update notes are now locate in GitHub Release.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Sep 17, 2024] Added DeepLX translation to change the language for audio&lt;/li&gt; &#xA; &lt;li&gt;[Sep 6, 2024] Added GroqWhisperASR&lt;/li&gt; &#xA; &lt;li&gt;[Sep 5, 2024] Better Docker support&lt;/li&gt; &#xA; &lt;li&gt;[Sep 1, 2024] Added voice interruption (and refactored the backend)&lt;/li&gt; &#xA; &lt;li&gt;[Jul 15, 2024] Added MeloTTS&lt;/li&gt; &#xA; &lt;li&gt;[Jul 15, 2024] Refactored llm and launch.py and reduced TTS latency&lt;/li&gt; &#xA; &lt;li&gt;[Jul 11, 2024] Added CosyVoiceTTS&lt;/li&gt; &#xA; &lt;li&gt;[Jul 11, 2024] Added FunASR with SenseVoiceSmall speech recognition model.&lt;/li&gt; &#xA; &lt;li&gt;[Jul 7, 2024] Totally untested Docker support with Nvidia GPU passthrough (no Mac, no AMD)&lt;/li&gt; &#xA; &lt;li&gt;[Jul 6, 2024] Support for Chinese 支持中文 and probably some other languages...&lt;/li&gt; &#xA; &lt;li&gt;[Jul 6, 2024] WhisperCPP with macOS GPU acceleration. Dramatically decreased latency on Mac&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implemented Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Talk to LLM with voice. Offline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;RAG on chat history&lt;/del&gt; &lt;em&gt;(temporarily removed)&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently supported LLM backend&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any OpenAI-API-compatible backend, such as Ollama, Groq, LM Studio, OpenAI, and more.&lt;/li&gt; &#xA; &lt;li&gt;MemGPT (setup required)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently supported Speech recognition backend&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/FunASR&#34;&gt;FunASR&lt;/a&gt;, which support &lt;a href=&#34;https://github.com/FunAudioLLM/SenseVoice&#34;&gt;SenseVoiceSmall&lt;/a&gt; and many other models. (&lt;del&gt;Local&lt;/del&gt; Currently requires an internet connection for loading. Compute locally)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper&#34;&gt;Faster-Whisper&lt;/a&gt; (Local)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;Whisper-CPP&lt;/a&gt; using the python binding &lt;a href=&#34;https://github.com/abdeladim-s/pywhispercpp&#34;&gt;pywhispercpp&lt;/a&gt; (Local, mac GPU acceleration can be configured)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; (local)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groq.com/&#34;&gt;Groq Whisper&lt;/a&gt; (API Key required). This is a hosted Whisper endpoint, which is fast and has a generous free limit every day.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/products/ai-services/speech-to-text&#34;&gt;Azure Speech Recognition&lt;/a&gt; (API Key required)&lt;/li&gt; &#xA; &lt;li&gt;The microphone in the server terminal will be used by default. You can change the setting &lt;code&gt;MIC_IN_BROWSER&lt;/code&gt; in the &lt;code&gt;conf.yaml&lt;/code&gt; to move the microphone (and voice activation detection) to the browser (at the cost of latency, for now). You might want to use the microphone on your client (the browser) rather than the one on your server if you run the backend on a different machine or inside a VM or docker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently supported Text to Speech backend&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thevickypedia/py3-tts&#34;&gt;py3-tts&lt;/a&gt; (Local, it uses your system&#39;s default TTS engine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/myshell-ai/MeloTTS&#34;&gt;meloTTS&lt;/a&gt; (Local, fast)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/idiap/coqui-ai-TTS&#34;&gt;Coqui-TTS&lt;/a&gt; (Local, speed depends on the model you run.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;bark&lt;/a&gt; (Local, very resource-consuming)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;CosyVoice&lt;/a&gt; (Local, very resource-consuming)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daswer123/xtts-api-server&#34;&gt;xTTSv2&lt;/a&gt; (Local, very resource-consuming)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rany2/edge-tts&#34;&gt;Edge TTS&lt;/a&gt; (online, no API key required)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/products/ai-services/text-to-speech&#34;&gt;Azure Text-to-Speech&lt;/a&gt; (online, API Key required)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Fast Text Synthesis&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Synthesize sentences as soon as they arrive, so there is no need to wait for the entire LLM response.&lt;/li&gt; &#xA; &lt;li&gt;Producer-consumer model with multithreading: Audio will be continuously synthesized in the background. They will be played one by one whenever the new audio is ready. The audio player will not block the audio synthesizer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Live2D Talking face&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change Live2D model with &lt;code&gt;config.yaml&lt;/code&gt; (model needs to be listed in model_dict.json)&lt;/li&gt; &#xA; &lt;li&gt;Load local Live2D models. Check &lt;code&gt;doc/live2d.md&lt;/code&gt; for documentation.&lt;/li&gt; &#xA; &lt;li&gt;Uses expression keywords in LLM response to control facial expression, so there is no additional model for emotion detection. The expression keywords are automatically loaded into the system prompt and excluded from the speech synthesis output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;live2d technical details&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uses &lt;a href=&#34;https://github.com/guansss/pixi-live2d-display&#34;&gt;guansss/pixi-live2d-display&lt;/a&gt; to display live2d models in &lt;em&gt;browser&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Uses WebSocket to control facial expressions and talking state between the server and the front end&lt;/li&gt; &#xA; &lt;li&gt;All the required packages are locally available, so the front end works offline.&lt;/li&gt; &#xA; &lt;li&gt;You can load live2d models from a URL or the one stored locally in the &lt;code&gt;live2d-models&lt;/code&gt; directory. The default &lt;code&gt;shizuku-local&lt;/code&gt; is stored locally and works offline. If the URL property of the model in the model_dict.json is a URL rather than a path starting with &lt;code&gt;/live2d-models&lt;/code&gt;, they will need to be fetched from the specified URL whenever the front end is opened. Read &lt;code&gt;doc/live2d.md&lt;/code&gt; for documentation on loading your live2D model from local.&lt;/li&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;server.py&lt;/code&gt; to run the WebSocket communication server, open the &lt;code&gt;index.html&lt;/code&gt; in the &lt;code&gt;./static&lt;/code&gt; folder to open the front end, and run &lt;del&gt;&lt;code&gt;launch.py&lt;/code&gt;&lt;/del&gt; &lt;code&gt;main.py&lt;/code&gt; to run the backend for LLM/ASR/TTS processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;New installation instruction is being created &lt;a href=&#34;https://github.com/t41372/Open-LLM-VTuber/wiki&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Requirements:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ffmpeg&lt;/li&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10, &amp;lt; 3.13&lt;/li&gt; &#xA; &lt;li&gt;More testing needs to be done on Python 3.13&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone this repository.&lt;/p&gt; &#xA;&lt;p&gt;You need to have &lt;a href=&#34;https://github.com/jmorganca/ollama&#34;&gt;Ollama&lt;/a&gt; or any other OpenAI-API-Compatible backend ready and running. If you want to use MemGPT as your backend, scroll down to the MemGPT section.&lt;/p&gt; &#xA;&lt;p&gt;Prepare the LLM of your choice. Edit the BASE_URL and MODEL in the project directory&#39;s &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Virtual Python environment like conda or venv is strongly recommended! (because the dependencies are a mess!).&lt;/p&gt; &#xA;&lt;p&gt;Run the following in the terminal to install the dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt # Run this in the project directory &#xA;# Install Speech recognition dependencies and text-to-speech dependencies according to the instructions below&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This project, by default, launches the audio interaction mode, meaning you can talk to the LLM by voice, and the LLM will talk back to you by voice.&lt;/p&gt; &#xA;&lt;p&gt;Edit the &lt;code&gt;conf.yaml&lt;/code&gt; for configurations. You can follow the configuration used in the demo video.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use live2d, run &lt;code&gt;server.py&lt;/code&gt;. Open the page &lt;code&gt;localhost:12393&lt;/code&gt; (you can change this) with your browser, and you are ready. Once the live2D model appears on the screen, it&#39;s ready to talk to you.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t want the live2d, you can run &lt;code&gt;main.py&lt;/code&gt; with Python for cli mode.&lt;/p&gt; &#xA;&lt;p&gt;Some models will be downloaded on your first launch, which may require an internet connection and may take a while.&lt;/p&gt; &#xA;&lt;h3&gt;Update&lt;/h3&gt; &#xA;&lt;p&gt;Back up the configuration files &lt;code&gt;conf.yaml&lt;/code&gt; if you&#39;ve edited them, and then update the repo. Or just clone the repo again and make sure to transfer your configurations. The configuration file will sometimes change because this project is still in its early stages. Be cautious when updating the program.&lt;/p&gt; &#xA;&lt;h2&gt;Install Speech Recognition (ASR)&lt;/h2&gt; &#xA;&lt;p&gt;Edit the ASR_MODEL settings in the &lt;code&gt;conf.yaml&lt;/code&gt; to change the provider.&lt;/p&gt; &#xA;&lt;p&gt;Here are the options you have for speech recognition:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;FunASR&lt;/code&gt; (&lt;del&gt;local&lt;/del&gt;) (Runs very fast even on CPU. Not sure how they did it)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/FunASR?tab=readme-ov-file&#34;&gt;FunASR&lt;/a&gt; is a Fundamental End-to-End Speech Recognition Toolkit from ModelScope that runs many ASR models. The result and speed are pretty good with the SenseVoiceSmall from &lt;a href=&#34;https://github.com/FunAudioLLM/SenseVoice&#34;&gt;FunAudioLLM&lt;/a&gt; at Alibaba Group.&lt;/li&gt; &#xA; &lt;li&gt;Install with &lt;code&gt;pip install -U funasr modelscope huggingface_hub&lt;/code&gt;. Also, ensure you have torch (torch&amp;gt;=1.13) and torchaudio. Install them with &lt;code&gt;pip install torch torchaudio&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;It requires an internet connection on launch &lt;em&gt;even if the models are locally available&lt;/em&gt;. See &lt;a href=&#34;https://github.com/modelscope/FunASR/issues/1897&#34;&gt;https://github.com/modelscope/FunASR/issues/1897&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Faster-Whisper&lt;/code&gt; (local)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Whisper, but faster. On macOS, it runs on CPU only, which is not so fast, but it&#39;s easy to use.&lt;/li&gt; &#xA; &lt;li&gt;For Nvidia GPU users, to use GPU acceleration, you need the following NVIDIA libraries to be installed: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cublas&#34;&gt;cuBLAS for CUDA 12&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN 8 for CUDA 12&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Or if you don&#39;t need the speed, you can set the &lt;code&gt;device&lt;/code&gt; setting under &lt;code&gt;Faster-Whisper&lt;/code&gt; in &lt;code&gt;conf.yaml&lt;/code&gt; to &lt;code&gt;cpu&lt;/code&gt; to reduce headaches.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;WhisperCPP&lt;/code&gt; (local) (runs super fast on a Mac if configured correctly)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are on a Mac, read below for instructions on setting up WhisperCPP with coreML support. If you want to use CPU or Nvidia GPU, install the package by running &lt;code&gt;pip install pywhispercpp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The whisper cpp python binding. It can run on coreML with configuration, which makes it very fast on macOS.&lt;/li&gt; &#xA; &lt;li&gt;On CPU or Nvidia GPU, it&#39;s probably slower than Faster-Whisper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;WhisperCPP coreML configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uninstall the original &lt;code&gt;pywhispercpp&lt;/code&gt; if you have already installed it. We are building the package.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;install_coreml_whisper.py&lt;/code&gt; with Python to automatically clone and build the coreML-supported &lt;code&gt;pywhispercpp&lt;/code&gt; for you.&lt;/li&gt; &#xA; &lt;li&gt;Prepare the appropriate coreML models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can either convert models to coreml according to the documentation on Whisper.cpp repo&lt;/li&gt; &#xA;   &lt;li&gt;...or you can find some &lt;a href=&#34;https://huggingface.co/chidiwilliams/whisper.cpp-coreml/tree/main&#34;&gt;magical huggingface repo&lt;/a&gt; that happens to have those converted models. Just remember to decompress them. If the program fails to load the model, it will produce a segmentation fault.&lt;/li&gt; &#xA;   &lt;li&gt;You don&#39;t need to include those weird prefixes in the model name in the &lt;code&gt;conf.yaml&lt;/code&gt;. For example, if the coreML model&#39;s name looks like &lt;code&gt;ggml-base-encoder.mlmodelc&lt;/code&gt;, just put &lt;code&gt;base&lt;/code&gt; into the &lt;code&gt;model_name&lt;/code&gt; under &lt;code&gt;WhisperCPP&lt;/code&gt; settings in the &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Whisper&lt;/code&gt; (local)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original Whisper from OpenAI. Install it with &lt;code&gt;pip install -U openai-whisper&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The slowest of all. Added as an experiment to see if it can utilize macOS GPU. It didn&#39;t.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;GroqWhisperASR&lt;/code&gt; (online, API Key required)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Whisper endpoint from Groq. It&#39;s very fast and has a lot of free usage every day. It&#39;s pre-installed. Get an API key from &lt;a href=&#34;https://console.groq.com/keys&#34;&gt;groq&lt;/a&gt; and add it into the GroqWhisper setting in the &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;API key and internet connection are required.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;AzureASR&lt;/code&gt; (online, API Key required)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Azure Speech Recognition. Install with &lt;code&gt;pip install azure-cognitiveservices-speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;API key and internet connection are required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;⚠️ ‼️ The &lt;code&gt;api_key.py&lt;/code&gt; was deprecated in &lt;code&gt;v0.2.5&lt;/code&gt;. Please set api keys in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install Speech Synthesis (text to speech) (TTS)&lt;/h2&gt; &#xA;&lt;p&gt;Install the respective package and turn it on using the &lt;code&gt;TTS_MODEL&lt;/code&gt; option in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pyttsx3TTS&lt;/code&gt; (local, fast)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install with the command &lt;code&gt;pip install py3-tts&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This package will use the default TTS engine on your system. It uses &lt;code&gt;sapi5&lt;/code&gt; on Windows, &lt;code&gt;nsss&lt;/code&gt; on Mac, and &lt;code&gt;espeak&lt;/code&gt; on other platforms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;py3-tts&lt;/code&gt; is used instead of the more famous &lt;code&gt;pyttsx3&lt;/code&gt; because &lt;code&gt;pyttsx3&lt;/code&gt; seems unmaintained, and I couldn&#39;t get the latest version of &lt;code&gt;pyttsx3&lt;/code&gt; working.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;meloTTS&lt;/code&gt; (local, fast)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install MeloTTS according to their &lt;a href=&#34;https://github.com/myshell-ai/MeloTTS/raw/main/docs/install.md&#34;&gt;documentation&lt;/a&gt; (don&#39;t install via docker) (A nice place to clone the repo is the submodule folder, but you can put it wherever you want). If you encounter a problem related to &lt;code&gt;mecab-python&lt;/code&gt;, try this &lt;a href=&#34;https://github.com/polm/MeloTTS&#34;&gt;fork&lt;/a&gt; (hasn&#39;t been merging into the main as of July 16, 2024).&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s not the best, but it&#39;s definitely better than pyttsx3TTS, and it&#39;s pretty fast on my mac. I would choose this for now if I can&#39;t access the internet (and I would use edgeTTS if I have the internet).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;coquiTTS&lt;/code&gt; (local, can be fast or slow depending on the model you run)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seems easy to install&lt;/li&gt; &#xA; &lt;li&gt;Install with the command &lt;code&gt;pip install &#34;coqui-tts[languages]&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support many different TTS models. List all supported models with &lt;code&gt;tts --list_models&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;The default model is an english only model.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;tts_models/zh-CN/baker/tacotron2-DDC-GST&lt;/code&gt; for Chinese model. (but the consistency is weird...)&lt;/li&gt; &#xA; &lt;li&gt;If you found some good model to use, let me know! There are too many models I don&#39;t even know where to start...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;barkTTS&lt;/code&gt; (local, slow)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the pip package with this command &lt;code&gt;pip install git+https://github.com/suno-ai/bark.git&lt;/code&gt; and turn it on in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The required models will be downloaded on the first launch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;cosyvoiceTTS&lt;/code&gt; (local, slow)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure &lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;CosyVoice&lt;/a&gt; and launch the WebUI demo according to their documentation.&lt;/li&gt; &#xA; &lt;li&gt;Edit &lt;code&gt;conf.yaml&lt;/code&gt; to match your desired configurations. Check their WebUI and the API documentation on the WebUI to see the meaning of the configurations under the setting &lt;code&gt;cosyvoiceTTS&lt;/code&gt; in the &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;xTTSv2&lt;/code&gt; (local, slow)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Recommend to use xtts-api-server, it has clear api docs and relative easy to deploy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;edgeTTS&lt;/code&gt; (online, no API key required)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the pip package with this command &lt;code&gt;pip install edge-tts&lt;/code&gt; and turn it on in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It sounds pretty good. Runs pretty fast.&lt;/li&gt; &#xA; &lt;li&gt;Remember to connect to the internet when using edge tts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;fishAPITTS&lt;/code&gt; (online, API key required) &lt;code&gt;(added in v0.3.0-beta)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install with &lt;code&gt;pip install fish-audio-sdk&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Register an account, get an API key, find a voice you want to use, and copy the reference id on &lt;a href=&#34;https://fish.audio/&#34;&gt;Fish Audio&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;conf.yaml&lt;/code&gt; file, set the &lt;code&gt;TTS_MODEL&lt;/code&gt; to &lt;code&gt;fishAPITTS&lt;/code&gt;, and under the &lt;code&gt;fishAPITTS&lt;/code&gt; setting, set the &lt;code&gt;api_key&lt;/code&gt; and &lt;code&gt;reference_id&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;AzureTTS&lt;/code&gt; (online, API key required) (This is the exact same TTS used by neuro-sama)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the Azure SDK with the command&lt;code&gt;pip install azure-cognitiveservices-speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get an API key (for text to speech) from Azure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;⚠️ ‼️ The &lt;code&gt;api_key.py&lt;/code&gt; was deprecated in &lt;code&gt;v0.2.5&lt;/code&gt;. Please set api keys in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;The default settings in the &lt;code&gt;conf.yaml&lt;/code&gt; is the voice used by neuro-sama.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using macOS, you need to enable the microphone permission of your terminal emulator (you run this program inside your terminal, right? Enable the microphone permission for your terminal). If you fail to do so, the speech recognition will not be able to hear you because it does not have permission to use your microphone.&lt;/p&gt; &#xA;&lt;h2&gt;Translation&lt;/h2&gt; &#xA;&lt;p&gt;DeepLX translation was implemented to let the program speaks in a language different from the conversation language. For example, the LLM might be thinking in English, the subtitle is in English, and you are speaking English, but the voice of the LLM is in Japanese. This is achieved by translating the sentence before it was sent for audio generation.&lt;/p&gt; &#xA;&lt;p&gt;DeepLX is the only supported translation backend for now. Other providers will be implemented soon.&lt;/p&gt; &#xA;&lt;h3&gt;Enable Audio Translation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set &lt;code&gt;TRANSLATE_AUDIO&lt;/code&gt; in &lt;code&gt;conf.yaml&lt;/code&gt; to True&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;DEEPLX_TARGET_LANG&lt;/code&gt; to your desired language. Make sure this language matches the language of the TTS speaker (for example, if the &lt;code&gt;DEEPLX_TARGET_LANG&lt;/code&gt; is &#34;JA&#34;, which is Japanese, the TTS should also be speaking Japanese.).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;MemGPT (Probably broken)&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; MemGPT has been renamed to Letta and changed a bunch of things related to its API and how things functions. As of now, the integration of MemGPT in this project is not updated with the latest changes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;MemGPT integration is very experimental and requires quite a lot of setup. In addition, MemGPT requires a powerful LLM (larger than 7b and quantization above Q5) with a lot of token footprint, which means it&#39;s a lot slower. MemGPT does have its own LLM endpoint for free, though. You can test things with it. Check their docs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This project can use &lt;a href=&#34;https://github.com/cpacker/MemGPT&#34;&gt;MemGPT&lt;/a&gt; as its LLM backend. MemGPT enables LLM with long-term memory.&lt;/p&gt; &#xA;&lt;p&gt;To use MemGPT, you need to have the MemGPT server configured and running. You can install it using &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;docker&lt;/code&gt; or run it on a different machine. Check their &lt;a href=&#34;https://github.com/cpacker/MemGPT&#34;&gt;GitHub repo&lt;/a&gt; and &lt;a href=&#34;https://memgpt.readme.io/docs/index&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; I recommend you install MemGPT either in a separate Python virtual environment or in docker because there is currently a dependency conflict between this project and MemGPT (on fast API, it seems). You can check this issue &lt;a href=&#34;https://github.com/cpacker/MemGPT/issues/1382&#34;&gt;Can you please upgrade typer version in your dependancies #1382&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Here is a checklist:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install memgpt&lt;/li&gt; &#xA; &lt;li&gt;Configure memgpt&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;memgpt&lt;/code&gt; using &lt;code&gt;memgpt server&lt;/code&gt; command. Remember to have the server running before launching Open-LLM-VTuber.&lt;/li&gt; &#xA; &lt;li&gt;Set up an agent either through its cli or web UI. Add your system prompt with the Live2D Expression Prompt and the expression keywords you want to use (find them in &lt;code&gt;model_dict.json&lt;/code&gt;) into MemGPT&lt;/li&gt; &#xA; &lt;li&gt;Copy the &lt;code&gt;server admin password&lt;/code&gt; and the &lt;code&gt;Agent id&lt;/code&gt; into &lt;code&gt;./llm/memgpt_config.yaml&lt;/code&gt;. &lt;em&gt;By the way, &lt;code&gt;agent id&lt;/code&gt; is not the agent&#39;s name&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;LLM_PROVIDER&lt;/code&gt; to &lt;code&gt;memgpt&lt;/code&gt; in &lt;code&gt;conf.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Remember, if you use &lt;code&gt;memgpt&lt;/code&gt;, all LLM-related configurations in &lt;code&gt;conf.yaml&lt;/code&gt; will be ignored because &lt;code&gt;memgpt&lt;/code&gt; doesn&#39;t work that way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Mem0 (In development)&lt;/h2&gt; &#xA;&lt;p&gt;Another long term memory solution. Still in development. Highly experimental.&lt;/p&gt; &#xA;&lt;p&gt;Pro&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s easier to set up compare to MemGPT&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s a bit faster than MemGPT (but still would take quite a lot more LLM tokens to process)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cons&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It remembers your preference and thoughts, nothing else. It doesn&#39;t remember what the LLM said.&lt;/li&gt; &#xA; &lt;li&gt;It doesn&#39;t always put stuff into memory.&lt;/li&gt; &#xA; &lt;li&gt;It sometimes remember wrong stuff&lt;/li&gt; &#xA; &lt;li&gt;It requires an LLM with very good function calling capability, which is quite difficult for smaller models&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Issues&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;PortAudio&lt;/code&gt; Missing&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;code&gt;libportaudio2&lt;/code&gt; to your computer via your package manager like apt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Running in a Container [highly experimental]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; This is highly experimental, but I think it works. Most of the time.&lt;/p&gt; &#xA;&lt;p&gt;You can either build the image youself or pull it from the docker hub. &lt;a href=&#34;https://hub.docker.com/r/t41372/open-llm-vtuber&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/t41372%2FOpen--LLM--VTuber-%25230db7ed.svg?logo=docker&amp;amp;logoColor=blue&amp;amp;labelColor=white&amp;amp;color=blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(but the image size is crazy large)&lt;/li&gt; &#xA; &lt;li&gt;The image on the docker hub might not updated as regularly as it can be. GitHub action can&#39;t build an image as big as this. I might look into other options.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Current issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Large image size (~20GB), and will require more space because some models are optional and will be downloaded only when used.&lt;/li&gt; &#xA; &lt;li&gt;Nvidia GPU required (GPU passthrough limitation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;Nvidia Container Toolkit&lt;/a&gt; needs to be configured for GPU passthrough.&lt;/li&gt; &#xA; &lt;li&gt;Some models will have to be downloaded again if you stop the container. (will be fixed)&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t build the image on an Arm machine. One of the dependencies (grpc, to be exact) will fail for some reason &lt;a href=&#34;https://github.com/grpc/grpc/issues/34998&#34;&gt;https://github.com/grpc/grpc/issues/34998&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;And as mentioned before, you can&#39;t run it on a remote server unless the web page has https. That&#39;s because the web mic on the front end will only launch in a secure context (which means localhost or https environment only).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most of the asr and tts will be pre-installed. However, bark TTS and the original OpenAI Whisper (&lt;code&gt;Whisper&lt;/code&gt;, not WhisperCPP) are NOT included in the default build process because they are huge (~8GB, which makes the whole container about 25GB). In addition, they don&#39;t deliver the best performance either. To include bark and/or whisper in the image, add the argument &lt;code&gt;--build-arg INSTALL_ORIGINAL_WHISPER=true --build-arg INSTALL_BARK=true&lt;/code&gt; to the image build command.&lt;/p&gt; &#xA;&lt;p&gt;Setup guide:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Review &lt;code&gt;conf.yaml&lt;/code&gt; before building (currently burned into the image, I&#39;m sorry):&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the image:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t open-llm-vtuber .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Grab a drink, this will take a while)&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Grab a &lt;code&gt;conf.yaml&lt;/code&gt; configuration file. Grab a &lt;code&gt;conf.yaml&lt;/code&gt; file from this repo. Or you can get it directly from this &lt;a href=&#34;https://raw.githubusercontent.com/t41372/Open-LLM-VTuber/main/conf.yaml&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the container:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;$(pwd)/conf.yaml&lt;/code&gt; should be the path of your &lt;code&gt;conf.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -it --net=host --rm -v $(pwd)/conf.yaml:/app/conf.yaml -p 12393:12393 open-llm-vtuber&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Open localhost:12393 to test&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Related Projects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ylxmf2005/LLM-Live2D-Desktop-Assitant&#34;&gt;ylxmf2005/LLM-Live2D-Desktop-Assitant&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Your Live2D desktop assistant powered by LLM! Available for both Windows and MacOS, it senses your screen, retrieves clipboard content, and responds to voice commands with a unique voice. Featuring voice wake-up, singing capabilities, and full computer control for seamless interaction with your favorite character.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;(this project is in the active prototyping stage, so many things will change)&lt;/p&gt; &#xA;&lt;p&gt;Some abbreviations used in this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM: Large Language Model&lt;/li&gt; &#xA; &lt;li&gt;TTS: Text-to-speech, Speech Synthesis, Voice Synthesis&lt;/li&gt; &#xA; &lt;li&gt;ASR: Automatic Speech Recognition, Speech recognition, Speech to text, STT&lt;/li&gt; &#xA; &lt;li&gt;VAD: Voice Activation Detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Regarding sample rates&lt;/h3&gt; &#xA;&lt;p&gt;You can assume that the sample rate is &lt;code&gt;16000&lt;/code&gt; throughout this project. The frontend stream chunks of &lt;code&gt;Float32Array&lt;/code&gt; with a sample rate of &lt;code&gt;16000&lt;/code&gt; to the backend.&lt;/p&gt; &#xA;&lt;h3&gt;Add support for new TTS providers&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Implement &lt;code&gt;TTSInterface&lt;/code&gt; defined in &lt;code&gt;tts/tts_interface.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add your new TTS provider into &lt;code&gt;tts_factory&lt;/code&gt;: the factory to instantiate and return the tts instance.&lt;/li&gt; &#xA; &lt;li&gt;Add configuration to &lt;code&gt;conf.yaml&lt;/code&gt;. The dict with the same name will be passed into the constructor of your TTSEngine as kwargs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Add support for new Speech Recognition provider&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Implement &lt;code&gt;ASRInterface&lt;/code&gt; defined in &lt;code&gt;asr/asr_interface.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add your new ASR provider into &lt;code&gt;asr_factory&lt;/code&gt;: the factory to instantiate and return the ASR instance.&lt;/li&gt; &#xA; &lt;li&gt;Add configuration to &lt;code&gt;conf.yaml&lt;/code&gt;. The dict with the same name will be passed into the constructor of your class as kwargs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Add support for new LLM provider&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Implement &lt;code&gt;LLMInterface&lt;/code&gt; defined in &lt;code&gt;llm/llm_interface.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add your new LLM provider into &lt;code&gt;llm_factory&lt;/code&gt;: the factory to instantiate and return the LLM instance.&lt;/li&gt; &#xA; &lt;li&gt;Add configuration to &lt;code&gt;conf.yaml&lt;/code&gt;. The dict with the same name will be passed into the constructor of your class as kwargs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;Awesome projects I learned from&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dnhkng/GlaDOS&#34;&gt;https://github.com/dnhkng/GlaDOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SchwabischesBauernbrot/unsuperior-ai-waifu&#34;&gt;https://github.com/SchwabischesBauernbrot/unsuperior-ai-waifu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codepen.io/guansss/pen/oNzoNoz&#34;&gt;https://codepen.io/guansss/pen/oNzoNoz&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ikaros-521/AI-Vtuber&#34;&gt;https://github.com/Ikaros-521/AI-Vtuber&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zixiiu/Digital_Life_Server&#34;&gt;https://github.com/zixiiu/Digital_Life_Server&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#t41372/open-llm-vtuber&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=t41372/open-llm-vtuber&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>