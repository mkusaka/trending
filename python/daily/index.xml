<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-10T01:40:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>laixintao/flameshow</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/laixintao/flameshow</id>
    <link href="https://github.com/laixintao/flameshow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A terminal Flamegraph viewer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Flameshow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/laixintao/flameshow/actions/workflows/pytest.yaml&#34;&gt;&lt;img src=&#34;https://github.com/laixintao/flameshow/actions/workflows/pytest.yaml/badge.svg?branch=main&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/laixintao/flameshow&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/laixintao/flameshow/graph/badge.svg?token=XQCGN9GBL4&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/flameshow/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/flameshow.svg?logo=pypi&amp;amp;label=PyPI&amp;amp;logoColor=gold&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/flameshow?logo=python&amp;amp;logoColor=gold&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/flameshow&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Flameshow is a terminal Flamegraph viewer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/laixintao/flameshow/main/docs/flameshow.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Renders Flamegraphs in your terminal&lt;/li&gt; &#xA; &lt;li&gt;Supports zooming in and displaying percentages&lt;/li&gt; &#xA; &lt;li&gt;Keyboard input is prioritized&lt;/li&gt; &#xA; &lt;li&gt;All operations can also be performed using the mouse.&lt;/li&gt; &#xA; &lt;li&gt;Can switch to different sample types&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Flameshow is written in pure Python, so you can install via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install flameshow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But you can also run it through &lt;a href=&#34;https://nixos.org/&#34;&gt;nix&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nix run github:laixintao/flameshow&#xA;# Or if you want to install it imperatively:&#xA;nix profile install github:laixintao/flameshow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;View golang&#39;s goroutine dump:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl http://localhost:9100/debug/pprof/goroutine -o goroutine.out&#xA;$ flameshow goroutine.out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After entering the TUI, the available actions are listed on Footer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;kbd&gt;q&lt;/kbd&gt; for quit&lt;/li&gt; &#xA; &lt;li&gt;&lt;kbd&gt;j&lt;/kbd&gt; &lt;kbd&gt;i&lt;/kbd&gt; &lt;kbd&gt;j&lt;/kbd&gt; &lt;kbd&gt;k&lt;/kbd&gt; or &lt;kbd&gt;←&lt;/kbd&gt; &lt;kbd&gt;↓&lt;/kbd&gt; &lt;kbd&gt;↑&lt;/kbd&gt; &lt;kbd&gt;→&lt;/kbd&gt; for moving around, and &lt;kbd&gt;Enter&lt;/kbd&gt; for zoom in, then &lt;kbd&gt;Esc&lt;/kbd&gt; for zoom out.&lt;/li&gt; &#xA; &lt;li&gt;You can also use a mouse, hover on a span will show it details, and click will zoom it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Formats&lt;/h2&gt; &#xA;&lt;p&gt;As far as I know, there is no standard specification for profiles. Different languages or tools might generate varying profile formats. I&#39;m actively working on supporting more formats. Admittedly, I might not be familiar with every tool and its specific format. So, if you&#39;d like Flameshow to integrate with a tool you love, please feel free to reach out and submit an issue.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Golang pprof&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.brendangregg.com/flamegraphs.html&#34;&gt;Brendan Gregg&#39;s Flamegraph&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to dive into the code and make some changes, start with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:laixintao/flameshow.git&#xA;cd flameshow&#xA;pip install poetry&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This project is proudly powered by &lt;a href=&#34;https://github.com/Textualize/textual&#34;&gt;textual&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jianchang512/stt</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/jianchang512/stt</id>
    <link href="https://github.com/jianchang512/stt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Voice Recognition to Text Tool / 一个离线运行的本地语音识别转文字服务，输出json、srt字幕带时间戳、纯文字格式&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/stt/main/README_EN.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://discord.gg/TMCM2PfHzQ&#34;&gt;Discord&lt;/a&gt; / Q群 902124277&lt;/p&gt; &#xA;&lt;h1&gt;语音识别转文字工具&lt;/h1&gt; &#xA;&lt;p&gt;这是一个离线运行的本地语音识别转文字工具，基于 fast-whipser 开源模型，可将视频/音频中的人类声音识别并转为文字，可输出json格式、srt字幕带时间戳格式、纯文字格式。可用于自行部署后替代 openai 的语音识别接口或百度语音识别等，准确率基本等同openai官方api接口。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;部署或下载后，双击 start.exe 自动调用本地浏览器打开本地网页。&lt;/p&gt; &#xA; &lt;p&gt;拖拽或点击选择要识别的音频视频文件，然后选择发声语言、输出文字格式、所用模型(已内置base模型),点击开始识别，识别完成后以所选格式输出在当前网页。&lt;/p&gt; &#xA; &lt;p&gt;全过程无需联网，完全本地运行，可部署于内网&lt;/p&gt; &#xA; &lt;p&gt;fast-whisper 开源模型有 base/small/medium/large-v3, 内置base模型，base-&amp;gt;large-v3识别效果越来越好，但所需计算机资源也更多，根据需要可自行下载后解压到 models 目录下即可。&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/tag/0.0&#34;&gt;全部模型下载地址&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;视频演示&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/assets/3378335/d716acb6-c20c-4174-9620-f574a7ff095d&#34;&gt;https://github.com/jianchang512/stt/assets/3378335/d716acb6-c20c-4174-9620-f574a7ff095d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/stt/assets/3378335/0f724ff1-21b3-4960-b6ba-5aa994ea414c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;预编译Win版使用方法/Linux和Mac源码部署&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases&#34;&gt;点击此处打开Releases页面下载&lt;/a&gt;预编译文件&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;下载后解压到某处，比如 E:/stt&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;双击 start.exe ，等待自动打开浏览器窗口即可&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;点击页面中的上传区域，在弹窗中找到想识别的音频或视频文件，或直接拖拽音频视频文件到上传区域，然后选择发生语言、文本输出格式、所用模型，点击“立即开始识别”，稍等片刻，底部文本框中会以所选格式显示识别结果&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果机器拥有英伟达GPU，并正确配置了CUDA环境，将自动使用CUDA加速&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;源码部署(Linux/Mac/Window)&lt;/h1&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;要求 python 3.9-&amp;gt;3.11&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建空目录，比如 E:/stt, 在这个目录下打开 cmd 窗口，方法是地址栏中输入 &lt;code&gt;cmd&lt;/code&gt;, 然后回车。&lt;/p&gt; &lt;p&gt;使用git拉取源码到当前目录 &lt;code&gt;git clone git@github.com:jianchang512/stt.git .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建虚拟环境 &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;激活环境，win下命令 &lt;code&gt;%cd%/venv/scripts/activate&lt;/code&gt;，linux和Mac下命令 &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装依赖: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;,如果报版本冲突错误，请执行 &lt;code&gt;pip install -r requirements.txt --no-deps&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;win下解压 ffmpeg.7z，将其中的&lt;code&gt;ffmpeg.exe&lt;/code&gt;和&lt;code&gt;ffprobe.exe&lt;/code&gt;放在项目目录下, linux和mac 到 &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpeg官网&lt;/a&gt;下载对应版本ffmpeg，解压其中的&lt;code&gt;ffmpeg&lt;/code&gt;和&lt;code&gt;ffprobe&lt;/code&gt;二进制程序放到项目根目录下&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/tag/0.0&#34;&gt;下载模型压缩包&lt;/a&gt;，根据需要下载模型，下载后将压缩包里的文件夹放到项目根目录的 models 文件夹内&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;执行 &lt;code&gt;python start.py &lt;/code&gt;，等待自动打开本地浏览器窗口。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;注意：&lt;/h1&gt; &#xA;&lt;p&gt;有时会遇到“cublasxx.dll不存在”的错误，此时需要下载 cuBLAS，然后将dll文件复制到系统目录下&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt/releases/download/0.0/cuBLAS_win.7z&#34;&gt;点击下载 cuBLAS&lt;/a&gt;，解压后将里面的dll文件复制到 C:/Windows/System32下&lt;/p&gt; &#xA;&lt;h1&gt;api接口&lt;/h1&gt; &#xA;&lt;p&gt;接口地址: &lt;a href=&#34;http://127.0.0.1:9977/api&#34;&gt;http://127.0.0.1:9977/api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;请求方法: POST&lt;/p&gt; &#xA;&lt;p&gt;请求参数:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;language: 语言代码:可选如下&#xA;&#xA;&amp;gt;&#xA;&amp;gt; 中文：zh&#xA;&amp;gt; 英语：en&#xA;&amp;gt; 法语：fr&#xA;&amp;gt; 德语：de&#xA;&amp;gt; 日语：ja&#xA;&amp;gt; 韩语：ko&#xA;&amp;gt; 俄语：ru&#xA;&amp;gt; 西班牙语：es&#xA;&amp;gt; 泰国语：th&#xA;&amp;gt; 意大利语：it&#xA;&amp;gt; 葡萄牙语：pt&#xA;&amp;gt; 越南语：vi&#xA;&amp;gt; 阿拉伯语：ar&#xA;&amp;gt; 土耳其语：tr&#xA;&amp;gt;&#xA;&#xA;model: 模型名称，可选如下&#xA;&amp;gt;&#xA;&amp;gt; base 对应于 models/models--Systran--faster-whisper-base&#xA;&amp;gt; small 对应于 models/models--Systran--faster-whisper-small&#xA;&amp;gt; medium 对应于 models/models--Systran--faster-whisper-medium&#xA;&amp;gt; large-v3 对应于 models/models--Systran--faster-whisper-large-v3&#xA;&amp;gt;&#xA;&#xA;response_format: 返回的字幕格式，可选 text|json|srt&#xA;&#xA;file: 音视频文件，二进制上传&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Api 请求示例&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    import requests&#xA;    # 请求地址&#xA;    url = &#34;http://127.0.0.1:9977/api&#34;&#xA;    # 请求参数  file:音视频文件，language：语言代码，model：模型，response_format:text|json|srt&#xA;    # 返回 code==0 成功，其他失败，msg==成功为ok，其他失败原因，data=识别后返回文字&#xA;    files = {&#34;file&#34;: open(&#34;C:\\Users\\c1\\Videos\\2.wav&#34;, &#34;rb&#34;)}&#xA;    data={&#34;language&#34;:&#34;zh&#34;,&#34;model&#34;:&#34;base&#34;,&#34;response_format&#34;:&#34;json&#34;}&#xA;    response = requests.request(&#34;POST&#34;, url, timeout=600, data=data,files=files)&#xA;    print(response.json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;CUDA 加速支持&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;安装CUDA工具&lt;/strong&gt; &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;详细安装方法&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你的电脑拥有 Nvidia 显卡，先升级显卡驱动到最新，然后去安装对应的 &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA Toolkit 11.8&lt;/a&gt; 和 &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-archive&#34;&gt;cudnn for CUDA11.X&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;安装完成成，按&lt;code&gt;Win + R&lt;/code&gt;,输入 &lt;code&gt;cmd&lt;/code&gt;然后回车，在弹出的窗口中输入&lt;code&gt;nvcc --version&lt;/code&gt;,确认有版本信息显示，类似该图 &lt;img src=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/e68de07f-4bb1-4fc9-bccd-8f841825915a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;然后继续输入&lt;code&gt;nvidia-smi&lt;/code&gt;,确认有输出信息，并且能看到cuda版本号，类似该图 &lt;img src=&#34;https://github.com/jianchang512/pyvideotrans/assets/3378335/71f1d7d3-07f9-4579-b310-39284734006b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;然后执行 `python testcuda.py`，如果提示成功，说明安装正确，否则请仔细检查重新安装&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;注意事项&lt;/h1&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;如果没有英伟达显卡或未配置好CUDA环境，不要使用 large/large-v3 模型，可能导致内存耗尽死机&lt;/li&gt; &#xA; &lt;li&gt;中文在某些情况下会输出繁体字&lt;/li&gt; &#xA; &lt;li&gt;有时会遇到“cublasxx.dll不存在”的错误，此时需要下载 cuBLAS，然后将dll文件复制到系统目录下，&lt;a href=&#34;https://github.com/jianchang512/stt/releases/download/0.0/cuBLAS_win.7z&#34;&gt;点击下载 cuBLAS&lt;/a&gt;，解压后将里面的dll文件复制到 C:/Windows/System32下&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;相关联项目&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/pyvideotrans&#34;&gt;视频翻译配音工具:翻译字幕并配音&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/clone-voice&#34;&gt;声音克隆工具:用任意音色合成语音&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/stt&#34;&gt;人声背景乐分离:极简的人声和背景音乐分离工具，本地化网页操作&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;致谢&lt;/h1&gt; &#xA;&lt;p&gt;本项目主要依赖的其他项目&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whipser&#34;&gt;https://github.com/openai/whipser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pallets/flask&#34;&gt;https://github.com/pallets/flask&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://layui.dev&#34;&gt;https://layui.dev&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>netease-youdao/BCEmbedding</title>
    <updated>2024-01-10T01:40:12Z</updated>
    <id>tag:github.com,2024-01-10:/netease-youdao/BCEmbedding</id>
    <link href="https://github.com/netease-youdao/BCEmbedding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Netease Youdao&#39;s open-source embedding and reranker models for RAG products.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;BCEmbedding: Bilingual and Crosslingual Embedding for RAG&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong style=&#34;background-color: green;&#34;&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/README_zh.md&#34; target=&#34;_Self&#34;&gt;简体中文&lt;/a&gt; &lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Click to Open Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-bilingual-and-crosslingual-superiority&#34; target=&#34;_Self&#34;&gt;🌐 Bilingual and Crosslingual Superiority&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-key-features&#34; target=&#34;_Self&#34;&gt;💡 Key Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-latest-updates&#34; target=&#34;_Self&#34;&gt;🚀 Latest Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-model-list&#34; target=&#34;_Self&#34;&gt;🍎 Model List&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-manual&#34; target=&#34;_Self&#34;&gt;📖 Manual&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#installation&#34; target=&#34;_Self&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#quick-start&#34; target=&#34;_Self&#34;&gt;Quick Start (&lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;sentence-transformers&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#integrations-for-rag-frameworks&#34; target=&#34;_Self&#34;&gt;Integrations for RAG Frameworks (&lt;code&gt;langchain&lt;/code&gt;, &lt;code&gt;llama_index&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#%EF%B8%8F-evaluation&#34; target=&#34;_Self&#34;&gt;⚙️ Evaluation&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#evaluate-semantic-representation-by-mteb&#34; target=&#34;_Self&#34;&gt;Evaluate Semantic Representation by MTEB&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#evaluate-rag-by-llamaindex&#34; target=&#34;_Self&#34;&gt;Evaluate RAG by LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-leaderboard&#34; target=&#34;_Self&#34;&gt;📈 Leaderboard&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#semantic-representation-evaluations-in-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-youdaos-bcembedding-api&#34; target=&#34;_Self&#34;&gt;🛠 Youdao&#39;s BCEmbedding API&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-wechat-group&#34; target=&#34;_Self&#34;&gt;🧲 WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#%EF%B8%8F-citation&#34; target=&#34;_Self&#34;&gt;✏️ Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-license&#34; target=&#34;_Self&#34;&gt;🔐 License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#-related-links&#34; target=&#34;_Self&#34;&gt;🔗 Related Links&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;ilingual and &lt;strong&gt;C&lt;/strong&gt;rosslingual &lt;strong&gt;Embedding&lt;/strong&gt; (&lt;code&gt;BCEmbedding&lt;/code&gt;), developed by NetEase Youdao, encompasses &lt;code&gt;EmbeddingModel&lt;/code&gt; and &lt;code&gt;RerankerModel&lt;/code&gt;. The &lt;code&gt;EmbeddingModel&lt;/code&gt; specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the &lt;code&gt;RerankerModel&lt;/code&gt; excels at refining search results and ranking tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;BCEmbedding&lt;/code&gt; serves as the cornerstone of Youdao&#39;s Retrieval Augmented Generation (RAG) implmentation, notably &lt;a href=&#34;http://qanything.ai&#34;&gt;QAnything&lt;/a&gt; [&lt;a href=&#34;https://github.com/netease-youdao/qanything&#34;&gt;github&lt;/a&gt;], an open-source implementation widely integrated in various Youdao products like &lt;a href=&#34;https://read.youdao.com/#/home&#34;&gt;Youdao Speed Reading&lt;/a&gt; and &lt;a href=&#34;https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation&#34;&gt;Youdao Translation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Distinguished for its bilingual and crosslingual proficiency, &lt;code&gt;BCEmbedding&lt;/code&gt; excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A high performence on &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#semantic-representation-evaluations-in-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌐 Bilingual and Crosslingual Superiority&lt;/h2&gt; &#xA;&lt;p&gt;Existing embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. &lt;code&gt;BCEmbedding&lt;/code&gt;, leveraging the strength of Youdao&#39;s translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;EmbeddingModel&lt;/code&gt; supports &lt;em&gt;&lt;strong&gt;Chinese (ch) and English (en)&lt;/strong&gt;&lt;/em&gt; (more languages support will come soon), while &lt;code&gt;RerankerModel&lt;/code&gt; supports &lt;em&gt;&lt;strong&gt;Chinese (ch), English (en), Japanese (ja) and Korean (ko)&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💡 Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bilingual and Crosslingual Proficiency&lt;/strong&gt;: Powered by Youdao&#39;s translation engine, excelling in Chinese, English and their crosslingual retrieval task, with upcoming support for additional languages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RAG-Optimized&lt;/strong&gt;: Tailored for diverse RAG tasks including &lt;strong&gt;translation, summarization, and question answering&lt;/strong&gt;, ensuring accurate &lt;strong&gt;query understanding&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#rag-evaluations-in-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient and Precise Retrieval&lt;/strong&gt;: Dual-encoder for efficient retrieval of &lt;code&gt;EmbeddingModel&lt;/code&gt; in first stage, and cross-encoder of &lt;code&gt;RerankerModel&lt;/code&gt; for enhanced precision and deeper semantic analysis in second stage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad Domain Adaptability&lt;/strong&gt;: Trained on diverse datasets for superior performance across various fields.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-Friendly Design&lt;/strong&gt;: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meaningful Reranking Scores&lt;/strong&gt;: &lt;code&gt;RerankerModel&lt;/code&gt; provides relevant scores to improve result quality and optimize large language model performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Proven in Production&lt;/strong&gt;: Successfully implemented and validated in Youdao&#39;s products.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Model Releases&lt;/strong&gt; - &lt;a href=&#34;https://huggingface.co/maidalun1020/bce-embedding-base_v1&#34;&gt;bce-embedding-base_v1&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/maidalun1020/bce-reranker-base_v1&#34;&gt;bce-reranker-base_v1&lt;/a&gt; are available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Eval Datasets&lt;/strong&gt; [&lt;a href=&#34;https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset&#34;&gt;CrosslingualMultiDomainsDataset&lt;/a&gt;] - Evaluate the performence of RAG, using &lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Eval Datasets&lt;/strong&gt; [&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/evaluation/c_mteb/Retrieval.py&#34;&gt;Details&lt;/a&gt;] - Evaluate the performence of crosslingual semantic representation, using &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🍎 Model List&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Languages&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bce-embedding-base_v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;EmbeddingModel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ch, en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;279M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/maidalun1020/bce-embedding-base_v1&#34;&gt;Huggingface&lt;/a&gt;, &lt;a href=&#34;https://www.modelscope.cn/models/maidalun/bce-embedding-base_v1/summary&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bce-reranker-base_v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;RerankerModel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ch, en, ja, ko&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;279M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/maidalun1020/bce-reranker-base_v1&#34;&gt;Huggingface&lt;/a&gt;, &lt;a href=&#34;https://www.modelscope.cn/models/maidalun/bce-reranker-base_v1/summary&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📖 Manual&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;First, create a conda environment and activate it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name bce python=3.10 -y&#xA;conda activate bce&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install &lt;code&gt;BCEmbedding&lt;/code&gt; for minimal installation (To avoid cuda version conflicting, you should install &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/a&gt; that is compatible to your system cuda version manually first):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install BCEmbedding==0.1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:netease-youdao/BCEmbedding.git&#xA;cd BCEmbedding&#xA;pip install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;h4&gt;1. Based on &lt;code&gt;BCEmbedding&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Use &lt;code&gt;EmbeddingModel&lt;/code&gt;, and &lt;code&gt;cls&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/models/embedding.py#L24&#34;&gt;pooler&lt;/a&gt; is default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from BCEmbedding import EmbeddingModel&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init embedding model&#xA;model = EmbeddingModel(model_name_or_path=&#34;maidalun1020/bce-embedding-base_v1&#34;)&#xA;&#xA;# extract embeddings&#xA;embeddings = model.encode(sentences)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;RerankerModel&lt;/code&gt; to calculate relevant scores and rerank:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from BCEmbedding import RerankerModel&#xA;&#xA;# your query and corresponding passages&#xA;query = &#39;input_query&#39;&#xA;passages = [&#39;passage_0&#39;, &#39;passage_1&#39;, ...]&#xA;&#xA;# construct sentence pairs&#xA;sentence_pairs = [[query, passage] for passage in passages]&#xA;&#xA;# init reranker model&#xA;model = RerankerModel(model_name_or_path=&#34;maidalun1020/bce-reranker-base_v1&#34;)&#xA;&#xA;# method 0: calculate scores of sentence pairs&#xA;scores = model.compute_score(sentence_pairs)&#xA;&#xA;# method 1: rerank passages&#xA;rerank_results = model.rerank(query, passages)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/models/reranker.py#L137&#34;&gt;&lt;code&gt;RerankerModel.rerank&lt;/code&gt;&lt;/a&gt; method, we provide an advanced preproccess that we use in production for making &lt;code&gt;sentence_pairs&lt;/code&gt;, when &#34;passages&#34; are very long.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Based on &lt;code&gt;transformers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;EmbeddingModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init model and tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;maidalun1020/bce-embedding-base_v1&#39;)&#xA;model = AutoModel.from_pretrained(&#39;maidalun1020/bce-embedding-base_v1&#39;)&#xA;&#xA;device = &#39;cuda&#39;  # if no GPU, set &#34;cpu&#34;&#xA;model.to(device)&#xA;&#xA;# get inputs&#xA;inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=&#34;pt&#34;)&#xA;inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}&#xA;&#xA;# get embeddings&#xA;outputs = model(**inputs_on_device, return_dict=True)&#xA;embeddings = outputs.last_hidden_state[:, 0]  # cls pooler&#xA;embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;RerankerModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForSequenceClassification&#xA;&#xA;# init model and tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;maidalun1020/bce-reranker-base_v1&#39;)&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#39;maidalun1020/bce-reranker-base_v1&#39;)&#xA;&#xA;device = &#39;cuda&#39;  # if no GPU, set &#34;cpu&#34;&#xA;model.to(device)&#xA;&#xA;# get inputs&#xA;inputs = tokenizer(sentence_pairs, padding=True, truncation=True, max_length=512, return_tensors=&#34;pt&#34;)&#xA;inputs_on_device = {k: v.to(device) for k, v in inputs.items()}&#xA;&#xA;# calculate scores&#xA;scores = model(**inputs_on_device, return_dict=True).logits.view(-1,).float()&#xA;scores = torch.sigmoid(scores)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Based on &lt;code&gt;sentence_transformers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;EmbeddingModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import SentenceTransformer&#xA;&#xA;# list of sentences&#xA;sentences = [&#39;sentence_0&#39;, &#39;sentence_1&#39;, ...]&#xA;&#xA;# init embedding model&#xA;## New update for sentence-trnasformers. So clean up your &#34;`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1&#34; or &#34;～/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1&#34; first for downloading new version.&#xA;model = SentenceTransformer(&#34;maidalun1020/bce-embedding-base_v1&#34;)&#xA;&#xA;# extract embeddings&#xA;embeddings = model.encode(sentences, normalize_embeddings=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;RerankerModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import CrossEncoder&#xA;&#xA;# init reranker model&#xA;model = CrossEncoder(&#39;maidalun1020/bce-reranker-base_v1&#39;, max_length=512)&#xA;&#xA;# calculate scores of sentence pairs&#xA;scores = model.predict(sentence_pairs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Integrations for RAG Frameworks&lt;/h3&gt; &#xA;&lt;h4&gt;1. Used in &lt;code&gt;langchain&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.embeddings import HuggingFaceEmbeddings&#xA;from langchain_community.vectorstores import FAISS&#xA;from langchain_community.vectorstores.utils import DistanceStrategy&#xA;&#xA;query = &#39;apples&#39;&#xA;passages = [&#xA;        &#39;I like apples&#39;, &#xA;        &#39;I like oranges&#39;, &#xA;        &#39;Apples and oranges are fruits&#39;&#xA;    ]&#xA;  &#xA;# init embedding model&#xA;model_name = &#39;maidalun1020/bce-embedding-base_v1&#39;&#xA;model_kwargs = {&#39;device&#39;: &#39;cuda&#39;}&#xA;encode_kwargs = {&#39;batch_size&#39;: 64, &#39;normalize_embeddings&#39;: True, &#39;show_progress_bar&#39;: False}&#xA;&#xA;embed_model = HuggingFaceEmbeddings(&#xA;    model_name=model_name,&#xA;    model_kwargs=model_kwargs,&#xA;    encode_kwargs=encode_kwargs&#xA;  )&#xA;&#xA;# example #1. extract embeddings&#xA;query_embedding = embed_model.embed_query(query)&#xA;passages_embeddings = embed_model.embed_documents(passages)&#xA;&#xA;# example #2. langchain retriever example&#xA;faiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)&#xA;&#xA;retriever = faiss_vectorstore.as_retriever(search_type=&#34;similarity&#34;, search_kwargs={&#34;score_threshold&#34;: 0.5, &#34;k&#34;: 3})&#xA;&#xA;related_passages = retriever.get_relevant_documents(query)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Used in &lt;code&gt;llama_index&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.embeddings import HuggingFaceEmbedding&#xA;from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader&#xA;from llama_index.node_parser import SimpleNodeParser&#xA;from llama_index.llms import OpenAI&#xA;&#xA;query = &#39;apples&#39;&#xA;passages = [&#xA;        &#39;I like apples&#39;, &#xA;        &#39;I like oranges&#39;, &#xA;        &#39;Apples and oranges are fruits&#39;&#xA;    ]&#xA;&#xA;# init embedding model&#xA;model_args = {&#39;model_name&#39;: &#39;maidalun1020/bce-embedding-base_v1&#39;, &#39;max_length&#39;: 512, &#39;embed_batch_size&#39;: 64, &#39;device&#39;: &#39;cuda&#39;}&#xA;embed_model = HuggingFaceEmbedding(**model_args)&#xA;&#xA;# example #1. extract embeddings&#xA;query_embedding = embed_model.get_query_embedding(query)&#xA;passages_embeddings = embed_model.get_text_embedding_batch(passages)&#xA;&#xA;# example #2. rag example&#xA;llm = OpenAI(model=&#39;gpt-3.5-turbo-0613&#39;, api_key=os.environ.get(&#39;OPENAI_API_KEY&#39;), api_base=os.environ.get(&#39;OPENAI_BASE_URL&#39;))&#xA;service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)&#xA;&#xA;documents = SimpleDirectoryReader(input_files=[&#34;BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf&#34;]).load_data()&#xA;node_parser = SimpleNodeParser.from_defaults(chunk_size=512)&#xA;nodes = node_parser.get_nodes_from_documents(documents[0:36])&#xA;index = VectorStoreIndex(nodes, service_context=service_context)&#xA;query_engine = index.as_query_engine()&#xA;response = query_engine.query(&#34;What is llama?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;⚙️ Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluate Semantic Representation by MTEB&lt;/h3&gt; &#xA;&lt;p&gt;We provide evaluation tools for &lt;code&gt;embedding&lt;/code&gt; and &lt;code&gt;reranker&lt;/code&gt; models, based on &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C_MTEB&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, install &lt;code&gt;MTEB&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install mteb==1.1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;1. Embedding Models&lt;/h4&gt; &#xA;&lt;p&gt;Just run following cmd to evaluate &lt;code&gt;your_embedding_model&lt;/code&gt; (e.g. &lt;code&gt;maidalun1020/bce-embedding-base_v1&lt;/code&gt;) in &lt;strong&gt;bilingual and crosslingual settings&lt;/strong&gt; (e.g. &lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The total evaluation tasks contain &lt;em&gt;&lt;strong&gt;114 datastes&lt;/strong&gt;&lt;/em&gt; of &lt;strong&gt;&#34;Retrieval&#34;, &#34;STS&#34;, &#34;PairClassification&#34;, &#34;Classification&#34;, &#34;Reranking&#34; and &#34;Clustering&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;All models are evaluated in their recommended pooling method (&lt;code&gt;pooler&lt;/code&gt;)&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;mean&lt;/code&gt; pooler: &#34;jina-embeddings-v2-base-en&#34;, &#34;m3e-base&#34;, &#34;m3e-large&#34;, &#34;e5-large-v2&#34;, &#34;multilingual-e5-base&#34;, &#34;multilingual-e5-large&#34; and &#34;gte-large&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cls&lt;/code&gt; pooler: Other models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&#34;jina-embeddings-v2-base-en&#34; model should be loaded with &lt;code&gt;trust_remote_code&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path {mean_pooler_models} --pooler mean&#xA;&#xA;python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Reranker Models&lt;/h4&gt; &#xA;&lt;p&gt;Run following cmd to evaluate &lt;code&gt;your_reranker_model&lt;/code&gt; (e.g. &#34;maidalun1020/bce-reranker-base_v1&#34;) in &lt;strong&gt;bilingual and crosslingual settings&lt;/strong&gt; (e.g. &lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The evaluation tasks contain &lt;em&gt;&lt;strong&gt;12 datastes&lt;/strong&gt;&lt;/em&gt; of &lt;strong&gt;&#34;Reranking&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;3. Metrics Visualization Tool&lt;/h4&gt; &#xA;&lt;p&gt;We proveide a one-click script to sumarize evaluation results of &lt;code&gt;embedding&lt;/code&gt; and &lt;code&gt;reranker&lt;/code&gt; models as &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluation Summary&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluation Summary&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluate RAG by LlamaIndex&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; is a famous data framework for LLM-based applications, particularly in RAG. Recently, a &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt; has evaluated the popular embedding and reranker models in RAG pipeline and attracts great attention. Now, we follow its pipeline to evaluate our &lt;code&gt;BCEmbedding&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, install LlamaIndex, and upgrade &lt;code&gt;transformers&lt;/code&gt; to 4.36.0:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers==4.36.0&#xA;&#xA;pip install llama-index==0.9.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Export your &#34;openai&#34; and &#34;cohere&#34; app keys, and openai base url (e.g. &#34;&lt;a href=&#34;https://api.openai.com/v1&#34;&gt;https://api.openai.com/v1&lt;/a&gt;&#34;) to env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_BASE_URL={openai_base_url}  # https://api.openai.com/v1&#xA;export OPENAI_API_KEY={your_openai_api_key}&#xA;export COHERE_APPKEY={your_cohere_api_key}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;1. Metrics Definition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Hit Rate:&lt;/p&gt; &lt;p&gt;Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it&#39;s about how often our system gets it right within the top few guesses. &lt;em&gt;&lt;strong&gt;The larger, the better.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mean Reciprocal Rank (MRR):&lt;/p&gt; &lt;p&gt;For each query, MRR evaluates the system&#39;s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it&#39;s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it&#39;s second, the reciprocal rank is 1/2, and so on. &lt;em&gt;&lt;strong&gt;The larger, the better.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Reproduce &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;In order to compare our &lt;code&gt;BCEmbedding&lt;/code&gt; with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our &lt;code&gt;BCEmbedding&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# There should be two GPUs available at least.&#xA;CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, sumarize the evaluation results by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_reproduce_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results Reproduced from the LlamaIndex Blog can be checked in &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md&#34;&gt;Reproduced Summary of RAG Evaluation&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;, with some obvious &lt;em&gt;&lt;strong&gt;conclusions&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. Broad Domain Adaptability&lt;/h4&gt; &#xA;&lt;p&gt;The evaluation of &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt; is &lt;strong&gt;monolingual, small amount of data, and specific domain&lt;/strong&gt; (just including &#34;llama2&#34; paper). In order to evaluate the &lt;strong&gt;broad domain adaptability, bilingual and crosslingual capability&lt;/strong&gt;, we follow the blog to build a multiple domains evaluation dataset (includding &#34;Computer Science&#34;, &#34;Physics&#34;, &#34;Biology&#34;, &#34;Economics&#34;, &#34;Math&#34;, and &#34;Quantitative Finance&#34;. &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/tools/eval_rag/eval_pdfs/&#34;&gt;Details&lt;/a&gt;), named &lt;a href=&#34;https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset&#34;&gt;CrosslingualMultiDomainsDataset&lt;/a&gt;, &lt;strong&gt;by OpenAI &lt;code&gt;gpt-4-1106-preview&lt;/code&gt; for high quality&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, run following cmd to evaluate the most popular and powerful embedding and reranker models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# There should be two GPUs available at least.&#xA;CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the following script to sumarize the evaluation results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir BCEmbedding/results/rag_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The summary of multiple domains evaluations can be seen in &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/#1-multiple-domains-scenarios&#34; target=&#34;_Self&#34;&gt;Multiple Domains Scenarios&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📈 Leaderboard&lt;/h2&gt; &#xA;&lt;h3&gt;Semantic Representation Evaluations in MTEB&lt;/h3&gt; &#xA;&lt;h4&gt;1. Embedding Models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dimensions&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pooler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Instructions&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval (47)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS (19)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification (5)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification (21)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking (12)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering (15)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;AVG&lt;/strong&gt;&lt;/em&gt; (119)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gte-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gte-large-zh&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.51&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;jina-embeddings-v2-base-en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;e5-large-v2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;multilingual-e5-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;multilingual-e5-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Need&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.79&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.61&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.09&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.50&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;cls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Free&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our &lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt; outperforms other opensource embedding models with comparable model sizes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;114 datastes including 119 eval results&lt;/strong&gt;&lt;/em&gt; (some dataset contain multiple languages) of &#34;Retrieval&#34;, &#34;STS&#34;, &#34;PairClassification&#34;, &#34;Classification&#34;, &#34;Reranking&#34; and &#34;Clustering&#34; in &lt;em&gt;&lt;strong&gt;&lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt; setting&lt;/strong&gt;&lt;/em&gt;, including &lt;strong&gt;MTEB and CMTEB&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/BCEmbedding/evaluation/c_mteb/Retrieval.py&#34;&gt;crosslingual evaluation datasets&lt;/a&gt; we released belong to &lt;code&gt;Retrieval&lt;/code&gt; task.&lt;/li&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluations&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Reranker Models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking (12)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;AVG&lt;/strong&gt;&lt;/em&gt; (12)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our &lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt; outperforms other opensource reranker models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;12 datastes&lt;/strong&gt;&lt;/em&gt; of &#34;Reranking&#34; in &lt;em&gt;&lt;strong&gt;&lt;code&gt;[&#34;en&#34;, &#34;zh&#34;, &#34;en-zh&#34;, &#34;zh-en&#34;]&lt;/code&gt; setting&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluations&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG Evaluations in LlamaIndex&lt;/h3&gt; &#xA;&lt;h4&gt;1. Multiple Domains Scenarios&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/assets/rag_eval_multiple_domains_summary.jpg&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consistent with our &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md&#34;&gt;Reproduced Results&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; of &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠 Youdao&#39;s BCEmbedding API&lt;/h2&gt; &#xA;&lt;p&gt;For users who prefer a hassle-free experience without the need to download and configure the model on their own systems, &lt;code&gt;BCEmbedding&lt;/code&gt; is readily accessible through Youdao&#39;s API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at &lt;a href=&#34;https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html&#34;&gt;Youdao BCEmbedding API&lt;/a&gt;. Here, you&#39;ll find all the necessary guidance to easily implement &lt;code&gt;BCEmbedding&lt;/code&gt; across a variety of use cases, ensuring a smooth and effective integration for optimal results.&lt;/p&gt; &#xA;&lt;h2&gt;🧲 WeChat Group&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to scan the QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/Docs/assets/Wechat.jpg&#34; width=&#34;20%&#34; height=&#34;auto&#34;&gt; &#xA;&lt;h2&gt;✏️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;BCEmbedding&lt;/code&gt; in your research or project, please feel free to cite and star it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{youdao_bcembedding_2023,&#xA;    title={BCEmbedding: Bilingual and Crosslingual Embedding for RAG},&#xA;    author={NetEase Youdao, Inc.},&#xA;    year={2023},&#xA;    howpublished={\url{https://github.com/netease-youdao/BCEmbedding}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔐 License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;BCEmbedding&lt;/code&gt; is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/BCEmbedding/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔗 Related Links&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/qanything&#34;&gt;Netease Youdao - QAnything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding&#34;&gt;FlagEmbedding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/embeddings-benchmark/mteb&#34;&gt;MTEB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C_MTEB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LLama Index&lt;/a&gt; | &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;LlamaIndex Blog&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>