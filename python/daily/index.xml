<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-08T01:21:55Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dmunozv04/iSponsorBlockTV</title>
    <updated>2024-05-08T01:21:55Z</updated>
    <id>tag:github.com,2024-05-08:/dmunozv04/iSponsorBlockTV</id>
    <link href="https://github.com/dmunozv04/iSponsorBlockTV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SponsorBlock client for all YouTube TV clients.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;iSponsorBlockTV&lt;/h1&gt; &#xA;&lt;p&gt;Skip sponsor segments in YouTube videos playing on a YouTube TV device (see below for compatibility details).&lt;/p&gt; &#xA;&lt;p&gt;This project is written in asynchronous python and should be pretty quick.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/dmunozv04/iSponsorBlockTV/wiki/Installation&#34;&gt;wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Warning: docker armv7 builds have been deprecated. Amd64 and arm64 builds are still available.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Legend: ‚úÖ = Working, ‚ùå = Not working, ‚ùî = Not tested&lt;/p&gt; &#xA;&lt;p&gt;Open an issue/pull request if you have tested a device that isn&#39;t listed here.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apple TV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Samsung TV (Tizen)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LG TV (WebOS)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Android TV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chromecast&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Google TV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Roku&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fire TV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CCwGTV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Nintendo Switch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Xbox One/Series&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Playstation 4/5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run iSponsorBlockTV on a computer that has network access. Auto discovery will require the computer to be on the same network as the device during setup. The device can also be manually added to iSponsorBlockTV with a YouTube TV code. This code can be found in the settings page of your YouTube application.&lt;/p&gt; &#xA;&lt;p&gt;It connects to the device, watches its activity and skips any sponsor segment using the &lt;a href=&#34;https://sponsor.ajay.app/&#34;&gt;SponsorBlock&lt;/a&gt; API. It can also skip/mute YouTube ads.&lt;/p&gt; &#xA;&lt;h2&gt;Libraries used&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FabioGNR/pyytlounge&#34;&gt;pyytlounge&lt;/a&gt; Used to interact with the device&lt;/li&gt; &#xA; &lt;li&gt;asyncio and &lt;a href=&#34;https://github.com/aio-libs/aiohttp&#34;&gt;aiohttp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamsinghrajat/async-cache&#34;&gt;async-cache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/textualize/textual/&#34;&gt;Textual&lt;/a&gt; Used for the amazing new graphical configurator&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/codingjoe/ssdp&#34;&gt;ssdp&lt;/a&gt; Used for auto discovery&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Projects using this project&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bertybuttface/addons/tree/main/isponsorblocktv&#34;&gt;Home Assistant Addon&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork it (&lt;a href=&#34;https://github.com/dmunozv04/iSponsorBlockTV/fork&#34;&gt;https://github.com/dmunozv04/iSponsorBlockTV/fork&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create your feature branch (&lt;code&gt;git checkout -b my-new-feature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes (&lt;code&gt;git commit -am &#39;Add some feature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin my-new-feature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create a new Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dmunozv04&#34;&gt;dmunozv04&lt;/a&gt; - creator and maintainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HaltCatchFire&#34;&gt;HaltCatchFire&lt;/a&gt; - updated dependencies and improved skip logic&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oxixes&#34;&gt;Oxixes&lt;/a&gt; - added support for channel whitelist and minor improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;&lt;img src=&#34;https://www.gnu.org/graphics/gplv3-127x51.png&#34; alt=&#34;GNU GPLv3&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/RADIO</title>
    <updated>2024-05-08T01:21:55Z</updated>
    <id>tag:github.com,2024-05-08:/NVlabs/RADIO</id>
    <link href="https://github.com/NVlabs/RADIO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for &#34;AM-RADIO: Reduce All Domains Into One&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/RADIO/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVlabs/RADIO.svg?style=social&#34; alt=&#34;Star on GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-NC-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06709&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/paper-arXiv.2312.06709-blue.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06709&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/paper-CVPR.2024-blue.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;[CVPR 2024] AM-RADIO: Agglomerative Vision Foundation Model - Reduce All Domains Into One&lt;/h1&gt; &#xA;&lt;!-- &lt;div align=&#34;left&#34;&gt; --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/assets/radio.png&#34; width=&#34;256&#34; align=&#34;right&#34;&gt; &#xA;&lt;!-- &lt;/div&gt; --&gt; &#xA;&lt;p&gt;Official PyTorch implementation of [CVPR 2024] &lt;a href=&#34;https://arxiv.org/abs/2312.06709&#34;&gt;&lt;strong&gt;AM-RADIO: Agglomerative Vision Foundation Model - Reduce All Domains Into One&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Mike Ranzinger, Greg Heinrich, &lt;a href=&#34;https://jankautz.com/&#34;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/research/&#34;&gt;NVIDIA Research&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2312.06709&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/#citing-radio&#34;&gt;BibTex&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br clear=&#34;left&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;News/Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[4.30.2024] üî• README is updated with more metrics, Arxiv is updated with new results.&lt;/li&gt; &#xA; &lt;li&gt;[3.21.2024] üî• RADIOv2.1 is released. Trained in bf16, improves metrics!&lt;/li&gt; &#xA; &lt;li&gt;[2.26.2024] AM-RADIO paper has been accepted to &lt;strong&gt;CVPR 2024&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2.15.2024] RADIOv2 is released. Trained with DFN CLIP; OpenAI CLIP; DINOv2; SAM teachers. Note that SAM teacher was not used in previous models.&lt;/li&gt; &#xA; &lt;li&gt;[1.5.2024] Initial github repo is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;AM-RADIO is a framework to distill Large Vision Foundation models into a single one. RADIO, a new vision foundation model, excels across visual domains, serving as a superior replacement for vision backbones. Integrating CLIP variants, DINOv2, and SAM through distillation, it preserves unique features like text grounding and segmentation correspondence. Outperforming teachers in ImageNet zero-shot (+6.8%), kNN (+2.39%), and linear probing segmentation (+3.8%) and vision-language models (LLaVa 1.5 up to 1.5%), it scales to any resolution, supports non-square images. We offer an efficient variant, E-RADIO, which achieves is 6-10x faster than CLIP and DINOv2.&lt;/p&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/assets/radio_overview_github.png&#34; width=&#34;768&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Quick start and model versions:&lt;/h2&gt; &#xA;&lt;p&gt;The latest model version is RADIOv2. We will update the description once new model is available.&lt;/p&gt; &#xA;&lt;h3&gt;TorchHub&lt;/h3&gt; &#xA;&lt;p&gt;To load in the TorchHub, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import torch&#xA;model_version=&#34;radio_v2.1&#34; # for RADIO&#xA;#model_version=&#34;e-radio_v2&#34; # for E-RADIO&#xA;model = torch.hub.load(&#39;NVlabs/RADIO&#39;, &#39;radio_model&#39;, version=model_version, progress=True, skip_validation=True)&#xA;model.cuda().eval()&#xA;x = torch.rand(1, 3, 512, 512, device=&#39;cuda&#39;)&#xA;&#xA;if &#34;e-radio&#34; in model_version:&#xA;    model.model.set_optimal_window_size(x.shape[2:]) #where it expects a tuple of (height, width) of the input image.&#xA;&#xA;# RADIO expects the input to have values between [0, 1]. It will automatically normalize them to have mean 0 std 1.&#xA;summary, spatial_features = model(x)&#xA;&#xA;# RADIO also supports running in mixed precision:&#xA;with torch.autocast(&#39;cuda&#39;, dtype=torch.bfloat16):&#xA;    summary, spatial_features = model(x)&#xA;&#xA;# If you&#39;d rather pre-normalize the inputs, then you can do this:&#xA;conditioner = model.make_preprocessor_external()&#xA;&#xA;# Now, the model won&#39;t change the inputs, and it&#39;s up to the user to call `cond_x = conditioner(x)` before&#xA;# calling `model(cond_x)`. You most likely would do this if you want to move the conditioning into your&#xA;# existing data processing pipeline.&#xA;with torch.autocast(&#39;cuda&#39;, dtype=torch.bfloat16):&#xA;    cond_x = conditioner(x)&#xA;    summary, spatial_features = model(cond_x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the previous version, use &lt;code&gt;radio_v1&lt;/code&gt; or &lt;code&gt;eradio_v1&lt;/code&gt; for the E-RADIO model.&lt;/p&gt; &#xA;&lt;h3&gt;HuggingFace (HF)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;&#xA;# hf_repo = &#34;nvidia/E-RADIO&#34; # For E-RADIO.&#xA;hf_repo = &#34;nvidia/RADIO&#34; # For RADIO.&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(hf_repo)&#xA;model = AutoModel.from_pretrained(hf_repo, trust_remote_code=True)&#xA;model.eval().cuda()&#xA;&#xA;image = Image.open(&#39;./assets/radio.png&#39;).convert(&#39;RGB&#39;)&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;, do_resize=True).pixel_values&#xA;pixel_values = pixel_values.cuda()&#xA;&#xA;summary, features = model(pixel_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see more details on usage in the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/#quick-start---torchhub&#34;&gt;Quick Start&lt;/a&gt; section. Information on how to load Adapters (teacher specific heads) is also available in the Quick Start section.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Previously trained models&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Name&lt;/th&gt; &#xA;    &lt;th&gt;Architecture&lt;/th&gt; &#xA;    &lt;th&gt;Precision&lt;/th&gt; &#xA;    &lt;th&gt;Teachers&lt;/th&gt; &#xA;    &lt;th&gt;Throughput&lt;/th&gt; &#xA;    &lt;th&gt;Zero Shot Top-1&lt;/th&gt; &#xA;    &lt;th&gt;kNN Top-1&lt;/th&gt; &#xA;    &lt;th&gt;ADE20k&lt;/th&gt; &#xA;    &lt;th&gt;VOC&lt;/th&gt; &#xA;    &lt;th&gt;GQA&lt;/th&gt; &#xA;    &lt;th&gt;TextVQA&lt;/th&gt; &#xA;    &lt;th&gt;VQAv2&lt;/th&gt; &#xA;    &lt;th&gt;SAM-COCO&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;radio_v2.1&lt;/td&gt; &#xA;    &lt;td&gt;ViT-H/16-CPE&lt;/td&gt; &#xA;    &lt;td&gt;BFloat16&lt;/td&gt; &#xA;    &lt;td&gt;DFN CLIP; OpenAI CLIP; DINOv2; SAM&lt;/td&gt; &#xA;    &lt;td&gt;556&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;82.93&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;86.06&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;51.34&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;84.71&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;63.01&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;56.32&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;79.28&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;76.58&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;radio_v2&lt;/td&gt; &#xA;    &lt;td&gt;ViT-H/16-CPE&lt;/td&gt; &#xA;    &lt;td&gt;Float32&lt;/td&gt; &#xA;    &lt;td&gt;DFN CLIP; OpenAI CLIP; DINOv2; SAM&lt;/td&gt; &#xA;    &lt;td&gt;556&lt;/td&gt; &#xA;    &lt;td&gt;82.71&lt;/td&gt; &#xA;    &lt;td&gt;85.92&lt;/td&gt; &#xA;    &lt;td&gt;51.33&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;62.78&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;56.37&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;79.00&lt;/td&gt; &#xA;    &lt;td&gt;76.21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;radio_v1&lt;/td&gt; &#xA;    &lt;td&gt;ViT-H/14-CPE&lt;/td&gt; &#xA;    &lt;td&gt;Float32&lt;/td&gt; &#xA;    &lt;td&gt;DFN CLIP; OpenAI CLIP; DINOv2&lt;/td&gt; &#xA;    &lt;td&gt;556&lt;/td&gt; &#xA;    &lt;td&gt;82.73&lt;/td&gt; &#xA;    &lt;td&gt;85.29&lt;/td&gt; &#xA;    &lt;td&gt;50.32&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;85.17&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;61.43&lt;/td&gt; &#xA;    &lt;td&gt;54.92&lt;/td&gt; &#xA;    &lt;td&gt;77.88&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;eradio_v1&lt;/td&gt; &#xA;    &lt;td&gt;E-RADIO&lt;/td&gt; &#xA;    &lt;td&gt;Float32&lt;/td&gt; &#xA;    &lt;td&gt;Meta CLIP; DINOv2&lt;/td&gt; &#xA;    &lt;td&gt;3697&lt;/td&gt; &#xA;    &lt;td&gt;77.87&lt;/td&gt; &#xA;    &lt;td&gt;83.73&lt;/td&gt; &#xA;    &lt;td&gt;45.50&lt;/td&gt; &#xA;    &lt;td&gt;79.95&lt;/td&gt; &#xA;    &lt;td&gt;59.55&lt;/td&gt; &#xA;    &lt;td&gt;46.31&lt;/td&gt; &#xA;    &lt;td&gt;72.05&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Model stats and summarization metrics:&lt;/h3&gt; &#xA;&lt;p&gt;For summarization results we use the summarization token of the model. For Zero-shot we use the corresponding language embedding for most models. For RADIO models we use language embedding from DFN CLIP 378 model.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Params (M)&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;Throughput&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet1K Zero-shot&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet1K k-NN&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;632&lt;/td&gt; &#xA;   &lt;td&gt;224&lt;/td&gt; &#xA;   &lt;td&gt;503&lt;/td&gt; &#xA;   &lt;td&gt;77.19&lt;/td&gt; &#xA;   &lt;td&gt;81.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MetaCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;632&lt;/td&gt; &#xA;   &lt;td&gt;224&lt;/td&gt; &#xA;   &lt;td&gt;486&lt;/td&gt; &#xA;   &lt;td&gt;80.51&lt;/td&gt; &#xA;   &lt;td&gt;82.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SigLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;428&lt;/td&gt; &#xA;   &lt;td&gt;384&lt;/td&gt; &#xA;   &lt;td&gt;241&lt;/td&gt; &#xA;   &lt;td&gt;82.61&lt;/td&gt; &#xA;   &lt;td&gt;85.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intern-ViT-6B&lt;/td&gt; &#xA;   &lt;td&gt;5,902&lt;/td&gt; &#xA;   &lt;td&gt;224&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;83.20&lt;/td&gt; &#xA;   &lt;td&gt;78.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;5,537&lt;/td&gt; &#xA;   &lt;td&gt;448&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;68.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFN CLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;633&lt;/td&gt; &#xA;   &lt;td&gt;378&lt;/td&gt; &#xA;   &lt;td&gt;170&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.90&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;85.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI CLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;305&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;414&lt;/td&gt; &#xA;   &lt;td&gt;75.54&lt;/td&gt; &#xA;   &lt;td&gt;79.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DINOv2-g/14-reg&lt;/td&gt; &#xA;   &lt;td&gt;1,137&lt;/td&gt; &#xA;   &lt;td&gt;224&lt;/td&gt; &#xA;   &lt;td&gt;294&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;83.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H/16&lt;/td&gt; &#xA;   &lt;td&gt;637&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;22.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;------------------------&lt;/td&gt; &#xA;   &lt;td&gt;------------&lt;/td&gt; &#xA;   &lt;td&gt;------------&lt;/td&gt; &#xA;   &lt;td&gt;------------&lt;/td&gt; &#xA;   &lt;td&gt;---------------------&lt;/td&gt; &#xA;   &lt;td&gt;-----------------&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E-RADIO-L&lt;/td&gt; &#xA;   &lt;td&gt;391&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;468&lt;/td&gt; &#xA;   &lt;td&gt;80.73&lt;/td&gt; &#xA;   &lt;td&gt;83.89&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RADIO-ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;653&lt;/td&gt; &#xA;   &lt;td&gt;432&lt;/td&gt; &#xA;   &lt;td&gt;158&lt;/td&gt; &#xA;   &lt;td&gt;82.93&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;86.06&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Segmentation metrics:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation setup: linear probing, simple head&lt;/li&gt; &#xA; &lt;li&gt;For SAM COCO results, we replace the vision backbone of the SAM model with the corresponding RADIO model. The decoder is frozen from the original model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Segmentation ADE20k&lt;/th&gt; &#xA;   &lt;th&gt;Segmentation VOC&lt;/th&gt; &#xA;   &lt;th&gt;SAM COCO&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;40.04&lt;/td&gt; &#xA;   &lt;td&gt;68.03&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MetaCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;35.39&lt;/td&gt; &#xA;   &lt;td&gt;62.62&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SigLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;40.53&lt;/td&gt; &#xA;   &lt;td&gt;70.31&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intern-ViT-6B&lt;/td&gt; &#xA;   &lt;td&gt;47.20&lt;/td&gt; &#xA;   &lt;td&gt;76.85&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;42.78&lt;/td&gt; &#xA;   &lt;td&gt;74.43&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFN CLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;39.00&lt;/td&gt; &#xA;   &lt;td&gt;70.29&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI CLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;36.51&lt;/td&gt; &#xA;   &lt;td&gt;67.04&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DINOv2-g/14-reg&lt;/td&gt; &#xA;   &lt;td&gt;48.68&lt;/td&gt; &#xA;   &lt;td&gt;82.78&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H/16&lt;/td&gt; &#xA;   &lt;td&gt;28.08&lt;/td&gt; &#xA;   &lt;td&gt;34.34&lt;/td&gt; &#xA;   &lt;td&gt;77.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;------------------------&lt;/td&gt; &#xA;   &lt;td&gt;---------------------&lt;/td&gt; &#xA;   &lt;td&gt;------------------&lt;/td&gt; &#xA;   &lt;td&gt;----------&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E-RADIO-L&lt;/td&gt; &#xA;   &lt;td&gt;48.22&lt;/td&gt; &#xA;   &lt;td&gt;81.64&lt;/td&gt; &#xA;   &lt;td&gt;76.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RADIO-ViT-H/16 (ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51.34&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.71&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;76.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Vision-language model performance metrics in LLaVa 1.5:&lt;/h3&gt; &#xA;&lt;p&gt;We replace the vision backbone and keep the same LLM and training recipe as in LLaVa 1.5:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;57.94&lt;/td&gt; &#xA;   &lt;td&gt;83.61&lt;/td&gt; &#xA;   &lt;td&gt;50.48&lt;/td&gt; &#xA;   &lt;td&gt;72.24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MetaCLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;60.57&lt;/td&gt; &#xA;   &lt;td&gt;84.76&lt;/td&gt; &#xA;   &lt;td&gt;53.65&lt;/td&gt; &#xA;   &lt;td&gt;75.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SigLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;57.70&lt;/td&gt; &#xA;   &lt;td&gt;84.85&lt;/td&gt; &#xA;   &lt;td&gt;56.65&lt;/td&gt; &#xA;   &lt;td&gt;71.94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intern-ViT-6B (224)&lt;/td&gt; &#xA;   &lt;td&gt;60.18&lt;/td&gt; &#xA;   &lt;td&gt;84.02&lt;/td&gt; &#xA;   &lt;td&gt;52.45&lt;/td&gt; &#xA;   &lt;td&gt;76.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(448)&lt;/td&gt; &#xA;   &lt;td&gt;61.19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87.23&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;60.36&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;78.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFN CLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;61.73&lt;/td&gt; &#xA;   &lt;td&gt;85.91&lt;/td&gt; &#xA;   &lt;td&gt;56.78&lt;/td&gt; &#xA;   &lt;td&gt;78.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI CLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;62.20&lt;/td&gt; &#xA;   &lt;td&gt;86.09&lt;/td&gt; &#xA;   &lt;td&gt;57.92&lt;/td&gt; &#xA;   &lt;td&gt;78.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DINOv2-g/14-reg&lt;/td&gt; &#xA;   &lt;td&gt;61.88&lt;/td&gt; &#xA;   &lt;td&gt;85.62&lt;/td&gt; &#xA;   &lt;td&gt;47.18&lt;/td&gt; &#xA;   &lt;td&gt;76.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H/16&lt;/td&gt; &#xA;   &lt;td&gt;49.92&lt;/td&gt; &#xA;   &lt;td&gt;81.76&lt;/td&gt; &#xA;   &lt;td&gt;43.91&lt;/td&gt; &#xA;   &lt;td&gt;57.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;---------------------&lt;/td&gt; &#xA;   &lt;td&gt;-------------------&lt;/td&gt; &#xA;   &lt;td&gt;--------------------&lt;/td&gt; &#xA;   &lt;td&gt;-----------------------&lt;/td&gt; &#xA;   &lt;td&gt;---------------------&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E-RADIO-L&lt;/td&gt; &#xA;   &lt;td&gt;61.70&lt;/td&gt; &#xA;   &lt;td&gt;85.07&lt;/td&gt; &#xA;   &lt;td&gt;51.47&lt;/td&gt; &#xA;   &lt;td&gt;76.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RADIO-ViT-H/16 (ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;63.01&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;86.20&lt;/td&gt; &#xA;   &lt;td&gt;56.32&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;79.28&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Probing 3D Awareness&lt;/h3&gt; &#xA;&lt;p&gt;Probing 3D Awareness: we use the code from &lt;a href=&#34;https://github.com/mbanani/probe3d&#34;&gt;Probing the 3D Awareness of Visual Foundation Models&lt;/a&gt; and evaluate our RADIO model and its teachers on monocular depth, surface normals and multi-view correspondance tasks, using the NAVI dataset. For each task we report the accuracy, averaged over all thresholds. RADIO preserves features of DINOv2 and performs much better than CLIP analogs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Backbone&lt;/th&gt; &#xA;   &lt;th&gt;Depth&lt;/th&gt; &#xA;   &lt;th&gt;Surface Normals&lt;/th&gt; &#xA;   &lt;th&gt;Multi-view corr.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFN CLIP-H/14&lt;/td&gt; &#xA;   &lt;td&gt;52.5&lt;/td&gt; &#xA;   &lt;td&gt;23.0&lt;/td&gt; &#xA;   &lt;td&gt;20.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI CLIP-L/14&lt;/td&gt; &#xA;   &lt;td&gt;53.7&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;   &lt;td&gt;20.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DINOv2-g/14-reg&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H/16&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;50.3&lt;/td&gt; &#xA;   &lt;td&gt;45.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-----------------------&lt;/td&gt; &#xA;   &lt;td&gt;-------&lt;/td&gt; &#xA;   &lt;td&gt;-----------------&lt;/td&gt; &#xA;   &lt;td&gt;------------------&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RADIO-ViT-H/16 (ours)&lt;/td&gt; &#xA;   &lt;td&gt;81.0&lt;/td&gt; &#xA;   &lt;td&gt;58.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;62.1&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Detailed usage&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Torch hub&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import torch&#xA;&#xA;# If you don&#39;t supply the `version` parameter, the latest ViT version will be returned.&#xA;model = torch.hub.load(&#39;NVlabs/RADIO&#39;, &#39;radio_model&#39;, version=&#39;radio_v2&#39;, progress=True)&#xA;model.cuda().eval()&#xA;&#xA;x = torch.rand(1, 3, 224, 224, device=&#39;cuda&#39;)&#xA;&#xA;# NOTE: RADIO models expect the input to have values in the range [0, 1]&#xA;# NOTE 2: `radio_v1` is a ViT-H/14 model, and supports inputs in the size range `224 &amp;lt; dim &amp;lt; 1008`&#xA;#           where each dimension must be divisible by 14.&#xA;#           Non-square inputs are supported.&#xA;# NOTE 3: `radio_v2` is a ViT-H/16 model, and supports inputs in the size range `224 &amp;lt; dim &amp;lt; 2048`&#xA;#           where each dimension must be divisible by 16.&#xA;summary, spatial_features = model(x)&#xA;&#xA;# RADIO also supports running in mixed precision, like so:&#xA;with torch.cuda.amp.autocast(dtype=torch.bfloat16):&#xA;    summary, spatial_features = model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;HuggingFace&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;&#xA;hf_repo = &#34;nvidia/RADIO&#34; # For RADIO.&#xA;# hf_repo = &#34;nvidia/E-RADIO&#34; # For E-RADIO.&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(hf_repo)&#xA;model = AutoModel.from_pretrained(hf_repo, trust_remote_code=True)&#xA;model.eval().cuda()&#xA;&#xA;image = Image.open(&#39;./examples/image1.png&#39;).convert(&#39;RGB&#39;)&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;&#xA;summary, features = model(pixel_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Usage&lt;/h3&gt; &#xA; &lt;p&gt;RADIO and E-RADIO will return a tuple with two tensors. The &lt;code&gt;summary&lt;/code&gt; is similar to the &lt;code&gt;cls_token&lt;/code&gt; in ViT and is meant to represent the general concept of the entire image. It has shape $(B,C)$ with $B$ being the batch dimension, and $C$ being some number of channels. The &lt;code&gt;spatial_features&lt;/code&gt; represent more localized content which should be suitable for dense tasks such as semantic segmentation, or for integration into an LLM. RADIO and E-RADIO return spatial features in different shapes:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;RADIO: spatial features have shape $(B,T,D)$ with $T$ being the flattened spatial tokens, and $D$ being the channels for spatial features. Note that $C \neq D$ in general.&lt;/li&gt; &#xA;  &lt;li&gt;E-RADIO: spatial features have shape $(B,H,W,D)$ with $H$ being the height, and $W$ being the width of the spatial features.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For RADIO, converting to a spatial tensor format can be done using the downsampling size of the model, combined with the input tensor shape. For &#39;radio_v1&#39;, the patch size is 14.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from einops import rearrange&#xA;spatial_features = rearrange(spatial_features, &#39;b (h w) d -&amp;gt; b d h w&#39;, h=x.shape[-2] // patch_size, w=x.shape[-1] // patch_size)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The resulting tensor will have shape $(B,D,H,W)$, as is typically seen with computer vision models.&lt;/p&gt; &#xA; &lt;h3&gt;RADIOv1/v2 Notes&lt;/h3&gt; &#xA; &lt;p&gt;We have trained this model to be flexible in input dimension. It supports arbitrary input sizes. There are useful properties set for the returned model that you may query:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;model.patch_size: int&#xA;model.max_resolution: int # (Images can be no larger than this value on either dimension)&#xA;model.preferred_resolution: Tuple[height, width] # This is the primary resolution that RADIO was trained at, and will likely&#xA;                                                 # produce best results for summary tasks. Dense tasks require experimentation&#xA;                                                 # to find the best resolution.&#xA;model.window_size: Optional[int] # If `vitdet_window_size` was specified, this is that value&#xA;model.min_resolution_step: int # Combines `patch_size` and `window_size` to define what each image dimension must be a multiple of.&#xA;                               # e.g. If `patch_size == 16`, then both width and height must be x*16&#xA;                               # If `patch_size == 14` and `window_size == 8` then width and height must be x*14*8&#xA;&#xA;# For convenience, you can also call this function to get the nearest valid input size for a given image&#xA;nearest_height, nearest_width = model.get_nearest_supported_resolution(height=1024, width=1024)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;RADIO allows non-square inputs. In fact, both RADIOv1 and RADIOv2 achieve higher zero-shot classification scores when allowing the larger image dimension to vary, and only fixing the smaller dimension.&lt;/p&gt; &#xA; &lt;h3&gt;Adaptors&lt;/h3&gt; &#xA; &lt;p&gt;&lt;em&gt;(Currently only supported with TorchHub)&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;You may additionally specify model adaptors to achieve extra behaviors. Currently, &#39;clip&#39; is the only supported adaptor. In this mode, radio will return a dict of tuples:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;model = torch.hub.load(..., adaptor_names=&#39;clip&#39;, ...)&#xA;&#xA;output = model(x)&#xA;&#xA;bb_summary, bb_features = output[&#39;backbone&#39;]&#xA;clip_summary, clip_features = output[&#39;clip&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Refer to &lt;code&gt;examples/zero_shot_imagenet.py&lt;/code&gt; for example usage.&lt;/p&gt; &#xA; &lt;h3&gt;Preprocessing&lt;/h3&gt; &#xA; &lt;p&gt;By default, RADIO expects the input images to have normalized values in the &lt;code&gt;[0, 1]&lt;/code&gt; range. If you already have an existing data pipeline, and you&#39;d like conditioning to occur there instead of within the RADIO model, you can call this function:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;preprocessor = model.make_preprocessor_external()&#xA;&#xA;images = preprocessor(images)&#xA;...&#xA;output = model(images)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;HuggingFace hub&lt;/summary&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;E-RADIO limitations&lt;/summary&gt; &#xA; &lt;p&gt;E-RADIO is a more efficient variant of RADIO, but it has some limitations:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;E-RADIO naively supports only images with size divisible by 32. Other resolutions are supported but might result in a performance drop.&lt;/li&gt; &#xA;  &lt;li&gt;E-RADIO performance is sensative to the window size of the windowed attention in the 3rd and 4th block. For the best performance automatically adjust the window size for the input resolution: &lt;code&gt;model.model.set_optimal_window_size(IMAGE_SHAPE)&lt;/code&gt;, where &lt;code&gt;IMAGE_SHAPE&lt;/code&gt; is a tuple of (height, width) of the input image.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Coming Soon&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#NVlabs/RADIO&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=NVlabs/RADIO&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=NVlabs/RADIO&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=NVlabs/RADIO&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citing RADIO&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving a star and citation:&lt;/p&gt; &#xA;&lt;h4&gt;CVPR 2024 Reference:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inProceedings{ranzinger2023amradio,&#xA;  title={AM-RADIO: Agglomerative Visual Foundation Model -- Reduce All Domains Into One},&#xA;  author={Mike Ranzinger and Greg Heinrich and Jan Kautz and Pavlo Molchanov},&#xA;  booktitle={CVPR},&#xA;  year={2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;ArXiv Reference:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ranzinger2023amradio,&#xA;      title={AM-RADIO: Agglomerative Model -- Reduce All Domains Into One},&#xA;      author={Mike Ranzinger and Greg Heinrich and Jan Kautz and Pavlo Molchanov},&#xA;      year={2023},&#xA;      eprint={2312.06709},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;Copyright ¬© 2024, NVIDIA Corporation. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;This work is made available under the NVIDIA Source Code License-NC. Click &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/RADIO/main/LICENSE&#34;&gt;here&lt;/a&gt; to view a copy of this license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/datatrove</title>
    <updated>2024-05-08T01:21:55Z</updated>
    <id>tag:github.com,2024-05-08:/huggingface/datatrove</id>
    <link href="https://github.com/huggingface/datatrove" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Freeing data processing from scripting madness by providing a set of platform-agnostic customizable pipeline processing blocks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DataTrove&lt;/h1&gt; &#xA;&lt;p&gt;DataTrove is a library to process, filter and deduplicate text data at a very large scale. It provides a set of prebuilt commonly used processing blocks with a framework to easily add custom functionality.&lt;/p&gt; &#xA;&lt;p&gt;DataTrove processing pipelines are platform-agnostic, running out of the box locally or on a slurm cluster. Its (relatively) low memory usage and multiple step design makes it ideal for large workloads, such as to process an LLM&#39;s training data.&lt;/p&gt; &#xA;&lt;p&gt;Local, remote and other file systems are supported through &lt;a href=&#34;https://filesystem-spec.readthedocs.io/en/latest/&#34;&gt;fsspec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#quickstart-examples&#34;&gt;Quickstart examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#pipeline&#34;&gt;Pipeline&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#datatrove-document&#34;&gt;DataTrove Document&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#types-of-pipeline-blocks&#34;&gt;Types of pipeline blocks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#full-pipeline&#34;&gt;Full pipeline&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#executors&#34;&gt;Executors&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#localpipelineexecutor&#34;&gt;LocalPipelineExecutor&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#slurmpipelineexecutor&#34;&gt;SlurmPipelineExecutor&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#datafolder--paths&#34;&gt;DataFolder / paths&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#practical-guides&#34;&gt;Practical guides&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#reading-data&#34;&gt;Reading data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#extracting-text&#34;&gt;Extracting text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#filtering-data&#34;&gt;Filtering data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#saving-data&#34;&gt;Saving data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#deduplicating-data&#34;&gt;Deduplicating data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#custom-blocks&#34;&gt;Custom blocks&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#simple-data&#34;&gt;Simple data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#custom-function&#34;&gt;Custom function&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#custom-block&#34;&gt;Custom block&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install datatrove[FLAVOUR]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available flavours (combine them with &lt;code&gt;,&lt;/code&gt; i.e. &lt;code&gt;[processing,s3]&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;all&lt;/code&gt; installs everything: &lt;code&gt;pip install datatrove[all]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;io&lt;/code&gt; dependencies to read &lt;code&gt;warc/arc/wet&lt;/code&gt; files and arrow/parquet formats: &lt;code&gt;pip install datatrove[io]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;processing&lt;/code&gt; dependencies for text extraction, filtering and tokenization: &lt;code&gt;pip install datatrove[processing]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s3&lt;/code&gt; s3 support: &lt;code&gt;pip install datatrove[s3]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cli&lt;/code&gt; for command line tools: &lt;code&gt;pip install datatrove[cli]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart examples&lt;/h2&gt; &#xA;&lt;p&gt;You can check the following &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples&#34;&gt;examples&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/process_common_crawl_dump.py&#34;&gt;process_common_crawl_dump.py&lt;/a&gt; full pipeline to read commoncrawl warc files, extract their text content, filters and save the resulting data to s3. Runs on slurm&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/tokenize_c4.py&#34;&gt;tokenize_c4.py&lt;/a&gt; reads data directly from huggingface&#39;s hub to tokenize the english portion of the C4 dataset using the &lt;code&gt;gpt2&lt;/code&gt; tokenizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/minhash_deduplication.py&#34;&gt;minhash_deduplication.py&lt;/a&gt; full pipeline to run minhash deduplication of text data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/sentence_deduplication.py&#34;&gt;sentence_deduplication.py&lt;/a&gt; example to run sentence level exact deduplication&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/exact_substrings.py&#34;&gt;exact_substrings.py&lt;/a&gt; example to run ExactSubstr (requires &lt;a href=&#34;https://github.com/google-research/deduplicate-text-datasets&#34;&gt;this repo&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pipeline&lt;/h2&gt; &#xA;&lt;h3&gt;DataTrove Document&lt;/h3&gt; &#xA;&lt;p&gt;Each pipeline block processes data in the datatrove &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/data.py&#34;&gt;&lt;code&gt;Document&lt;/code&gt;&lt;/a&gt; format:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text&lt;/code&gt; the actual text content for each sample&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;id&lt;/code&gt; a unique id (string) for this sample&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;metadata&lt;/code&gt; a dictionary where any additional info may be stored&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Types of pipeline blocks&lt;/h3&gt; &#xA;&lt;p&gt;Each pipeline block takes a generator of &lt;code&gt;Document&lt;/code&gt; as input and returns another generator of &lt;code&gt;Document&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/readers&#34;&gt;readers&lt;/a&gt;&lt;/strong&gt; read data from different formats and yield &lt;code&gt;Document&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/writers&#34;&gt;writers&lt;/a&gt;&lt;/strong&gt; save &lt;code&gt;Document&lt;/code&gt; to disk/cloud in different formats&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/extractors&#34;&gt;extractors&lt;/a&gt;&lt;/strong&gt; extract text content from raw formats (such as webpage html)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/filters&#34;&gt;filters&lt;/a&gt;&lt;/strong&gt; filter out (remove) some &lt;code&gt;Document&lt;/code&gt;s based on specific rules/criteria&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/stats&#34;&gt;stats&lt;/a&gt;&lt;/strong&gt; blocks to collect statistics on the dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/tokens&#34;&gt;tokens&lt;/a&gt;&lt;/strong&gt; blocks to tokenize data or count tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/dedup&#34;&gt;dedup&lt;/a&gt;&lt;/strong&gt; blocks for deduplication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Full pipeline&lt;/h3&gt; &#xA;&lt;p&gt;A pipeline is defined as a list of pipeline blocks. As an example, the following pipeline would read data from disk, randomly filter (remove) some documents and write them back to disk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.pipeline.readers import CSVReader&#xA;from datatrove.pipeline.filters import SamplerFilter&#xA;from datatrove.pipeline.writers import JsonlWriter&#xA;&#xA;pipeline = [&#xA;    CSVReader(&#xA;        data_folder=&#34;/my/input/path&#34;&#xA;    ),&#xA;    SamplerFilter(rate=0.5),&#xA;    JsonlWriter(&#xA;        output_folder=&#34;/my/output/path&#34;&#xA;    )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Executors&lt;/h2&gt; &#xA;&lt;p&gt;Pipelines are platform-agnostic, which means that the same pipeline can smoothly run on different execution environments without any changes to its steps. Each environment has its own PipelineExecutor. Some options common to all executors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; a list consisting of the pipeline steps that should be run&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;logging_dir&lt;/code&gt; a datafolder where log files, statistics and more should be saved. Do not reuse folders for different pipelines/jobs as this will overwrite your stats, logs and completions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;skip_completed&lt;/code&gt; (&lt;em&gt;bool&lt;/em&gt;, &lt;code&gt;True&lt;/code&gt; by default) datatrove keeps track of completed tasks so that when you relaunch a job they can be skipped. Set this to &lt;code&gt;False&lt;/code&gt; to disable this behaviour&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Call an executor&#39;s &lt;code&gt;run&lt;/code&gt; method to execute its pipeline.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Datatrove keeps track of which tasks successfully completed by creating a marker (an empty file) in the &lt;code&gt;${logging_dir}/completions&lt;/code&gt; folder. Once the job finishes, if some of its tasks have failed, you can &lt;strong&gt;simply relaunch the exact same executor&lt;/strong&gt; and datatrove will check and only run the tasks that were not previously completed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] If you relaunch a pipeline because some tasks failed, &lt;strong&gt;do not change the total number of tasks&lt;/strong&gt; as this will affect the distribution of input files/sharding.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;LocalPipelineExecutor&lt;/h3&gt; &#xA;&lt;p&gt;This executor will launch a pipeline on a local machine. Options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tasks&lt;/code&gt; total number of tasks to run&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;workers&lt;/code&gt; how many tasks to run simultaneously. If &lt;code&gt;-1&lt;/code&gt;, no limit. Anything &lt;code&gt;&amp;gt; 1&lt;/code&gt; will use multiprocessing to execute the tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start_method&lt;/code&gt; method to use to spawn a multiprocessing Pool. Ignored if &lt;code&gt;workers&lt;/code&gt; is 1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Example executor&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.executor import LocalPipelineExecutor&#xA;executor = LocalPipelineExecutor(&#xA;    pipeline=[&#xA;        ...&#xA;    ],&#xA;    logging_dir=&#34;logs/&#34;,&#xA;    tasks=10,&#xA;    workers=5&#xA;)&#xA;executor.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multi-node parallelism&lt;/summary&gt; &#xA; &lt;p&gt;You can have different nodes/machines process different parts of the total tasks by using the &lt;code&gt;local_tasks&lt;/code&gt; and &lt;code&gt;local_rank_offset&lt;/code&gt;. For each node/instance/machine, launch with the following options:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;tasks&lt;/code&gt; the total tasks to be executed (across all machines). &lt;strong&gt;This value must be the same on each machine or the input file distribution may overlap!&lt;/strong&gt; Example: 500&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;local_tasks&lt;/code&gt; how many tasks of the total will be executed on this particular machine. Note that you can use different values for each machine. Example: 100&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;local_rank_offset&lt;/code&gt; the rank of the first task to be executed on this machine. If this is the 3rd machine where you are launching a job, and the 2 previous machines each ran 250 and 150 jobs, this would be &lt;code&gt;400&lt;/code&gt; for the current machine.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To get final merged stats you will have to invoke the &lt;code&gt;merge_stats&lt;/code&gt; script manually on a path containing the stats from all machines.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SlurmPipelineExecutor&lt;/h3&gt; &#xA;&lt;p&gt;This executor will launch a pipeline on a slurm cluster, using slurm job arrays to group and manage tasks. Options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tasks&lt;/code&gt; total number of tasks to run. &lt;strong&gt;required&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;time&lt;/code&gt; slurm time limit string. &lt;strong&gt;required&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;partition&lt;/code&gt; slurm partition. &lt;strong&gt;required&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;workers&lt;/code&gt; how many tasks to run simultaneously. If &lt;code&gt;-1&lt;/code&gt;, no limit. Slurm will run &lt;code&gt;workers&lt;/code&gt; tasks at a time. (default: &lt;code&gt;-1&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;job_name&lt;/code&gt; slurm job name (default: &#34;data_processing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;depends&lt;/code&gt; another SlurmPipelineExecutor instance, which will be a dependency of this pipeline (current pipeline will only start executing after the depended on pipeline successfully completes)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbatch_args&lt;/code&gt; dictionary with any other arguments you would like to pass to sbatch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;slurm_logs_folder&lt;/code&gt; where to save the slurm log files. If using a local path for &lt;code&gt;logging_dir&lt;/code&gt;, they will be saved on &lt;code&gt;logging_dir/slurm_logs&lt;/code&gt;. If not, they will be saved as a subdir of the current directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Other options&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;cpus_per_task&lt;/code&gt; how many cpus to give each task (default: &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;qos&lt;/code&gt; slurm qos (default: &#34;normal&#34;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;mem_per_cpu_gb&lt;/code&gt; memory per cpu, in GB (default: 2)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;env_command&lt;/code&gt; custom command to activate a python environment, if needed&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;condaenv&lt;/code&gt; conda environment to activate&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;venv_path&lt;/code&gt; path to a python environment to activate&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;max_array_size&lt;/code&gt; the &lt;em&gt;MaxArraySize&lt;/em&gt; value in &lt;code&gt;$ scontrol show config&lt;/code&gt;. If number of tasks exceeds this number, it will split into multiple array jobs (default: 1001)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;max_array_launch_parallel&lt;/code&gt; if we need multiple jobs due to max_array_size, whether to launch them all in one go (parallel) or sequentially (default: &lt;code&gt;False&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;stagger_max_array_jobs&lt;/code&gt; when max_array_launch_parallel is True, this determines how many seconds to wait between launching each of the parallel jobs (default: &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;run_on_dependency_fail&lt;/code&gt; start executing when a job we depend on finishes even if it has failed (default: &lt;code&gt;False&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;randomize_start&lt;/code&gt; randomize the start of each task in a job in a ~3 min window. Useful when heavily hitting an s3 bucket for example. (default: &lt;code&gt;False&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Example executor&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.executor import SlurmPipelineExecutor&#xA;executor1 = SlurmPipelineExecutor(&#xA;    pipeline=[&#xA;        ...&#xA;    ],&#xA;    job_name=&#34;my_cool_job1&#34;,&#xA;    logging_dir=&#34;logs/job1&#34;,&#xA;    tasks=500,&#xA;    workers=100,  # omit to run all at once&#xA;    time=&#34;10:00:00&#34;,  # 10 hours&#xA;    partition=&#34;hopper-cpu&#34;&#xA;)&#xA;executor2 = SlurmPipelineExecutor(&#xA;    pipeline=[&#xA;        ...&#xA;    ],&#xA;    job_name=&#34;my_cool_job2&#34;,&#xA;    logging_dir=&#34;logs/job2&#34;,&#xA;    tasks=1,&#xA;    time=&#34;5:00:00&#34;,  # 5 hours&#xA;    partition=&#34;hopper-cpu&#34;,&#xA;    depends=executor1  # this pipeline will only be launched after executor1 successfully completes&#xA;)&#xA;# executor1.run()&#xA;executor2.run() # this will actually launch executor1, as it is a dependency, so no need to launch it explicitly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;For a pipeline with &lt;code&gt;logging_dir&lt;/code&gt; &lt;strong&gt;mylogspath/exp1&lt;/strong&gt;, the following folder structure would be created:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;See folder structure&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;‚îî‚îÄ‚îÄ mylogspath/exp1&#xA;    ‚îÇ‚îÄ‚îÄ executor.json ‚üµ json dump of the executor options and pipeline steps&#xA;    ‚îÇ‚îÄ‚îÄ launch_script.slurm ‚üµ the slurm config created and used to launch this job (if running on slurm)&#xA;    ‚îÇ‚îÄ‚îÄ executor.pik ‚üµ the slurm config created and used to launch this job (if running on slurm)&#xA;    ‚îÇ‚îÄ‚îÄ ranks_to_run.json ‚üµ list of tasks that are being run&#xA;    ‚îÇ‚îÄ‚îÄ logs/&#xA;    ‚îÇ   ‚îî‚îÄ‚îÄ[task_00000.log, task_00001.log, task_00002.log, ...] ‚üµ individual logging files for each task&#xA;    ‚îÇ‚îÄ‚îÄ completions/&#xA;    ‚îÇ   ‚îî‚îÄ‚îÄ[00004, 00007, 00204, ...] ‚üµ empty files marking a task as completed. Using when relaunching/resuming a job (only unfinished tasks will be run)&#xA;    ‚îÇ‚îÄ‚îÄ stats/&#xA;    ‚îÇ   ‚îî‚îÄ‚îÄ[00000.json, 00001.json, 00002.json, ...] ‚üµ individual stats for each task (number of samples processed, filtered, removed, etc)&#xA;    ‚îî‚îÄ‚îÄ stats.json ‚üµ global stats from all tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;DataFolder / paths&lt;/h2&gt; &#xA;&lt;p&gt;Datatrove supports a wide variety of input/output sources through &lt;a href=&#34;https://filesystem-spec.readthedocs.io/en/latest/&#34;&gt;fsspec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are a few ways to provide a path to a datatrove block (for &lt;code&gt;input_folder&lt;/code&gt;, &lt;code&gt;logging_dir&lt;/code&gt;, &lt;code&gt;data_folder&lt;/code&gt; and so on arguments):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;str&lt;/code&gt;: the simplest way is to pass a single string. Example: &lt;code&gt;/home/user/mydir&lt;/code&gt;, &lt;code&gt;s3://mybucket/myinputdata&lt;/code&gt;, &lt;code&gt;hf://datasets/allenai/c4/en/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;(str, fsspec filesystem instance)&lt;/code&gt;: a string path and a fully initialized filesystem object. Example: &lt;code&gt;(&#34;s3://mybucket/myinputdata&#34;, S3FileSystem(client_kwargs={&#34;endpoint_url&#34;: endpoint_uri}))&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;(str, dict)&lt;/code&gt;: a string path and a dictionary with options to initialize a fs. Example (equivalent to the previous line): &lt;code&gt;(&#34;s3://mybucket/myinputdata&#34;, {&#34;client_kwargs&#34;: {&#34;endpoint_url&#34;: endpoint_uri}})&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;DataFolder&lt;/code&gt;: you can initialize a &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/io.py&#34;&gt;DataFolder&lt;/a&gt; object directly and pass it as an argument&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Under the hood these argument combinations are parsed by &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/io.py#116&#34;&gt;&lt;code&gt;get_datafolder&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Practical guides&lt;/h2&gt; &#xA;&lt;h3&gt;Reading data&lt;/h3&gt; &#xA;&lt;p&gt;Usually, pipelines will start with a &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/readers&#34;&gt;Reader&lt;/a&gt; block. Most readers take a &lt;code&gt;data_folder&lt;/code&gt; argument ‚Äî a path to a folder containing the data to be read.&lt;/p&gt; &#xA;&lt;p&gt;These files will be distributed across each task. If you have &lt;code&gt;N&lt;/code&gt; tasks, task with rank &lt;code&gt;i&lt;/code&gt; (0-based) will process files &lt;code&gt;i, i+N, i+2N, i+3N,...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Internally, each reader reads data and converts it into a dictionary before creating a &lt;code&gt;Document&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some options common to most readers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text_key&lt;/code&gt; the dictionary key containing the text content for each sample. Default: &lt;code&gt;text&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;id_key&lt;/code&gt; the dictionary key containing the id for each sample. Default: &lt;code&gt;id&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;default_metadata&lt;/code&gt; a dictionary for any default metadata values you would like to add (such as their source, for example)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;recursive&lt;/code&gt; whether to look for files recursively in &lt;code&gt;data_folder&lt;/code&gt;&#39;s subdirectories&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;glob_pattern&lt;/code&gt; use this field to match specific files. For instance, &lt;code&gt;glob_pattern=&#34;*/warc/*.warc.gz&#34;&lt;/code&gt; will match files with a &lt;code&gt;.warc.gz&lt;/code&gt; file extension on the &lt;code&gt;warc/&lt;/code&gt; folder of each of the &lt;code&gt;data_folder&lt;/code&gt;&#39;s subdirectories&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;adapter&lt;/code&gt; this function takes the raw dictionary obtained from the reader and returns a dictionary with &lt;code&gt;Document&lt;/code&gt;&#39;s field names. You may overwrite this function (&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/readers/base.py&#34;&gt;_default_adapter&lt;/a&gt;) if you would like.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;limit&lt;/code&gt; read only a certain number of samples. Useful for testing/debugging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extracting text&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/extractors&#34;&gt;extractors&lt;/a&gt; to extract text content from raw html. The most commonly used extractor in datatrove is &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/extractors/trafilatura.py&#34;&gt;Trafilatura&lt;/a&gt;, which uses the &lt;a href=&#34;https://trafilatura.readthedocs.io/en/latest/&#34;&gt;trafilatura&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;h3&gt;Filtering data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/filters&#34;&gt;Filters&lt;/a&gt; are some of the most important blocks of any data processing pipeline. Datatrove&#39;s filter blocks take a &lt;code&gt;Document&lt;/code&gt; and return a boolean (&lt;code&gt;True&lt;/code&gt; to keep a document, &lt;code&gt;False&lt;/code&gt; to remove it). Removed samples do not continue to the next pipeline stage. You can also save the removed samples to disk by passing a &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/writers&#34;&gt;Writer&lt;/a&gt; to the &lt;code&gt;excluded_writer&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;h3&gt;Saving data&lt;/h3&gt; &#xA;&lt;p&gt;Once you are done processing your data you will probably want to save it somewhere. For this you can use a &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/writers/jsonl.py&#34;&gt;writer&lt;/a&gt;. Writers require an &lt;code&gt;output_folder&lt;/code&gt; (the path where data should be saved). You can choose the &lt;code&gt;compression&lt;/code&gt; to use (default: &lt;code&gt;gzip&lt;/code&gt;) and the filename to save each file as. For the &lt;code&gt;output_filename&lt;/code&gt;, a template is applied using the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;${rank}&lt;/code&gt; replaced with the current task&#39;s rank. Note that if this tag isn&#39;t present, &lt;strong&gt;different tasks may try to write to the same location&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;${id}&lt;/code&gt; replaced with the sample id&lt;/li&gt; &#xA; &lt;li&gt;metadata: any other &lt;code&gt;${tag}&lt;/code&gt; will be replaced with the corresponding &lt;code&gt;document.metadata[&#39;tag&#39;]&lt;/code&gt; value&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example to separate samples by language based on their &lt;code&gt;lang&lt;/code&gt; metadata field:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;JsonlWriter(&#xA;    f&#34;{MAIN_OUTPUT_PATH}/non_english/&#34;,&#xA;    output_filename=&#34;${language}/&#34; + DUMP + &#34;/${rank}.jsonl.gz&#34;,  # folder structure: language/dump/file&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deduplicating data&lt;/h3&gt; &#xA;&lt;p&gt;For deduplication check the examples &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/minhash_deduplication.py&#34;&gt;minhash_deduplication.py&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/sentence_deduplication.py&#34;&gt;sentence_deduplication.py&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/examples/exact_substrings.py&#34;&gt;exact_substrings.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Custom blocks&lt;/h3&gt; &#xA;&lt;h4&gt;Simple data&lt;/h4&gt; &#xA;&lt;p&gt;You can pass an iterable of &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/data.py&#34;&gt;&lt;code&gt;Document&lt;/code&gt;&lt;/a&gt; directly as a pipeline block like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.data import Document&#xA;from datatrove.pipeline.filters import SamplerFilter&#xA;from datatrove.pipeline.writers import JsonlWriter&#xA;&#xA;pipeline = [&#xA;    [&#xA;        Document(text=&#34;some data&#34;, id=&#34;0&#34;),&#xA;        Document(text=&#34;some more data&#34;, id=&#34;1&#34;),&#xA;        Document(text=&#34;even more data&#34;, id=&#34;2&#34;),&#xA;    ],&#xA;    SamplerFilter(rate=0.5),&#xA;    JsonlWriter(&#xA;        output_folder=&#34;/my/output/path&#34;&#xA;    )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Do note, however, that this iterable will not be sharded (if you launch more than 1 task they will all get the full iterable). This is usually useful for small workloads/testing.&lt;/p&gt; &#xA;&lt;h4&gt;Custom function&lt;/h4&gt; &#xA;&lt;p&gt;For simple processing you can simply pass in a custom function with the following signature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.data import DocumentsPipeline&#xA;&#xA;def uppercase_everything(data: DocumentsPipeline, rank: int = 0, world_size: int = 1) -&amp;gt; DocumentsPipeline:&#xA;    &#34;&#34;&#34;&#xA;        `data` is a generator of Document. You must also return a generator of Document (yield)&#xA;        You can optionally use `rank` and `world_size` for sharding&#xA;    &#34;&#34;&#34;&#xA;    for document in data:&#xA;        document.text = document.text.upper()&#xA;        yield document&#xA;&#xA;pipeline = [&#xA;    ...,&#xA;    uppercase_everything,&#xA;    ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You might have some pickling issues due to the imports. If this happens, simply move whatever imports you need inside the function body.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Custom block&lt;/h4&gt; &#xA;&lt;p&gt;You can also define a full block inheriting from &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/base.py&#34;&gt;&lt;code&gt;PipelineStep&lt;/code&gt;&lt;/a&gt; or one of its subclasses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datatrove.pipeline.base import PipelineStep&#xA;from datatrove.data import DocumentsPipeline&#xA;from datatrove.io import DataFolderLike, get_datafolder&#xA;&#xA;&#xA;class UppercaserBlock(PipelineStep):&#xA;    def __init__(self, some_folder: DataFolderLike, some_param: int = 5):&#xA;        super().__init__()&#xA;        # you can take whatever parameters you need and save them here&#xA;        self.some_param = some_param&#xA;        # to load datafolders use get_datafolder()&#xA;        self.some_folder = get_datafolder(some_folder)&#xA;&#xA;    def run(self, data: DocumentsPipeline, rank: int = 0, world_size: int = 1) -&amp;gt; DocumentsPipeline:&#xA;        # you could also load data from the `some_folder`:&#xA;        for filepath in self.some_folder.get_shard(rank, world_size): # it also accepts a glob pattern, among other things&#xA;            with self.some_folder.open(filepath, &#34;rt&#34;) as f:&#xA;                # do something&#xA;                ...&#xA;                yield doc&#xA;&#xA;        #&#xA;        # OR process data from previous blocks (`data`)&#xA;        #&#xA;&#xA;        for doc in data:&#xA;            with self.track_time():&#xA;                # you can wrap the main processing code in `track_time` to know how much each document took to process&#xA;                nr_uppercase_letters = sum(map(lambda c: c.isupper(), doc.text))&#xA;                # you can also keep track of stats per document using stat_update&#xA;                self.stat_update(&#34;og_upper_letters&#34;, value=nr_uppercase_letters)&#xA;                doc.text = doc.text.upper()&#xA;            # make sure you keep the yield outside the track_time block, or it will affect the time calculation&#xA;            yield doc&#xA;&#xA;        #&#xA;        # OR save data to disk&#xA;        #&#xA;&#xA;        with self.some_folder.open(&#34;myoutput&#34;, &#34;wt&#34;) as f:&#xA;            for doc in data:&#xA;                f.write(doc...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipeline = [&#xA;    ...,&#xA;    UppercaserBlock(&#34;somepath&#34;),&#xA;    ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You could also inherit from &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/extractors/base.py&#34;&gt;&lt;code&gt;BaseExtractor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/filters/base_filter.py&#34;&gt;&lt;code&gt;BaseFilter&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/readers/base.py&#34;&gt;&lt;code&gt;BaseReader&lt;/code&gt;/&lt;code&gt;BaseDiskReader&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/datatrove/main/src/datatrove/pipeline/writers/disk_base.py&#34;&gt;&lt;code&gt;DiskWriter&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:huggingface/datatrove.git &amp;amp;&amp;amp; cd datatrove&#xA;pip install -e &#34;.[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install pre-commit code style hooks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest -sv ./tests/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{penedo2024datatrove,&#xA;  author = {Penedo, Guilherme and Cappelli, Alessandro and Wolf, Thomas and Sasko, Mario},&#xA;  title = {DataTrove: large scale data processing},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  url = {https://github.com/huggingface/datatrove}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>