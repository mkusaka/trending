<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-11T01:34:25Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haoheliu/AudioLDM2</title>
    <updated>2023-08-11T01:34:25Z</updated>
    <id>tag:github.com,2023-08-11:/haoheliu/AudioLDM2</id>
    <link href="https://github.com/haoheliu/AudioLDM2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text-to-Audio/Music Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioLDM 2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2301.12503-brightgreen.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audioldm.github.io/audioldm2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo currently support Text-to-Audio Generation (including Music)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add the text-to-speech checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add the text-to-audio checkpoint that does not use FLAN-T5 Cross Attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open-source the AudioLDM 1 &amp;amp; 2 training code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support the generation of longer audio (&amp;gt; 10s)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimizing the inference speed of the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integration with the Diffusers library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web APP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare running environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n audioldm python=3.8; conda activate audioldm&#xA;pip3 install git+https://github.com/haoheliu/AudioLDM2.git&#xA;git clone https://github.com/haoheliu/AudioLDM2; cd AudioLDM2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start the web application (powered by Gradio)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;A link will be printed out. Click the link to open the browser and play.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Commandline Usage&lt;/h2&gt; &#xA;&lt;p&gt;Prepare running environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Optional&#xA;conda create -n audioldm python=3.8; conda activate audioldm&#xA;# Install AudioLDM&#xA;pip3 install git+https://github.com/haoheliu/AudioLDM2.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate based on a text prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate based on a list of text&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 -tl batch.lst&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Random Seed Matters&lt;/h2&gt; &#xA;&lt;p&gt;Sometimes model may not perform well (sounds wired or low quality) when changing into a different hardware. In this case, please adjust the random seed and find the optimal one for your hardware.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 --seed 1234 -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;You can choose model checkpoint by setting up &#34;model_name&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 --model_name &#34;audioldm2-full-large-650k&#34; -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have three checkpoints you can choose for now:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-full&lt;/strong&gt; (default): This checkpoint can perform both sound effect and music generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-music-665k&lt;/strong&gt;: This checkpoint is specialized on music generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-full-large-650k&lt;/strong&gt;: This checkpoint is the larger version of audioldm2-full.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Other options&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  usage: audioldm2 [-h] [-t TEXT] [-tl TEXT_LIST] [-s SAVE_PATH] [--model_name {audioldm2-full,audioldm2-music-665k,audioldm2-full-large-650k}] [-b BATCHSIZE] [--ddim_steps DDIM_STEPS] [-gs GUIDANCE_SCALE]&#xA;                  [-n N_CANDIDATE_GEN_PER_TEXT] [--seed SEED]&#xA;&#xA;  optional arguments:&#xA;    -h, --help            show this help message and exit&#xA;    -t TEXT, --text TEXT  Text prompt to the model for audio generation&#xA;    -tl TEXT_LIST, --text_list TEXT_LIST&#xA;                          A file that contains text prompt to the model for audio generation&#xA;    -s SAVE_PATH, --save_path SAVE_PATH&#xA;                          The path to save model output&#xA;    --model_name {audioldm2-full,audioldm2-music-665k,audioldm2-full-large-650k}&#xA;                          The checkpoint you gonna use&#xA;    -b BATCHSIZE, --batchsize BATCHSIZE&#xA;                          Generate how many samples at the same time&#xA;    --ddim_steps DDIM_STEPS&#xA;                          The sampling step for DDIM&#xA;    -gs GUIDANCE_SCALE, --guidance_scale GUIDANCE_SCALE&#xA;                          Guidance scale (Large =&amp;gt; better quality and relavancy to text; Small =&amp;gt; better diversity)&#xA;    -n N_CANDIDATE_GEN_PER_TEXT, --n_candidate_gen_per_text N_CANDIDATE_GEN_PER_TEXT&#xA;                          Automatic quality control. This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with&#xA;                          heavier computation&#xA;    --seed SEED           Change this value (any integer number) will lead to a different generation result.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite this work&lt;/h2&gt; &#xA;&lt;p&gt;If you found this tool useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;    AudioLDM 2 paper coming soon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023audioldm,&#xA;  title={AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},&#xA;  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},&#xA;  journal={arXiv preprint arXiv:2301.12503},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>charlesbel/Microsoft-Rewards-Farmer</title>
    <updated>2023-08-11T01:34:25Z</updated>
    <id>tag:github.com,2023-08-11:/charlesbel/Microsoft-Rewards-Farmer</id>
    <link href="https://github.com/charlesbel/Microsoft-Rewards-Farmer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple bot that uses selenium to farm Microsoft Rewards written in Python&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://forthebadge.com/images/badges/made-with-python.svg?sanitize=true&#34; alt=&#34;Made with Python&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/built-by-developers.svg?sanitize=true&#34; alt=&#34;Built by Developers&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/uses-git.svg?sanitize=true&#34; alt=&#34;Uses Git&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/built-with-love.svg?sanitize=true&#34; alt=&#34;Build with Love&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ascii&#34;&gt;███╗   ███╗███████╗    ███████╗ █████╗ ██████╗ ███╗   ███╗███████╗██████╗ &#xA;████╗ ████║██╔════╝    ██╔════╝██╔══██╗██╔══██╗████╗ ████║██╔════╝██╔══██╗&#xA;██╔████╔██║███████╗    █████╗  ███████║██████╔╝██╔████╔██║█████╗  ██████╔╝&#xA;██║╚██╔╝██║╚════██║    ██╔══╝  ██╔══██║██╔══██╗██║╚██╔╝██║██╔══╝  ██╔══██╗&#xA;██║ ╚═╝ ██║███████║    ██║     ██║  ██║██║  ██║██║ ╚═╝ ██║███████╗██║  ██║&#xA;╚═╝     ╚═╝╚══════╝    ╚═╝     ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═╝&#xA;       by Charles Bel (@charlesbel)          version 3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?style=for-the-badge&#34; alt=&#34;Maintained&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge&#34; alt=&#34;MIT&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;👋&lt;/span&gt; Welcome to the future of automation&lt;/h2&gt; &#xA;&lt;h3&gt;A simple bot that uses selenium to farm Microsoft Rewards written in Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- Use it at your own risk, Microsoft may ban your account (and I would not be responsible for it)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install requirements with the following command :&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have Chrome installed&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;del&gt;Install ChromeDriver:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;You no longer need to do this step since selenium &amp;gt;=4.10.0 include a webdriver manager&lt;/p&gt; &lt;p&gt;To update your selenium version, run this command : &lt;code&gt;pip install selenium --upgrade&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit the &lt;code&gt;accounts.json.sample&lt;/code&gt; with your accounts credentials and rename it by removing &lt;code&gt;.sample&lt;/code&gt; at the end (ex. &lt;code&gt;accounts.json&lt;/code&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you want to add more than one account, the syntax is the following:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    [&#xA;        {&#xA;            &#34;username&#34;: &#34;Your Email&#34;,&#xA;            &#34;password&#34;: &#34;Your Password&#34;&#xA;        },&#xA;        {&#xA;            &#34;username&#34;: &#34;Your Email&#34;,&#xA;            &#34;password&#34;: &#34;Your Password&#34;&#xA;        }&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bing searches (Desktop, Mobile and Edge) with User-Agents&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically the daily set&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically punch cards&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically the others promotions&lt;/li&gt; &#xA; &lt;li&gt;Headless Mode&lt;/li&gt; &#xA; &lt;li&gt;Multi-Account Management&lt;/li&gt; &#xA; &lt;li&gt;Session storing (3.0)&lt;/li&gt; &#xA; &lt;li&gt;2FA Support (3.0)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GUI&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>FlagOpen/FlagEmbedding</title>
    <updated>2023-08-11T01:34:25Z</updated>
    <id>tag:github.com,2023-08-11:/FlagOpen/FlagEmbedding</id>
    <link href="https://github.com/FlagOpen/FlagEmbedding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source Embeddings&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;FlagEmbedding&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/Contribution-Welcome-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/LICENSE-MIT-green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://huggingface.co/C-MTEB&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/C_MTEB-🤗-yellow&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/FlagEmbedding-1.0-red&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#model-list&#34;&gt;Model List&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#frequently-asked-questions&#34;&gt;FAQ&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#usage&#34;&gt;Usage&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#evaluation&#34;&gt;Evaluation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#train&#34;&gt;Train&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#contact&#34;&gt;Contact&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#license&#34;&gt;License&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/README_zh.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector database for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;************* 🌟&lt;strong&gt;Updates&lt;/strong&gt;🌟 *************&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;08/09/2023: BGE Models are integrated into &lt;strong&gt;Langchain&lt;/strong&gt;, you can use it like &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#using-langchain&#34;&gt;this&lt;/a&gt;; C-MTEB &lt;strong&gt;leaderboard&lt;/strong&gt; is &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;avaliable&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;08/05/2023: Release base-scale and small-scale models, &lt;strong&gt;best performance among the models of the same size 🤗&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;08/02/2023: Release &lt;code&gt;bge-large-*&lt;/code&gt;(short for BAAI General Embedding) Models, &lt;strong&gt;rank 1st on MTEB and C-MTEB benchmark!&lt;/strong&gt; &lt;span&gt;🎉&lt;/span&gt; &lt;span&gt;🎉&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;08/01/2023: We release the &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/C_MTEB&#34;&gt;Chinese Massive Text Embedding Benchmark&lt;/a&gt; (&lt;strong&gt;C-MTEB&lt;/strong&gt;), consisting of 31 test dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;bge&lt;/code&gt; is short for &lt;code&gt;BAAI general embedding&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Language&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;query instruction for retrieval*&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-en&#34;&gt;BAAI/bge-large-en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;🏆&lt;/span&gt; rank &lt;strong&gt;1st&lt;/strong&gt; in &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB&lt;/a&gt; leaderboard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Represent this sentence for searching relevant passages: &lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-base-en&#34;&gt;BAAI/bge-base-en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;rank &lt;strong&gt;2nd&lt;/strong&gt; in &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB&lt;/a&gt; leaderboard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Represent this sentence for searching relevant passages: &lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-small-en&#34;&gt;BAAI/bge-small-en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;a small-scale model but with competitive performance&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Represent this sentence for searching relevant passages: &lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-zh&#34;&gt;BAAI/bge-large-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;🏆&lt;/span&gt; rank &lt;strong&gt;1st&lt;/strong&gt; in &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C-MTEB&lt;/a&gt; benchmark&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;为这个句子生成表示以用于检索相关文章：&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-zh-noinstruct&#34;&gt;BAAI/bge-large-zh-noinstruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This model is trained without instruction, and rank &lt;strong&gt;2nd&lt;/strong&gt; in &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB&#34;&gt;C-MTEB&lt;/a&gt; benchmark&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-base-zh&#34;&gt;BAAI/bge-base-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;a base-scale model but has similar ability with &lt;code&gt;bge-large-zh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;为这个句子生成表示以用于检索相关文章：&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-small-zh&#34;&gt;BAAI/bge-small-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;a small-scale model but with competitive performance&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;为这个句子生成表示以用于检索相关文章：&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*: If you need to search the &lt;strong&gt;long&lt;/strong&gt; relevant passages to a &lt;strong&gt;short&lt;/strong&gt; query (s2p retrieval task), you need to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, &lt;strong&gt;no instruction&lt;/strong&gt; need to be added to passages.&lt;/p&gt; &#xA;&lt;h2&gt;Frequently asked questions&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The similarity score between two dissimilar sentence is higher than 0.5&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The similarity distribution of the current BGE model is not a uniform distribution over the interval [0-1]. So a similarity score greater than 0.5 does not necessarily indicate that the two sentence are similar. Especially for the similarity between short sentences, the similarity value of the current model will be high. &lt;strong&gt;If you need to filter similar sentences based on a similarity threshold, please select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;When do the query instruction need to be used&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For a retrieval task that uses short queries to find long related documents, it is recommended to add instructions for these short queries. For other tasks, it is recommended not to add instructions. For example, in Quora task, which needs to use a short question to search another related short questions, the instruction is not recommended to add. The best method to decide whether to add instructions for queries is choosing the setting which can achieve better performance in your task. In all cases, the documents/passages do not need to add the instruction, only need to consider whether to add the instruction for queries.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here are some examples to use &lt;code&gt;bge&lt;/code&gt; models with &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#using-flagembedding&#34;&gt;FlagEmbedding&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#using-sentence-transformers&#34;&gt;Sentence-Transformers&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#using-langchain&#34;&gt;Langchain&lt;/a&gt;, or &lt;a href=&#34;https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/#using-huggingface-transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Using FlagEmbedding&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U FlagEmbedding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If it doesn&#39;t work for you, you can see &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/FlagEmbedding/baai_general_embedding/README.md&#34;&gt;FlagEmbedding&lt;/a&gt; for more methods to install FlagEmbedding.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from FlagEmbedding import FlagModel&#xA;sentences = [&#34;样例数据-1&#34;, &#34;样例数据-2&#34;]&#xA;model = FlagModel(&#39;BAAI/bge-large-zh&#39;, query_instruction_for_retrieval=&#34;为这个句子生成表示以用于检索相关文章：&#34;)&#xA;embeddings_1 = model.encode(sentences)&#xA;embeddings_2 = model.encode(sentences)&#xA;similarity = embeddings_1 @ embeddings_2.T&#xA;print(similarity)&#xA;&#xA;# for s2p(short query to long passage) retrieval task, please use encode_queries() which will automatically add the instruction to each query&#xA;# corpus in retrieval task can still use encode() or encode_corpus(), since they don&#39;t need instruction&#xA;queries = [&#39;query_1&#39;, &#39;query_2&#39;]&#xA;passages = [&#34;样例文档-1&#34;, &#34;样例文档-2&#34;]&#xA;q_embeddings = model.encode_queries(queries)&#xA;p_embeddings = model.encode(passages)&#xA;scores = q_embeddings @ p_embeddings.T&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The value of argument &lt;code&gt;query_instruction_for_retrieval&lt;/code&gt; see &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list&#34;&gt;Model List&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FlagModel will use all available GPUs when encoding, please set &lt;code&gt;os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;]&lt;/code&gt; to choose GPU. You also can set &lt;code&gt;os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;]=&#34;&#34;&lt;/code&gt; to make GPUs unavailable.&lt;/p&gt; &#xA;&lt;h4&gt;Using Sentence-Transformers&lt;/h4&gt; &#xA;&lt;p&gt;Using this model also is easy when you have &lt;a href=&#34;https://www.SBERT.net&#34;&gt;sentence-transformers&lt;/a&gt; installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U sentence-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import SentenceTransformer&#xA;sentences = [&#34;样例数据-1&#34;, &#34;样例数据-2&#34;]&#xA;model = SentenceTransformer(&#39;BAAI/bge-large-zh&#39;)&#xA;embeddings_1 = model.encode(sentences, normalize_embeddings=True)&#xA;embeddings_2 = model.encode(sentences, normalize_embeddings=True)&#xA;similarity = embeddings_1 @ embeddings_2.T&#xA;print(similarity)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For s2p(short query to long passage) retrieval task, each short query should start with an instruction (instructions see &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list&#34;&gt;Model List&lt;/a&gt;). But the instruction is not needed for passages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import SentenceTransformer&#xA;queries = [&#39;query_1&#39;, &#39;query_2&#39;]&#xA;passages = [&#34;样例文档-1&#34;, &#34;样例文档-2&#34;]&#xA;instruction = &#34;为这个句子生成表示以用于检索相关文章：&#34;&#xA;&#xA;model = SentenceTransformer(&#39;BAAI/bge-large-zh&#39;)&#xA;q_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)&#xA;p_embeddings = model.encode(passages, normalize_embeddings=True)&#xA;scores = q_embeddings @ p_embeddings.T&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Langchain&lt;/h4&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;bge&lt;/code&gt; in langchain like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.embeddings import HuggingFaceBgeEmbeddings&#xA;model_name = &#34;BAAI/bge-small-en&#34;&#xA;model_kwargs = {&#39;device&#39;: &#39;cuda&#39;}&#xA;encode_kwargs = {&#39;normalize_embeddings&#39;: True} # set True to compute cosine similarity&#xA;model_norm = HuggingFaceBgeEmbeddings(&#xA;    model_name=model_name,&#xA;    model_kwargs=model_kwargs,&#xA;    encode_kwargs=encode_kwargs&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using HuggingFace Transformers&lt;/h4&gt; &#xA;&lt;p&gt;With transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of first token (i.e., [CLS]) as the sentence embedding.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModel&#xA;import torch&#xA;# Sentences we want sentence embeddings for&#xA;sentences = [&#34;样例数据-1&#34;, &#34;样例数据-2&#34;]&#xA;&#xA;# Load model from HuggingFace Hub&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;BAAI/bge-large-zh&#39;)&#xA;model = AutoModel.from_pretrained(&#39;BAAI/bge-large-zh&#39;)&#xA;&#xA;# Tokenize sentences&#xA;encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=&#39;pt&#39;)&#xA;# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)&#xA;# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors=&#39;pt&#39;)&#xA;&#xA;# Compute token embeddings&#xA;with torch.no_grad():&#xA;    model_output = model(**encoded_input)&#xA;    # Perform pooling. In this case, cls pooling.&#xA;    sentence_embeddings = model_output[0][:, 0]&#xA;# normalize embeddings&#xA;sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)&#xA;print(&#34;Sentence embeddings:&#34;, sentence_embeddings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;baai-general-embedding&lt;/code&gt; models achieve &lt;strong&gt;state-of-the-art performance on both MTEB and C-MTEB leaderboard!&lt;/strong&gt; More details and evaluation tools see our &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/C_MTEB/README.md&#34;&gt;scripts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MTEB&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dimension&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average (56)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval (15)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering (11)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pair Classification (3)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking (4)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS (10)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Summarization (1)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification (12)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-en&#34;&gt;&lt;strong&gt;bge-large-en&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;63.98&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;46.98&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.48&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.21&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-base-en&#34;&gt;&lt;strong&gt;bge-base-en&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/thenlper/gte-large&#34;&gt;gte-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.33&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/thenlper/gte-base&#34;&gt;gte-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/intfloat/e5-large-v2&#34;&gt;e5-large-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-small-en&#34;&gt;&lt;strong&gt;bge-small-en&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.82&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hkunlp/instructor-xl&#34;&gt;instructor-xl&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.79&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.79&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/intfloat/e5-base-v2&#34;&gt;e5-base-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/thenlper/gte-small&#34;&gt;gte-small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.46&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.89&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;text-embedding-ada-002&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1536&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8192&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.89&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/intfloat/e5-base-v2&#34;&gt;e5-small-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/sentence-transformers/sentence-t5-xxl&#34;&gt;sentence-t5-xxl&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/sentence-transformers/all-mpnet-base-v2&#34;&gt;all-mpnet-base-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;514&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco&#34;&gt;sgpt-bloom-7b1-msmarco&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.59&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.65&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2&#34;&gt;all-MiniLM-L12-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&#34;&gt;all-MiniLM-L6-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/nthakur/contriever-base-msmarco&#34;&gt;contriever-base-msmarco&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/sentence-transformers/sentence-t5-base&#34;&gt;sentence-t5-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;C-MTEB&lt;/strong&gt;:&lt;br&gt; We create a benchmark C-MTEB for chinese text embedding which consists of 31 datasets from 6 tasks. Please refer to &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/C_MTEB/README.md&#34;&gt;C_MTEB&lt;/a&gt; for a detailed introduction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Embedding dimension&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-zh&#34;&gt;&lt;strong&gt;bge-large-zh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;64.20&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.53&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.23&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;78.94&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.11&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-zh-noinstruct&#34;&gt;&lt;strong&gt;bge-large-zh-noinstruct&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.77&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;72.49&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.01&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-base-zh&#34;&gt;&lt;strong&gt;BAAI/bge-base-zh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-small-zh&#34;&gt;&lt;strong&gt;BAAI/bge-small-zh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.09&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/moka-ai/m3e-base&#34;&gt;m3e-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/moka-ai/m3e-large&#34;&gt;m3e-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings/what-are-embeddings&#34;&gt;text-embedding-ada-002(OpenAI)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1536&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/silk-road/luotuo-bert-medium&#34;&gt;luotuo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese&#34;&gt;text2vec&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.79&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.71&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese&#34;&gt;text2vec-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;This section will introduce the way we used to train the general embedding. The training scripts are in &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/FlagEmbedding/baai_general_embedding/README.md&#34;&gt;FlagEmbedding&lt;/a&gt;, and we provide some examples to do &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/examples/pretrain/README.md&#34;&gt;pre-train&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/examples/finetune/README.md&#34;&gt;fine-tune&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. RetroMAE Pre-train&lt;/strong&gt;&lt;br&gt; We pre-train the model following the method &lt;a href=&#34;https://github.com/staoxiao/RetroMAE&#34;&gt;retromae&lt;/a&gt;, which shows promising improvement in retrieval task (&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.35.pdf&#34;&gt;paper&lt;/a&gt;). The pre-training was conducted on 24 A100(40G) GPUs with a batch size of 720. In retromae, the mask ratio of encoder and decoder are 0.3, 0.5 respectively. We used the AdamW optimizer and the learning rate is 2e-5.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-training data&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;English: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;Pile&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/wikipedia&#34;&gt;wikipedia&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus&#34;&gt;msmarco&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Chinese: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/BAAI-WuDao/Data&#34;&gt;wudao&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Finetune&lt;/strong&gt;&lt;br&gt; We fine-tune the model using a contrastive objective. The format of input data is a triple&lt;code&gt;(query, positive, negative)&lt;/code&gt;. Besides the negative in the triple, we also adopt in-batch negatives strategy. We employ the cross-device negatives sharing method to share negatives among different GPUs, which can dramatically &lt;strong&gt;increase the number of negatives&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We trained our model on 48 A100(40G) GPUs with a large batch size of 32,768 (so there are &lt;strong&gt;65,535&lt;/strong&gt; negatives for each query in a batch). We used the AdamW optimizer and the learning rate is 1e-5. The temperature for contrastive loss is 0.01.&lt;/p&gt; &#xA;&lt;p&gt;Besides, we add instruction to the query for s2p(short query to long passage) retrieval task in the training (add nothing to passages). For English, the instruction is &lt;code&gt;Represent this sentence for searching relevant passages: &lt;/code&gt;; For Chinese, the instruction is &lt;code&gt;为这个句子生成表示以用于检索相关文章：&lt;/code&gt;. In the evaluation, the instruction should be added for queries in retrieval task, not be added for other tasks. Noted that the instruction is not needed for passages.&lt;/p&gt; &#xA;&lt;p&gt;The finetune script is accessible in this repository: &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/FlagEmbedding/baai_general_embedding/README.md&#34;&gt;FlagEmbedding&lt;/a&gt;. You can easily finetune your model with it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training data&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For English, we collect 230M text pairs from &lt;a href=&#34;https://huggingface.co/datasets/wikipedia&#34;&gt;wikipedia&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/cc_net&#34;&gt;cc-net&lt;/a&gt;, and so on.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For chinese, we collect 120M text pairs from &lt;a href=&#34;https://github.com/BAAI-WuDao/Data&#34;&gt;wudao&lt;/a&gt;, &lt;a href=&#34;https://github.com/CLUEbenchmark/SimCLUE&#34;&gt;simclue&lt;/a&gt; and so on.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;The data collection is to be released in the future.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Schedule&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Chinese Massive Text Embedding Benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release baai-general-embedding models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release codes for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Multilingual model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training Datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will continually update the embedding models and training codes, hoping to promote the development of the embedding model community.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question or suggestion related to this project, feel free to open an issue or pull a request. You also can email Shitao Xiao(&lt;a href=&#34;mailto:stxiao@baai.ac.cn&#34;&gt;stxiao@baai.ac.cn&lt;/a&gt;) and Zheng Liu(&lt;a href=&#34;mailto:liuzheng@baai.ac.cn&#34;&gt;liuzheng@baai.ac.cn&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;FlagEmbedding is licensed under &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;. The released models can be used for commercial purposes free of charge.&lt;/p&gt;</summary>
  </entry>
</feed>