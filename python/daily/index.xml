<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-12T01:36:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Significant-Gravitas/AutoGPT</title>
    <updated>2024-07-12T01:36:05Z</updated>
    <id>tag:github.com,2024-07-12:/Significant-Gravitas/AutoGPT</id>
    <link href="https://github.com/Significant-Gravitas/AutoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoGPT: build &amp;amp; use AI agents&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/autogpt&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/autogpt?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://twitter.com/Auto_GPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Auto_GPT?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AutoGPT&lt;/strong&gt; is a generalist LLM based AI agent that can autonomously accomplish minor tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Look up and summarize this research paper&lt;/li&gt; &#xA; &lt;li&gt;Write a marketing for food supplements&lt;/li&gt; &#xA; &lt;li&gt;Write a blog post detailing the news in AI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our mission is to provide the tools, so that you can focus on what matters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üèóÔ∏è &lt;strong&gt;Building&lt;/strong&gt; - Lay the foundation for something amazing.&lt;/li&gt; &#xA; &lt;li&gt;üß™ &lt;strong&gt;Testing&lt;/strong&gt; - Fine-tune your agent to perfection.&lt;/li&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Delegating&lt;/strong&gt; - Let AI work for you, and have your ideas come to life.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Be part of the revolution! &lt;strong&gt;AutoGPT&lt;/strong&gt; is here to stay, at the forefront of AI innovation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìñ &lt;a href=&#34;https://docs.agpt.co&#34;&gt;Documentation&lt;/a&gt;&lt;/strong&gt; ‚ÄÇ|‚ÄÇ &lt;strong&gt;üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/strong&gt; ‚ÄÇ|‚ÄÇ &lt;strong&gt;üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/FORGE-QUICKSTART.md&#34;&gt;Build your own Agent - Quickstart&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üß± Building blocks&lt;/h2&gt; &#xA;&lt;h3&gt;üèóÔ∏è Forge&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Forge your own agent!&lt;/strong&gt; ‚Äì Forge is a ready-to-go template for your agent application. All the boilerplate code is already handled, letting you channel all your creativity into the things that set &lt;em&gt;your&lt;/em&gt; agent apart. All tutorials are located &lt;a href=&#34;https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec&#34;&gt;here&lt;/a&gt;. Components from the &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/forge/forge/sdk&#34;&gt;&lt;code&gt;forge.sdk&lt;/code&gt;&lt;/a&gt; can also be used individually to speed up development and reduce boilerplate in your agent project.&lt;/p&gt; &#xA;&lt;p&gt;üöÄ &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/raw/master/forge/tutorials/001_getting_started.md&#34;&gt;&lt;strong&gt;Getting Started with Forge&lt;/strong&gt;&lt;/a&gt; ‚Äì This guide will walk you through the process of creating your own agent and using the benchmark and user interface.&lt;/p&gt; &#xA;&lt;p&gt;üìò &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/tree/master/forge&#34;&gt;Learn More&lt;/a&gt; about Forge&lt;/p&gt; &#xA;&lt;h3&gt;üéØ Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measure your agent&#39;s performance!&lt;/strong&gt; The &lt;code&gt;agbenchmark&lt;/code&gt; can be used with any agent that supports the agent protocol, and the integration with the project&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/#-cli&#34;&gt;CLI&lt;/a&gt; makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.&lt;/p&gt; &#xA;&lt;!-- TODO: insert visual demonstrating the benchmark --&gt; &#xA;&lt;p&gt;üì¶ &lt;a href=&#34;https://pypi.org/project/agbenchmark/&#34;&gt;&lt;code&gt;agbenchmark&lt;/code&gt;&lt;/a&gt; on Pypi ‚ÄÇ|‚ÄÇ üìò &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/raw/master/benchmark&#34;&gt;Learn More&lt;/a&gt; about the Benchmark&lt;/p&gt; &#xA;&lt;h3&gt;üíª UI&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Makes agents easy to use!&lt;/strong&gt; The &lt;code&gt;frontend&lt;/code&gt; gives you a user-friendly interface to control and monitor your agents. It connects to agents through the &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/#-agent-protocol&#34;&gt;agent protocol&lt;/a&gt;, ensuring compatibility with many agents from both inside and outside of our ecosystem.&lt;/p&gt; &#xA;&lt;!-- TODO: insert screenshot of front end --&gt; &#xA;&lt;p&gt;The frontend works out-of-the-box with all agents in the repo. Just use the &lt;a href=&#34;https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/#-cli&#34;&gt;CLI&lt;/a&gt; to run your agent of choice!&lt;/p&gt; &#xA;&lt;p&gt;üìò &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/tree/master/frontend&#34;&gt;Learn More&lt;/a&gt; about the Frontend&lt;/p&gt; &#xA;&lt;h3&gt;‚å®Ô∏è CLI&lt;/h3&gt; &#xA;&lt;p&gt;To make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ./run&#xA;Usage: cli.py [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;Options:&#xA;  --help  Show this message and exit.&#xA;&#xA;Commands:&#xA;  agent      Commands to create, start and stop agents&#xA;  benchmark  Commands to start the benchmark and list tests and categories&#xA;  setup      Installs dependencies needed for your system.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Just clone the repo, install dependencies with &lt;code&gt;./run setup&lt;/code&gt;, and you should be good to go!&lt;/p&gt; &#xA;&lt;h2&gt;ü§î Questions? Problems? Suggestions?&lt;/h2&gt; &#xA;&lt;h3&gt;Get help - &lt;a href=&#34;https://discord.gg/autogpt&#34;&gt;Discord üí¨&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/autogpt&#34;&gt;&lt;img src=&#34;https://invidget.switchblade.xyz/autogpt&#34; alt=&#34;Join us on Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To report a bug or request a feature, create a &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose&#34;&gt;GitHub Issue&lt;/a&gt;. Please ensure someone else hasn‚Äôt created an issue for the same topic.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Sister projects&lt;/h2&gt; &#xA;&lt;h3&gt;üîÑ Agent Protocol&lt;/h3&gt; &#xA;&lt;p&gt;To maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the &lt;a href=&#34;https://agentprotocol.ai/&#34;&gt;agent protocol&lt;/a&gt; standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#Significant-Gravitas/AutoGPT&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;amp;type=Date&#34;&gt; &#xA;   &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;amp;type=Date&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>urchade/GLiNER</title>
    <updated>2024-07-12T01:36:05Z</updated>
    <id>tag:github.com,2024-07-12:/urchade/GLiNER</id>
    <link href="https://github.com/urchade/GLiNER" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generalist and Lightweight Model for Named Entity Recognition (Extract any entity types from texts) @ NAACL 2024&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üëë GLiNER: Generalist and Lightweight Model for Named Entity Recognition&lt;/h1&gt; &#xA;&lt;p&gt;GLiNER is a Named Entity Recognition (NER) model capable of identifying any entity type using a bidirectional transformer encoder (BERT-like). It provides a practical alternative to traditional NER models, which are limited to predefined entities, and Large Language Models (LLMs) that, despite their flexibility, are costly and large for resource-constrained scenarios.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;: üìÑ &lt;a href=&#34;https://arxiv.org/abs/2311.08526&#34;&gt;GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Getting Started:&lt;/strong&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/drive/1mhalKWzmfSTqMnR0wQBZvt9-ktTsATHB?usp=sharing&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;List of available models&lt;/strong&gt;: ü§ó &lt;a href=&#34;https://huggingface.co/models?library=gliner&amp;amp;sort=trending&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo:&lt;/strong&gt; ü§ó &lt;a href=&#34;https://huggingface.co/spaces/urchade/gliner_mediumv2.1&#34;&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discord server:&lt;/strong&gt; &lt;a href=&#34;https://discord.gg/Y2yVxpSQnG&#34;&gt;https://discord.gg/Y2yVxpSQnG&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ† Installation &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;!pip install gliner&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;After the installation of the GLiNER library, import the &lt;code&gt;GLiNER&lt;/code&gt; class. Following this, you can load your chosen model with &lt;code&gt;GLiNER.from_pretrained&lt;/code&gt; and utilize &lt;code&gt;predict_entities&lt;/code&gt; to discern entities within your text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gliner import GLiNER&#xA;&#xA;# Initialize GLiNER with the base model&#xA;model = GLiNER.from_pretrained(&#34;urchade/gliner_mediumv2.1&#34;)&#xA;&#xA;# Sample text for entity prediction&#xA;text = &#34;&#34;&#34;&#xA;Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [k…æi ÉÀàtj…ênu  Å…îÀànaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d&#39;Or awards,[note 3] a record three UEFA Men&#39;s Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Labels for entity prediction&#xA;# Most GLiNER models should work best when entity types are in lower case or title case&#xA;labels = [&#34;Person&#34;, &#34;Award&#34;, &#34;Date&#34;, &#34;Competitions&#34;, &#34;Teams&#34;]&#xA;&#xA;# Perform entity prediction&#xA;entities = model.predict_entities(text, labels, threshold=0.5)&#xA;&#xA;# Display predicted entities and their labels&#xA;for entity in entities:&#xA;    print(entity[&#34;text&#34;], &#34;=&amp;gt;&#34;, entity[&#34;label&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Expected Output&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;Cristiano Ronaldo dos Santos Aveiro =&amp;gt; person&#xA;5 February 1985 =&amp;gt; date&#xA;Al Nassr =&amp;gt; teams&#xA;Portugal national team =&amp;gt; teams&#xA;Ballon d&#39;Or =&amp;gt; award&#xA;UEFA Men&#39;s Player of the Year Awards =&amp;gt; award&#xA;European Golden Shoes =&amp;gt; award&#xA;UEFA Champions Leagues =&amp;gt; competitions&#xA;UEFA European Championship =&amp;gt; competitions&#xA;UEFA Nations League =&amp;gt; competitions&#xA;European Championship =&amp;gt; competitions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üë®‚Äçüíª Model Authors&lt;/h2&gt; &#xA;&lt;p&gt;The model authors are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/urchade&#34;&gt;Urchade Zaratiana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nadi Tomeh&lt;/li&gt; &#xA; &lt;li&gt;Pierre Holat&lt;/li&gt; &#xA; &lt;li&gt;Thierry Charnois&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find GLiNER useful in your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zaratiana2023gliner,&#xA;      title={GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer}, &#xA;      author={Urchade Zaratiana and Nadi Tomeh and Pierre Holat and Thierry Charnois},&#xA;      year={2023},&#xA;      eprint={2311.08526},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Support and funding&lt;/h2&gt; &#xA;&lt;p&gt;This project has been supported and funded by &lt;strong&gt;FI Group&lt;/strong&gt; and &lt;strong&gt;Laboratoire Informatique de Paris Nord&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Over the past 20 years, &lt;a href=&#34;https://fr.fi-group.com&#34;&gt;FI Group&lt;/a&gt; has become a specialist in public funding strategies for R&amp;amp;D&amp;amp;I¬≤ (Research and Development, Innovation and Investment). FI Group&#39;s consultants, all engineers or PhDs, support customers from R&amp;amp;D through to the production of their innovations.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;logo/FI Group.png&#34; alt=&#34;FI Group&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We also extend our heartfelt gratitude to the open-source community for their invaluable contributions, which have been instrumental in the success of this project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>milesial/Pytorch-UNet</title>
    <updated>2024-07-12T01:36:05Z</updated>
    <id>tag:github.com,2024-07-12:/milesial/Pytorch-UNet</id>
    <link href="https://github.com/milesial/Pytorch-UNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of the U-Net for image semantic segmentation with high quality images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;U-Net: Semantic segmentation with PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/milesial/PyTorch-UNet/main.yml?logo=github&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/milesial/unet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docker%20image-available-blue?logo=Docker&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PyTorch-v1.13+-red.svg?logo=PyTorch&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-v3.6+-blue.svg?logo=python&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/GD8FcB7.png&#34; alt=&#34;input and output for a random image in the test dataset&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Customized implementation of the &lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34;&gt;U-Net&lt;/a&gt; in PyTorch for Kaggle&#39;s &lt;a href=&#34;https://www.kaggle.com/c/carvana-image-masking-challenge&#34;&gt;Carvana Image Masking Challenge&lt;/a&gt; from high definition images.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#quick-start&#34;&gt;Quick start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#without-docker&#34;&gt;Without Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#with-docker&#34;&gt;With Docker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#prediction&#34;&gt;Prediction&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#weights--biases&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#pretrained-model&#34;&gt;Pretrained model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milesial/Pytorch-UNet/master/#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Without Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;Install CUDA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Install PyTorch 1.13 or later&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download the data and run training:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_data.sh&#xA;python train.py --amp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;With Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Install Docker 19.03 or later:&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://get.docker.com | sh &amp;amp;&amp;amp; sudo systemctl --now enable docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;Install the NVIDIA container toolkit:&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \&#xA;   &amp;amp;&amp;amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \&#xA;   &amp;amp;&amp;amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list&#xA;sudo apt-get update&#xA;sudo apt-get install -y nvidia-docker2&#xA;sudo systemctl restart docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/milesial/unet&#34;&gt;Download and run the image:&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download the data and run training:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_data.sh&#xA;python train.py --amp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This model was trained from scratch with 5k images and scored a &lt;a href=&#34;https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient&#34;&gt;Dice coefficient&lt;/a&gt; of 0.988423 on over 100k test images.&lt;/p&gt; &#xA;&lt;p&gt;It can be easily used for multiclass segmentation, portrait segmentation, medical segmentation, ...&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note : Use Python 3.6 or newer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;A docker image containing the code and the dependencies is available on &lt;a href=&#34;https://hub.docker.com/repository/docker/milesial/unet&#34;&gt;DockerHub&lt;/a&gt;. You can download and jump in the container with (&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;docker &amp;gt;=19.03&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; python train.py -h&#xA;usage: train.py [-h] [--epochs E] [--batch-size B] [--learning-rate LR]&#xA;                [--load LOAD] [--scale SCALE] [--validation VAL] [--amp]&#xA;&#xA;Train the UNet on images and target masks&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --epochs E, -e E      Number of epochs&#xA;  --batch-size B, -b B  Batch size&#xA;  --learning-rate LR, -l LR&#xA;                        Learning rate&#xA;  --load LOAD, -f LOAD  Load model from a .pth file&#xA;  --scale SCALE, -s SCALE&#xA;                        Downscaling factor of the images&#xA;  --validation VAL, -v VAL&#xA;                        Percent of the data that is used as validation (0-100)&#xA;  --amp                 Use mixed precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the &lt;code&gt;scale&lt;/code&gt; is 0.5, so if you wish to obtain better results (but use more memory), set it to 1.&lt;/p&gt; &#xA;&lt;p&gt;Automatic mixed precision is also available with the &lt;code&gt;--amp&lt;/code&gt; flag. &lt;a href=&#34;https://arxiv.org/abs/1710.03740&#34;&gt;Mixed precision&lt;/a&gt; allows the model to use less memory and to be faster on recent GPUs by using FP16 arithmetic. Enabling AMP is recommended.&lt;/p&gt; &#xA;&lt;h3&gt;Prediction&lt;/h3&gt; &#xA;&lt;p&gt;After training your model and saving it to &lt;code&gt;MODEL.pth&lt;/code&gt;, you can easily test the output masks on your images via the CLI.&lt;/p&gt; &#xA;&lt;p&gt;To predict a single image and save it:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python predict.py -i image.jpg -o output.jpg&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To predict a multiple images and show them without saving them:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python predict.py -i image1.jpg image2.jpg --viz --no-save&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; python predict.py -h&#xA;usage: predict.py [-h] [--model FILE] --input INPUT [INPUT ...] &#xA;                  [--output INPUT [INPUT ...]] [--viz] [--no-save]&#xA;                  [--mask-threshold MASK_THRESHOLD] [--scale SCALE]&#xA;&#xA;Predict masks from input images&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --model FILE, -m FILE&#xA;                        Specify the file in which the model is stored&#xA;  --input INPUT [INPUT ...], -i INPUT [INPUT ...]&#xA;                        Filenames of input images&#xA;  --output INPUT [INPUT ...], -o INPUT [INPUT ...]&#xA;                        Filenames of output images&#xA;  --viz, -v             Visualize the images as they are processed&#xA;  --no-save, -n         Do not save the output masks&#xA;  --mask-threshold MASK_THRESHOLD, -t MASK_THRESHOLD&#xA;                        Minimum probability value to consider a mask pixel white&#xA;  --scale SCALE, -s SCALE&#xA;                        Scale factor for the input images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify which model file to use with &lt;code&gt;--model MODEL.pth&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Weights &amp;amp; Biases&lt;/h2&gt; &#xA;&lt;p&gt;The training progress can be visualized in real-time using &lt;a href=&#34;https://wandb.ai/&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;. Loss curves, validation curves, weights and gradient histograms, as well as predicted masks are logged to the platform.&lt;/p&gt; &#xA;&lt;p&gt;When launching a training, a link will be printed in the console. Click on it to go to your dashboard. If you have an existing W&amp;amp;B account, you can link it by setting the &lt;code&gt;WANDB_API_KEY&lt;/code&gt; environment variable. If not, it will create an anonymous run which is automatically deleted after 7 days.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained model&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/milesial/Pytorch-UNet/releases/tag/v3.0&#34;&gt;pretrained model&lt;/a&gt; is available for the Carvana dataset. It can also be loaded from torch.hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net = torch.hub.load(&#39;milesial/Pytorch-UNet&#39;, &#39;unet_carvana&#39;, pretrained=True, scale=0.5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available scales are 0.5 and 1.0.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;The Carvana data is available on the &lt;a href=&#34;https://www.kaggle.com/c/carvana-image-masking-challenge/data&#34;&gt;Kaggle website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also download it using the helper script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The input images and target masks should be in the &lt;code&gt;data/imgs&lt;/code&gt; and &lt;code&gt;data/masks&lt;/code&gt; folders respectively (note that the &lt;code&gt;imgs&lt;/code&gt; and &lt;code&gt;masks&lt;/code&gt; folder should not contain any sub-folder or any other files, due to the greedy data-loader). For Carvana, images are RGB and masks are black and white.&lt;/p&gt; &#xA;&lt;p&gt;You can use your own dataset as long as you make sure it is loaded properly in &lt;code&gt;utils/data_loading.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Original paper by Olaf Ronneberger, Philipp Fischer, Thomas Brox:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jeDVpqF.png&#34; alt=&#34;network architecture&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>