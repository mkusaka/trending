<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-18T01:39:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xaitax/SploitScan</title>
    <updated>2024-01-18T01:39:54Z</updated>
    <id>tag:github.com,2024-01-18:/xaitax/SploitScan</id>
    <link href="https://github.com/xaitax/SploitScan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SploitScan is a sophisticated cybersecurity utility designed to provide detailed information on vulnerabilities and associated proof-of-concept (PoC) exploits.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SploitScan&lt;/h1&gt; &#xA;&lt;h2&gt;üìú Description&lt;/h2&gt; &#xA;&lt;p&gt;SploitScan is a powerful and user-friendly tool designed to streamline the process of identifying exploits for known vulnerabilities and their respective exploitation probability. Empowering cybersecurity professionals with the capability to swiftly identify and apply known and test exploits. It&#39;s particularly valuable for professionals seeking to enhance their security measures or develop robust detection strategies against emerging threats.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CVE Information Retrieval&lt;/strong&gt;: Fetches CVE details from the National Vulnerability Database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EPSS Integration&lt;/strong&gt;: Includes Exploit Prediction Scoring System (EPSS) data, offering a probability score for the likelihood of CVE exploitation, aiding in prioritization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PoC Exploits Aggregation&lt;/strong&gt;: Gathers publicly available PoC exploits, enhancing the understanding of vulnerabilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CISA KEV&lt;/strong&gt;: Shows if the CVE has been listed in the Known Exploited Vulnerabilities (KEV) of CISA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patching Priority System&lt;/strong&gt;: Evaluates and assigns a priority rating for patching based on various factors including public exploits availability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-CVE Support and Export Options&lt;/strong&gt;: Supports multiple CVEs in a single run and allows exporting the results to JSON and CSV formats.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-Friendly Interface&lt;/strong&gt;: Easy to use, providing clear and concise information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Security Tool&lt;/strong&gt;: Ideal for quick security assessments and staying informed about recent vulnerabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;img width=&#34;867&#34; alt=&#34;image&#34; src=&#34;https://github.com/xaitax/SploitScan/assets/5014849/1b62ba53-9bf4-498f-bd39-469aca695c83&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Regular&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sploitscan.py CVE-YYYY-NNNNN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enter one or more CVE IDs to fetch data. Separate multiple CVE IDs with spaces.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sploitscan.py CVE-YYYY-NNNNN CVE-YYYY-NNNNN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional: Export the results to a JSON or CSV file. Specify the format: &#39;json&#39; or &#39;csv&#39;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sploitscan.py CVE-YYYY-NNNNN -e JSON&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ°Ô∏è Patching Prioritization System&lt;/h2&gt; &#xA;&lt;p&gt;The Patching Prioritization System in SploitScan provides a strategic approach to prioritizing security patches based on the severity and exploitability of vulnerabilities. It&#39;s influenced by the model from &lt;a href=&#34;https://github.com/TURROKS/CVE_Prioritizer&#34;&gt;CVE Prioritizer&lt;/a&gt;, with enhancements for handling publicly available exploits. Here&#39;s how it works:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A+ Priority: Assigned to CVEs listed in CISA&#39;s KEV or those with publicly available exploits. This reflects the highest risk and urgency for patching.&lt;/li&gt; &#xA; &lt;li&gt;A to D Priority: Based on a combination of CVSS scores and EPSS probability percentages. The decision matrix is as follows: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A: CVSS score &amp;gt;= 6.0 and EPSS score &amp;gt;= 0.2. High severity with a significant probability of exploitation.&lt;/li&gt; &#xA;   &lt;li&gt;B: CVSS score &amp;gt;= 6.0 but EPSS score &amp;lt; 0.2. High severity but lower probability of exploitation.&lt;/li&gt; &#xA;   &lt;li&gt;C: CVSS score &amp;lt; 6.0 and EPSS score &amp;gt;= 0.2. Lower severity but higher probability of exploitation.&lt;/li&gt; &#xA;   &lt;li&gt;D: CVSS score &amp;lt; 6.0 and EPSS score &amp;lt; 0.2. Lower severity and lower probability of exploitation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This system assists users in making informed decisions on which vulnerabilities to patch first, considering both their potential impact and the likelihood of exploitation. Thresholds can be changed to your business needs.&lt;/p&gt; &#xA;&lt;h2&gt;üìÜ Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;[15th January 2024] - Enhancement Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple CVE Support&lt;/strong&gt;: Now capable of handling multiple CVE IDs in a single execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;JSON and CSV Export&lt;/strong&gt;: Added functionality to export results to JSON and CSV files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced CVE Display&lt;/strong&gt;: Improved visual differentiation and information layout for each CVE.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patching Priority System&lt;/strong&gt;: Introduced a priority rating system for patching, influenced by various factors including the availability of public exploits.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;[13th January 2024] - Initial Release&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial release of SploitScan.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome. Please feel free to fork, modify, and make pull requests or report issues.&lt;/p&gt; &#xA;&lt;h2&gt;üìå Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alexander Hagenah&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://primepage.de&#34;&gt;URL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/xaitax&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëè Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvd.nist.gov/developers/vulnerabilities&#34;&gt;NIST NVD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.first.org/epss/api&#34;&gt;FIRST EPSS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cisa.gov/known-exploited-vulnerabilities-catalog&#34;&gt;CISA Known Exploited Vulnerabilities Catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://poc-in-github.motikan2010.net/&#34;&gt;nomi-sec PoC-in-GitHub API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This tool is meant for educational and professional purposes only.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>netease-youdao/QAnything</title>
    <updated>2024-01-18T01:39:54Z</updated>
    <id>tag:github.com,2024-01-18:/netease-youdao/QAnything</id>
    <link href="https://github.com/netease-youdao/QAnything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Question and Answer based on Anything.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt; &#xA;  &lt;!-- Please provide path to your logo here --&gt; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_logo.png&#34; alt=&#34;Logo&#34; width=&#34;800&#34;&gt; &lt;/a&gt; &#xA; &lt;h1&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://qanything.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-qanything.ai-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://read.youdao.com#/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-read.youdao.com-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/netease-youdao/QAnything/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#What-is-QAnything&#34;&gt;What is QAnything&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Key-features&#34;&gt;Key features&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#API-Document&#34;&gt;API Document&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#WeChat-Group&#34;&gt;WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Acknowledgments&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What is QAnything?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt; (&lt;code&gt;QAnything&lt;/code&gt;) is a local knowledge base question-answering system designed to support a wide range of file formats and databases, allowing for offline installation and use.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;QAnything&lt;/code&gt;, you can simply drop any locally stored file of any format and receive accurate, fast, and reliable answers.&lt;/p&gt; &#xA;&lt;p&gt;Currently supported formats include: &lt;strong&gt;PDF, Word (doc/docx), PPT, Markdown, Eml, TXT, Images (jpg, png, etc.), Web links&lt;/strong&gt; and more formats coming soon‚Ä¶&lt;/p&gt; &#xA;&lt;h3&gt;Key features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Security&lt;/strong&gt;, supports installation and usage with network cable unplugged throughout the process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-language QA support&lt;/strong&gt;, freely switch between Chinese and English QA, regardless of the language of the document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supports massive data QA&lt;/strong&gt;, two-stage retrieval ranking, solving the degradation problem of large-scale data retrieval; the more data, the better the performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance production-grade system&lt;/strong&gt;, directly deployable for enterprise applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-friendly&lt;/strong&gt;, no need for cumbersome configurations, one-click installation and deployment, ready to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi knowledge base QA&lt;/strong&gt; Support selecting multiple knowledge bases for Q&amp;amp;A&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_arch.png&#34; width=&#34;700&#34; alt=&#34;qanything_system&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Why 2 stage retrieval?&lt;/h4&gt; &#xA;&lt;p&gt;In scenarios with a large volume of knowledge base data, the advantages of a two-stage approach are very clear. If only a first-stage embedding retrieval is used, there will be a problem of retrieval degradation as the data volume increases, as indicated by the green line in the following graph. However, after the second-stage reranking, there can be a stable increase in accuracy, &lt;strong&gt;the more data, the better the performance&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/two_stage_retrieval.jpg&#34; width=&#34;500&#34; alt=&#34;two stage retrievaal&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;QAnything uses the retrieval component &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;, which is distinguished for its bilingual and crosslingual proficiency. BCEmbedding excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A high performence on &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-semantic-representation-by-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-rag-by-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1st RetrievalÔºàembeddingÔºâ&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;jina-embeddings-v2-base-en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.73&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.95&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;59.43&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluation Summary&lt;/a&gt;„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2nd RetrievalÔºàrerankÔºâ&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluation Summary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;RAG Evaluations in LlamaIndexÔºàembedding and rerankÔºâ&lt;/h4&gt; &#xA;&lt;img src=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/assets/rag_eval_multiple_domains_summary.jpg&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use embedding and rerank separately, please refer to &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LLM&lt;/h4&gt; &#xA;&lt;p&gt;The open source version of QAnything is based on QwenLM and has been fine-tuned on a large number of professional question-answering datasets. It greatly enhances the ability of question-answering. If you need to use it for commercial purposes, please follow the license of QwenLM. For more details, please refer to: &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;QwenLM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qanything.ai&#34;&gt;&lt;span&gt;üëâ&lt;/span&gt; try QAnything online&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required item&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Minimum Requirement&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NVIDIA GPU Memory&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 16GB&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA 3090 recommended&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NVIDIA Driver Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 525.105.17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 12.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;docker compose version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 2.12.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;docker compose install&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;step1: pull qanything repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/netease-youdao/QAnything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;step2: download the model and unzip it to the root directory of the current project.&lt;/h4&gt; &#xA;&lt;p&gt;This project provides multiple model download platforms. Choose one of the methods for downloading.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wisemodel.cn/models/Netease_Youdao/qanything&#34;&gt;üëâ„ÄêWiseModel„Äë&lt;/a&gt; &lt;a href=&#34;https://www.modelscope.cn/models/netease-youdao/QAnything&#34;&gt;üëâ„ÄêModelScope„Äë&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/netease-youdao/QAnything&#34;&gt;üëâ„ÄêHuggingFace„Äë&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Download method 1ÔºöWiseModelÔºàrecommendüëçÔºâ&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;cd QAnything&#xA;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://www.wisemodel.cn/Netease_Youdao/qanything.git&#xA;unzip qanything/models.zip   # in root directory of the current project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Download method 2ÔºöModelScope&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;cd QAnything&#xA;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://www.modelscope.cn/netease-youdao/QAnything.git&#xA;unzip QAnything/models.zip   # in root directory of the current project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Download method 3ÔºöHuggingFace&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;cd QAnything&#xA;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/netease-youdao/QAnything&#xA;unzip QAnything/models.zip   # in root directory of the current project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;step3: change config&lt;/h4&gt; &#xA;&lt;h5&gt;in the Windows system&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim docker-compose-windows.yaml # change CUDA_VISIBLE_DEVICES to your gpu device id&#xA;vim front_end/.env.production # set the excetly host.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;in the Linux system&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim docker-compose-linux.yaml # change CUDA_VISIBLE_DEVICES to your gpu device id&#xA;vim front_end/.env.production # set the excetly host.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;step4: start server&lt;/h4&gt; &#xA;&lt;h5&gt;in the Windows system&lt;/h5&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Beginner&#39;s recommendation!&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Front desk startup, log prints to the screen in real time, press ctrl+c to stop.&#xA;docker-compose -f docker-compose-windows.yaml up qanything_local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Recommended for experienced players!&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Background startup, ctrl+c will not stop.&#xA;docker-compose -f docker-compose-windows.yaml up -d&#xA;# Execute the following command to view the log.&#xA;docker-compose -f docker-compose-windows.yaml logs qanything_local&#xA;# Stop service&#xA;docker-compose -f docker-compose-windows.yaml down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h5&gt;in the Linux system&lt;/h5&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Beginner&#39;s recommendation!&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Front desk startup, log prints to the screen in real time, press ctrl+c to stop.&#xA;docker-compose -f docker-compose-linux.yaml up qanything_local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Recommended for experienced players!&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Background startup, ctrl+c will not stop.&#xA;docker-compose -f docker-compose-linux.yaml up -d&#xA;# Execute the following command to view the log.&#xA;docker-compose -f docker-compose-linux.yaml logs qanything_local&#xA;# Stop service&#xA;docker-compose -f docker-compose-linux.yaml down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;After successful installation, you can experience the application by entering the following addresses in your web browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Frontend address: http://{your_host}:5052/qanything/&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;API address: http://{your_host}:5052/api/&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed API documentation, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API ÊñáÊ°£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Cross-lingual: Multiple English paper Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/8915277f-c136-42b8-9332-78f64bf5df22&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/multi_paper_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Information extraction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/b9e3be94-183b-4143-ac49-12fa005a8a9a&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/information_extraction.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Various files&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/7ede63c1-4c7f-4557-bd2c-7c51a44c8e0b&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/various_files_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/d30942f7-6dbd-4013-a4b6-82f7c2a5fbee&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/web_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API Document&lt;/h3&gt; &#xA;&lt;p&gt;If you need to access the API, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;WeChat Group&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to scan the QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/Wechat.jpg&#34; width=&#34;20%&#34; height=&#34;auto&#34;&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Reach out to the maintainer at one of the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/issues&#34;&gt;Github issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Contact options listed on &lt;a href=&#34;https://github.com/netease-youdao&#34;&gt;this GitHub profile&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#netease-youdao/QAnything&amp;amp;netease-youdao/BCEmbedding&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=netease-youdao/QAnything,netease-youdao/BCEmbedding&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; adopts dependencies from the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to our &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt; for the excellent embedding and rerank model.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt; for strong base language models.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/triton-inference-server/server&#34;&gt;Triton Inference Server&lt;/a&gt; for providing great open source inference serving.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt; for highly optimized LLM inference backend.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;Langchain&lt;/a&gt; for the wonderful llm application framework.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat&lt;/a&gt; for the inspiration provided on local knowledge base Q&amp;amp;A.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/milvus-io/milvus&#34;&gt;Milvus&lt;/a&gt; for the excellent semantic search library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt; for its ease-to-use OCR library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/sanic-org/sanic&#34;&gt;Sanic&lt;/a&gt; for the powerful web service framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>piddnad/DDColor</title>
    <updated>2024-01-18T01:39:54Z</updated>
    <id>tag:github.com,2024-01-18:/piddnad/DDColor</id>
    <link href="https://github.com/piddnad/DDColor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] Official implementation of &#34;DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üé® DDColor&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch implementation of ICCV 2023 Paper &#34;DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11613&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.11613-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/piddnad/DDColor-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-FF8000&#34; alt=&#34;HuggingFace&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_ddcolor_image-colorization/summary&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%91%BE%20ModelScope-Demo-8A2BE2&#34; alt=&#34;ModelScope demo&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=piddnad/DDColor&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Xiaoyang Kang, Tao Yang, Wenqi Ouyang, Peiran Ren, Lingzhi Li, Xuansong Xie&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;DAMO Academy, Alibaba Group&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;ü™Ñ DDColor can provide vivid and natural colorization for historical black and white old photos.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/piddnad/DDColor/master/assets/teaser.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;üé≤ It can even colorize/recolor landscapes from anime games, transforming your animated scenery into a realistic real-life style! (Image source: Genshin Impact)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/piddnad/DDColor/master/assets/anime_landscapes.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023-12-13] Release the DDColor-tiny pre-trained model!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023-09-07] Add the Model Zoo and release three pretrained models!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023-05-15] Code release for training and inference!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023-05-05] The online demo is available!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide &lt;a href=&#34;https://modelscope.cn/models/damo/cv_ddcolor_image-colorization/summary&#34;&gt;online demo&lt;/a&gt; via ModelScope. Feel free to try it out!&lt;/p&gt; &#xA;&lt;h2&gt;Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;In short:&lt;/em&gt; DDColor uses multi-scale visual features to optimize &lt;strong&gt;learnable color tokens&lt;/strong&gt; (i.e. color queries) and achieves state-of-the-art performance on automatic image colorization.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/piddnad/DDColor/master/assets/network_arch.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7&lt;/li&gt; &#xA; &lt;li&gt;PyTorch &amp;gt;= 1.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install with conda (Recommend)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n ddcolor python=3.8&#xA;conda activate ddcolor&#xA;pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html&#xA;pip install -r requirements.txt&#xA;&#xA;python3 setup.py develop  # install basicsr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Inference with modelscope library&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install modelscope:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;modelscope[cv]&#34; -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the following codes:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;import cv2&#xA;from modelscope.outputs import OutputKeys&#xA;from modelscope.pipelines import pipeline&#xA;from modelscope.utils.constant import Tasks&#xA;&#xA;img_colorization = pipeline(Tasks.image_colorization, model=&#39;damo/cv_ddcolor_image-colorization&#39;)&#xA;result = img_colorization(&#39;https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/audrey_hepburn.jpg&#39;)&#xA;cv2.imwrite(&#39;result.png&#39;, result[OutputKeys.OUTPUT_IMG])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will automatically download the DDColor models.&lt;/p&gt; &#xA;&lt;p&gt;You can find the model file &lt;code&gt;pytorch_model.pt&lt;/code&gt; in the local path ~/.cache/modelscope/hub/damo.&lt;/p&gt; &#xA;&lt;h3&gt;Inference from local script&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the pretrained model file by simply running:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;from modelscope.hub.snapshot_download import snapshot_download&#xA;&#xA;model_dir = snapshot_download(&#39;damo/cv_ddcolor_image-colorization&#39;, cache_dir=&#39;./modelscope&#39;)&#xA;print(&#39;model assets saved to %s&#39;%model_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then the weights will be &lt;code&gt;modelscope/damo/cv_ddcolor_image-colorization/pytorch_model.pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or, download the model from &lt;a href=&#34;https://huggingface.co/piddnad/DDColor-models&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/inference.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;We provide several different versions of pretrained models, please check out &lt;a href=&#34;https://raw.githubusercontent.com/piddnad/DDColor/master/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Dataset preparation: download &lt;a href=&#34;https://www.image-net.org/&#34;&gt;ImageNet&lt;/a&gt; dataset, or prepare any custom dataset of your own. Use the following script to get the dataset list file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python data_list/get_meta_file.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weights for &lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth&#34;&gt;ConvNeXt&lt;/a&gt; and &lt;a href=&#34;https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth&#34;&gt;InceptionV3&lt;/a&gt; and put it into &lt;code&gt;pretrain&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Specify &#39;meta_info_file&#39; and other options in &lt;code&gt;options/train/train_ddcolor.yml&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work is helpful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{kang2023ddcolor,&#xA;  title={DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders},&#xA;  author={Kang, Xiaoyang and Yang, Tao and Ouyang, Wenqi and Ren, Peiran and Li, Lingzhi and Xie, Xuansong},&#xA;  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},&#xA;  pages={328--338},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We thank the authors of BasicSR for the awesome training pipeline.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Xintao Wang, Ke Yu, Kelvin C.K. Chan, Chao Dong and Chen Change Loy. BasicSR: Open Source Image and Video Restoration Toolbox. &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;https://github.com/xinntao/BasicSR&lt;/a&gt;, 2020.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Some codes are adapted from &lt;a href=&#34;https://github.com/jixiaozhong/ColorFormer&#34;&gt;ColorFormer&lt;/a&gt;, &lt;a href=&#34;https://github.com/KIMGEONUNG/BigColor&#34;&gt;BigColor&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/ConvNeXt&#34;&gt;ConvNeXt&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/Mask2Former&#34;&gt;Mask2Former&lt;/a&gt;, and &lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;DETR&lt;/a&gt;. Thanks for their excellent work!&lt;/p&gt;</summary>
  </entry>
</feed>