<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-01T01:43:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AabyssZG/SpringBoot-Scan</title>
    <updated>2023-02-01T01:43:36Z</updated>
    <id>tag:github.com,2023-02-01:/AabyssZG/SpringBoot-Scan</id>
    <link href="https://github.com/AabyssZG/SpringBoot-Scan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;针对SpringBoot的开源渗透框架，以及高危漏洞利用工具&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SpringBoot-Scan&lt;/h1&gt; &#xA;&lt;p&gt;日常渗透过程中，经常会碰到Spring Boot搭建的微服务，于是就想做一个针对Spring Boot的开源渗透框架，主要用作扫描SpringBoot的敏感信息泄露端点，并可以直接测试Spring Boot的相关高危漏洞。&lt;/p&gt; &#xA;&lt;p&gt;于是，就写了这么一个工具：SpringBoot-Scan 【简称：“SB-Scan”（错乱】&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;同时，后期将加入更多漏洞利用内置模块（各位师傅能不能赏个Star嘛~码代码挺辛苦的哈哈）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;我还整理了一篇SpringBootd的相关渗透姿势在我的个人博客，欢迎各位师傅前来交流哈哈：&lt;a href=&#34;https://blog.zgsec.cn/index.php/archives/129/&#34;&gt;https://blog.zgsec.cn/index.php/archives/129/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;工具使用&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;  ______                       __                      _______                        __&#xA; /      \                     |  \                    |       \                      |  \&#xA;|  $$$$$$\  ______    ______   \$$ _______    ______  | $$$$$$$\  ______    ______  _| $$_&#xA;| $$___\$$ /      \  /      \ |  \|       \  /      \ | $$__/ $$ /      \  /      \|   $$ \&#xA; \$$    \ |  $$$$$$\|  $$$$$$\| $$| $$$$$$$\|  $$$$$$\| $$    $$|  $$$$$$\|  $$$$$$\\$$$$$$&#xA; _\$$$$$$\| $$  | $$| $$   \$$| $$| $$  | $$| $$  | $$| $$$$$$$\| $$  | $$| $$  | $$ | $$ __&#xA;|  \__| $$| $$__/ $$| $$      | $$| $$  | $$| $$__| $$| $$__/ $$| $$__/ $$| $$__/ $$ | $$|  \&#xA; \$$    $$| $$    $$| $$      | $$| $$  | $$ \$$    $$| $$    $$ \$$    $$ \$$    $$  \$$  $$&#xA;  \$$$$$$ | $$$$$$$  \$$       \$$ \$$   \$$ _\$$$$$$$ \$$$$$$$   \$$$$$$   \$$$$$$    \$$$$&#xA;          | $$                              |  \__| $$&#xA;          | $$                               \$$    $$&#xA;           \$$                                \$$$$$$&#xA;            ______&#xA;           /      \&#xA;          |  $$$$$$\  _______  ______   _______      +-------------------------------------+&#xA;          | $$___\$$ /       \|      \ |       \     +                                     +&#xA;           \$$    \ |  $$$$$$$ \$$$$$$\| $$$$$$$\    + Version: 1.02                       +&#xA;           _\$$$$$$\| $$      /      $$| $$  | $$    + Author: 曾哥(@AabyssZG)             +&#xA;          |  \__| $$| $$_____|  $$$$$$$| $$  | $$    + Whoami: https://github.com/AabyssZG +&#xA;           \$$    $$ \$$     \\$$    $$| $$  | $$    +                                     +&#xA;            \$$$$$$   \$$$$$$$ \$$$$$$$ \$$   \$$    +-------------------------------------+&#xA;&#xA;&#xA;&#xA;&#xA;用法:&#xA;        对单一URL进行信息泄露扫描:         python3 SpringBoot-Scan.py -u example.com&#xA;        读取目标TXT进行批量信息泄露扫描:    python3 SpringBoot-Scan.py -f url.txt&#xA;        对单一URL进行漏洞利用:             python3 SpringBoot-Scan.py -v example.com&#xA;        扫描并下载SpringBoot敏感文件:      python3 SpringBoot-Scan.py -d example.com&#xA;&#xA;参数:&#xA;        -u  --url       对单一URL进行信息泄露扫描&#xA;        -f  --file      读取目标TXT进行批量信息泄露扫描&#xA;        -v  --vul       对单一URL进行漏洞利用&#xA;        -d  --dump      扫描并下载SpringBoot敏感文件（可提取敏感信息）&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意，本工具优化了使用者体验，不管是对单一URL扫描还是读取TXT进行批量扫描，&lt;code&gt;example.com&lt;/code&gt; 和&lt;code&gt;http://example.com/&lt;/code&gt; 以及&lt;code&gt;http://example.com&lt;/code&gt; 都不会报错，程序会自行判断并识别&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;同时，解决了SSL证书问题，可以对采用SSL证书的Spring Boot框架进行扫描（自签名证书请改成 &lt;code&gt;http://&lt;/code&gt; 即可）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;安装Python依赖库&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;工具演示&lt;/h1&gt; &#xA;&lt;h3&gt;信息泄露字典&lt;/h3&gt; &#xA;&lt;p&gt;Dir.txt为内置的信息泄露端点字典，我基本收集齐了Spring Boot的相关敏感信息泄露端点&lt;/p&gt; &#xA;&lt;p&gt;如果有遗漏，欢迎各位师傅跟我联系哈哈&lt;/p&gt; &#xA;&lt;h3&gt;对单一URL进行信息泄露扫描&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 SpringBoot-Scan.py -u example.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AabyssZG/SpringBoot-Scan/main/pic/%E6%89%AB%E6%8F%8F%E5%8D%95%E4%B8%80URL.png&#34; alt=&#34;扫描单一URL&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注：扫描结束后，会把成功的结果导出为同目录下的urlout.txt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;读取目标TXT进行批量信息泄露扫描&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 SpringBoot-Scan.py -f url.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AabyssZG/SpringBoot-Scan/main/pic/%E8%AF%BB%E5%8F%96TXT%E5%B9%B6%E6%89%B9%E9%87%8F%E6%89%AB%E6%8F%8F.png&#34; alt=&#34;读取TXT并批量扫描&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注：扫描结束后，会把成功的结果导出为同目录下的output.txt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;对单一URL进行漏洞利用&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 SpringBoot-Scan.py -v example.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AabyssZG/SpringBoot-Scan/main/pic/%E5%AF%B9%E5%8D%95%E4%B8%80URL%E8%BF%9B%E8%A1%8C%E6%BC%8F%E6%B4%9E%E5%88%A9%E7%94%A8.png&#34; alt=&#34;对单一URL进行漏洞利用&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;默认执行 &lt;code&gt;id&lt;/code&gt; Payload，只是证明漏洞存在即可，有需要可以提issue来添加一个命令自定义功能&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;同时，后期将加入更多漏洞利用内置模块，请师傅们敬请期待~&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;扫描并下载SpringBoot敏感文件&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 SpringBoot-Scan.py -d example.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AabyssZG/SpringBoot-Scan/main/pic/%E6%89%AB%E6%8F%8F%E5%B9%B6%E4%B8%8B%E8%BD%BDSpringBoot%E6%95%8F%E6%84%9F%E6%96%87%E4%BB%B6.png&#34; alt=&#34;扫描并下载SpringBoot敏感文件&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注：扫描到的敏感文件，会自动下载到脚本的运行目录，有进度条可以看到实时下载进度&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;目前敏感文件目录内置了5个，如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;actuator/heapdump&#xA;gateway/actuator/heapdump&#xA;heapdump&#xA;heapdump.json&#xA;hystrix.stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果有师傅有其他敏感文件的目录，可以提交issues，谢谢！！！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/CutLER</title>
    <updated>2023-02-01T01:43:36Z</updated>
    <id>tag:github.com,2023-02-01:/facebookresearch/CutLER</id>
    <link href="https://github.com/facebookresearch/CutLER" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for &#34;Cut and Learn for Unsupervised Object Detection and Instance Segmentation&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cut and Learn for Unsupervised Object Detection and Instance Segmentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cut&lt;/strong&gt;-and-&lt;strong&gt;LE&lt;/strong&gt;a&lt;strong&gt;R&lt;/strong&gt;n (&lt;strong&gt;CutLER&lt;/strong&gt;) is a simple approach for training object detection and instance segmentation models without human annotations. It outperforms previous SOTA by &lt;strong&gt;2.7 times&lt;/strong&gt; for AP50 and &lt;strong&gt;2.6 times&lt;/strong&gt; for AR on &lt;strong&gt;11 benchmarks&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/docs/teaser_img.jpg&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/&#34;&gt;&lt;strong&gt;Cut and Learn for Unsupervised Object Detection and Instance Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~xdwang/&#34;&gt;Xudong Wang&lt;/a&gt;, &lt;a href=&#34;https://rohitgirdhar.github.io/&#34;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&#34;https://www1.icsi.berkeley.edu/~stellayu/&#34;&gt;Stella X. Yu&lt;/a&gt;, &lt;a href=&#34;https://imisra.github.io/&#34;&gt;Ishan Misra&lt;/a&gt;&lt;br&gt; Tech report&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/&#34;&gt;project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2301.11320&#34;&gt;arxiv&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1NgEyFHvOfuA2MZZnfNPWg1w5gSr3HOBb?usp=sharing&#34;&gt;colab&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/#citation&#34;&gt;bibtex&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We propose MaskCut approach to generate pseudo-masks for multiple objects in an image.&lt;/li&gt; &#xA; &lt;li&gt;CutLER can learn unsupervised object detectors and instance segmentors solely on ImageNet-1K.&lt;/li&gt; &#xA; &lt;li&gt;CutLER exhibits strong robustness to domain shifts when evaluated on 11 different benchmarks across domains like natural images, video frames, paintings, sketches, etc.&lt;/li&gt; &#xA; &lt;li&gt;CutLER can serve as a pretrained model for fully/semi-supervised detection and segmentation tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/INSTALL.md&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Preparation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/datasets/README.md&#34;&gt;Preparing Datasets for CutLER&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Method Overview&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/docs/pipeline.jpg&#34; width=&#34;55%&#34;&gt; &lt;/p&gt; Cut-and-Learn has two stages: 1) generating pseudo-masks with MaskCut and 2) learning unsupervised detectors from pseudo-masks of unlabeled data. &#xA;&lt;h3&gt;1. MaskCut&lt;/h3&gt; &#xA;&lt;p&gt;MaskCut can be used to provide segmentation masks for multiple instances of each image.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/docs/maskcut.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;MaskCut Demo&lt;/h3&gt; &#xA;&lt;p&gt;Try out the MaskCut demo using Colab (no GPU needed): &lt;a href=&#34;https://colab.research.google.com/drive/1X05lKL_IBRvZB7q6n6pb4w00_tIYjGlf?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try out the web demo: &lt;a href=&#34;https://huggingface.co/spaces/facebook/MaskCut&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; (thanks to &lt;a href=&#34;https://github.com/hysts&#34;&gt;@hysts&lt;/a&gt;!)&lt;/p&gt; &#xA;&lt;p&gt;If you want to run MaskCut locally, we provide &lt;code&gt;demo.py&lt;/code&gt; that is able to visualize the pseudo-masks produced by MaskCut. Run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd maskcut&#xA;python demo.py --img-path imgs/demo2.jpg \&#xA;  --N 3 --tau 0.15 --vit-arch base --patch-size 8 \&#xA;  [--other-options]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We give a few demo images in maskcut/imgs/. If you want to run demo.py with cpu, simply add &#34;--cpu&#34; when running the demo script. For imgs/demo4.jpg, you need to use &#34;--N 6&#34; to segment all six instances in the image. Following, we give some visualizations of the pseudo-masks on the demo images.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/docs/maskcut-demo.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Generating Annotations for ImageNet-1K with MaskCut&lt;/h3&gt; &#xA;&lt;p&gt;To generate pseudo-masks for ImageNet-1K using MaskCut, first set up the ImageNet-1K dataset according to the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/datasets/README.md&#34;&gt;datasets/README.md&lt;/a&gt;, then execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd maskcut&#xA;python maskcut.py \&#xA;--vit-arch base --patch-size 8 \&#xA;--tau 0.15 --fixed_size 480 --N 3 \&#xA;--num-folder-per-job 1000 --job-index 0 \&#xA;--dataset-path /path/to/dataset/traindir \&#xA;--out-dir /path/to/save/annotations \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As the process of generating pseudo-masks for all 1.3 million images in 1,000 folders takes a significant amount of time, it is recommended to use multiple runs. Each run should process the pseudo-mask generation for a smaller number of image folders by setting &#34;--num-folder-per-job&#34; and &#34;--job-index&#34;. Once all runs are completed, you can merge all the resulting json files by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge_jsons.py \&#xA;--base-dir /path/to/save/annotations \&#xA;--num-folder-per-job 2 --fixed-size 480 \&#xA;--tau 0.15 --N 3 \&#xA;--save-path imagenet_train_fixsize480_tau0.15_N3.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &#34;--num-folder-per-job&#34;, &#34;--fixed-size&#34;, &#34;--tau&#34; and &#34;--N&#34; of merge_jsons.py should match the ones used to run maskcut.py.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a submitit script to launch the pseudo-mask generation process with multiple nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd maskcut&#xA;bash run_maskcut_with_submitit.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you can use &#34;merge_jsons.py&#34; to merge all these json files as described above.&lt;/p&gt; &#xA;&lt;h3&gt;2. CutLER&lt;/h3&gt; &#xA;&lt;h3&gt;Inference Demo for CutLER with Pre-trained Models&lt;/h3&gt; &#xA;&lt;p&gt;Try out the CutLER demo using Colab (no GPU needed): &lt;a href=&#34;https://colab.research.google.com/drive/1NgEyFHvOfuA2MZZnfNPWg1w5gSr3HOBb?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try out the web demo: &lt;a href=&#34;https://huggingface.co/spaces/facebook/CutLER&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; (thanks to &lt;a href=&#34;https://github.com/hysts&#34;&gt;@hysts&lt;/a&gt;!)&lt;/p&gt; &#xA;&lt;p&gt;If you want to run CutLER demos locally,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pick a model and its config file from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;, for example, &lt;code&gt;model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We provide &lt;code&gt;demo.py&lt;/code&gt; that is able to demo builtin configs. Run it with:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd cutler&#xA;python demo/demo.py --config-file model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN.yaml \&#xA;  --input demo/imgs/*.jpg \&#xA;  [--other-options]&#xA;  --opts MODEL.WEIGHTS /path/to/cutler_w_cascade_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The configs are made for training, therefore we need to specify &lt;code&gt;MODEL.WEIGHTS&lt;/code&gt; to a model from model zoo for evaluation. This command will run the inference and show visualizations in an OpenCV window.&lt;/p&gt; &#xA;&lt;!-- For details of the command line arguments, see `demo.py -h` or look at its source code&#xA;to understand its behavior. Some common arguments are: --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run &lt;strong&gt;on cpu&lt;/strong&gt;, add &lt;code&gt;MODEL.DEVICE cpu&lt;/code&gt; after &lt;code&gt;--opts&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To save outputs to a directory (for images) or a file (for webcam or video), use &lt;code&gt;--output&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Following, we give some visualizations of the model predictions on the demo images.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/docs/cutler-demo.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Unsupervised Model Learning&lt;/h3&gt; &#xA;&lt;p&gt;Before training the detector, it is necessary to use MaskCut to generate pseudo-masks for all ImageNet data. You can either use the pre-generated json file directly by downloading it from &lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/maskcut/imagenet_train_fixsize480_tau0.15_N3.json&#34;&gt;here&lt;/a&gt; and placing it under &#34;DETECTRON2_DATASETS/imagenet/annotations/&#34;, or generate your own pseudo-masks by following the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/#1-maskcut&#34;&gt;MaskCut&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide a script &lt;code&gt;train_net.py&lt;/code&gt;, that is made to train all the configs provided in CutLER. To train a model with &#34;train_net.py&#34;, first setup the ImageNet-1K dataset following &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/datasets/README.md&#34;&gt;datasets/README.md&lt;/a&gt;, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd cutler&#xA;export DETECTRON2_DATASETS=/path/to/DETECTRON2_DATASETS/&#xA;python train_net.py --num-gpus 8 \&#xA;  --config-file model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to train a model using multiple nodes, you may need to adjust &lt;a href=&#34;https://arxiv.org/abs/1706.02677&#34;&gt;some model parameters&lt;/a&gt; and some SBATCH command options in &#34;tools/train-1node.sh&#34; and &#34;tools/single-node_run.sh&#34;, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd cutler&#xA;sbatch tools/train-1node.sh \&#xA;  --config-file model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN.yaml \&#xA;  MODEL.WEIGHTS /path/to/dino/d2format/model \&#xA;  OUTPUT_DIR output/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also convert a pre-trained DINO model to detectron2&#39;s format by yourself following &lt;a href=&#34;https://github.com/facebookresearch/moco/tree/main/detection&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Self-training&lt;/h3&gt; &#xA;&lt;p&gt;We further improve performance by self-training the model on its predictions.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, we can get model predictions on ImageNet via running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --num-gpus 8 \&#xA;  --config-file model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN.yaml \&#xA;  --test-dataset imagenet_train \&#xA;  --eval-only TEST.DETECTIONS_PER_IMAGE 30 \&#xA;  MODEL.WEIGHTS output/model_final.pth \ # load previous stage/round checkpoints&#xA;  OUTPUT_DIR output/ # path to save model predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Secondly, we can run the following command to generate the json file for the first round of self-training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tools/get_self_training_ann.py \&#xA;  --new-pred output/inference/coco_instances_results.json \ # load model predictions&#xA;  --prev-ann DETECTRON2_DATASETS/imagenet/annotations/imagenet_train_fixsize480_tau0.15_N3.json \ # path to the old annotation file.&#xA;  --save-path DETECTRON2_DATASETS/imagenet/annotations/cutler_imagenet1k_train_r1.json \ # path to save a new annotation file.&#xA;  --threshold 0.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, place &#34;cutler_imagenet1k_train_r1.json&#34; under &#34;DETECTRON2_DATASETS/imagenet/annotations/&#34;, then launch the self-training process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --num-gpus 8 \&#xA;  --config-file model_zoo/configs/CutLER-ImageNet/cascade_mask_rcnn_R_50_FPN_self_train.yaml \&#xA;  --train-dataset imagenet_train_r1 \&#xA;  MODEL.WEIGHTS output/model_final.pth \ # load previous stage/round checkpoints&#xA;  OUTPUT_DIR output/self-train-r1/ # path to save checkpoints&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can repeat the steps above to perform multiple rounds of self-training and adjust some arguments as needed (e.g., &#34;--threshold&#34; for round 1 and 2 can be set to 0.7 and 0.65, respectively; &#34;--train-dataset&#34; for round 1 and 2 can be set to &#34;imagenet_train_r1&#34; and &#34;imagenet_train_r2&#34;, respectively; MODEL.WEIGHTS for round 1 and 2 should point to the previous stage/round checkpoints). Ensure that all annotation files are placed under DETECTRON2_DATASETS/imagenet/annotations/. Please ensure that &#34;--train-dataset&#34;, json file names and locations match the ones specified in &#34;cutler/data/datasets/builtin.py&#34;. Please refer to this &lt;a href=&#34;https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html&#34;&gt;instruction&lt;/a&gt; for guidance on using custom datasets.&lt;/p&gt; &#xA;&lt;p&gt;You can also directly download the MODEL.WEIGHTS and annotations used for each round of self-training:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE BODY --&gt; &#xA;  &lt;!-- ROW: round 1 --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;round 1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_r1.pth&#34;&gt;cutler_cascade_r1.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/maskcut/cutler_imagenet1k_train_r1.json&#34;&gt;cutler_imagenet1k_train_r1.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- ROW: round 2 --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;round 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_r2.pth&#34;&gt;cutler_cascade_r2.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/maskcut/cutler_imagenet1k_train_r2.json&#34;&gt;cutler_imagenet1k_train_r2.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Unsupervised Zero-shot Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate a model&#39;s performance on 11 different datasets, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/datasets/README.md&#34;&gt;datasets/README.md&lt;/a&gt; for instructions on preparing the datasets. Next, select a model from the model zoo, specify the &#34;model_weights&#34;, &#34;config_file&#34; and the path to &#34;DETECTRON2_DATASETS&#34; in &lt;code&gt;tools/eval.sh&lt;/code&gt;, then run the script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash tools/eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;We show zero-shot unsupervised object detection performance (AP50&amp;nbsp;|&amp;nbsp;AR) on 11 different datasets spanning a variety of domains. ^: CutLER using Mask R-CNN as a detector; *: CutLER using Cascade Mask R-CNN as a detector.&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;Methods&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;COCO&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;COCO20K&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;VOC&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;LVIS&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;UVO&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Clipart&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Comic&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Watercolor&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;KITTI&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Objects365&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;OpenImages&lt;/th&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Prev. SOTA&lt;/td&gt; &#xA;   &lt;td valign=&#34;bottom&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.6&amp;nbsp;|&amp;nbsp;12.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.7&amp;nbsp;|&amp;nbsp;12.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.9&amp;nbsp;|&amp;nbsp;21.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8&amp;nbsp;|&amp;nbsp;6.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.0&amp;nbsp;|&amp;nbsp;14.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.9&amp;nbsp;|&amp;nbsp;15.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.9&amp;nbsp;|&amp;nbsp;16.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.7&amp;nbsp;|&amp;nbsp;16.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.7&amp;nbsp;|&amp;nbsp;7.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.1&amp;nbsp;|&amp;nbsp;10.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.9&amp;nbsp;|&amp;nbsp;14.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- ROW: Box/Mask AP for CutLER --&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;CutLER^&lt;/td&gt; &#xA;   &lt;td valign=&#34;bottom&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_mrcnn_final.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.1&amp;nbsp;|&amp;nbsp;29.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.6&amp;nbsp;|&amp;nbsp;30.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&amp;nbsp;|&amp;nbsp;41.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.7&amp;nbsp;|&amp;nbsp;18.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.8&amp;nbsp;|&amp;nbsp;38.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.9&amp;nbsp;|&amp;nbsp;38.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.2&amp;nbsp;|&amp;nbsp;37.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.3&amp;nbsp;|&amp;nbsp;39.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.3&amp;nbsp;|&amp;nbsp;25.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.5&amp;nbsp;|&amp;nbsp;30.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.1&amp;nbsp;|&amp;nbsp;26.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- ROW: Box/Mask AP for CutLER --&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;CutLER*&lt;/td&gt; &#xA;   &lt;td valign=&#34;bottom&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_final.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.9&amp;nbsp;|&amp;nbsp;32.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&amp;nbsp;|&amp;nbsp;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.9&amp;nbsp;|&amp;nbsp;44.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.4&amp;nbsp;|&amp;nbsp;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.7&amp;nbsp;|&amp;nbsp;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.1&amp;nbsp;|&amp;nbsp;41.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.4&amp;nbsp;|&amp;nbsp;38.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.5&amp;nbsp;|&amp;nbsp;44.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.4&amp;nbsp;|&amp;nbsp;27.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.6&amp;nbsp;|&amp;nbsp;34.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.3&amp;nbsp;|&amp;nbsp;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Semi-supervised and Fully-supervised Learning&lt;/h2&gt; &#xA;&lt;p&gt;CutLER can also serve as a pretrained model for training fully supervised object detection and instance segmentation models and improves performance on COCO, including on few-shot benchmarks.&lt;/p&gt; &#xA;&lt;h3&gt;Training &amp;amp; Evaluation in Command Line&lt;/h3&gt; &#xA;&lt;p&gt;You can find all the semi-supervised and fully-supervised learning configs provided in CutLER under &lt;code&gt;model_zoo/configs/COCO-Semisupervised&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To train a model using K% labels with &lt;code&gt;train_net.py&lt;/code&gt;, first set up the COCO dataset according to &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/datasets/README.md&#34;&gt;datasets/README.md&lt;/a&gt; and specify K value in the config file, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --num-gpus 8 \&#xA;  --config-file model_zoo/configs/COCO-Semisupervised/cascade_mask_rcnn_R_50_FPN_{K}perc.yaml \&#xA;  MODEL.WEIGHTS /path/to/cutler_pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find all config files used to train supervised models under &lt;code&gt;model_zoo/configs/COCO-Semisupervised&lt;/code&gt;. The configs are made for 8-GPU training. To train on 1 GPU, you may need to &lt;a href=&#34;https://arxiv.org/abs/1706.02677&#34;&gt;change some parameters&lt;/a&gt;, e.g. number of GPUs (num-gpus your_num_gpus), learning rates (SOLVER.BASE_LR your_base_lr) and batch size (SOLVER.IMS_PER_BATCH your_batch_size).&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate a model&#39;s performance, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py \&#xA;  --config-file model_zoo/configs/COCO-Semisupervised/cascade_mask_rcnn_R_50_FPN_{K}perc.yaml \&#xA;  --eval-only MODEL.WEIGHTS /path/to/checkpoint_file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more options, see &lt;code&gt;python train_net.py -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune a Cascade R-CNN model initialized with CutLER or MoCo-v2 on varying amounts of labeled COCO data, and show results (Box&amp;nbsp;|&amp;nbsp;Mask AP) on the val2017 split below:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;% of labels&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;1%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;2%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;5%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;10%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;20%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;30%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;40%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;50%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;60%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;80%&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;100%&lt;/th&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;   &lt;!-- ROW: Box/Mask AP for CutLER --&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;MoCo-v2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.8&amp;nbsp;|&amp;nbsp;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&amp;nbsp;|&amp;nbsp;13.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.5&amp;nbsp;|&amp;nbsp;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.5&amp;nbsp;|&amp;nbsp;23.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&amp;nbsp;|&amp;nbsp;28.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.5&amp;nbsp;|&amp;nbsp;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.3&amp;nbsp;|&amp;nbsp;32.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.7&amp;nbsp;|&amp;nbsp;33.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.9&amp;nbsp;|&amp;nbsp;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&amp;nbsp;|&amp;nbsp;36.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&amp;nbsp;|&amp;nbsp;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- ROW: Mask AP --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;CutLER&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.8&amp;nbsp;|&amp;nbsp;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.6&amp;nbsp;|&amp;nbsp;18.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.8&amp;nbsp;|&amp;nbsp;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.2&amp;nbsp;|&amp;nbsp;28.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&amp;nbsp;|&amp;nbsp;31.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.2&amp;nbsp;|&amp;nbsp;33.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.9&amp;nbsp;|&amp;nbsp;34.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.5&amp;nbsp;|&amp;nbsp;35.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.3&amp;nbsp;|&amp;nbsp;36.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.8&amp;nbsp;|&amp;nbsp;37.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&amp;nbsp;|&amp;nbsp;38.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- ROW: Model Downloads --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Download&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_1perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_2perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_5perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_10perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_20perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_30perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_40perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_50perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_60perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_semi_80perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_fully_100perc.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Both MoCo-v2 and our CutLER are trained for the 1x schedule using Detectron2, except for extremely low-shot settings with 1% or 2% labels. When training with 1% or 2% labels, we train both MoCo-v2 and our model for 3,600 iterations with a batch size of 16.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of CutLER, Detectron2 and DINO are licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/CutLER/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;, however portions of the project are available under separate license terms: TokenCut, Bilateral Solver and CRF are licensed under the MIT license; If you later add other third party code, please keep this license info updated, and please let us know if that component is licensed under something other than CC-BY-NC, MIT, or CC0.&lt;/p&gt; &#xA;&lt;h2&gt;Ethical Considerations&lt;/h2&gt; &#xA;&lt;p&gt;CutLER&#39;s wide range of detection capabilities may introduce similar challenges to many other visual recognition recognition methods. As the image can contain arbitrary instances, it may impact the model output.&lt;/p&gt; &#xA;&lt;h2&gt;How to get support from us?&lt;/h2&gt; &#xA;&lt;p&gt;If you have any general questions, feel free to email us at &lt;a href=&#34;mailto:xdwang@eecs.berkeley.edu&#34;&gt;Xudong Wang&lt;/a&gt;, &lt;a href=&#34;mailto:imisra@meta.com&#34;&gt;Ishan Misra&lt;/a&gt; and &lt;a href=&#34;mailto:rgirdhar@meta.com&#34;&gt;Rohit Girdhar&lt;/a&gt;. If you have code or implementation-related questions, please feel free to send emails to us or open an issue in this codebase (We recommend that you open an issue in this codebase, because your questions may help others).&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work inspiring or use our codebase in your research, please consider giving a star ⭐ and a citation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2023cut,&#xA;  title={Cut and Learn for Unsupervised Object Detection and Instance Segmentation},&#xA;  author={Wang, Xudong and Girdhar, Rohit and Yu, Stella X and Misra, Ishan},&#xA;  journal={arXiv preprint arXiv:2301.11320},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>showlab/Tune-A-Video</title>
    <updated>2023-02-01T01:43:36Z</updated>
    <id>tag:github.com,2023-02-01:/showlab/Tune-A-Video</id>
    <link href="https://github.com/showlab/Tune-A-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tune-A-Video&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Tune-A-Video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/a&gt;&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://zhangjiewu.github.io/&#34;&gt;Jay Zhangjie Wu&lt;/a&gt;, &lt;a href=&#34;https://geyixiao.com/&#34;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Stan Weixian Lei&lt;/a&gt;, &lt;a href=&#34;https://ycgu.site/&#34;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&#34;https://www.comp.nus.edu.sg/~whsu/&#34;&gt;Wynne Hsu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=mk-F69UAAAAJ&amp;amp;hl=en&#34;&gt;Xiaohu Qie&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/showlab&#34;&gt;Mike Zheng Shou&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tuneavideo.github.io/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Installing &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt; is highly recommended for more efficiency and speed on GPUs. To enable xformers, set &lt;code&gt;enable_xformers_memory_efficient_attention=True&lt;/code&gt; (default).&lt;/p&gt; &#xA;&lt;h3&gt;Weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download the pre-trained &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Stable Diffusion&lt;/a&gt; models (e.g., &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;Stable Diffusion v1-4&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git lfs install&#xA;git clone https://huggingface.co/CompVis/stable-diffusion-v1-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can use a personalized &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;DreamBooth&lt;/a&gt; model (e.g., &lt;a href=&#34;https://huggingface.co/sd-dreambooth-library/mr-potato-head&#34;&gt;mr-potato-head&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git lfs install&#xA;git clone https://huggingface.co/sd-dreambooth-library/mr-potato-head&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To fine-tune the text-to-image diffusion models for text-to-video generation, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_tuneavideo.py --config=&#34;configs/man-surfing.yaml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Once the training is done, run inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline&#xA;from tuneavideo.models.unet import UNet3DConditionModel&#xA;from tuneavideo.util import save_videos_grid&#xA;import torch&#xA;&#xA;model_id = &#34;path-to-your-trained-model&#34;&#xA;unet = UNet3DConditionModel.from_pretrained(model_id, subfolder=&#39;unet&#39;, torch_dtype=torch.float16).to(&#39;cuda&#39;)&#xA;pipe = TuneAVideoPipeline.from_pretrained(&#34;CompVis/stable-diffusion-v1-4&#34;, unet=unet, torch_dtype=torch.float16).to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a panda is surfing&#34;&#xA;video = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos&#xA;&#xA;save_videos_grid(video, f&#34;{prompt}.gif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Fine-tuning on Stable Diffusion&lt;/h3&gt; &#xA;&lt;table width=&#34;100%&#34; align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/man-surfing/train.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/stablediffusion/panda-surfing.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/stablediffusion/ironman-desert.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/stablediffusion/raccoon-cartoon.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;[Training] a man is surfing.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;a panda is surfing.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;Iron Man is surfing in the desert.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;a raccoon is surfing, cartoon style.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning on DreamBooth&lt;/h3&gt; &#xA;&lt;table width=&#34;100%&#34; align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/dreambooth/mr-potato-head.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/dreambooth/pink-hat.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/dreambooth/potato-sunglasses.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/static/results/repo/dreambooth/potato-forest.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;sks mr potato head.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;sks mr potato head, wearing a pink hat, is surfing.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;sks mr potato head, wearing sunglasses, is surfing.&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;sks mr potato head is surfing in the forest.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wu2022tuneavideo,&#xA;    title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},&#xA;    author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},&#xA;    journal={arXiv preprint arXiv:2212.11565},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>