<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-12T01:35:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kyegomez/AlphaFold3</title>
    <updated>2024-05-12T01:35:24Z</updated>
    <id>tag:github.com,2024-05-12:/kyegomez/AlphaFold3</id>
    <link href="https://github.com/kyegomez/AlphaFold3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Alpha Fold 3 from the paper: &#34;Accurate structure prediction of biomolecular interactions with AlphaFold3&#34; in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/AlphaFold3/main/agorabanner.png&#34; alt=&#34;Multi-Modality&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;AlphaFold3&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of Alpha Fold 3 from the paper: &#34;Accurate structure prediction of biomolecular interactions with AlphaFold3&#34; in PyTorch&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;$ pip install alphafold3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Input Tensor Size Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# Define the batch size, number of nodes, and number of features&#xA;batch_size = 1&#xA;num_nodes = 5&#xA;num_features = 64&#xA;&#xA;# Generate random pair representations using torch.randn&#xA;# Shape: (batch_size, num_nodes, num_nodes, num_features)&#xA;pair_representations = torch.randn(&#xA;    batch_size, num_nodes, num_nodes, num_features&#xA;)&#xA;&#xA;# Generate random single representations using torch.randn&#xA;# Shape: (batch_size, num_nodes, num_features)&#xA;single_representations = torch.randn(&#xA;    batch_size, num_nodes, num_features&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Genetic Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;Need review but basically it operates on atomic coordinates.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from alphafold3.diffusion import GeneticDiffusion&#xA;&#xA;# Create an instance of the GeneticDiffusionModuleBlock&#xA;model = GeneticDiffusion(channels=3, training=True)&#xA;&#xA;# Generate random input coordinates&#xA;input_coords = torch.randn(10, 100, 100, 3)&#xA;&#xA;# Generate random ground truth coordinates&#xA;ground_truth = torch.randn(10, 100, 100, 3)&#xA;&#xA;# Pass the input coordinates and ground truth coordinates through the model&#xA;output_coords, loss = model(input_coords, ground_truth)&#xA;&#xA;# Print the output coordinates&#xA;print(output_coords)&#xA;&#xA;# Print the loss value&#xA;print(loss)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Full Model Example Forward pass&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch &#xA;from alphafold3 import AlphaFold3&#xA;&#xA;# Create random tensors&#xA;x = torch.randn(1, 5, 5, 64)  # Shape: (batch_size, seq_len, seq_len, dim)&#xA;y = torch.randn(1, 5, 64)  # Shape: (batch_size, seq_len, dim)&#xA;&#xA;# Initialize AlphaFold3 model&#xA;model = AlphaFold3(&#xA;    dim=64,&#xA;    seq_len=5,&#xA;    heads=8,&#xA;    dim_head=64,&#xA;    attn_dropout=0.0,&#xA;    ff_dropout=0.0,&#xA;    global_column_attn=False,&#xA;    pair_former_depth=48,&#xA;    num_diffusion_steps=1000,&#xA;    diffusion_depth=30,&#xA;)&#xA;&#xA;# Forward pass through the model&#xA;output = model(x, y)&#xA;&#xA;# Print the shape of the output tensor&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Abramson2024-fj,&#xA;  title    = &#34;Accurate structure prediction of biomolecular interactions with&#xA;              {AlphaFold} 3&#34;,&#xA;  author   = &#34;Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans,&#xA;              Richard and Green, Tim and Pritzel, Alexander and Ronneberger,&#xA;              Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick,&#xA;              Joshua and Bodenstein, Sebastian W and Evans, David A and Hung,&#xA;              Chia-Chun and O&#39;Neill, Michael and Reiman, David and&#xA;              Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e},&#xA;              Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and&#xA;              Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and&#xA;              Congreve, Miles and Cowen-Rivers, Alexander I and Cowie, Andrew&#xA;              and Figurnov, Michael and Fuchs, Fabian B and Gladman, Hannah and&#xA;              Jain, Rishub and Khan, Yousuf A and Low, Caroline M R and Perlin,&#xA;              Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and&#xA;              Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine&#xA;              and Yakneen, Sergei and Zhong, Ellen D and Zielinski, Michal and&#xA;              {\v Z}{\&#39;\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet&#xA;              and Jaderberg, Max and Hassabis, Demis and Jumper, John M&#34;,&#xA;  journal  = &#34;Nature&#34;,&#xA;  month    =  may,&#xA;  year     =  2024&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Notes&lt;/h1&gt; &#xA;&lt;p&gt;-&amp;gt; pairwise representation -&amp;gt; explicit atomic positions -&amp;gt; within the trunk, msa processing is de emphasized with a simpler MSA block, 4 blocks -&amp;gt; msa processing -&amp;gt; pair weighted averaging -&amp;gt; pairformer: replaces evoformer, operates on pair representation and single representation -&amp;gt; pairformer 48 blocks -&amp;gt; pair and single representation together with the input representation are passed to the diffusion module -&amp;gt; diffusion takes in 3 tensors [pair, single representation, with new pairformer representation] -&amp;gt; diffusion module operates directory on raw atom coordinates -&amp;gt; standard diffusion approach, model is trained to receiev noised atomic coordinates then predict the true coordinates -&amp;gt; the network learns protein structure at a variety of length scales where the denoising task at small noise emphasizes large scale structure of the system. -&amp;gt; at inference time, random noise is sampled and then recurrently denoised to produce a final structure -&amp;gt; diffusion module produces a distribution of answers -&amp;gt; for each answer the local structure will be sharply defined -&amp;gt; diffusion models are prone to hallucination where the model may hallucinate plausible looking structures -&amp;gt; to counteract hallucination, they use a novel cross distillation method where they enrich the training data with alphafold multimer v2.3 predicted strutctures. -&amp;gt; confidence measures predicts the atom level and pairwise errors in final structures, this is done by regressing the error in the outut of the structure mdule in training, -&amp;gt; Utilizes diffusion rollout procedure for the full structure generation during training ( using a largeer step suze than normal) -&amp;gt; diffused predicted structure is used to permute the ground truth and ligands to compute metrics to train the confidence head. -&amp;gt; confidence head uses the pairwise representation to predict the lddt (pddt) and a predicted aligned error matrix as used in alphafold 2 as well as distance error matrix which is the error in the distance matrix of the predicted structure as compared to the true structure -&amp;gt; confidence measures also preduct atom level and pairwise errors -&amp;gt; early stopping using a weighted average of all above metic -&amp;gt; af3 can predict srtructures from input polymer sequences, rediue modifications, ligand smiles -&amp;gt; uses structures below 1000 residues -&amp;gt; alphafold3 is able to predict protein nuclear structures with thousnads of residues -&amp;gt; Covalent modifications (bonded ligands, glycosylation, and modified protein residues and 202 nucleic acid bases) are also accurately predicted by AF -&amp;gt; distills alphafold2 preductions -&amp;gt; key problem in protein structure prediction is they predict static structures and not the dynamical behavior -&amp;gt; multiple random seeds for either the diffusion head or network does not product an approximation of the solution ensenble -&amp;gt; in future: generate large number of predictions and rank them -&amp;gt; inference: top confidence sample from 5 seed runs and 5 diffusion samples per model seed for a total of 25 samples -&amp;gt; interface accuracy via interface lddt which is calculated from distances netween atoms across different chains in the interface -&amp;gt; uses a lddt to polymer metric which considers differences from each atom of a entity to any c or c1 polymer atom within aradius&lt;/p&gt; &#xA;&lt;h1&gt;Todo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement Figure A, implement triangle update, transition,&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Impelment Figure B, per token, cond,&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement Figure C: Network Chunk,&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement confidence module&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement Template Module&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>stanford-oval/storm</title>
    <updated>2024-05-12T01:35:24Z</updated>
    <id>tag:github.com,2024-05-12:/stanford-oval/storm</id>
    <link href="https://github.com/stanford-oval/storm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;http://storm.genie.stanford.edu&#34;&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;Documentation (WIP)&lt;/b&gt; | &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Latest News&lt;/strong&gt; 🔥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/04] We release refactored version of STORM codebase! We define &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/interface.py&#34;&gt;interface&lt;/a&gt; for STORM pipeline and reimplement STORM-wiki (check out &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/storm_wiki&#34;&gt;&lt;code&gt;src/storm_wiki&lt;/code&gt;&lt;/a&gt;) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.png&#34; style=&#34;width: 90%; height: auto;&#34;&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. &#xA;&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Try out our &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;live research preview&lt;/a&gt; to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system 🙏!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How STORM works&lt;/h2&gt; &#xA;&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Based on the separation of the two stages, STORM is implemented in a highly modular way using &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Setup&lt;/h3&gt; &#xA;&lt;p&gt;Below, we provide a quick start guide to run STORM locally.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the git repository.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/stanford-oval/storm.git&#xA;cd storm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n storm python=3.11&#xA;conda activate storm&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up OpenAI API key (if you want to use OpenAI models to power STORM) and &lt;a href=&#34;https://api.you.com/&#34;&gt;You.com search API&lt;/a&gt; key. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Set up OpenAI API key.&#xA;OPENAI_API_KEY=&#34;your_openai_api_key&#34;&#xA;# If you are using the API service provided by OpenAI, include the following line:&#xA;OPENAI_API_TYPE=&#34;openai&#34;&#xA;# If you are using the API service provided by Microsoft Azure, include the following lines:&#xA;OPENAI_API_TYPE=&#34;azure&#34;&#xA;AZURE_API_BASE=&#34;your_azure_api_base_url&#34;&#xA;AZURE_API_VERSION=&#34;your_azure_api_version&#34;&#xA;# Set up You.com search API key.&#xA;YDC_API_KEY=&#34;your_youcom_api_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. Running STORM-wiki locally&lt;/h3&gt; &#xA;&lt;p&gt;Currently, we provide example scripts under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; to demonstrate how you can run STORM using different models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;To run STORM with &lt;code&gt;gpt&lt;/code&gt; family models&lt;/strong&gt;: Make sure you have set up the OpenAI API key and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run_storm_wiki_gpt.py \&#xA;    --output_dir $OUTPUT_DIR \&#xA;    --do-research \&#xA;    --do-generate-outline \&#xA;    --do-generate-article \&#xA;    --do-polish-article&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-generate-outline&lt;/code&gt;: If True, generate an outline for the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-generate-article&lt;/code&gt;: If True, generate an article for the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: If True, polish the article by adding a summarization section and (optionally) removing duplicate content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;To run STORM with &lt;code&gt;mistral&lt;/code&gt; family models on local VLLM server&lt;/strong&gt;: have a VLLM server running with the &lt;code&gt;Mistral-7B-Instruct-v0.2&lt;/code&gt; model and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run_storm_wiki_mistral.py \&#xA;    --url $URL \&#xA;    --port $PORT \&#xA;    --output_dir $OUTPUT_DIR \&#xA;    --do-research \&#xA;    --do-generate-outline \&#xA;    --do-generate-article \&#xA;    --do-polish-article&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--url&lt;/code&gt; URL of the VLLM server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--port&lt;/code&gt; Port of the VLLM server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Customize STORM&lt;/h2&gt; &#xA;&lt;h3&gt;Customization of the Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;STORM is a knowledge curation engine consisting of 4 modules:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Knowledge Curation Module: Collects a broad coverage of information about the given topic.&lt;/li&gt; &#xA; &lt;li&gt;Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Article Generation Module: Populates the generated outline with the collected information.&lt;/li&gt; &#xA; &lt;li&gt;Article Polishing Module: Refines and enhances the written article for better presentation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The interface for each module is defined in &lt;code&gt;src/interface.py&lt;/code&gt;, while their implementations are instantiated in &lt;code&gt;src/storm_wiki/modules/*&lt;/code&gt;. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🌟&lt;/span&gt; &lt;strong&gt;You can share your customization of &lt;code&gt;Engine&lt;/code&gt; by making PRs to this repo!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Customization of Retriever Module&lt;/h3&gt; &#xA;&lt;p&gt;As a knowledge curation engine, STORM grabs information from the Retriever module. The interface for the Retriever module is defined in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/interface.py&#34;&gt;&lt;code&gt;src/interface.py&lt;/code&gt;&lt;/a&gt;. Please consult the interface documentation if you plan to create a new instance or replace the default search engine API. By default, STORM utilizes the You.com search engine API (see &lt;code&gt;YouRM&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/rm.py&#34;&gt;&lt;code&gt;src/rm.py&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🌟&lt;/span&gt; &lt;strong&gt;PRs for integrating more search engines/retrievers are highly appreciated!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Customization of Language Models&lt;/h3&gt; &#xA;&lt;p&gt;STORM provides the following language model implementations in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/lm.py&#34;&gt;&lt;code&gt;src/lm.py&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OpenAIModel&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ClaudeModel&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;VLLMClient&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TGIClient&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TogetherClient&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;🌟&lt;/span&gt; &lt;strong&gt;PRs for integrating more language model clients are highly appreciated!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;💡&lt;/span&gt; &lt;strong&gt;For a good practice,&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;choose a cheaper/faster model for &lt;code&gt;conv_simulator_lm&lt;/code&gt; which is used to split queries, synthesize answers in the conversation.&lt;/li&gt; &#xA; &lt;li&gt;if you need to conduct the actual writing step, choose a more powerful model for &lt;code&gt;article_gen_lm&lt;/code&gt;. Based on our experiments, weak models are bad at generating text with citations.&lt;/li&gt; &#xA; &lt;li&gt;for open models, adding one-shot example can help it better follow instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to the scripts in the &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for concrete guidance on customizing the language model used in the pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Replicate NAACL2024 result&lt;/h2&gt; &#xA;&lt;p&gt;Please switch to the branch &lt;code&gt;NAACL-2024-code-backup&lt;/code&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show me instructions&lt;/summary&gt; &#xA; &lt;h3&gt;Paper Experiments&lt;/h3&gt; &#xA; &lt;p&gt;The FreshWiki dataset used in our experiments can be found in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/FreshWiki&#34;&gt;./FreshWiki&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Run the following commands under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src&#34;&gt;./src&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;Pre-writing Stage&lt;/h4&gt; &#xA; &lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; (choices=[&lt;code&gt;gpt-4&lt;/code&gt;, &lt;code&gt;gpt-35-turbo&lt;/code&gt;]): the LLM engine used for generating the outline&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--max-conv-turn&lt;/code&gt;: the maximum number of questions for each information-seeking conversation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--max-perspective&lt;/code&gt;: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is &lt;code&gt;max_turn * (max_perspective + 1)&lt;/code&gt;. &lt;span&gt;💡&lt;/span&gt; Reducing &lt;code&gt;max_turn&lt;/code&gt; or &lt;code&gt;max_perspective&lt;/code&gt; can speed up the process and reduce the cost but may result in less comprehensive outline.&lt;/li&gt; &#xA;    &lt;li&gt;The parameter will not have any effect if &lt;code&gt;--disable-perspective&lt;/code&gt; is set (the perspective-driven question asking is disabled).&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5 --do-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt; and the &lt;code&gt;Ground truth url&lt;/code&gt; that will be excluded. If you do not have any url to exclude, leave that field empty.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The generated outline will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_outline.txt&lt;/code&gt; and the collected references will be saved in &lt;code&gt;{output_dir}/{topic}/raw_search_results.json&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;Writing Stage&lt;/h4&gt; &#xA; &lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: if True, polish the article by adding a summarization section and removing duplicate content if &lt;code&gt;--remove-duplicate&lt;/code&gt; is set True.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt;. Please enter the same topic as the one used in the pre-writing stage.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The generated article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article.txt&lt;/code&gt; and the references corresponding to citation index will be saved in &lt;code&gt;{output_dir}/{topic}/url_to_info.json&lt;/code&gt;. If &lt;code&gt;--do-polish-article&lt;/code&gt; is set, the polished article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article_polished.txt&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Customize the STORM Configurations&lt;/h3&gt; &#xA; &lt;p&gt;We set up the default LLM configuration in &lt;code&gt;LLMConfigs&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/modules/utils.py&#34;&gt;src/modules/utils.py&lt;/a&gt;. You can use &lt;code&gt;set_conv_simulator_lm()&lt;/code&gt;,&lt;code&gt;set_question_asker_lm()&lt;/code&gt;, &lt;code&gt;set_outline_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_polish_lm()&lt;/code&gt; to override the default configuration. These functions take in an instance from &lt;code&gt;dspy.dsp.LM&lt;/code&gt; or &lt;code&gt;dspy.dsp.HFModel&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Automatic Evaluation&lt;/h3&gt; &#xA; &lt;p&gt;In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.&lt;/p&gt; &#xA; &lt;h4&gt;Outline Quality&lt;/h4&gt; &#xA; &lt;p&gt;We introduce &lt;em&gt;heading soft recall&lt;/em&gt; and &lt;em&gt;heading entity recall&lt;/em&gt; to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.&lt;/p&gt; &#xA; &lt;p&gt;Run the following command under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval&#34;&gt;./eval&lt;/a&gt; to compute the metrics on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Full-length Article Quality&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/eval_article_quality.py&#34;&gt;eval/eval_article_quality.py&lt;/a&gt; provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under &lt;code&gt;eval&lt;/code&gt; to compute the metrics:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Use the Metric Yourself&lt;/h4&gt; &#xA; &lt;p&gt;The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/metrics.py&#34;&gt;eval/metrics.py&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For rubric grading, we use the &lt;a href=&#34;https://huggingface.co/prometheus-eval/prometheus-13b-v1.0&#34;&gt;prometheus-13b-v1.0&lt;/a&gt; introduced in &lt;a href=&#34;https://arxiv.org/abs/2310.08491&#34;&gt;this paper&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/evaluation_prometheus.py&#34;&gt;eval/evaluation_prometheus.py&lt;/a&gt; provides the entry point of using the metric.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;mailto:shaoyj@stanford.edu&#34;&gt;Yijia Shao&lt;/a&gt; and &lt;a href=&#34;mailto:yuchengj@stanford.edu&#34;&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao2024assisting,&#xA;      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, &#xA;      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},&#xA;      year={2024},&#xA;      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>IAHispano/Applio</title>
    <updated>2024-05-12T01:35:24Z</updated>
    <id>tag:github.com,2024-05-12:/IAHispano/Applio</id>
    <link href="https://github.com/IAHispano/Applio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VITS-based Voice Conversion focused on simplicity, quality and performance.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://applio.org&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/IAHispano/Applio/assets/133521603/a5cc5c72-ed68-48a5-954f-db9f1dc4e7de&#34; alt=&#34;Applio&#34;&gt;&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Contributors&#34; src=&#34;https://img.shields.io/github/contributors/iahispano/applio?style=for-the-badge&amp;amp;color=00AA68&#34;&gt; &lt;img alt=&#34;Release&#34; src=&#34;https://img.shields.io/github/release/iahispano/applio?style=for-the-badge&amp;amp;color=00AA68&#34;&gt; &lt;img alt=&#34;Stars&#34; src=&#34;https://img.shields.io/github/stars/iahispano/applio?style=for-the-badge&amp;amp;color=00AA68&#34;&gt; &lt;img alt=&#34;Fork&#34; src=&#34;https://img.shields.io/github/forks/iahispano/applio?style=for-the-badge&amp;amp;color=00AA68&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/iahispano/applio?style=for-the-badge&amp;amp;color=00AA68&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;VITS-based Voice Conversion focused on simplicity, quality and performance&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://applio.org&#34; target=&#34;_blank&#34;&gt;🌐 Website&lt;/a&gt; • &lt;a href=&#34;https://docs.applio.org&#34; target=&#34;_blank&#34;&gt;📚 Documentation&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/iahispano&#34; target=&#34;_blank&#34;&gt;☎️ Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/IAHispano/Applio-Plugins&#34; target=&#34;_blank&#34;&gt;🛒 Plugins&lt;/a&gt; • &lt;a href=&#34;https://huggingface.co/IAHispano/Applio/tree/main/Compiled&#34; target=&#34;_blank&#34;&gt;📦 Compiled&lt;/a&gt; • &lt;a href=&#34;https://applio.org/playground&#34; target=&#34;_blank&#34;&gt;🎮 Playground&lt;/a&gt; • &lt;a href=&#34;https://colab.research.google.com/github/iahispano/applio/blob/master/assets/Applio.ipynb&#34; target=&#34;_blank&#34;&gt;🔎 Google Colab (UI)&lt;/a&gt; • &lt;a href=&#34;https://colab.research.google.com/github/iahispano/applio/blob/master/assets/Applio_NoUI.ipynb&#34; target=&#34;_blank&#34;&gt;🔎 Google Colab (No UI)&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Content Table&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#installation&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#windows&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#linux&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#makefile&#34;&gt;Makefile&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#usage&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#windows-1&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#linux-1&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#makefile-1&#34;&gt;Makefile&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#repository-enhancements&#34;&gt;&lt;strong&gt;Repository Enhancements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#references&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IAHispano/Applio/main/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest version from &lt;a href=&#34;https://github.com/IAHispano/Applio-RVC-Fork/releases&#34;&gt;GitHub Releases&lt;/a&gt; or use the &lt;a href=&#34;https://huggingface.co/IAHispano/Applio/tree/main/Compiled&#34;&gt;Compiled Versions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./run-install.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;Certain Linux-based operating systems may encounter complications with the installer. In such instances, we suggest installing the &lt;code&gt;requirements.txt&lt;/code&gt; within a Python environment version 3.9 to 3.11.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x run-install.sh&#xA;./run-install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Makefile&lt;/h3&gt; &#xA;&lt;p&gt;For platforms such as &lt;a href=&#34;https://www.paperspace.com/&#34;&gt;Paperspace&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run-install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://docs.applio.org/&#34;&gt;Applio Documentation&lt;/a&gt; for a detailed UI usage explanation.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./run-applio.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x run-applio.sh&#xA;./run-applio.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Makefile&lt;/h3&gt; &#xA;&lt;p&gt;For platforms such as &lt;a href=&#34;https://www.paperspace.com/&#34;&gt;Paperspace&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run-applio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository Enhancements&lt;/h2&gt; &#xA;&lt;p&gt;This repository has undergone significant enhancements to improve its functionality and maintainability:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modular Codebase:&lt;/strong&gt; Restructured codebase following a modular approach for better organization, readability, and maintenance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hop Length Implementation:&lt;/strong&gt; Implemented hop length, courtesy of &lt;a href=&#34;https://github.com/Mangio621/Mangio-RVC-Fork&#34;&gt;@Mangio621&lt;/a&gt;, boosting efficiency and performance, especially on Crepe (formerly Mangio-Crepe).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Translations in 30+ Languages:&lt;/strong&gt; Added support for translations in over 30 languages, enhancing accessibility for a global audience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Platform Compatibility:&lt;/strong&gt; Ensured seamless operation across various platforms for a consistent user experience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimized Requirements:&lt;/strong&gt; Fine-tuned project requirements for enhanced performance and resource efficiency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streamlined Installation:&lt;/strong&gt; Simplified installation process for a user-friendly setup experience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid F0 Estimation:&lt;/strong&gt; Introduced a personalized &#39;hybrid&#39; F0 estimation method utilizing nanmedian, combining F0 calculations from various methods to achieve optimal results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy-to-Use UI:&lt;/strong&gt; Implemented a user-friendly interface for intuitive interaction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimized Code &amp;amp; Dependencies:&lt;/strong&gt; Enhanced code and streamlined dependencies for improved efficiency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin System:&lt;/strong&gt; Introduced a plugin system for extending functionality and customization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overtraining Detector:&lt;/strong&gt; Implemented an overtraining detector which halts training once a specified epoch limit is reached, preventing excessive training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Search:&lt;/strong&gt; Integrated a model search feature directly into the application interface, facilitating easy model discovery.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancements in Pretrained Models:&lt;/strong&gt; Introduced additional functionalities such as custom pretrained models, allowing users to utilize their preferred pretrained models without requiring RVC1 pretrained models upon installation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Blender:&lt;/strong&gt; Developed a voice blender feature that combines two trained models to create a new one, offering versatility in model generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accessibility Improvements:&lt;/strong&gt; Enhanced accessibility with descriptive tooltips indicating the function of each element in the user interface, making it more user-friendly for all users.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New F0 Extraction Methods:&lt;/strong&gt; Introduced new F0 extraction methods such as FCPE or Hybrid, expanding options for pitch extraction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output Format Selection:&lt;/strong&gt; Implemented an output format selection feature, allowing users to choose the format in which they want to save their audio files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hashing System:&lt;/strong&gt; Implemented a hashing system where each created model is assigned a unique ID to prevent unauthorized duplication or theft.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Download System:&lt;/strong&gt; Added support for downloading models from various websites such as Google Drive, Yandex, Pixeldrain, Discord, Hugging Face, or Applio.org, enhancing model accessibility.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTS Enhancements:&lt;/strong&gt; Improved Text-to-Speech functionality with support for uploading TXT files, increasing flexibility in input methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Split Audio:&lt;/strong&gt; Implemented audio splitting functionality which divides audio into segments for inference, subsequently merging them to create the final audio, resulting in faster processing times and potentially better outcomes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discord Presence:&lt;/strong&gt; Displayed presence on Discord indicating active usage of Applio, with plans to incorporate different statuses based on activities within the application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flask Integration:&lt;/strong&gt; Integration with Flask, initially disabled by default, allows for automatic model downloads from the web by simply clicking the Applio button next to the model download button in the settings tab.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support Tab:&lt;/strong&gt; Added a support tab enabling users to record their screen to demonstrate encountered issues, facilitating faster issue resolution by allowing users to create GitHub issues for review and troubleshooting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These enhancements contribute to a more robust and scalable codebase, making the repository more accessible for contributors and users alike.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend Contributions:&lt;/strong&gt; If you want to contribute to the backend, make your pull requests &lt;a href=&#34;https://github.com/blaise-tk/RVC_CLI&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend Contributions:&lt;/strong&gt; For interface or script-related contributions, feel free to contribute to this repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We appreciate all contributions ❤️&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/gstaff/gradio-screen-recorder&#34;&gt;gradio-screen-recorder&lt;/a&gt; by gstaff&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blaise-tk/RVC_CLI&#34;&gt;RVC_CLI&lt;/a&gt; by blaise-tk&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/IAHispano/Applio/graphs/contributors&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=IAHispano/Applio&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>