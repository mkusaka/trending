<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-08T01:35:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PKU-YuanGroup/Open-Sora-Plan</title>
    <updated>2024-03-08T01:35:32Z</updated>
    <id>tag:github.com,2024-03-08:/PKU-YuanGroup/Open-Sora-Plan</id>
    <link href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project aim to reproducing Sora (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-Sora Plan&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&#34;&gt;[‰∏≠Êñá‰∏ªÈ°µ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/fqpmStRX&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/issues/53#issuecomment-1980312563&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%8A%A0%E5%85%A5-green?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/LinBin46984/status/1763476690385424554?s=20&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Twitter@LinBin46984-black?logo=twitter&amp;amp;logoColor=1D9BF0&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors-anon/PKU-YuanGroup/Open-Sora-Plan?style=flat&amp;amp;label=Contributors&#34; alt=&#34;GitHub repo contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/commits/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PKU-YuanGroup/Open-Sora-Plan?label=Commit&#34; alt=&#34;GitHub Commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr-closed-raw/PKU-YuanGroup/Open-Sora-Plan.svg?label=Merged+PRs&amp;amp;color=green&#34; alt=&#34;Pr&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/issues?q=is%3Aopen+is%3Aissue&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PKU-YuanGroup/Open-Sora-Plan?color=critical&amp;amp;label=Issues&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/PKU-YuanGroup/Open-Sora-Plan?color=success&amp;amp;label=Issues&#34; alt=&#34;GitHub closed issues&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars&#34; alt=&#34;GitHub repo stars&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/network&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/PKU-YuanGroup/Open-Sora-Plan?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks&#34; alt=&#34;GitHub repo forks&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/watchers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/PKU-YuanGroup/Open-Sora-Plan?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers&#34; alt=&#34;GitHub repo watchers&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/archive/refs/heads/main.zip&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/repo-size/PKU-YuanGroup/Open-Sora-Plan?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size&#34; alt=&#34;GitHub repo size&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí™ Goal&lt;/h2&gt; &#xA;&lt;p&gt;This project aims to create a simple and scalable repo, to reproduce &lt;a href=&#34;https://openai.com/sora&#34;&gt;Sora&lt;/a&gt; (OpenAI, but we prefer to call it &#34;ClosedAI&#34; ) and build knowledge about Video-VQVAE (VideoGPT) + DiT at scale. However, since we have limited resources, we deeply wish all open-source community can contribute to this project. Pull requests are welcome!!!&lt;/p&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÂ∏åÊúõÈÄöËøáÂºÄÊ∫êÁ§æÂå∫ÁöÑÂäõÈáèÂ§çÁé∞SoraÔºåÁî±ÂåóÂ§ß-ÂÖîÂ±ïAIGCËÅîÂêàÂÆûÈ™åÂÆ§ÂÖ±ÂêåÂèëËµ∑ÔºåÂΩìÂâçÊàë‰ª¨ËµÑÊ∫êÊúâÈôê‰ªÖÊê≠Âª∫‰∫ÜÂü∫Á°ÄÊû∂ÊûÑÔºåÊó†Ê≥ïËøõË°åÂÆåÊï¥ËÆ≠ÁªÉÔºåÂ∏åÊúõÈÄöËøáÂºÄÊ∫êÁ§æÂå∫ÈÄêÊ≠•Â¢ûÂä†Ê®°ÂùóÂπ∂Á≠πÈõÜËµÑÊ∫êËøõË°åËÆ≠ÁªÉÔºåÂΩìÂâçÁâàÊú¨Á¶ªÁõÆÊ†áÂ∑ÆË∑ùÂ∑®Â§ßÔºå‰ªçÈúÄÊåÅÁª≠ÂÆåÂñÑÂíåÂø´ÈÄüËø≠‰ª£ÔºåÊ¨¢ËøéPull requestÔºÅÔºÅÔºÅ&lt;/p&gt; &#xA;&lt;p&gt;Project stages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Primary&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup the codebase and train a un-conditional model on a landscape dataset.&lt;/li&gt; &#xA; &lt;li&gt;Train models that boost resolution and duration.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extensions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Conduct text2video experiments on landscape dataset.&lt;/li&gt; &#xA; &lt;li&gt;Train the 1080p model on video2text dataset.&lt;/li&gt; &#xA; &lt;li&gt;Control model with more conditions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div style=&#34;display: flex; justify-content: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/we_want_you.jpg&#34; width=&#34;200&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/framework.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.08]&lt;/strong&gt; We support the training code of text condition with 16 frames of 512x512, but the environment is different from the class condition training code (please use the official command on &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install torch 2.1 and cuda 11.8 ). The code is written according to the sample code of &lt;a href=&#34;https://github.com/Vchitect/Latte?tab=readme-ov-file&#34;&gt;Latte&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.07]&lt;/strong&gt; We support training with 128 frames (when sample rate = 3, which is about 13 seconds) of 256x256, or 64 frames (which is about 6 seconds) of 512x512.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.05]&lt;/strong&gt; See our latest &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#todo&#34;&gt;todo&lt;/a&gt;, pull requests are welcome.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; We re-organizes and modulizes our code to make it easy to &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#how-to-contribute-to-the-open-sora-plan-community&#34;&gt;contribute&lt;/a&gt; to the project, to contribute please see the &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#repo-structure&#34;&gt;Repo structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.03]&lt;/strong&gt; We opened some &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/discussions&#34;&gt;discussions&lt;/a&gt; to clarify several issues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.01]&lt;/strong&gt; Training code is available now! Learn more on our &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;project page&lt;/a&gt;. Please feel free to watch üëÄ this repository for the latest updates.&lt;/p&gt; &#xA;&lt;h2&gt;‚úä Todo&lt;/h2&gt; &#xA;&lt;h4&gt;Setup the codebase and train a unconditional model on landscape dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix typos &amp;amp; Update readme. ü§ù Thanks to &lt;a href=&#34;https://github.com/mio2333&#34;&gt;@mio2333&lt;/a&gt;, &lt;a href=&#34;https://github.com/CreamyLong&#34;&gt;@CreamyLong&lt;/a&gt;, &lt;a href=&#34;https://github.com/chg0901&#34;&gt;@chg0901&lt;/a&gt;, &lt;a href=&#34;https://github.com/Nyx-177&#34;&gt;@Nyx-177&lt;/a&gt;, &lt;a href=&#34;https://github.com/HowardLi1984&#34;&gt;@HowardLi1984&lt;/a&gt;, &lt;a href=&#34;https://github.com/sennnnn&#34;&gt;@sennnnn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Setup repo-structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add docker file. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enable type hints for functions. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Video-VQGAN model, which is borrowed from &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support variable aspect ratios, resolutions, durations training on &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support Dynamic mask input inspired by &lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add class-conditioning on embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Incorporating &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; as main codebase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add VAE model, which is borrowed from &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Joint dynamic mask input with VAE.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add VQVAE from &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;VQGAN&lt;/a&gt;. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make the codebase ready for the cluster training. Add SLURM scripts. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Refactor VideoGPT. ü§ù Thanks to &lt;a href=&#34;https://github.com/qqingzheng&#34;&gt;@qqingzheng&lt;/a&gt;, &lt;a href=&#34;https://github.com/luo3300612&#34;&gt;@luo3300612&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add sampling script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Incorporate &lt;a href=&#34;https://github.com/willisma/SiT&#34;&gt;SiT&lt;/a&gt;. ü§ù Thanks to &lt;a href=&#34;https://github.com/khan-yin&#34;&gt;@khan-yin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add eavluation scripts (FVD, CLIP score). üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Train models that boost resolution and duration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add &lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;PI&lt;/a&gt; to support out-of-domain size. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add 2D RoPE to improve generalization ability as &lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Extract offline feature.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add frame interpolation model. ü§ù Thanks to &lt;a href=&#34;https://github.com/yunyangge&#34;&gt;@yunyangge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add super resolution model. ü§ù Thanks to &lt;a href=&#34;https://github.com/Linzy19&#34;&gt;@Linzy19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add accelerate to automatically manage training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Joint training with images. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporate &lt;a href=&#34;https://arxiv.org/abs/2307.06304&#34;&gt;NaViT&lt;/a&gt;. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add &lt;a href=&#34;https://github.com/arthur-qiu/FreeNoise-LaVie&#34;&gt;FreeNoise&lt;/a&gt; support for training-free longer video generation. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Conduct text2video experiments on landscape dataset.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish data loading, pre-processing utils. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add CLIP and T5 support. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add text2image training script. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add prompt captioner. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt; üöÄ &lt;strong&gt;[Require more computation]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Train the 1080p model on video2text dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Looking for a suitable dataset, welcome to discuss and recommend. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish data loading, and pre-processing utils. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support memory friendly training. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add flash-attention2 from pytorch.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add xformers.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support mixed precision training.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add gradient checkpoint.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support for ReBased and Ring attention. ü§ù Thanks to &lt;a href=&#34;https://github.com/kabachuha&#34;&gt;@kabachuha&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train using the deepspeed engine. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate with &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/issues/59#issue-2170735221&#34;&gt;Colossal-AI&lt;/a&gt; for a cheaper, faster, and more efficient. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train with a text condition. Here we could conduct different experiments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train with T5 conditioning. üöÄ &lt;strong&gt;[Require more computation]&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train with CLIP conditioning. üöÄ &lt;strong&gt;[Require more computation]&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train with CLIP + T5 conditioning (probably costly during training and experiments). üöÄ &lt;strong&gt;[Require more computation]&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Control model with more condition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Load pretrained weights from &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;. ‚åõ [WIP]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;. üôè &lt;strong&gt;[Need your contribution]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÇ Repo structure (WIP)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ README.md&#xA;‚îú‚îÄ‚îÄ docs&#xA;‚îÇ   ‚îú‚îÄ‚îÄ Data.md                    -&amp;gt; Datasets description.&#xA;‚îÇ   ‚îú‚îÄ‚îÄ Contribution_Guidelines.md -&amp;gt; Contribution guidelines description.&#xA;‚îú‚îÄ‚îÄ scripts                        -&amp;gt; All scripts.&#xA;‚îú‚îÄ‚îÄ opensora&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ dataset&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ models&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ ae                     -&amp;gt; Compress videos to latents&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ imagebase&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ vae&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ vqvae&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ videobase&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ vae&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ vqvae&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ captioner&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ diffusion              -&amp;gt; Denoise latents&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ diffusion         &#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ dit&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ latte&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ unet&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ frame_interpolation&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ super_resolution&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ sample&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train                      -&amp;gt; Training code&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;The requirements are as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/PKU-YuanGroup/Open-Sora-Plan&#xA;cd Open-Sora-Plan&#xA;conda create -n opensora python=3.8 -y&#xA;conda activate opensora&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üóùÔ∏è Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/docs/Data.md&#34;&gt;Data.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video-VQVAE (VideoGPT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;p&gt;To train VQVAE, run the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scripts/train_vqvae.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can modify the training parameters within the script. For training parameters, please refer to &lt;a href=&#34;https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments&#34;&gt;transformers.TrainingArguments&lt;/a&gt;. Other parameters are explained as follows:&lt;/p&gt; &#xA;&lt;h5&gt;VQ-VAE Specific Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--embedding_dim&lt;/code&gt;: number of dimensions for codebooks embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_codes 2048&lt;/code&gt;: number of codes in the codebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_hiddens 240&lt;/code&gt;: number of hidden features in the residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_res_layers 4&lt;/code&gt;: number of residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--downsample &#34;4,4,4&#34;&lt;/code&gt;: T H W downsampling stride of the encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Dataset Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path &amp;lt;path&amp;gt;&lt;/code&gt;: path to an &lt;code&gt;hdf5&lt;/code&gt; file or a folder containing &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; folders with subdirectories of videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resolution 128&lt;/code&gt;: spatial resolution to train on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_length 16&lt;/code&gt;: temporal resolution, or video clip length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reconstructing&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python examples/rec_video.py --video-path &#34;assets/origin_video_0.mp4&#34; --rec-path &#34;rec_video_0.mp4&#34; --num-frames 500 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python examples/rec_video.py --video-path &#34;assets/origin_video_1.mp4&#34; --rec-path &#34;rec_video_1.mp4&#34; --resolution 196 --num-frames 600 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We present four reconstructed videos in this demonstration, arranged from left to right as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;3s 596x336&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;10s 256x256&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;18s 196x196&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;24s 168x96&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_2.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_0.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_1.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_3.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Others&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to the document &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/docs/VQVAE.md&#34;&gt;VQVAE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;VideoDiT (DiT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/loss.jpg&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Sampling&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/sample.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ù How to Contribute to the Open-Sora Plan Community&lt;/h2&gt; &#xA;&lt;p&gt;We greatly appreciate your contributions to the Open-Sora Plan open-source community and helping us make it even better than it is now!&lt;/p&gt; &#xA;&lt;p&gt;For more details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/docs/Contribution_Guidelines.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üëç Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;: The &lt;strong&gt;main codebase&lt;/strong&gt; we built upon and it is an wonderful video gererated model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT&lt;/a&gt;: Video Generation using VQ-VAE and Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;: Flexible Vision Transformer for Diffusion Model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;Positional Interpolation&lt;/a&gt;: Extending Context Window of Large Language Models via Positional Interpolation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîí License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only. See &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ú® Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PKU-YuanGroup/Open-Sora-Plan&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PKU-YuanGroup/Open-Sora-Plan&#34; alt=&#34;Star History&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KimMeen/Time-LLM</title>
    <updated>2024-03-08T01:35:32Z</updated>
    <id>tag:github.com,2024-03-08:/KimMeen/Time-LLM</id>
    <link href="https://github.com/KimMeen/Time-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2024] Official implementation of &#34; ü¶ô Time-LLM: Time Series Forecasting by Reprogramming Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;h1&gt;&lt;b&gt; Time-LLM &lt;/b&gt;&lt;/h1&gt; --&gt; &#xA; &lt;!-- &lt;h2&gt;&lt;b&gt; Time-LLM &lt;/b&gt;&lt;/h2&gt; --&gt; &#xA; &lt;h2&gt;&lt;b&gt; (ICLR&#39;24) Time-LLM: Time Series Forecasting by Reprogramming Large Language Models &lt;/b&gt;&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/KimMeen/Time-LLM?color=green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/KimMeen/Time-LLM?color=yellow&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/KimMeen/Time-LLM?color=lightblue&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;[&lt;a href=&#34;https://arxiv.org/abs/2310.01728&#34;&gt;Paper Page&lt;/a&gt;]&lt;/strong&gt; &lt;strong&gt;[&lt;a href=&#34;https://www.youtube.com/watch?v=L-hRexVa32k&amp;amp;t=160s&#34;&gt;Paper Explained&lt;/a&gt;]&lt;/strong&gt; &lt;strong&gt;[&lt;a href=&#34;https://mp.weixin.qq.com/s/FSxUdvPI713J2LiHnNaFCw&#34;&gt;‰∏≠ÊñáËß£ËØª1&lt;/a&gt;]&lt;/strong&gt; &lt;strong&gt;[&lt;a href=&#34;https://mp.weixin.qq.com/s/nUiQGnHOkWznoBPqM0KHXg&#34;&gt;‰∏≠ÊñáËß£ËØª2&lt;/a&gt;]&lt;/strong&gt; &lt;strong&gt;[&lt;a href=&#34;https://zhuanlan.zhihu.com/p/676256783&#34;&gt;‰∏≠ÊñáËß£ËØª3&lt;/a&gt;]&lt;/strong&gt; &lt;strong&gt;[&lt;a href=&#34;https://mp.weixin.qq.com/s/ZnR33epXCB7N5Y_kO0YJ_w&#34;&gt;‰∏≠ÊñáËß£ËØª4&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KimMeen/Time-LLM/main/figures/logo.png&#34; width=&#34;60&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üôã Please let us know if you find out a mistake or have any suggestions!&lt;/p&gt; &#xA; &lt;p&gt;üåü If you find this resource helpful, please consider to star this repository and cite our research:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{jin2023time,&#xA;  title={{Time-LLM}: Time series forecasting by reprogramming large language models},&#xA;  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},&#xA;  booktitle={International Conference on Learning Representations (ICLR)},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Time-LLM is a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. Notably, we show that time series analysis (e.g., forecasting) can be cast as yet another &#34;language task&#34; that can be effectively tackled by an off-the-shelf LLM.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KimMeen/Time-LLM/main/figures/framework.png&#34; height=&#34;360&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Time-LLM comprises two key components: (1) reprogramming the input time series into text prototype representations that are more natural for the LLM, and (2) augmenting the input context with declarative prompts (e.g., domain expert knowledge and task instructions) to guide LLM reasoning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KimMeen/Time-LLM/main/figures/method-detailed-illustration.png&#34; height=&#34;190&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;accelerate==0.20.3&lt;/li&gt; &#xA; &lt;li&gt;einops==0.7.0&lt;/li&gt; &#xA; &lt;li&gt;matplotlib==3.7.0&lt;/li&gt; &#xA; &lt;li&gt;numpy==1.23.5&lt;/li&gt; &#xA; &lt;li&gt;pandas==1.5.3&lt;/li&gt; &#xA; &lt;li&gt;scikit_learn==1.2.2&lt;/li&gt; &#xA; &lt;li&gt;scipy==1.5.4&lt;/li&gt; &#xA; &lt;li&gt;torch==2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tqdm==4.65.0&lt;/li&gt; &#xA; &lt;li&gt;peft==0.4.0&lt;/li&gt; &#xA; &lt;li&gt;transformers==4.31.0&lt;/li&gt; &#xA; &lt;li&gt;deepspeed==0.13.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To install all dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;You can access the well pre-processed datasets from &lt;a href=&#34;https://drive.google.com/file/d/1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP/view?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;, then place the downloaded contents under &lt;code&gt;./dataset&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Demos&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download datasets and place them under &lt;code&gt;./dataset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tune the model. We provide five experiment scripts for demonstration purpose under the folder &lt;code&gt;./scripts&lt;/code&gt;. For example, you can evaluate on ETT datasets by:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/TimeLLM_ETTh1.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/TimeLLM_ETTh2.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/TimeLLM_ETTm1.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/TimeLLM_ETTm2.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Detailed usage&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;run_main.py&lt;/code&gt; and &lt;code&gt;run_m4.py&lt;/code&gt; for the detailed description of each hyperparameter.&lt;/p&gt; &#xA;&lt;h2&gt;Further Reading&lt;/h2&gt; &#xA;&lt;p&gt;1, &lt;a href=&#34;https://arxiv.org/abs/2310.10196&#34;&gt;&lt;strong&gt;Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook&lt;/strong&gt;&lt;/a&gt;, in &lt;em&gt;arXiv&lt;/em&gt; 2023. &lt;a href=&#34;https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM&#34;&gt;[GitHub Repo]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Ming Jin, Qingsong Wen*, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li (IEEE Fellow), Shirui Pan*, Vincent S. Tseng (IEEE Fellow), Yu Zheng (IEEE Fellow), Lei Chen (IEEE Fellow), Hui Xiong (IEEE Fellow)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{jin2023lm4ts,&#xA;  title={Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook}, &#xA;  author={Ming Jin and Qingsong Wen and Yuxuan Liang and Chaoli Zhang and Siqiao Xue and Xue Wang and James Zhang and Yi Wang and Haifeng Chen and Xiaoli Li and Shirui Pan and Vincent S. Tseng and Yu Zheng and Lei Chen and Hui Xiong},&#xA;  journal={arXiv preprint arXiv:2310.10196},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;2, &lt;a href=&#34;https://arxiv.org/abs/2402.02713&#34;&gt;&lt;strong&gt;Position Paper: What Can Large Language Models Tell Us about Time Series Analysis&lt;/strong&gt;&lt;/a&gt;, in &lt;em&gt;arXiv&lt;/em&gt; 2024.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang*, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen*&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{jin2024position,&#xA;   title={Position Paper: What Can Large Language Models Tell Us about Time Series Analysis}, &#xA;   author={Ming Jin and Yifan Zhang and Wei Chen and Kexin Zhang and Yuxuan Liang and Bin Yang and Jindong Wang and Shirui Pan and Qingsong Wen},&#xA;  journal={arXiv preprint arXiv:2402.02713},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;3, &lt;a href=&#34;https://arxiv.org/abs/2202.07125&#34;&gt;&lt;strong&gt;Transformers in Time Series: A Survey&lt;/strong&gt;&lt;/a&gt;, in IJCAI 2023. &lt;a href=&#34;https://github.com/qingsongedu/time-series-transformers-review&#34;&gt;[GitHub Repo]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, Liang Sun&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{wen2023transformers,&#xA;  title={Transformers in time series: A survey},&#xA;  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},&#xA;  booktitle={International Joint Conference on Artificial Intelligence(IJCAI)},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our implementation adapts &lt;a href=&#34;https://github.com/thuml/Time-Series-Library&#34;&gt;Time-Series-Library&lt;/a&gt; and &lt;a href=&#34;https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All&#34;&gt;GPT4TS&lt;/a&gt; as the code base and have extensively modified it to our purposes. We thank the authors for sharing their implementations and related resources.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>anthropics/anthropic-sdk-python</title>
    <updated>2024-03-08T01:35:32Z</updated>
    <id>tag:github.com,2024-03-08:/anthropics/anthropic-sdk-python</id>
    <link href="https://github.com/anthropics/anthropic-sdk-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic Python API library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/anthropic/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/anthropic.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Anthropic Python library provides convenient access to the Anthropic REST API from any Python 3.7+ application. It includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href=&#34;https://github.com/encode/httpx&#34;&gt;httpx&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The REST API documentation can be found &lt;a href=&#34;https://docs.anthropic.com/claude/reference/&#34;&gt;on docs.anthropic.com&lt;/a&gt;. The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install from PyPI&#xA;pip install anthropic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from anthropic import Anthropic&#xA;&#xA;client = Anthropic(&#xA;    # This is the default and can be omitted&#xA;    api_key=os.environ.get(&#34;ANTHROPIC_API_KEY&#34;),&#xA;)&#xA;&#xA;message = client.messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;)&#xA;print(message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href=&#34;https://pypi.org/project/python-dotenv/&#34;&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;ANTHROPIC_API_KEY=&#34;my-anthropic-api-key&#34;&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API Key is not stored in source control.&lt;/p&gt; &#xA;&lt;h2&gt;Async usage&lt;/h2&gt; &#xA;&lt;p&gt;Simply import &lt;code&gt;AsyncAnthropic&lt;/code&gt; instead of &lt;code&gt;Anthropic&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import asyncio&#xA;from anthropic import AsyncAnthropic&#xA;&#xA;client = AsyncAnthropic(&#xA;    # This is the default and can be omitted&#xA;    api_key=os.environ.get(&#34;ANTHROPIC_API_KEY&#34;),&#xA;)&#xA;&#xA;&#xA;async def main() -&amp;gt; None:&#xA;    message = await client.messages.create(&#xA;        max_tokens=1024,&#xA;        messages=[&#xA;            {&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;            }&#xA;        ],&#xA;        model=&#34;claude-3-opus-20240229&#34;,&#xA;    )&#xA;    print(message.content)&#xA;&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; &#xA;&lt;h2&gt;Streaming Responses&lt;/h2&gt; &#xA;&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anthropic import Anthropic&#xA;&#xA;client = Anthropic()&#xA;&#xA;stream = client.messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;    stream=True,&#xA;)&#xA;for event in stream:&#xA;    print(event.type)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anthropic import AsyncAnthropic&#xA;&#xA;client = AsyncAnthropic()&#xA;&#xA;stream = await client.messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;    stream=True,&#xA;)&#xA;async for event in stream:&#xA;    print(event.type)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Streaming Helpers&lt;/h3&gt; &#xA;&lt;p&gt;This library provides several conveniences for streaming messages, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import asyncio&#xA;from anthropic import AsyncAnthropic&#xA;&#xA;client = AsyncAnthropic()&#xA;&#xA;async def main() -&amp;gt; None:&#xA;    async with client.messages.stream(&#xA;        max_tokens=1024,&#xA;        messages=[&#xA;            {&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: &#34;Say hello there!&#34;,&#xA;            }&#xA;        ],&#xA;        model=&#34;claude-3-opus-20240229&#34;,&#xA;    ) as stream:&#xA;        async for text in stream.text_stream:&#xA;            print(text, end=&#34;&#34;, flush=True)&#xA;        print()&#xA;&#xA;    message = await stream.get_final_message()&#xA;    print(message.model_dump_json(indent=2))&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Streaming with &lt;code&gt;client.messages.stream(...)&lt;/code&gt; exposes &lt;a href=&#34;https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/helpers.md&#34;&gt;various helpers for your convenience&lt;/a&gt; including event handlers and accumulation.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can use &lt;code&gt;client.messages.create(..., stream=True)&lt;/code&gt; which only returns an async iterable of the events in the stream and thus uses less memory (it does not build up a final message object for you).&lt;/p&gt; &#xA;&lt;h2&gt;AWS Bedrock&lt;/h2&gt; &#xA;&lt;p&gt;This library also provides support for the &lt;a href=&#34;https://aws.amazon.com/bedrock/claude/&#34;&gt;Anthropic Bedrock API&lt;/a&gt; if you install this library with the &lt;code&gt;bedrock&lt;/code&gt; extra, e.g. &lt;code&gt;pip install -U anthropic[bedrock]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then import and instantiate a separate &lt;code&gt;AnthropicBedrock&lt;/code&gt; class, the rest of the API is the same.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from anthropic import AnthropicBedrock&#xA;&#xA;client = AnthropicBedrock()&#xA;&#xA;message = client.messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello!&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;,&#xA;)&#xA;print(message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more fully fledged example see &lt;a href=&#34;https://github.com/anthropics/anthropic-sdk-python/raw/main/examples/bedrock.py&#34;&gt;&lt;code&gt;examples/bedrock.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Token counting&lt;/h2&gt; &#xA;&lt;p&gt;You can see the exact usage for a given request through the &lt;code&gt;usage&lt;/code&gt; response property, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;message = client.messages.create(...)&#xA;message.usage&#xA;# Usage(input_tokens=25, output_tokens=13)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using types&lt;/h2&gt; &#xA;&lt;p&gt;Nested request parameters are &lt;a href=&#34;https://docs.python.org/3/library/typing.html#typing.TypedDict&#34;&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href=&#34;https://docs.pydantic.dev&#34;&gt;Pydantic models&lt;/a&gt;, which provide helper methods for things like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.model_dump_json(indent=2, exclude_unset=True)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.model_dump(exclude_unset=True)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Handling errors&lt;/h2&gt; &#xA;&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;anthropic.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; &#xA;&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;anthropic.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; &#xA;&lt;p&gt;All errors inherit from &lt;code&gt;anthropic.APIError&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import anthropic&#xA;from anthropic import Anthropic&#xA;&#xA;client = Anthropic()&#xA;&#xA;try:&#xA;    client.messages.create(&#xA;        max_tokens=1024,&#xA;        messages=[&#xA;            {&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;            }&#xA;        ],&#xA;        model=&#34;claude-3-opus-20240229&#34;,&#xA;    )&#xA;except anthropic.APIConnectionError as e:&#xA;    print(&#34;The server could not be reached&#34;)&#xA;    print(e.__cause__)  # an underlying Exception, likely raised within httpx.&#xA;except anthropic.RateLimitError as e:&#xA;    print(&#34;A 429 status code was received; we should back off a bit.&#34;)&#xA;except anthropic.APIStatusError as e:&#xA;    print(&#34;Another non-200-range status code was received&#34;)&#xA;    print(e.status_code)&#xA;    print(e.response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Error codes are as followed:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Status Code&lt;/th&gt; &#xA;   &lt;th&gt;Error Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;401&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;403&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;404&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;422&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;429&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;gt;=500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Retries&lt;/h3&gt; &#xA;&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anthropic import Anthropic&#xA;&#xA;# Configure the default for all requests:&#xA;client = Anthropic(&#xA;    # default is 2&#xA;    max_retries=0,&#xA;)&#xA;&#xA;# Or, configure per-request:&#xA;client.with_options(max_retries=5).messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Timeouts&lt;/h3&gt; &#xA;&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href=&#34;https://www.python-httpx.org/advanced/#fine-tuning-the-configuration&#34;&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anthropic import Anthropic&#xA;&#xA;# Configure the default for all requests:&#xA;client = Anthropic(&#xA;    # 20 seconds (default is 10 minutes)&#xA;    timeout=20.0,&#xA;)&#xA;&#xA;# More granular control:&#xA;client = Anthropic(&#xA;    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),&#xA;)&#xA;&#xA;# Override per-request:&#xA;client.with_options(timeout=5 * 1000).messages.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; &#xA;&lt;p&gt;Note that requests that time out are &lt;a href=&#34;https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#retries&#34;&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Default Headers&lt;/h2&gt; &#xA;&lt;p&gt;We automatically send the &lt;code&gt;anthropic-version&lt;/code&gt; header set to &lt;code&gt;2023-06-01&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you need to, you can override it by setting default headers per-request or on the client object.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that doing so may result in incorrect types and other unexpected or undefined behavior in the SDK.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anthropic import Anthropic&#xA;&#xA;client = Anthropic(&#xA;    default_headers={&#34;anthropic-version&#34;: &#34;My-Custom-Value&#34;},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced&lt;/h2&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;We use the standard library &lt;a href=&#34;https://docs.python.org/3/library/logging.html&#34;&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;ANTHROPIC_LOG&lt;/code&gt; to &lt;code&gt;debug&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export ANTHROPIC_LOG=debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; &#xA;&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;if response.my_field is None:&#xA;  if &#39;my_field&#39; not in response.model_fields_set:&#xA;    print(&#39;Got json like {}, without a &#34;my_field&#34; key present at all.&#39;)&#xA;  else:&#xA;    print(&#39;Got json like {&#34;my_field&#34;: null}.&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;raw&#34; Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from anthropic import Anthropic&#xA;&#xA;client = Anthropic()&#xA;response = client.messages.with_raw_response.create(&#xA;    max_tokens=1024,&#xA;    messages=[{&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;    }],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;)&#xA;print(response.headers.get(&#39;X-My-Header&#39;))&#xA;&#xA;message = response.parse()  # get the object that `messages.create()` would have returned&#xA;print(message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These methods return an &lt;a href=&#34;https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_legacy_response.py&#34;&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we&#39;re changing it slightly in the next major version.&lt;/p&gt; &#xA;&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; &#xA;&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; &#xA;&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; &#xA;&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href=&#34;https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_response.py&#34;&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href=&#34;https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_response.py&#34;&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with client.messages.with_streaming_response.create(&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Hello, Claude&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;) as response:&#xA;    print(response.headers.get(&#34;X-My-Header&#34;))&#xA;&#xA;    for line in response.iter_lines():&#xA;        print(line)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; &#xA;&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; &#xA;&lt;p&gt;You can directly override the &lt;a href=&#34;https://www.python-httpx.org/api/#client&#34;&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for proxies&lt;/li&gt; &#xA; &lt;li&gt;Custom transports&lt;/li&gt; &#xA; &lt;li&gt;Additional &lt;a href=&#34;https://www.python-httpx.org/advanced/#client-instances&#34;&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import httpx&#xA;from anthropic import Anthropic&#xA;&#xA;client = Anthropic(&#xA;    # Or use the `ANTHROPIC_BASE_URL` env var&#xA;    base_url=&#34;http://my.test.server.example.com:8083&#34;,&#xA;    http_client=httpx.Client(&#xA;        proxies=&#34;http://my.test.proxy.example.com&#34;,&#xA;        transport=httpx.HTTPTransport(local_address=&#34;0.0.0.0&#34;),&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; &#xA;&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href=&#34;https://docs.python.org/3/reference/datamodel.html#object.__del__&#34;&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; &#xA;&lt;h2&gt;Versioning&lt;/h2&gt; &#xA;&lt;p&gt;This package generally follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; &#xA; &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals)&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; &#xA;&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&#34;https://www.github.com/anthropics/anthropic-sdk-python/issues&#34;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.7 or higher.&lt;/p&gt;</summary>
  </entry>
</feed>