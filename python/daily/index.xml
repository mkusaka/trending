<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-04T01:29:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fishaudio/fish-speech</title>
    <updated>2024-05-04T01:29:33Z</updated>
    <id>tag:github.com,2024-05-04:/fishaudio/fish-speech</id>
    <link href="https://github.com/fishaudio/fish-speech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Brand new TTS solution&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fish Speech&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://discord.gg/Es5qTB9BcN&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=jCKlUP7QgSm9kh95UlBoYv6s1I-Apl1M&amp;amp;authKey=xI5ttVAp3do68IpEYEalwXSYZFdfxZSkah%2BctF5FIMyN2NqAa003vFtLqJyAVRfF&amp;amp;noverify=0&amp;amp;group_code=593946093&#34;&gt; &lt;img alt=&#34;QQ&#34; src=&#34;https://img.shields.io/badge/QQ Group-%2312B7F5?logo=tencent-qq&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://hub.docker.com/r/lengyue233/fish-speech&#34;&gt; &lt;img alt=&#34;Docker&#34; src=&#34;https://img.shields.io/docker/pulls/lengyue233/fish-speech?style=flat-square&amp;amp;logo=docker&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/fishaudio/fish-speech/actions/workflows/build-windows-package.yml&#34;&gt; &lt;img alt=&#34;Docker&#34; src=&#34;https://img.shields.io/github/actions/workflow/status/fishaudio/fish-speech/build-windows-package.yml?style=flat-square&amp;amp;label=Build%20Windows%20Package&amp;amp;logo=github&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This codebase is released under BSD-3-Clause License, and all models are released under CC-BY-NC-SA-4.0 License. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;此代码库根据 BSD-3-Clause 许可证发布, 所有模型根据 CC-BY-NC-SA-4.0 许可证发布。请参阅 &lt;a href=&#34;https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; 了解更多细节.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer / 免责声明&lt;/h2&gt; &#xA;&lt;p&gt;We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.&lt;br&gt; 我们不对代码库的任何非法使用承担任何责任. 请参阅您当地关于 DMCA (数字千年法案) 和其他相关法律法规.&lt;/p&gt; &#xA;&lt;h2&gt;Documents / 文档&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/en/&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/&#34;&gt;中文&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Samples / 例子&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/en/samples/&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/samples/&#34;&gt;中文&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits / 鸣谢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;VITS2 (daniilrobnikov)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fishaudio/Bert-VITS2&#34;&gt;Bert-VITS2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/innnky/gpt-vits&#34;&gt;GPT VITS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/b04901014/MQTTS&#34;&gt;MQTTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;GPT Fast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsor / 赞助&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://6block.com/&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/60573493&#34; width=&#34;100&#34; height=&#34;100&#34; alt=&#34;6Block Avatar&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://6block.com/&#34;&gt;数据处理服务器由 6Block 提供 (Data Processing sponsor by 6Block)&lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>dnhkng/GlaDOS</title>
    <updated>2024-05-04T01:29:33Z</updated>
    <id>tag:github.com,2024-05-04:/dnhkng/GlaDOS</id>
    <link href="https://github.com/dnhkng/GlaDOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the Personality Core for GLaDOS, the first steps towards a real-life implementation of the AI from the Portal series by Valve.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLaDOS Personality Core&lt;/h1&gt; &#xA;&lt;p&gt;This is a project dedicated to building a real-life version of GLaDOS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KbUfWpykBGg&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/KbUfWpykBGg/0.jpg&#34; alt=&#34;localGLaDOS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This is a hardware and software project that will create an aware, interactive, and embodied GLaDOS.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will entail:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train GLaDOS voice generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generate a prompt that leads to a realistic &#34;Personality Core&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate a &lt;a href=&#34;https://memgpt.readthedocs.io/en/latest/&#34;&gt;MemGPT&lt;/a&gt; medium- and long-term memory for GLaDOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Give GLaDOS vision via &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create 3D-printable parts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Design the animatronics system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The initial goals are to develop a low-latency platform, where GLaDOS can respond to voice interactions within 600ms.&lt;/p&gt; &#xA;&lt;p&gt;To do this, the system constantly records data to a circular buffer, waiting for &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;voice to be detected&lt;/a&gt;. When it&#39;s determined that the voice has stopped (including detection of normal pauses), it will be &lt;a href=&#34;https://github.com/huggingface/distil-whisper&#34;&gt;transcribed quickly&lt;/a&gt;. This is then passed to streaming &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;local Large Language Model&lt;/a&gt;, where the streamed text is broken by sentence, and passed to a &lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;text-to-speech system&lt;/a&gt;. This means further sentences can be generated while the current is playing, reducing latency substantially.&lt;/p&gt; &#xA;&lt;h3&gt;Subgoals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The other aim of the project is to minimize dependencies, so this can run on constrained hardware. That means no PyTorch or other large packages.&lt;/li&gt; &#xA; &lt;li&gt;As I want to fully understand the system, I have removed a large amount of redirection: which means extracting and rewriting code. i.e. as GLaDOS only speaks English, I have rewritten the wrapper around &lt;a href=&#34;https://espeak.sourceforge.net/&#34;&gt;espeak&lt;/a&gt; and the entire Text-to-Speech subsystem is about 500 LOC and has only 3 dependencies: numpy, onnxruntime, and sounddevice.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hardware System&lt;/h2&gt; &#xA;&lt;p&gt;This will be based on servo- and stepper-motors. 3D printable STL will be provided to create GlaDOS&#39;s body, and she will be given a set of animations to express herself. The vision system will allow her to track and turn toward people and things of interest.&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instruction&lt;/h2&gt; &#xA;&lt;p&gt;If you want to install the TTS Engine on your machine, please follow the steps below. This has only been tested on Linux, but I think it will work on Windows with small tweaks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng&#34;&gt;&lt;code&gt;espeak&lt;/code&gt;&lt;/a&gt; synthesizer according to the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng/raw/master/docs/guide.md&#34;&gt;installation instructions&lt;/a&gt; for your operating system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required Python packages, e.g., by running &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For the LLM, install &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;, and compile it for your CPU or GPU. Edit the LLAMA_SERVER_PATH parameter in glados.py to match your installation path.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For voice recognition, install &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;Whisper.cpp&lt;/a&gt;, and after compiling, run &lt;code&gt;make libwhisper.so&lt;/code&gt; and then move the &#34;libwhisper.so&#34; file to the &#34;glados&#34; folder or add it to your path. For Windows, check out the discussion in my &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/pull/1524&#34;&gt;whisper pull request&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the models:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/ggml-medium-32-2.en.bin?download=true&#34;&gt;voice recognition model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-IQ3_XS.gguf?download=true&#34;&gt;Llama-3 8B&lt;/a&gt; or&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct.IQ4_XS.gguf?download=true&#34;&gt;Llama-3 70B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;and put them in the &#34;models&#34; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;You can test the systems by exploring the &#39;demo.ipynb&#39;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dnhkng/GlaDOS&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dnhkng/GlaDOS&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/9828&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/9828&#34; alt=&#34;dnhkng%2FGlaDOS | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GistNoesis/FourierKAN</title>
    <updated>2024-05-04T01:29:33Z</updated>
    <id>tag:github.com,2024-05-04:/GistNoesis/FourierKAN</id>
    <link href="https://github.com/GistNoesis/FourierKAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FourierKAN&lt;/h1&gt; &#xA;&lt;p&gt;Pytorch Layer for FourierKAN&lt;/p&gt; &#xA;&lt;p&gt;It is a layer intended to be a substitution for Linear + non-linear activation&lt;/p&gt; &#xA;&lt;p&gt;This is inspired by Kolmogorov-Arnold Networks but using 1d fourier coefficients instead of splines coefficients It should be easier to optimize as fourier are more dense than spline (global vs local) Once convergence is reached you can replace the 1d function with spline approximation for faster evaluation giving almost the same result The other advantage of using fourier over spline is that the function are periodic, and therefore more numerically bounded Avoiding the issues of going out of grid&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;put the file in the same directory then&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;from fftKAN import NaiveFourierKANLayer&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;alternatively you can run &lt;code&gt;python fftKAN.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;to see the demo.&lt;/p&gt; &#xA;&lt;p&gt;Code runs, cpu and gpu, but is untested.&lt;/p&gt; &#xA;&lt;p&gt;This is a naive version that use memory proportional to the gridsize, where as a fused version doesn&#39;t require temporary memory&lt;/p&gt; &#xA;&lt;h1&gt;Highlight of the core :&lt;/h1&gt; &#xA;&lt;p&gt;You can either do the simple thing of materializing the product and then do the sum, or you can use einsum to do the reduction. Einsum should use less memory but be slower&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/GistNoesis/FourierKAN/raw/9a8c75311b74ef9a858020edcc29e1a2059cb41e/fftKAN.py#L28-L58&#34;&gt;https://github.com/GistNoesis/FourierKAN/blob/9a8c75311b74ef9a858020edcc29e1a2059cb41e/fftKAN.py#L28-L58&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;License is MIT, but future evolutions (including fused kernels ) will be proprietary.&lt;/p&gt; &#xA;&lt;h1&gt;Fused Operations&lt;/h1&gt; &#xA;&lt;p&gt;This layer use a lot of memory, but by fusing operations we don&#39;t need any extra memory, and we can even use trigonometry tricks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/GistNoesis/FusedFourierKAN&#34;&gt;https://github.com/GistNoesis/FusedFourierKAN&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>