<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-07T01:37:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tomaarsen/attention_sinks</title>
    <updated>2023-10-07T01:37:18Z</updated>
    <id>tag:github.com,2023-10-07:/tomaarsen/attention_sinks</id>
    <link href="https://github.com/tomaarsen/attention_sinks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Extend existing LLMs way beyond the original training length with constant memory usage, and without retraining&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Attention Sinks in Transformers for endless fluent generation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: &lt;code&gt;attention_sinks&lt;/code&gt; adapts pre-trained LLMs to use a modified form of sliding window attention that remains able to produce fluent text indefinitely.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark Findings&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/#benchmark-setups&#34;&gt;Benchmark Setups&lt;/a&gt; for information on how these benchmarks were carried out.&lt;/p&gt; &#xA;&lt;h3&gt;Perplexity&lt;/h3&gt; &#xA;&lt;p&gt;The following figures plot model perplexities under the various different approaches. A higher perplexity is indicative that the model is losing the ability to produce proper language.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Llama-2-7b-hf&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Falcon-7B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tomaarsen/attention_sinks/assets/37621491/8d2e5b88-7158-41ac-8b3a-5a7abe38020d&#34; alt=&#34;llama_2_7b_ppl_vram_plotted&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tomaarsen/attention_sinks/assets/37621491/1be07370-6de7-4a7e-b5ab-3092a5ecb412&#34; alt=&#34;falcon_7b_ppl_vram_plotted&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;MPT-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Pythia-6.9B&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mit-han-lab/streaming-llm/assets/37621491/c96cff66-92a3-43ab-bc21-40232f2740a0&#34; alt=&#34;mpt_7b_ppl_vram_plotted&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tomaarsen/attention_sinks/assets/37621491/b0fee168-fa5a-457d-9e27-8395eb6dfb38&#34; alt=&#34;pythia_6 8b_ppl_vram_plotted&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Mistral-7B-v0.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/torchscale/assets/37621491/3a4c5634-cc1b-42d1-a35a-afb376a4f970&#34; alt=&#34;mistral_7b_ppl_vram_plotted&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The results are clear as day:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;: The VRAM usage is linear as it doesn&#39;t do any windowing. The performance heavily falls after ~4096 tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;windowed&lt;/code&gt;: The VRAM is constant usage due to the windowing at 1024 tokens. However, it fails as soon as the first tokens leave the window.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;attention_sinks&lt;/code&gt;: Constant VRAM usage due to windowing with 4 attention sink tokens + the 1020 most recent tokens. This approach never fails despite the constant VRAM usage.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Fluency during endless generation&lt;/h3&gt; &#xA;&lt;p&gt;See here text generated by the same Llama 2 7B model using the same settings, but loaded using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tomaarsen/attention_sinks/raw/main/demo/endless_logs/transformers/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt;: Loses fluency after ~1900 tokens and starts endlessly generating broken unicode characters like &lt;code&gt; ü§ñüß†üë®‚ÄçÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ&lt;/code&gt; ‚ùå.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tomaarsen/attention_sinks/raw/main/demo/endless_logs/windowed/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;window&lt;/code&gt; attention&lt;/a&gt;: Loses fluency after ~1000 tokens, generates hundreds of newlines interspersed with text like &lt;code&gt;OOOMMOÃ∂OANOOAMOOÃ∂OMMO&lt;/code&gt; ‚ùå.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tomaarsen/attention_sinks/raw/main/demo/endless_logs/attention_sinks/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;attention_sinks&lt;/code&gt;&lt;/a&gt;: Fluent for the full 10k tokens of the test ‚úÖ.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Fluency during subsequent prompting for chat-style LLMs&lt;/h3&gt; &#xA;&lt;p&gt;In this benchmark, I sent subsequent prompts from &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts&#34;&gt;MT-Bench&lt;/a&gt; and automatically detect when fluency gets lost. For Llama-2-7b-chat, &lt;code&gt;transformers&lt;/code&gt; runs out of VRAM, so it can only handle a handful of subsequent prompts.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mistral-7B-Instruct-v0.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Llama-2-7b-chat-hf&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tomaarsen/attention_sinks/assets/37621491/03b3d68b-c315-4ea3-838b-311f3f21402d&#34; alt=&#34;streaming_fluency_loss_mistral_7b_full&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tomaarsen/attention_sinks/assets/37621491/d1a083c4-b2b1-47ad-a181-05f9c802a2f1&#34; alt=&#34;streaming_fluency_loss_llama_7b_full&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Loading models using &lt;code&gt;attention_sinks&lt;/code&gt; has a very positive impact on the fluency of the models across subsequent prompts. However, as can be seen for Llama-2-7B-chat-hf, it does not completely avoid fluency issues.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repository is an open-source implementation of the &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;Efficient Streaming Language Models with Attention Sinks&lt;/a&gt; paper.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extend existing LLMs (e.g. Llama 2) to produce fluent text indefinitely without sacrificing efficiency and performance, without any retraining. Ideal for multi-step LLMs, e.g. chat assistants. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Model perplexities were stable even after 4 million tokens!&lt;/li&gt; &#xA;   &lt;li&gt;Unlike with regular &lt;code&gt;transformers&lt;/code&gt;, memory usage is constant and thus the inference does not get extremely slow due to memory issues at higher sequence lengths.&lt;/li&gt; &#xA;   &lt;li&gt;Models using attention sinks have been shown to perform very well at the task of recalling a value from 20 lines back, even if the model has already processed hundreds of thousands of lines, whereas models using regular dense or window attention fall to 0% after having processed a few thousand tokens.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;attention_sinks&lt;/code&gt; API allows for a drop-in replacement of the &lt;code&gt;transformers&lt;/code&gt; API: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from attention_sinks import AutoModel&#xA;&#xA;model = AutoModel.from_pretrained(&#34;meta-llama/Llama-2-7b-hf&#34;, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support for Llama, Falcon, MPT, GPTNeoX (Pythia) and Mistral models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note: All of these models must be loaded &lt;strong&gt;without&lt;/strong&gt; &lt;code&gt;trust_remote_code=True&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;New parameters to &lt;code&gt;AutoModel....from_pretrained&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;attention_sink_size&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, defaults to 4: The number of initial tokens to use as the attention sink. These tokens are always included in the Attention Sink KV Cache.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;attention_sink_window_size&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, defaults to 1020: The size of the sliding window, i.e. the number of &#34;recent tokens&#34; to include in the Attention Sink KV Cache. A larger window size costs more memory.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See also the &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/#faq&#34;&gt;FAQ&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install &lt;code&gt;attention_sinks&lt;/code&gt; like so&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install attention_sinks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Loading any Llama, Falcon, MPT, GPTNeoX (Pythia) or Mistral model is as simple as loading it in &lt;code&gt;transformers&lt;/code&gt;, the only change is that the model class must be imported from &lt;code&gt;attention_sinks&lt;/code&gt; rather than &lt;code&gt;transformers&lt;/code&gt;, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from attention_sinks import AutoModel&#xA;&#xA;model = AutoModel.from_pretrained(&#34;mosaicml/mpt-7b&#34;, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generation can be done like you would expect from &lt;code&gt;transformers&lt;/code&gt;, e.g. like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, TextStreamer, GenerationConfig&#xA;from attention_sinks import AutoModelForCausalLM&#xA;&#xA;&#xA;# model_id = &#34;meta-llama/Llama-2-7b-hf&#34;&#xA;# model_id = &#34;mistralai/Mistral-7B-v0.1&#34;&#xA;model_id = &#34;mosaicml/mpt-7b&#34;&#xA;# model_id = &#34;tiiuae/falcon-7b&#34;&#xA;# model_id = &#34;EleutherAI/pythia-6.9b-deduped&#34;&#xA;# Note: instruct or chat models also work.&#xA;&#xA;# Load the chosen model and corresponding tokenizer&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_id,&#xA;    # for efficiency:&#xA;    device_map=&#34;auto&#34;,&#xA;    torch_dtype=torch.float16,&#xA;    # `attention_sinks`-specific arguments:&#xA;    attention_sink_size=4,&#xA;    attention_sink_window_size=252, # &amp;lt;- Low for the sake of faster generation&#xA;)&#xA;model.eval()&#xA;tokenizer = AutoTokenizer.from_pretrained(model_id)&#xA;tokenizer.pad_token_id = tokenizer.eos_token_id&#xA;&#xA;# Our input text&#xA;text = &#34;Vaswani et al. (2017) introduced the Transformers&#34;&#xA;&#xA;# Encode the text&#xA;input_ids = tokenizer.encode(text, return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;with torch.no_grad():&#xA;    # A TextStreamer prints tokens as they&#39;re being generated&#xA;    streamer = TextStreamer(tokenizer)&#xA;    generated_tokens = model.generate(&#xA;        input_ids,&#xA;        generation_config=GenerationConfig(&#xA;            # use_cache=True is required, the rest can be changed up.&#xA;            use_cache=True,&#xA;            min_new_tokens=100_000,&#xA;            max_new_tokens=1_000_000,&#xA;            penalty_alpha=0.6,&#xA;            top_k=5,&#xA;            pad_token_id=tokenizer.pad_token_id,&#xA;            eos_token_id=tokenizer.eos_token_id,&#xA;        ),&#xA;        streamer=streamer,&#xA;    )&#xA;    # Decode the final generated text&#xA;    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example will happily generate between 100k and 1m tokens without forgetting how to speak, even on a low-VRAM environment like Google Colab when using &lt;code&gt;load_in_4bit=True&lt;/code&gt; in the &lt;code&gt;AutoModelForCausalLM.from_pretrained&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Demos&lt;/h4&gt; &#xA;&lt;p&gt;You can find a demo script for this endless generation in &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_generation.py&#34;&gt;demo/endless_generation.py&lt;/a&gt;. I already ran this script a few times, resulting in logs for up to 10000 tokens using &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/attention_sinks/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;attention_sinks&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/transformers/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/windowed/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;windowed&lt;/code&gt; (attention)&lt;/a&gt; with Llama 2 7B. The generation settings aren&#39;t ideal, but the logs clearly show that Llama 2 7B with &lt;code&gt;attention_sinks&lt;/code&gt; is the only approach that remains able to generate fluent text.&lt;/p&gt; &#xA;&lt;p&gt;However, if you want to do multi-step generation, which is what &lt;code&gt;attention_sinks&lt;/code&gt; models are well suited for, then you&#39;ll want to try the &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/streaming.py&#34;&gt;demo/streaming.py&lt;/a&gt; demo. This approach is required as the regular &lt;code&gt;model.generate&lt;/code&gt; does not return the required &lt;code&gt;past_key_values&lt;/code&gt; parameter to be passed as history in the next prompt.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark Setups&lt;/h2&gt; &#xA;&lt;h3&gt;Perplexity&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;ve measured the perplexity by computing the negative loss likelihoods against a large text, specifically a book from the &lt;a href=&#34;https://huggingface.co/datasets/emozilla/pg19-test&#34;&gt;pg19&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;p&gt;A collection of ready-to-go scripts have been prepared in &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/benchmark/scripts&#34;&gt;benchmark/scripts&lt;/a&gt; for various model architectures like Llama 2, Falcon, MPT, Mistral and GPT-NeoX (Pythia). Each of these scripts runs the benchmarking and plotting tools described below for pure &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/tomaarsen/attention_sinks&#34;&gt;&lt;code&gt;attention_sinks&lt;/code&gt;&lt;/a&gt; and a third alternative: &lt;code&gt;windowed&lt;/code&gt;, which involves simple windowed attention at a window size of 1024 tokens. Upon completion, the script will plot the figures from &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/#benchmark-findings&#34;&gt;Benchmark Findings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/benchmark&#34;&gt;benchmark&lt;/a&gt; directory also contains directories with outputs of the perplexity benchmarking tool.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Run the benchmarking scripts&lt;/summary&gt; &#xA; &lt;h4&gt;Benchmarking tool&lt;/h4&gt; &#xA; &lt;p&gt;You can run a few benchmarks to compute the perplexity of various models over time using the provided &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/benchmark/perplexity.py&#34;&gt;perplexity.py&lt;/a&gt; benchmarking script. This is done by computing the negative log likelihood losses of the chosen model when it is provided a full book with 60k+ tokens. By default, the scripts stop after 8192 tokens, but this can be modified. An ideal solution continuously has a low log perplexity and a constant CUDA VRAM usage.&lt;/p&gt; &#xA; &lt;p&gt;To use the script, you can run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python benchmark/perplexity.py --experiment attention_sinks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;details&gt;&#xA;  &lt;summary&gt;Full argument list&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code&gt;usage: perplexity.py [-h] [--experiment {attention_sinks,transformers,windowed}] [--model_name_or_path MODEL_NAME_OR_PATH] [--revision REVISION]&#xA;                     [--trust_remote_code] [--dataset_name DATASET_NAME] [--data_column DATA_COLUMN] [--task TASK] [--split {validation,test}]&#xA;                     [--num_tokens NUM_TOKENS] [--output_dir OUTPUT_DIR] [--window_size WINDOW_SIZE] [--attention_sink_size ATTENTION_SINK_SIZE]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --experiment {attention_sinks,transformers,windowed}&#xA;  --model_name_or_path MODEL_NAME_OR_PATH&#xA;  --revision REVISION&#xA;  --trust_remote_code&#xA;  --dataset_name DATASET_NAME&#xA;  --data_column DATA_COLUMN&#xA;  --task TASK&#xA;  --split {validation,test}&#xA;  --num_tokens NUM_TOKENS&#xA;  --output_dir OUTPUT_DIR&#xA;  --window_size WINDOW_SIZE&#xA;  --attention_sink_size ATTENTION_SINK_SIZE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;This script will create a &lt;code&gt;csv&lt;/code&gt; file in the output directory (&lt;code&gt;&#34;benchmarks/outputs&#34;&lt;/code&gt; by default) for that experiment, with information about perplexities, CUDA VRAM usage and latencies.&lt;/p&gt; &#xA; &lt;h4&gt;Plotting tool&lt;/h4&gt; &#xA; &lt;p&gt;The information from the benchmarking tool can be plotted using the &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/benchmark%5Cplot_perplexity.py&#34;&gt;plot_perplexity.py&lt;/a&gt; script. In particular, you can plot any combination of the following features:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;perplexity&lt;/code&gt;,&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;vram&lt;/code&gt;, i.e. CUDA VRAM usage,&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;latency&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For example:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python benchmark/plot_perplexity.py --features perplexity latency --title &#34;Log perplexity &amp;amp; latency of Llama 2 7B as a function of input lengths&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;details&gt;&#xA;  &lt;summary&gt;Full argument list&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code&gt;usage: plot_perplexity.py [-h] [--output_dir OUTPUT_DIR] [--features {perplexity,vram,latency} [{perplexity,vram,latency} ...]] [--title TITLE]&#xA;                          [--log_perplexity_limit LOG_PERPLEXITY_LIMIT] [--skip_first SKIP_FIRST]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --output_dir OUTPUT_DIR&#xA;  --features {perplexity,vram,latency} [{perplexity,vram,latency} ...]&#xA;  --title TITLE&#xA;  --log_perplexity_limit LOG_PERPLEXITY_LIMIT&#xA;  --skip_first SKIP_FIRST&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;This script takes all &lt;code&gt;csv&lt;/code&gt; files from the output directory (&lt;code&gt;&#34;benchmark/outputs&#34;&lt;/code&gt; by default), and creates a plot like so:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python benchmark/plot_perplexity.py --features perplexity vram --title &#34;Log perplexity &amp;amp; VRAM usage of Llama 2 7B as a function of input lengths&#34; --output_dir benchmark/outputs_llama_2_7b --log_perplexity_limit 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/mit-han-lab/streaming-llm/assets/37621491/18802ec4-ed48-42be-ab26-ad9bfb83d0b7&#34; alt=&#34;llama_2_7b_ppl_vram_plotted&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Fluency during endless generation&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;ve measured the fluency during endless generation by running &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_generation.py&#34;&gt;&lt;code&gt;demo/endless_generation.py&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;attention_sinks&lt;/code&gt;, &lt;code&gt;transformers&lt;/code&gt;, and &lt;code&gt;windowed&lt;/code&gt; modes. I ran this script with Llama-2-7B-hf for up to 10000 tokens and manually observed the outputs, which are logged in &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/attention_sinks/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;attention_sinks&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/transformers/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/endless_logs/windowed/meta-llama/Llama-2-7b-hf.txt&#34;&gt;&lt;code&gt;windowed&lt;/code&gt; (attention)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I stopped the generations after I observed loss of fluency.&lt;/p&gt; &#xA;&lt;h3&gt;Fluency across subsequent prompts for chat-style LLMs&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;ve measured the fluency across subsequent prompts by running &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/demo/streaming.py&#34;&gt;&lt;code&gt;demo/streaming.py&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;attention_sinks&lt;/code&gt; and &lt;code&gt;windowed&lt;/code&gt; modes, and parsing the logs. In particular, I automatically classified a response as a failure if it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;contains less than 26 different characters, and&lt;/li&gt; &#xA; &lt;li&gt;is more than 1000 tokens long.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;This FAQ was created by the &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;paper&lt;/a&gt; authors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What does &#34;working on infinite-length inputs&#34; imply for LLMs?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Handling infinite-length text with LLMs presents challenges. Notably, storing all previous Key and Value (KV) states demands significant memory, and models might struggle to generate text beyond their training sequence length. Attention Sink models addresses this by retaining only the most recent tokens and attention sinks, discarding intermediate tokens. This enables the model to generate coherent text from recent tokens without a cache reset ‚Äî a capability not seen in earlier methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Is the context window of LLMs expanded?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;No. The context window remains unchanged. Only the most recent tokens and attention sinks are retained, discarding middle tokens. This means the model can only process the latest tokens. The context window remains constrained by its initial pre-training. For instance, if Llama-2 is pre-trained with a context window of 4096 tokens, then the maximum cache size for an Attention Sink model on Llama-2 remains 4096.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can I input an extensive text, like a book, into an Attention Sink model for summarization?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While you can input a lengthy text, the model will only recognize the latest tokens. Thus, if a book is an input, an Attention Sink model might only summarize the concluding paragraphs, which might not be very insightful. As emphasized earlier, we neither expand the LLMs&#39; context window nor enhance their long-term memory. An Attention Sink model&#39;s strength lies in generating fluent text from recent tokens without needing a cache refresh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What is the ideal use case for Attention Sink models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Attention Sink models are optimized for streaming applications, such as multi-round dialogues. It&#39;s ideal for scenarios where a model needs to operate continually without requiring extensive memory or dependency on past data. An example is a daily assistant based on LLMs. Attention Sink models would let the model function continuously, basing its responses on recent conversations without needing to refresh its cache. Earlier methods would either need a cache reset when the conversation length exceeded the training length (losing recent context) or recompute KV states from recent text history, which can be time-consuming.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How does the Attention Sink approach relate to recent works on context extension?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Attention Sink method is orthogonal to recent context extension methods and can be integrated with them. In the context of Attention Sink models, &#34;context extension&#34; refers to the possibility of using a larger cache size to store more recent tokens. For a practical demonstration, refer to Figure 9 in the &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;paper&lt;/a&gt;, where LongChat-7B-v1.5-32K and Llama-2-7B-32K-Instruct are adapted with Attention Sinks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tomaarsen/attention_sinks/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for all release information.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by, and adapted from &lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;StreamingLLM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{xiao2023streamingllm,&#xA;    title={Efficient Streaming Language Models with Attention Sinks},&#xA;    author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},&#xA;    journal={arXiv},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mouredev/python-web</title>
    <updated>2023-10-07T01:37:18Z</updated>
    <id>tag:github.com,2023-10-07:/mouredev/python-web</id>
    <link href="https://github.com/mouredev/python-web" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Curso para aprender desarrollo frontend Web con Python puro desde cero. Elaborado durante las emisiones en directo desde Twitch de MoureDev.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Web&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.11+-yellow?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Reflex-0.2.8+-5646ED?style=for-the-badge&amp;amp;logo=reflex&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;FastAPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Curso para aprender desarrollo web frontend con Python puro y Reflex desde cero&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/python-web/main/Images/header.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Proyecto realizado durante emisiones en directo desde &lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h5&gt;Si consideras √∫til el curso, ap√≥yalo haciendo &#34;‚òÖ Star&#34; en el repositorio. ¬°Gracias!&lt;/h5&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pr√≥xima Clase: 11/10/2023&lt;/h2&gt; &#xA;&lt;h3&gt;‚è∞ 20:00 (hora Espa√±a) en directo desde &lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;üåê Consulta el horario por pa√≠s y crea un recordatorio desde &lt;a href=&#34;https://discord.gg/szCvz8xV?event=1159409862787211284&#34;&gt;Discord&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Clases anteriores&lt;/h2&gt; &#xA;&lt;h3&gt;Clase 1 (04/10/2023): Introducci√≥n, instalaci√≥n y configuraci√≥n ‚ñ∂Ô∏è &lt;a href=&#34;https://www.twitch.tv/videos/1942562640?t=00h18m05s&#34;&gt;Ver clase en v√≠deo&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Proyecto&lt;/h2&gt; &#xA;&lt;p&gt;Durante el curso aprenderemos desarrollo web con Python puro utilizando el framework &lt;a href=&#34;https://github.com/reflex-dev/reflex&#34;&gt;Reflex&lt;/a&gt;. Realizaremos un proyecto pr√°ctico que consistir√° en desarrollar y publicar mi nueva web de links &lt;a href=&#34;https://moure.dev/&#34;&gt;moure.dev&lt;/a&gt; (a√±adi√©ndole muchas nuevas funcionalidades). Todo el c√≥digo estar√° disponible para que cualquiera pueda usarlo.&lt;/p&gt; &#xA;&lt;h3&gt;üíª &lt;a href=&#34;https://raw.githubusercontent.com/mouredev/python-web/main/link_bio&#34;&gt;Accede al c√≥digo del proyecto&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Informaci√≥n importante y preguntas frecuentes&lt;/h2&gt; &#xA;&lt;p&gt;Este curso se encuentra en desarrollo. Todo el contenido se crea en directo desde &lt;a href=&#34;https://www.twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;, y en este repositorio podr√°s encontrar las clases en v√≠deo, el c√≥digo programado, enlaces de inter√©s y la informaci√≥n de la pr√≥xima clase.&lt;/p&gt; &#xA;&lt;p&gt;Una vez se finalice, se crear√° un v√≠deo que agrupe todas las clases y se publicar√° en &lt;a href=&#34;https://www.youtube.com/@mouredev&#34;&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Es un curso desde cero y no necesitas conocimientos previos sobre desarrollo web.&lt;/li&gt; &#xA; &lt;li&gt;Recuerda que he creado en el &lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt; un canal &#34;üêçpython&#34; para que puedas comentar lo que quieras.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Enlaces de inter√©s&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Web oficial de Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reflex.dev/&#34;&gt;Web oficial de Reflex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reflex.dev/docs/&#34;&gt;Documentaci√≥n oficial de Reflex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reflex-dev/reflex&#34;&gt;Repositorio en GitHub de Reflex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vscode.dev/&#34;&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.w3schools.com/css/&#34;&gt;Documentaci√≥n CSS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.dev/learn/css/&#34;&gt;Curso de CSS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chakra-ui.com/&#34;&gt;Chakra UI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Curso de Python desde cero&lt;/h2&gt; &#xA;&lt;h3&gt;Aprende Python desde sus fundamentos&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/hello-python&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/Hello-Python/main/Images/header.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Si quieres aprender desde cero, tienes gratis todos los tutoriales que he creado. M√°s de 25 horas desde fundamentos, backend o integraci√≥n con IA.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/hello-python&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/hello-python?label=Curso%20Python%20desde%20cero&amp;amp;style=social&#34; alt=&#34;Curso Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Puedes apoyar mi trabajo haciendo &#34;‚òÜ Star&#34; en el repo o nominarme a &#34;GitHub Star&#34;. ¬°Gracias!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stars.github.com/nominate/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-Nominar_a_star-yellow?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;GitHub Star&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Si quieres unirte a nuestra comunidad de desarrollo, aprender programaci√≥n de Apps, mejorar tus habilidades y ayudar a la continuidad del proyecto, puedes encontrarnos en:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitch-Programaci%C3%B3n_en_directo-9146FF?style=for-the-badge&amp;amp;logo=twitch&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Twitch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Servidor_de_la_comunidad-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://moure.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Links_de_inter%C3%A9s-moure.dev-39E09B?style=for-the-badge&amp;amp;logo=Linktree&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png&#34; alt=&#34;https://mouredev.com&#34;&gt; Hola, mi nombre es Brais Moure.&lt;/h2&gt; &#xA;&lt;h3&gt;Freelance full-stack iOS &amp;amp; Android engineer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtube.com/mouredevapps?sub_confirmation=1&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCxPD7bsocoAMq8Dj18kmGyQ?style=social&#34; alt=&#34;YouTube Channel Subscribers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitch.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitch/status/mouredev?style=social&#34; alt=&#34;Twitch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/729672926432985098?style=social&amp;amp;label=Discord&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mouredev?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/followers/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Soy ingeniero de software desde hace m√°s de 12 a√±os. Desde hace 4 a√±os combino mi trabajo desarrollando Apps con creaci√≥n de contenido formativo sobre programaci√≥n y tecnolog√≠a en diferentes redes sociales como &lt;strong&gt;&lt;a href=&#34;https://moure.dev&#34;&gt;@mouredev&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;En mi perfil de GitHub tienes m√°s informaci√≥n&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-MoureDev-14a1f0?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Web&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JayZeeDesign/microsoft-autogen-experiments</title>
    <updated>2023-10-07T01:37:18Z</updated>
    <id>tag:github.com,2023-10-07:/JayZeeDesign/microsoft-autogen-experiments</id>
    <link href="https://github.com/JayZeeDesign/microsoft-autogen-experiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>