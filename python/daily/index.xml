<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-06T01:37:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>commixproject/commix</title>
    <updated>2025-03-06T01:37:04Z</updated>
    <id>tag:github.com,2025-03-06:/commixproject/commix</id>
    <link href="https://github.com/commixproject/commix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automated All-in-One OS Command Injection Exploitation Tool.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; &lt;img alt=&#34;CommixProject&#34; src=&#34;https://commixproject.com/images/logo.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/commixproject/commix/actions/workflows/builds.yml&#34;&gt;&lt;img alt=&#34;Builds Tests&#34; src=&#34;https://github.com/commixproject/commix/actions/workflows/builds.yml/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.python.org/download/&#34;&gt;&lt;img alt=&#34;Python 2.6|2.7|3.x&#34; src=&#34;https://img.shields.io/badge/python-2.6%7C2.7%7C3.x-yellow.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/commixproject/commix/raw/master/LICENSE.txt&#34;&gt;&lt;img alt=&#34;GPLv3 License&#34; src=&#34;https://img.shields.io/badge/license-GPLv3-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/commixproject/commix/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img alt=&#34;GitHub closed issues&#34; src=&#34;https://img.shields.io/github/issues-closed-raw/commixproject/commix.svg?colorB=ff0000&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/commixproject&#34;&gt;&lt;img alt=&#34;X&#34; src=&#34;https://img.shields.io/badge/x-@commixproject-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Commix&lt;/strong&gt; (short for [&lt;strong&gt;comm&lt;/strong&gt;]and [&lt;strong&gt;i&lt;/strong&gt;]njection e[&lt;strong&gt;x&lt;/strong&gt;]ploiter) is an open source penetration testing tool, written by &lt;strong&gt;&lt;a href=&#34;https://github.com/stasinopoulos&#34;&gt;Anastasios Stasinopoulos&lt;/a&gt;&lt;/strong&gt; (&lt;strong&gt;&lt;a href=&#34;https://x.com/ancst&#34;&gt;@ancst&lt;/a&gt;&lt;/strong&gt;), that automates the detection and exploitation of &lt;strong&gt;&lt;a href=&#34;https://www.owasp.org/index.php/Command_Injection&#34;&gt;command injection&lt;/a&gt;&lt;/strong&gt; vulnerabilities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://commixproject.com/images/background.png&#34; alt=&#34;Screenshot&#34;&gt; You can visit the &lt;a href=&#34;https://github.com/commixproject/commix/wiki/Screenshots&#34;&gt;collection of screenshots&lt;/a&gt; demonstrating some of the features on the wiki.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can download commix on any platform by cloning the official Git repository :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/commixproject/commix.git commix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can download the latest &lt;a href=&#34;https://github.com/commixproject/commix/tarball/master&#34;&gt;tarball&lt;/a&gt; or &lt;a href=&#34;https://github.com/commixproject/commix/zipball/master&#34;&gt;zipball&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;http://www.python.org/download/&#34;&gt;Python&lt;/a&gt;&lt;/strong&gt; (version &lt;strong&gt;2.6&lt;/strong&gt;, &lt;strong&gt;2.7&lt;/strong&gt; or &lt;strong&gt;3.x&lt;/strong&gt;) is required for running commix.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To get a list of all options and switches use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python commix.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get an overview of commix available options, switches and/or basic ideas on how to use commix, check &lt;strong&gt;&lt;a href=&#34;https://github.com/commixproject/commix/wiki/Usage&#34;&gt;usage&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&#34;https://github.com/commixproject/commix/wiki/Usage-Examples&#34;&gt;usage examples&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://github.com/commixproject/commix/wiki/Filters-Bypasses&#34;&gt;filters bypasses&lt;/a&gt;&lt;/strong&gt; wiki pages.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;User&#39;s manual: &lt;a href=&#34;https://github.com/commixproject/commix/wiki&#34;&gt;https://github.com/commixproject/commix/wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Issues tracker: &lt;a href=&#34;https://github.com/commixproject/commix/issues&#34;&gt;https://github.com/commixproject/commix/issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/commixproject/commix/raw/master/doc/translations/README-fa-FA.md&#34;&gt;Farsi(Persian)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/commixproject/commix/raw/master/doc/translations/README-gr-GR.md&#34;&gt;Greek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/commixproject/commix/raw/master/doc/translations/README-idn-IDN.md&#34;&gt;Indonesian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/commixproject/commix/raw/master/doc/translations/README-tr-TR.md&#34;&gt;Turkish&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/FoundationPose</title>
    <updated>2025-03-06T01:37:04Z</updated>
    <id>tag:github.com,2025-03-06:/NVlabs/FoundationPose</id>
    <link href="https://github.com/NVlabs/FoundationPose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024 Highlight] FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.08344&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://nvlabs.github.io/FoundationPose/&#34;&gt;[Website]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of our paper to be appeared in CVPR 2024 (Highlight)&lt;/p&gt; &#xA;&lt;p&gt;Contributors: Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield&lt;/p&gt; &#xA;&lt;p&gt;We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/intro.jpg&#34; width=&#34;70%&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;ðŸ¤– For ROS version, please check &lt;a href=&#34;https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation&#34;&gt;Isaac ROS Pose Estimation&lt;/a&gt;, which enjoys TRT fast inference and C++ speed up.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt; &lt;strong&gt;ðŸ¥‡ No. 1 on the world-wide &lt;a href=&#34;https://bop.felk.cvut.cz/leaderboards/pose-estimation-unseen-bop23/core-datasets/&#34;&gt;BOP leaderboard&lt;/a&gt; (as of 2024/03) for model-based novel object pose estimation.&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/bop.jpg&#34; width=&#34;80%&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;Robotic Applications:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/FoundationPose/assets/23078192/aa341004-5a15-4293-b3da-000471fd74ed&#34;&gt;https://github.com/NVlabs/FoundationPose/assets/23078192/aa341004-5a15-4293-b3da-000471fd74ed&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AR Applications:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/FoundationPose/assets/23078192/80e96855-a73c-4bee-bcef-7cba92df55ca&#34;&gt;https://github.com/NVlabs/FoundationPose/assets/23078192/80e96855-a73c-4bee-bcef-7cba92df55ca&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Results on YCB-Video dataset:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/FoundationPose/assets/23078192/9b5bedde-755b-44ed-a973-45ec85a10bbe&#34;&gt;https://github.com/NVlabs/FoundationPose/assets/23078192/9b5bedde-755b-44ed-a973-45ec85a10bbe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Bibtex&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{foundationposewen2024,&#xA;author        = {Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield},&#xA;title         = {{FoundationPose}: Unified 6D Pose Estimation and Tracking of Novel Objects},&#xA;booktitle     = {CVPR},&#xA;year          = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find the model-free setup useful, please also consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{bundlesdfwen2023,&#xA;author        = {Bowen Wen and Jonathan Tremblay and Valts Blukis and Stephen Tyree and Thomas M\&#34;{u}ller and Alex Evans and Dieter Fox and Jan Kautz and Stan Birchfield},&#xA;title         = {{BundleSDF}: {N}eural 6-{DoF} Tracking and {3D} Reconstruction of Unknown Objects},&#xA;booktitle     = {CVPR},&#xA;year          = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Data prepare&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download all network weights from &lt;a href=&#34;https://drive.google.com/drive/folders/1DFezOAD0oD1BblsXVxqDsl8fj0qzB82i?usp=sharing&#34;&gt;here&lt;/a&gt; and put them under the folder &lt;code&gt;weights/&lt;/code&gt;. For the refiner, you will need &lt;code&gt;2023-10-28-18-33-37&lt;/code&gt;. For scorer, you will need &lt;code&gt;2024-01-11-20-02-45&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1pRyFmxYXmAnpku7nGRioZaKrVJtIsroP?usp=sharing&#34;&gt;Download demo data&lt;/a&gt; and extract them under the folder &lt;code&gt;demo_data/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Optional] Download our large-scale training data: &lt;a href=&#34;https://drive.google.com/drive/folders/1s4pB6p4ApfWMiMjmTXOFco8dHbNXikp-?usp=sharing&#34;&gt;&#34;FoundationPose Dataset&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Optional] Download our preprocessed reference views &lt;a href=&#34;https://drive.google.com/drive/folders/1PXXCOJqHXwQTbwPwPbGDN9_vLVe0XpFS?usp=sharing&#34;&gt;here&lt;/a&gt; in order to run model-free few-shot version.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Env setup option 1: docker (recommended)&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd docker/&#xA;docker pull wenbowen123/foundationpose &amp;amp;&amp;amp; docker tag wenbowen123/foundationpose foundationpose  # Or to build from scratch: docker build --network host -t foundationpose .&#xA;bash docker/run_container.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If it&#39;s the first time you launch the container, you need to build extensions. Run this command &lt;em&gt;inside&lt;/em&gt; the Docker container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash build_all.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Later you can execute into the container without re-build.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker exec -it foundationpose bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more recent GPU such as 4090, refer to &lt;a href=&#34;https://github.com/NVlabs/FoundationPose/issues/27&#34;&gt;this&lt;/a&gt;. In short, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull shingarey/foundationpose_custom_cuda121:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then modify the bash script to use this image instead of &lt;code&gt;foundationpose:latest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Env setup option 2: conda (experimental)&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup conda environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create conda environment&#xA;conda create -n foundationpose python=3.9&#xA;&#xA;# activate conda environment&#xA;conda activate foundationpose&#xA;&#xA;# Install Eigen3 3.4.0 under conda environment&#xA;conda install conda-forge::eigen=3.4.0&#xA;export CMAKE_PREFIX_PATH=&#34;$CMAKE_PREFIX_PATH:/eigen/path/under/conda&#34;&#xA;&#xA;# install dependencies&#xA;python -m pip install -r requirements.txt&#xA;&#xA;# Install NVDiffRast&#xA;python -m pip install --quiet --no-cache-dir git+https://github.com/NVlabs/nvdiffrast.git&#xA;&#xA;# Kaolin (Optional, needed if running model-free setup)&#xA;python -m pip install --quiet --no-cache-dir kaolin==0.15.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.0.0_cu118.html&#xA;&#xA;# PyTorch3D&#xA;python -m pip install --quiet --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py39_cu118_pyt200/download.html&#xA;&#xA;# Build extensions&#xA;CMAKE_PREFIX_PATH=$CONDA_PREFIX/lib/python3.9/site-packages/pybind11/share/cmake/pybind11 bash build_all_conda.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Run model-based demo&lt;/h1&gt; &#xA;&lt;p&gt;The paths have been set in argparse by default. If you need to change the scene, you can pass the args accordingly. By running on the demo data, you should be able to see the robot manipulating the mustard bottle. Pose estimation is conducted on the first frame, then it automatically switches to tracking mode for the rest of the video. The resulting visualizations will be saved to the &lt;code&gt;debug_dir&lt;/code&gt; specified in the argparse. (Note the first time running could be slower due to online compilation)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/demo.jpg&#34; width=&#34;50%&#34;&gt; &#xA;&lt;p&gt;Feel free to try on other objects (&lt;strong&gt;no need to retrain&lt;/strong&gt;) such as driller, by changing the paths in argparse.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/demo_driller.jpg&#34; width=&#34;50%&#34;&gt; &#xA;&lt;h1&gt;Run on public datasets (LINEMOD, YCB-Video)&lt;/h1&gt; &#xA;&lt;p&gt;For this you first need to download LINEMOD dataset and YCB-Video dataset.&lt;/p&gt; &#xA;&lt;p&gt;To run model-based version on these two datasets respectively, set the paths based on where you download. The results will be saved to &lt;code&gt;debug&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_linemod.py --linemod_dir /mnt/9a72c439-d0a7-45e8-8d20-d7a235d02763/DATASET/LINEMOD --use_reconstructed_mesh 0&#xA;&#xA;python run_ycb_video.py --ycbv_dir /mnt/9a72c439-d0a7-45e8-8d20-d7a235d02763/DATASET/YCB_Video --use_reconstructed_mesh 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run model-free few-shot version. You first need to train Neural Object Field. &lt;code&gt;ref_view_dir&lt;/code&gt; is based on where you download in the above &#34;Data prepare&#34; section. Set the &lt;code&gt;dataset&lt;/code&gt; flag to your interested dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python bundlesdf/run_nerf.py --ref_view_dir /mnt/9a72c439-d0a7-45e8-8d20-d7a235d02763/DATASET/YCB_Video/bowen_addon/ref_views_16 --dataset ycbv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the similar command as the model-based version with some small modifications. Here we are using YCB-Video as example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_ycb_video.py --ycbv_dir /mnt/9a72c439-d0a7-45e8-8d20-d7a235d02763/DATASET/YCB_Video --use_reconstructed_mesh 1 --ref_view_dir /mnt/9a72c439-d0a7-45e8-8d20-d7a235d02763/DATASET/YCB_Video/bowen_addon/ref_views_16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For more recent GPU such as 4090, refer to &lt;a href=&#34;https://github.com/NVlabs/FoundationPose/issues/27&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For setting up on Windows, refer to &lt;a href=&#34;https://github.com/NVlabs/FoundationPose/issues/148&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are getting unreasonable results, check &lt;a href=&#34;https://github.com/NVlabs/FoundationPose/issues/44#issuecomment-2048141043&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;https://github.com/030422Lee/FoundationPose_manual&#34;&gt;this&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training data download&lt;/h1&gt; &#xA;&lt;p&gt;Our training data include scenes using 3D assets from GSO and Objaverse, rendered with high quality photo-realism and large domain randomization. Each data point includes &lt;strong&gt;RGB, depth, object pose, camera pose, instance segmentation, 2D bounding box&lt;/strong&gt;. &lt;a href=&#34;https://drive.google.com/drive/folders/1s4pB6p4ApfWMiMjmTXOFco8dHbNXikp-?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/train_data_vis.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To parse the camera params including extrinsics and intrinsics &lt;pre&gt;&lt;code&gt;glcam_in_cvcam = np.array([[1,0,0,0],&#xA;                        [0,-1,0,0],&#xA;                        [0,0,-1,0],&#xA;                        [0,0,0,1]]).astype(float)&#xA;W, H = camera_params[&#34;renderProductResolution&#34;]&#xA;with open(f&#39;{base_dir}/camera_params/camera_params_000000.json&#39;,&#39;r&#39;) as ff:&#xA;  camera_params = json.load(ff)&#xA;world_in_glcam = np.array(camera_params[&#39;cameraViewTransform&#39;]).reshape(4,4).T&#xA;cam_in_world = np.linalg.inv(world_in_glcam)@glcam_in_cvcam&#xA;world_in_cam = np.linalg.inv(cam_in_world)&#xA;focal_length = camera_params[&#34;cameraFocalLength&#34;]&#xA;horiz_aperture = camera_params[&#34;cameraAperture&#34;][0]&#xA;vert_aperture = H / W * horiz_aperture&#xA;focal_y = H * focal_length / vert_aperture&#xA;focal_x = W * focal_length / horiz_aperture&#xA;center_y = H * 0.5&#xA;center_x = W * 0.5&#xA;&#xA;fx, fy, cx, cy = focal_x, focal_y, center_x, center_y&#xA;K = np.eye(3)&#xA;K[0,0] = fx&#xA;K[1,1] = fy&#xA;K[0,2] = cx&#xA;K[1,2] = cy&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Notes&lt;/h1&gt; &#xA;&lt;p&gt;Due to the legal restrictions of Stable-Diffusion that is trained on LAION dataset, we are not able to release the diffusion-based texture augmented data, nor the pretrained weights using it. We thus release the version without training on diffusion-augmented data. Slight performance degradation is expected.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;We would like to thank Jeff Smith for helping with the code release; NVIDIA Isaac Sim and Omniverse team for the support on synthetic data generation; Tianshi Cao for the valuable discussions. Finally, we are also grateful for the positive feebacks and constructive suggestions brought up by reviewers and AC at CVPR.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/FoundationPose/main/assets/cvpr_review.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The code and data are released under the NVIDIA Source Code License. Copyright Â© 2024, NVIDIA Corporation. All rights reserved.&lt;/p&gt; &#xA;&lt;h1&gt;Contact&lt;/h1&gt; &#xA;&lt;p&gt;For questions, please contact &lt;a href=&#34;https://wenbowen123.github.io/&#34;&gt;Bowen Wen&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gpustack/gpustack</title>
    <updated>2025-03-06T01:37:04Z</updated>
    <id>tag:github.com,2025-03-06:/gpustack/gpustack</id>
    <link href="https://github.com/gpustack/gpustack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Manage GPU clusters for running AI models&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;GPUStack&#34; src=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-logo.png&#34; width=&#34;300px&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.gpustack.ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/badge/Docs-GPUStack-blue?logo=readthedocs&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/gpustack/gpustack?logo=github&amp;amp;logoColor=white&amp;amp;label=License&amp;amp;color=blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/wechat-assistant.png&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;WeChat&#34; src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1%E7%BE%A4-GPUStack-blue?logo=wechat&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/VXYJzuaqwD&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-GPUStack-blue?logo=discord&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=gpustack_ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Follow on X(Twitter)&#34; src=&#34;https://img.shields.io/twitter/follow/gpustack_ai?logo=X&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/README_CN.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-demo.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPUStack is an open-source GPU cluster manager for running AI models.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad Hardware Compatibility:&lt;/strong&gt; Run with different brands of GPUs in Apple Macs, Windows PCs, and Linux servers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad Model Support:&lt;/strong&gt; From LLMs and diffusion models to audio, embedding, and reranker models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scales with Your GPU Inventory:&lt;/strong&gt; Easily add more GPUs or nodes to scale up your operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed Inference&lt;/strong&gt;: Supports both single-node multi-GPU and multi-node inference and serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Inference Backends&lt;/strong&gt;: Supports llama-box (llama.cpp &amp;amp; stable-diffusion.cpp), vox-box and vLLM as the inference backends.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight Python Package:&lt;/strong&gt; Minimal dependencies and operational overhead.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI-compatible APIs:&lt;/strong&gt; Serve APIs that are compatible with OpenAI standards.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User and API key management:&lt;/strong&gt; Simplified management of users and API keys.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU metrics monitoring:&lt;/strong&gt; Monitor GPU performance and utilization in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Token usage and rate metrics:&lt;/strong&gt; Track token usage and manage rate limits effectively.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Linux or macOS&lt;/h3&gt; &#xA;&lt;p&gt;GPUStack provides a script to install it as a service on systemd or launchd based systems with default port 80. To install GPUStack using this method, just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sfL https://get.gpustack.ai | sh -s -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Run PowerShell as administrator (&lt;strong&gt;avoid&lt;/strong&gt; using PowerShell ISE), then run the following command to install GPUStack:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Invoke-Expression (Invoke-WebRequest -Uri &#34;https://get.gpustack.ai&#34; -UseBasicParsing).Content&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Installation Methods&lt;/h3&gt; &#xA;&lt;p&gt;For manual installation, docker installation or detailed configuration options, please refer to the &lt;a href=&#34;https://docs.gpustack.ai/latest/installation/installation-script/&#34;&gt;Installation Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run and chat with the &lt;strong&gt;llama3.2&lt;/strong&gt; model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gpustack chat llama3.2 &#34;tell me a joke.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run and generate an image with the &lt;strong&gt;stable-diffusion-v3-5-large-turbo&lt;/strong&gt; model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;ðŸ’¡ Tip&lt;/h3&gt; &#xA; &lt;p&gt;This command downloads the model (~12GB) from Hugging Face. The download time depends on your network speed. Ensure you have enough disk space and VRAM (12GB) to run the model. If you encounter issues, you can skip this step and move to the next one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \&#xA;&#34;A minion holding a sign that says &#39;GPUStack&#39;. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors.&#34; \&#xA;--sample-steps 5 --show&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the command completes, the generated image will appear in the default viewer. You can experiment with the prompt and CLI options to customize the output.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/quickstart-minion.png&#34; alt=&#34;Generated Image&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Open &lt;code&gt;http://myserver&lt;/code&gt; in the browser to access the GPUStack UI. Log in to GPUStack with username &lt;code&gt;admin&lt;/code&gt; and the default password. You can run the following command to get the password for the default setup:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linux or macOS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /var/lib/gpustack/initial_admin_password&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Get-Content -Path &#34;$env:APPDATA\gpustack\initial_admin_password&#34; -Raw&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Playground - Chat&lt;/code&gt; in the navigation menu. Now you can chat with the LLM in the UI playground.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/playground-screenshot.png&#34; alt=&#34;Playground Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Click &lt;code&gt;API Keys&lt;/code&gt; in the navigation menu, then click the &lt;code&gt;New API Key&lt;/code&gt; button.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fill in the &lt;code&gt;Name&lt;/code&gt; and click the &lt;code&gt;Save&lt;/code&gt; button.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can use the API key to access the OpenAI-compatible API. For example, use curl as the following:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GPUSTACK_API_KEY=myapikey&#xA;curl http://myserver/v1-openai/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -H &#34;Authorization: Bearer $GPUSTACK_API_KEY&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;llama3.2&#34;,&#xA;    &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: &#34;You are a helpful assistant.&#34;&#xA;      },&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Hello!&#34;&#xA;      }&#xA;    ],&#xA;    &#34;stream&#34;: true&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported Platforms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; macOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Linux&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Accelerators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Apple Metal (M-series chips)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; NVIDIA CUDA (&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;Compute Capability&lt;/a&gt; 6.0 and above)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AMD ROCm&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Ascend CANN&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Moore Threads MUSA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Hygon DTK&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We plan to support the following accelerators in future releases.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Intel oneAPI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Qualcomm AI Engine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;GPUStack uses &lt;a href=&#34;https://github.com/gpustack/llama-box&#34;&gt;llama-box&lt;/a&gt; (bundled &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/leejet/stable-diffusion.cpp&#34;&gt;stable-diffusion.cpp&lt;/a&gt; server), &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; and &lt;a href=&#34;https://github.com/gpustack/vox-box&#34;&gt;vox-box&lt;/a&gt; as the backends and supports a wide range of models. Models from the following sources are supported:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/&#34;&gt;ModelScope&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ollama.com/library&#34;&gt;Ollama Library&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Local File Path&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Example Models:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Large Language Models(LLMs)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?search=Qwen/Qwen&#34;&gt;Qwen&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=deepseek-ai/deepseek&#34;&gt;Deepseek&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=microsoft/phi&#34;&gt;Phi&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=01-ai/Yi&#34;&gt;Yi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Vision Language Models(VLMs)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;search=llama3.2&#34;&gt;Llama3.2-Vision&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=pixtral&#34;&gt;Pixtral&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/models?search=Qwen/Qwen2-VL&#34;&gt;Qwen2-VL&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=llava&#34;&gt;LLaVA&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=internvl2&#34;&gt;InternVL2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Diffusion Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?search=gpustack/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=gpustack/flux&#34;&gt;FLUX&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Embedding Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/gpustack/bge-m3-GGUF&#34;&gt;BGE&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/gpustack/bce-embedding-base_v1-GGUF&#34;&gt;BCE&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=gpustack/jina-embeddings&#34;&gt;Jina&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Reranker Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/gpustack/bge-reranker-v2-m3-GGUF&#34;&gt;BGE&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/gpustack/bce-reranker-base_v1-GGUF&#34;&gt;BCE&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=gpustack/jina-reranker&#34;&gt;Jina&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Audio Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?search=Systran/faster&#34;&gt;Whisper&lt;/a&gt; (speech-to-text), &lt;a href=&#34;https://huggingface.co/models?search=FunAudioLLM/CosyVoice&#34;&gt;CosyVoice&lt;/a&gt; (text-to-speech)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For full list of supported models, please refer to the supported models section in the &lt;a href=&#34;https://docs.gpustack.ai/latest/user-guide/inference-backends/&#34;&gt;inference backends&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI-Compatible APIs&lt;/h2&gt; &#xA;&lt;p&gt;GPUStack serves the following OpenAI compatible APIs under the &lt;code&gt;/v1-openai&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/models/list&#34;&gt;List Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/completions/create&#34;&gt;Create Completion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;Create Chat Completion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/embeddings/create&#34;&gt;Create Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/images/create&#34;&gt;Create Image&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/images/createEdit&#34;&gt;Create Image Edit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio/createSpeech&#34;&gt;Create Speech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio/createTranscription&#34;&gt;Create Transcription&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, you can use the official &lt;a href=&#34;https://github.com/openai/openai-python&#34;&gt;OpenAI Python API library&lt;/a&gt; to consume the APIs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;client = OpenAI(base_url=&#34;http://myserver/v1-openai&#34;, api_key=&#34;myapikey&#34;)&#xA;&#xA;completion = client.chat.completions.create(&#xA;  model=&#34;llama3.2&#34;,&#xA;  messages=[&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}&#xA;  ]&#xA;)&#xA;&#xA;print(completion.choices[0].message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GPUStack users can generate their own API keys in the UI.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://docs.gpustack.ai&#34;&gt;official docs site&lt;/a&gt; for complete documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python (version 3.10 to 3.12).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;make build&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can find the built wheel package in &lt;code&gt;dist&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/contributing.md&#34;&gt;Contributing Guide&lt;/a&gt; if you&#39;re interested in contributing to GPUStack.&lt;/p&gt; &#xA;&lt;h2&gt;Join Community&lt;/h2&gt; &#xA;&lt;p&gt;Any issues or have suggestions, feel free to join our &lt;a href=&#34;https://discord.gg/VXYJzuaqwD&#34;&gt;Community&lt;/a&gt; for support.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2024 The GPUStack authors&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://raw.githubusercontent.com/gpustack/gpustack/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
</feed>