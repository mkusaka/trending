<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-03T01:39:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>UKPLab/sentence-transformers</title>
    <updated>2022-09-03T01:39:33Z</updated>
    <id>tag:github.com,2022-09-03:/UKPLab/sentence-transformers</id>
    <link href="https://github.com/UKPLab/sentence-transformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multilingual Sentence &amp; Image Embeddings with BERT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/UKPLab/sentence-transformers/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/UKPLab/sentence-transformers?logo=github&amp;amp;style=flat&amp;amp;color=green&#34; alt=&#34;GitHub - License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/sentence-transformers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&amp;amp;style=flat&amp;amp;color=blue&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/sentence-transformers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&amp;amp;style=flat&amp;amp;color=orange&#34; alt=&#34;PyPI - Package Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/sentence-transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/pn/conda-forge/sentence-transformers?logo=anaconda&amp;amp;style=flat&#34; alt=&#34;Conda - Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/sentence-transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/sentence-transformers?logo=anaconda&amp;amp;style=flat&amp;amp;color=orange&#34; alt=&#34;Conda (channel only)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.sbert.net/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?logo=github&amp;amp;style=flat&amp;amp;color=pink&amp;amp;label=docs&amp;amp;message=sentence-transformers&#34; alt=&#34;Docs - GitHub.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;[![PyPI - Downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&amp;style=flat&amp;color=green)][#pypi-package]&#xA;[![Conda](https://img.shields.io/conda/dn/conda-forge/sentence-transformers?logo=anaconda)][#conda-forge-package] &#xA;---&gt; &#xA;&lt;!-- BADGES: END ---&gt; &#xA;&lt;h1&gt;Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT &amp;amp; Co.&lt;/h1&gt; &#xA;&lt;p&gt;This framework provides an easy method to compute dense vector representations for &lt;strong&gt;sentences&lt;/strong&gt;, &lt;strong&gt;paragraphs&lt;/strong&gt;, and &lt;strong&gt;images&lt;/strong&gt;. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.&lt;/p&gt; &#xA;&lt;p&gt;We provide an increasing number of &lt;strong&gt;&lt;a href=&#34;https://www.sbert.net/docs/pretrained_models.html&#34;&gt;state-of-the-art pretrained models&lt;/a&gt;&lt;/strong&gt; for more than 100 languages, fine-tuned for various use-cases.&lt;/p&gt; &#xA;&lt;p&gt;Further, this framework allows an easy &lt;strong&gt;&lt;a href=&#34;https://www.sbert.net/docs/training/overview.html&#34;&gt;fine-tuning of custom embeddings models&lt;/a&gt;&lt;/strong&gt;, to achieve maximal performance on your specific task.&lt;/p&gt; &#xA;&lt;p&gt;For the &lt;strong&gt;full documentation&lt;/strong&gt;, see &lt;strong&gt;&lt;a href=&#34;https://www.sbert.net&#34;&gt;www.SBERT.net&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following publications are integrated in this framework:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.10084&#34;&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/a&gt; (EMNLP 2019)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.09813&#34;&gt;Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation&lt;/a&gt; (EMNLP 2020)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.08240&#34;&gt;Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks&lt;/a&gt; (NAACL 2021)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.14210&#34;&gt;The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes&lt;/a&gt; (arXiv 2020)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.06979&#34;&gt;TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning&lt;/a&gt; (arXiv 2021)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.08663&#34;&gt;BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models&lt;/a&gt; (arXiv 2021)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We recommend &lt;strong&gt;Python 3.6&lt;/strong&gt; or higher, &lt;strong&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch 1.6.0&lt;/a&gt;&lt;/strong&gt; or higher and &lt;strong&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers v4.6.0&lt;/a&gt;&lt;/strong&gt; or higher. The code does &lt;strong&gt;not&lt;/strong&gt; work with Python 2.7.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install with pip&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;em&gt;sentence-transformers&lt;/em&gt; with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U sentence-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install with conda&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can install the &lt;em&gt;sentence-transformers&lt;/em&gt; with &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c conda-forge sentence-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install from sources&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can also clone the latest version from the &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;repository&lt;/a&gt; and install it directly from the source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch with CUDA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch - Get Started&lt;/a&gt; for further details how to install PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.sbert.net/docs/quickstart.html&#34;&gt;Quickstart&lt;/a&gt; in our documenation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/computing-embeddings/computing_embeddings.py&#34;&gt;This example&lt;/a&gt; shows you how to use an already trained Sentence Transformer model to embed sentences for another task.&lt;/p&gt; &#xA;&lt;p&gt;First download a pretrained model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sentence_transformers import SentenceTransformer&#xA;model = SentenceTransformer(&#39;all-MiniLM-L6-v2&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then provide some sentences to the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentences = [&#39;This framework generates embeddings for each input sentence&#39;,&#xA;    &#39;Sentences are passed as a list of string.&#39;, &#xA;    &#39;The quick brown fox jumps over the lazy dog.&#39;]&#xA;sentence_embeddings = model.encode(sentences)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And that&#39;s it already. We now have a list of numpy arrays with the embeddings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for sentence, embedding in zip(sentences, sentence_embeddings):&#xA;    print(&#34;Sentence:&#34;, sentence)&#xA;    print(&#34;Embedding:&#34;, embedding)&#xA;    print(&#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pre-Trained Models&lt;/h2&gt; &#xA;&lt;p&gt;We provide a large list of &lt;a href=&#34;https://www.sbert.net/docs/pretrained_models.html&#34;&gt;Pretrained Models&lt;/a&gt; for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases. Pre-trained models can be loaded by just passing the model name: &lt;code&gt;SentenceTransformer(&#39;model_name&#39;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.sbert.net/docs/pretrained_models.html&#34;&gt;Â» Full list of pretrained models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;This framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.sbert.net/docs/training/overview.html&#34;&gt;Training Overview&lt;/a&gt; for an introduction how to train your own embedding models. We provide &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers/tree/master/examples/training&#34;&gt;various examples&lt;/a&gt; how to train models on various datasets.&lt;/p&gt; &#xA;&lt;p&gt;Some highlights are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support of various transformer networks including BERT, RoBERTa, XLM-R, DistilBERT, Electra, BART, ...&lt;/li&gt; &#xA; &lt;li&gt;Multi-Lingual and multi-task learning&lt;/li&gt; &#xA; &lt;li&gt;Evaluation during training to find optimal model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/docs/package_reference/losses.html&#34;&gt;10+ loss-functions&lt;/a&gt; allowing to tune models specifically for semantic search, paraphrase mining, semantic similarity comparison, clustering, triplet loss, contrastive loss.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Our models are evaluated extensively on 15+ datasets including challening domains like Tweets, Reddit, emails. They achieve by far the &lt;strong&gt;best performance&lt;/strong&gt; from all available sentence embedding methods. Further, we provide several &lt;strong&gt;smaller models&lt;/strong&gt; that are &lt;strong&gt;optimized for speed&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.sbert.net/docs/pretrained_models.html&#34;&gt;Â» Full list of pretrained models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Application Examples&lt;/h2&gt; &#xA;&lt;p&gt;You can use this framework for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/computing-embeddings/README.html&#34;&gt;Computing Sentence Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/docs/usage/semantic_textual_similarity.html&#34;&gt;Semantic Textual Similarity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/clustering/README.html&#34;&gt;Clustering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/paraphrase-mining/README.html&#34;&gt;Paraphrase Mining&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html&#34;&gt;Translated Sentence Mining&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/semantic-search/README.html&#34;&gt;Semantic Search&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/retrieve_rerank/README.html&#34;&gt;Retrieve &amp;amp; Re-Rank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/text-summarization/README.html&#34;&gt;Text Summarization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/examples/applications/image-search/README.html&#34;&gt;Multilingual Image Search, Clustering &amp;amp; Duplicate Detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and many more use-cases.&lt;/p&gt; &#xA;&lt;p&gt;For all examples, see &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications&#34;&gt;examples/applications&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing &amp;amp; Authors&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository helpful, feel free to cite our publication &lt;a href=&#34;https://arxiv.org/abs/1908.10084&#34;&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{reimers-2019-sentence-bert,&#xA;    title = &#34;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&#34;,&#xA;    author = &#34;Reimers, Nils and Gurevych, Iryna&#34;,&#xA;    booktitle = &#34;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&#34;,&#xA;    month = &#34;11&#34;,&#xA;    year = &#34;2019&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://arxiv.org/abs/1908.10084&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use one of the multilingual models, feel free to cite our publication &lt;a href=&#34;https://arxiv.org/abs/2004.09813&#34;&gt;Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{reimers-2020-multilingual-sentence-bert,&#xA;    title = &#34;Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation&#34;,&#xA;    author = &#34;Reimers, Nils and Gurevych, Iryna&#34;,&#xA;    booktitle = &#34;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing&#34;,&#xA;    month = &#34;11&#34;,&#xA;    year = &#34;2020&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://arxiv.org/abs/2004.09813&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please have a look at &lt;a href=&#34;https://www.sbert.net/docs/publications.html&#34;&gt;Publications&lt;/a&gt; for our different publications that are integrated into SentenceTransformers.&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;https://www.nils-reimers.de&#34;&gt;Nils Reimers&lt;/a&gt;, &lt;a href=&#34;mailto:info@nils-reimers.de&#34;&gt;info@nils-reimers.de&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ukp.tu-darmstadt.de/&#34;&gt;https://www.ukp.tu-darmstadt.de/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn&#39;t be) or if you have further questions.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>kedro-org/kedro</title>
    <updated>2022-09-03T01:39:33Z</updated>
    <id>tag:github.com,2022-09-03:/kedro-org/kedro</id>
    <link href="https://github.com/kedro-org/kedro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Python framework for creating reproducible, maintainable and modular data science code.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kedro-org/kedro/develop/static/img/kedro_banner.png&#34; alt=&#34;Kedro Logo Banner&#34;&gt; &lt;a href=&#34;https://pypi.org/project/kedro/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10-blue.svg?sanitize=true&#34; alt=&#34;Python version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/kedro/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/kedro.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/kedro&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/kedro.svg?sanitize=true&#34; alt=&#34;Conda version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kedro-org/kedro/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/akJDeVaxnB&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/778216384475693066.svg?color=7289da&amp;amp;label=Kedro%20Discord&amp;amp;logo=discord&amp;amp;style=flat-square&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/circleci/build/github/kedro-org/kedro/main?label=main&#34; alt=&#34;CircleCI - Main Branch&#34;&gt; &lt;img src=&#34;https://img.shields.io/circleci/build/github/kedro-org/kedro/develop?label=develop&#34; alt=&#34;Develop Branch Build&#34;&gt; &lt;a href=&#34;https://kedro.readthedocs.io/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/kedro/badge/?version=stable&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;Kedro is an open-source Python framework for creating reproducible, maintainable and modular data science code. It borrows concepts from software engineering and applies them to machine-learning code; applied concepts include modularity, separation of concerns and versioning. Kedro is hosted by the &lt;a href=&#34;https://lfaidata.foundation/&#34;&gt;LF AI &amp;amp; Data Foundation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How do I install Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;To install Kedro from the Python Package Index (PyPI) simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install kedro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also possible to install Kedro using &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c conda-forge kedro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/get_started/prerequisites.html&#34;&gt;Get Started guide&lt;/a&gt; contains full installation instructions, and includes how to set up Python virtual environments.&lt;/p&gt; &#xA;&lt;h2&gt;What are the main features of Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kedro-org/kedro-viz/raw/main/.github/img/banner.png&#34; alt=&#34;Kedro-Viz Pipeline Visualisation&#34;&gt; &lt;em&gt;A pipeline visualisation generated using &lt;a href=&#34;https://github.com/kedro-org/kedro-viz&#34;&gt;Kedro-Viz&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;What is this?&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Project Template&lt;/td&gt; &#xA;   &lt;td&gt;A standard, modifiable and easy-to-use project template based on &lt;a href=&#34;https://github.com/drivendata/cookiecutter-data-science/&#34;&gt;Cookiecutter Data Science&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Catalog&lt;/td&gt; &#xA;   &lt;td&gt;A series of lightweight data connectors used to save and load data across many different file formats and file systems, including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pipeline Abstraction&lt;/td&gt; &#xA;   &lt;td&gt;Automatic resolution of dependencies between pure Python functions and data pipeline visualisation using &lt;a href=&#34;https://github.com/kedro-org/kedro-viz&#34;&gt;Kedro-Viz&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Coding Standards&lt;/td&gt; &#xA;   &lt;td&gt;Test-driven development using &lt;a href=&#34;https://github.com/pytest-dev/pytest&#34;&gt;&lt;code&gt;pytest&lt;/code&gt;&lt;/a&gt;, produce well-documented code using &lt;a href=&#34;http://www.sphinx-doc.org/en/master/&#34;&gt;Sphinx&lt;/a&gt;, create linted code with support for &lt;a href=&#34;https://github.com/PyCQA/flake8&#34;&gt;&lt;code&gt;flake8&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/PyCQA/isort&#34;&gt;&lt;code&gt;isort&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;code&gt;black&lt;/code&gt;&lt;/a&gt; and make use of the standard Python logging library.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flexible Deployment&lt;/td&gt; &#xA;   &lt;td&gt;Deployment strategies that include single or distributed-machine deployment as well as additional support for deploying on Argo, Prefect, Kubeflow, AWS Batch and Databricks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How do I use Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/&#34;&gt;Kedro documentation&lt;/a&gt; includes three examples to help get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A typical &#34;Hello World&#34; example, for an &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/get_started/hello_kedro.html&#34;&gt;entry-level description of the main Kedro concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/get_started/example_project.html&#34;&gt;introduction to the project template&lt;/a&gt; using the Iris dataset&lt;/li&gt; &#xA; &lt;li&gt;A more detailed &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/tutorial/tutorial_template.html&#34;&gt;spaceflights tutorial&lt;/a&gt; to give you hands-on experience&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why does Kedro exist?&lt;/h2&gt; &#xA;&lt;p&gt;Kedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To address the main shortcomings of Jupyter notebooks, one-off scripts, and glue-code because there is a focus on creating &lt;strong&gt;maintainable data science code&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;To enhance &lt;strong&gt;team collaboration&lt;/strong&gt; when different team members have varied exposure to software engineering concepts&lt;/li&gt; &#xA; &lt;li&gt;To increase efficiency, because applied concepts like modularity and separation of concerns inspire the creation of &lt;strong&gt;reusable analytics code&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The humans behind Kedro&lt;/h2&gt; &#xA;&lt;p&gt;Kedro is maintained by &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/faq/faq.html&#34;&gt;a product team&lt;/a&gt; and a number of &lt;a href=&#34;https://github.com/kedro-org/kedro/releases&#34;&gt;contributors from across the world&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Can I contribute?&lt;/h2&gt; &#xA;&lt;p&gt;Yes! Want to help build Kedro? Check out our &lt;a href=&#34;https://github.com/kedro-org/kedro/raw/main/CONTRIBUTING.md&#34;&gt;guide to contributing to Kedro&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Where can I learn more?&lt;/h2&gt; &#xA;&lt;p&gt;There is a growing community around Kedro. Have a look at the &lt;a href=&#34;https://kedro.readthedocs.io/en/stable/faq/faq.html#how-can-i-find-out-more-about-kedro&#34;&gt;Kedro FAQs&lt;/a&gt; to find projects using Kedro and links to articles, podcasts and talks.&lt;/p&gt; &#xA;&lt;h2&gt;Who likes Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;There are Kedro users across the world, who work at start-ups, major enterprises and academic institutions like &lt;a href=&#34;https://www.absa.co.za/&#34;&gt;Absa&lt;/a&gt;, &lt;a href=&#34;https://acensi.eu/page/home&#34;&gt;Acensi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:6863494681372721152/&#34;&gt;Advanced Programming Solutions SL&lt;/a&gt;, &lt;a href=&#34;https://makerspace.aisingapore.org/2020/08/leveraging-kedro-in-100e/&#34;&gt;AI Singapore&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/posts/augment-partners_kedro-cheat-sheet-by-augment-activity-6858927624631283712-Ivqk&#34;&gt;Augment Partners&lt;/a&gt;, &lt;a href=&#34;https://www.axa.co.uk/&#34;&gt;AXA UK&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/posts/vangansen_mlops-machinelearning-kedro-activity-6772379995953238016-JUmo&#34;&gt;Belfius&lt;/a&gt;, &lt;a href=&#34;https://medium.com/hacking-talent/production-code-for-data-science-and-our-experience-with-kedro-60bb69934d1f&#34;&gt;Beamery&lt;/a&gt;, &lt;a href=&#34;https://www.caterpillar.com/&#34;&gt;Caterpillar&lt;/a&gt;, &lt;a href=&#34;https://www.crim.ca/en/&#34;&gt;CRIM&lt;/a&gt;, &lt;a href=&#34;https://www.dendra.io/&#34;&gt;Dendra Systems&lt;/a&gt;, &lt;a href=&#34;https://www.elementai.com/&#34;&gt;Element AI&lt;/a&gt;, &lt;a href=&#34;https://getindata.com/blog/running-machine-learning-pipelines-kedro-kubeflow-airflow&#34;&gt;GetInData&lt;/a&gt;, &lt;a href=&#34;https://recruit.gmo.jp/engineer/jisedai/engineer/jisedai/engineer/jisedai/engineer/jisedai/engineer/jisedai/blog/kedro_and_mlflow_tracking/&#34;&gt;GMO&lt;/a&gt;, &lt;a href=&#34;https://medium.com/indiciumtech/how-to-build-models-as-products-using-mlops-part-2-machine-learning-pipelines-with-kedro-10337c48de92&#34;&gt;Indicium&lt;/a&gt;, &lt;a href=&#34;https://github.com/dssg/barefoot-winnie-public&#34;&gt;Imperial College London&lt;/a&gt;, &lt;a href=&#34;https://www.ing.com&#34;&gt;ING&lt;/a&gt;, &lt;a href=&#34;https://junglescouteng.medium.com/jungle-scout-case-study-kedro-airflow-and-mlflow-use-on-production-code-150d7231d42e&#34;&gt;Jungle Scout&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/posts/lionel-trebuchon_mlflow-kedro-ml-ugcPost-6747074322164154368-umKw&#34;&gt;Helvetas&lt;/a&gt;, &lt;a href=&#34;https://www.lftechnology.com/blog/ai-pipeline-kedro/&#34;&gt;Leapfrog&lt;/a&gt;, &lt;a href=&#34;https://www.mckinsey.com/alumni/news-and-insights/global-news/firm-news/kedro-from-proprietary-to-open-source&#34;&gt;McKinsey &amp;amp; Company&lt;/a&gt;, &lt;a href=&#34;https://www.mercadolibre.com.ar&#34;&gt;Mercado Libre Argentina&lt;/a&gt;, &lt;a href=&#34;https://www.modec.com/&#34;&gt;Modec&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=fCWGevB366g&#34;&gt;Mosaic Data Science&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=_0kMmRfltEQ&#34;&gt;NaranjaX&lt;/a&gt;, &lt;a href=&#34;https://github.com/nasa/ML-airport-taxi-out&#34;&gt;NASA&lt;/a&gt;, &lt;a href=&#34;https://www.odesla.org/&#34;&gt;Open Data Science LatAm&lt;/a&gt;, &lt;a href=&#34;https://prediqt.co/&#34;&gt;Prediqt&lt;/a&gt;, &lt;a href=&#34;https://medium.com/quantumblack/introducing-kedro-the-open-source-library-for-production-ready-machine-learning-code-d1c6d26ce2cf&#34;&gt;QuantumBlack&lt;/a&gt;, &lt;a href=&#34;https://tech.retrieva.jp/entry/2020/07/28/181414&#34;&gt;Retrieva&lt;/a&gt;, &lt;a href=&#34;https://www.roche.com/&#34;&gt;Roche&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/posts/seleznev-artem_welcome-to-kedros-documentation-kedro-activity-6767523561109385216-woTt&#34;&gt;Sber&lt;/a&gt;, &lt;a href=&#34;https://www.societegenerale.com/en&#34;&gt;SociÃ©tÃ© GÃ©nÃ©rale&lt;/a&gt;, &lt;a href=&#34;https://medium.com/life-at-telkomsel/how-we-build-a-production-grade-data-pipeline-7004e56c8c98&#34;&gt;Telkomsel&lt;/a&gt;, &lt;a href=&#34;https://github.com/vchaparro/MasterThesis-wind-power-forecasting/raw/master/thesis.pdf&#34;&gt;Universidad Rey Juan Carlos&lt;/a&gt;, &lt;a href=&#34;https://urbanlogiq.com/&#34;&gt;UrbanLogiq&lt;/a&gt;, &lt;a href=&#34;https://wildlifestudios.com&#34;&gt;Wildlife Studios&lt;/a&gt;, &lt;a href=&#34;https://www.wovenlight.com/&#34;&gt;WovenLight&lt;/a&gt; and &lt;a href=&#34;https://youtu.be/wgnGOVNkXqU?t=2210&#34;&gt;XP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Kedro has also won &lt;a href=&#34;https://awards.ai/the-awards/previous-awards/the-4th-ai-award-winners/&#34;&gt;Best Technical Tool or Framework for AI&lt;/a&gt; in the 2019 Awards AI competition and a merit award for the 2020 &lt;a href=&#34;https://uktcawards.com/announcing-the-award-winners-for-2020/&#34;&gt;UK Technical Communication Awards&lt;/a&gt;. It is listed on the 2020 &lt;a href=&#34;https://www.thoughtworks.com/radar/languages-and-frameworks/kedro&#34;&gt;ThoughtWorks Technology Radar&lt;/a&gt; and the 2020 &lt;a href=&#34;https://mattturck.com/data2020/&#34;&gt;Data &amp;amp; AI Landscape&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How can I cite Kedro?&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re an academic, Kedro can also help you, for example, as a tool to solve the problem of reproducible research. Use the &#34;Cite this repository&#34; button on &lt;a href=&#34;https://github.com/kedro-org/kedro&#34;&gt;our repository&lt;/a&gt; to generate a citation from the &lt;a href=&#34;https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files&#34;&gt;CITATION.cff file&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TingsongYu/PyTorch_Tutorial</title>
    <updated>2022-09-03T01:39:33Z</updated>
    <id>tag:github.com,2022-09-03:/TingsongYu/PyTorch_Tutorial</id>
    <link href="https://github.com/TingsongYu/PyTorch_Tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ãPytorchæ¨¡åè®­ç»å®ç¨æç¨ãä¸­éå¥ä»£ç &lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pytorchæ¨¡åè®­ç»å®ç¨æç¨&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/tensor-yu/PyTorch_Tutorial/raw/master/Data/cover.png&#34; alt=&#34;Image text&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;1.ç®ä»&lt;/h1&gt; &#xA;&lt;p&gt;æ¬ä»£ç ä¸ºæç¨ââãPytorchæ¨¡åè®­ç»å®ç¨æç¨ãä¸­éå¥ä»£ç ï¼&lt;br&gt; ãPytorchæ¨¡åè®­ç»å®ç¨æç¨ãå¯éè¿å¦ä¸æ¹å¼è·åï¼&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensor-yu/PyTorch_Tutorial/tree/master/Data&#34;&gt;https://github.com/tensor-yu/PyTorch_Tutorial/tree/master/Data&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;QQç¾¤ï¼671103375(å·²æ»¡) 773031536 &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;2.ç¯å¢éç½®&lt;/h1&gt; &#xA;&lt;p&gt;ä»£ç å¨ä»¥ä¸ä¸¤ç§ç¯å¢æµè¯è¿ï¼&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;win10 64ä½ + python3.5 + pytorch==0.4.0 &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;mac + python3.6 + pytorch==0.4.1/ pytorch==1.0.0 &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¬¬ä¸æ­¥ å®è£åä¾èµåï¼&lt;/strong&gt;&lt;br&gt; pip install -r requirements.txt&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¬¬äºæ­¥ æå¨å®è£pytorchåtorchvisionï¼&lt;/strong&gt;&lt;br&gt; åéæ©æ gpuçæ¬è¿è¡å®è£ï¼è¿å¥å®ç½éæ©ç¸åºçæä»¤è¿è¡å®è£ &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;3.é®é¢åé¦&lt;/h1&gt; &#xA;&lt;p&gt;è¥åç°ä»»ä½é®é¢åæ¹è¿æè§ï¼è¯·æ¨éæ¶èç³»æã&lt;br&gt; èç³»æ¹å¼ï¼&lt;a href=&#34;mailto:yts3221@126.com&#34;&gt;yts3221@126.com&lt;/a&gt;&lt;br&gt; è¯»èqqç¾¤ï¼&lt;/p&gt; &#xA;&lt;p&gt;â ä¸ç¾¤ï¼671103375 (å·²æ»¡) &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;â äºç¾¤ï¼773031536 (å·²æ»¡ï¼&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;â ä¸ç¾¤ï¼514974779&lt;/p&gt; &#xA;&lt;h1&gt;4.ä¿®æ¹è®°å½&lt;/h1&gt; &#xA;&lt;p&gt;0.0.5ï¼&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;1.6å°èåè¯¯ï¼å°36*36æ¹ä¸º40*40ï¼&lt;/li&gt; &#xA; &lt;li&gt;2.3å°èå é¤æ³¨éï¼&lt;/li&gt; &#xA; &lt;li&gt;ä¿®æ¹æå¼åå§åæè°ä¸­ççè§£éè¯¯ï¼&lt;/li&gt; &#xA; &lt;li&gt;å¨æä»£ç ç¼©è¿ã&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Stargazers over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/TingsongYu/PyTorch_Tutorial&#34;&gt;&lt;img src=&#34;https://starchart.cc/TingsongYu/PyTorch_Tutorial.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>