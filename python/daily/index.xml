<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-02T01:39:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jianchang512/ChatTTS-ui</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/jianchang512/ChatTTS-ui</id>
    <link href="https://github.com/jianchang512/ChatTTS-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;一个简单的本地网页界面，直接使用ChatTTS将文字合成为语音，同时支持对外提供API接口。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatTTS webUI &amp;amp; API&lt;/h1&gt; &#xA;&lt;p&gt;一个简单的本地网页界面，直接在网页使用 &lt;a href=&#34;https://github.com/2noise/chattts&#34;&gt;ChatTTS&lt;/a&gt; 将文字合成为语音，同时支持对外提供API接口。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/ChatTTS-ui/releases&#34;&gt;Releases中可下载Windows整合包&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;界面预览&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/6ed7c993-3882-4c34-9abd-f0635b133012&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;试听合成语音效果&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/03cf1c0f-0245-44b5-8007-370d9db2bda8&#34;&gt;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/03cf1c0f-0245-44b5-8007-370d9db2bda8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Windows预打包版&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;从 &lt;a href=&#34;https://github.com/jianchang512/chatTTS-ui/releases&#34;&gt;Releases&lt;/a&gt;中下载压缩包，解压后双击 app.exe 即可使用&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Linux 下源码部署&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;配置好 python3.9+环境&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建空目录 &lt;code&gt;/data/chattts&lt;/code&gt; 执行命令 &lt;code&gt;cd /data/chattts &amp;amp;&amp;amp; git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建虚拟环境 &lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;激活虚拟环境 &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装依赖 &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果不需要CUDA加速，执行 &lt;code&gt;pip3 install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;p&gt;如果需要CUDA加速，执行&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&#xA;&#x9;&#x9;&#xA;pip install nvidia-cublas-cu11 nvidia-cudnn-cu11&#xA;&#x9;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;另需安装 CUDA11.8+ ToolKit，请自行搜索安装方法 或参考 &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;https://juejin.cn/post/7318704408727519270&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;执行 &lt;code&gt;python3 app.py&lt;/code&gt; 启动，将自动打开浏览器窗口，默认地址 &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;MacOS 下源码部署&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;配置好 python3.9+环境,安装git ，执行命令 &lt;code&gt;brew install git python@3.10&lt;/code&gt; 继续执行&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PATH=&#34;/usr/local/opt/python@3.10/bin:$PATH&#34;&#xA;&#xA;source ~/.bash_profile &#xA;&#xA;source ~/.zshrc&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建空目录 &lt;code&gt;/data/chattts&lt;/code&gt; 执行命令 &lt;code&gt;cd /data/chattts &amp;amp;&amp;amp; git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建虚拟环境 &lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;激活虚拟环境 &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装依赖 &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装torch &lt;code&gt;pip3 install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;执行 &lt;code&gt;python3 app.py&lt;/code&gt; 启动，将自动打开浏览器窗口，默认地址 &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Macos下可能会到一些问题，请查看 &lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/ChatTTS-ui/main/faq.md&#34;&gt;常见问题与报错解决方法&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Windows源码部署&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;下载python3.9+，安装时注意选中&lt;code&gt;Add Python to environment variables&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;下载并安装git，&lt;a href=&#34;https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe&#34;&gt;https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建空文件夹 &lt;code&gt;D:/chattts&lt;/code&gt; 并进入，地址栏输入 &lt;code&gt;cmd&lt;/code&gt;回车，在弹出的cmd窗口中执行命令 &lt;code&gt;git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;创建虚拟环境，执行命令 &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;激活虚拟环境，执行 &lt;code&gt;.\venv\scripts\activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装依赖,执行 &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果不需要CUDA加速，执行 &lt;code&gt;pip install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;p&gt;如果需要CUDA加速，执行&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/p&gt; &lt;p&gt;另需安装 CUDA11.8+ ToolKit，请自行搜索安装方法或参考 &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;https://juejin.cn/post/7318704408727519270&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;执行 &lt;code&gt;python app.py&lt;/code&gt; 启动，将自动打开浏览器窗口，默认地址 &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;源码部署注意&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;源码部署启动后，会先从 modelscope下载模型，但modelscope缺少spk_stat.pt，会报错，请点击链接 &lt;a href=&#34;https://huggingface.co/2Noise/ChatTTS/blob/main/asset/spk_stat.pt&#34;&gt;https://huggingface.co/2Noise/ChatTTS/blob/main/asset/spk_stat.pt&lt;/a&gt; 下载 spk_stat.pt，将该文件复制到 &lt;code&gt;项目目录/models/pzc163/chatTTS/asset/ 文件夹内&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;注意 modelscope 仅允许中国大陆ip下载模型，如果遇到 proxy 类错误，请关闭代理。如果你希望从 huggingface.co 下载模型，请打开 &lt;code&gt;app.py&lt;/code&gt; 查看大约第50行-60行的注释。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果需要GPU加速，必须是英伟达显卡，并且安装 cuda版本的torch。&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 默认从 modelscope 下载模型,如果想从huggingface下载模型，请将以下3行注释掉&#xA;CHATTTS_DIR = snapshot_download(&#39;pzc163/chatTTS&#39;,cache_dir=MODEL_DIR)&#xA;chat = ChatTTS.Chat()&#xA;chat.load_models(source=&#34;local&#34;,local_path=CHATTTS_DIR)&#xA;&#xA;# 如果希望从 huggingface.co下载模型，将以下注释删掉。将上方3行内容注释掉&#xA;#os.environ[&#39;HF_HUB_CACHE&#39;]=MODEL_DIR&#xA;#os.environ[&#39;HF_ASSETS_CACHE&#39;]=MODEL_DIR&#xA;#chat = ChatTTS.Chat()&#xA;#chat.load_models()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/ChatTTS-ui/main/faq.md&#34;&gt;常见问题与报错解决方法&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;修改http地址&lt;/h2&gt; &#xA;&lt;p&gt;默认地址是 &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;,如果想修改，可打开目录下的 &lt;code&gt;.env&lt;/code&gt;文件，将 &lt;code&gt;WEB_ADDRESS=127.0.0.1:9966&lt;/code&gt;改为合适的ip和端口，比如修改为&lt;code&gt;WEB_ADDRESS=192.168.0.10:9966&lt;/code&gt;以便局域网可访问&lt;/p&gt; &#xA;&lt;h2&gt;使用API请求 v0.5+&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;请求方法:&lt;/strong&gt; POST&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;请求地址:&lt;/strong&gt; &lt;a href=&#34;http://127.0.0.1:9966/tts&#34;&gt;http://127.0.0.1:9966/tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;请求参数:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;text: str| 必须， 要合成语音的文字&lt;/p&gt; &#xA;&lt;p&gt;voice: int| 可选，默认 2222, 决定音色的数字， 2222 | 7869 | 6653 | 4099 | 5099，可选其一，或者任意传入将随机使用音色&lt;/p&gt; &#xA;&lt;p&gt;prompt: str| 可选，默认 空， 设定 笑声、停顿，例如 [oral_2][laugh_0][break_6]&lt;/p&gt; &#xA;&lt;p&gt;temperature: float| 可选， 默认 0.3&lt;/p&gt; &#xA;&lt;p&gt;top_p: float| 可选， 默认 0.7&lt;/p&gt; &#xA;&lt;p&gt;top_k: int| 可选， 默认 20&lt;/p&gt; &#xA;&lt;p&gt;skip_refine: int| 可选， 默认0， 1=跳过 refine text，0=不跳过&lt;/p&gt; &#xA;&lt;p&gt;custom_voice: int| 可选， 默认0，自定义获取音色值时的种子值，需要大于0的整数，如果设置了则以此为准，将忽略 &lt;code&gt;voice&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;返回:json数据&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;成功返回: {code:0,msg:ok,audio_files:[dict1,dict2]}&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;其中 audio_files 是字典数组，每个元素dict为 {filename:wav文件绝对路径，url:可下载的wav网址}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;失败返回:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{code:1,msg:错误原因}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;# API调用代码&#xA;&#xA;import requests&#xA;&#xA;res = requests.post(&#39;http://127.0.0.1:9966/tts&#39;, data={&#xA;  &#34;text&#34;: &#34;若不懂无需填写&#34;,&#xA;  &#34;prompt&#34;: &#34;&#34;,&#xA;  &#34;voice&#34;: &#34;3333&#34;,&#xA;  &#34;temperature&#34;: 0.3,&#xA;  &#34;top_p&#34;: 0.7,&#xA;  &#34;top_k&#34;: 20,&#xA;  &#34;skip_refine&#34;: 0,&#xA;  &#34;custom_voice&#34;: 0&#xA;})&#xA;print(res.json())&#xA;&#xA;#ok&#xA;{code:0, msg:&#39;ok&#39;, audio_files:[{filename: E:/python/chattts/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav, url: http://127.0.0.1:9966/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav}]}&#xA;&#xA;#error&#xA;{code:1, msg:&#34;error&#34;}&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;在pyVideoTrans软件中使用&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;升级 pyVideoTrans 到 1.82+ &lt;a href=&#34;https://github.com/jianchang512/pyvideotrans&#34;&gt;https://github.com/jianchang512/pyvideotrans&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;点击菜单-设置-ChatTTS，填写请求地址，默认应该填写 &lt;a href=&#34;http://127.0.0.1:9966&#34;&gt;http://127.0.0.1:9966&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;测试无问题后，在主界面中选择&lt;code&gt;ChatTTS&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/7118325f-2b9a-46ce-a584-1d5c6dc8e2da&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tencent-ailab/V-Express</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/tencent-ailab/V-Express</id>
    <link href="https://github.com/tencent-ailab/V-Express" rel="alternate"></link>
    <summary type="html">&lt;p&gt;V-Express aims to generate a talking head video under the control of a reference image, an audio, and a sequence of V-Kps images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;&lt;em&gt;V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation&lt;/em&gt;&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/tk93/V-Express&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![GitHub](https://img.shields.io/github/stars/tencent-ailab/IP-Adapter?style=social)](https://github.com/tencent-ailab/IP-Adapter/) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals can vary in strength, including text, audio, image reference, pose, depth map, etc. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as pose and original image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through a series of progressive drop operations. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account pose, input image, and audio.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/V-Express/main/assets/global_framework.png&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/05/29] 🔥 We have added video post-processing that can effectively mitigate the flicker problem.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05/23] 🔥 We release the code and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# install requirements&#xA;pip install diffusers==0.24.0&#xA;pip install imageio-ffmpeg==0.4.9&#xA;pip install insightface==0.7.3&#xA;pip install omegaconf==2.2.3&#xA;pip install onnxruntime==1.16.3&#xA;pip install safetensors==0.4.2&#xA;pip install torch==2.0.1&#xA;pip install torchaudio==2.0.2&#xA;pip install torchvision==0.15.2&#xA;pip install transformers==4.30.2&#xA;pip install einops==0.4.1&#xA;pip install tqdm==4.66.1&#xA;pip install xformers==0.0.22&#xA;pip install av==11.0.0&#xA;&#xA;# download the codes&#xA;git clone https://github.com/tencent-ailab/V-Express&#xA;&#xA;# download the models&#xA;cd V-Express&#xA;git lfs install&#xA;git clone https://huggingface.co/tk93/V-Express&#xA;mv V-Express/model_ckpts model_ckpts&#xA;&#xA;# then you can use the scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Models&lt;/h2&gt; &#xA;&lt;p&gt;you can download models from &lt;a href=&#34;https://huggingface.co/tk93/V-Express&#34;&gt;here&lt;/a&gt;. We have included all the required models in the model card. You can also download the models separately from the original repository.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;stabilityai/sd-vae-ft-mse&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;. Only the model configuration file for unet is needed here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-base-960h&#34;&gt;facebook/wav2vec2-base-960h&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip&#34;&gt;insightface/buffalo_l&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;span style=&#34;color:red&#34;&gt;Important Reminder&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Important! Important!! Important!!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the talking-face generation task, when the target video is not the same person as the reference character, the retarget of the face will be a &lt;span style=&#34;color:red&#34;&gt;very important&lt;/span&gt; part. And choosing a target video that is more similar to the pose of the reference face will be able to get better results. In addition, our model now performs better on English, and other languages have not yet been tested in detail.&lt;/p&gt; &#xA;&lt;h3&gt;Run the demo (step1, &lt;em&gt;optional&lt;/em&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;If you have a target talking video, you can follow the script below to extract the audio and face V-kps sequences from the video. You can also skip this step and run the script in Step 2 directly to try the example we provided.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/extract_kps_sequence_and_audio.py \&#xA;    --video_path &#34;./test_samples/short_case/AOC/gt.mp4&#34; \&#xA;    --kps_sequence_save_path &#34;./test_samples/short_case/AOC/kps.pth&#34; \&#xA;    --audio_save_path &#34;./test_samples/short_case/AOC/aud.mp3&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend cropping a clear square face image as in the example below and making sure the resolution is no lower than 512x512. The green to red boxes in the image below are the recommended cropping ranges.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/V-Express/main/assets/crop_example.jpeg&#34; alt=&#34;drawing&#34; style=&#34;width:500px;&#34;&gt; &#xA;&lt;h3&gt;Run the demo (step2, &lt;em&gt;core&lt;/em&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scenario 1 (A&#39;s picture and A&#39;s talking video.) (Best Practice)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have a picture of A and a talking video of A in another scene. Then you should run the following script. Our model is able to generate speaking videos that are consistent with the given video. &lt;em&gt;You can see more examples on our &lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;project page&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/AOC/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/AOC/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/AOC/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_AOC_no_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;no_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/17dd4103-eaf7-4045-8bc0-e90093deaee8&#34; style=&#34;width: 80%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;p&gt;&lt;strong&gt;Scenario 2 (A&#39;s picture and any talking audio.)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you only have a picture and any talking audio. With the following script, our model can generate vivid mouth movements for fixed faces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_fix_face.mp4&#34; \&#xA;    --retarget_strategy &#34;fix_face&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/fe782c16-f341-424d-83ce-89531af2a292&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;p&gt;&lt;strong&gt;Scenario 3 (A&#39;s picture and B&#39;s talking video.)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the script below, our model generates vivid mouth movements accompanied by slight facial motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/tys/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_offset_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;offset_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/4951d06c-579d-499e-994d-14fa7e524713&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the following script, our model generates a video with the same movements as the target video, and the character&#39;s lip-synching matches the target audio.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We have only implemented the very naive retarget strategy so far, which allows us to achieve driving the reference face with different character videos under limited conditions. To get better results, we strongly recommend you to choose a target video that is closer to the reference face. We are also trying to implement a more robust face retargeting strategy, which hopefully can further solve the problem of inconsistency between the reference face and the target face. We also welcome experienced people who can help.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/tys/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_naive_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;naive_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/d555ed02-56eb-44e5-94e5-772edcd3338b&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;h3&gt;More parameters&lt;/h3&gt; &#xA;&lt;p&gt;For different types of input condition, such as reference image and target audio, we provide parameters for adjusting the role played by that condition information in the model prediction. We refer to these two parameters as &lt;code&gt;reference_attention_weight&lt;/code&gt; and &lt;code&gt;audio_attention_weight&lt;/code&gt;. Different parameters can be applied to achieve different effects using the following script. &lt;code&gt;Through our experiments, we suggest that reference_attention_weight takes the value 0.9-1.0 and audio_attention_weight takes the value 1.0-3.0.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/10/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/10/aud.mp3&#34; \&#xA;    --output_path &#34;./output/short_case/talk_10_fix_face_with_weight.mp4&#34; \&#xA;    --retarget_strategy &#34;fix_face&#34; \    # this strategy do not need kps info&#xA;    --reference_attention_weight 0.95 \&#xA;    --audio_attention_weight 3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We show the different effects produced by different parameters in the following video. You can adjust the parameters accordingly to your needs.&lt;/p&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/2e977b8c-c69b-4815-8565-d4d7c3c349a9&#34; style=&#34;width: 100%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors to the &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;magic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/discussions/1236&#34;&gt;sd-webui-controlnet&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;p&gt;The code of V-Express is released for both academic and commercial usage. However, both manual-downloading and auto-downloading models from V-Express are for non-commercial research purposes. Our released checkpoints are also for research purposes only. Users are granted the freedom to create videos using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find V-Express useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024V-Express,&#xA;  title={V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation},&#xA;  author={Wang, Cong and Tian, Kuan and Zhang, Jun and Guan, Yonghang and Luo, Feng and Shen, Fei and Jiang, Zhiwei and Gu, Qing and Han, Xiao and Yang, Wei},&#xA;  booktitle={arXiv preprint arxiv: comming soon},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ToonCrafter/ToonCrafter</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/ToonCrafter/ToonCrafter</id>
    <link href="https://github.com/ToonCrafter/ToonCrafter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a research paper for generative cartoon interpolation&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;em&gt;&lt;strong&gt;&lt;em&gt;&lt;strong&gt;ToonCrafter: Generative Cartoon Interpolation&lt;/strong&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;!-- ![](./assets/logo_long.png#gh-light-mode-only){: width=&#34;50%&#34;} --&gt; &#xA;&lt;!-- ![](./assets/logo_long_dark.png#gh-dark-mode-only=100x20) --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🔆 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ Please check our &lt;a href=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/#disc&#34;&gt;disclaimer&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;p&gt;🤗 ToonCrafter can interpolate two cartoon images by leveraging the pre-trained image-to-video diffusion priors. Please check our project page and paper for more information. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.1 Showcases (512x320)&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72109_125.mp4_00-00.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72109_125.mp4_00-01.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/00.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_2_062266_s2_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_2_062266_s2_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/03.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_1_070321_s3_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_1_070321_s3_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/02.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/74302_1349_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/74302_1349_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/01.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;1.2 Sparse sketch guidance&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Input sketch guidance&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72105_388.mp4_00-00.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72105_388.mp4_00-01.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/06.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/07.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72110_255.mp4_00-00.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72110_255.mp4_00-01.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/12.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/13.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;2. Applications&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 Cartoon Sketch Interpolation (see project page for more details)&lt;/h4&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_10.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0016_10.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/10.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_11.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0016_11.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/11.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.2 Reference-based Sketch Colorization&lt;/h4&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input sketch&lt;/td&gt; &#xA;   &lt;td&gt;Input reference&lt;/td&gt; &#xA;   &lt;td&gt;Colorization results&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/04.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_05.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/05.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/08.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_09.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/09.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📝 Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add sketch control and colorization function.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.05.29]&lt;/strong&gt;: 🔥🔥 Release code and model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.05.28]&lt;/strong&gt;: Launch the project page and update the arXiv preprint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;🧰 Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;GPU Mem. &amp;amp; Inference Time (A100, ddim 50steps)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ToonCrafter_512&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;320x512&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TBD (&lt;code&gt;perframe_ae=True&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/Doubiiu/ToonCrafter/blob/main/model.ckpt&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Currently, our ToonCrafter can support generating videos of up to 16 frames with a resolution of 512x320. The inference time can be reduced by using fewer DDIM steps.&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install Environment via Anaconda (Recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n tooncrafter python=3.8.5&#xA;conda activate tooncrafter&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💫 Inference&lt;/h2&gt; &#xA;&lt;h3&gt;1. Command line&lt;/h3&gt; &#xA;&lt;p&gt;Download pretrained ToonCrafter_512 and put the &lt;code&gt;model.ckpt&lt;/code&gt; in &lt;code&gt;checkpoints/tooncrafter_512_interp_v1/model.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  sh scripts/run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Local Gradio demo&lt;/h3&gt; &#xA;&lt;p&gt;Download the pretrained model and put it in the corresponding directory according to the previous guidelines.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  python gradio_app.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- ## 🤝 Community Support --&gt; &#xA;&lt;p&gt;&lt;a name=&#34;disc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📢 Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Calm down. Our framework opens up the era of generative cartoon interpolation, but due to the variaity of generative video prior, the success rate is not guaranteed.&lt;/p&gt; &#xA;&lt;p&gt;⚠️This is an open-source research exploration, instead of commercial products. It can&#39;t meet all your expectations.&lt;/p&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
</feed>