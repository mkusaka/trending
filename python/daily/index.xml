<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-04T01:36:57Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wenquanlu/HandRefiner</title>
    <updated>2024-01-04T01:36:57Z</updated>
    <id>tag:github.com,2024-01-04:/wenquanlu/HandRefiner</id>
    <link href="https://github.com/wenquanlu/HandRefiner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/wenquanlu/HandRefiner/main/%5Bhttps://arxiv.org/abs/2305.02034%5D(https://arxiv.org/abs/2311.17957)&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-&lt;color&gt;&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.12.1&lt;/strong&gt; The paper is post on arxiv!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.12.29&lt;/strong&gt; First code commit released.&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;This is the official repository of the paper &lt;a href=&#34;https://arxiv.org/abs/2311.17957&#34;&gt; HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting &lt;/a&gt;&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/wenquanlu/HandRefiner/main/Figs/banner.png&#34;&gt; &#xA; &lt;figcaption align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;Figure 1: Stable Diffusion (first two rows) and SDXL (last row) generate malformed hands (left in each pair), e.g., incorrect number of fingers or irregular shapes, which can be effectively rectified by our HandRefiner (right in each pair). &lt;/b&gt;&#xA; &lt;/figcaption&gt; &#xA;&lt;/figure&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;left&#34;&gt; In this study, we introduce a lightweight post-processing solution called &lt;b&gt;HandRefiner&lt;/b&gt; to correct malformed hands in generated images. HandRefiner employs a conditional inpainting approach to rectify malformed hands while leaving other parts of the image untouched. We leverage the hand mesh reconstruction model that consistently adheres to the correct number of fingers and hand shape, while also being capable of fitting the desired hand pose in the generated image. Given a generated failed image due to malformed hands, we utilize ControlNet modules to re-inject such correct hand information. Additionally, we uncover a phase transition phenomenon within ControlNet as we vary the control strength. It enables us to take advantage of more readily available synthetic data without suffering from the domain gap between realistic and synthetic hands. &lt;/p&gt;&#xA;&lt;h1&gt;Visual Results&lt;/h1&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/wenquanlu/HandRefiner/main/Figs/github_results.png&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/wenquanlu/HandRefiner/main/docs/installation.md&#34;&gt;installation.md&lt;/a&gt; for installation instructions.&lt;/p&gt; &#xA;&lt;h1&gt;Manual&lt;/h1&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/wenquanlu/HandRefiner/main/docs/manual.md&#34;&gt;manual.md&lt;/a&gt; for an explanation of commands to execute the HandRefiner.&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;For single image rectification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python handrefiner.py --input_img test/1.jpg --out_dir output --strength 0.55 --weights models/inpaint_depth_control.ckpt --prompt &#34;a man facing the camera, making a hand gesture, indoor&#34; --seed 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multiple image rectifications:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python handrefiner.py --input_dir test --out_dir output --strength 0.55 --weights models/inpaint_depth_control.ckpt --prompt_file test/test.json --seed 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Important Q&amp;amp;A&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;b&gt;What kind of images can be rectified?&lt;/b&gt;&lt;/li&gt; &#xA; &lt;p&gt;Like any method, this method also has its limits. If the original hands are so bad that are inrecognisable from human eyes, then it is pretty much impossible for neural networks to fit a reasonable mesh. Also, due to the fitting nature of the method, we do not rectify the hand size. So if you have a giant malformed hand in the original image, you will still get a giant hand back in the rectified image. Thus malformed hands with hand-like shape and appropriate size can be rectified.&lt;/p&gt; &#xA; &lt;li&gt; &lt;b&gt;Can we use it on SDXL images?&lt;/b&gt; &lt;p&gt;In the paper, the SDXL images are resized to 512x512 before the rectification, because the base model used in this project is sd1.5. Solution for SDXL: However, it is certainly not difficult to implement it in SDXL, and I believe many implementations already have the functionality of using inpainting SDXL combined with depth controlnet to inpaint the image. So what you can do is get the depth map and masks from the pipeline of this repository, then pipe them to the whatever implementation for SDXL you use for inpainting the image. A caveat is that I have not tested this before, and as mentioned in the paper, since depth controlnet is not fine-tuned on the hand mesh data, it may have a high rate of failed inpainting. In that case, you can use the technique mentioned in the paper, using available synthetic data to fine-tune the depth sdxl controlnet, for example, using these two datasets here &lt;a href=&#34;https://synthesis.ai/static-gestures-dataset/&#34;&gt;[1]&lt;/a&gt;&lt;a href=&#34;https://synthesis.ai/animated-gestures-dataset/&#34;&gt;[2]&lt;/a&gt;, then you can adjust control strength to get the desired texture and appearance.&lt;/p&gt; &lt;/li&gt;&#xA; &lt;li&gt; &lt;b&gt;What if the generation failed?&lt;/b&gt; &lt;p&gt;The first thing is to check the depth map, if the depth map is bad, you can consider using a different mesh reconstruction model to reconstruct the mesh.&lt;/p&gt; &lt;p&gt;Second things is to check if the masks of hands fully cover the malformed hands, some malformed hand can have very long fingers so it may not be covered by the detected masks, to fix this&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Consider using a greater padding by adjusting the pad parameter in the argument&lt;/li&gt; &#xA;   &lt;li&gt;Provide a hand-drawn mask&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;If all of the previous steps are ok, then you may need to regenerate several times or try different control strengths. &amp;lt;- changing the seed can be very helpful.&lt;/p&gt; &lt;/li&gt;&#xA; &lt;li&gt; &lt;b&gt;Since the small hands is a limitation mentioned in the paper, what is the appropriate hand size for the SD v1.5 weight?&lt;/b&gt; &lt;p&gt;Generally, hands with size at least 60px × 60px is recommended for the current weights. To make it applicable for small hands, consider scale up the image using some super-resolution methods.&lt;/p&gt; &lt;/li&gt;&#xA; &lt;li&gt; &lt;b&gt;How to contribute to this project?&lt;/b&gt; &lt;p&gt;In the last decade, the CV community has produced dozens of highly accurate mesh reconstruction models, in this project we use the recent SOTA model Mesh Graphormer on the FreiHAND benchmark. However, it is very welcome to contribute to this project by porting other models here, I have written a template parent class for models under preprocessor folder.&lt;/p&gt; &lt;/li&gt;&#xA; &lt;li&gt; &lt;b&gt; Why use other model if Mesh Graphormer is already SOTA? &lt;/b&gt; &lt;p&gt;In this project, we leverage mesh reconstruction model to project these malformed hands onto a reasonable pose space, Mesh Graphormer is only SOTA in terms of reconstructing mesh given correct hand pictures. In particular, Mesh Graphormer utilises a model-free approach to reconstruct hand mesh, through our experiments, this approach is usually more accurate (i.e., more natural appearance ) but less robust (i.e., higher failed reconstruction rate). Another common approach is model-based which uses a parametric deformable hand model to reconstruct mesh, this approach is usually more robust (i.e., lower failed reconstruction rate) but less accurate (i.e., less natural appearance).&lt;/p&gt; &lt;/li&gt;&#xA; &lt;li&gt; &lt;b&gt;Can I use it for Anime hands or other styles?&lt;/b&gt; &lt;p&gt;As long as the hand detection model and the mesh reconstruction model are able to detect the hands and reconstruct meshes, it should work for other styles. However, from my understanding, these models are not trained on cartoon or anime images, so there is a great chance that the mesh reconstruction stage may fail.&lt;/p&gt; &lt;/li&gt;&#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our codebase builds heavily on &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/MeshGraphormer&#34;&gt;MeshGraphormer&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find HandRefiner helpful, please consider giving this repo a star &lt;span&gt;⭐&lt;/span&gt; and citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lu2023handrefiner,&#xA;      title={HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting}, &#xA;      author={Wenquan Lu and Yufei Xu and Jing Zhang and Chaoyue Wang and Dacheng Tao},&#xA;      year={2023},&#xA;      eprint={2311.17957},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>frappe/erpnext</title>
    <updated>2024-01-04T01:36:57Z</updated>
    <id>tag:github.com,2024-01-04:/frappe/erpnext</id>
    <link href="https://github.com/frappe/erpnext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://erpnext.com&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/erpnext-logo.png&#34; height=&#34;128&#34;&gt; &lt;/a&gt; &#xA; &lt;h2&gt;ERPNext&lt;/h2&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;ERP made simple&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests.yml/badge.svg?branch=develop&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/erpnext/erpnext_ui_tests/actions/workflows/ui-tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/erpnext/erpnext_ui_tests/actions/workflows/ui-tests.yml/badge.svg?branch=develop&amp;amp;event=schedule&#34; alt=&#34;UI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codetriage.com/frappe/erpnext&#34;&gt;&lt;img src=&#34;https://www.codetriage.com/frappe/erpnext/badges/users.svg?sanitize=true&#34; alt=&#34;Open Source Helpers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/frappe/erpnext&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/frappe/erpnext/branch/develop/graph/badge.svg?token=0TwvyUg3I5&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/frappe/erpnext-worker&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true&#34; alt=&#34;docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://erpnext.com&#34;&gt;https://erpnext.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;ERPNext as a monolith includes the following areas for managing businesses:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-accounting&#34;&gt;Accounting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/distribution/warehouse-management-system&#34;&gt;Warehouse Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-crm&#34;&gt;CRM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-sales-purchase&#34;&gt;Sales&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-sales-purchase&#34;&gt;Purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-hrms&#34;&gt;HRMS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-projects&#34;&gt;Project Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-help-desk-software&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-asset-management-software&#34;&gt;Asset Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/quality-management&#34;&gt;Quality Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-manufacturing-erp-software&#34;&gt;Manufacturing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-website-builder-software&#34;&gt;Website Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/customize-erpnext&#34;&gt;Customize ERPNext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/&#34;&gt;And More&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ERPNext is built on the &lt;a href=&#34;https://github.com/frappe/frappe&#34;&gt;Frappe Framework&lt;/a&gt;, a full-stack web app framework built with Python &amp;amp; JavaScript.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;max-height: 40px;&#34;&gt; &#xA; &lt;a href=&#34;https://frappecloud.com/erpnext/signup&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/.github/try-on-f-cloud-button.svg?sanitize=true&#34; height=&#34;40&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/frappe/frappe_docker/main/pwd.yml&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&#34; alt=&#34;Try in PWD&#34; height=&#34;37&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Login for the PWD site: (username: Administrator, password: admin)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Containerized Installation&lt;/h3&gt; &#xA;&lt;p&gt;Use docker to deploy ERPNext in production or for development of &lt;a href=&#34;https://github.com/frappe/frappe&#34;&gt;Frappe&lt;/a&gt; apps. See &lt;a href=&#34;https://github.com/frappe/frappe_docker&#34;&gt;https://github.com/frappe/frappe_docker&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Install&lt;/h3&gt; &#xA;&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href=&#34;https://github.com/frappe/bench&#34;&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;New passwords will be created for the ERPNext &#34;Administrator&#34; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; &#xA;&lt;h2&gt;Learning and community&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://frappe.school&#34;&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.erpnext.com/&#34;&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discuss.erpnext.com/&#34;&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext_public.t.me&#34;&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Issue-Guidelines&#34;&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/security&#34;&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Contribution-Guidelines&#34;&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GNU/General Public License (see &lt;a href=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/license.txt&#34;&gt;license.txt&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.&lt;/p&gt; &#xA;&lt;p&gt;By contributing to ERPNext, you agree that your contributions will be licensed under its GNU General Public License (v3).&lt;/p&gt; &#xA;&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; &#xA;&lt;p&gt;Please read our &lt;a href=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md&#34;&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>myshell-ai/OpenVoice</title>
    <updated>2024-01-04T01:36:57Z</updated>
    <id>tag:github.com,2024-01-04:/myshell-ai/OpenVoice</id>
    <link href="https://github.com/myshell-ai/OpenVoice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instant voice cloning by MyShell.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/openvoicelogo.jpg&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://research.myshell.ai/open-voice&#34;&gt;Website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Join Our Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/myshell&#34;&gt;Discord community&lt;/a&gt; and select the &lt;code&gt;Developer&lt;/code&gt; role upon joining to gain exclusive access to our developer-only channel! Don&#39;t miss out on valuable discussions and collaboration opportunities.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;As we detailed in our &lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://research.myshell.ai/open-voice&#34;&gt;website&lt;/a&gt;, the advantages of OpenVoice are three-fold:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Accurate Tone Color Cloning.&lt;/strong&gt; OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Flexible Voice Style Control.&lt;/strong&gt; OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Zero-shot Cross-lingual Voice Cloning.&lt;/strong&gt; Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/myshell-ai/OpenVoice/assets/40556743/3cba936f-82bf-476c-9e52-09f0f417bb2f&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/framework-ipa.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;OpenVoice has been powering the instant voice cloning capability of &lt;a href=&#34;https://app.myshell.ai/explore&#34;&gt;myshell.ai&lt;/a&gt; since May 2023. Until Nov 2023, the voice cloning model has been used tens of millions of times by users worldwide, and witnessed the explosive user growth on the platform.&lt;/p&gt; &#xA;&lt;h2&gt;Main Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.qinzy.tech&#34;&gt;Zengyi Qin&lt;/a&gt; at MIT and MyShell&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wl-zhao.github.io&#34;&gt;Wenliang Zhao&lt;/a&gt; at Tsinghua University&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yuxumin.github.io&#34;&gt;Xumin Yu&lt;/a&gt; at Tsinghua University&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/ethan_myshell&#34;&gt;Ethan Sun&lt;/a&gt; at MyShell&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Live Demo&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.lepton.ai/playground/openvoice&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/lepton.jpg&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://app.myshell.ai/bot/z6Bvua/1702636181&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/myshell.jpg&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is a implementation that approximates the performance of the internal voice clone technology of &lt;a href=&#34;https://app.myshell.ai/explore&#34;&gt;myshell.ai&lt;/a&gt;. The online version in myshell.ai has better 1) audio quality, 2) voice cloning similarity, 3) speech naturalness and 4) computational efficiency.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repo, and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n openvoice python=3.9&#xA;conda activate openvoice&#xA;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the checkpoint from &lt;a href=&#34;https://myshell-public-repo-hosting.s3.amazonaws.com/checkpoints_1226.zip&#34;&gt;here&lt;/a&gt; and extract it to the &lt;code&gt;checkpoints&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Flexible Voice Style Control.&lt;/strong&gt; Please see &lt;a href=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/demo_part1.ipynb&#34;&gt;&lt;code&gt;demo_part1.ipynb&lt;/code&gt;&lt;/a&gt; for an example usage of how OpenVoice enables flexible style control over the cloned voice.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Cross-Lingual Voice Cloning.&lt;/strong&gt; Please see &lt;a href=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/demo_part2.ipynb&#34;&gt;&lt;code&gt;demo_part2.ipynb&lt;/code&gt;&lt;/a&gt; for an example for languages seen or unseen in the MSML training set.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Gradio Demo.&lt;/strong&gt; Launch a local gradio demo with &lt;a href=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/openvoice_app.py&#34;&gt;&lt;code&gt;python -m openvoice_app --share&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Advanced Usage.&lt;/strong&gt; The base speaker model can be replaced with any model (in any language and style) that the user prefer. Please use the &lt;code&gt;se_extractor.get_se&lt;/code&gt; function as demonstrated in the demo to extract the tone color embedding for the new base speaker.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;5. Tips to Generate Natural Speech.&lt;/strong&gt; There are many single or multi-speaker TTS methods that can generate natural speech, and are readily available. By simply replacing the base speaker model with the model you prefer, you can push the speech naturalness to a level you desire.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tone color converter model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-style base speaker model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-style and multi-lingual demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Base speaker model in other languages&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EN base speaker model with better naturalness&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qin2023openvoice,&#xA;  title={OpenVoice: Versatile Instant Voice Cloning},&#xA;  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},&#xA;  journal={arXiv preprint arXiv:2312.01479},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, which prohibits commercial usage. &lt;strong&gt;MyShell reserves the ability to detect whether an audio is generated by OpenVoice&lt;/strong&gt;, no matter whether the watermark is added or not.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is based on several excellent projects, &lt;a href=&#34;https://github.com/coqui-ai/TTS&#34;&gt;TTS&lt;/a&gt;, &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt;, and &lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;VITS2&lt;/a&gt;. Thanks for their awesome work!&lt;/p&gt;</summary>
  </entry>
</feed>