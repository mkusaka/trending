<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-30T01:43:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>biobootloader/mentat</title>
    <updated>2023-07-30T01:43:49Z</updated>
    <id>tag:github.com,2023-07-30:/biobootloader/mentat</id>
    <link href="https://github.com/biobootloader/mentat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mentat - The AI Coding Assistant&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/bio_bootloader&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/bio_bootloader?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/zbvd9qx9Pb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/XbPdxAMJte?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mentat-ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mentat-ai?color=blue&#34; alt=&#34;Stable Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/biobootloader/mentat/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/mentat-ai.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üßô‚Äç‚ôÇÔ∏è Mentat ‚ö°&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;It is by will alone I set my mind in motion&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;The Mentat Mantra&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The Mentats of Dune combine human creativity with computer-like processing - and now you can too.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Mentat is the AI tool that assists you with any coding task, right from your command line.&lt;/p&gt; &#xA;&lt;p&gt;Unlike Copilot, Mentat coordinates edits across multiple locations and files. And unlike ChatGPT, Mentat already has the context of your project - no copy and pasting required!&lt;/p&gt; &#xA;&lt;p&gt;Want help understanding a new codebase? Need to add a new feature? Refactor existing code? Mentat can do it!&lt;/p&gt; &#xA;&lt;h1&gt;üçø Example Videos (üîä on!)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/biobootloader/mentat/assets/128252497/35b027a9-d639-452c-a53c-ef019a645719&#34;&gt;https://github.com/biobootloader/mentat/assets/128252497/35b027a9-d639-452c-a53c-ef019a645719&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See more videos on &lt;a href=&#34;https://twitter.com/bio_bootloader/status/1683906735248125955&#34;&gt;Twitter&lt;/a&gt; or YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lODjaWclwpY&#34;&gt;Intro (2 min - same video as above)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qSyTWMFOjPs&#34;&gt;Explaining and editing Llama2.c (3 min)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YJLDIqq8k2A&#34;&gt;More Mentat features (4 min)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Setup&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bVJP8hY8uRM&#34;&gt;Installation and Setup Demonstration Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Before installing, it&#39;s suggested that you create a virtual environment to install it in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Python 3.10 or higher is required&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you&#39;ll have to have activated the virtual environment to run mentat if you install it there.&lt;/p&gt; &#xA;&lt;p&gt;There are then 3 install methods. The first two will just let you run it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyPI: &lt;code&gt;python -m pip install mentat-ai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Github: &lt;code&gt;python -m pip install git+https://github.com/biobootloader/mentat.git&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The third option is useful if you&#39;d also like to modify Mentat&#39;s code, as well as run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/biobootloader/mentat.git&#xA;cd mentat&#xA;&#xA;# install with pip in editable mode:&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Add your OpenAI API Key&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need to have API access to GPT-4 to run Mentat. There are a few options to provide Mentat with your OpenAI API key:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file with the line &lt;code&gt;OPENAI_API_KEY=&amp;lt;your-api-key&amp;gt;&lt;/code&gt; in the directory you plan to run mentat in or in &lt;code&gt;~/.mentat/.env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;export OPENAI_API_KEY=&amp;lt;your key here&amp;gt;&lt;/code&gt; prior to running Mentat&lt;/li&gt; &#xA; &lt;li&gt;Place the previous command in your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.zshrc&lt;/code&gt; to export your key on every terminal startup&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;For custom configuration options see &lt;a href=&#34;https://raw.githubusercontent.com/biobootloader/mentat/main/docs/configuration.md&#34;&gt;configuration.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ Usage&lt;/h1&gt; &#xA;&lt;p&gt;Run Mentat from within your project directory. Mentat uses git, so if your project doesn&#39;t already have git set up, run &lt;code&gt;git init&lt;/code&gt;. Then you can run Mentat with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;mentat &amp;lt;paths to files or directories&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you provide a directory, Mentat will add all non-hidden text files in that directory to it&#39;s context. If this exceeds the GPT-4 token context limit, try running Mentat with just the files you need it to see.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>llm-attacks/llm-attacks</title>
    <updated>2023-07-30T01:43:49Z</updated>
    <id>tag:github.com,2023-07-30:/llm-attacks/llm-attacks</id>
    <link href="https://github.com/llm-attacks/llm-attacks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Universal and Transferable Attacks on Aligned Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Attacks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository for &#34;&lt;a href=&#34;https://llm-attacks.org/zou2023universal.pdf&#34;&gt;Universal and Transferable Adversarial Attacks on Aligned Language Models&lt;/a&gt;&#34; by &lt;a href=&#34;https://andyzoujm.github.io/&#34;&gt;Andy Zou&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/west.cmu.edu/zifan-wang/home&#34;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&#34;https://zicokolter.com/&#34;&gt;J. Zico Kolter&lt;/a&gt;, and &lt;a href=&#34;https://www.cs.cmu.edu/~mfredrik/&#34;&gt;Matt Fredrikson&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://llm-attacks.org/&#34;&gt;website and demo here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#reproducibility&#34;&gt;Reproducibility&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We need FastChat to create conversations. At the current moment, we install it from &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;source&lt;/a&gt; by taking the following steps (we suggest to git clone FastChat outside the root of this repository).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lm-sys/FastChat.git&#xA;cd FastChat&#xA;pip3 install --upgrade pip  # enable PEP 660 support&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;llm-attacks&lt;/code&gt; package can be installed by running the following command at the root of this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the instructions to download Vicuna-7B or/and LLaMA-2-7B-Chat first (we use the weights converted by HuggingFace &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;here&lt;/a&gt;). Our script by default assumes models are stored in a root directory named as &lt;code&gt;/DIR&lt;/code&gt;. To modify the paths to your models and tokenizers, please add the following lines in &lt;code&gt;experiments/configs/individual_xxx.py&lt;/code&gt; (for individual experiment) and &lt;code&gt;experiments/configs/transfer_xxx.py&lt;/code&gt; (for multiple behaviors or transfer experiment). An example is given as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    config.model_paths = [&#xA;        &#34;/DIR/vicuna/vicuna-7b-v1.3&#34;,&#xA;        ... # more models&#xA;    ]&#xA;    config.tokenizer_paths = [&#xA;        &#34;/DIR/vicuna/vicuna-7b-v1.3&#34;,&#xA;        ... # more tokenizers&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Our code to run experiments with GCG is included &lt;code&gt;experiments&lt;/code&gt; folder. To run individual experiments with harmful behaviors and harmful strings mentioned in the paper, run the following code inside &lt;code&gt;experiments&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd launch_scripts&#xA;bash run_gcg_individual.sh vicuna behaviors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Changing &lt;code&gt;vicuna&lt;/code&gt; to &lt;code&gt;llama2&lt;/code&gt; and changing &lt;code&gt;behaviors&lt;/code&gt; to &lt;code&gt;strings&lt;/code&gt; will switch to different experiment setups.&lt;/p&gt; &#xA;&lt;p&gt;To perform multiple behaviors experiments (i.e. 25 behaviors, 1 model), run the following code inside &lt;code&gt;experiments&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd launch_scripts&#xA;bash run_gcg_multiple.sh vicuna # or llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To perform transfer experiments (i.e. 25 behaviors, 2 models), run the following code inside &lt;code&gt;experiments&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd launch_scripts&#xA;bash run_gcg_transfer.sh vicuna 2 # or vicuna_guanaco 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To perform evaluation experiments, please follow the directions in &lt;code&gt;experiments/parse_results.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Notice that all hyper-parameters in our experiments are handled by the &lt;code&gt;ml_collections&lt;/code&gt; package &lt;a href=&#34;https://github.com/google/ml_collections&#34;&gt;here&lt;/a&gt;. You can directly change those hyper-parameters at the place they are defined, e.g. &lt;code&gt;experiments/configs/individual_xxx.py&lt;/code&gt;. However, a recommended way of passing different hyper-parameters -- for instance you would like to try another model -- is to do it in the launch script. Check out our launch scripts in &lt;code&gt;experiments/launch_scripts&lt;/code&gt; for examples. For more information about &lt;code&gt;ml_collections&lt;/code&gt;, please refer to their &lt;a href=&#34;https://github.com/google/ml_collections&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducibility&lt;/h2&gt; &#xA;&lt;p&gt;We include a few examples people told us when reproducing our results. They might also include workaround for solving a similar issue in your situation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/llm-attacks/llm-attacks/issues/8&#34;&gt;Prompting Llama-2-7B-Chat-GGML&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zou2023universal,&#xA;      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, &#xA;      author={Andy Zou and Zifan Wang and and J. Zico Kolter and Matt Fredrikson},&#xA;      year={2023},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;llm-attacks&lt;/code&gt; is licensed under the terms of the MIT license. See LICENSE for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jiawen-zhu/HQTrack</title>
    <updated>2023-07-30T01:43:49Z</updated>
    <id>tag:github.com,2023-07-30:/jiawen-zhu/HQTrack</id>
    <link href="https://github.com/jiawen-zhu/HQTrack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tracking Anything in High Quality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tracking Anything in High Quality&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Technical Report&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2307.13974&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Report-arXiv:2307.13974-green&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img width=&#34;100%&#34; alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/bird-2.gif/&#34;&gt;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img width=&#34;100%&#34; alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/basketball.gif/&#34;&gt;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img width=&#34;100%&#34; alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/ants1.gif/&#34;&gt;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Tracking Anything in High Quality (HQTrack) is a framework for high performance video object tracking and segmentation. It mainly consists of a Video Multi-Object Segmenter (VMOS) and a Mask Refiner (MR), can track multiple target objects at the same time and output accurate object masks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üç∫&lt;/span&gt; &lt;strong&gt;HQTrack obtains runner-up in the Visual Object Tracking and Segmentaion (VOTS2023) challenge.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üì¢&lt;/span&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/7/30] We provide a demo code that can be run locally.&lt;/li&gt; &#xA; &lt;li&gt;[2023/7/22] We author a &lt;a href=&#34;https://arxiv.org/abs/2307.13974&#34;&gt;technical report&lt;/a&gt; for HQTrack.&lt;/li&gt; &#xA; &lt;li&gt;[2023/7/3] HQTrack ranks the 2nd place in the VOTS2023 challenge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî•&lt;/span&gt;Demo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img width=&#34;100%&#34; alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/point_prompt_demo.gif/&#34;&gt;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img width=&#34;100%&#34; alt=&#34;&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/box_prompt_demo.gif/&#34;&gt;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;We also provide a &lt;a href=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/demo/demo.py&#34;&gt;demo script&lt;/a&gt;, which supports box and point prompts as inputs. This is a pure python script that allows the user to test arbitrary videos.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üêç&lt;/span&gt;Pipeline&lt;/h2&gt; &#xA;&lt;img width=&#34;900&#34; alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/jiawen-zhu/HQTrack/main/assets/framework1.png&#34;&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìë&lt;/span&gt;Intallation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the conda environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n hqtrack python=3.8&#xA;conda activate hqtrack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Pytorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;conda install pytorch==1.9 torchvision cudatoolkit=10.2 -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install HQ-SAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd segment_anything_hq&#xA;pip install -e .&#xA;pip install opencv-python pycocotools matplotlib onnxruntime onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Pytorch-Correlation-extension package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd packages/Pytorch-Correlation-extension/&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install ops_dcnv3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd HQTrack/networks/encoders/ops_dcnv3&#xA;./make.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install vots toolkit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install vot-toolkit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install other packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install easydict&#xA;pip install lmdb&#xA;pip install einops&#xA;pip install jpeg4py&#xA;pip install &#39;protobuf~=3.19.0&#39;&#xA;conda install setuptools==58.0.4&#xA;pip install timm&#xA;pip install tb-nightly&#xA;pip install tensorboardx&#xA;pip install scikit-image&#xA;pip install rsa&#xA;pip install six&#xA;pip install pillow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöó&lt;/span&gt;Run HQTrack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model Preparation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Download VMOS model from &lt;a href=&#34;https://drive.google.com/drive/folders/1Hh10hLtz3YL_zE3PfBCiLB0tvj3Yvv26?usp=sharing&#34;&gt;Google Driver&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/120iNn5OEVrqDz0SKjEooig?pwd=vots&#34;&gt;Baidu Driver&lt;/a&gt; and put it under&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/path/to/HQTrack/result/default_InternT_MSDeAOTL_V2/YTB_DAV_VIP/ckpt/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://drive.google.com/file/d/1qobFYrI4eyIANfBSmYcGuWRaSIXfMOQ8/view?usp=sharing&#34;&gt;HQ-SAM_h&lt;/a&gt; and put it under&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/path/to/HQTrack/segment_anything_hq/pretrained_model/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initialize the vots workspace&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /path/to/VOTS23_workspace&#xA;vot initialize tests/multiobject&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy our trackers.ini to your vot workspace&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp /path/to/our/trackers.ini /path/to/VOTS23_workspace/trackers.ini&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify your path in trackers.ini&lt;/li&gt; &#xA; &lt;li&gt;test the tracker and pack the results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find HQTrack useful for you, please consider citing &lt;span&gt;üì£&lt;/span&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hqtrack,&#xA;      title={Tracking Anything in High Quality}, &#xA;      Author = {Jiawen Zhu and Zhenyu Chen and Zeqi Hao and Shijie Chang and Lu Zhang and Dong Wang and Huchuan Lu and Bin Luo and Jun-Yan He and Jin-Peng Lan and Hanyuan Chen and Chenyang Li},&#xA;      Title = {Tracking Anything in High Quality},&#xA;      Year = {2023},&#xA;      Eprint = {arXiv:2307.13974},&#xA;      PrimaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ô•&lt;/span&gt; Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;DeAOT&lt;/a&gt;, &lt;a href=&#34;https://github.com/SysCV/SAM-HQ&#34;&gt;HQ-SAM&lt;/a&gt;, and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;. Thanks for these excellent works.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìß&lt;/span&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, feel free to email &lt;a href=&#34;mailto:jiawen@mail.dlut.edu.cn&#34;&gt;jiawen@mail.dlut.edu.cn&lt;/a&gt;. ^_^&lt;/p&gt;</summary>
  </entry>
</feed>