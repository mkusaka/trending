<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-21T01:34:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiyouga/EasyR1</title>
    <updated>2025-06-21T01:34:37Z</updated>
    <id>tag:github.com,2025-06-21:/hiyouga/EasyR1</id>
    <link href="https://github.com/hiyouga/EasyR1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/EasyR1/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/EasyR1&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/llamafactory_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/llamafactory_ai&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Used by &lt;a href=&#34;https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/&#34;&gt;Amazon Web Services&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This project is a clean fork of the original &lt;a href=&#34;https://github.com/volcengine/verl&#34;&gt;veRL&lt;/a&gt; project to support vision language models, we thank all the authors for providing such a high-performance RL training framework.&lt;/p&gt; &#xA;&lt;p&gt;EasyR1 is efficient and scalable due to the design of &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256&#34;&gt;HybirdEngine&lt;/a&gt;&lt;/strong&gt; and the latest release of &lt;strong&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/strong&gt;&#39;s SPMD mode.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported models&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Llama3/Qwen2/Qwen2.5/Qwen3 language models&lt;/li&gt; &#xA;   &lt;li&gt;Qwen2/Qwen2.5-VL vision language models&lt;/li&gt; &#xA;   &lt;li&gt;DeepSeek-R1 distill models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported algorithms&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GRPO&lt;/li&gt; &#xA;   &lt;li&gt;DAPO&lt;/li&gt; &#xA;   &lt;li&gt;Reinforce++&lt;/li&gt; &#xA;   &lt;li&gt;ReMax&lt;/li&gt; &#xA;   &lt;li&gt;RLOO&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported datasets&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Any text, vision-text dataset in a &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/#custom-dataset&#34;&gt;specific format&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported tricks&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Padding-free training&lt;/li&gt; &#xA;   &lt;li&gt;Resuming from checkpoint&lt;/li&gt; &#xA;   &lt;li&gt;Wandb &amp;amp; SwanLab &amp;amp; Mlflow &amp;amp; Tensorboard tracking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Software Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9+&lt;/li&gt; &#xA; &lt;li&gt;transformers&amp;gt;=4.51.0&lt;/li&gt; &#xA; &lt;li&gt;flash-attn&amp;gt;=2.4.3&lt;/li&gt; &#xA; &lt;li&gt;vllm&amp;gt;=0.8.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; to easily build environments.&lt;/p&gt; &#xA;&lt;p&gt;We recommend using the &lt;a href=&#34;https://hub.docker.com/r/hiyouga/verl&#34;&gt;pre-built docker image&lt;/a&gt; in EasyR1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull hiyouga/verl:ngc-th2.7.0-cu12.6-vllm0.9.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;1.5B&lt;/th&gt; &#xA;   &lt;th&gt;3B&lt;/th&gt; &#xA;   &lt;th&gt;7B&lt;/th&gt; &#xA;   &lt;th&gt;32B&lt;/th&gt; &#xA;   &lt;th&gt;72B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPO Full Fine-Tuning&lt;/td&gt; &#xA;   &lt;td&gt;AMP&lt;/td&gt; &#xA;   &lt;td&gt;2*24GB&lt;/td&gt; &#xA;   &lt;td&gt;4*40GB&lt;/td&gt; &#xA;   &lt;td&gt;8*40GB&lt;/td&gt; &#xA;   &lt;td&gt;16*80GB&lt;/td&gt; &#xA;   &lt;td&gt;32*80GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPO Full Fine-Tuning&lt;/td&gt; &#xA;   &lt;td&gt;BF16&lt;/td&gt; &#xA;   &lt;td&gt;1*24GB&lt;/td&gt; &#xA;   &lt;td&gt;1*40GB&lt;/td&gt; &#xA;   &lt;td&gt;4*40GB&lt;/td&gt; &#xA;   &lt;td&gt;8*80GB&lt;/td&gt; &#xA;   &lt;td&gt;16*80GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Use &lt;code&gt;worker.actor.fsdp.torch_dtype=bf16&lt;/code&gt; and &lt;code&gt;worker.actor.optim.strategy=adamw_bf16&lt;/code&gt; to enable bf16 training.&lt;/p&gt; &#xA; &lt;p&gt;We are working hard to reduce the VRAM in RL training, LoRA support will be integrated in next updates.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Tutorial: Run Qwen2.5-VL GRPO on &lt;a href=&#34;https://huggingface.co/datasets/hiyouga/geometry3k&#34;&gt;Geometry3K&lt;/a&gt; Dataset in Just 3 Steps&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/assets/qwen2_5_vl_7b_geo.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/EasyR1.git&#xA;cd EasyR1&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GRPO Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash examples/qwen2_5_vl_7b_geo3k_grpo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Merge Checkpoint in Hugging Face Format&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 scripts/model_merger.py --local_dir checkpoints/easy_r1/exp_name/global_step_1/actor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you encounter issues with connecting to Hugging Face, consider using &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;If you want to use SwanLab logger, consider using &lt;code&gt;bash examples/qwen2_5_vl_7b_geo3k_swanlab.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Custom Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the example datasets to prepare your own dataset.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text dataset: &lt;a href=&#34;https://huggingface.co/datasets/hiyouga/math12k&#34;&gt;https://huggingface.co/datasets/hiyouga/math12k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image-text dataset: &lt;a href=&#34;https://huggingface.co/datasets/hiyouga/geometry3k&#34;&gt;https://huggingface.co/datasets/hiyouga/geometry3k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multi-image-text dataset: &lt;a href=&#34;https://huggingface.co/datasets/hiyouga/journeybench-multi-image-vqa&#34;&gt;https://huggingface.co/datasets/hiyouga/journeybench-multi-image-vqa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text-image mixed dataset: &lt;a href=&#34;https://huggingface.co/datasets/hiyouga/rl-mixed-dataset&#34;&gt;https://huggingface.co/datasets/hiyouga/rl-mixed-dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Understand GRPO in EasyR1&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/assets/easyr1_grpo.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To learn about the GRPO algorithm, you can refer to &lt;a href=&#34;https://huggingface.co/docs/trl/v0.16.1/en/grpo_trainer&#34;&gt;Hugging Face&#39;s blog&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Run 70B+ Model in Multi-node Environment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the Ray head node.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ray start --head --port=6379 --dashboard-host=0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start the Ray worker node and connect to the head node.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ray start --address=&amp;lt;head_node_ip&amp;gt;:6379&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Check the Ray resource pool.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ray status&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run training script on the Ray head node only.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash examples/qwen2_5_vl_7b_geo3k_grpo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;strong&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/multinode.html&#34;&gt;veRL&#39;s official doc&lt;/a&gt;&lt;/strong&gt; for more details about multi-node training and Ray debugger.&lt;/p&gt; &#xA;&lt;h2&gt;Other Baselines&lt;/h2&gt; &#xA;&lt;p&gt;We also reproduced the following two baselines of the &lt;a href=&#34;https://github.com/deep-agent/R1-V&#34;&gt;R1-V&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/examples/baselines/qwen2_5_vl_3b_clevr.sh&#34;&gt;CLEVR-70k-Counting&lt;/a&gt;: Train the Qwen2.5-VL-3B-Instruct model on counting problem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/examples/baselines/qwen2_5_vl_3b_geoqa8k.sh&#34;&gt;GeoQA-8k&lt;/a&gt;: Train the Qwen2.5-VL-3B-Instruct model on GeoQA problem.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Baselines&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/assets/baselines.md&#34;&gt;baselines.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Awesome Work using EasyR1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MMR1&lt;/strong&gt;: Advancing the Frontiers of Multimodal Reasoning. &lt;a href=&#34;https://github.com/LengSicong/MMR1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LengSicong/MMR1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vision-R1&lt;/strong&gt;: Incentivizing Reasoning Capability in Multimodal Large Language Models. &lt;a href=&#34;https://github.com/Osilly/Vision-R1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Osilly/Vision-R1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.06749&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2503.06749-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Seg-Zero&lt;/strong&gt;: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement. &lt;a href=&#34;https://github.com/dvlab-research/Seg-Zero&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dvlab-research/Seg-Zero&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.06520&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2503.06520-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MetaSpatial&lt;/strong&gt;: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse. &lt;a href=&#34;https://github.com/PzySeere/MetaSpatial&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PzySeere/MetaSpatial&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.18470&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2503.18470-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Temporal-R1&lt;/strong&gt;: Envolving Temporal Reasoning Capability into LMMs via Temporal Consistent Reward. &lt;a href=&#34;https://github.com/appletea233/Temporal-R1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/appletea233/Temporal-R1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;NoisyRollout&lt;/strong&gt;: Reinforcing Visual Reasoning with Data Augmentation. &lt;a href=&#34;https://github.com/John-AI-Lab/NoisyRollout&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/John-AI-Lab/NoisyRollout&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2504.13055&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2504.13055-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GUI-R1&lt;/strong&gt;: A Generalist R1-Style Vision-Language Action Model For GUI Agents. &lt;a href=&#34;https://github.com/ritzz-ai/GUI-R1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ritzz-ai/GUI-R1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2504.10458&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2504.10458-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;R1-Track&lt;/strong&gt;: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning. &lt;a href=&#34;https://github.com/Wangbiao2/R1-Track&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Wangbiao2/R1-Track&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VisionReasoner&lt;/strong&gt;: Unified Visual Perception and Reasoning via Reinforcement Learning. &lt;a href=&#34;https://github.com/dvlab-research/VisionReasoner&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dvlab-research/VisionReasoner&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2505.12081&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2505.12081-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MM-UPT&lt;/strong&gt;: Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO. &lt;a href=&#34;https://github.com/waltonfuture/MM-UPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/waltonfuture/MM-UPT&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2505.22453&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2505.22453-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RL-with-Cold-Start&lt;/strong&gt;: Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start. &lt;a href=&#34;https://github.com/waltonfuture/RL-with-Cold-Start&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/waltonfuture/RL-with-Cold-Start&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2505.22334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2505.22334-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ViGoRL&lt;/strong&gt;: Grounded Reinforcement Learning for Visual Reasoning. &lt;a href=&#34;https://github.com/Gabesarch/grounded-rl&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Gabesarch/grounded-rl&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2505.23678&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2505.22334-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Revisual-R1&lt;/strong&gt;: Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning. &lt;a href=&#34;https://github.com/CSfufu/Revisual-R1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CSfufu/Revisual-R1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2506.04207&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2506.04207-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SophiaVL-R1&lt;/strong&gt;: Reinforcing MLLMs Reasoning with Thinking Reward. &lt;a href=&#34;https://github.com/kxfan2002/SophiaVL-R1&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kxfan2002/SophiaVL-R1&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2505.17018&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2505.17018-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vision-Matters&lt;/strong&gt;: Simple Visual Perturbations Can Boost Multimodal Math Reasoning. &lt;a href=&#34;https://github.com/YutingLi0606/Vision-Matters&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YutingLi0606/Vision-Matters&#34; alt=&#34;[code]&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2506.09736&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-2506.09736-blue&#34; alt=&#34;[arxiv]&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support LoRA (high priority).&lt;/li&gt; &#xA; &lt;li&gt;Support ulysses parallelism for VLMs (middle priority).&lt;/li&gt; &#xA; &lt;li&gt;Support more VLM architectures.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We will not provide scripts for supervised fine-tuning and inference in this project. If you have such requirements, we recommend using &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;LLaMA-Factory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Known bugs&lt;/h3&gt; &#xA;&lt;p&gt;These features are temporarily disabled for now, we plan to fix them one-by-one in the future updates.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vision language models are not compatible with ulysses parallelism yet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Discussion Group&lt;/h2&gt; &#xA;&lt;p&gt;üëã Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/EasyR1/main/assets/wechat.jpg&#34;&gt;WeChat group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ValueError: Image features and image tokens do not match: tokens: 8192, features 9800&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Increase the &lt;code&gt;data.max_prompt_length&lt;/code&gt; or reduce the &lt;code&gt;data.max_pixels&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;RuntimeError: CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Reduce the &lt;code&gt;worker.rollout.gpu_memory_utilization&lt;/code&gt; and enable &lt;code&gt;worker.actor.offload.offload_params&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;RuntimeError: 0 active drivers ([]). There should only be one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Uninstall &lt;code&gt;deepspeed&lt;/code&gt; from the current python environment.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Core contributors: &lt;a href=&#34;https://github.com/hiyouga&#34;&gt;Yaowei Zheng&lt;/a&gt;, &lt;a href=&#34;https://github.com/AL-377&#34;&gt;Junting Lu&lt;/a&gt;, &lt;a href=&#34;https://github.com/Shenzhi-Wang&#34;&gt;Shenzhi Wang&lt;/a&gt;, &lt;a href=&#34;https://github.com/BUAADreamer&#34;&gt;Zhangchi Feng&lt;/a&gt;, &lt;a href=&#34;https://github.com/Kuangdd01&#34;&gt;Dongdong Kuang&lt;/a&gt; and Yuwen Xiong&lt;/p&gt; &#xA;&lt;p&gt;We also thank Guangming Sheng and Chi Zhang for helpful discussions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zheng2025easyr1,&#xA;  title        = {EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework},&#xA;  author       = {Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, Yuwen Xiong},&#xA;  howpublished = {\url{https://github.com/hiyouga/EasyR1}},&#xA;  year         = {2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend to also cite the original work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{sheng2024hybridflow,&#xA;  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},&#xA;  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},&#xA;  year    = {2024},&#xA;  journal = {arXiv preprint arXiv: 2409.19256}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lyuwenyu/RT-DETR</title>
    <updated>2025-06-21T01:34:37Z</updated>
    <id>tag:github.com,2025-06-21:/lyuwenyu/RT-DETR</id>
    <link href="https://github.com/lyuwenyu/RT-DETR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024] Official RT-DETR (RTDETR paddle pytorch), Real-Time DEtection TRansformer, DETRs Beat YOLOs on Real-time Object Detection. üî• üî• üî•&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;RT-DETR: DETRs Beat YOLOs on Real-time Object Detection&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/blob/main/LICENSE&#34;&gt;&#xA;        &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/LICENSE-Apache%202.0-blue&#34;&gt;&#xA;    &lt;/a&gt; --&gt; &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/github/license/lyuwenyu/RT-DETR&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/pulls&#34;&gt; &lt;img alt=&#34;prs&#34; src=&#34;https://img.shields.io/github/issues-pr/lyuwenyu/RT-DETR&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues&#34;&gt; &lt;img alt=&#34;issues&#34; src=&#34;https://img.shields.io/github/issues/lyuwenyu/RT-DETR?color=pink&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt; &lt;img alt=&#34;issues&#34; src=&#34;https://img.shields.io/github/stars/lyuwenyu/RT-DETR&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.08069&#34;&gt; &lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-2304.08069-red&#34;&gt; &lt;/a&gt; &lt;a href=&#34;mailto:%20lyuwenyu@foxmail.com&#34;&gt; &lt;img alt=&#34;emal&#34; src=&#34;https://img.shields.io/badge/contact_me-email-yellow&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This is the official implementation of papers&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.08069&#34;&gt;DETRs Beat YOLOs on Real-time Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.17140&#34;&gt;RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Fig&lt;/summary&gt; &#xA; &lt;table&gt;&#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://github.com/lyuwenyu/RT-DETR/assets/77494834/0ede1dc1-a854-43b6-9986-cf9090f11a61&#34; border=&#34;0&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/437877e9-1d4f-4d30-85e8-aafacfa0ec56&#34; border=&#34;0&#34; width=&#34;500&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üöÄ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.11.28] Add torch tool for parameters and flops statistics. see &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetrv2_pytorch/tools/run_profile.py&#34;&gt;run_profile.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024.10.10] Add sliced inference support for small object detecion. &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/pull/468&#34;&gt;#468&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024.09.23] Add ‚úÖ&lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetr_pytorch&#34;&gt;Regnet and DLA34&lt;/a&gt; for RTDETR.&lt;/li&gt; &#xA; &lt;li&gt;[2024.08.27] Add hubconf.py file to support torch hub.&lt;/li&gt; &#xA; &lt;li&gt;[2024.08.22] Improve the performance of ‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetrv2_pytorch/&#34;&gt;RT-DETRv2-S&lt;/a&gt; to 48.1 mAP (&lt;font color=&#34;green&#34;&gt;+1.6&lt;/font&gt; compared to RT-DETR-R18).&lt;/li&gt; &#xA; &lt;li&gt;[2024.07.24] Release ‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetrv2_pytorch/&#34;&gt;RT-DETRv2&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024.02.27] Our work has been accepted to CVPR 2024!&lt;/li&gt; &#xA; &lt;li&gt;[2024.01.23] Fix difference on data augmentation with paper in rtdetr_pytorch &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/commit/5dc64138e439247b4e707dd6cebfe19d8d77f5b1&#34;&gt;#84&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.11.07] Add pytorch ‚úÖ &lt;em&gt;rtdetr_r34vd&lt;/em&gt; for requests &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/107&#34;&gt;#107&lt;/a&gt;, &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/114&#34;&gt;#114&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.11.05] Upgrade the logic of &lt;code&gt;remap_mscoco_category&lt;/code&gt; to facilitate training of custom datasets, see detils in &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetr_pytorch/&#34;&gt;&lt;em&gt;Train custom data&lt;/em&gt;&lt;/a&gt; part. &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/commit/95fc522fd7cf26c64ffd2ad0c622c392d29a9ebf&#34;&gt;#81&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.23] Add &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/95&#34;&gt;&lt;em&gt;discussion for deployments&lt;/em&gt;&lt;/a&gt;, supported onnxruntime, TensorRT, openVINO.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.12] Add tuning code for pytorch version, now you can tuning rtdetr based on pretrained weights.&lt;/li&gt; &#xA; &lt;li&gt;[2023.09.19] Upload ‚úÖ &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/42&#34;&gt;&lt;em&gt;pytorch weights&lt;/em&gt;&lt;/a&gt; convert from paddle version.&lt;/li&gt; &#xA; &lt;li&gt;[2023.08.24] Release RT-DETR-R18 pretrained models on objects365. &lt;em&gt;49.2 mAP&lt;/em&gt; and &lt;em&gt;217 FPS&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.08.22] Upload ‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetr_pytorch/&#34;&gt;&lt;em&gt;rtdetr_pytorch&lt;/em&gt;&lt;/a&gt; source code. Please enjoy it!&lt;/li&gt; &#xA; &lt;li&gt;[2023.08.15] Release RT-DETR-R101 pretrained models on objects365. &lt;em&gt;56.2 mAP&lt;/em&gt; and &lt;em&gt;74 FPS&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.07.30] Release RT-DETR-R50 pretrained models on objects365. &lt;em&gt;55.3 mAP&lt;/em&gt; and &lt;em&gt;108 FPS&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.07.28] Fix some bugs, and add some comments. &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/pull/14&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/commit/3b5cbcf8ae3b907e6b8bb65498a6be7c6736eabc&#34;&gt;2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.07.13] Upload ‚úÖ &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/8&#34;&gt;&lt;em&gt;training logs on coco&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023.05.17] Release RT-DETR-R18, RT-DETR-R34, RT-DETR-R50-mÔºàexample for scaled).&lt;/li&gt; &#xA; &lt;li&gt;[2023.04.17] Release RT-DETR-R50, RT-DETR-R101, RT-DETR-L, RT-DETR-X.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì£ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RTDETR and RTDETRv2 are now available in Hugging Face Transformers. &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/413&#34;&gt;#413&lt;/a&gt;, &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/issues/549&#34;&gt;#549&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;RTDETR is now available in &lt;a href=&#34;https://docs.ultralytics.com/zh/models/rtdetr/&#34;&gt;ultralytics/ultralytics&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìç Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• RT-DETRv2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;paddle: &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetrv2_paddle/&#34;&gt;code&amp;amp;weight&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;pytorch: &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetrv2_pytorch/&#34;&gt;code&amp;amp;weight&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üî• RT-DETR &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;paddle: &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetr_paddle&#34;&gt;code&amp;amp;weight&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;pytorch: &lt;a href=&#34;https://raw.githubusercontent.com/lyuwenyu/RT-DETR/main/rtdetr_pytorch&#34;&gt;code&amp;amp;weight&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input shape&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;$AP^{val}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;$AP^{val}_{50}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params(M)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs(G)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;T4 TensorRT FP16(FPS)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;217&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;161&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R50-m&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;145&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;108&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R101&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;259&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-HGNetv2-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;114&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-HGNetv2-X&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO + Objects365&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;217&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO + Objects365&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;73.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;108&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RT-DETR-R101&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO + Objects365&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;74.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;259&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;74&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;RT-DETRv2-S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.1&lt;/strong&gt; &lt;font color=&#34;green&#34;&gt;(+1.6)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;217&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;RT-DETRv2-M&lt;/strong&gt;&lt;sup&gt;*&lt;sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.9&lt;/strong&gt; &lt;font color=&#34;green&#34;&gt;(+1.0)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;67.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;161&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;RT-DETRv2-M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.9&lt;/strong&gt; &lt;font color=&#34;green&#34;&gt;(+0.6)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;145&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;RT-DETRv2-L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.4&lt;/strong&gt; &lt;font color=&#34;green&#34;&gt;(+0.3)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;108&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;RT-DETRv2-X&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;72.8&lt;/strong&gt; &lt;font color=&#34;green&#34;&gt;(+0.1)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;259&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;COCO + Objects365&lt;/code&gt; in the table means finetuned model on COCO using pretrained weights trained on Objects365.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü¶Ñ Performance&lt;/h2&gt; &#xA;&lt;h3&gt;üèïÔ∏è Complex Scenarios&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/lyuwenyu/RT-DETR/assets/77494834/52743892-68c8-4e53-b782-9f89221739e4&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;üåã Difficult Conditions&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/lyuwenyu/RT-DETR/assets/77494834/213cf795-6da6-4261-8549-11947292d3cb&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;RT-DETR&lt;/code&gt; or &lt;code&gt;RTDETRv2&lt;/code&gt; in your work, please use the following BibTeX entries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lv2023detrs,&#xA;      title={DETRs Beat YOLOs on Real-time Object Detection},&#xA;      author={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},&#xA;      year={2023},&#xA;      eprint={2304.08069},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{lv2024rtdetrv2improvedbaselinebagoffreebies,&#xA;      title={RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer}, &#xA;      author={Wenyu Lv and Yian Zhao and Qinyao Chang and Kui Huang and Guanzhong Wang and Yi Liu},&#xA;      year={2024},&#xA;      eprint={2407.17140},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2407.17140}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tadata-org/fastapi_mcp</title>
    <updated>2025-06-21T01:34:37Z</updated>
    <id>tag:github.com,2025-06-21:/tadata-org/fastapi_mcp</id>
    <link href="https://github.com/tadata-org/fastapi_mcp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&#34; alt=&#34;fastapi-to-mcp&#34; height=&#34;100/&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;FastAPI-MCP&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/fastapi-mcp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/fastapi-mcp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white&#34; alt=&#34;FastAPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&#34; alt=&#34;fastapi-mcp-usage&#34; height=&#34;400&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI&#39;s ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hosted Solution&lt;/h2&gt; &#xA;&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href=&#34;https://tadata.com&#34;&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv add fastapi-mcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastapi-mcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI&#xA;from fastapi_mcp import FastApiMCP&#xA;&#xA;app = FastAPI()&#xA;&#xA;mcp = FastApiMCP(app)&#xA;&#xA;# Mount the MCP server directly to your FastAPI app&#xA;mcp.mount()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI-MCP provides &lt;a href=&#34;https://fastapi-mcp.tadata.com/&#34;&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples&#34;&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; &#xA;&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn&#39;t need to run separately from the MCP server (though &lt;a href=&#34;https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app&#34;&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; &#xA;&lt;h2&gt;Development and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; &#xA;&lt;p&gt;Before you get started, please see our &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg&#34;&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; &#xA; &lt;li&gt;uv&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</summary>
  </entry>
</feed>