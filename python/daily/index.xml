<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-04T01:40:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JayZeeDesign/research-agents-3.0</title>
    <updated>2023-12-04T01:40:20Z</updated>
    <id>tag:github.com,2023-12-04:/JayZeeDesign/research-agents-3.0</id>
    <link href="https://github.com/JayZeeDesign/research-agents-3.0" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Autogen + GPTs - build a swarm AI researchers&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>Vchitect/LaVie</title>
    <updated>2023-12-04T01:40:20Z</updated>
    <id>tag:github.com,2023-12-04:/Vchitect/LaVie</id>
    <link href="https://github.com/Vchitect/LaVie" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official PyTorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2309.15103&#34;&gt;LaVie&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LaVie&lt;/strong&gt; is a Text-to-Video (T2V) generation framework, and main part of video generation system &lt;a href=&#34;http://vchitect.intern-ai.org.cn/&#34;&gt;Vchitect&lt;/a&gt;. You can also check our fine-tuned Image-to-Video (I2V) model &lt;a href=&#34;https://github.com/Vchitect/SEINE&#34;&gt;SEINE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.15103&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2309.15103-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/LaVie-project/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Website-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/lavie&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/lavie/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vchitect/LaVie&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FVchitect%2FLaVie%2F&amp;amp;count_bg=%23368ED7&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=Visitors&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)]()&#xA;--&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/lavie.gif&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml &#xA;conda activate lavie&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Pre-Trained models&lt;/h2&gt; &#xA;&lt;p&gt;Download pre-trained &lt;a href=&#34;https://huggingface.co/YaohuiW/LaVie/tree/main&#34;&gt;LaVie models&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main&#34;&gt;Stable Diffusion 1.4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler/tree/main&#34;&gt;stable-diffusion-x4-upscaler&lt;/a&gt; to &lt;code&gt;./pretrained_models&lt;/code&gt;. You should be able to see the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── pretrained_models&#xA;│   ├── lavie_base.pt&#xA;│   ├── lavie_interpolation.pt&#xA;│   ├── lavie_vsr.pt&#xA;│   ├── stable-diffusion-v1-4&#xA;│   │   ├── ...&#xA;└── └── stable-diffusion-x4-upscaler&#xA;        ├── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Gallery:&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/two_teddy_bears_playing_poker_under_water,_highly_detailed,_oil_painting_style.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_teddy_bear_skating_under_water,_highly_detailed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_cat_reading_a_book_on_the_table,__Van_Gogh_style.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;two teddy bears playing poker under water, highly detailed, oil painting style&lt;/td&gt; &#xA;   &lt;td&gt;a teddy bear skateboarding under water, highly detailed&lt;/td&gt; &#xA;   &lt;td&gt;a cat reading a book on the table, Van Gogh style&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_cute_raccoon_playing_guitar_in_the_park_at_sunrise,_oil_painting_style.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_teddy_bear_walking_in_the_park_at_sunrise_oil_painting_style.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/laviea_teddy_bear_reading_a_book_near_a_small_river,_oil_painting_style-.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;a cute raccoon playing guitar in the park at sunrise, oil painting style&lt;/td&gt; &#xA;   &lt;td&gt;a teddy bear walking in the park at sunrise, oil painting style&lt;/td&gt; &#xA;   &lt;td&gt;a teddy bear reading a book near a small river, oil painting style&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/Elon_Musk_in_spacesuit_standing_besides_a_rocket,_high_quality05.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_teddy_bear_in_a__suit_having_dinner.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/Iron_Man_flying_in_the_sky.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Elon Musk in a space suit standing besides a rocket, high quality&lt;/td&gt; &#xA;   &lt;td&gt;a teddy bear in a suit having dinner in a well-decorated house&lt;/td&gt; &#xA;   &lt;td&gt;Iron Man flying in the sky, 4k, high quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to try different prompts, and share with us which one you like the most!&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;The inference contains &lt;strong&gt;Base T2V&lt;/strong&gt;, &lt;strong&gt;Video Interpolation&lt;/strong&gt; and &lt;strong&gt;Video Super-Resolution&lt;/strong&gt; three steps. We provide several options to generate videos:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Step1&lt;/th&gt; &#xA;   &lt;th&gt;Step2&lt;/th&gt; &#xA;   &lt;th&gt;Step3&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;Length&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;option1&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;320x512&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;option2&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;320x512&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;option3&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;1280x2048&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;option4&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;1280x2048&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to try different options :)&lt;/p&gt; &#xA;&lt;h3&gt;Step1. Base T2V&lt;/h3&gt; &#xA;&lt;p&gt;Run following command to generate videos from base T2V model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd base&#xA;python pipelines/sample.py --config configs/sample.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In &lt;strong&gt;configs/sample.yaml&lt;/strong&gt;, arguments for inference:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ckpt_path:&lt;/strong&gt; Path to the downloaded LaVie base model, default is &lt;code&gt;../pretrained_models/lavie_base.pt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;pretrained_models:&lt;/strong&gt; Path to the downloaded SD1.4, default is &lt;code&gt;../pretrained_models&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;output_folder:&lt;/strong&gt; Path to save generated results, default is &lt;code&gt;../res/base&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;seed:&lt;/strong&gt; Seed to be used, &lt;code&gt;None&lt;/code&gt; for random generation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;sample_method:&lt;/strong&gt; Scheduler to use, default is &lt;code&gt;ddpm&lt;/code&gt;, options are &lt;code&gt;ddpm&lt;/code&gt;, &lt;code&gt;ddim&lt;/code&gt; and &lt;code&gt;eulerdiscrete&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;guidance_scale:&lt;/strong&gt; CFG scale to use, default is &lt;code&gt;7.5&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;num_sampling_steps:&lt;/strong&gt; Denoising steps, default is &lt;code&gt;50&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;text_prompt:&lt;/strong&gt; Prompt for generation&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Following results can be reproduced with the arguments:&lt;/p&gt; &#xA;&lt;p&gt;seed: &lt;code&gt;400&lt;/code&gt;, sample_method: &lt;code&gt;ddpm&lt;/code&gt;, guidance_scale: &lt;code&gt;7.0&lt;/code&gt;, num_sampling_steps: &lt;code&gt;50&lt;/code&gt;&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_Corgi_walking_in_the_park_at_sunrise,_oil_painting_style.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_panda_taking_a_selfie,_2k,_high_quality.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_polar_bear_playing_drum_kit_in_NYC_Times_Square,_4k,_high_resolution.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;a Corgi walking in the park at sunrise, oil painting style&lt;/td&gt; &#xA;   &lt;td&gt;a panda taking a selfie, 2k, high quality&lt;/td&gt; &#xA;   &lt;td&gt;a polar bear playing drum kit in NYC Times Square, 4k, high resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_shark_swimming_in_clear_Carribean_ocean,_2k,_high_quality.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/a_teddy_bear_walking_on_the_street,_2k,_high_quality.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/jungle_river_at_sunset,_ultra_quality.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;a shark swimming in clear Carribean ocean, 2k, high quality&lt;/td&gt; &#xA;   &lt;td&gt;a teddy bear walking on the street, 2k, high quality&lt;/td&gt; &#xA;   &lt;td&gt;jungle, river, at sunset, ultra quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Step2 (optional). Video Interpolation&lt;/h3&gt; &#xA;&lt;p&gt;Run following command to conduct video interpolation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd interpolation&#xA;python sample.py --config configs/sample.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default input video path is &lt;code&gt;./res/base&lt;/code&gt;, results will be saved under &lt;code&gt;./res/interpolation&lt;/code&gt;. In &lt;code&gt;configs/sample.yaml&lt;/code&gt;, you could modify default &lt;code&gt;input_folder&lt;/code&gt; with &lt;code&gt;YOUR_INPUT_FOLDER&lt;/code&gt; in &lt;code&gt;configs/sample.yaml&lt;/code&gt;. Input videos should be named as &lt;code&gt;prompt1.mp4&lt;/code&gt;, &lt;code&gt;prompt2.mp4&lt;/code&gt;, ... and put under &lt;code&gt;YOUR_INPUT_FOLDER&lt;/code&gt;. Launching the code will process all the input videos in &lt;code&gt;input_folder&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/interpolation/a_teddy_bear_walking_on_the_street,_2k,_high_quality.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/interpolation/a_Corgi_walking_in_the_park_at_sunrise,_oil_painting_style.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vchitect/LaVie/main/assets/interpolation/a_panda_taking_a_selfie,_2k,_high_quality.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;a teddy bear walking on the street, 2k, high_quality&lt;/td&gt; &#xA;   &lt;td&gt;a Corgi walking in the park at sunrise, oil painting style&lt;/td&gt; &#xA;   &lt;td&gt;a panda taking a selfie, 2k, high quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Step3 (optional). Video Super-Resolution&lt;/h3&gt; &#xA;&lt;p&gt;Run following command to conduct video super-resolution.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd vsr&#xA;python sample.py --config configs/sample.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default input video path is &lt;code&gt;./res/base&lt;/code&gt; and results will be saved under &lt;code&gt;./res/vsr&lt;/code&gt;. You could modify default &lt;code&gt;input_path&lt;/code&gt; with &lt;code&gt;YOUR_INPUT_FOLDER&lt;/code&gt; in &lt;code&gt;configs/sample.yaml&lt;/code&gt;. Smiliar to Step2, input videos should be named as &lt;code&gt;prompt1.mp4&lt;/code&gt;, &lt;code&gt;prompt2.mp4&lt;/code&gt;, ... and put under &lt;code&gt;YOUR_INPUT_FOLDER&lt;/code&gt;. Launching the code will process all the input videos in &lt;code&gt;input_folder&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;BibTex&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2023lavie,&#xA;  title={LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models},&#xA;  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},&#xA;  journal={arXiv preprint arXiv:2309.15103},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chen2023seine,&#xA;title={SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction},&#xA;author={Chen, Xinyuan and Wang, Yaohui and Zhang, Lingjun and Zhuang, Shaobin and Ma, Xin and Yu, Jiashuo and Wang, Yali and Lin, Dahua and Qiao, Yu and Liu, Ziwei},&#xA;journal={arXiv preprint arXiv:2310.20700},&#xA;year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;We disclaim responsibility for user-generated content. The model was not trained to realistically represent people or events, so using it to generate such content is beyond the model&#39;s capabilities. It is prohibited for pornographic, violent and bloody content generation, and to generate content that is demeaning or harmful to people or their environment, culture, religion, etc. Users are solely liable for their actions. The project contributors are not legally affiliated with, nor accountable for users&#39; behaviors. Use the generative model responsibly, adhering to ethical and legal standards.&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Yaohui Wang&lt;/strong&gt;: &lt;a href=&#34;mailto:wangyaohui@pjlab.org.cn&#34;&gt;wangyaohui@pjlab.org.cn&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Xinyuan Chen&lt;/strong&gt;: &lt;a href=&#34;mailto:chenxinyuan@pjlab.org.cn&#34;&gt;chenxinyuan@pjlab.org.cn&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Xin Ma&lt;/strong&gt;: &lt;a href=&#34;mailto:xin.ma1@monash.edu&#34;&gt;xin.ma1@monash.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The code is built upon &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, we thank all the contributors for open-sourcing.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is licensed under Apache-2.0, model weights are fully open for academic research and also allow &lt;strong&gt;free&lt;/strong&gt; commercial usage. To apply for a commercial license, please contact &lt;a href=&#34;mailto:vchitect@pjlab.org.cn&#34;&gt;vchitect@pjlab.org.cn&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Avaiga/taipy</title>
    <updated>2023-12-04T01:40:20Z</updated>
    <id>tag:github.com,2023-12-04:/Avaiga/taipy</id>
    <link href="https://github.com/Avaiga/taipy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Turns Data and AI algorithms into production-ready web applications in no time.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/readme_logo.png&#34; alt=&#34;Taipy Logo&#34; width=&#34;20%&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Taipy -Your Web Application Builder. Pure Python.&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://pypi.python.org/pypi/taipy/&#34; alt=&#34;Taipy version&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/taipy.svg?label=pip&amp;amp;logo=PyPI&amp;amp;color=ff462b&amp;amp;labelColor=283282&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/taipy&#34; alt=&#34;Python version&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/pyversions/taipy?color=ff462b&amp;amp;labelColor=283282&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/@taipy8009&#34; alt=&#34;YouTube&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/youtube-click_to_watch_videos-red.svg?color=ff462b&amp;amp;labelColor=283282&amp;amp;logo=youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/Taipy_io&#34; alt=&#34;Twitter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/twitter-click_for_tweets-red.svg?color=ff462b&amp;amp;labelColor=283282&amp;amp;logo=twitter&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Taipy is an open-source Python library for building your web application front-end &amp;amp; back-end.&lt;/h3&gt; &#xA;&lt;h3&gt;Turns data and AI algorithms into production-ready web applications in no time.&lt;/h3&gt; &#xA;&lt;h3&gt;&#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.taipy.io/en/latest/&#34;&gt;Documentation&lt;/a&gt; • &#xA;  &lt;a href=&#34;https://discord.com/invite/SJyz2VJGxV&#34;&gt;Join our Discord&lt;/a&gt; • &#xA;  &lt;a href=&#34;https://docs.taipy.io/en/latest/knowledge_base/demos/&#34;&gt;Check out Taipy Applications&lt;/a&gt;&#xA; &lt;/div&gt;&lt;/h3&gt;  &#xA;&lt;br&gt; &#xA;&lt;h1&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   📊 We make both ends meet ⚙️ &#xA; &lt;/div&gt;&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;User Interface Generation&lt;/th&gt; &#xA;    &lt;th&gt;Scenario and Data Management&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/gui_creation.webp&#34; alt=&#34;Interface Animation&#34; width=&#34;850px&#34; height=&#34;250px&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/scenario_and_data_mgt.gif&#34; alt=&#34;Back-End Animation&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install taipy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;You&#39;re all set! All aboard the Taipy journey 🚂&lt;/em&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/XcFhrJZru3&#34;&gt;Discord&lt;/a&gt; to give us feedback, share your creations or just to have a chat with us.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Ready, Set, GUI&lt;/h2&gt; &#xA;&lt;h3&gt;Tiny User Interface Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from taipy import Gui&#xA;&#xA;excitement_page = &#34;&#34;&#34;&#xA;# Welcome to Taipy&#xA;### How excited are you to try Taipy?&#xA;&#xA;&amp;lt;|{excitement}|slider|min=1|max=100|&amp;gt;&#xA;&#xA;My excitement level: &amp;lt;|{excitement}|&amp;gt;&#xA;&#34;&#34;&#34;&#xA;excitement = 100&#xA;&#xA;Gui(page=excitement_page).run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;RUN&lt;/em&gt;🏃🏽‍♀️&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; 🎊 TA-DA! 🎊&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/tiny_demo_readme.gif&#34; width=&#34;50%&#34; alt=&#34;Tiny Demo&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;em&gt;&lt;strong&gt;Check out our &lt;a href=&#34;https://docs.taipy.io/en/latest/getting_started/&#34;&gt;Getting Started&lt;/a&gt; and &lt;a href=&#34;https://docs.taipy.io/en/latest/manuals/gui/&#34;&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Scenario and Data Management ⚙️&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;left&#34;&gt;&#xA; &lt;strong&gt;Let&#39;s create a &lt;em&gt;Scenario&lt;/em&gt; in Taipy to filter movie data based on the genre you choose. This scenario models a simple pipeline. It is submitted (for execution) each time the genre selection changes and outputs the seven most popular movies of that genre. &lt;/strong&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  ⚠️ Here, the back-end involves the execution of a very simple pipeline (made of a single task). Note that Taipy is designed to build much more complex pipelines 🚀 (with many tasks!) &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;Here is our filter function: a standard Python function that is used by the unique task in the scenario&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def filter_genre(initial_dataset: pd.DataFrame, selected_genre):&#xA;    filtered_dataset = initial_dataset[initial_dataset[&#39;genres&#39;].str.contains(selected_genre)]&#xA;    filtered_data = filtered_dataset.nlargest(7, &#39;Popularity %&#39;)&#xA;    return filtered_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;This is the execution graph of the scenario we are implementing&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/readme_exec_graph.png&#34; alt=&#34;Demo Execution Graph&#34; width=&#34;50%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Taipy Studio - The easy peasy way&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;You can use the Taipy Studio extension in Visual Studio Code to configure your scenario with no code&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/readme_demo_studio.gif&#34; width=&#34;80%&#34; alt=&#34;Demo Studio Gif&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;Your configuration is automatically saved as a TOML file&lt;/em&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;em&gt;&lt;strong&gt;Check out our &lt;a href=&#34;https://docs.taipy.io/en/latest/manuals/studio/&#34;&gt;Documentation&lt;/a&gt; &lt;/strong&gt;&lt;/em&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Taipy Scenario &amp;amp; Data Management - a walk on the code side&lt;/h3&gt; &#xA;&lt;div align=&#34;left&#34;&gt;&#xA; For more advanced use cases or if you prefer coding your configurations instead of using Taipy Studio, Taipy has your back! &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;left&#34;&gt;&#xA; &lt;em&gt;Check out the movie genre demo scenario creation with this &lt;a href=&#34;https://docs.taipy.io/en/latest/knowledge_base/demos/movie_genre_selector/&#34;&gt;Demo&lt;/a&gt; &lt;/em&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;em&gt;&lt;strong&gt;Check out our &lt;a href=&#34;https://docs.taipy.io/en/latest/getting_started/&#34;&gt;Getting Started&lt;/a&gt; and &lt;a href=&#34;https://docs.taipy.io/en/latest/manuals/core/&#34;&gt;Documentation&lt;/a&gt; &lt;/strong&gt;&lt;/em&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;User Interface Generation ➕ Scenario &amp;amp; Data Management&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Now, let&#39;s load this configuration and add a user interface on top for a 🎉FULL APPLICATION🎉&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import taipy as tp&#xA;import pandas as pd&#xA;from taipy import Config, Scope, Gui&#xA;&#xA;# Taipy Scenario &amp;amp; Data Management&#xA;&#xA;# Filtering function - task&#xA;def filter_genre(initial_dataset: pd.DataFrame, selected_genre):&#xA;    filtered_dataset = initial_dataset[initial_dataset[&#34;genres&#34;].str.contains(selected_genre)]&#xA;    filtered_data = filtered_dataset.nlargest(7, &#34;Popularity %&#34;)&#xA;    return filtered_data&#xA;&#xA;# Load the configuration made with Taipy Studio&#xA;Config.load(&#34;config.toml&#34;)&#xA;scenario_cfg = Config.scenarios[&#34;scenario&#34;]&#xA;&#xA;# Start Taipy Core service&#xA;tp.Core().run()&#xA;&#xA;# Create a scenario&#xA;scenario = tp.create_scenario(scenario_cfg)&#xA;&#xA;&#xA;# Taipy User Interface&#xA;# Let&#39;s add a GUI to our Scenario Management for a full application&#xA;&#xA;# Callback definition - submits scenario with genre selection&#xA;def on_genre_selected(state):&#xA;    scenario.selected_genre_node.write(state.selected_genre)&#xA;    tp.submit(scenario)&#xA;    state.df = scenario.filtered_data.read()&#xA;&#xA;# Get list of genres&#xA;genres = [&#xA;    &#34;Action&#34;, &#34;Adventure&#34;, &#34;Animation&#34;, &#34;Children&#34;, &#34;Comedy&#34;, &#34;Fantasy&#34;, &#34;IMAX&#34;&#xA;    &#34;Romance&#34;,&#34;Sci-FI&#34;, &#34;Western&#34;, &#34;Crime&#34;, &#34;Mystery&#34;, &#34;Drama&#34;, &#34;Horror&#34;, &#34;Thriller&#34;, &#34;Film-Noir&#34;,&#34;War&#34;, &#34;Musical&#34;, &#34;Documentary&#34;&#xA;    ]&#xA;&#xA;# Initialization of variables&#xA;df = pd.DataFrame(columns=[&#34;Title&#34;, &#34;Popularity %&#34;])&#xA;selected_genre = &#34;Action&#34;&#xA;&#xA;## Set initial value to Action&#xA;def on_init(state):&#xA;    on_genre_selected(state)&#xA;&#xA;# User interface definition&#xA;my_page = &#34;&#34;&#34;&#xA;# Film recommendation&#xA;&#xA;## Choose your favorite genre&#xA;&amp;lt;|{selected_genre}|selector|lov={genres}|on_change=on_genre_selected|dropdown|&amp;gt;&#xA;&#xA;## Here are the top seven picks by popularity&#xA;&amp;lt;|{df}|chart|x=Title|y=Popularity %|type=bar|title=Film Popularity|&amp;gt;&#xA;&#34;&#34;&#34;&#xA;&#xA;Gui(page=my_page).run()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;RUN&lt;/em&gt;🏃🏽‍♀️&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; 🎊TA-DA!🎊&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/readme_app.gif&#34; width=&#34;80%&#34; alt=&#34;Image of a Taipy demonstration application&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;☁️Taipy Cloud☁️&lt;/h2&gt; &#xA;&lt;p&gt;With Taipy Cloud, you can deploy your Taipy applications in a &lt;em&gt;few clicks&lt;/em&gt; and &lt;em&gt;for free&lt;/em&gt;!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/readme_img/readme_cloud_demo.gif&#34; alt=&#34;Demonstration of Taipy Cloud&#34; width=&#34;60%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;em&gt;&lt;strong&gt; Click &lt;a href=&#34;https://www.taipy.io/taipy-cloud/&#34;&gt;here&lt;/a&gt; to get started for free &lt;/strong&gt;&lt;/em&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Contributing ⚒⚒&lt;/h2&gt; &#xA;&lt;p&gt;Want to help build &lt;em&gt;Taipy&lt;/em&gt;? Check out our &lt;a href=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Code of conduct&lt;/h2&gt; &#xA;&lt;p&gt;Want to be part of the &lt;em&gt;Taipy&lt;/em&gt; community? Check out our &lt;a href=&#34;https://raw.githubusercontent.com/Avaiga/taipy/develop/CODE_OF_CONDUCT.md&#34;&gt;&lt;code&gt;CODE_OF_CONDUCT.md&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2023 Avaiga Private Limited&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.txt&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
</feed>