<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-26T01:41:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hylarucoder/svd-webui</title>
    <updated>2023-11-26T01:41:35Z</updated>
    <id>tag:github.com,2023-11-26:/hylarucoder/svd-webui</id>
    <link href="https://github.com/hylarucoder/svd-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;svd-webui&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can also use ComfyUI for only 8 vram try my workflow &lt;a href=&#34;https://github.com/hylarucoder/comfyui-workflow/raw/main/svd/svd-image-to-video.json&#34;&gt;https://github.com/hylarucoder/comfyui-workflow/blob/main/svd/svd-image-to-video.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hylarucoder/svd-webui&#xA;cd svd-webui&#xA;py3.10 -m venv venv&#xA;# linux/macOS&#xA;source ./venv/bin/activate&#xA;# windows&#xA;venv\Scripts\activate.bat&#xA;pip install pdm &#xA;pdm install&#xA;# linux/windows&#xA;python -m pip install torch==2.1.0 torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121&#xA;python -m pip install onnxruntime-gpu&#xA;# macos&#xA;python -m pip install torch torchvision torchaudio &#xA;python -m pip install onnxruntime-silicon&#xA;&#xA;# &#xA;python -m pip install pytorch_lightning torchdata webdataset transformers kornia open-clip-torch&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Online GPU&lt;/h2&gt; &#xA;&lt;h3&gt;AutoDL&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/hylarucoder/status/1727922785828417805&#34;&gt;https://twitter.com/hylarucoder/status/1727922785828417805&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hylarucoder/svd-webui/raw/main/svd_webui_colab.ipynb&#34;&gt;https://github.com/hylarucoder/svd-webui/blob/main/svd_webui_colab.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;https://github.com/Stability-AI/generative-models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>sh-lee-prml/HierSpeechpp</title>
    <updated>2023-11-26T01:41:35Z</updated>
    <id>tag:github.com,2023-11-26:/sh-lee-prml/HierSpeechpp</id>
    <link href="https://github.com/sh-lee-prml/HierSpeechpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official implementation of HierSpeech++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation by Hierarchical Variational Inference for Zero-shot Speech Synthesis &lt;br&gt;&lt;sub&gt;The official implementation of HierSpeech++&lt;/sub&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a src=&#34;https://img.shields.io/badge/cs.CV-2311.12454-b31b1b?logo=arxiv&amp;amp;logoColor=red&#34; href=&#34;http://arxiv.org/abs/2311.12454&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/cs.CV-2311.12454-b31b1b?logo=arxiv&amp;amp;logoColor=red&#34;&gt;&lt;/a&gt;|&lt;a href=&#34;https://huggingface.co/spaces/HierSpeech/HierSpeech_TTS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;|&lt;a href=&#34;https://sh-lee-prml.github.io/HierSpeechpp-demo/&#34;&gt;Demo page&lt;/a&gt;|&lt;a href=&#34;https://drive.google.com/drive/folders/1-L_90BlCkbPyKWWHTUjt5Fsu3kz0du0w?usp=sharing&#34;&gt;Checkpoint&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Department of Artificial Intelligence, Korea University, Seoul, Korea&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Large language models (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness. This paper proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and voice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and expressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech even in zero-shot speech synthesis scenarios. For text-to-speech, we adopt the text-to-vec framework, which generates a self-supervised speech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates speech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech synthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality zero-shot speech synthesis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/sh-lee-prml/HierSpeechpp/assets/56749640/8f0b5f24-8491-4908-ae06-e0dfcc7d9e52&#34; alt=&#34;Fig1_pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü™ê A PyTorch implementation of HierSpeech++ (TTV, Hierarchical Speech Synthesizer, SpeechSR)&lt;/li&gt; &#xA; &lt;li&gt;‚ö°Ô∏è Pre-trained HierSpeech++ models trained on LibriTTS (Train-460, Train-960, and more dataset)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/HierSpeech/HierSpeech_TTS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; Gradio Demo on HuggingFace. HuggingFace provides us with a community GPU grant. Thanks üòä&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;- üí• A Colab notebook for running pre-trained HierSpeech++ models (Soon..)&#xA;üõ∏ A HierSpeech++ training script (Will be released soon)&#xA;--&gt; &#xA;&lt;h2&gt;Previous Our Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[NeurIPS2022] HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis&lt;/li&gt; &#xA; &lt;li&gt;[Interspeech2023] HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This paper is an extension version of above papers.&lt;/p&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;h3&gt;Hierarchical Speech Synthesizer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; HierSpeechpp-Backbone (LibriTTS-train-460)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; HierSpeechpp-Backbone (LibriTTS-train-960)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; HierSpeechpp-Backbone-60epoch (LibriTTS-train-960, Libri-light (Medium), Expresso, MSSS(Kor), NIKL(Kor))&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; HierSpeechpp-Backbone-200epoch (LibriTTS-train-960, Libri-light (Medium), Expresso, MSSS(Kor), NIKL(Kor))&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;- [ ] HierSpeech-Lite (Fast and Efficient Zero-shot Speech Synthesizer)&#xA;- [ ] HierSinger (Zero-shot Singing Voice Synthesizer)&#xA;- [ ] HierSpeech2-24k-Large-Full (For High-resolutional and High-quality Speech Synthesizer)&#xA;- [ ] HierSpeech2-48k-Large-Full (For Industrial-level High-resolution and High-quality Speech Synthesizer)&#xA;--&gt; &#xA;&lt;h3&gt;Text-to-Vec (TTV)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; TTV-v1 (LibriTTS-train-960)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TTV-v2 (We are currently training a multi-lingual TTV model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--&#xA;- [ ] Hierarchical Text-to-Vec (For Much More Expressive Text-to-Speech)&#xA;--&gt; &#xA;&lt;h3&gt;Speech Super-resolution (16k --&amp;gt; 24k or 48k)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SpeechSR-24k&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SpeechSR-48k&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cleaning Up the Source Code&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Clean Code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training code (Will be released after paper acceptance)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TTV&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Hierarchical Speech Synthesizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SpeechSR&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-requisites&lt;/h3&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Pytorch &amp;gt;=1.13 and torchaudio &amp;gt;= 0.13&lt;/li&gt; &#xA; &lt;li&gt;Install requirements&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Phonemizer&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install phonemizer&#xA;sudo apt-get install espeak-ng&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Checkpoint &lt;a href=&#34;https://drive.google.com/drive/folders/1-L_90BlCkbPyKWWHTUjt5Fsu3kz0du0w?usp=sharing&#34;&gt;[Download]&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Hierarchical Speech Synthesizer&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sampling Rate&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hour&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speaker&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HierSpeech2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16 kHz&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LibriTTS (train-460)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;245&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1,151&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/14FTu0ZWux0zAD7ev4O1l6lKslQcdmebL?usp=sharing&#34;&gt;[Download]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HierSpeech2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16 kHz&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LibriTTS (train-960)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;555&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2,311&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1sFQP-8iS8z9ofCkE7szXNM_JEy4nKg41?usp=drive_link&#34;&gt;[Download]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HierSpeech2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16 kHz&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LibriTTS (train-960), Libri-light (Small, Medium), Expresso, MSSS(Kor), NIKL(Kor)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2,796&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7,299&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/14jaDUBgrjVA7bCODJqAEirDwRlvJe272?usp=drive_link&#34;&gt;[Download]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;| HierSpeech2-Lite|16 kHz|-| LibriTTS (train-960))  |-|&#xA;| HierSpeech2-Lite|16 kHz|-| LibriTTS (train-960) NIKL, AudioBook-Korean)  |-|&#xA;| HierSpeech2-Large-CL|16 kHz|200M| LibriTTS (train-960), Libri-Light, NIKL, AudioBook-Korean, Japanese, Chinese, CSS, MLS)  |-|&#xA;--&gt; &#xA;&lt;h3&gt;TTV&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Language&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hour&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speaker&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TTV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Eng&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;107M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LibriTTS (train-960)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;555&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2,311&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1QiFFdPhqhiLFo8VXc0x7cFHKXArx7Xza?usp=drive_link&#34;&gt;[Download]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;| TTV |Kor|100M| NIKL |114|118|-|&#xA;| TTV |Eng|50M| LibriTTS (train-960) |555|2,311|-|&#xA;| TTV-Large |Eng|100M| LibriTTS (train-960) |555|2,311|-|&#xA;| TTV-Lite |Eng|10M| LibriTTS (train-960) |555|2,311|-|&#xA;| TTV |Kor|50M| NIKL |114|118|-|&#xA;--&gt; &#xA;&lt;h3&gt;SpeechSR&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sampling Rate&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeechSR-24k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16kHz --&amp;gt; 24 kHz&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LibriTTS (train-960), MSSS (Kor)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp/raw/main/speechsr24k/G_340000.pth&#34;&gt;speechsr24k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeechSR-48k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16kHz --&amp;gt; 48 kHz&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSSS (Kor), Expresso (Eng), VCTK (Eng)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp/raw/main/speechsr48k/G_100000.pth&#34;&gt;speechsr48k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Text-to-Speech&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh inference.sh&#xA;&#xA;# --ckpt &#34;logs/hierspeechpp_libritts460/hierspeechpp_lt460_ckpt.pth&#34; \ LibriTTS-460&#xA;# --ckpt &#34;logs/hierspeechpp_libritts960/hierspeechpp_lt960_ckpt.pth&#34; \ LibriTTS-960&#xA;# --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1_ckpt.pth&#34; \ Large_v1 epoch 60 (paper version)&#xA;# --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1.1_ckpt.pth&#34; \ Large_v1.1 epoch 200 (20. Nov. 2023)&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python3 inference.py \&#xA;                --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1.1_ckpt.pth&#34; \&#xA;                --ckpt_text2w2v &#34;logs/ttv_libritts_v1/ttv_lt960_ckpt.pth&#34; \&#xA;                --output_dir &#34;tts_results_eng_kor_v2&#34; \&#xA;                --noise_scale_vc &#34;0.333&#34; \&#xA;                --noise_scale_ttv &#34;0.333&#34; \&#xA;                --denoise_ratio &#34;0&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For better robustness, we recommend a noise_scale of 0.333&lt;/li&gt; &#xA; &lt;li&gt;For better expressiveness, we recommend a noise_scale of 0.667&lt;/li&gt; &#xA; &lt;li&gt;Find your best parameters for your style prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Noise Control&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# without denoiser&#xA;--denoise_ratio &#34;0&#34;&#xA;# with denoiser&#xA;--denoise_ratio &#34;1&#34;&#xA;# Mixup (Recommend 0.6~0.8)&#xA;--denoise_rate &#34;0.8&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Voice Conversion&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This method only utilize a hierarchical speech synthesizer for voice conversion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh inference_vc.sh&#xA;&#xA;# --ckpt &#34;logs/hierspeechpp_libritts460/hierspeechpp_lt460_ckpt.pth&#34; \ LibriTTS-460&#xA;# --ckpt &#34;logs/hierspeechpp_libritts960/hierspeechpp_lt960_ckpt.pth&#34; \ LibriTTS-960&#xA;# --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1_ckpt.pth&#34; \ Large_v1 epoch 60 (paper version)&#xA;# --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1.1_ckpt.pth&#34; \ Large_v1.1 epoch 200 (20. Nov. 2023)&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python3 inference_vc.py \&#xA;                --ckpt &#34;logs/hierspeechpp_eng_kor/hierspeechpp_v1.1_ckpt.pth&#34; \&#xA;                --output_dir &#34;vc_results_eng_kor_v2&#34; \&#xA;                --noise_scale_vc &#34;0.333&#34; \&#xA;                --noise_scale_ttv &#34;0.333&#34; \&#xA;                --denoise_ratio &#34;0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For better robustness, we recommend a noise_scale of 0.333&lt;/li&gt; &#xA; &lt;li&gt;For better expressiveness, we recommend a noise_scale of 0.667&lt;/li&gt; &#xA; &lt;li&gt;Find your best parameters for your style prompt&lt;/li&gt; &#xA; &lt;li&gt;Voice Conversion is vulnerable to noisy target prompt so we recommend to utilize a denoiser with noisy prompt&lt;/li&gt; &#xA; &lt;li&gt;For noisy source speech, a wrong F0 may be extracted by YAPPT resulting in a quality degradation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Speech Super-resolution&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpeechSR-24k and SpeechSR-48 are provided in TTS pipeline. If you want to use SpeechSR only, please refer inference_speechsr.py.&lt;/li&gt; &#xA; &lt;li&gt;If you change the output resolution, add this&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;--output_sr &#34;48000&#34; # Default&#xA;--output_sr &#34;24000&#34; # &#xA;--output_sr &#34;16000&#34; # without super-resolution.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speech Denoising for Noise-free Speech Synthesis (Only used in Speaker Encoder during Inference)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For denoised style prompt, we utilize a denoiser &lt;a href=&#34;https://github.com/yxlu-0102/MP-SENet&#34;&gt;(MP-SENet)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;When using a long reference audio, there is an out-of-memory issue with this model so we have a plan to learn a memory efficient speech denoiser in the future.&lt;/li&gt; &#xA; &lt;li&gt;If you have a problem, we recommend to use a clean reference audio or denoised audio before TTS pipeline or denoise the audio with cpu (but this will be slowüò•).&lt;/li&gt; &#xA; &lt;li&gt;(21, Nov. 2023) Sliced window denoising. This may reduce a burden for denoising a speech. &lt;pre&gt;&lt;code&gt;      if denoise == 0:&#xA;          audio = torch.cat([audio.cuda(), audio.cuda()], dim=0)&#xA;      else:&#xA;          with torch.no_grad():&#xA;              &#xA;              if ori_prompt_len &amp;gt; 80000:&#xA;                  denoised_audio = []&#xA;                  for i in range((ori_prompt_len//80000)):&#xA;                      denoised_audio.append(denoise(audio.squeeze(0).cuda()[i*80000:(i+1)*80000], denoiser, hps_denoiser))&#xA;                  &#xA;                  denoised_audio.append(denoise(audio.squeeze(0).cuda()[(i+1)*80000:], denoiser, hps_denoiser))&#xA;                  denoised_audio = torch.cat(denoised_audio, dim=1)&#xA;              else:&#xA;                  denoised_audio = denoise(audio.squeeze(0).cuda(), denoiser, hps_denoiser)&#xA;&#xA;          audio = torch.cat([audio.cuda(), denoised_audio[:,:audio.shape[-1]]], dim=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TTV-v2 (WIP)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TTV-v1 is a simple model which is very slightly modified from VITS. Although this simple TTV could synthesize a speech with high-quality and high speaker similarity, we thought that there is room for improvement in terms of expressiveness such as prosody modeling.&lt;/li&gt; &#xA; &lt;li&gt;For TTV-v2, we modify some components and training process (Model size: 107M --&amp;gt; 278M) &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Intermediate hidden size: 256 --&amp;gt; 384&lt;/li&gt; &#xA;   &lt;li&gt;Loss masking for wav2vec reconstruction loss (I left out masking the loss for zero-padding sequencesüò•)&lt;/li&gt; &#xA;   &lt;li&gt;For long sentence generation, we finetune the model with full LibriTTS-train dataset without data filtering (Decrease the learning rate to 2e-5 with batch size of 8 per gpus)&lt;/li&gt; &#xA;   &lt;li&gt;Multi-lingual Dataset (We are training the model with Eng, Indic, and Kor dataset now)&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GAN VS Diffusion&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; [Read More] &lt;/summary&gt; We think that we could not confirm which is better yet. There are many advatanges for each model so you can utilize each model for your own purposes and each study must be actively conducted simultaneously. &#xA; &lt;h3&gt;GAN (Specifically, GAN-based End-to-End Speech Synthesis Models)&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;(pros) Fast Inference Speed&lt;/li&gt; &#xA;  &lt;li&gt;(pros) High-quality Audio&lt;/li&gt; &#xA;  &lt;li&gt;(cons) Slow Training Speed (Over 7~20 Days)&lt;/li&gt; &#xA;  &lt;li&gt;(cons) Lower Voice Style Transfer Performance than Diffusion Models&lt;/li&gt; &#xA;  &lt;li&gt;(cons) Perceptually High-quality but Over-smoothed Audio because of Information Bottleneck by the sampling from the low-dimensional Latent Variable&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Diffusion (Diffusion-based Mel-spectrogram Generation Models)&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;(pros) Fast Training Speed (within 3 Days)&lt;/li&gt; &#xA;  &lt;li&gt;(pros) High-quality Voice Style Transfer&lt;/li&gt; &#xA;  &lt;li&gt;(cons) Slow Inference Speed&lt;/li&gt; &#xA;  &lt;li&gt;(cons) Lower Audio quality than End-to-End Speech Synthesis Models&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;(In this wors) Our Approaches for GAN-based End-to-End Speech Synthesis Models&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Improving Voice Style Transfer Performance in End-to-End Speech Synthesis Models for OOD (Zero-shot Voice Style Transfer for Novel Speaker)&lt;/li&gt; &#xA;  &lt;li&gt;Improving the Audio Quality beyond Perceptal Quality for Much more High-fidelity Audio Generation&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;(Our other works) Diffusion-based Mel-spectrogram Generation Models&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;DDDM-VC: Disentangled Denoising Diffusion Models for High-quality and High-diversity Speech Synthesis Models&lt;/li&gt; &#xA;  &lt;li&gt;Diff-hierVC: Hierarhical Diffusion-based Speech Synthesis Model with Diffusion-based Pitch Modeling&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Our Goals&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Integrating each model for High-quality, High-diversity and High-fidelity Speech Synthesis Models&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLM-based Models&lt;/h2&gt; &#xA;&lt;p&gt;We hope to compare LLM-based models for zero-shot TTS baselines. However, there is no public-available official implementation of LLM-based TTS models. Unfortunately, unofficial models have a poor performance in zero-shot TTS so we hope they will release their model for a fair comparison and reproducibility and for our speech community. THB I could not stand the inference speed almost 1,000 times slower than e2e models It takes 5 days to synthesize the full sentences of LibriTTS-test subsets. Even, the audio quality is so bad. I hope they will release their official source code soon.&lt;/p&gt; &#xA;&lt;p&gt;In my very personal opinion, VITS is still the best TTS model I have ever seen. But, I acknowledge that LLM-based models have much powerful potential for their creative generative performance from the large-scale dataset but not now.&lt;/p&gt; &#xA;&lt;h2&gt;Limitation of our work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Slow training speed and Relatively large model size (Compared with VITS) --&amp;gt; Future work: Light-weight and Fast training pipeline and much larger model...&lt;/li&gt; &#xA; &lt;li&gt;Could not generate realistic background sound --&amp;gt; Future work: adding audio generation part by disentangling speech and sound.&lt;/li&gt; &#xA; &lt;li&gt;Could not generate a speech from a too long sentence becauase of our training setting. We see increasing max length could improve the model performance. I hope to use GPUs with 80 GB üò¢ &lt;pre&gt;&lt;code&gt; # Data Filtering for limited computation resource. &#xA;  wav_min = 32&#xA;  wav_max = 600 # 12s &#xA;  text_min = 1&#xA;  text_max = 200&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;TTV v2 may reduce this issue significantly...!&lt;/p&gt; &#xA;&lt;h2&gt;Results &lt;a href=&#34;https://drive.google.com/drive/folders/1xCrZQy9s5MT38RMQxKAtkoWUgxT5qYYW?usp=sharing&#34;&gt;[Download]&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We have attached all samples from LibriTTS test-clean and test-other.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;Our repository is heavily based on &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVIDIA/BigVGAN&#34;&gt;BigVGAN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; [Read More] &lt;/summary&gt; &#xA; &lt;h3&gt;Our Previous Works&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;HierSpeech/HierSpeech-U for Hierarchical Speech Synthesis Framework: &lt;a href=&#34;https://openreview.net/forum?id=awdyRVnfQKX&#34;&gt;https://openreview.net/forum?id=awdyRVnfQKX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;HierVST for Baseline Speech Backbone: &lt;a href=&#34;https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html&#34;&gt;https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;DDDM-VC: &lt;a href=&#34;https://dddm-vc.github.io/&#34;&gt;https://dddm-vc.github.io/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Diff-HierVC: &lt;a href=&#34;https://diff-hiervc.github.io/&#34;&gt;https://diff-hiervc.github.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Baseline Model&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;VITS: &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;https://github.com/jaywalnut310/vits&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;NaturalSpeech: &lt;a href=&#34;https://speechresearch.github.io/naturalspeech/&#34;&gt;https://speechresearch.github.io/naturalspeech/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;NANSY for Audio Perturbation: &lt;a href=&#34;https://github.com/revsic/torch-nansy&#34;&gt;https://github.com/revsic/torch-nansy&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Speech Resynthesis: &lt;a href=&#34;https://github.com/facebookresearch/speech-resynthesis&#34;&gt;https://github.com/facebookresearch/speech-resynthesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Waveform Generator for High-quality Audio Generation&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;HiFi-GAN: &lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;https://github.com/jik876/hifi-gan&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;BigVGAN for High-quality Generator: &lt;a href=&#34;https://arxiv.org/abs/2206.04658&#34;&gt;https://arxiv.org/abs/2206.04658&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;UnivNET: &lt;a href=&#34;https://github.com/mindslab-ai/univnet&#34;&gt;https://github.com/mindslab-ai/univnet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;EnCodec: &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;https://github.com/facebookresearch/encodec&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Self-supervised Speech Model&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Wav2Vec 2.0: &lt;a href=&#34;https://arxiv.org/abs/2006.11477&#34;&gt;https://arxiv.org/abs/2006.11477&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;XLS-R: &lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-xls-r-300m&#34;&gt;https://huggingface.co/facebook/wav2vec2-xls-r-300m&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;MMS: &lt;a href=&#34;https://huggingface.co/facebook/facebook/mms-300m&#34;&gt;https://huggingface.co/facebook/facebook/mms-300m&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Other Large Language Model based Speech Synthesis Model&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;VALL-E &amp;amp; VALL-E-X&lt;/li&gt; &#xA;  &lt;li&gt;SPEAR-TTS&lt;/li&gt; &#xA;  &lt;li&gt;Make-a-Voice&lt;/li&gt; &#xA;  &lt;li&gt;MEGA-TTS &amp;amp; MEGA-TTS 2&lt;/li&gt; &#xA;  &lt;li&gt;UniAudio&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Diffusion-based Model&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;NaturalSpeech 2&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;AdaLN-zero&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Dit: &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;https://github.com/facebookresearch/DiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Thanks for all nice works.&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>passivestar/quickmenu</title>
    <updated>2023-11-26T01:41:35Z</updated>
    <id>tag:github.com,2023-11-26:/passivestar/quickmenu</id>
    <link href="https://github.com/passivestar/quickmenu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Blender addon that simplifies access to useful operators and adds missing functionality&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;960&#34; alt=&#34;qm&#34; src=&#34;https://github.com/passivestar/quickmenu/assets/60579014/6b419737-61f4-419c-9e14-5db46e104fcc&#34;&gt; &#xA;&lt;h1&gt;Quick Menu is a minimalistic productivity addon for Blender&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click on &lt;code&gt;Releases&lt;/code&gt; on the right and download &lt;code&gt;quickmenu.zip&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;In Blender go to &lt;code&gt;Edit -&amp;gt; Preferences -&amp;gt; Addons&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;Install...&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Select the archive (don&#39;t unpack it)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Press &lt;code&gt;D&lt;/code&gt; in 3D view to open the menu.&lt;/p&gt; &#xA;&lt;p&gt;Compatible with Blender &lt;code&gt;4.x.x&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/pPHQ5HQ&#34;&gt;discord&lt;/a&gt; for discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Things to know about Quick Menu:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;It&#39;s &lt;strong&gt;minimalistic&lt;/strong&gt;. The addon is designed to be as unintrusive as possible. It only takes one hotkey (&lt;code&gt;D&lt;/code&gt; by default) and doesn&#39;t have any UI (other than the menu itself)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It&#39;s &lt;strong&gt;quick&lt;/strong&gt;. It&#39;s designed to be used with one hand, so you can keep your other hand on the mouse. It also promotes usage of accelerator keys, i.e &lt;code&gt;d11&lt;/code&gt; to Separate/Join, &lt;code&gt;d13&lt;/code&gt; to Hide/Unhide, etc&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It&#39;s &lt;strong&gt;customizable&lt;/strong&gt;. You can remove any button from the menu, reorder them, create your own submenus through a JSON config. You can also add your own operators to it, even if they come from third-party addons!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It&#39;s &lt;strong&gt;node-driven&lt;/strong&gt;. The addon makes use of Blender 4 node tools where possible, making its python footprint as small as possible, which in turn makes it easier for me to maintain and expand it. You can even look into the &lt;code&gt;nodetools.blend&lt;/code&gt; file yourself to see how tools are put together. You can also make your own node tools and put them into the menu!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>