<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-19T01:43:23Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>smol-ai/developer</title>
    <updated>2023-05-19T01:43:23Z</updated>
    <id>tag:github.com,2023-05-19:/smol-ai/developer</id>
    <link href="https://github.com/smol-ai/developer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;with 100k context windows on the way, it&#39;s now feasible for every dev to have their own smol developer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;smol developer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Human-centric &amp;amp; Coherent Whole Program Synthesis&lt;/strong&gt;&lt;/em&gt; aka your own personal junior developer&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/swyx/status/1657578738345979905&#34;&gt;Build the thing that builds the thing!&lt;/a&gt; a &lt;code&gt;smol dev&lt;/code&gt; for every dev in every situation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;this is a prototype of a &#34;junior developer&#34; agent (aka &lt;code&gt;smol dev&lt;/code&gt;) that scaffolds an entire codebase out for you once you give it a product spec, but does not end the world or overpromise AGI. instead of making and maintaining specific, rigid, one-shot starters, like &lt;code&gt;create-react-app&lt;/code&gt;, or &lt;code&gt;create-nextjs-app&lt;/code&gt;, this is basically &lt;a href=&#34;https://news.ycombinator.com/item?id=35942352&#34;&gt;&lt;code&gt;create-anything-app&lt;/code&gt;&lt;/a&gt; where you develop your scaffolding prompt in a tight loop with your smol dev.&lt;/p&gt; &#xA;&lt;p&gt;AI that is helpful, harmless, and honest is complemented by a codebase that is simple, safe, and smol - &amp;lt;200 lines of Python and Prompts, so this is easy to understand and customize.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;200&#34; src=&#34;https://pbs.twimg.com/media/FwEzVCcaMAE7t4h?format=jpg&amp;amp;name=large&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;engineering with prompts, rather than prompt engineering&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The demo example in &lt;code&gt;prompt.md&lt;/code&gt; shows the potential of AI-enabled, but still firmly human developer centric, workflow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Human writes a basic prompt for the app they want to build&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt; generates code&lt;/li&gt; &#xA; &lt;li&gt;Human runs/reads the code&lt;/li&gt; &#xA; &lt;li&gt;Human can: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;simply add to the prompt as they discover underspecified parts of the prompt&lt;/li&gt; &#xA;   &lt;li&gt;manually runs the code and identifies errors&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;paste the error into the prompt&lt;/em&gt; just like they would file a GitHub issue&lt;/li&gt; &#xA;   &lt;li&gt;for extra help, they can use &lt;code&gt;debugger.py&lt;/code&gt; which reads the whole codebase to make specific code change suggestions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Loop until happiness is attained. Notice that AI is only used as long as it is adding value - once it gets in your way, just take over the codebase from your smol junior developer with no fuss and no hurt feelings. (&lt;em&gt;we could also have smol-dev take over an existing codebase and bootstrap its own prompt... but that&#39;s a Future Direction&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Not no code, not low code, but some third thing.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Perhaps a higher order evolution of programming where you still need to be technical, but no longer have to implement every detail at least to scaffold things out.&lt;/p&gt; &#xA;&lt;h2&gt;6 minute video demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/UCo7YeTy-aE&#34;&gt;https://youtu.be/UCo7YeTy-aE&lt;/a&gt; (sorry for sped up audio, we were optimizing for twitter, bad call)&lt;/p&gt; &#xA;&lt;p&gt;Another user shared their example (prompt =&amp;gt; app in one image)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/RobertCaracaus/status/1659312419485761536?s=20&#34;&gt;&lt;img src=&#34;https://github.com/smol-ai/developer/assets/6764957/15fa189a-3f52-4618-ac8e-2a77b6500264&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m actively working on more examples, sorry for the lack of examples, I know that is frustrating but I wasnt ready for so many of you lol&lt;/p&gt; &#xA;&lt;h2&gt;arch diagram&lt;/h2&gt; &#xA;&lt;p&gt;naturally generated with gpt4, like &lt;a href=&#34;https://twitter.com/swyx/status/1648724820316786688&#34;&gt;we did for babyagi&lt;/a&gt; &lt;img src=&#34;https://github.com/smol-ai/developer/assets/6764957/f8fc68f4-77f6-43ee-852f-a35fb195430a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;innovations and insights&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Please subscribe to &lt;a href=&#34;https://latent.space/&#34;&gt;https://latent.space/&lt;/a&gt; for a fuller writeup and insights and reflections&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Markdown is all you need&lt;/strong&gt; - Markdown is the perfect way to prompt for whole program synthesis because it is easy to mix english and code (whether &lt;code&gt;variable_names&lt;/code&gt; or entire ``` code fenced code samples) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;turns out you can specify prompts in code in prompts and gpt4 obeys that to the letter&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Copy and paste programming&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;teaching the program to understand how to code around a new API (Anthropic&#39;s API is after GPT3&#39;s knowledge cutoff) by just pasting in the &lt;code&gt;curl&lt;/code&gt; input and output&lt;/li&gt; &#xA;   &lt;li&gt;pasting error messages into the prompt and vaguely telling the program how you&#39;d like it handled. it kind of feels like &#34;logbook driven programming&#34;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Debugging by &lt;code&gt;cat&lt;/code&gt;ing&lt;/strong&gt; the whole codebase with your error message and getting specific fix suggestions - particularly delightful!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tricks for whole program coherence&lt;/strong&gt; - our chosen example usecase, Chrome extensions, have a lot of indirect dependencies across files. Any hallucination of cross dependencies causes the whole program to error. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We solved this by adding an intermediate step asking GPT to think through &lt;code&gt;shared_dependencies.md&lt;/code&gt;, and then insisting on using that in generating each file. This basically means GPT is able to talk to itself...&lt;/li&gt; &#xA;   &lt;li&gt;... but it&#39;s not perfect, yet. &lt;code&gt;shared_dependencies.md&lt;/code&gt; is sometimes not comperehensive in understanding what are hard dependencies between files. So we just solved it by specifying a specific &lt;code&gt;name&lt;/code&gt; in the prompt. felt dirty at first but it works, and really it&#39;s just clear unambiguous communication at the end of the day.&lt;/li&gt; &#xA;   &lt;li&gt;see &lt;code&gt;prompt.md&lt;/code&gt; for SOTA smol-dev prompting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low activation energy for unfamiliar APIs&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;we have never really learned css animations, but now can just say we want a &#34;juicy css animated red and white candy stripe loading indicator&#34; and it does the thing.&lt;/li&gt; &#xA;   &lt;li&gt;ditto for Chrome Extension Manifest v3 - the docs are an abject mess, but fortunately we don&#39;t have to read them now to just get a basic thing done&lt;/li&gt; &#xA;   &lt;li&gt;the Anthropic docs (bad bad) were missing guidance on what return signature they have. so just curl it and dump it in the prompt lol.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modal is all you need&lt;/strong&gt; - we chose Modal to solve 4 things: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;solve python dependency hell in dev and prod&lt;/li&gt; &#xA;   &lt;li&gt;parallelizable code generation&lt;/li&gt; &#xA;   &lt;li&gt;simple upgrade path from local dev to cloud hosted endpoints (in future)&lt;/li&gt; &#xA;   &lt;li&gt;fault tolerant openai api calls with retries/backoff, and attached storage (for future use)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Please subscribe to &lt;a href=&#34;https://latent.space/&#34;&gt;https://latent.space/&lt;/a&gt; for a fuller writeup and insights and reflections&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;caveats&lt;/h3&gt; &#xA;&lt;p&gt;We were working on a Chrome Extension, which requires images to be generated, so we added some usecase specific code in there to skip destroying/regenerating them, that we haven&#39;t decided how to generalize.&lt;/p&gt; &#xA;&lt;p&gt;We dont have access to GPT4-32k, but if we did, we&#39;d explore dumping entire API/SDK documentation into context.&lt;/p&gt; &#xA;&lt;p&gt;The feedback loop is very slow right now (&lt;code&gt;time&lt;/code&gt; says about 2-4 mins to generate a program with GPT4, even with parallelization due to Modal (occasionally spiking higher)), but it&#39;s a safe bet that it will go down over time (see also &#34;future directions&#34; below).&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;p&gt;it&#39;s basically:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/smol-ai/developer&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;copy over &lt;code&gt;.example.env&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; filling in your API keys.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are no python dependencies to wrangle thanks to using Modal as a &lt;a href=&#34;https://www.google.com/search?q=self+provisioning+runtime&#34;&gt;self-provisioning runtime&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unfortunately this project also uses 3 waitlisted things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modal.com - &lt;code&gt;pip install modal-client&lt;/code&gt; (private beta - hit up the modal team to get an invite, and login) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can run this project w/o Modal following these instructions:&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;python main_no_modal.py YOUR_PROMPT_HERE&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-4 api (private beta) - can use 3.5 but obviously wont be as good&lt;/li&gt; &#xA; &lt;li&gt;(for the demo project) anthropic claude 100k context api (private beta)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;yes, the most important skill in being an ai engineer is social engineering to get off waitlists. Modal will let you in if you say the keyword &#34;swyx&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;you&#39;ll have to adapt this code on a fork if you want to use it on other infra. please open issues/PRs and i&#39;ll happily highlight your fork here.&lt;/p&gt; &#xA;&lt;h3&gt;trying the example chrome extension&lt;/h3&gt; &#xA;&lt;p&gt;the &lt;code&gt;/generated&lt;/code&gt; and &lt;code&gt;/exampleChromeExtension&lt;/code&gt; folder contains &lt;code&gt;a Chrome Manifest V3 extension that reads the current page, and offers a popup UI that has the page title+content and a textarea for a prompt (with a default value we specify). When the user hits submit, it sends the page title+content to the Anthropic Claude API along with the up to date prompt to summarize it. The user can modify that prompt and re-send the prompt+content to get another summary view of the content.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;go to Manage Extensions in Chrome&lt;/li&gt; &#xA; &lt;li&gt;load unpacked&lt;/li&gt; &#xA; &lt;li&gt;find the relevant folder in your file system and load it&lt;/li&gt; &#xA; &lt;li&gt;go to any content heavy site&lt;/li&gt; &#xA; &lt;li&gt;click the cute bird&lt;/li&gt; &#xA; &lt;li&gt;see it work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;this entire extension was generated by the prompt in &lt;code&gt;prompt.md&lt;/code&gt; (except for the images), and was built up over time by adding more words to the prompt in an iterative process.&lt;/p&gt; &#xA;&lt;h2&gt;smol dev&lt;/h2&gt; &#xA;&lt;p&gt;basic usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;modal run main.py --prompt &#34;a Chrome extension that, when clicked, opens a small window with a page where you can enter a prompt for reading the currently open page and generating some response from openai&#34;   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after a while of adding to your prompt, you can extract your prompt to a file, as long as your &#34;prompt&#34; ends in a .md extension we&#39;ll go look for that file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;modal run main.py --prompt prompt.md   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;each time you run this, the generated directory is deleted (except for images) and all files are rewritten from scratch.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;shared_dependencies.md&lt;/code&gt; file is a helper file that ensures coherence between files.&lt;/p&gt; &#xA;&lt;p&gt;if you make a tweak to the prompt and only want it to affect one file, and keep the rest of the files, specify the file param:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;modal run main.py --prompt prompt.md  --file popup.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;smol debugger&lt;/h2&gt; &#xA;&lt;p&gt;take the entire contents of the generated directory in context, feed in an error, get a response. this basically takes advantage of longer (32k-100k) context so we basically dont have to do any embedding of the source.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;modal run debugger.py --prompt &#34;Uncaught (in promise) TypeError: Cannot destructure property &#39;pageTitle&#39; of &#39;(intermediate value)&#39; as it is undefined.    at init (popup.js:59:11)&#34;&#xA;&#xA;# gpt4&#xA;modal run debugger.py --prompt &#34;your_error msg_here&#34; --model=gpt-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;smol pm&lt;/h2&gt; &#xA;&lt;p&gt;take the entire contents of the generated directory in context, and get a prompt back that could synthesize the whole program. basically &lt;code&gt;smol dev&lt;/code&gt;, in reverse.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;modal run code2prompt.py # ~0.5 second&#xA;&#xA;# use gpt4&#xA;modal run code2prompt.py --model=gpt-4 # 2 mins, MUCH better results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have done indicative runs of both, stored in &lt;code&gt;code2prompt-gpt3.md&lt;/code&gt; vs &lt;code&gt;code2prompt-gpt4.md&lt;/code&gt;. Note how incredibly better gpt4 is at prompt engineering its future self.&lt;/p&gt; &#xA;&lt;p&gt;Naturally, we had to try &lt;code&gt;code2prompt2code&lt;/code&gt;...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# add prompt... this needed a few iterations to get right&#xA;modal run code2prompt.py --prompt &#34;make sure all the id&#39;s of the DOM elements, and the data structure of the page content (stored with {pageTitle, pageContent }) , referenced/shared by the js files match up exactly. take note to only use Chrome Manifest V3 apis. rename the extension to code2prompt2code&#34; --model=gpt-4 # takes 4 mins. produces semi working chrome extension copy based purely on the model-generated description of a different codebase&#xA;&#xA;# must go deeper&#xA;modal run main.py --prompt code2prompt-gpt4.md --directory code2prompt2code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We leave the social and technical impacts of multilayer generative deep-frying of codebases as an exercise to the reader.&lt;/p&gt; &#xA;&lt;h2&gt;future directions&lt;/h2&gt; &#xA;&lt;p&gt;things to try/would accept open issue discussions and PRs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;specify .md files for each generated file&lt;/strong&gt;, with further prompts that could finetune the output in each of them &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;so basically like &lt;code&gt;popup.html.md&lt;/code&gt; and &lt;code&gt;content_script.js.md&lt;/code&gt; and so on&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;bootstrap the &lt;code&gt;prompt.md&lt;/code&gt;&lt;/strong&gt; for existing codebases - write a script to read in a codebase and write a descriptive, bullet pointed prompt that generates it &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;done by &lt;code&gt;smol pm&lt;/code&gt;, but its not very good yet - would love for some focused polish/effort until we have quine smol developer that can generate itself lmao&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ability to install its own dependencies&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;this leaks into depending on the execution environment, which we all know is the path to dependency madness. how to avoid? dockerize? nix? &lt;a href=&#34;https://twitter.com/litbid/status/1658154530385670150&#34;&gt;web container&lt;/a&gt;?&lt;/li&gt; &#xA;   &lt;li&gt;Modal has an interesting possibility: generate functions that speak modal which also solves the dependency thing &lt;a href=&#34;https://twitter.com/akshat_b/status/1658146096902811657&#34;&gt;https://twitter.com/akshat_b/status/1658146096902811657&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;self-heal&lt;/strong&gt; by running the code itself and use errors as information for reprompting &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;however its a bit hard to get errors from the chrome extension environment so we did not try this&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;using anthropic as the coding layer&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;you can run &lt;code&gt;modal run anthropic.py --prompt prompt.md --outputdir=anthropic&lt;/code&gt; to try it&lt;/li&gt; &#xA;   &lt;li&gt;but it doesnt work because anthropic doesnt follow instructions to generate file code very well.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;make agents that autonomously run this code in a loop/watch the prompt file&lt;/strong&gt; and regenerate code each time, on a new git branch &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;the code could be generated on 5 simultaneous git branches and checking their output would just involve switching git branches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>salesforce/CodeT5</title>
    <updated>2023-05-19T01:43:23Z</updated>
    <id>tag:github.com,2023-05-19:/salesforce/CodeT5</id>
    <link href="https://github.com/salesforce/CodeT5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Home of CodeT5: Open Code LLMs for Code Understanding and Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CodeT5 and CodeT5+&lt;/h1&gt; &#xA;&lt;p&gt;Official research release for &lt;strong&gt;CodeT5&lt;/strong&gt; and &lt;strong&gt;CodeT5+&lt;/strong&gt; models for &lt;strong&gt;Code Understanding and Generation&lt;/strong&gt; from Salesforce Research, which are introduced by the following papers:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Title&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2305.07922.pdf&#34;&gt;CodeT5+: Open Code Large Language Models for Code Understanding and Generation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Authors&lt;/em&gt;: &lt;a href=&#34;https://yuewang-cuhk.github.io/&#34;&gt;Yue Wang&lt;/a&gt;*, &lt;a href=&#34;https://sites.google.com/view/henryle2018/home?pli=1&#34;&gt;Hung Le&lt;/a&gt;*, &lt;a href=&#34;https://akhileshgotmare.github.io/&#34;&gt;Akhilesh Deepak Gotmare&lt;/a&gt;, &lt;a href=&#34;https://bdqnghi.github.io/&#34;&gt;Nghi D.Q. Bui&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/junnanlics&#34;&gt;Junnan Li&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/stevenhoi/home&#34;&gt;Steven C.H. Hoi&lt;/a&gt; (* indicates equal contribution)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;Title&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2109.00859.pdf&#34;&gt;CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Authors&lt;/em&gt;: &lt;a href=&#34;https://yuewang-cuhk.github.io/&#34;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/weishi-wang/&#34;&gt;Weishi Wang&lt;/a&gt; , &lt;a href=&#34;https://raihanjoty.github.io/&#34;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/stevenhoi/home&#34;&gt;Steven C.H. Hoi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In practice, CodeT5 and CodeT5+ models can be deployed as an AI-powered coding assistant to boost the productivity of software developers. At Salesforce, we build an AI coding assistant demo using CodeT5 as a VS Code plugin to provide three capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text-to-code generation&lt;/strong&gt;: generate code based on the natural language description.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code autocompletion&lt;/strong&gt;: complete the whole function of code given the target function name.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code summarization&lt;/strong&gt;: generate the summary of a function in natural language description.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/salesforce/CodeT5/main/codet5.gif&#34; alt=&#34;CodeT5 demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New: üéâ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;May 2023&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CodeT5+&lt;/strong&gt; paper and models are releasedÔºÅüî• &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.07922.pdf&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/salesforce/CodeT5/tree/main/CodeT5+&#34;&gt;code&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/models?sort=downloads&amp;amp;search=codet5p&#34;&gt;model&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sep 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our &lt;strong&gt;CodeRL&lt;/strong&gt; paper has been accepted to NeurIPS 2022! &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2207.01780.pdf&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/salesforce/CodeRL&#34;&gt;code&lt;/a&gt; | &lt;a href=&#34;https://blog.salesforceairesearch.com/coderl&#34;&gt;blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;July 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We release two large-sized CodeT5 checkpoints at HuggingFace: &lt;a href=&#34;https://huggingface.co/Salesforce/codet5-large&#34;&gt;Salesforce/codet5-large&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/Salesforce/codet5-large-ntp-py&#34;&gt;Salesforce/codet5-large-ntp-py&lt;/a&gt;, which are introduced by the &lt;a href=&#34;https://arxiv.org/pdf/2207.01780.pdf&#34;&gt;CodeRL paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Oct 2021&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://console.cloud.google.com/storage/browser/sfr-codet5-data-research/finetuned_models&#34;&gt;fine-tuned checkpoints&lt;/a&gt; for all the downstream tasks covered in the paper. Besides, we release a CodeT5-base fine-tuned checkpoint (&lt;a href=&#34;https://huggingface.co/Salesforce/codet5-base-multi-sum&#34;&gt;Salesforce/codet5-base-multi-sum&lt;/a&gt;) for multilingual code summarization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sep, 2021&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CodeT5&lt;/strong&gt; paper accepted to EMNLP 2021 and models are released! &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2109.00859.pdf&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/salesforce/CodeT5/tree/main/CodeT5&#34;&gt;code&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/models?sort=downloads&amp;amp;search=codet5&#34;&gt;model&lt;/a&gt; | &lt;a href=&#34;https://github.com/salesforce/CodeT5/raw/main/CodeT5/CodeT5_model_card.pdf&#34;&gt;model card&lt;/a&gt; | &lt;a href=&#34;https://blog.salesforceairesearch.com/codet5/&#34;&gt;blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code to be useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;    wang2021codet5,&#xA;    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, &#xA;    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},&#xA;    booktitle={EMNLP},&#xA;    year={2021},&#xA;}&#xA;&#xA;@inproceedings{&#xA;    le2022coderl,&#xA;    title={CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},&#xA;    author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven C. H.},&#xA;    journal={NeurIPS},&#xA;    year={2022}&#xA;}&#xA;&#xA;@article{&#xA;    wang2023codet5plus,&#xA;    title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},&#xA;    author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D.Q. and Li, Junnan and Hoi, Steven C. H.},&#xA;    journal={arXiv preprint},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is released under the BSD-3 License (see &lt;code&gt;LICENSE.txt&lt;/code&gt; for details), but we also ask that users respect the following:&lt;/p&gt; &#xA;&lt;p&gt;This software should not be used to promote or profit from:&lt;/p&gt; &#xA;&lt;p&gt;violence, hate, and division,&lt;/p&gt; &#xA;&lt;p&gt;environmental destruction,&lt;/p&gt; &#xA;&lt;p&gt;abuse of human rights, or&lt;/p&gt; &#xA;&lt;p&gt;the destruction of people&#39;s physical and mental health.&lt;/p&gt; &#xA;&lt;p&gt;We encourage users of this software to tell us about the applications in which they are putting it to use by emailing &lt;a href=&#34;mailto:codeT5@salesforce.com&#34;&gt;codeT5@salesforce.com&lt;/a&gt;, and to use &lt;a href=&#34;https://arxiv.org/abs/1810.03993&#34;&gt;appropriate&lt;/a&gt; &lt;a href=&#34;https://www.partnershiponai.org/about-ml/&#34;&gt;documentation&lt;/a&gt; when developing high-stakes applications of this model.&lt;/p&gt; &#xA;&lt;h2&gt;Get Involved&lt;/h2&gt; &#xA;&lt;p&gt;Please create a GitHub issue if you have any questions, suggestions, requests or bug-reports. We welcome PRs!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>101dotxyz/GPTeam</title>
    <updated>2023-05-19T01:43:23Z</updated>
    <id>tag:github.com,2023-05-19:/101dotxyz/GPTeam</id>
    <link href="https://github.com/101dotxyz/GPTeam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPTeam uses GPT-4 to create multiple agents who collaborate to achieve predefined goals.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;GPTeam: Collaborative AI Agents&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/101dotxyz/gpteam.svg?style=for-the-badge&amp;amp;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/101dotxyz/gpteam.svg?style=for-the-badge&amp;amp;&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/101dotxyz/gpteam/issues&#34;&gt;&lt;b&gt;Report Bug&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=cIxhI1d6NsM&#34;&gt;&lt;b&gt;Video Demo&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/101dotxyz/GPTeam/main/assets/gpteam.png&#34; alt=&#34;GPTeam&#34; width=&#34;400&#34; height=&#34;267&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About GPTeam&lt;/h2&gt; &#xA;&lt;p&gt;GPTeam uses GPT-4 to create multiple agents who collaborate to achieve predefined goals. The main objective of this project is to explore the potential of GPT models in enhancing multi-agent productivity and effective communication.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;To begin exploring GPTeam, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the project repository to your local machine&lt;/li&gt; &#xA; &lt;li&gt;Move to the repository: &lt;code&gt;cd gpteam&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python setup.py&lt;/code&gt; to check your environment setup and configure it as needed&lt;/li&gt; &#xA; &lt;li&gt;Update the environment variables in &lt;code&gt;.env&lt;/code&gt; with your API Keys. You will need an OpenAI API key, which you can obtain &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;. Supplying API keys for optional services will enable the use of other tools.&lt;/li&gt; &#xA; &lt;li&gt;Launch the world by running &lt;code&gt;poetry run world&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To run the world cheaply, you can use &lt;code&gt;poetry run world --turbo&lt;/code&gt;. This will use gpt3.5-turbo for all LLM calls which is a lot cheaper, but expect worse results!&lt;/p&gt; &#xA;&lt;p&gt;Now you can observe the world in action and watch as the agents interact with each other, working together to accomplish their assigned directives.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;GPTeam employs separate agents, each equipped with a memory, that interact with one another using communication as a tool. The implementation of agent memory and reflection is inspired by &lt;a href=&#34;https://arxiv.org/pdf/2304.03442.pdf&#34;&gt;this research paper&lt;/a&gt;. Agents move around the world and perform tasks in different locations, depending on what they are doing and where other agents are located. They can speak to eachother and collaborate on tasks, working in parallel towards common goals.&lt;/p&gt; &#xA;&lt;h2&gt;Viewing Agents&lt;/h2&gt; &#xA;&lt;p&gt;The world is a busy place! To get a view of what different agents are doing whilst the world is running, you can visit the &lt;code&gt;agents/&lt;/code&gt; folder where there is a txt file for each agent containing a summary of their current state.&lt;/p&gt; &#xA;&lt;h2&gt;Changing the world&lt;/h2&gt; &#xA;&lt;p&gt;To change the world, all you need to do is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make changes to the &lt;code&gt;config.json&lt;/code&gt; by updating the available agents or locations&lt;/li&gt; &#xA; &lt;li&gt;Reset your database: &lt;code&gt;poetry run db-reset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the world again: &lt;code&gt;poetry run world&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setting up the Discord Integration&lt;/h2&gt; &#xA;&lt;p&gt;Read through the dedicated &lt;a href=&#34;https://raw.githubusercontent.com/101dotxyz/GPTeam/main/DISCORD.md&#34;&gt;Discord setup docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using with Anthropic Claude&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have an &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in your env, then you can use &lt;code&gt;poetry run world --claude&lt;/code&gt; which will run the world using &lt;code&gt;claude-v1&lt;/code&gt; for some calls and &lt;code&gt;claude-v1-instant&lt;/code&gt; for others.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We enthusiastically welcome contributions to GPTeam! To contribute, please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the project repository to your own account&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your changes&lt;/li&gt; &#xA; &lt;li&gt;Implement your changes to the project code&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request to the main project repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We will review your pull request and provide feedback as necessary.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/101dotxyz/GPTeam/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>