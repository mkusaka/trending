<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-30T01:39:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PeterL1n/RobustVideoMatting</title>
    <updated>2022-12-30T01:39:15Z</updated>
    <id>tag:github.com,2022-12-30:/PeterL1n/RobustVideoMatting</id>
    <link href="https://github.com/PeterL1n/RobustVideoMatting" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Robust Video Matting (RVM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/image/teaser.gif&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/README_zh_Hans.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official repository for the paper &lt;a href=&#34;https://peterl1n.github.io/RobustVideoMatting/&#34;&gt;Robust High-Resolution Video Matting with Temporal Guidance&lt;/a&gt;. RVM is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs. It achieves &lt;strong&gt;4K 76FPS&lt;/strong&gt; and &lt;strong&gt;HD 104FPS&lt;/strong&gt; on an Nvidia GTX 1080 Ti GPU. The project was developed at &lt;a href=&#34;https://www.bytedance.com/&#34;&gt;ByteDance Inc.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Nov 03 2021] Fixed a bug in &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/commit/48effc91576a9e0e7a8519f3da687c0d3522045f&#34;&gt;train.py&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[Sep 16 2021] Code is re-released under GPL-3.0 license.&lt;/li&gt; &#xA; &lt;li&gt;[Aug 25 2021] Source code and pretrained models are published.&lt;/li&gt; &#xA; &lt;li&gt;[Jul 27 2021] Paper is accepted by WACV 2022.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Showreel&lt;/h2&gt; &#xA;&lt;p&gt;Watch the showreel video (&lt;a href=&#34;https://youtu.be/Jvzltozpbpk&#34;&gt;YouTube&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1Z3411B7g7/&#34;&gt;Bilibili&lt;/a&gt;) to see the model&#39;s performance.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://youtu.be/Jvzltozpbpk&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/image/showreel.gif&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;All footage in the video are available in &lt;a href=&#34;https://drive.google.com/drive/folders/1VFnWwuu-YXDKG-N6vcjK_nL7YZMFapMU?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://peterl1n.github.io/RobustVideoMatting/#/demo&#34;&gt;Webcam Demo&lt;/a&gt;: Run the model live in your browser. Visualize recurrent states.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt;: Test our model on your own videos with free GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;We recommend MobileNetv3 models for most use cases. ResNet50 models are the larger variant with small performance improvements. Our model is available on various inference frameworks. See &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md&#34;&gt;inference documentation&lt;/a&gt; for more instructions.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Framework&lt;/td&gt; &#xA;   &lt;td&gt;Download&lt;/td&gt; &#xA;   &lt;td&gt;Notes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth&#34;&gt;rvm_mobilenetv3.pth&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50.pth&#34;&gt;rvm_resnet50.pth&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Official weights for PyTorch. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#pytorch&#34;&gt;Doc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TorchHub&lt;/td&gt; &#xA;   &lt;td&gt; Nothing to Download. &lt;/td&gt; &#xA;   &lt;td&gt; Easiest way to use our model in your PyTorch project. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#torchhub&#34;&gt;Doc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TorchScript&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.torchscript&#34;&gt;rvm_mobilenetv3_fp32.torchscript&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.torchscript&#34;&gt;rvm_mobilenetv3_fp16.torchscript&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.torchscript&#34;&gt;rvm_resnet50_fp32.torchscript&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.torchscript&#34;&gt;rvm_resnet50_fp16.torchscript&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; If inference on mobile, consider export int8 quantized models yourself. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#torchscript&#34;&gt;Doc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ONNX&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.onnx&#34;&gt;rvm_mobilenetv3_fp32.onnx&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.onnx&#34;&gt;rvm_mobilenetv3_fp16.onnx&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.onnx&#34;&gt;rvm_resnet50_fp32.onnx&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.onnx&#34;&gt;rvm_resnet50_fp16.onnx&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Tested on ONNX Runtime with CPU and CUDA backends. Provided models use opset 12. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#onnx&#34;&gt;Doc&lt;/a&gt;, &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/tree/onnx&#34;&gt;Exporter&lt;/a&gt;. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tf.zip&#34;&gt;rvm_mobilenetv3_tf.zip&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_tf.zip&#34;&gt;rvm_resnet50_tf.zip&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; TensorFlow 2 SavedModel. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#tensorflow&#34;&gt;Doc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TensorFlow.js&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tfjs_int8.zip&#34;&gt;rvm_mobilenetv3_tfjs_int8.zip&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Run the model on the web. &lt;a href=&#34;https://peterl1n.github.io/RobustVideoMatting/#/demo&#34;&gt;Demo&lt;/a&gt;, &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/tree/tfjs&#34;&gt;Starter Code&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CoreML&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel&#34;&gt;rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel&#34;&gt;rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel&#34;&gt;rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel&#34;&gt;rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;td&gt; CoreML does not support dynamic resolution. Other resolutions can be exported yourself. Models require iOS 13+. &lt;code&gt;s&lt;/code&gt; denotes &lt;code&gt;downsample_ratio&lt;/code&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md#coreml&#34;&gt;Doc&lt;/a&gt;, &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting/tree/coreml&#34;&gt;Exporter&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models are available in &lt;a href=&#34;https://drive.google.com/drive/folders/1pBsG-SCTatv-95SnEuxmnvvlRx208VKj?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; and &lt;a href=&#34;https://pan.baidu.com/s/1puPSxQqgBFOVpW4W7AolkA&#34;&gt;Baidu Pan&lt;/a&gt; (code: gym7).&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;PyTorch Example&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements_inference.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Load the model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import MattingNetwork&#xA;&#xA;model = MattingNetwork(&#39;mobilenetv3&#39;).eval().cuda()  # or &#34;resnet50&#34;&#xA;model.load_state_dict(torch.load(&#39;rvm_mobilenetv3.pth&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To convert videos, we provide a simple conversion API:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from inference import convert_video&#xA;&#xA;convert_video(&#xA;    model,                           # The model, can be on any device (cpu or cuda).&#xA;    input_source=&#39;input.mp4&#39;,        # A video file or an image sequence directory.&#xA;    output_type=&#39;video&#39;,             # Choose &#34;video&#34; or &#34;png_sequence&#34;&#xA;    output_composition=&#39;com.mp4&#39;,    # File path if video; directory path if png sequence.&#xA;    output_alpha=&#34;pha.mp4&#34;,          # [Optional] Output the raw alpha prediction.&#xA;    output_foreground=&#34;fgr.mp4&#34;,     # [Optional] Output the raw foreground prediction.&#xA;    output_video_mbps=4,             # Output video mbps. Not needed for png sequence.&#xA;    downsample_ratio=None,           # A hyperparameter to adjust or use None for auto.&#xA;    seq_chunk=12,                    # Process n frames at once for better parallelism.&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Or write your own inference code:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;from torchvision.transforms import ToTensor&#xA;from inference_utils import VideoReader, VideoWriter&#xA;&#xA;reader = VideoReader(&#39;input.mp4&#39;, transform=ToTensor())&#xA;writer = VideoWriter(&#39;output.mp4&#39;, frame_rate=30)&#xA;&#xA;bgr = torch.tensor([.47, 1, .6]).view(3, 1, 1).cuda()  # Green background.&#xA;rec = [None] * 4                                       # Initial recurrent states.&#xA;downsample_ratio = 0.25                                # Adjust based on your video.&#xA;&#xA;with torch.no_grad():&#xA;    for src in DataLoader(reader):                     # RGB tensor normalized to 0 ~ 1.&#xA;        fgr, pha, *rec = model(src.cuda(), *rec, downsample_ratio)  # Cycle the recurrent states.&#xA;        com = fgr * pha + bgr * (1 - pha)              # Composite to green background. &#xA;        writer.write(com)                              # Write frame.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;The models and converter API are also available through TorchHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the model.&#xA;model = torch.hub.load(&#34;PeterL1n/RobustVideoMatting&#34;, &#34;mobilenetv3&#34;) # or &#34;resnet50&#34;&#xA;&#xA;# Converter API.&#xA;convert_video = torch.hub.load(&#34;PeterL1n/RobustVideoMatting&#34;, &#34;converter&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/inference.md&#34;&gt;inference documentation&lt;/a&gt; for details on &lt;code&gt;downsample_ratio&lt;/code&gt; hyperparameter, more converter arguments, and more advanced usage.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Training and Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/PeterL1n/RobustVideoMatting/master/documentation/training.md&#34;&gt;training documentation&lt;/a&gt; to train and evaluate your own model.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Speed&lt;/h2&gt; &#xA;&lt;p&gt;Speed is measured with &lt;code&gt;inference_speed_test.py&lt;/code&gt; for reference.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;dType&lt;/th&gt; &#xA;   &lt;th&gt;HD (1920x1080)&lt;/th&gt; &#xA;   &lt;th&gt;4K (3840x2160)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTX 3090&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;172 FPS&lt;/td&gt; &#xA;   &lt;td&gt;154 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTX 2060 Super&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;134 FPS&lt;/td&gt; &#xA;   &lt;td&gt;108 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GTX 1080 Ti&lt;/td&gt; &#xA;   &lt;td&gt;FP32&lt;/td&gt; &#xA;   &lt;td&gt;104 FPS&lt;/td&gt; &#xA;   &lt;td&gt;74 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note 1: HD uses &lt;code&gt;downsample_ratio=0.25&lt;/code&gt;, 4K uses &lt;code&gt;downsample_ratio=0.125&lt;/code&gt;. All tests use batch size 1 and frame chunk 1.&lt;/li&gt; &#xA; &lt;li&gt;Note 2: GPUs before Turing architecture does not support FP16 inference, so GTX 1080 Ti uses FP32.&lt;/li&gt; &#xA; &lt;li&gt;Note 3: We only measure tensor throughput. The provided video conversion script in this repo is expected to be much slower, because it does not utilize hardware video encoding/decoding and does not have the tensor transfer done on parallel threads. If you are interested in implementing hardware video encoding/decoding in Python, please refer to &lt;a href=&#34;https://github.com/NVIDIA/VideoProcessingFramework&#34;&gt;PyNvCodec&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Project Members&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/shanchuanlin/&#34;&gt;Shanchuan Lin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/linjieyang89/&#34;&gt;Linjie Yang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/imran-saleemi/&#34;&gt;Imran Saleemi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~soumya91/&#34;&gt;Soumyadip Sengupta&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Third-Party Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting&#34;&gt;NCNN C++ Android&lt;/a&gt; (&lt;a href=&#34;https://github.com/FeiGeChuanShu&#34;&gt;@FeiGeChuanShu&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/RobustVideoMatting.lite.ai.toolkit&#34;&gt;lite.ai.toolkit&lt;/a&gt; (&lt;a href=&#34;https://github.com/DefTruth&#34;&gt;@DefTruth&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Robust-Video-Matting&#34;&gt;Gradio Web Demo&lt;/a&gt; (&lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.natml.ai/@natsuite/robust-video-matting&#34;&gt;Unity Engine demo with NatML&lt;/a&gt; (&lt;a href=&#34;https://github.com/natsuite&#34;&gt;@natsuite&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/mnn/cv/mnn_rvm.cpp&#34;&gt;MNN C++ Demo&lt;/a&gt; (&lt;a href=&#34;https://github.com/DefTruth&#34;&gt;@DefTruth&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/tnn/cv/tnn_rvm.cpp&#34;&gt;TNN C++ Demo&lt;/a&gt; (&lt;a href=&#34;https://github.com/DefTruth&#34;&gt;@DefTruth&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ToniRV/NeRF-SLAM</title>
    <updated>2022-12-30T01:39:15Z</updated>
    <id>tag:github.com,2022-12-30:/ToniRV/NeRF-SLAM</id>
    <link href="https://github.com/ToniRV/NeRF-SLAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-Time Dense Monocular SLAM with Neural Radiance Fields. https://arxiv.org/abs/2210.13641 + Sigma-Fusion: Probabilistic Volumetric Fusion for Dense Monocular SLAM https://arxiv.org/abs/2210.01276&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;http://mit.edu/sparklab/&#34;&gt; &lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/media/sparklab_logo.png&#34; width=&#34;80&#34; alt=&#34;sparklab&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://marinerobotics.mit.edu/&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/media/mrg_logo.png&#34; width=&#34;150&#34; alt=&#34;kimera&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.mit.edu/~arosinol/&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/media/mit.png&#34; width=&#34;100&#34; alt=&#34;mit&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;NeRF-SLAM&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Real-Time Dense Monocular SLAM with Neural Radiance Fields&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.mit.edu/~arosinol/&#34;&gt;&lt;strong&gt;Antoni Rosinol&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://marinerobotics.mit.edu/&#34;&gt;&lt;strong&gt;John J. Leonard&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://web.mit.edu/sparklab/&#34;&gt;&lt;strong&gt;Luca Carlone&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;h2 align=&#34;center&#34;&gt;In Review&lt;/h2&gt; --&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.13641&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=-6ufRJugcEU&#34;&gt;Video&lt;/a&gt; | &#xA; &lt;!-- &lt;a href=&#34;&#34;&gt;Project Page&lt;/a&gt;--&gt; &lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/media/intro.gif&#34; alt=&#34;&#34; width=&#34;90%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;details open style=&#34;padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;&#34;&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#install&#34;&gt;Install&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#download-sample-data&#34;&gt;Download Datasets&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#run&#34;&gt;Run&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#citation&#34;&gt;Citation&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#license&#34;&gt;License&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/#contact&#34;&gt;Contact&lt;/a&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Clone repo with submodules:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ToniRV/NeRF-SLAM.git --recurse-submodules&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From this point on, use a virtual environment... Install torch (see &lt;a href=&#34;https://pytorch.org/get-started/previous-versions&#34;&gt;here&lt;/a&gt; for other versions):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# CUDA 11.3&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pip install requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;pip install -r ./thirdparty/gtsam/python/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compile ngp (you need cmake&amp;gt;3.22):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake ./thirdparty/instant-ngp -B build_ngp&#xA;cmake --build build_ngp --config RelWithDebInfo -j&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compile gtsam and enable the python wrapper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake ./thirdparty/gtsam -DGTSAM_BUILD_PYTHON=1 -B build_gtsam &#xA;cmake --build build_gtsam --config RelWithDebInfo -j&#xA;cd build_gtsam&#xA;make python-install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Sample Data&lt;/h2&gt; &#xA;&lt;p&gt;This will just download one of the replica scenes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./scripts/download_replica_sample.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./examples/slam_demo.py --dataset_dir=./datasets/Replica/office0 --dataset_name=nerf --buffer=100 --slam --parallel_run --img_stride=2 --fusion=&#39;nerf&#39; --multi_gpu --gui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This repo also implements &lt;a href=&#34;https://arxiv.org/abs/2210.01276&#34;&gt;Sigma-Fusion&lt;/a&gt;: just change &lt;code&gt;--fusion=&#39;sigma&#39;&lt;/code&gt; to run that.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;GPU Memory&lt;/h3&gt; &#xA;&lt;p&gt;This is a GPU memory intensive pipeline, to monitor your GPU usage, I&#39;d recommend to use &lt;code&gt;nvitop&lt;/code&gt;. Install nvitop in a local env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install --upgrade nvitop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Keep it running on a terminal, and monitor GPU memory usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nvitop --monitor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you consistently see &#34;out-of-memory&#34; errors, you may either need to change parameters or buy better GPUs :). The memory consuming parts of this pipeline are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Frame to frame correlation volumes (but can be avoided using on-the-fly correlation computation).&lt;/li&gt; &#xA; &lt;li&gt;Volumetric rendering (intrinsically memory intensive, tricks exist, but ultimately we need to move to light fields or some better representation (OpenVDB?)).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation issues&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gtsam not working: check that the python wrapper is installed, check instructions here: &lt;a href=&#34;https://github.com/ToniRV/gtsam-1/raw/develop/python/README.md&#34;&gt;gtsam_python&lt;/a&gt;. Make sure you use our gtsam fork, which exposes more of gtsam&#39;s functionality to python.&lt;/li&gt; &#xA; &lt;li&gt;Gtsam&#39;s dependency is not really needed, I just used to experiment adding IMU and/or stereo cameras, and have an easier interface to build factor-graphs. This didn&#39;t quite work though, because the network seemed to have a concept of scale, and it didn&#39;t quite work when updating poses/landmarks and then optical flow.&lt;/li&gt; &#xA; &lt;li&gt;Somehow the parser converts &lt;a href=&#34;https://github.com/borglab/gtsam/compare/develop...ToniRV:gtsam-1:feature/nerf_slam#diff-add3627555fb7411e36ea4d863c15f4187e018b6e00b608ab260e3221aef057aR345&#34;&gt;this&lt;/a&gt; to &lt;code&gt;const std::vector&amp;lt;const gtsam::Matrix&amp;amp;&amp;gt;&amp;amp;&lt;/code&gt;, and I need to remove manually in &lt;code&gt;gtsam/build/python/linear.cpp&lt;/code&gt; the inner &lt;code&gt;const X&amp;amp; ...&lt;/code&gt;, and also add &lt;code&gt;&amp;lt;pybind11/stl.h&amp;gt;&lt;/code&gt; because:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Did you forget to `#include &amp;lt;pybind11/stl.h&amp;gt;`?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{rosinol2022nerf,&#xA;  title={NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields},&#xA;  author={Rosinol, Antoni and Leonard, John J and Carlone, Luca},&#xA;  journal={arXiv preprint arXiv:2210.13641},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repo is BSD Licensed. It reimplements parts of Droid-SLAM (BSD Licensed). Our changes to instant-NGP (Nvidia License) are released in our &lt;a href=&#34;https://github.com/ToniRV/instant-ngp&#34;&gt;fork of instant-ngp&lt;/a&gt; (branch &lt;code&gt;feature/nerf_slam&lt;/code&gt;) and added here as a thirdparty dependency using git submodules.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This work has been possible thanks to the open-source code from &lt;a href=&#34;https://github.com/princeton-vl/DROID-SLAM&#34;&gt;Droid-SLAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Instant-NGP&lt;/a&gt;, as well as the open-source datasets &lt;a href=&#34;https://github.com/facebookresearch/Replica-Dataset&#34;&gt;Replica&lt;/a&gt; and &lt;a href=&#34;https://github.com/jc211/nerf-cube-diorama-dataset&#34;&gt;Cube-Diorama&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;I have many ideas on how to improve this approach, but I just graduated so I won&#39;t have much time to do another PhD... If you are interested in building on top of this, feel free to reach out :) &lt;a href=&#34;https://raw.githubusercontent.com/ToniRV/NeRF-SLAM/master/arosinol@mit.edu&#34;&gt;arosinol@mit.edu&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LibreTranslate/LibreTranslate</title>
    <updated>2022-12-30T01:39:15Z</updated>
    <id>tag:github.com,2022-12-30:/LibreTranslate/LibreTranslate</id>
    <link href="https://github.com/LibreTranslate/LibreTranslate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free and Open Source Machine Translation API. 100% self-hosted, offline capable and easy to setup.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LibreTranslate&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://libretranslate.com&#34;&gt;Try it online!&lt;/a&gt; | &lt;a href=&#34;https://libretranslate.com/docs&#34;&gt;API Docs&lt;/a&gt; | &lt;a href=&#34;https://community.libretranslate.com/&#34;&gt;Community Forum&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/libretranslate&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/libretranslate&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/actions?query=workflow%3A%22Run+tests%22&#34;&gt;&lt;img src=&#34;https://github.com/LibreTranslate/LibreTranslate/workflows/Run%20tests/badge.svg?sanitize=true&#34; alt=&#34;Run tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml&#34;&gt;&lt;img src=&#34;https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Publish Docker Image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml&#34;&gt;&lt;img src=&#34;https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/humanetech-community/awesome-humane-tech&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true&#34; alt=&#34;Awesome Humane Tech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Free and Open Source Machine Translation API, entirely self-hosted. Unlike other APIs, it doesn&#39;t rely on proprietary providers such as Google or Azure to perform translations. Instead, its translation engine is powered by the open source &lt;a href=&#34;https://github.com/argosopentech/argos-translate&#34;&gt;Argos Translate&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/64697405/139015751-279f31ac-36f1-4950-9ea7-87e76bf65f51.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://libretranslate.com&#34;&gt;Try it online!&lt;/a&gt; | &lt;a href=&#34;https://libretranslate.com/docs&#34;&gt;API Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;API Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Simple&lt;/h3&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const res = await fetch(&#34;https://libretranslate.com/translate&#34;, {&#xA;&#x9;method: &#34;POST&#34;,&#xA;&#x9;body: JSON.stringify({&#xA;&#x9;&#x9;q: &#34;Hello!&#34;,&#xA;&#x9;&#x9;source: &#34;en&#34;,&#xA;&#x9;&#x9;target: &#34;es&#34;&#xA;&#x9;}),&#xA;&#x9;headers: { &#34;Content-Type&#34;: &#34;application/json&#34; }&#xA;});&#xA;&#xA;console.log(await res.json());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;    &#34;translatedText&#34;: &#34;¡Hola!&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Auto Detect Language&lt;/h3&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const res = await fetch(&#34;https://libretranslate.com/translate&#34;, {&#xA;&#x9;method: &#34;POST&#34;,&#xA;&#x9;body: JSON.stringify({&#xA;&#x9;&#x9;q: &#34;Ciao!&#34;,&#xA;&#x9;&#x9;source: &#34;auto&#34;,&#xA;&#x9;&#x9;target: &#34;en&#34;&#xA;&#x9;}),&#xA;&#x9;headers: { &#34;Content-Type&#34;: &#34;application/json&#34; }&#xA;});&#xA;&#xA;console.log(await res.json());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;    &#34;detectedLanguage&#34;: {&#xA;        &#34;confidence&#34;: 83,&#xA;        &#34;language&#34;: &#34;it&#34;&#xA;    },&#xA;    &#34;translatedText&#34;: &#34;Bye!&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;HTML (beta)&lt;/h3&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const res = await fetch(&#34;https://libretranslate.com/translate&#34;, {&#xA;&#x9;method: &#34;POST&#34;,&#xA;&#x9;body: JSON.stringify({&#xA;&#x9;&#x9;q: &#39;&amp;lt;p class=&#34;green&#34;&amp;gt;Hello!&amp;lt;/p&amp;gt;&#39;,&#xA;&#x9;&#x9;source: &#34;en&#34;,&#xA;&#x9;&#x9;target: &#34;es&#34;,&#xA;&#x9;&#x9;format: &#34;html&#34;&#xA;&#x9;}),&#xA;&#x9;headers: { &#34;Content-Type&#34;: &#34;application/json&#34; }&#xA;});&#xA;&#xA;console.log(await res.json());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;    &#34;translatedText&#34;: &#34;&amp;lt;p class=\&#34;green\&#34;&amp;gt;¡Hola!&amp;lt;/p&amp;gt;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install and Run&lt;/h2&gt; &#xA;&lt;p&gt;You can run your own API server with just a few lines of setup!&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have Python installed (3.8 or higher is recommended), then simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install libretranslate&#xA;libretranslate [args]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open a web browser to &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Ubuntu 20.04 you can also use the install script available at &lt;a href=&#34;https://github.com/argosopentech/LibreTranslate-init&#34;&gt;https://github.com/argosopentech/LibreTranslate-init&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build and Run&lt;/h2&gt; &#xA;&lt;p&gt;If you want to make changes to the code, you can build from source, and run the API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LibreTranslate/LibreTranslate&#xA;cd LibreTranslate&#xA;pip install -e .&#xA;libretranslate [args]&#xA;&#xA;# Or&#xA;python main.py [args]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open a web browser to &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run with Docker&lt;/h3&gt; &#xA;&lt;p&gt;Linux/MacOS: &lt;code&gt;./run.sh [args]&lt;/code&gt; Windows: &lt;code&gt;run.bat [args]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then open a web browser to &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build with Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build [--build-arg with_models=true] -t libretranslate .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run the Docker image in a complete offline environment, you need to add the &lt;code&gt;--build-arg with_models=true&lt;/code&gt; parameter. Then the language models are downloaded during the build process of the image. Otherwise these models get downloaded on the first run of the image/container.&lt;/p&gt; &#xA;&lt;p&gt;Run the built image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it -p 5000:5000 libretranslate [args]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or build and run using &lt;code&gt;docker-compose&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Feel free to change the &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/raw/main/docker-compose.yml&#34;&gt;&lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/a&gt; file to adapt it to your deployment needs, or use an extra &lt;code&gt;docker-compose.prod.yml&lt;/code&gt; file for your deployment configuration.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The models are stored inside the container under &lt;code&gt;/home/libretranslate/.local/share&lt;/code&gt; and &lt;code&gt;/home/libretranslate/.local/cache&lt;/code&gt;. Feel free to use volumes if you do not want to redownload the models when the container is destroyed. To update the models, use the &lt;code&gt;--update-models&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;CUDA&lt;/h3&gt; &#xA;&lt;p&gt;You can use hardware acceleration to speed up translations on a GPU machine with CUDA 11.2 and &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;nvidia-docker&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;Run this version with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f docker-compose.cuda.yml up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Arguments&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Argument&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Env. name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--host&lt;/td&gt; &#xA;   &lt;td&gt;Set host to bind the server to&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_HOST&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--port&lt;/td&gt; &#xA;   &lt;td&gt;Set port to bind the server to&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_PORT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--char-limit&lt;/td&gt; &#xA;   &lt;td&gt;Set character limit&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No limit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_CHAR_LIMIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--req-limit&lt;/td&gt; &#xA;   &lt;td&gt;Set maximum number of requests per minute per client&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No limit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_REQ_LIMIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--req-limit-storage&lt;/td&gt; &#xA;   &lt;td&gt;Storage URI to use for request limit data storage. See &lt;a href=&#34;https://flask-limiter.readthedocs.io/en/stable/configuration.html&#34;&gt;Flask Limiter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;memory://&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_REQ_LIMIT_STORAGE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--batch-limit&lt;/td&gt; &#xA;   &lt;td&gt;Set maximum number of texts to translate in a batch request&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No limit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_BATCH_LIMIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--ga-id&lt;/td&gt; &#xA;   &lt;td&gt;Enable Google Analytics on the API client page by providing an ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No tracking&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_GA_ID&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--debug&lt;/td&gt; &#xA;   &lt;td&gt;Enable debug environment&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_DEBUG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--ssl&lt;/td&gt; &#xA;   &lt;td&gt;Whether to enable SSL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_SSL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--frontend-language-source&lt;/td&gt; &#xA;   &lt;td&gt;Set frontend default language - source&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_FRONTEND_LANGUAGE_SOURCE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--frontend-language-target&lt;/td&gt; &#xA;   &lt;td&gt;Set frontend default language - target&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;es&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_FRONTEND_LANGUAGE_TARGET&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--frontend-timeout&lt;/td&gt; &#xA;   &lt;td&gt;Set frontend translation timeout&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;500&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_FRONTEND_TIMEOUT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--api-keys&lt;/td&gt; &#xA;   &lt;td&gt;Enable API keys database for per-user rate limits lookup&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Don&#39;t use API keys&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_API_KEYS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--api-keys-db-path&lt;/td&gt; &#xA;   &lt;td&gt;Use a specific path inside the container for the local database. Can be absolute or relative&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;db/api_keys.db&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_API_KEYS_DB_PATH&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--api-keys-remote&lt;/td&gt; &#xA;   &lt;td&gt;Use this remote endpoint to query for valid API keys instead of using the local database&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Use local API key database&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_API_KEYS_REMOTE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--get-api-key-link&lt;/td&gt; &#xA;   &lt;td&gt;Show a link in the UI where to direct users to get an API key&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Don&#39;t show a link&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_GET_API_KEY_LINK&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--require-api-key-origin&lt;/td&gt; &#xA;   &lt;td&gt;Require use of an API key for programmatic access to the API, unless the request origin matches this domain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No restrictions on domain origin&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_REQUIRE_API_KEY_ORIGIN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--load-only&lt;/td&gt; &#xA;   &lt;td&gt;Set available languages&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;all from argostranslate&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_LOAD_ONLY&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--threads&lt;/td&gt; &#xA;   &lt;td&gt;Set number of threads&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_THREADS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--suggestions&lt;/td&gt; &#xA;   &lt;td&gt;Allow user suggestions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_SUGGESTIONS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--disable-files-translation&lt;/td&gt; &#xA;   &lt;td&gt;Disable files translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_DISABLE_FILES_TRANSLATION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--disable-web-ui&lt;/td&gt; &#xA;   &lt;td&gt;Disable web ui&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_DISABLE_WEB_UI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--update-models&lt;/td&gt; &#xA;   &lt;td&gt;Update language models at startup&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_UPDATE_MODELS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--metrics&lt;/td&gt; &#xA;   &lt;td&gt;Enable the /metrics endpoint for exporting &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; usage metrics&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_METRICS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--metrics-auth-token&lt;/td&gt; &#xA;   &lt;td&gt;Protect the /metrics endpoint by allowing only clients that have a valid Authorization Bearer token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;No auth&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LT_METRICS_AUTH_TOKEN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that each argument has an equivalent environment variable that can be used instead. The env. variables overwrite the default values but have lower priority than the command arguments and are particularly useful if used with Docker. The environment variable names are the upper-snake-case of the equivalent command argument&#39;s name with a &lt;code&gt;LT&lt;/code&gt; prefix.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;h3&gt;Software&lt;/h3&gt; &#xA;&lt;p&gt;If you installed with pip:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -U libretranslate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using docker:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker pull libretranslate/libretranslate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Language Models&lt;/h3&gt; &#xA;&lt;p&gt;Start the program with the &lt;code&gt;--update-models&lt;/code&gt; argument. For example: &lt;code&gt;libretranslate --update-models&lt;/code&gt; or &lt;code&gt;./run.sh --update-models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively you can also run the &lt;code&gt;install_models.py&lt;/code&gt; script.&lt;/p&gt; &#xA;&lt;h2&gt;Run with WSGI and Gunicorn&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gunicorn&#xA;gunicorn --bind 0.0.0.0:5000 &#39;wsgi:app&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass application arguments directly to Gunicorn via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gunicorn --bind 0.0.0.0:5000 &#39;wsgi:app(api_keys=True)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run with Kubernetes&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://jmrobles.medium.com/libretranslate-your-own-translation-service-on-kubernetes-b46c3e1af630&#34;&gt;&#34;LibreTranslate: your own translation service on Kubernetes&#34; by JM Robles&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Manage API Keys&lt;/h2&gt; &#xA;&lt;p&gt;LibreTranslate supports per-user limit quotas, e.g. you can issue API keys to users so that they can enjoy higher requests limits per minute (if you also set &lt;code&gt;--req-limit&lt;/code&gt;). By default all users are rate-limited based on &lt;code&gt;--req-limit&lt;/code&gt;, but passing an optional &lt;code&gt;api_key&lt;/code&gt; parameter to the REST endpoints allows a user to enjoy higher request limits.&lt;/p&gt; &#xA;&lt;p&gt;To use API keys simply start LibreTranslate with the &lt;code&gt;--api-keys&lt;/code&gt; option. If you modified the API keys database path with the option &lt;code&gt;--api-keys-db-path&lt;/code&gt;, you must specify the path with the same argument flag when using the &lt;code&gt;ltmanage keys&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;h3&gt;Add New Keys&lt;/h3&gt; &#xA;&lt;p&gt;To issue a new API key with 120 requests per minute limits:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ltmanage keys add 120&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you changed the API keys database path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ltmanage keys --api-keys-db-path path/to/db/dbName.db add 120&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Remove Keys&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ltmanage keys remove &amp;lt;api-key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;View Keys&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ltmanage keys&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prometheus Metrics&lt;/h2&gt; &#xA;&lt;p&gt;LibreTranslate has Prometheus &lt;a href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;&gt;exporter&lt;/a&gt; capabilities when you pass the &lt;code&gt;--metrics&lt;/code&gt; argument at startup (disabled by default). When metrics are enabled, a &lt;code&gt;/metrics&lt;/code&gt; endpoint is mounted on the instance:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://localhost:5000/metrics&#34;&gt;http://localhost:5000/metrics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# HELP libretranslate_http_requests_in_flight Multiprocess metric&#xA;# TYPE libretranslate_http_requests_in_flight gauge&#xA;libretranslate_http_requests_in_flight{api_key=&#34;&#34;,endpoint=&#34;/translate&#34;,request_ip=&#34;127.0.0.1&#34;} 0.0&#xA;# HELP libretranslate_http_request_duration_seconds Multiprocess metric&#xA;# TYPE libretranslate_http_request_duration_seconds summary&#xA;libretranslate_http_request_duration_seconds_count{api_key=&#34;&#34;,endpoint=&#34;/translate&#34;,request_ip=&#34;127.0.0.1&#34;,status=&#34;200&#34;} 0.0&#xA;libretranslate_http_request_duration_seconds_sum{api_key=&#34;&#34;,endpoint=&#34;/translate&#34;,request_ip=&#34;127.0.0.1&#34;,status=&#34;200&#34;} 0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then configure &lt;code&gt;prometheus.yml&lt;/code&gt; to read the metrics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scrape_configs:&#xA;  - job_name: &#34;libretranslate&#34;&#xA;    &#xA;    # Needed only if you use --metrics-auth-token&#xA;    #authorization:&#xA;      #credentials: &#34;mytoken&#34;&#xA;    &#xA;    static_configs:&#xA;      - targets: [&#34;localhost:5000&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To secure the &lt;code&gt;/metrics&lt;/code&gt; endpoint you can also use &lt;code&gt;--metrics-auth-token mytoken&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use Gunicorn, make sure to create a directory for storing multiprocess data metrics and set &lt;code&gt;PROMETHEUS_MULTIPROC_DIR&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p /tmp/prometheus_data&#xA;rm /tmp/prometheus_data/*&#xA;export PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_data &#xA;gunicorn -c gunicorn_conf.py --bind 0.0.0.0:5000 &#39;wsgi:app(metrics=True)&#39; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Language Bindings&lt;/h2&gt; &#xA;&lt;p&gt;You can use the LibreTranslate API using the following bindings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rust: &lt;a href=&#34;https://github.com/DefunctLizard/libretranslate-rs&#34;&gt;https://github.com/DefunctLizard/libretranslate-rs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Node.js: &lt;a href=&#34;https://github.com/franciscop/translate&#34;&gt;https://github.com/franciscop/translate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;.Net: &lt;a href=&#34;https://github.com/sigaloid/LibreTranslate.Net&#34;&gt;https://github.com/sigaloid/LibreTranslate.Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go: &lt;a href=&#34;https://github.com/SnakeSel/libretranslate&#34;&gt;https://github.com/SnakeSel/libretranslate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://github.com/argosopentech/LibreTranslate-py&#34;&gt;https://github.com/argosopentech/LibreTranslate-py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PHP: &lt;a href=&#34;https://github.com/jefs42/libretranslate&#34;&gt;https://github.com/jefs42/libretranslate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++: &lt;a href=&#34;https://github.com/argosopentech/LibreTranslate-cpp&#34;&gt;https://github.com/argosopentech/LibreTranslate-cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Swift: &lt;a href=&#34;https://github.com/wacumov/libretranslate&#34;&gt;https://github.com/wacumov/libretranslate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Unix: &lt;a href=&#34;https://github.com/argosopentech/LibreTranslate-sh&#34;&gt;https://github.com/argosopentech/LibreTranslate-sh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Shell: &lt;a href=&#34;https://github.com/Hayao0819/Hayao-Tools/tree/master/libretranslate-sh&#34;&gt;https://github.com/Hayao0819/Hayao-Tools/tree/master/libretranslate-sh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java: &lt;a href=&#34;https://github.com/suuft/libretranslate-java&#34;&gt;https://github.com/suuft/libretranslate-java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Discourse Plugin&lt;/h2&gt; &#xA;&lt;p&gt;You can use this &lt;a href=&#34;https://github.com/LibreTranslate/discourse-translator&#34;&gt;discourse translator plugin&lt;/a&gt; to translate &lt;a href=&#34;https://discourse.org&#34;&gt;Discourse&lt;/a&gt; topics. To install it simply modify &lt;code&gt;/var/discourse/containers/app.yml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;## Plugins go here&#xA;## see https://meta.discourse.org/t/19157 for details&#xA;hooks:&#xA;  after_code:&#xA;    - exec:&#xA;        cd: $home/plugins&#xA;        cmd:&#xA;          - git clone https://github.com/discourse/docker_manager.git&#xA;          - git clone https://github.com/LibreTranslate/discourse-translator&#xA;&#x9;  ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then issue &lt;code&gt;./launcher rebuild app&lt;/code&gt;. From the Discourse&#39;s admin panel then select &#34;LibreTranslate&#34; as a translation provider and set the relevant endpoint configurations.&lt;/p&gt; &#xA;&lt;h2&gt;Mobile Apps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitlab.com/BeowuIf/libretranslater&#34;&gt;LibreTranslater&lt;/a&gt; is an Android app &lt;a href=&#34;https://play.google.com/store/apps/details?id=de.beowulf.libretranslater&#34;&gt;available on the Play Store&lt;/a&gt; and &lt;a href=&#34;https://f-droid.org/packages/de.beowulf.libretranslater/&#34;&gt;in the F-Droid store&lt;/a&gt; that uses the LibreTranslate API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/viktorkalyniuk/LiTranslate-iOS&#34;&gt;LiTranslate&lt;/a&gt; is an iOS app &lt;a href=&#34;https://apps.apple.com/us/app/litranslate/id1644385339&#34;&gt;available on the App Store&lt;/a&gt; that uses the LibreTranslate API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web browser&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://minbrowser.org/&#34;&gt;minbrowser&lt;/a&gt; is a web browser with &lt;a href=&#34;https://github.com/argosopentech/argos-translate/discussions/158#discussioncomment-1141551&#34;&gt;integrated LibreTranslate support&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A LibreTranslate Firefox addon is &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/issues/55&#34;&gt;currently a work in progress&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Mirrors&lt;/h2&gt; &#xA;&lt;p&gt;This is a list of public LibreTranslate instances, some require an API key. If you want to add a new URL, please open a pull request.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;API Key Required&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://libretranslate.com&#34;&gt;libretranslate.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portal.libretranslate.com&#34;&gt;Get API Key&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://libretranslate.de&#34;&gt;libretranslate.de&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://translate.argosopentech.com/&#34;&gt;translate.argosopentech.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://translate.api.skitzen.com/&#34;&gt;translate.api.skitzen.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://translate.fortytwo-it.com/&#34;&gt;translate.fortytwo-it.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://translate.terraprint.co/&#34;&gt;translate.terraprint.co&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lt.vern.cc&#34;&gt;lt.vern.cc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TOR/i2p Mirrors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;API Key Required&lt;/th&gt; &#xA;   &lt;th&gt;Payment Link&lt;/th&gt; &#xA;   &lt;th&gt;Cost&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://lt.vernccvbvyi5qhfzyqengccj7lkove6bjot2xhh5kajhwvidqafczrad.onion/&#34;&gt;lt.vernccvbvyi5qhfzyqengccj7lkove6bjot2xhh5kajhwvidqafczrad.onion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://vernf45n7mxwqnp5riaax7p67pwcl7wcefdcnqqvim7ckdx4264a.b32.i2p/&#34;&gt;lt.vern.i2p&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Adding New Languages&lt;/h2&gt; &#xA;&lt;p&gt;To add new languages you first need to train an Argos Translate model. See &lt;a href=&#34;https://odysee.com/@argosopentech:7/training-an-Argos-Translate-model-tutorial-2022:2?r=DMnK7NqdPNHRCfwhmKY9LPow3PqVUUgw&#34;&gt;this video&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;First you need to collect data, for example from &lt;a href=&#34;http://opus.nlpl.eu/&#34;&gt;Opus&lt;/a&gt;, then you need to add the data to &lt;a href=&#34;https://github.com/argosopentech/argos-train/raw/master/data-index.json&#34;&gt;data-index.json&lt;/a&gt; in the &lt;a href=&#34;https://github.com/argosopentech/argos-train&#34;&gt;Argos Train&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Help us by opening a pull request!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A docker image (thanks &lt;a href=&#34;https://github.com/vemonet&#34;&gt;@vemonet&lt;/a&gt; !)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto-detect input language (thanks &lt;a href=&#34;https://github.com/vemonet&#34;&gt;@vemonet&lt;/a&gt; !)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; User authentication / tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Language bindings for every computer language&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://community.libretranslate.com/t/the-best-way-to-train-models/172&#34;&gt;Improved translations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Can I use your API server at libretranslate.com for my application in production?&lt;/h3&gt; &#xA;&lt;p&gt;In short, no. &lt;a href=&#34;https://portal.libretranslate.com&#34;&gt;You need to buy an API key&lt;/a&gt;. You can always run LibreTranslate for free on your own server of course.&lt;/p&gt; &#xA;&lt;h3&gt;Can I use LibreTranslate behind a reverse proxy, like Apache2 or Caddy?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, here are config examples for Apache2 and Caddy that redirect a subdomain (with HTTPS certificate) to LibreTranslate running on a docker at localhost.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo docker run -ti --rm -p 127.0.0.1:5000:5000 libretranslate/libretranslate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can remove &lt;code&gt;127.0.0.1&lt;/code&gt; on the above command if you want to be able to access it from &lt;code&gt;domain.tld:5000&lt;/code&gt;, in addition to &lt;code&gt;subdomain.domain.tld&lt;/code&gt; (this can be helpful to determine if there is an issue with Apache2 or the docker container).&lt;/p&gt; &#xA;&lt;p&gt;Add &lt;code&gt;--restart unless-stopped&lt;/code&gt; if you want this docker to start on boot, unless manually stopped.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Apache config&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;Replace [YOUR_DOMAIN] with your full domain; for example, &lt;code&gt;translate.domain.tld&lt;/code&gt; or &lt;code&gt;libretranslate.domain.tld&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Remove &lt;code&gt;#&lt;/code&gt; on the ErrorLog and CustomLog lines to log requests.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-ApacheConf&#34;&gt;#Libretranslate&#xA;&#xA;#Redirect http to https&#xA;&amp;lt;VirtualHost *:80&amp;gt;&#xA;    ServerName http://[YOUR_DOMAIN]&#xA;    Redirect / https://[YOUR_DOMAIN]&#xA;    # ErrorLog ${APACHE_LOG_DIR}/error.log&#xA;    # CustomLog ${APACHE_LOG_DIR}/tr-access.log combined&#xA; &amp;lt;/VirtualHost&amp;gt;&#xA;&#xA;#https&#xA;&amp;lt;VirtualHost *:443&amp;gt;&#xA;    ServerName https://[YOUR_DOMAIN]&#xA;    &#xA;    ProxyPass / http://127.0.0.1:5000/&#xA;    ProxyPassReverse / http://127.0.0.1:5000/&#xA;    ProxyPreserveHost On&#xA;   &#xA;    SSLEngine on&#xA;    SSLCertificateFile /etc/letsencrypt/live/[YOUR_DOMAIN]/fullchain.pem&#xA;    SSLCertificateKeyFile /etc/letsencrypt/live/[YOUR_DOMAIN]/privkey.pem&#xA;    SSLCertificateChainFile /etc/letsencrypt/live/[YOUR_DOMAIN]/fullchain.pem&#xA;    &#xA;    # ErrorLog ${APACHE_LOG_DIR}/tr-error.log&#xA;    # CustomLog ${APACHE_LOG_DIR}/tr-access.log combined&#xA;&amp;lt;/VirtualHost&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Add this to an existing site config, or a new file in &lt;code&gt;/etc/apache2/sites-available/new-site.conf&lt;/code&gt; and run &lt;code&gt;sudo a2ensite new-site.conf&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;To get a HTTPS subdomain certificate, install &lt;code&gt;certbot&lt;/code&gt; (snap), run &lt;code&gt;sudo certbot certonly --manual --preferred-challenges dns&lt;/code&gt; and enter your information (with &lt;code&gt;subdomain.domain.tld&lt;/code&gt; as the domain). Add a DNS TXT record with your domain registrar when asked. This will save your certificate and key to &lt;code&gt;/etc/letsencrypt/live/{subdomain.domain.tld}/&lt;/code&gt;. Alternatively, comment the SSL lines out if you don&#39;t want to use HTTPS.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Caddy config&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;Replace [YOUR_DOMAIN] with your full domain; for example, &lt;code&gt;translate.domain.tld&lt;/code&gt; or &lt;code&gt;libretranslate.domain.tld&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Caddyfile&#34;&gt;#Libretranslate&#xA;[YOUR_DOMAIN] {&#xA;  reverse_proxy localhost:5000&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Add this to an existing Caddyfile or save it as &lt;code&gt;Caddyfile&lt;/code&gt; in any directory and run &lt;code&gt;sudo caddy reload&lt;/code&gt; in that same directory.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;NGINX config&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;Replace [YOUR_DOMAIN] with your full domain; for example, &lt;code&gt;translate.domain.tld&lt;/code&gt; or &lt;code&gt;libretranslate.domain.tld&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Remove &lt;code&gt;#&lt;/code&gt; on the &lt;code&gt;access_log&lt;/code&gt; and &lt;code&gt;error_log&lt;/code&gt; lines to disable logging.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-NginxConf&#34;&gt;server {&#xA;  listen 80;&#xA;  server_name [YOUR_DOMAIN];&#xA;  return 301 https://$server_name$request_uri;&#xA;}&#xA;&#xA;server {&#xA;  listen 443 http2 ssl;&#xA;  server_name [YOUR_DOMAIN];&#xA;&#xA;  #access_log off;&#xA;  #error_log off;&#xA;  &#xA;  # SSL Section&#xA;  ssl_certificate /etc/letsencrypt/live/[YOUR_DOMAIN]/fullchain.pem;&#xA;  ssl_certificate_key /etc/letsencrypt/live/[YOUR_DOMAIN]/privkey.pem;&#xA;  &#xA;  ssl_protocols TLSv1.2 TLSv1.3;&#xA;&#xA;  # Using the recommended cipher suite from: https://wiki.mozilla.org/Security/Server_Side_TLS&#xA;  ssl_ciphers &#39;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384&#39;;&#xA;&#xA;  ssl_session_timeout 10m;&#xA;  ssl_session_cache shared:MozSSL:10m;  # about 40000 sessions&#xA;  ssl_session_tickets off;&#xA;&#xA;  # Specifies a curve for ECDHE ciphers.&#xA;  ssl_ecdh_curve prime256v1;&#xA;  # Server should determine the ciphers, not the client&#xA;  ssl_prefer_server_ciphers on;&#xA;&#xA;  &#xA;  # Header section&#xA;  add_header Strict-Transport-Security  &#34;max-age=31536000; includeSubDomains; preload&#34; always;&#xA;  add_header Referrer-Policy            &#34;strict-origin&#34; always;&#xA;&#xA;  add_header X-Frame-Options            &#34;SAMEORIGIN&#34;    always;&#xA;  add_header X-XSS-Protection           &#34;1; mode=block&#34; always;&#xA;  add_header X-Content-Type-Options     &#34;nosniff&#34;       always;&#xA;  add_header X-Download-Options         &#34;noopen&#34;        always;&#xA;  add_header X-Robots-Tag               &#34;none&#34;          always;&#xA;&#xA;  add_header Feature-Policy             &#34;microphone &#39;none&#39;; camera &#39;none&#39;; geolocation &#39;none&#39;;&#34;  always;&#xA;  # Newer header but not everywhere supported&#xA;  add_header Permissions-Policy         &#34;microphone=(), camera=(), geolocation=()&#34; always;&#xA;&#xA;  # Remove X-Powered-By, which is an information leak&#xA;  fastcgi_hide_header X-Powered-By;&#xA;&#xA;  # Do not send nginx server header&#xA;  server_tokens off;&#xA;  &#xA;  # GZIP Section&#xA;  gzip on;&#xA;  gzip_disable &#34;msie6&#34;;&#xA;&#xA;  gzip_vary on;&#xA;  gzip_proxied any;&#xA;  gzip_comp_level 6;&#xA;  gzip_buffers 16 8k;&#xA;  gzip_http_version 1.1;&#xA;  gzip_min_length 256;&#xA;  gzip_types text/xml text/javascript font/ttf font/eot font/otf application/x-javascript application/atom+xml application/javascript application/json application/manifest+json application/rss+xml application/x-web-app-manifest+json application/xhtml+xml application/xml image/svg+xml image/x-icon text/css text/plain;&#xA;&#xA;  location / {&#xA;      proxy_pass http://127.0.0.1:5000/;&#xA;      proxy_set_header Host $http_host;&#xA;      proxy_set_header X-Real-IP $remote_addr;&#xA;      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&#xA;      proxy_set_header X-Forwarded-Proto $scheme;&#xA;      client_max_body_size 0;&#xA;  }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Add this to an existing NGINX config or save it as &lt;code&gt;libretranslate&lt;/code&gt; in the &lt;code&gt;/etc/nginx/site-enabled&lt;/code&gt; directory and run &lt;code&gt;sudo nginx -s reload&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This work is largely possible thanks to &lt;a href=&#34;https://github.com/argosopentech/argos-translate&#34;&gt;Argos Translate&lt;/a&gt;, which powers the translation engine.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.en.html&#34;&gt;GNU Affero General Public License v3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Trademark&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/LibreTranslate/LibreTranslate/raw/main/TRADEMARK.md&#34;&gt;Trademark Guidelines&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>