<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-29T01:42:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JingyunLiang/SwinIR</title>
    <updated>2023-05-29T01:42:31Z</updated>
    <id>tag:github.com,2023-05-29:/JingyunLiang/SwinIR</id>
    <link href="https://github.com/JingyunLiang/SwinIR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SwinIR: Image Restoration Using Swin Transformer (official repository)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SwinIR: Image Restoration Using Swin Transformer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jingyunliang.github.io&#34;&gt;Jingyun Liang&lt;/a&gt;, &lt;a href=&#34;https://www.jiezhangcao.com/&#34;&gt;Jiezhang Cao&lt;/a&gt;, &lt;a href=&#34;https://vision.ee.ethz.ch/people-details.MjYzMjMw.TGlzdC8zMjg5LC0xOTcxNDY1MTc4.html&#34;&gt;Guolei Sun&lt;/a&gt;, &lt;a href=&#34;https://cszn.github.io/&#34;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=TwMib_QAAAAJ&amp;amp;hl=en&#34;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&#34;http://people.ee.ethz.ch/~timofter/&#34;&gt;Radu Timofte&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Computer Vision Lab, ETH Zurich&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.10257&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/SwinIR?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/JingyunLiang/SwinIR/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=jingyunliang/SwinIR&#34; alt=&#34;visitors&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.ai/jingyunliang/swinir&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Replicate&amp;amp;message=Demo and Docker Image&amp;amp;color=blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://playtorch.dev/snack/@playtorch/swinir/&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/playtorch/raw/main/website/static/assets/playtorch_badge.svg?sanitize=true&#34; alt=&#34;PlayTorch Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/SwinIR&#34;&gt;Gradio Web Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository is the official PyTorch implementation of SwinIR: Image Restoration Using Shifted Window Transformer (&lt;a href=&#34;https://arxiv.org/pdf/2108.10257.pdf&#34;&gt;arxiv&lt;/a&gt;, &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;supp&lt;/a&gt;, &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;pretrained models&lt;/a&gt;, &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;visual results&lt;/a&gt;). SwinIR achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; in&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bicubic/lighweight/real-world image SR&lt;/li&gt; &#xA; &lt;li&gt;grayscale/color image denoising&lt;/li&gt; &#xA; &lt;li&gt;grayscale/color JPEG compression artifact reduction&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸš€&lt;/span&gt; &lt;span&gt;ðŸš€&lt;/span&gt; &lt;span&gt;ðŸš€&lt;/span&gt; &lt;strong&gt;News&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aug. 16, 2022&lt;/strong&gt;: Add PlayTorch Demo on running the real-world image SR model on mobile devices &lt;a href=&#34;https://playtorch.dev/snack/@playtorch/swinir/&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/playtorch/raw/main/website/static/assets/playtorch_badge.svg?sanitize=true&#34; alt=&#34;PlayTorch Demo&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aug. 01, 2022&lt;/strong&gt;: Add pretrained models and results on JPEG compression artifact reduction for color images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jun. 10, 2022&lt;/strong&gt;: See our work on video restoration &lt;span&gt;ðŸ”¥&lt;/span&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; &lt;a href=&#34;https://github.com/JingyunLiang/VRT&#34;&gt;VRT: A Video Restoration Transformer&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/VRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/VRT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/VRT/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/JingyunLiang/VRT/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/JingyunLiang/RVRT&#34;&gt;RVRT: Recurrent Video Restoration Transformer&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/RVRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/RVRT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/RVRT/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/JingyunLiang/RVRT/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; for video SR, video deblurring, video denoising, video frame interpolation and space-time video SR.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sep. 07, 2021&lt;/strong&gt;: We provide an interactive online Colab demo for real-world image SR &lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; for comparison with &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;the first practical degradation model BSRGAN (ICCV2021) &lt;img src=&#34;https://img.shields.io/github/stars/cszn/BSRGAN?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; and a recent model RealESRGAN. Try to super-resolve your own images on Colab!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Real-World Image (x4)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;BSRGAN, ICCV2021&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;SwinIR (ours)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;SwinIR-Large (ours)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/ETH_LR.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/ETH_BSRGAN.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/ETH_realESRGAN.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/ETH_SwinIR.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/ETH_SwinIR-L.png&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/OST_009_crop_LR.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/OST_009_crop_BSRGAN.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/OST_009_crop_realESRGAN.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/OST_009_crop_SwinIR.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/OST_009_crop_SwinIR-L.png&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Aug. 26, 2021&lt;/strong&gt;: See our recent work on &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;real-world image SR: a pratical degrdation model BSRGAN, ICCV2021&lt;/a&gt; &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cszn/BSRGAN?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Aug. 26, 2021&lt;/strong&gt;: See our recent work on &lt;a href=&#34;https://github.com/JingyunLiang/HCFlow&#34;&gt;generative modelling of image SR and image rescaling: normalizing-flow-based HCFlow, ICCV2021&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/HCFlow&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/HCFlow?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/cdb3fef89ebd174eaa43794accb6f59d/hcflow-demo-on-x8-face-image-sr.ipynb&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/cdb3fef89ebd174eaa43794accb6f59d/hcflow-demo-on-x8-face-image-sr.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Aug. 26, 2021&lt;/strong&gt;: See our recent work on &lt;a href=&#34;https://github.com/JingyunLiang/MANet&#34;&gt;blind SR: spatially variant kernel estimation (MANet, ICCV2021)&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/MANet&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/MANet?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/4ed2524d6e08343710ee408a4d997e1c/manet-demo-on-spatially-variant-kernel-estimation.ipynb&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/4ed2524d6e08343710ee408a4d997e1c/manet-demo-on-spatially-variant-kernel-estimation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/JingyunLiang/FKP&#34;&gt;unsupervised kernel estimation (FKP, CVPR2021)&lt;/a&gt; &lt;a href=&#34;https://github.com/JingyunLiang/FKP&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JingyunLiang/FKP?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14~0.45dB, while the total number of parameters can be reduced by up to 67%.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;/blockquote&gt; &#xA;&lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/SwinIR_archi.png&#34;&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Contents&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/#Training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/#Testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/#Results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/#Citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/#License-and-Acknowledgement&#34;&gt;License and Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Used training and testing sets can be downloaded as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Training Set&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Testing Set&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Visual Results&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;classical/lightweight image SR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/DIV2K.tar&#34;&gt;DIV2K&lt;/a&gt; (800 training images) or DIV2K +&lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar&#34;&gt;Flickr2K&lt;/a&gt; (2650 images)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Set5 + Set14 + BSD100 + Urban100 + Manga109 &lt;a href=&#34;https://drive.google.com/drive/folders/1B3DJGQKB6eNdwuQIhdskA64qUuVKLZ9u&#34;&gt;download all&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;here&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;real-world image SR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinIR-M (middle size): &lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/DIV2K.tar&#34;&gt;DIV2K&lt;/a&gt; (800 training images) +&lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar&#34;&gt;Flickr2K&lt;/a&gt; (2650 images) + &lt;a href=&#34;https://openmmlab.oss-cn-hangzhou.aliyuncs.com/datasets/OST_dataset.zip&#34;&gt;OST&lt;/a&gt; (&lt;a href=&#34;https://drive.google.com/drive/folders/1iZfzAxAwOpeutz27HC56_y5RNqnsPPKr&#34;&gt;alternative link&lt;/a&gt;, 10324 images for sky,water,grass,mountain,building,plant,animal) &lt;br&gt; SwinIR-L (large size): DIV2K + Flickr2K + OST + &lt;a href=&#34;http://ivc.uwaterloo.ca/database/WaterlooExploration/exploration_database_and_code.rar&#34;&gt;WED&lt;/a&gt;(4744 images) + &lt;a href=&#34;https://drive.google.com/drive/folders/1tZUcXDBeOibC6jcMCtgRRz67pzrAHeHL&#34;&gt;FFHQ&lt;/a&gt; (first 2000 images, face) + Manga109 (manga) + &lt;a href=&#34;https://universityofadelaide.box.com/shared/static/py5uwlfyyytbb2pxzq9czvu6fuqbjdh8.zip&#34;&gt;SCUT-CTW1500&lt;/a&gt; (first 100 training images, texts) &lt;br&gt;&lt;br&gt; *&lt;strong&gt;We use the pionnerring practical degradation model from &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;BSRGAN, ICCV2021 &lt;img src=&#34;https://img.shields.io/github/stars/cszn/BSRGAN?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/RealSRSet+5images.zip&#34;&gt;RealSRSet+5images&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;here&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;color/grayscale image denoising&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/DIV2K.tar&#34;&gt;DIV2K&lt;/a&gt; (800 training images) + &lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar&#34;&gt;Flickr2K&lt;/a&gt; (2650 images) + &lt;a href=&#34;http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz&#34;&gt;BSD500&lt;/a&gt; (400 training&amp;amp;testing images) + &lt;a href=&#34;http://ivc.uwaterloo.ca/database/WaterlooExploration/exploration_database_and_code.rar&#34;&gt;WED&lt;/a&gt;(4744 images) &lt;br&gt;&lt;br&gt; *BSD68/BSD100 images are not used in training.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;grayscale: Set12 + BSD68 + Urban100 &lt;br&gt; color: CBSD68 + Kodak24 + McMaster + Urban100 &lt;a href=&#34;https://github.com/cszn/FFDNet/tree/master/testsets&#34;&gt;download all&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;here&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;grayscale/color JPEG compression artifact reduction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/DIV2K.tar&#34;&gt;DIV2K&lt;/a&gt; (800 training images) + &lt;a href=&#34;https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar&#34;&gt;Flickr2K&lt;/a&gt; (2650 images) + &lt;a href=&#34;http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz&#34;&gt;BSD500&lt;/a&gt; (400 training&amp;amp;testing images) + &lt;a href=&#34;http://ivc.uwaterloo.ca/database/WaterlooExploration/exploration_database_and_code.rar&#34;&gt;WED&lt;/a&gt;(4744 images)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;grayscale: Classic5 +LIVE1 &lt;a href=&#34;https://github.com/cszn/DnCNN/tree/master/testsets&#34;&gt;download all&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;here&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;| Task                 | Training Set | Testing Set|        Pretrained Model and Visual Results of SwinIR     | &#xA;| :---                 | :---:        |     :---:      |:---:      |&#xA;| image denoising (real)      | [SIDD-Medium-sRGB](https://www.eecs.yorku.ca/~kamel/sidd/dataset.php) (320 images, [preprocess]()) + [RENOIR](http://ani.stat.fsu.edu/~abarbu/Renoir.html) (221 images, [preprocess](https://github.com/zsyOAOA/DANet/blob/master/datasets/preparedata/Renoir_big2small_all.py)) + [Poly](https://github.com/csjunxu/PolyU-Real-World-Noisy-Images-Dataset) (40 images in ./OriginalImages) |    [SIDD validation set](https://drive.google.com/drive/folders/1S44fHXaVxAYW3KLNxK41NYCnyX9S79su) (1280 patches, identical to official [.mat](https://www.eecs.yorku.ca/~kamel/sidd/benchmark.php) version) +  [DND](https://noise.visinf.tu-darmstadt.de/downloads/) (pre-defined 100 patches of 50 images, [online eval](https://noise.visinf.tu-darmstadt.de/submit/)) + [Nam](https://www.dropbox.com/s/24kds7c436i5i11/real_image_noise_dataset.zip?dl=0) (random 100 patches of 17 images, [preprocess](https://github.com/zsyOAOA/DANet/blob/master/datasets/preparedata/Nam_patch_prepare.py))|[download model]() [download results]() |&#xA;| image deblurring (synthetic)   | [GoPro](https://drive.google.com/drive/folders/1AsgIP9_X0bg0olu2-1N6karm2x15cJWE) (2103 training images)  |  [GoPro](https://drive.google.com/drive/folders/1a2qKfXWpNuTGOm2-Jex8kfNSzYJLbqkf) (1111 images) + [HIDE](https://drive.google.com/drive/folders/1nRsTXj4iTUkTvBhTcGg8cySK8nd3vlhK) (2050 images) + [RealBlur_J](https://drive.google.com/drive/folders/1KYtzeKCiDRX9DSvC-upHrCqvC4sPAiJ1) (real blur, 980 images) + [RealBlur_R](https://drive.google.com/drive/folders/1EwDoajf5nStPIAcU4s9rdc8SPzfm3tW1) (real blur, 980 images) | [download model]() [download results]()|&#xA;| image deraining (synthetic)  | [Multiple datasets](https://drive.google.com/drive/folders/1Hnnlc5kI0v9_BtfMytC2LR5VpLAFZtVe) (13711 training images, see Table 1 of [MPRNet](https://github.com/swz30/MPRNet) for details.)  |  Rain100H (100 images) + Rain100L (100 images) + Test100 (100 images) + Test2800 (2800 images) + Test1200 (1200 images), [download all](https://drive.google.com/drive/folders/1PDWggNh8ylevFmrjo-JEvlmqsDlWWvZs)  | [download model]() [download results]()|&#xA;&#xA;Note: above datasets may come from the official release or some awesome collections ([BasicSR](https://github.com/xinntao/BasicSR), [MPRNet](https://github.com/swz30/MPRNet)).&#xA;&#xA;--&gt; &#xA;&lt;p&gt;The training code is at &lt;a href=&#34;https://github.com/cszn/KAIR/raw/master/docs/README_SwinIR.md&#34;&gt;KAIR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Testing (without preparing datasets)&lt;/h2&gt; &#xA;&lt;p&gt;For your convience, we provide some example datasets (~20Mb) in &lt;code&gt;/testsets&lt;/code&gt;. If you just want codes, downloading &lt;code&gt;models/network_swinir.py&lt;/code&gt;, &lt;code&gt;utils/util_calculate_psnr_ssim.py&lt;/code&gt; and &lt;code&gt;main_test_swinir.py&lt;/code&gt; is enough. Following commands will download &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;pretrained models&lt;/a&gt; &lt;strong&gt;automatically&lt;/strong&gt; and put them in &lt;code&gt;model_zoo/swinir&lt;/code&gt;. &lt;strong&gt;&lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;All visual results of SwinIR can be downloaded here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide an &lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb&#34;&gt;online Colab demo for real-world image SR &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; for comparison with &lt;a href=&#34;https://github.com/cszn/BSRGAN&#34;&gt;the first practical degradation model BSRGAN (ICCV2021) &lt;img src=&#34;https://img.shields.io/github/stars/cszn/BSRGAN?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; and a recent model &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;RealESRGAN&lt;/a&gt;. Try to test your own images on Colab!&lt;/p&gt; &#xA;&lt;p&gt;We provide a PlayTorch demo &lt;a href=&#34;https://playtorch.dev/snack/@playtorch/swinir/&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/playtorch/raw/main/website/static/assets/playtorch_badge.svg?sanitize=true&#34; alt=&#34;PlayTorch Demo&#34;&gt;&lt;/a&gt; for real-world image SR to showcase how to run the SwinIR model in mobile application built with React Native.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 001 Classical Image Super-Resolution (middle size)&#xA;# Note that --training_patch_size is just used to differentiate two different settings in Table 2 of the paper. Images are NOT tested patch by patch.&#xA;# (setting1: when model is trained on DIV2K and with training_patch_size=48)&#xA;python main_test_swinir.py --task classical_sr --scale 2 --training_patch_size 48 --model_path model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth --folder_lq testsets/Set5/LR_bicubic/X2 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 3 --training_patch_size 48 --model_path model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x3.pth --folder_lq testsets/Set5/LR_bicubic/X3 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 4 --training_patch_size 48 --model_path model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth --folder_lq testsets/Set5/LR_bicubic/X4 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 8 --training_patch_size 48 --model_path model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x8.pth --folder_lq testsets/Set5/LR_bicubic/X8 --folder_gt testsets/Set5/HR&#xA;&#xA;# (setting2: when model is trained on DIV2K+Flickr2K and with training_patch_size=64)&#xA;python main_test_swinir.py --task classical_sr --scale 2 --training_patch_size 64 --model_path model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth --folder_lq testsets/Set5/LR_bicubic/X2 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 3 --training_patch_size 64 --model_path model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x3.pth --folder_lq testsets/Set5/LR_bicubic/X3 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 4 --training_patch_size 64 --model_path model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x4.pth --folder_lq testsets/Set5/LR_bicubic/X4 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task classical_sr --scale 8 --training_patch_size 64 --model_path model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x8.pth --folder_lq testsets/Set5/LR_bicubic/X8 --folder_gt testsets/Set5/HR&#xA;&#xA;&#xA;# 002 Lightweight Image Super-Resolution (small size)&#xA;python main_test_swinir.py --task lightweight_sr --scale 2 --model_path model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x2.pth --folder_lq testsets/Set5/LR_bicubic/X2 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task lightweight_sr --scale 3 --model_path model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x3.pth --folder_lq testsets/Set5/LR_bicubic/X3 --folder_gt testsets/Set5/HR&#xA;python main_test_swinir.py --task lightweight_sr --scale 4 --model_path model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x4.pth --folder_lq testsets/Set5/LR_bicubic/X4 --folder_gt testsets/Set5/HR&#xA;&#xA;&#xA;# 003 Real-World Image Super-Resolution (use --tile 400 if you run out-of-memory)&#xA;# (middle size)&#xA;python main_test_swinir.py --task real_sr --scale 4 --model_path model_zoo/swinir/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth --folder_lq testsets/RealSRSet+5images --tile&#xA;&#xA;# (larger size + trained on more datasets)&#xA;python main_test_swinir.py --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/RealSRSet+5images&#xA;&#xA;&#xA;# 004 Grayscale Image Deoising (middle size)&#xA;python main_test_swinir.py --task gray_dn --noise 15 --model_path model_zoo/swinir/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth --folder_gt testsets/Set12&#xA;python main_test_swinir.py --task gray_dn --noise 25 --model_path model_zoo/swinir/004_grayDN_DFWB_s128w8_SwinIR-M_noise25.pth --folder_gt testsets/Set12&#xA;python main_test_swinir.py --task gray_dn --noise 50 --model_path model_zoo/swinir/004_grayDN_DFWB_s128w8_SwinIR-M_noise50.pth --folder_gt testsets/Set12&#xA;&#xA;&#xA;# 005 Color Image Deoising (middle size)&#xA;python main_test_swinir.py --task color_dn --noise 15 --model_path model_zoo/swinir/005_colorDN_DFWB_s128w8_SwinIR-M_noise15.pth --folder_gt testsets/McMaster&#xA;python main_test_swinir.py --task color_dn --noise 25 --model_path model_zoo/swinir/005_colorDN_DFWB_s128w8_SwinIR-M_noise25.pth --folder_gt testsets/McMaster&#xA;python main_test_swinir.py --task color_dn --noise 50 --model_path model_zoo/swinir/005_colorDN_DFWB_s128w8_SwinIR-M_noise50.pth --folder_gt testsets/McMaster&#xA;&#xA;&#xA;# 006 JPEG Compression Artifact Reduction (middle size, using window_size=7 because JPEG encoding uses 8x8 blocks)&#xA;# grayscale&#xA;python main_test_swinir.py --task jpeg_car --jpeg 10 --model_path model_zoo/swinir/006_CAR_DFWB_s126w7_SwinIR-M_jpeg10.pth --folder_gt testsets/classic5&#xA;python main_test_swinir.py --task jpeg_car --jpeg 20 --model_path model_zoo/swinir/006_CAR_DFWB_s126w7_SwinIR-M_jpeg20.pth --folder_gt testsets/classic5&#xA;python main_test_swinir.py --task jpeg_car --jpeg 30 --model_path model_zoo/swinir/006_CAR_DFWB_s126w7_SwinIR-M_jpeg30.pth --folder_gt testsets/classic5&#xA;python main_test_swinir.py --task jpeg_car --jpeg 40 --model_path model_zoo/swinir/006_CAR_DFWB_s126w7_SwinIR-M_jpeg40.pth --folder_gt testsets/classic5&#xA;&#xA;# color&#xA;python main_test_swinir.py --task color_jpeg_car --jpeg 10 --model_path model_zoo/swinir/006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg10.pth --folder_gt testsets/LIVE1&#xA;python main_test_swinir.py --task color_jpeg_car --jpeg 20 --model_path model_zoo/swinir/006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg20.pth --folder_gt testsets/LIVE1&#xA;python main_test_swinir.py --task color_jpeg_car --jpeg 30 --model_path model_zoo/swinir/006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg30.pth --folder_gt testsets/LIVE1&#xA;python main_test_swinir.py --task color_jpeg_car --jpeg 40 --model_path model_zoo/swinir/006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg40.pth --folder_gt testsets/LIVE1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;We achieved state-of-the-art performance on classical/lightweight/real-world image SR, grayscale/color image denoising and JPEG compression artifact reduction. Detailed results can be found in the &lt;a href=&#34;https://arxiv.org/abs/2108.10257&#34;&gt;paper&lt;/a&gt;. All visual results of SwinIR can be downloaded &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Classical Image Super-Resolution (click me)&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/classic_image_sr.png&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/classic_image_sr_visual.png&#34;&gt; &lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;More detailed comparison between SwinIR and a representative CNN-based model RCAN (classical image SR, X4)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Method&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training Set&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training time &lt;br&gt; (8GeForceRTX2080Ti &lt;br&gt; batch=32, iter=500k)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Y-PSNR/Y-SSIM &lt;br&gt; on Manga109&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Run time &lt;br&gt; (1GeForceRTX2080Ti,&lt;br&gt; on 256x256 LR image)*&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;#FLOPs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Testing memory&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;RCAN&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIV2K&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.6 days&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.22/0.9173&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.180s&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;15.6M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;850.6G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;593.1M&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;SwinIR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIV2K&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.8 days&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.67/0.9226&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.539s&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;11.9M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;788.6G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;986.8M&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;* We re-test the runtime when the GPU is idle. We refer to the evluation code &lt;a href=&#34;https://github.com/cszn/KAIR/raw/master/main_challenge_sr.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Results on DIV2K-validation (100 images)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Training Set&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;scale factor&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;PSNR (RGB)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;PSNR (Y)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;SSIM (RGB)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;SSIM (Y)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K (800 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;35.25&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.77&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9423&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9500&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K+Flickr2K (2650 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;35.34&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.86&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9430&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9507&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K (800 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.50&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;32.97&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8832&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8965&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K+Flickr2K (2650 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.63&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;33.10&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8854&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8985&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K (800 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29.48&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;30.94&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8311&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8492&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;DIV2K+Flickr2K (2650 images)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29.63&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.08&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8347&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8523&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Lightweight Image Super-Resolution&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/lightweight_image_sr.png&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Real-World Image Super-Resolution&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/real_world_image_sr.png&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Grayscale Image Deoising&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/gray_image_denoising.png&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Color Image Deoising&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/color_image_denoising.png&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;JPEG Compression Artifact Reduction&lt;/summary&gt; &#xA; &lt;p&gt;on grayscale images&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; src=&#34;https://raw.githubusercontent.com/JingyunLiang/SwinIR/main/figs/jepg_compress_artfact_reduction.png&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;on color images&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Training Set&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;quality factor&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;PSNR (RGB)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;PSNR-B (RGB)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;SSIM (RGB)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LIVE1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;28.06&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;27.76&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8089&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LIVE1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;30.45&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29.97&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.8741&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LIVE1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;30&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.82&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31.24&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9018&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LIVE1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;32.75&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;32.12&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.9174&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liang2021swinir,&#xA;  title={SwinIR: Image Restoration Using Swin Transformer},&#xA;  author={Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},&#xA;  journal={arXiv preprint arXiv:2108.10257},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License and Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the Apache 2.0 license. The codes are based on &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer&#34;&gt;Swin Transformer&lt;/a&gt; and &lt;a href=&#34;https://github.com/cszn/KAIR&#34;&gt;KAIR&lt;/a&gt;. Please also follow their licenses. Thanks for their awesome works.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ShishirPatil/gorilla</title>
    <updated>2023-05-29T01:42:31Z</updated>
    <id>tag:github.com,2023-05-29:/ShishirPatil/gorilla</id>
    <link href="https://github.com/ShishirPatil/gorilla" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gorilla: An API store for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/h1&gt; &#xA;&lt;p&gt;By Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez (&lt;a href=&#34;https://shishirpatil.github.io/gorilla/&#34;&gt;Project Website&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/ShishirPatil/gorilla/raw/gh-pages/assets/img/logo.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸ—ž&lt;/span&gt; Checkout our paper!&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.15334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2305.15334-%3CCOLOR%3E.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸ‘‹&lt;/span&gt; Join our Discord!&lt;/strong&gt; &lt;a href=&#34;https://discord.gg/3apqwwME&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1111172801899012102?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=green&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸš€&lt;/span&gt; Try Gorilla in 60s&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Gorilla&lt;/code&gt; enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;ðŸš€&lt;/span&gt; [05/27] Released the first Gorilla model! &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0&#34;&gt;&lt;span&gt;ðŸ¤—&lt;/span&gt;&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; [05/27] We released the APIZoo contribution guide for community API contributions!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; [05/25] We release the APIBench dataset and the evaluation code of Gorilla!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Use the following command to install dependencies. These are only for evaluation, additional dependencies for inference and training are in their respective sub-directories.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n gorilla python=3.8&#xA;conda activate gorilla&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have included prompts and responces for the APIBench with and without retrievers along with the Abstract Syntax Tree (AST) matching evaluation script at &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Organization&lt;/h2&gt; &#xA;&lt;p&gt;Our repository organization is shown below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;data&lt;/code&gt; folder contains all the evaluation APIs &lt;code&gt;(APIBench)&lt;/code&gt; and the community contributed APIs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;eval&lt;/code&gt; folder contains all our evaluation code as well as the Gorilla outputs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;inference&lt;/code&gt; folder contains all the inference code for running Gorilla locally.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span style=&#34;color:hr&#34;&gt;[Coming Soon!]&lt;/span&gt; The &lt;code&gt;train&lt;/code&gt; folder contains all the training code associated with Gorilla finetuning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For our dataset collections, all the 1640 API documentation is in &lt;code&gt;data/api&lt;/code&gt;. We also include the &lt;code&gt;APIBench&lt;/code&gt; dataset created by self-instruct in &lt;code&gt;data/apibench&lt;/code&gt;. For evaluation, we convert this into a LLM-friendly chat format, and the questions are in &lt;code&gt;eval/eval-data/questions&lt;/code&gt;, and the corresponding responces are in &lt;code&gt;eval/eval-data/responses&lt;/code&gt;. We have also included the evaluation scripts are in &lt;code&gt;eval/eval-scripts&lt;/code&gt;. This would be entirely sufficient to train Gorilla yourself, and reproduce our results. Please see &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt; for the details on how to use our evaluation pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, to make it more accessible, we have also released the model weights &lt;code&gt;gorilla-7b-hf-v0&lt;/code&gt; that lets you invoke over 925 HuggingFace APIs. We will release TorchHub, TensorFlow, all three combined with generic chat capability and community contributed APIs as soon as we can scale infrastructure. You can run it locally from isntructions in the &lt;code&gt;inference/&lt;/code&gt; sub-directory, or we also provide a hosted Gorilla chat completion API! If you have any suggestions, or if you run into any issues please feel free to reach out to us either through Discord or email or raise a Github issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gorilla&#xA;â”œâ”€â”€ data&#xA;â”‚   â”œâ”€â”€ api (TF/HF/TH APIs used in generating apibench)&#xA;â”‚   â”‚   â”œâ”€â”€ {api_name}_api.jsonl&#xA;â”‚   â”œâ”€â”€ apibench (Evaluating LLM models) v-1.0&#xA;â”‚   â”‚   â”œâ”€â”€ {api_name}_train.jsonl, {api_name}_eval.jsonl&#xA;|   |â”€â”€ apizoo (Contributed by the community - evolving)&#xA;â”‚   |   â”œâ”€â”€ username1.json&#xA;â”‚   â”‚   â”œâ”€â”€ username2.json&#xA;â”‚   â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ eval&#xA;â”‚   â”œâ”€â”€ README.md&#xA;â”‚   â”œâ”€â”€ get_llm_responses.py&#xA;â”‚   â”œâ”€â”€ eval-scripts&#xA;â”‚   â”‚   â”œâ”€â”€ ast_eval_{api_name}.py&#xA;â”‚   â”œâ”€â”€ eval-data&#xA;â”‚   â”‚   â”œâ”€â”€ questions&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ API name&#xA;â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ questions_{api_name}_{eval_metric}.jsonl&#xA;â”‚   â”‚   â”œâ”€â”€ responses&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ API name&#xA;â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ responses_{api_name}_Gorilla_FT_{eval_metric}.jsonl&#xA;â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ responses_{api_name}_Gorilla_RT_{eval_metric}.jsonl&#xA;â”œâ”€â”€ inference&#xA;â”œâ”€â”€ train (Coming Soon!)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing Your API&lt;/h2&gt; &#xA;&lt;p&gt;We aim to build an open-source, one-stop-shop for all APIs, LLMs can interact with! Any suggestions and contributions are welcome! Please see the details on &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/data/README.md&#34;&gt;how to contribute&lt;/a&gt;. THIS WILL ALWAYS REMAIN OPEN SOURCE.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ(s)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;I would like to use Gorilla commercially. Is there going to be a Apache 2.0 licensed version?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Yes! We are actively working on it. We will release a Gorilla model with Apache 2.0 license by Jun 5. Please stay tuned, and let us know if you are interested.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Can we use Gorilla with Langchain, Toolformer, AutoGPT etc?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Absolutely! You&#39;ve highlighted a great aspect of our tools. Gorilla is an end-to-end model, specifically tailored to serve correct API calls without requiring any additional coding. It&#39;s designed to work as part of a wider ecosystem and can be flexibly integrated with other tools.&lt;/p&gt; &#xA;&lt;p&gt;Langchain, is a versatile developer tool. Its &#34;agents&#34; can efficiently swap in any LLM, Gorilla included, making it a highly adaptable solution for various needs.&lt;/p&gt; &#xA;&lt;p&gt;AutoGPT, on the other hand, concentrates on the art of prompting GPT series models. It&#39;s worth noting that Gorilla, as a fully fine-tuned model, consistently shows remarkable accuracy, and lowers hallucination, outperforming GPT-4 in making specific API calls.&lt;/p&gt; &#xA;&lt;p&gt;Now, when it comes to ToolFormer, Toolformer zeroes in on a select set of tools, providing specialized functionalities. Gorilla, in contrast, has the capacity to manage thousands of API calls, offering a broader coverage over a more extensive range of tools.&lt;/p&gt; &#xA;&lt;p&gt;The beauty of these tools truly shines when they collaborate, complementing each other&#39;s strengths and capabilities to create an even more powerful and comprehensive solution. This is where your contribution can make a difference. We enthusiastically welcome any inputs to further refine and enhance these tools.&lt;/p&gt; &#xA;&lt;h2&gt;Project Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;In the immediate future, we plan to release the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset and Eval Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Opening up the APIZoo for contributions from community&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Hosted Gorilla LLM chat for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release weights for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Run Gorilla LLM locally [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Release weights for all APIs from APIBench [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Release a commercially usable, Apache 2.0 licensed Gorilla model [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Train a model with first batch of community contributed APIs from APIZoo [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Release training code [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Train SOTA Gorilla LLM with expanded APIBench and APIZoo &lt;span&gt;ðŸš€&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Propose a new task you would like to work on &lt;span&gt;ðŸ¤©&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Gorilla or APIBench, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@article{patil2023gorilla,&#xA;  title={Gorilla: Large Language Model Connected with Massive APIs},&#xA;  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},&#xA;  year={2023},&#xA;  journal={arXiv preprint arXiv:2305.15334},&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>MineDojo/Voyager</title>
    <updated>2023-05-29T01:42:31Z</updated>
    <id>tag:github.com,2023-05-29:/MineDojo/Voyager</id>
    <link href="https://github.com/MineDojo/Voyager" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open-Ended Embodied Agent with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Voyager: An Open-Ended Embodied Agent with Large Language Models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://voyager.minedojo.org/&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34;&gt;[Arxiv]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.16291.pdf&#34;&gt;[PDF]&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DrJimFan/status/1662115266933972993?s=20&#34;&gt;[Tweet]&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/MineDojo/Voyager&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.9-blue.svg?sanitize=true&#34; alt=&#34;Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MineDojo/Voyager/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/MineDojo/Voyager&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/MineDojo/Voyager/assets/25460983/ce29f45b-43a5-4399-8fd8-5dd105fd64f2&#34;&gt;https://github.com/MineDojo/Voyager/assets/25460983/ce29f45b-43a5-4399-8fd8-5dd105fd64f2&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MineDojo/Voyager/main/images/pull.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agentâ€™s abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3Ã— more unique items, travels 2.3Ã— longer distances, and unlocks key tech tree milestones up to 15.3Ã— faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, we provide Voyager code. This codebase is under &lt;a href=&#34;https://raw.githubusercontent.com/MineDojo/Voyager/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Voyager requires Python â‰¥ 3.9 and Node.js â‰¥ 16.13.0. We have tested on Ubuntu 20.04, Windows 11, and macOS. You need to follow the instructions below to install Voyager.&lt;/p&gt; &#xA;&lt;h2&gt;Python Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/MineDojo/Voyager&#xA;cd Voyager&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Node.js Install&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the Python dependencies, you need to install the following Node.js packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd voyager/env/mineflayer&#xA;npm install -g npx&#xA;npm install&#xA;cd mineflayer-collectblock&#xA;npx tsc&#xA;cd ..&#xA;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Minecraft Instance Install&lt;/h2&gt; &#xA;&lt;p&gt;Voyager depends on Minecraft game. You need to install Minecraft game and set up a Minecraft instance.&lt;/p&gt; &#xA;&lt;p&gt;Follow the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/MineDojo/Voyager/main/installation/minecraft_instance_install.md&#34;&gt;Minecraft Login Tutorial&lt;/a&gt; to set up your Minecraft Instance.&lt;/p&gt; &#xA;&lt;h2&gt;Fabric Mods Install&lt;/h2&gt; &#xA;&lt;p&gt;You need to install fabric mods to support all the features in Voyager. Remember to use the correct Fabric version of all the mods.&lt;/p&gt; &#xA;&lt;p&gt;Follow the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/MineDojo/Voyager/main/installation/fabric_mods_install.md&#34;&gt;Fabric Mods Install&lt;/a&gt; to install the mods.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;Voyager uses OpenAI&#39;s GPT-4 as the language model. You need to have an OpenAI API key to use Voyager. You can get one from &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After the installation process, you can run Voyager by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from voyager import Voyager&#xA;&#xA;# you can also use mc_port instead of azure_login, but azure_login is highly recommended&#xA;azure_login = {&#xA;    &#34;client_id&#34;: &#34;YOUR_CLIENT_ID&#34;,&#xA;    &#34;redirect_url&#34;: &#34;https://127.0.0.1/auth-response&#34;,&#xA;    &#34;secret_value&#34;: &#34;[OPTIONAL] YOUR_SECRET_VALUE&#34;,&#xA;    &#34;version&#34;: &#34;fabric-loader-0.14.18-1.19&#34;, # the version Voyager is tested on&#xA;}&#xA;openai_api_key = &#34;YOUR_API_KEY&#34;&#xA;&#xA;voyager = Voyager(&#xA;    azure_login=azure_login,&#xA;    openai_api_key=openai_api_key,&#xA;)&#xA;&#xA;# start lifelong learning&#xA;voyager.learn()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are running with &lt;code&gt;Azure Login&lt;/code&gt; for the first time, it will ask you to follow the command line instruction to generate a config file.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;Azure Login&lt;/code&gt;, you also need to select the world and open the world to LAN by yourself. After you run &lt;code&gt;voyager.learn()&lt;/code&gt; the game will pop up soon, you need to: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Select &lt;code&gt;Singleplayer&lt;/code&gt; and press &lt;code&gt;Create New World&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Set Game Mode to &lt;code&gt;Creative&lt;/code&gt; and Difficulty to &lt;code&gt;Peaceful&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;After the world is created, press &lt;code&gt;Esc&lt;/code&gt; key and press &lt;code&gt;Open to LAN&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Select &lt;code&gt;Allow cheats: ON&lt;/code&gt; and press &lt;code&gt;Start LAN World&lt;/code&gt;. You will see the bot join the world soon.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;If you have any questions, please check our &lt;a href=&#34;https://raw.githubusercontent.com/MineDojo/Voyager/main/FAQ.md&#34;&gt;FAQ&lt;/a&gt; first before opening an issue.&lt;/p&gt; &#xA;&lt;h1&gt;Paper and Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2023voyager,&#xA;  title   = {Voyager: An Open-Ended Embodied Agent with Large Language Models},&#xA;  author  = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},&#xA;  year    = {2023},&#xA;  journal = {arXiv preprint arXiv: Arxiv-2305.16291}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Disclaimer: This project is strictly for research purposes, and not an official product from NVIDIA.&lt;/p&gt;</summary>
  </entry>
</feed>