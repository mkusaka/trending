<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-21T01:35:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hkchengrex/XMem</title>
    <updated>2022-07-21T01:35:07Z</updated>
    <id>tag:github.com,2022-07-21:/hkchengrex/XMem</id>
    <link href="https://github.com/hkchengrex/XMem" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ECCV 2022] XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XMem&lt;/h1&gt; &#xA;&lt;h2&gt;Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hkchengrex.github.io/&#34;&gt;Ho Kei Cheng&lt;/a&gt;, &lt;a href=&#34;https://www.alexander-schwing.de/&#34;&gt;Alexander Schwing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;University of Illinois Urbana-Champaign&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.07115&#34;&gt;[arXiv]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2207.07115.pdf&#34;&gt;[PDF]&lt;/a&gt; &lt;a href=&#34;https://hkchengrex.github.io/XMem/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1RXK5QsUo2-CnOiy5AOSjoZggPVHOPh1m?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Handling long-term occlusion:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/7107196/177921527-7a1bd593-2162-4598-9adf-f2112763fccf.mp4&#34;&gt;https://user-images.githubusercontent.com/7107196/177921527-7a1bd593-2162-4598-9adf-f2112763fccf.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Very-long video; masked layer insertion:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/7107196/179089789-3d69adea-0405-4c83-ac28-45f59fe1e1c1.mp4&#34;&gt;https://user-images.githubusercontent.com/7107196/179089789-3d69adea-0405-4c83-ac28-45f59fe1e1c1.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://www.youtube.com/watch?v=q5Xr0F4a0iU&#34;&gt;https://www.youtube.com/watch?v=q5Xr0F4a0iU&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Out-of-domain case:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/7107196/177920383-161f1da1-33f9-48b3-b8b2-09e450432e2b.mp4&#34;&gt;https://user-images.githubusercontent.com/7107196/177920383-161f1da1-33f9-48b3-b8b2-09e450432e2b.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: かぐや様は告らせたい ～天才たちの恋愛頭脳戦～ Ep.3; A1 Pictures&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/FAILURE_CASES.md&#34;&gt;[Failure Cases]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handle very long videos with limited GPU memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Quite fast. Expect ~20 FPS even with long videos (hardware dependent).&lt;/li&gt; &#xA; &lt;li&gt;Come with a GUI (modified from &lt;a href=&#34;https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN&#34;&gt;MiVOS&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/RESULTS.md&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/DEMO.md&#34;&gt;Interactive GUI demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/#traininginference&#34;&gt;Training/inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imgur.com/ToE2frx.jpg&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We frame Video Object Segmentation (VOS), first and foremost, as a &lt;em&gt;memory&lt;/em&gt; problem. Prior works mostly use a single type of feature memory. This can be in the form of network weights (i.e., online learning), last frame segmentation (e.g., MaskTrack), spatial hidden representation (e.g., Conv-RNN-based methods), spatial-attentional features (e.g., STM, STCN, AOT), or some sort of long-term compact features (e.g., AFB-URR).&lt;/p&gt; &#xA;&lt;p&gt;Methods with a short memory span are not robust to changes while those with a large memory bank are subject to a catastrophic increase in computation and GPU memory usage. Attempts at long-term attentional VOS like AFB-URR compress features eagerly as soon as they are generated, leading to a loss of feature resolution.&lt;/p&gt; &#xA;&lt;p&gt;Our method is inspired by the Atkinson-Shiffrin human memory model that has a &lt;em&gt;sensory memory&lt;/em&gt;, a &lt;em&gt;working memory&lt;/em&gt;, and a &lt;em&gt;long-term memory&lt;/em&gt;. These memory stores have different temporal scales and complement each other in our memory reading mechanism. It performs well in both short-term and long-term video datasets, handling videos with more than 10,000 frames with ease.&lt;/p&gt; &#xA;&lt;h3&gt;Training/inference&lt;/h3&gt; &#xA;&lt;p&gt;First, install the required python packages and datasets following &lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/GETTING_STARTED.md&#34;&gt;GETTING_STARTED.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For training, see &lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/TRAINING.md&#34;&gt;TRAINING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For inference, see &lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/XMem/main/docs/INFERENCE.md&#34;&gt;INFERENCE.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Please cite our paper if you find this repo useful!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{cheng2022xmem,&#xA;  title={{XMem}: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model},&#xA;  author={Cheng, Ho Kei and Alexander G. Schwing},&#xA;  booktitle={ECCV},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Related projects that this paper is developed upon:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{cheng2021stcn,&#xA;  title={Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation},&#xA;  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},&#xA;  booktitle={NeurIPS},&#xA;  year={2021}&#xA;}&#xA;&#xA;@inproceedings{cheng2021mivos,&#xA;  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},&#xA;  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},&#xA;  booktitle={CVPR},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We use f-BRS in the interactive demo: &lt;a href=&#34;https://github.com/saic-vul/fbrs_interactive_segmentation&#34;&gt;https://github.com/saic-vul/fbrs_interactive_segmentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And if you want to cite the datasets:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;p&gt;bibtex&lt;/p&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shi2015hierarchicalECSSD,&#xA;  title={Hierarchical image saliency detection on extended CSSD},&#xA;  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},&#xA;  booktitle={TPAMI},&#xA;  year={2015},&#xA;}&#xA;&#xA;@inproceedings{wang2017DUTS,&#xA;  title={Learning to Detect Salient Objects with Image-level Supervision},&#xA;  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang &#xA;  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, &#xA;  booktitle={CVPR},&#xA;  year={2017}&#xA;}&#xA;&#xA;@inproceedings{FSS1000,&#xA;  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},&#xA;  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},&#xA;  booktitle={CVPR},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{zeng2019towardsHRSOD,&#xA;  title = {Towards High-Resolution Salient Object Detection},&#xA;  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},&#xA;  booktitle = {ICCV},&#xA;  year = {2019}&#xA;}&#xA;&#xA;@inproceedings{cheng2020cascadepsp,&#xA;  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},&#xA;  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},&#xA;  booktitle={CVPR},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{xu2018youtubeVOS,&#xA;  title={Youtube-vos: A large-scale video object segmentation benchmark},&#xA;  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},&#xA;  booktitle = {ECCV},&#xA;  year={2018}&#xA;}&#xA;&#xA;@inproceedings{perazzi2016benchmark,&#xA;  title={A benchmark dataset and evaluation methodology for video object segmentation},&#xA;  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},&#xA;  booktitle={CVPR},&#xA;  year={2016}&#xA;}&#xA;&#xA;@inproceedings{denninger2019blenderproc,&#xA;  title={BlenderProc},&#xA;  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},&#xA;  booktitle={arXiv:1911.01911},&#xA;  year={2019}&#xA;}&#xA;&#xA;@inproceedings{shapenet2015,&#xA;  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},&#xA;  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},&#xA;  booktitle   = {arXiv:1512.03012},&#xA;  year        = {2015}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Contact: &lt;a href=&#34;mailto:hkchengrex@gmail.com&#34;&gt;hkchengrex@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RimoChan/unvcode</title>
    <updated>2022-07-21T01:35:07Z</updated>
    <id>tag:github.com,2022-07-21:/RimoChan/unvcode</id>
    <link href="https://github.com/RimoChan/unvcode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;【幼女Code】反和谐超级武器！&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;【幼女Code】反和谐超级武器！&lt;/h1&gt; &#xA;&lt;p&gt;你还在因为在群名里加入色图而被QQ改成一个「*」而苦恼吗？你还在因为在红包祝福里写「年轻人好好自慰」而白白花钱吗？&lt;/p&gt; &#xA;&lt;p&gt;快使用&lt;strong&gt;幼女Code&lt;/strong&gt;吧！&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;幼女Code&lt;/strong&gt;使用Librian幼女娱乐中心最新研发的&lt;strong&gt;unvcode&lt;/strong&gt;，可以快速解决你的一切问题！&lt;/p&gt; &#xA;&lt;h2&gt;原理&lt;/h2&gt; &#xA;&lt;p&gt;在unicode中&lt;sub&gt;(注意这不是unvcode)&lt;/sub&gt;，有很多字，它们看起来长得很像，但是它们的ord不一样。&lt;/p&gt; &#xA;&lt;p&gt;这样一来，只要把字符串里原本的字……啊，点到为止，再说下去就不好玩了。&lt;/p&gt; &#xA;&lt;h2&gt;效果&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RimoChan/unvcode/slave/doc/2333.jpg&#34; alt=&#34;./doc/2333.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;幼女Code&lt;/strong&gt;真是太棒了！&lt;/p&gt; &#xA;&lt;h2&gt;在线Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://unvcode.librian.net/&#34;&gt;https://unvcode.librian.net/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;因为JS处理图像很麻烦，所以这是把中间结果打表到代码里的，如果和Python的输出对不齐是正常现象。&lt;/p&gt; &#xA;&lt;h2&gt;接口&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def unvcode(s: str, skip_ascii=True, mse=0.1) -&amp;gt; Tuple[str, Tuple[float, ...]]:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;输入一个字符串，返回改变后的字符串、每个字符被改变后与原本的像素差异&lt;sub&gt;(没变就是None)&lt;/sub&gt;。&lt;/p&gt; &#xA;&lt;p&gt;如果&lt;code&gt;skip_ascii&lt;/code&gt;开启则会跳过ascii字符。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;mse&lt;/code&gt;是字符相似度的阈值。&lt;/p&gt; &#xA;&lt;p&gt;样例:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import unvcode&#xA;s, var = unvcode.unvcode(&#39;Librian幼女娱乐中心开业了，注册即送色图！&#39;)&#xA;print(s) &#xA;print(var) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;输出:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Librian幼⼥娱乐㆗⼼开业了，注册即送⾊图！&#xA;(None, None, None, None, None, None, None, None, 0.0, None, None, 0.009146429779930796, 0.0, None, None, 0.0, None, None, None, None, None, 0.0, None, None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意，这句话看起来的样子取决于你的系统字体，所以我也不知道它会是什么样的……因为有一些字在A字体下看起来是一样的，但是在B字体下看起来就不一样。&lt;/p&gt; &#xA;&lt;p&gt;默认的字体是思源宋体，思源宋体好啊。&lt;br&gt; 如果你要选择字体，比如用微软雅黑，那就&lt;code&gt;unvcode.font = &#39;msyh.ttc&#39;&lt;/code&gt;，顺便一提，我发现&lt;code&gt;YuGothM.ttc&lt;/code&gt;的效果是最好的……&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install unvcode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后在代码里&lt;code&gt;import unvcode&lt;/code&gt;就行了，就是这么简单！&lt;/p&gt; &#xA;&lt;h2&gt;结束&lt;/h2&gt; &#xA;&lt;p&gt;如果你觉得幼女Code对你的工作或学习有所帮助，欢迎给作者送一些幼女过来。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmdetection</title>
    <updated>2022-07-21T01:35:07Z</updated>
    <id>tag:github.com,2022-07-21:/open-mmlab/mmdetection</id>
    <link href="https://github.com/open-mmlab/mmdetection" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Detection Toolbox and Benchmark&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/resources/mmdet-logo.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmdet&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmdet&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmdetection/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmdetection&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmdetection/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmdetection.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmdetection.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmdetection.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmdetection.readthedocs.io/en/stable/&#34;&gt;📘Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmdetection.readthedocs.io/en/stable/get_started.html&#34;&gt;🛠️Installation&lt;/a&gt; | &lt;a href=&#34;https://mmdetection.readthedocs.io/en/stable/model_zoo.html&#34;&gt;👀Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmdetection.readthedocs.io/en/stable/changelog.html&#34;&gt;🆕Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/projects&#34;&gt;🚀Ongoing Projects&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/issues/new/choose&#34;&gt;🤔Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.5+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/12907710/137271636-56ba1cd2-b110-4812-8221-b4c120320aa9.png&#34;&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Major features&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Design&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support of multiple frameworks out of box&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The toolbox directly supports popular and contemporary detection frameworks, &lt;em&gt;e.g.&lt;/em&gt; Faster RCNN, Mask RCNN, RetinaNet, etc.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;High efficiency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron2&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/maskrcnn-benchmark&#34;&gt;maskrcnn-benchmark&lt;/a&gt; and &lt;a href=&#34;https://github.com/TuSimple/simpledet&#34;&gt;SimpleDet&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;State of the art&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The toolbox stems from the codebase developed by the &lt;em&gt;MMDet&lt;/em&gt; team, who won &lt;a href=&#34;http://cocodataset.org/#detection-leaderboard&#34;&gt;COCO Detection Challenge&lt;/a&gt; in 2018, and we keep pushing it forward.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Apart from MMDetection, we also released a library &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;mmcv&lt;/a&gt; for computer vision research, which is heavily depended on by this toolbox.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.25.0&lt;/strong&gt; was released in 1/6/2022:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support dedicated &lt;code&gt;MMDetWandbHook&lt;/code&gt; hook&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/convnext&#34;&gt;ConvNeXt&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ddod&#34;&gt;DDOD&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/solov2&#34;&gt;SOLOv2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/mask2former&#34;&gt;Mask2Former&lt;/a&gt; for instance segmentation&lt;/li&gt; &#xA; &lt;li&gt;Rename &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/mask2former&#34;&gt;config files of Mask2Former&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;p&gt;For compatibility changes between different versions of MMDetection, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/compatibility.md&#34;&gt;compatibility.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/get_started.md/#Installation&#34;&gt;Installation&lt;/a&gt; for installation instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/get_started.md&#34;&gt;get_started.md&lt;/a&gt; for the basic usage of MMDetection. We provide &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/demo/MMDet_Tutorial.ipynb&#34;&gt;colab tutorial&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/demo/MMDet_InstanceSeg_Tutorial.ipynb&#34;&gt;instance segmentation colab tutorial&lt;/a&gt;, and other tutorials for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/1_exist_data_model.md&#34;&gt;with existing dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/2_new_data_model.md&#34;&gt;with new dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/3_exist_data_new_model.md&#34;&gt;with existing dataset_new_model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/config.md&#34;&gt;learn about configs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/customize_dataset.md&#34;&gt;customize_datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/data_pipeline.md&#34;&gt;customize data pipelines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/customize_models.md&#34;&gt;customize_models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/customize_runtime.md&#34;&gt;customize runtime settings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/customize_losses.md&#34;&gt;customize_losses&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/finetune.md&#34;&gt;finetuning models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/pytorch2onnx.md&#34;&gt;export a model to ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/onnx2tensorrt.md&#34;&gt;export ONNX to TRT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/init_cfg.md&#34;&gt;weight initialization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/tutorials/how_to.md&#34;&gt;how to xxx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview of Benchmark and Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/model_zoo.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Architectures&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Object Detection&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Instance Segmentation&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Panoptic Segmentation&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Other&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/fast_rcnn&#34;&gt;Fast R-CNN (ICCV&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/faster_rcnn&#34;&gt;Faster R-CNN (NeurIPS&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/rpn&#34;&gt;RPN (NeurIPS&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ssd&#34;&gt;SSD (ECCV&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/retinanet&#34;&gt;RetinaNet (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/cascade_rcnn&#34;&gt;Cascade R-CNN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/yolo&#34;&gt;YOLOv3 (ArXiv&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/cornernet&#34;&gt;CornerNet (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/grid_rcnn&#34;&gt;Grid R-CNN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/guided_anchoring&#34;&gt;Guided Anchoring (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/fsaf&#34;&gt;FSAF (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/centernet&#34;&gt;CenterNet (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/libra_rcnn&#34;&gt;Libra R-CNN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/tridentnet&#34;&gt;TridentNet (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/fcos&#34;&gt;FCOS (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/reppoints&#34;&gt;RepPoints (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/free_anchor&#34;&gt;FreeAnchor (NeurIPS&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/cascade_rpn&#34;&gt;CascadeRPN (NeurIPS&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/foveabox&#34;&gt;Foveabox (TIP&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/double_heads&#34;&gt;Double-Head R-CNN (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/atss&#34;&gt;ATSS (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/nas_fcos&#34;&gt;NAS-FCOS (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/centripetalnet&#34;&gt;CentripetalNet (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/autoassign&#34;&gt;AutoAssign (ArXiv&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/sabl&#34;&gt;Side-Aware Boundary Localization (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/dynamic_rcnn&#34;&gt;Dynamic R-CNN (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/detr&#34;&gt;DETR (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/paa&#34;&gt;PAA (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/vfnet&#34;&gt;VarifocalNet (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/sparse_rcnn&#34;&gt;Sparse R-CNN (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/yolof&#34;&gt;YOLOF (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/yolox&#34;&gt;YOLOX (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/deformable_detr&#34;&gt;Deformable DETR (ICLR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/tood&#34;&gt;TOOD (ICCV&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ddod&#34;&gt;DDOD (ACM MM&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/mask_rcnn&#34;&gt;Mask R-CNN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/cascade_rcnn&#34;&gt;Cascade Mask R-CNN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ms_rcnn&#34;&gt;Mask Scoring R-CNN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/htc&#34;&gt;Hybrid Task Cascade (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/yolact&#34;&gt;YOLACT (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/instaboost&#34;&gt;InstaBoost (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/solo&#34;&gt;SOLO (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/point_rend&#34;&gt;PointRend (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/detectors&#34;&gt;DetectoRS (ArXiv&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/solov2&#34;&gt;SOLOv2 (NeurIPS&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/scnet&#34;&gt;SCNet (AAAI&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/queryinst&#34;&gt;QueryInst (ICCV&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/mask2former&#34;&gt;Mask2Former (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/panoptic_fpn&#34;&gt;Panoptic FPN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/maskformer&#34;&gt;MaskFormer (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/mask2former&#34;&gt;Mask2Former (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt;  &lt;li&gt;&lt;b&gt;Contrastive Learning&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/selfsup_pretrain&#34;&gt;SwAV (NeurIPS&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/selfsup_pretrain&#34;&gt;MoCo (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/selfsup_pretrain&#34;&gt;MoCov2 (ArXiv&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/ul&gt;  &lt;li&gt;&lt;b&gt;Distillation&lt;/b&gt;&lt;/li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ld&#34;&gt;Localization Distillation (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/lad&#34;&gt;Label Assignment Distillation (WACV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/ul&gt;  &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Components&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;VGG (ICLR&#39;2015)&lt;/li&gt; &#xA;     &lt;li&gt;ResNet (CVPR&#39;2016)&lt;/li&gt; &#xA;     &lt;li&gt;ResNeXt (CVPR&#39;2017)&lt;/li&gt; &#xA;     &lt;li&gt;MobileNetV2 (CVPR&#39;2018)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/hrnet&#34;&gt;HRNet (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/empirical_attention&#34;&gt;Generalized Attention (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/gcnet&#34;&gt;GCNet (ICCVW&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/res2net&#34;&gt;Res2Net (TPAMI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/regnet&#34;&gt;RegNet (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/resnest&#34;&gt;ResNeSt (ArXiv&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/pvt&#34;&gt;PVT (ICCV&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/swin&#34;&gt;Swin (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/pvt&#34;&gt;PVTv2 (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/resnet_strikes_back&#34;&gt;ResNet strikes back (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/efficientnet&#34;&gt;EfficientNet (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/convnext&#34;&gt;ConvNeXt (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/pafpn&#34;&gt;PAFPN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/nas_fpn&#34;&gt;NAS-FPN (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/carafe&#34;&gt;CARAFE (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/fpg&#34;&gt;FPG (ArXiv&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/groie&#34;&gt;GRoIE (ICPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/dyhead&#34;&gt;DyHead (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/ghm&#34;&gt;GHM (AAAI&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/gfl&#34;&gt;Generalized Focal Loss (NeurIPS&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/seesaw_loss&#34;&gt;Seasaw Loss (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py&#34;&gt;OHEM (CVPR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/gn&#34;&gt;Group Normalization (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/dcn&#34;&gt;DCN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/dcnv2&#34;&gt;DCNv2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/gn+ws&#34;&gt;Weight Standardization (ArXiv&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/pisa&#34;&gt;Prime Sample Attention (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/strong_baselines&#34;&gt;Strong Baselines (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/resnet_strikes_back&#34;&gt;Resnet strikes back (ArXiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Some other methods are also supported in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/projects.md&#34;&gt;projects using MMDetection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docs/en/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMDetection. Ongoing projects can be found in out &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/projects&#34;&gt;GitHub Projects&lt;/a&gt;. Welcome community users to participate in these projects. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMDetection is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this toolbox or benchmark in your research, please cite this project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mmdetection,&#xA;  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},&#xA;  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and&#xA;             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and&#xA;             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and&#xA;             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and&#xA;             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong&#xA;             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},&#xA;  journal= {arXiv preprint arXiv:1906.07155},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmdetection/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>