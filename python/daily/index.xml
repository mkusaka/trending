<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-11T01:36:25Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>phidatahq/phidata</title>
    <updated>2024-02-11T01:36:25Z</updated>
    <id>tag:github.com,2024-02-11:/phidatahq/phidata</id>
    <link href="https://github.com/phidatahq/phidata" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build AI Assistants using function calling&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; phidata &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Function calling is all you need &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://python.org/pypi/phidata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/phidata?color=blue&amp;amp;label=version&#34; alt=&#34;version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/phidatahq/phidata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-&gt;=3.9-blue&#34; alt=&#34;pythonversion&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/phidatahq/phidata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://pepy.tech/badge/phidata&#34; alt=&#34;downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/phidatahq/phidata/actions/workflows/build.yml&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://github.com/phidatahq/phidata/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build-status&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;What is phidata?&lt;/h2&gt; &#xA;&lt;p&gt;Phidata is a toolkit for building AI Assistants using function calling.&lt;/p&gt; &#xA;&lt;p&gt;Function calling enables LLMs to achieve tasks by calling functions and intelligently choosing their next step based on the response, just like how humans solve problems.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/phidatahq/phidata/assets/22579644/facb618c-17bd-4ab8-99eb-c4c8309e0f45&#34; alt=&#34;assistants&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Create an &lt;code&gt;Assistant&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Add Tools (functions), Knowledge (vectordb) and Storage (database)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Serve using Streamlit, FastApi or Django to build your AI application&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U phidata&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create an Assistant&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a file &lt;code&gt;assistant.py&lt;/code&gt; and install openai using &lt;code&gt;pip install openai&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.assistant import Assistant&#xA;&#xA;assistant = Assistant(description=&#34;You help people with their health and fitness goals.&#34;)&#xA;assistant.print_response(&#34;Share a quick healthy breakfast recipe.&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;Assistant&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Let it search the web&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.assistant import Assistant&#xA;from phi.tools.duckduckgo import DuckDuckGo&#xA;&#xA;assistant = Assistant(tools=[DuckDuckGo()], show_tool_calls=True)&#xA;assistant.print_response(&#34;Whats happening in France?&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;Assistant&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckduckgo-search&#xA;&#xA;python assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;Checkout these AI apps showcasing the advantage of function calling:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pdf.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PDF AI&lt;/a&gt; that summarizes and answers questions from PDFs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ArXiv AI&lt;/a&gt; that answers questions about ArXiv papers using the ArXiv API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hn.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;HackerNews AI&lt;/a&gt; that interacts with the HN API to summarize stories, users, find out what&#39;s trending, summarize topics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demo.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Demo Streamlit App&lt;/a&gt; serving a PDF, Image and Website Assistant (password: admin)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://api.aidev.run/docs&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Demo FastApi &lt;/a&gt; serving a PDF Assistant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VNoBVR5t1yI&amp;amp;t&#34; title=&#34;Phidata Tutorial&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/VNoBVR5t1yI/0.jpg&#34; alt=&#34;Phidata Tutorial&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;AI Applications&lt;/h2&gt; &#xA;&lt;p&gt;After building an Assistant, serve it using &lt;strong&gt;Streamlit&lt;/strong&gt;, &lt;strong&gt;FastApi&lt;/strong&gt; or &lt;strong&gt;Django&lt;/strong&gt; to build your AI application. Instead of wiring tools manually, phidata provides &lt;strong&gt;pre-built&lt;/strong&gt; templates for AI Apps that you can run locally or deploy to AWS with 1 command. Here&#39;s how they work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create your AI App using a template: &lt;code&gt;phi ws create&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run your app locally: &lt;code&gt;phi ws up&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run your app on AWS: &lt;code&gt;phi ws up prd:aws&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Assistant that calls the HackerNews API&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;api_assistant.py&lt;/code&gt; that can call the HackerNews API to get top stories.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;import httpx&#xA;&#xA;from phi.assistant import Assistant&#xA;&#xA;&#xA;def get_top_hackernews_stories(num_stories: int = 10) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Use this function to get top stories from Hacker News.&#xA;&#xA;    Args:&#xA;        num_stories (int): Number of stories to return. Defaults to 10.&#xA;&#xA;    Returns:&#xA;        str: JSON string of top stories.&#xA;    &#34;&#34;&#34;&#xA;&#xA;    # Fetch top story IDs&#xA;    response = httpx.get(&#39;https://hacker-news.firebaseio.com/v0/topstories.json&#39;)&#xA;    story_ids = response.json()&#xA;&#xA;    # Fetch story details&#xA;    stories = []&#xA;    for story_id in story_ids[:num_stories]:&#xA;        story_response = httpx.get(f&#39;https://hacker-news.firebaseio.com/v0/item/{story_id}.json&#39;)&#xA;        story = story_response.json()&#xA;        if &#34;text&#34; in story:&#xA;            story.pop(&#34;text&#34;, None)&#xA;        stories.append(story)&#xA;    return json.dumps(stories)&#xA;&#xA;assistant = Assistant(tools=[get_top_hackernews_stories], show_tool_calls=True)&#xA;assistant.print_response(&#34;Summarize the top stories on hackernews?&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;api_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python api_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;See it work through the problem&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;╭──────────┬───────────────────────────────────────────────────────────────────╮&#xA;│ Message  │ Summarize the top stories on hackernews?                          │&#xA;├──────────┼───────────────────────────────────────────────────────────────────┤&#xA;│ Response │                                                                   │&#xA;│ (51.1s)  │  • Running: get_top_hackernews_stories(num_stories=5)             │&#xA;│          │                                                                   │&#xA;│          │ Here&#39;s a summary of the top stories on Hacker News:               │&#xA;│          │                                                                   │&#xA;│          │  1 Boeing Whistleblower: Max 9 Production Line Has &#34;Enormous      │&#xA;│          │    Volume of Defects&#34; A whistleblower has revealed that Boeing&#39;s  │&#xA;│          │    Max 9 production line is riddled with an &#34;enormous volume of   │&#xA;│          │    defects,&#34; with instances where bolts were not installed. The   │&#xA;│          │    story has garnered attention with a score of 140. Read more    │&#xA;│          │  2 Arno A. Penzias, 90, Dies; Nobel Physicist Confirmed Big Bang  │&#xA;│          │    Theory Arno A. Penzias, a Nobel Prize-winning physicist known  │&#xA;│          │    for his work that confirmed the Big Bang Theory, has passed    │&#xA;│          │    away at the age of 90. His contributions to science have been  │&#xA;│          │    significant, leading to discussions and tributes in the        │&#xA;│          │    scientific community. The news has a score of 207. Read more   │&#xA;│          │  3 Why the fuck are we templating YAML? (2019) This provocative   │&#xA;│          │    article from 2019 questions the proliferation of YAML          │&#xA;│          │    templating in software, sparking a larger conversation about   │&#xA;│          │    the complexities and potential pitfalls of this practice. With │&#xA;│          │    a substantial score of 149, it remains a hot topic of debate.  │&#xA;│          │    Read more                                                      │&#xA;│          │  4 Forging signed commits on GitHub Researchers have discovered a │&#xA;│          │    method for forging signed commits on GitHub which is causing   │&#xA;│          │    concern within the tech community about the implications for   │&#xA;│          │    code security and integrity. The story has a current score of  │&#xA;│          │    94. Read more                                                  │&#xA;│          │  5 Qdrant, the Vector Search Database, raised $28M in a Series A  │&#xA;│          │    round Qdrant, a company specializing in vector search          │&#xA;│          │    databases, has successfully raised $28 million in a Series A   │&#xA;│          │    funding round. This financial milestone indicates growing      │&#xA;│          │    interest and confidence in their technology. The story has     │&#xA;│          │    attracted attention with a score of 55. Read more              │&#xA;╰──────────┴───────────────────────────────────────────────────────────────────╯&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Assistant that analyzes data using SQL&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;The &lt;code&gt;DuckDbAssistant&lt;/code&gt; can perform data analysis using SQL queries.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;data_assistant.py&lt;/code&gt; and install duckdb using &lt;code&gt;pip install duckdb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;from phi.assistant.duckdb import DuckDbAssistant&#xA;&#xA;duckdb_assistant = DuckDbAssistant(&#xA;    semantic_model=json.dumps({&#xA;        &#34;tables&#34;: [&#xA;            {&#xA;                &#34;name&#34;: &#34;movies&#34;,&#xA;                &#34;description&#34;: &#34;Contains information about movies from IMDB.&#34;,&#xA;                &#34;path&#34;: &#34;https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv&#34;,&#xA;            }&#xA;        ]&#xA;    }),&#xA;)&#xA;&#xA;duckdb_assistant.print_response(&#34;What is the average rating of movies? Show me the SQL.&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;data_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python data_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;See it work through the problem&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;INFO     Running: SHOW TABLES&#xA;INFO     Running: CREATE TABLE IF NOT EXISTS &#39;movies&#39;&#xA;         AS SELECT * FROM&#xA;         &#39;https://phidata-public.s3.amazonaws.com/demo_&#xA;         data/IMDB-Movie-Data.csv&#39;&#xA;INFO     Running: DESCRIBE movies&#xA;INFO     Running: SELECT AVG(Rating) AS average_rating&#xA;         FROM movies&#xA;╭──────────┬────────────────────────────────────────────────────────╮&#xA;│ Message  │ What is the average rating of movies? Show me the SQL. │&#xA;├──────────┼────────────────────────────────────────────────────────┤&#xA;│ Response │ The average rating of movies in the dataset is 6.72.   │&#xA;│ (7.6s)   │                                                        │&#xA;│          │ Here is the SQL query used to calculate the average    │&#xA;│          │ rating:                                                │&#xA;│          │                                                        │&#xA;│          │                                                        │&#xA;│          │  SELECT AVG(Rating) AS average_rating                  │&#xA;│          │  FROM movies;                                          │&#xA;│          │                                                        │&#xA;╰──────────┴────────────────────────────────────────────────────────╯&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Assistant that runs python code&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;The &lt;code&gt;PythonAssistant&lt;/code&gt; can perform virtually any task using python code.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;python_assistant.py&lt;/code&gt; and install pandas using &lt;code&gt;pip install pandas&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.assistant.python import PythonAssistant&#xA;from phi.file.local.csv import CsvFile&#xA;&#xA;python_assistant = PythonAssistant(&#xA;    files=[&#xA;        CsvFile(&#xA;            path=&#34;https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv&#34;,&#xA;            description=&#34;Contains information about movies from IMDB.&#34;,&#xA;        )&#xA;    ],&#xA;    pip_install=True,&#xA;    show_tool_calls=True,&#xA;)&#xA;&#xA;python_assistant.print_response(&#34;What is the average rating of movies?&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;python_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python python_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;See it work through the problem&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;WARNING  PythonTools can run arbitrary code, please provide human supervision.&#xA;INFO     Saved: /Users/zu/ai/average_rating&#xA;INFO     Running /Users/zu/ai/average_rating&#xA;╭──────────┬───────────────────────────────────────────────────────────────────╮&#xA;│ Message  │ What is the average rating of movies?                             │&#xA;├──────────┼───────────────────────────────────────────────────────────────────┤&#xA;│ Response │                                                                   │&#xA;│ (4.1s)   │  • Running: save_to_file_and_run(file_name=average_rating,        │&#xA;│          │    code=..., variable_to_return=average_rating)                   │&#xA;│          │                                                                   │&#xA;│          │ The average rating of movies is approximately 6.72.               │&#xA;╰──────────┴───────────────────────────────────────────────────────────────────╯&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Assistant that generates pydantic models&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;One of our favorite features is generating structured data (i.e. a pydantic model) from sparse information. Meaning we can use Assistants to return pydantic models and generate content which previously could not be possible. In this example, our movie assistant generates an object of the &lt;code&gt;MovieScript&lt;/code&gt; class.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;pydantic_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List&#xA;from pydantic import BaseModel, Field&#xA;from rich.pretty import pprint&#xA;from phi.assistant import Assistant&#xA;&#xA;&#xA;class MovieScript(BaseModel):&#xA;    setting: str = Field(..., description=&#34;Provide a nice setting for a blockbuster movie.&#34;)&#xA;    ending: str = Field(..., description=&#34;Ending of the movie. If not available, provide a happy ending.&#34;)&#xA;    genre: str = Field(..., description=&#34;Genre of the movie. If not available, select action, thriller or romantic comedy.&#34;)&#xA;    name: str = Field(..., description=&#34;Give a name to this movie&#34;)&#xA;    characters: List[str] = Field(..., description=&#34;Name of characters for this movie.&#34;)&#xA;    storyline: str = Field(..., description=&#34;3 sentence storyline for the movie. Make it exciting!&#34;)&#xA;&#xA;&#xA;movie_assistant = Assistant(&#xA;    description=&#34;You help people write movie ideas.&#34;,&#xA;    output_model=MovieScript,&#xA;)&#xA;&#xA;pprint(movie_assistant.run(&#34;New York&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;pydantic_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pydantic_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;See how the assistant generates a structured output&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;MovieScript(&#xA;│   setting=&#39;A bustling and vibrant New York City&#39;,&#xA;│   ending=&#39;The protagonist saves the city and reconciles with their estranged family.&#39;,&#xA;│   genre=&#39;action&#39;,&#xA;│   name=&#39;City Pulse&#39;,&#xA;│   characters=[&#39;Alex Mercer&#39;, &#39;Nina Castillo&#39;, &#39;Detective Mike Johnson&#39;],&#xA;│   storyline=&#39;In the heart of New York City, a former cop turned vigilante, Alex Mercer, teams up with a street-smart activist, Nina Castillo, to take down a corrupt political figure who threatens to destroy the city. As they navigate through the intricate web of power and deception, they uncover shocking truths that push them to the brink of their abilities. With time running out, they must race against the clock to save New York and confront their own demons.&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;A PDF Assistant with Knowledge &amp;amp; Storage&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Lets create a PDF Assistant that can answer questions from a PDF. We&#39;ll use &lt;code&gt;PgVector&lt;/code&gt; for knowledge and storage.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Knowledge Base:&lt;/strong&gt; information that the Assistant can search to improve its responses (uses a vector db).&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; provides long term memory for Assistants (uses a database).&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Run PgVector&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://docs.docker.com/desktop/install/mac-install/&#34;&gt;docker desktop&lt;/a&gt; for running PgVector in a container.&lt;/li&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;resources.py&lt;/code&gt; with the following contents&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.docker.app.postgres import PgVectorDb&#xA;from phi.docker.resources import DockerResources&#xA;&#xA;# -*- PgVector running on port 5432:5432&#xA;vector_db = PgVectorDb(&#xA;    pg_user=&#34;ai&#34;,&#xA;    pg_password=&#34;ai&#34;,&#xA;    pg_database=&#34;ai&#34;,&#xA;    debug_mode=True,&#xA;)&#xA;&#xA;# -*- DockerResources&#xA;dev_docker_resources = DockerResources(apps=[vector_db])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Start &lt;code&gt;PgVector&lt;/code&gt; using&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi start resources.py -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Create PDF Assistant&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;pdf_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import typer&#xA;from rich.prompt import Prompt&#xA;from typing import Optional, List&#xA;from phi.assistant import Assistant&#xA;from phi.storage.assistant.postgres import PgAssistantStorage&#xA;from phi.knowledge.pdf import PDFUrlKnowledgeBase&#xA;from phi.vectordb.pgvector import PgVector2&#xA;&#xA;from resources import vector_db&#xA;&#xA;knowledge_base = PDFUrlKnowledgeBase(&#xA;    urls=[&#34;https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&#34;],&#xA;    vector_db=PgVector2(&#xA;        collection=&#34;recipes&#34;,&#xA;        db_url=vector_db.get_db_connection_local(),&#xA;    ),&#xA;)&#xA;# Comment out after first run&#xA;knowledge_base.load(recreate=False)&#xA;&#xA;storage = PgAssistantStorage(&#xA;    table_name=&#34;pdf_assistant&#34;,&#xA;    db_url=vector_db.get_db_connection_local(),&#xA;)&#xA;&#xA;&#xA;def pdf_assistant(new: bool = False, user: str = &#34;user&#34;):&#xA;    run_id: Optional[str] = None&#xA;&#xA;    if not new:&#xA;        existing_run_ids: List[str] = storage.get_all_run_ids(user)&#xA;        if len(existing_run_ids) &amp;gt; 0:&#xA;            run_id = existing_run_ids[0]&#xA;&#xA;    assistant = Assistant(&#xA;        run_id=run_id,&#xA;        user_id=user,&#xA;        knowledge_base=knowledge_base,&#xA;        storage=storage,&#xA;        # use_tools=True adds functions to&#xA;        # search the knowledge base and chat history&#xA;        use_tools=True,&#xA;        show_tool_calls=True,&#xA;        # Uncomment the following line to use traditional RAG&#xA;        # add_references_to_prompt=True,&#xA;    )&#xA;    if run_id is None:&#xA;        run_id = assistant.run_id&#xA;        print(f&#34;Started Run: {run_id}\n&#34;)&#xA;    else:&#xA;        print(f&#34;Continuing Run: {run_id}\n&#34;)&#xA;&#xA;    while True:&#xA;        message = Prompt.ask(f&#34;[bold] &lt;span&gt;😎&lt;/span&gt; {user} [/bold]&#34;)&#xA;        if message in (&#34;exit&#34;, &#34;bye&#34;):&#xA;            break&#xA;        assistant.print_response(message, markdown=True)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    typer.run(pdf_assistant)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Install libraries&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U pgvector pypdf psycopg sqlalchemy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Run PDF Assistant&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pdf_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Ask a question:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;How do I make pad thai?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;See how the Assistant searches the knowledge base and returns a response.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Show output&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Started Run: d28478ea-75ed-4710-8191-22564ebfb140&#xA;&#xA;INFO     Loading knowledge base&#xA;INFO     Reading:&#xA;         https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf&#xA;INFO     Loaded 82 documents to knowledge base&#xA; 😎 user : How do I make chicken tikka salad?&#xA;╭──────────┬─────────────────────────────────────────────────────────────────────────────────╮&#xA;│ Message  │ How do I make chicken tikka salad?                                              │&#xA;├──────────┼─────────────────────────────────────────────────────────────────────────────────┤&#xA;│ Response │                                                                                 │&#xA;│ (7.2s)   │  • Running: search_knowledge_base(query=chicken tikka salad)                    │&#xA;│          │                                                                                 │&#xA;│          │ I found a recipe for Chicken Tikka Salad that serves 2. Here are the            │&#xA;│          │ ingredients and steps:                                                          │&#xA;│          │                                                                                 │&#xA;│          │ Ingredients:                                                                    │&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Message &lt;code&gt;bye&lt;/code&gt; to exit, start the assistant again using &lt;code&gt;python pdf_assistant.py&lt;/code&gt; and ask:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;What was my last message?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;See how the assistant now maintains storage across sessions.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;pdf_assistant.py&lt;/code&gt; file with the &lt;code&gt;--new&lt;/code&gt; flag to start a new run.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pdf_assistant.py --new&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;Stop PgVector&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Play around and then stop &lt;code&gt;PgVector&lt;/code&gt; using &lt;code&gt;phi stop resources.py&lt;/code&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi stop resources.py -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Build an AI App using Streamlit, FastApi and PgVector&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Let&#39;s build an &lt;strong&gt;AI App&lt;/strong&gt; using GPT-4 as the LLM, Streamlit as the chat interface, FastApi as the API and PgVector for knowledge and storage. Read the full tutorial &lt;a href=&#34;https://docs.phidata.com/ai-app/run-local&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Create your codebase&lt;/h3&gt; &#xA; &lt;p&gt;Create your codebase using the &lt;code&gt;ai-app&lt;/code&gt; template&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi ws create -t ai-app -n ai-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This will create a folder &lt;code&gt;ai-app&lt;/code&gt; with a pre-built AI App that you can customize and make your own.&lt;/p&gt; &#xA; &lt;h3&gt;Serve your App using Streamlit&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://streamlit.io&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Streamlit&lt;/a&gt; allows us to build micro front-ends and is extremely useful for building basic applications in pure python. Start the &lt;code&gt;app&lt;/code&gt; group using:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi ws up --group app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Press Enter&lt;/strong&gt; to confirm and give a few minutes for the image to download.&lt;/p&gt; &#xA; &lt;h4&gt;PDF Assistant&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Open &lt;a href=&#34;http://localhost:8501&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;localhost:8501&lt;/a&gt; to view streamlit apps that you can customize and make your own.&lt;/li&gt; &#xA;  &lt;li&gt;Click on &lt;strong&gt;PDF Assistant&lt;/strong&gt; in the sidebar&lt;/li&gt; &#xA;  &lt;li&gt;Enter a username and wait for the knowledge base to load.&lt;/li&gt; &#xA;  &lt;li&gt;Choose either the &lt;code&gt;RAG&lt;/code&gt; or &lt;code&gt;Autonomous&lt;/code&gt; Assistant type.&lt;/li&gt; &#xA;  &lt;li&gt;Ask &#34;How do I make pad thai?&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Upload PDFs and ask questions&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;We provide a default PDF of ThaiRecipes that you can clear using the &lt;code&gt;Clear Knowledge Base&lt;/code&gt; button. The PDF is only for testing.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;img width=&#34;800&#34; alt=&#34;chat-with-pdf&#34; src=&#34;https://github.com/phidatahq/phidata/assets/22579644/a8eff0ac-963c-43cb-a784-920bd6713a48&#34;&gt; &#xA; &lt;h3&gt;Optional: Serve your App using FastApi&lt;/h3&gt; &#xA; &lt;p&gt;Streamlit is great for building micro front-ends but any production application will be built using a front-end framework like &lt;code&gt;next.js&lt;/code&gt; backed by a RestApi built using a framework like &lt;code&gt;FastApi&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Your AI App comes ready-to-use with FastApi endpoints.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Update the &lt;code&gt;workspace/settings.py&lt;/code&gt; file and set &lt;code&gt;dev_api_enabled=True&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;...&#xA;ws_settings = WorkspaceSettings(&#xA;    ...&#xA;    # Uncomment the following line&#xA;    dev_api_enabled=True,&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Start the &lt;code&gt;api&lt;/code&gt; group using:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi ws up --group api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Press Enter&lt;/strong&gt; to confirm and give a few minutes for the image to download.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;View API Endpoints&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;http://localhost:8000/docs&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;localhost:8000/docs&lt;/a&gt; to view the API Endpoints.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Load the knowledge base using &lt;code&gt;/v1/assitants/load-knowledge-base&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Test the &lt;code&gt;v1/assitants/chat&lt;/code&gt; endpoint with &lt;code&gt;{&#34;message&#34;: &#34;How do I make chicken curry?&#34;}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;The Api comes pre-built with endpoints that you can integrate with your front-end.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Optional: Run Jupyterlab&lt;/h3&gt; &#xA; &lt;p&gt;A jupyter notebook is a must-have for AI development and your &lt;code&gt;ai-app&lt;/code&gt; comes with a notebook pre-installed with the required dependencies. Enable it by updating the &lt;code&gt;workspace/settings.py&lt;/code&gt; file:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;...&#xA;ws_settings = WorkspaceSettings(&#xA;    ...&#xA;    # Uncomment the following line&#xA;    dev_jupyter_enabled=True,&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Start &lt;code&gt;jupyter&lt;/code&gt; using:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi ws up --group jupyter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Press Enter&lt;/strong&gt; to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.&lt;/p&gt; &#xA; &lt;h4&gt;View Jupyterlab UI&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;http://localhost:8888&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;localhost:8888&lt;/a&gt; to view the Jupyterlab UI. Password: &lt;strong&gt;admin&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Play around with cookbooks in the &lt;code&gt;notebooks&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Delete local resources&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Stop the workspace&lt;/h3&gt; &#xA; &lt;p&gt;Play around and stop the workspace using:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;phi ws down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Run your AI App on AWS&lt;/h3&gt; &#xA; &lt;p&gt;Read how to &lt;a href=&#34;https://docs.phidata.com/quickstart/run-aws&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;run your AI App on AWS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook&#34;&gt;Checkout the cookbook for more examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can find the full documentation &lt;a href=&#34;https://docs.phidata.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can also chat with us on &lt;a href=&#34;https://discord.gg/4MtYHHrgA8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Or email us at &lt;a href=&#34;mailto:help@phidata.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;/a&gt;&lt;a href=&#34;mailto:help@phidata.com&#34;&gt;help@phidata.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building AI for your product?&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve helped many companies build AI for their products, the general workflow is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train an assistant&lt;/strong&gt; with proprietary data to perform tasks specific to your product.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect your product&lt;/strong&gt; to the assistant via an API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customize, Monitor and Improve&lt;/strong&gt; the AI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We provide dedicated support and development for AI products. &lt;a href=&#34;https://cal.com/phidata/intro&#34;&gt;Book a call&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re an open-source project and welcome contributions, please read the &lt;a href=&#34;https://raw.githubusercontent.com/phidatahq/phidata/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Request a feature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have a feature request, please open an issue or make a pull request.&lt;/li&gt; &#xA; &lt;li&gt;If you have ideas on how we can improve, please create a discussion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Our roadmap is available &lt;a href=&#34;https://github.com/orgs/phidatahq/projects/2/views/1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;. If you have a feature request, please open an issue/discussion.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MrForExample/ComfyUI-3D-Pack</title>
    <updated>2024-02-11T01:36:25Z</updated>
    <id>tag:github.com,2024-02-11:/MrForExample/ComfyUI-3D-Pack</id>
    <link href="https://github.com/MrForExample/ComfyUI-3D-Pack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An extensive node suite that enables ComfyUI to process 3D inputs (Mesh &amp; UV Texture, etc) using cutting edge algorithms (3DGS, NeRF, etc.)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-3D-Pack&lt;/h1&gt; &#xA;&lt;p&gt;An extensive node suite that enables ComfyUI to process 3D inputs (Mesh &amp;amp; UV Texture, etc) using cutting edge algorithms (3DGS, NeRF, Differentiable Rendering, SDS/VSD Optimization, etc.)&lt;/p&gt; &#xA;&lt;span style=&#34;font-size:1.5em;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#currently-support&#34;&gt;Features&lt;/a&gt; — &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt; — &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#install&#34;&gt;Install&lt;/a&gt; — &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#run&#34;&gt;Run&lt;/a&gt; — &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#tips&#34;&gt;Tips&lt;/a&gt; &lt;/span&gt; &#xA;&lt;h3&gt;Note: this project is still a WIP and not been released into ComFyUI package database yet&lt;/h3&gt; &#xA;&lt;h2&gt;Currently support:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For use case please check &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/&#34;&gt;Example Workflows&lt;/a&gt;. [&lt;strong&gt;Last update: 09/02/2024&lt;/strong&gt;]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; you need to put &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Inputs_Files/&#34;&gt;Example Inputs Files &amp;amp; Folders&lt;/a&gt; under ComfyUI Root Directory\ComfyUI\input folder before you can run the example workflow&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Large Multiview Gaussian Model&lt;/strong&gt;: &lt;a href=&#34;https://github.com/3DTopia/LGM&#34;&gt;3DTopia/LGM&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Enable single image to 3D Gaussian in less than 30 seconds on a RTX3080 GPU, later you can also convert 3D Gaussian to mesh&lt;/p&gt; &lt;p&gt;&#xA;     &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/e221d7f8-49ac-4ed4-809b-d4c790b6270e&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Triplane Gaussian Transformers&lt;/strong&gt;: &lt;a href=&#34;https://github.com/VAST-AI-Research/TriplaneGaussian&#34;&gt;VAST-AI-Research/TriplaneGaussian&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Enable single image to 3D Gaussian in less than 10 seconds on a RTX3080 GPU, later you can also convert 3D Gaussian to mesh&lt;/p&gt; &lt;p&gt;&#xA;     &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/90e7f298-bdbd-4c15-9378-1ca46cbb4871&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preview 3DGS and 3D Mesh&lt;/strong&gt;: 3D Visualization inside ComfyUI:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Using &lt;a href=&#34;https://github.com/huggingface/gsplat.js/tree/main&#34;&gt;gsplat.js&lt;/a&gt; and &lt;a href=&#34;https://github.com/mrdoob/three.js/tree/dev&#34;&gt;three.js&lt;/a&gt; for 3DGS &amp;amp; 3D Mesh visualization respectively&lt;/p&gt; &lt;p&gt;&#xA;     &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/9f3c56b1-afb3-4bf1-8845-ab1025a87463&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stack Orbit Camera Poses&lt;/strong&gt;: Automatically generate all range of camera pose combinations&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;You can use it to conditioning the &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/3d/&#34;&gt;StableZero123 (You need to Download the checkpoint first)&lt;/a&gt;, with full range of camera poses in one prompt pass&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;You can use it to generate the orbit camera poses and directly input to other 3D process node (e.g. GaussianSplatting and BakeTextureToMesh)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Example usage:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Clockwise_Camposes.png&#34; width=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Counter_Clockwise_Camposes.png&#34; width=&#34;256&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Clockwise.gif&#34; width=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Counter_Clockwise.gif&#34; width=&#34;256&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Coordinate system:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Azimuth: In top view, from angle 0 rotate 360 degree with step -90 you get (0, -90, -180/180, 90, 0), in this case camera rotates clock-wise, vice versa.&lt;/li&gt; &#xA;     &lt;li&gt;Elevation: 0 when camera points horizontally forward, pointing down to the ground is negitive angle, vice versa.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D Gaussian Splatting&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ashawkey/diff-gaussian-rasterization&#34;&gt;Improved Differential Gaussian Rasterization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Better Compactness-based Densification method from &lt;a href=&#34;https://gsgen3d.github.io/&#34;&gt;Gsgen&lt;/a&gt;,&lt;/li&gt; &#xA;   &lt;li&gt;Support initialize gaussians from given 3D mesh (Optional)&lt;/li&gt; &#xA;   &lt;li&gt;Support mini-batch optimazation&lt;/li&gt; &#xA;   &lt;li&gt;Multi-View images as inputs&lt;/li&gt; &#xA;   &lt;li&gt;Export to standard 3DGS .ply format supported&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gaussian Splatting Orbit Renderer&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Render 3DGS to images sequences or video, given a 3DGS file and camera poses generated by &lt;strong&gt;Stack Orbit Camera Poses&lt;/strong&gt; node&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bake Multi-View images into UVTexture of given 3D mesh using &lt;a href=&#34;https://github.com/NVlabs/nvdiffrast&#34;&gt;Nvdiffrast&lt;/a&gt;, supports:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Export to .obj, .ply, .glb&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Marching Tetrahedrons&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Allow convert 3DGS .ply file to 3D mesh &lt;br&gt; &lt;em&gt;Note: I didn&#39;t spent time to turn the hyperprameters yet, the result will be improved in the future!&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Save &amp;amp; Load 3D file&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;.obj, .ply, .glb for 3D Mesh&lt;/li&gt; &#xA;   &lt;li&gt;.ply for 3DGS&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Switch Axis for 3DGS &amp;amp; 3D Mesh&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Since different algorithms likely use different coordinate system, so the ability to re-mapping the axis of coordinate is crucial for passing generated result between differnt nodes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Add DMTet algorithm to allow conversion from points cloud(Gaussian/.ply) to mesh (.obj, .ply, .glb)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Integrate &lt;a href=&#34;https://zouzx.github.io/TriplaneGaussian/&#34;&gt;Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Add interactive 3D UI inside ComfuUI to visulaize training and generated results for 3D representations&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Add a new node to generate renderer image sequence given a 3D gaussians and orbit camera poses (So we can later feed it to the differentiable renderer to bake it onto a given mesh)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Integrate &lt;a href=&#34;https://me.kiui.moe/lgm/&#34;&gt;LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Add a general SDS/ISM Optimization algorithm to allow training 3D representations with diffusion model, &lt;em&gt;The real fun starts here&lt;/em&gt; ;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Need to do some in-depth research on Interval Score Matching (ISM), since math behind it makes perfect sense and also there are so many ways we could improve upon the result obtained from &lt;a href=&#34;https://github.com/EnVision-Research/LucidDreamer&#34;&gt;LucidDreamer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Improve 3DGS to Mesh conversion algorithms:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support to training DMTet with images(RGB, Alpha, Normal Map)&lt;/li&gt; &#xA;   &lt;li&gt;Find better methods to converts 3DGS or Points Cloud to Mesh (Normal maps reconstruction maybe?)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Add Structure from motion (SfM) initialization for 3DGS (Better first guess -&amp;gt; Faster convergence &amp;amp; Better result)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Add a few best Nerf algorithms (No idea yet, &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;instant-ngp&lt;/a&gt; maybe?)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[IMPORTANT!!!]&lt;/strong&gt; &lt;br&gt; Currently this package is only been tested in following setups:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows 10/11&lt;/li&gt; &#xA; &lt;li&gt;Miniconda/Conda Python 3.11.7 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I tried install this package with ComfyUI embed python env first, but I can&#39;t find a way to build CUDA related libraries, e.g. diff-gaussian-rasterization, nvdiffrast, simple-knn.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Torch version: 2.1.2+cu121/V.2.1.2+cu118&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Assume you have already downloaded &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;First download &lt;a href=&#34;https://docs.conda.io/projects/miniconda/en/latest/&#34;&gt;Miniconda&lt;/a&gt; (&lt;em&gt;One of the best way to manage a clean and separated python envirments&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alternatively you can check this tutorial: &lt;a href=&#34;https://www.comflowy.com/preparation-for-study/install#step-two-download-comfyui&#34;&gt;Installing ComfyUI with Miniconda On Windows and Mac&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Go to your Your ComfyUI root directory, for my example:&#xA;cd C:\Users\reall\Softwares\ComfyUI_windows_portable &#xA;&#xA;conda create -p ./python_miniconda_env/ComfyUI python=3.11&#xA;&#xA;# conda will tell what command to use to activate the env&#xA;conda activate C:\Users\reall\Softwares\ComfyUI_windows_portable\python_miniconda_env\ComfyUI&#xA;&#xA;# This package also works with cu118&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;&#xA;pip install -r ./ComfyUI/requirements.txt&#xA;&#xA;# Then go to ComfyUI-3D-Pack directory under the ComfyUI Root Directory\ComfyUI\custom_nodes for my example is:&#xA;cd C:\Users\reall\Softwares\ComfyUI_windows_portable\ComfyUI\custom_nodes\ComfyUI-3D-Pack&#xA;# Finally you can double click following .bat script or run it in CLI:&#xA;install.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Install Plan B:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Just in case &lt;code&gt;install.bat&lt;/code&gt; may not working in your PC, you could also run the following commands under this package&#39;s root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# First make sure the Conda env: python_miniconda_env\ComfyUI is activated, then go to ComfyUI Root Directory\ComfyUI\custom_nodes\ComfyUI-3D-Pack and:&#xA;pip install -r requirements.txt&#xA;&#xA;git clone --recursive https://github.com/ashawkey/diff-gaussian-rasterization&#xA;pip install ./diff-gaussian-rasterization&#xA;&#xA;pip install ./simple-knn&#xA;&#xA;pip install -U xformers --index-url https://download.pytorch.org/whl/cu121&#xA;&#xA;git clone --recursive https://github.com/NVlabs/nvdiffrast/`&#xA;pip install ./nvdiffrast&#xA;&#xA;# Install pointnet2_ops&#xA;cd tgs/models/snowflake/pointnet2_ops_lib &amp;amp;&amp;amp; python setup.py install &amp;amp;&amp;amp; cd ../../../../&#xA;&#xA;# Install pytorch_scatter&#xA;pip install git+https://github.com/rusty1s/pytorch_scatter.git&#xA;&#xA;# Install pytorch3d&#xA;pip install git+https://github.com/facebookresearch/pytorch3d.git@stable&#xA;&#xA;# Install kiuikit (Computer Vision Library)&#xA;pip install git+https://github.com/ashawkey/kiuikit.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Install with ComfyUI&#39;s embed python env on Windows:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If you have already installed ComfyUI on Windows with embed python environment and you don&#39;t want to switch to Miniconda/Conda and reinstall all your packages &lt;br&gt; &lt;em&gt;(Which is ideal, but unfortunately so far it seems I&#39;m not experienced enough in this matter, but I&#39;ll continue to looking for the better solutions and please give me some suggestions if ou know better, thanks :)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;According to &lt;a href=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/issues/5&#34;&gt;@doctorpangloss&lt;/a&gt;, you can setup the c++/cuda build environments in windows by using &lt;a href=&#34;https://chocolatey.org/&#34;&gt;chocolatey&lt;/a&gt; with following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# using git bash for the sake of simplicity&#xA;# enable developer mode&#xA;# google this: allow os.symlink on windows by adding your username to the local security policy entry for it.&#xA;# you will have to restart your computer&#xA;# install chocolatey using powershell, then install the prereqs for compilation on Windows&#xA;choco install -y visualstudio2022buildtools&#xA;choco install -y visualstudio2022-workload-vctools --package-parameters &#34;--add Microsoft.VisualStudio.Component.VC.Llvm.ClangToolset --add Microsoft.VisualStudio.Component.VC.Llvm.Clang&#34;&#xA;# I couldn&#39;t find cuda version 12.1, you may need to install cudav12.1 manually&#xA;choco install -y cuda &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then just go to ComfyUI-3D-Pack directory under the ComfyUI Root Directory\ComfyUI\custom_nodes and running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r ./ComfyUI/requirements.txt&#xA;&#xA;install.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run:&lt;/h2&gt; &#xA;&lt;p&gt;Copy the files inside folder &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_New_ComfyUI_Bats/&#34;&gt;__New_ComfyUI_Bats&lt;/a&gt; to your ComfyUI root directory, and double click run_nvidia_gpu_miniconda.bat to start ComfyUI!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alternatively you can just activate the Conda env: python_miniconda_env\ComfyUI, and go to your ComfyUI root directory then run command &lt;code&gt;python ./ComfyUI/main.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The world &amp;amp; camera coordinate system is the same as OpenGL:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;    World            Camera        &#xA;  &#xA;     +y              up  target                                              &#xA;     |               |  /                                            &#xA;     |               | /                                                &#xA;     |______+x       |/______right                                      &#xA;    /                /         &#xA;   /                /          &#xA;  /                /           &#xA; +z               forward           &#xA;&#xA;elevation: in (-90, 90), from +y to -y is (-90, 90)&#xA;azimuth: in (-180, 180), from +z to +x is (0, 90)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you encounter OpenGL errors (e.g., &lt;code&gt;[F glutil.cpp:338] eglInitialize() failed&lt;/code&gt;), then set &lt;code&gt;force_cuda_rasterize&lt;/code&gt; to true on corresponding node&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alexmolas/microsearch</title>
    <updated>2024-02-11T01:36:25Z</updated>
    <id>tag:github.com,2024-02-11:/alexmolas/microsearch</id>
    <link href="https://github.com/alexmolas/microsearch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;microsearch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;microsearch&lt;/code&gt; is a minimal Python search engine designed for simplicity and efficiency. The project allows users to perform searches using Python, and it also provides an option to deploy a FastAPI app with an endpoint and a website for a user-friendly experience. It has been designed to provide users with a straightforward way to deploy their own search engine and search documents from their favorite blogs. The project includes a script for asynchronously downloading all the posts from a series of RSS feeds.&lt;/p&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python Implementation&lt;/strong&gt;: &lt;code&gt;microsearch&lt;/code&gt; is entirely implemented in Python, making it accessible and easy to understand for developers with varying levels of experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI App Deployment&lt;/strong&gt;: The project provides an option to deploy a FastAPI app, allowing users to interact with the search engine through a dedicated endpoint and a user-friendly website.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RSS Feed Crawling Script&lt;/strong&gt;: To populate the search engine with data, &lt;code&gt;microsearch&lt;/code&gt; offers a script for asynchronously downloading posts from a series of RSS feeds. This feature ensures that users can conveniently aggregate content from their chosen blogs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The first step is to download this repo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/alexmolas/microsearch.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, I recommend you install everything in a virtual environment. I usually use &lt;code&gt;virtualenv&lt;/code&gt; but any other environment manager should work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virtualenv -p python3.10 venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;activate the environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and install the package and the dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Crawl data&lt;/h2&gt; &#xA;&lt;p&gt;Now we need to download the content of the blogs. I&#39;m sharing &lt;a href=&#34;https://github.com/alexmolas/microsearch/raw/main/feeds.txt&#34;&gt;here&lt;/a&gt; a list of feed examples, but please feel free to use your own. To download the content do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download_content.py --feed-path feeds.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Launch app&lt;/h2&gt; &#xA;&lt;p&gt;Finally, once the content is crawled and stored you can run the app as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m app.app --data-path output.parquet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and if you navigate to &lt;a href=&#34;http://127.0.0.1:8000/&#34;&gt;http://127.0.0.1:8000/&lt;/a&gt; you&#39;ll be able to query the engine.&lt;/p&gt;</summary>
  </entry>
</feed>