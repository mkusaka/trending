<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-22T01:42:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SamsungLabs/NeuralHaircut</title>
    <updated>2023-07-22T01:42:36Z</updated>
    <id>tag:github.com,2023-07-22:/SamsungLabs/NeuralHaircut</id>
    <link href="https://github.com/SamsungLabs/NeuralHaircut" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.05872&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://samsunglabs.github.io/NeuralHaircut/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains official inference code for Neural Haircut.&lt;/p&gt; &#xA;&lt;p&gt;This code helps you to create strand-based hairstyle using multi-view images or monocular video.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and install requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.sec.samsung.net/v-sklyarova/NeuralHaircut.git&#xA;cd NeuralHaircut&#xA;conda env create -n neuralhaircut -f neural_haircut.yaml&#xA;conda activate neuralhaircut&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Initialize submodules of &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;k-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;NeuS&lt;/a&gt;, &lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/tjpulkl/CDGNet&#34;&gt;CDGNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/rakhimovv/npbgpp&#34;&gt;npbgpp&lt;/a&gt;. Download pretrained weights for &lt;a href=&#34;https://github.com/tjpulkl/CDGNet&#34;&gt;CDGNet&lt;/a&gt; and &lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd npbgpp &amp;amp;&amp;amp; python setup.py build develop&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained NeuralHaircut models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdown --folder https://drive.google.com/drive/folders/1TCdJ0CKR3Q6LviovndOkJaKm8S1T9F_8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;h3&gt;Fitting the FLAME coarse geometry using multiview images&lt;/h3&gt; &#xA;&lt;p&gt;More details could be find in &lt;a href=&#34;https://raw.githubusercontent.com/SamsungLabs/NeuralHaircut/main/src/multiview_optimization&#34;&gt;multiview_optimization&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Launching the first stage on &lt;a href=&#34;https://github.com/CrisalixSA/h3ds&#34;&gt;H3ds dataset&lt;/a&gt; or custom monocular dataset:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_geometry_reconstruction.py --case CASE --conf ./configs/SCENE_TYPE/neural_strands.yaml --exp_name first_stage_SCENE_TYPE_CASE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;SCENE_TYPE = [h3ds|monocular]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you want to add camera fitting:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_geometry_reconstruction.py --case CASE --conf ./configs/SCENE_TYPE/neural_strands_w_camera_fitting.yaml --exp_name first_stage_SCENE_TYPE_CASE --train_cameras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At the end of first stage do the &lt;a href=&#34;https://raw.githubusercontent.com/SamsungLabs/NeuralHaircut/main/preprocess_custom_data&#34;&gt;following steps&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to continue from checkpoint add flag &lt;code&gt;--is_continue&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to obtain mesh in higher resolution add flags &lt;code&gt;--is_continue --mode validate_mesh&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Launching the second stage on &lt;a href=&#34;https://github.com/CrisalixSA/h3ds&#34;&gt;H3ds dataset&lt;/a&gt; or custom monocular dataset:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_strands_optimization.py --case CASE --scene_type SCENE_TYPE --conf ./configs/SCENE_TYPE/neural_strands.yaml  --hair_conf ./configs/hair_strands_textured.yaml --exp_name second_stage_SCENE_TYPE_CASE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If during the first stage you also fitted the cameras, then use the following:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_strands_optimization.py --case CASE --scene_type SCENE_TYPE --conf ./configs/SCENE_TYPE/neural_strands_w_camera_fitted.yaml  --hair_conf ./configs/hair_strands_textured.yaml --exp_name second_stage_SCENE_TYPE_CASE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train NeuralHaircut with your custom data&lt;/h2&gt; &#xA;&lt;p&gt;More information can be found in &lt;a href=&#34;https://raw.githubusercontent.com/SamsungLabs/NeuralHaircut/main/preprocess_custom_data&#34;&gt;preprocess_custom_data.&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You could run the scripts on our &lt;a href=&#34;https://raw.githubusercontent.com/SamsungLabs/NeuralHaircut/main/example&#34;&gt;monocular scene&lt;/a&gt; for convenience.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This code and model are available for scientific research purposes as defined in the LICENSE.txt file. By downloading and using the project you agree to the terms in the LICENSE.txt.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;p&gt;This work is based on the great project &lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;NeuS&lt;/a&gt;. Also we acknowledge additional projects that were essential and speed up the developement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;NeuS&lt;/a&gt; for geometry reconstruction;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rakhimovv/npbgpp&#34;&gt;npbgpp&lt;/a&gt; for rendering of soft rasterized features;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;k-diffusion&lt;/a&gt; for diffusion network;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/tjpulkl/CDGNet&#34;&gt;CDGNet&lt;/a&gt; used to obtain silhouette and hair segmentations;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/yfeng95/PIXIE&#34;&gt;PIXIE&lt;/a&gt; used to obtain initialization for shape and pose parameters;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Cite as below if you find this repository is helpful to your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{sklyarova2023neural_haircut,&#xA;title = {Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction},&#xA;author = {Sklyarova, Vanessa and Chelishev, Jenya and Dogaru, Andreea and Medvedev, Igor and Lempitsky, Victor and Zakharov, Egor},&#xA;booktitle = {Proceedings of IEEE International Conference on Computer Vision (ICCV)},&#xA;year = {2023}&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>dabeaz-course/practical-python</title>
    <updated>2023-07-22T01:42:36Z</updated>
    <id>tag:github.com,2023-07-22:/dabeaz-course/practical-python</id>
    <link href="https://github.com/dabeaz-course/practical-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practical Python Programming (course by @dabeaz)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome!&lt;/h1&gt; &#xA;&lt;p&gt;When I first learned Python nearly 27 years ago, I was immediately struck by how I could productively apply it to all sorts of messy work projects. Fast-forward a decade and I found myself teaching others the same fun. The result of that teaching is this course--A no-nonsense treatment of Python that has been actively taught to more than 400 in-person groups since 2007. Traders, systems admins, astronomers, tinkerers, and even a few hundred rocket scientists who used Python to help land a rover on Mars--they&#39;ve all taken this course. Now, I&#39;m pleased to make it available under a Creative Commons license--completely free of spam, signups, and other nonsense. Enjoy!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dabeaz-course.github.io/practical-python&#34;&gt;GitHub Pages&lt;/a&gt; | &lt;a href=&#34;https://github.com/dabeaz-course/practical-python&#34;&gt;GitHub Repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;--David Beazley (&lt;a href=&#34;https://dabeaz.com&#34;&gt;https://dabeaz.com&lt;/a&gt;), &lt;a href=&#34;https://mastodon.social/@dabeaz&#34;&gt;@dabeaz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(P.S. This course is about Python. If you want a Python course that&#39;s about programming, you might consider &lt;a href=&#34;https://www.dabeaz.com/advprog.html&#34;&gt;Advanced Programming with Python&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;What is This?&lt;/h2&gt; &#xA;&lt;p&gt;The material you see here is the heart of an instructor-led Python training course used for corporate training and professional development. It was in continual development from 2007 to 2019 and battle tested in real-world classrooms. Usually, it&#39;s taught in-person over the span of three or four days--requiring approximately 25-35 hours of intense work. This includes the completion of approximately 130 hands-on coding exercises.&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;Students of this course are usually professional scientists, engineers, and programmers who already have experience in at least one other programming language. No prior knowledge of Python is required, but knowledge of common programming topics is assumed. Most participants find the course challenging--even if they&#39;ve already been doing a bit of Python programming.&lt;/p&gt; &#xA;&lt;h2&gt;Course Objectives&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this course is to cover foundational aspects of Python programming with an emphasis on script writing, basic data manipulation, and program organization. By the end of this course, students should be able to start writing useful Python programs on their own or be able to understand and modify Python code written by their coworkers.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To complete this course, you need nothing more than a basic installation of Python 3.6 or newer and time to work on it.&lt;/p&gt; &#xA;&lt;h2&gt;What This Course is Not&lt;/h2&gt; &#xA;&lt;p&gt;This is not a course for absolute beginners on how to program a computer. It is assumed that you already have programming experience in some other programming language or Python itself.&lt;/p&gt; &#xA;&lt;p&gt;This is not a course on web development. That&#39;s a different circus. However, if you stick around for this circus, you&#39;ll still see some interesting acts--just nothing involving animals.&lt;/p&gt; &#xA;&lt;p&gt;This is not a course on using tools that happen to be written in Python. It&#39;s about learning the core Python language.&lt;/p&gt; &#xA;&lt;p&gt;This is not a course for software engineers on how to write or maintain a one-million line Python application. I don&#39;t write programs like that, nor do most companies who use Python, and neither should you. Delete something already!&lt;/p&gt; &#xA;&lt;h2&gt;Take me to the Course Already!&lt;/h2&gt; &#xA;&lt;p&gt;Ok, ok. Point your browser &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/practical-python/master/Notes/Contents.md&#34;&gt;HERE&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Community Discussion&lt;/h2&gt; &#xA;&lt;p&gt;Want to discuss the course? You can join the conversation on &lt;a href=&#34;https://gitter.im/dabeaz-course/practical-python&#34;&gt;Gitter&lt;/a&gt;. I can&#39;t promise an individual response, but perhaps others can jump in to help.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Llorenç Muntaner was instrumental in converting the course content from Apple Keynote to the online structure that you see here.&lt;/p&gt; &#xA;&lt;p&gt;Various instructors have presented this course at one time or another over the last 12 years. This includes (in alphabetical order): Ned Batchelder, Juan Pablo Claude, Mark Fenner, Michael Foord, Matt Harrison, Raymond Hettinger, Daniel Klein, Travis Oliphant, James Powell, Michael Selik, Hugo Shi, Ian Stokes-Rees, Yarko Tymciurak, Bryan Van de ven, Peter Wang, and Mark Wiebe.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;d also like to thank the thousands of students who have taken this course and contributed to its success with their feedback and discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Questions and Answers&lt;/h2&gt; &#xA;&lt;h3&gt;Q: Are there course videos I can watch?&lt;/h3&gt; &#xA;&lt;p&gt;No. This course is about you writing Python code, not watching someone else.&lt;/p&gt; &#xA;&lt;h3&gt;Q: How is this course licensed?&lt;/h3&gt; &#xA;&lt;p&gt;Practical Python Programming is licensed under a Creative Commons Attribution ShareAlike 4.0 International License.&lt;/p&gt; &#xA;&lt;h3&gt;Q: May I use this material to teach my own Python course?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, as long as appropriate attribution is given.&lt;/p&gt; &#xA;&lt;h3&gt;Q: May I make derivative works?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, as long as such works carry the same license terms and provide attribution.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Can I translate this to another language?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, that would be awesome. Send me a link when you&#39;re done.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Can I live-stream the course or make a video?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, go for it! You&#39;ll probably learn a lot of Python doing that.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Why wasn&#39;t topic X covered?&lt;/h3&gt; &#xA;&lt;p&gt;There is only so much material that you can cover in 3-4 days. If it wasn&#39;t covered, it was probably because it was once covered and it caused everyone&#39;s head to explode or there was never enough time to cover it in the first place. Also, this is a course, not a Python reference manual.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Why isn&#39;t awesome &lt;code&gt;{command}&lt;/code&gt; in awesome &lt;code&gt;{tool}&lt;/code&gt; covered?&lt;/h3&gt; &#xA;&lt;p&gt;The focus of this course is on learning the core Python language, not learning the names of commands in tools.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Is this course being maintained or updated?&lt;/h3&gt; &#xA;&lt;p&gt;This course represents a &#34;finished product&#34; that was taught and developed for more than decade. I have no plans to significantly revise the material at this time, but will occasionally fix bugs and add clarification.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Do you accept pull requests?&lt;/h3&gt; &#xA;&lt;p&gt;Bug reports are appreciated and may be filed through the &lt;a href=&#34;https://github.com/dabeaz-course/practical-python/issues&#34;&gt;issue tracker&lt;/a&gt;. Pull requests are not accepted except by invitation. Please file an issue first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Melelery/c-binance-future-quant</title>
    <updated>2023-07-22T01:42:36Z</updated>
    <id>tag:github.com,2023-07-22:/Melelery/c-binance-future-quant</id>
    <link href="https://github.com/Melelery/c-binance-future-quant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;low-cost, high-efficiency, easy-to-implement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;简介&lt;/h1&gt; &#xA;&lt;p&gt;这是一套经过长时间实盘验证，有超过100亿美金交易量，包含币安合约的数据录入，风控，交易的架构实现，但不包含具体的策略，仅提供一个简单的交易实践演示数据读取，开仓，平仓，以及止盈止损，风控，简单的前端数据展示和分析，前端演示地址：&lt;a href=&#34;http://8.217.121.203/&#34;&gt;8.217.121.203&lt;/a&gt;，目前放1000美金运行着一个年化100%~200%的高频左侧回归策略&lt;/p&gt; &#xA;&lt;p&gt;你可以利用它简单，低成本的实现你的交易逻辑，其大量运用阿里云服务器进行分布式架构，多进程处理，以及飞书进行异常报错和交易信息披露...&lt;/p&gt; &#xA;&lt;p&gt;如果你愿意详细阅读该readme的所有信息，尤其是 &lt;a href=&#34;https://raw.githubusercontent.com/Melelery/c-binance-future-quant/main/#%E6%A8%A1%E5%9D%97%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90&#34;&gt;模块详细解析&lt;/a&gt; ，那么他同时也会是一部关于币安合约的交易风控，设计架构的经验理解历史，总结了几乎本人所有成功和失败的经验，希望能让后人少踩些坑&lt;/p&gt; &#xA;&lt;h1&gt;优势&lt;/h1&gt; &#xA;&lt;p&gt;低成本，高效率，简单实现是这套系统的三个优势&lt;/p&gt; &#xA;&lt;p&gt;不到1000人民币一个月的成本，实现每分钟扫描1500万次交易对是否满足交易条件&lt;/p&gt; &#xA;&lt;p&gt;除了撮合服务器（C++）外都采用python编写，简单易懂&lt;/p&gt; &#xA;&lt;p&gt;大量的分布式架构实现更快的速度，并且可以根据个人需求，自由伸缩的调控服务器数量来实现成本和性能的平衡&lt;/p&gt; &#xA;&lt;p&gt;通过多重接口读取行情/账户信息，并根据更新时间戳进行整合，最大程度的降低数据风险&lt;/p&gt; &#xA;&lt;p&gt;企业级别的风控安全解决方案&lt;/p&gt; &#xA;&lt;h1&gt;架构&lt;/h1&gt; &#xA;&lt;p&gt;该系统通过一个C++服务器作为主撮合服务器，大量的可伸缩调整的分布式python服务器作为数据采集服务器。&lt;/p&gt; &#xA;&lt;p&gt;将采集的数据，包括Kline ，trades，tick等，喂给C++服务器。&lt;/p&gt; &#xA;&lt;p&gt;交易服务器再从C++服务器统一读取数据，并且在本地端维护一个K线账本，来避开交易所的频率限制，实现高效率，低成本的数据读取。&lt;/p&gt; &#xA;&lt;p&gt;又比如，账户的余额，持仓数据，在币安上有三种方式获取，a是position risk，b是account，c是ws，那么会有三台服务器分别采用这三种方式读取，然后汇入C++服务器进行校对，通过对比更新时间截取最新的数据，然后服务给交易服务器&lt;/p&gt; &#xA;&lt;p&gt;前端数据板块，通过阿里云oss为中介，网页读取oss数据的方式进行展示，隔离数据风险&lt;/p&gt; &#xA;&lt;h1&gt;作者自述&lt;/h1&gt; &#xA;&lt;p&gt;2021年，我从一家top量化公司辞职后做起量化交易，主要战场在币安，这两年间，我从做市商-&amp;gt;趋势-&amp;gt;套利等类型均有涉及，最高峰的时候，在币安一个月有接近20亿美金的交易额。&lt;/p&gt; &#xA;&lt;p&gt;至2023年7月，因为各种原因，大方向上失败了，只遗留下一个朋友的资金在继续运行一个比较稳定盈利的左侧交易策略。&lt;/p&gt; &#xA;&lt;p&gt;这是在这两年时间里面探索出来的一套高效率，低成本的数据读取，录入框架，同时包含了一套风控系统，他更像一个架构，而不是一个实现，你同样可以通过简单的修改替换运用到okex，bybit等等上&lt;/p&gt; &#xA;&lt;p&gt;资金合作或者工作机会（不谈任何涉及策略原理和源码，请开门见山节省双方时间），请联系微信号 melelery 或邮件至&lt;a href=&#34;mailto:c.binance.quant@gmail.com&#34;&gt;c.binance.quant@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;模块详细解析&lt;/h1&gt; &#xA;&lt;p&gt;我方设计的模块包括 &lt;a href=&#34;https://raw.githubusercontent.com/Melelery/c-binance-future-quant/main/#%E9%80%9A%E7%94%A8%E9%83%A8%E5%88%86&#34;&gt;通用部分&lt;/a&gt; ，&lt;a href=&#34;https://raw.githubusercontent.com/Melelery/c-binance-future-quant/main/#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86&#34;&gt;数据处理部分&lt;/a&gt; ，&lt;a href=&#34;https://raw.githubusercontent.com/Melelery/c-binance-future-quant/main/#%E5%85%B3%E9%94%AE%E6%93%8D%E4%BD%9C%E9%83%A8%E5%88%86&#34;&gt;关键操作部分&lt;/a&gt; ，&lt;a href=&#34;https://raw.githubusercontent.com/Melelery/c-binance-future-quant/main/#%E5%AE%89%E5%85%A8%E9%A3%8E%E6%8E%A7%E9%83%A8%E5%88%86&#34;&gt;安全风控部分&lt;/a&gt; ，以及没有开源的具体交易逻辑部分&lt;/p&gt; &#xA;&lt;p&gt;该项目的最小化开启需求为 一台web服务器，一台ws服务器，一台tick数据读取服务器，一台one min kline数据读取服务器，以及一台交易服务器&lt;/p&gt; &#xA;&lt;p&gt;你可以根据自己的需求，扩展相关的服务器，例如需要 15 m 的kline，需要trades等等，只需要在这个基础上增加即可，这里没有盘口数据的整合，除了买一卖一。&lt;/p&gt; &#xA;&lt;p&gt;如果你需要更高维度的盘口数据，建议是直接在交易服务器里面进行读取，因为除了kline数据，其他数据进行这种整合并没有太大的实际意义，200个交易对的kline数据，通过这种方式可以减少调用接口的频率，以及在某些接口有延迟的情况下依然能通过校对获得最新数据，但是盘口数据的api就一个，并没有这种需求。&lt;/p&gt; &#xA;&lt;p&gt;盘口数据的延迟只能通过交易服务器的分布式运行稍微解决，比如我方架设了五台交易服务器，运行着一样的开仓关仓逻辑，通过多台服务器的读取数据去避免一些情况下部分服务器读取不到或者延迟较大等意外情况。&lt;/p&gt; &#xA;&lt;p&gt;这里顺便引申出一个交易服务器发出订单的方式，有两种，一种是直接在交易服务器上发出，每台交易服务器承担平均的交易量，比方说我有五台交易服务器，我一个订单的交易量需求是100u，那么就每台服务器负责20u的交易量，如果延迟了或是其他问题，某一台服务器丢失了信号那就是损失20u的交易量，另外一个是做一个中心化的 trade 的web服务器，trade web服务器会以第一次收到交易服务器 发出的交易信号为开仓条件，第一次即开出100%的交易量，两种方式各有适用的地方和优势，需要自行抉择，实际上在webserver文件里面已经整合了第二种方式。&lt;/p&gt; &#xA;&lt;p&gt;我方实盘的项目运行环境为Ubuntu 22.04 64位，Python 3.10.6，由于项目使用的全部库和包都是官方或者热门的，在google可以查找到安装方式，此处不再累述如何安装环境，如果需要最简启动方案，可邮件 &lt;a href=&#34;mailto:c.binance.quant@gmail.com&#34;&gt;c.binance.quant@gmail.com&lt;/a&gt; 联系我方，直接共享系统镜像到你方阿里云账号，收取100 USDT的技术费用&lt;/p&gt; &#xA;&lt;p&gt;我方目前维护的实盘项目，具体服务器配置为，（服务器名即为开源的文件名）&lt;/p&gt; &#xA;&lt;p&gt;一台 wsPosition 服务器，用于通过ws的方式读取币安的仓位和余额数据，汇入ws数据撮合服务器&lt;/p&gt; &#xA;&lt;p&gt;一台positionRisk服务器，用于读取/fapi/v2/positionRisk接口，通过该接口获取仓位信息并实时判断损失，以便于及时止损，该服务器和wsPosition，makerStopLoss，getBinancePosition具备类似功能，之所以设计多种交叉相同功能的服务器，是为了最大限度的防止风险，在币安的某一接口出现延迟的情况下，系统依然可以健壮的运行，以下不再重述&lt;/p&gt; &#xA;&lt;p&gt;一台makerStopLoss服务器，用于从getBinancePosition服务器读取仓位信息后，读取单独的挂单信息，然后预设止损单，之所以与getBinancePosition服务器进行拆分，是因为读取挂单需要的权重较高，拆分成两个ip可以更高频率的进行操作&lt;/p&gt; &#xA;&lt;p&gt;一台getBinancePosition服务器，用于读取/fapi/v2/account接口，获得仓位信息和余额后汇入ws数据撮合服务器&lt;/p&gt; &#xA;&lt;p&gt;一台commission服务器，用于记录流水信息&lt;/p&gt; &#xA;&lt;p&gt;一台checkTimeoutOrders服务器，用于读取/fapi/v1/openOrders接口，查询全部挂单，然后取消超过一定时间的挂单，或者进行一些额外的操作，例如挂单三秒没被吃，则转换成take订单等等&lt;/p&gt; &#xA;&lt;p&gt;一台cancelServer服务器，本质上也是web server服务器，运行webServer文件，用于取消订单&lt;/p&gt; &#xA;&lt;p&gt;一台webServer服务器，本质上也是web server服务器，运行webServer文件，用于大部分程序一开始读取交易对等等&lt;/p&gt; &#xA;&lt;p&gt;两台oneMinKlineToWs服务器，用于低频率读取一分钟线的kline&lt;/p&gt; &#xA;&lt;p&gt;两台volAndRate服务器，用于读取交易量数据做分析并提供给其他服务器&lt;/p&gt; &#xA;&lt;p&gt;十台specialOneMinKlineToWs 服务器，用于高频率读取一分钟线的kline&lt;/p&gt; &#xA;&lt;p&gt;十台tickToWs服务器，用于读取 tick 群信息&lt;/p&gt; &#xA;&lt;p&gt;一台ws服务器，用于C++的数据戳合服务器&lt;/p&gt; &#xA;&lt;p&gt;以及五台交易服务器&lt;/p&gt; &#xA;&lt;p&gt;合计38台服务器，以及一台最低配置的mysql数据库，大部分只需要选用最低配置的抢占式服务器，实际情况下，对于开仓服务器和ws服务器，我购买了高主频类型的服务器，有一倍速度的提升，其他的行情和数据录取服务器并无必要进行这个优化，综合成本一个月不足3000元人民币，大头在流量费用，基本可以满足大部分量化的风控和延迟需求，个人认为上述数目对半砍依然可以满足大部分需求&lt;/p&gt; &#xA;&lt;p&gt;在该项目里，一个单独的模块，需要一台服务器一个IP单独运行，目前基本已经将单模块的https读取频率调教到币安允许的最大值。&lt;/p&gt; &#xA;&lt;p&gt;此处需要说明的是，这里都是以阿里云东京为例子，币安的服务器在亚马逊云东京。&lt;/p&gt; &#xA;&lt;p&gt;而本文所叙述的延迟，其实包含两种延迟，一是读取频率的延迟，二是网络的延迟，这两种延迟综合计算后，才是真实环境下的最终延迟。&lt;/p&gt; &#xA;&lt;p&gt;之所以使用阿里云是因为阿里云抢占式服务器具备成本优势，从而具备读取频率延迟的优势。&lt;/p&gt; &#xA;&lt;p&gt;阿里云网络延迟约为10ms，而亚马逊在不申请内网权限的情况下预计为1~3ms，申请内网则面临锁定ip的问题，即无法通过铺开更多ip的手段去降低延迟。&lt;/p&gt; &#xA;&lt;p&gt;虽然亚马逊云具备更低的延迟，但是由于&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;ws类型的数据读取通常被币安锁定100ms以上的延迟，且ws类型的读取数据具备某些不可确认的风险因素，所以该方式被排除&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果采取https读取的方式，某些数据的读取权重高达20，甚至是30，由此推演出需要多IP，进行分布式读取才能具备更高频率，而这个时候，单个IP的成本价格即成了需要考虑的因素，阿里云抢占式服务器的成本一个月不足20人民币，在综合的性价比考虑后，我方选择了日本阿里云。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;该方案并不是一套服务于高频（纳秒级别）交易的解决方案，否则全部都会用C++编写，实际上他是一套追求成本，延迟，开发速度，三者均衡最优解的方案，并为毫秒级别的策略服务&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;通用部分&lt;/h2&gt; &#xA;&lt;h3&gt;react_front文件夹&lt;/h3&gt; &#xA;&lt;p&gt;前端文件，该网页为对外网页，所以强制锁定了一分钟的时间间隔，展示的数据也比较简单，毕竟是对外的，我亦有设计内网详细的数据分析网站，但是这一部分不在此处披露。&lt;/p&gt; &#xA;&lt;p&gt;另外一提，交易后的数据处理，以及前端代码，都是以实现功能为主，所以并不注重性能等其他指标，可参考最下方faq&lt;/p&gt; &#xA;&lt;h3&gt;afterTrade文件夹&lt;/h3&gt; &#xA;&lt;p&gt;服务器如何更新到前端数据的文件，主要原理是利用阿里云oss作为中间件，隔离数据风险&lt;/p&gt; &#xA;&lt;p&gt;其中的tradesUpdate为更细trade记录，需要你在下单的时候先调用webServer的begin_trade_record接口插入交易数据&lt;/p&gt; &#xA;&lt;p&gt;请注意这个接口的设计是基于我自身的需求，因为每个人的量化模型，参数等不同，需要研究的也不一样，所以建议这一块自己重写&lt;/p&gt; &#xA;&lt;p&gt;positionRecord主要记录每分钟的账户余额和持仓价值 ，服务于下面的文件&lt;/p&gt; &#xA;&lt;p&gt;webOssUpdate会整理数据，上传到oss，web前端则从oss中读取数据，并且会整理交易流水记录，形成一个以天为单位的统计表格，该处只是简单的统计了盈利和手续费，你可以自行扩展。&lt;/p&gt; &#xA;&lt;h3&gt;binance_f文件夹&lt;/h3&gt; &#xA;&lt;p&gt;币安涉及到api key的接口的处理包，是从官方推荐的github下载后，进行了二次改造的版本&lt;/p&gt; &#xA;&lt;h3&gt;config.py&lt;/h3&gt; &#xA;&lt;p&gt;通用配置，需要自行配置mysql数据库，以及申请飞书api key等，亦可的替换成其他提醒工具&lt;/p&gt; &#xA;&lt;h3&gt;commonFunction.py&lt;/h3&gt; &#xA;&lt;p&gt;通用方法&lt;/p&gt; &#xA;&lt;h3&gt;updateSymbol/trade_symbol.sql&lt;/h3&gt; &#xA;&lt;p&gt;在数据库中生成trade_symbol表格，该表格将控制系统可执行交易的交易对信息，你也可以改造为录入文件中并从文件中读取&lt;/p&gt; &#xA;&lt;h3&gt;updateSymbol/updateTradeSymbol.sql&lt;/h3&gt; &#xA;&lt;p&gt;向trade_symbol表格录入交易对信息，此处进行了一些特殊处理，主要是适应我方的情况，包括只录入usdt的交易对，不录入指数类型的交易对（如btcdom，football这类型），此处部分字段为配合专业手操工具而设计，用于量化的数据字段实际上只有symbol，status&lt;/p&gt; &#xA;&lt;h3&gt;simpleTrade&lt;/h3&gt; &#xA;&lt;p&gt;一个最基础的交易演示程序，当某个交易对，持仓价值为0且一分钟涨幅&amp;gt;1%的时候开多，当他持仓价值&amp;gt;0且一分钟跌幅小于-0.5%平多，如果你是新手，建议关注updateSymbolInfo()这个函数，价格精度，数量精度，最大平仓数量应该是新手会遇到最多的问题。&lt;/p&gt; &#xA;&lt;h2&gt;数据处理部分&lt;/h2&gt; &#xA;&lt;h3&gt;wsServer.cpp&lt;/h3&gt; &#xA;&lt;p&gt;撮合服务器&lt;/p&gt; &#xA;&lt;p&gt;所有的数据都会汇入这里，部分多来源数据会根据数据自带的更新时间戳判断是否更新该条数据，使用以下命令行可编译成可执行文件&lt;/p&gt; &#xA;&lt;p&gt;g++ wsServer.cpp -o wsServer.out -lboost_system&lt;/p&gt; &#xA;&lt;p&gt;该源码使用了两个库 一个是websocketpp，一个是boost&lt;/p&gt; &#xA;&lt;h3&gt;dataPy/uploadDataPy&lt;/h3&gt; &#xA;&lt;p&gt;将程序从一个阿里云的主控服务器，上传到各个对应的阿里云服务器，运行，然后销毁。&lt;/p&gt; &#xA;&lt;p&gt;这里只展示tick，oneMinKlineToWs，specialOneMinKlineToWs三个数据录入程序的使用，其他程序亦同理&lt;/p&gt; &#xA;&lt;p&gt;该程序可以简单快速的发布分布式运行的程序，到所有符合命名规则的服务器上云运行和销毁。&lt;/p&gt; &#xA;&lt;p&gt;使用前需要将阿里云服务器进行统一命名，如tickToWs_1,tickToWs_2...&lt;/p&gt; &#xA;&lt;p&gt;使用前先从本地上传文件到某一个主控服务器，然后在主控服务器运行该程序，正常运行后，包括主控服务器和实际运行的服务器上的所有硬盘应都被覆盖掉源文件的信息。&lt;/p&gt; &#xA;&lt;p&gt;程序会调用get_aliyun_private_ip_arr_by_name函数，搜索对应字符段的阿里云服务器的私网地址，然后上传，执行，并且在三秒后判断有没有在正常运行&lt;/p&gt; &#xA;&lt;p&gt;如正常运行，则其后会对硬盘数据进行覆盖销毁，防止机密数据泄露，只保留程序在内存运行&lt;/p&gt; &#xA;&lt;p&gt;由于是私网地址的操作，需要在同地域的阿里云服务器上执行该程序&lt;/p&gt; &#xA;&lt;h3&gt;dataPy/oneMinKlineToWs.py&lt;/h3&gt; &#xA;&lt;p&gt;该程序属于分布式运行架构，只需要标准化命名即可无限扩展服务器降低延迟 &lt;img src=&#34;https://github.com/Melelery/c-binance-future-quant/assets/139823868/801409a3-25b7-41c8-b795-d7aa0efd0fe6&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;缓更新的1分钟的K线数据读取程序&lt;/p&gt; &#xA;&lt;p&gt;每次程序运行的时候，会像ws服务器发送目前交易对的总数，每次读取kline数据前，会从ws服务器拿到一个交易对编号，拿取的同时，ws服务器会对编号执行 +1的操作，确保分布式架构的时候，每台扩展oneMinKlineToWs都可以按照最佳顺序读取交易对的kline数据。&lt;/p&gt; &#xA;&lt;p&gt;由于币安的k线数据更新延迟略高于tick数据，k线数据来源于数据库，而tick数据来源于缓存，同时，单symbol的延迟会低于全部交易对信息，所以每次更新前还会再读取一次交易对单独的tick数据去校正最后一条k线数据&lt;/p&gt; &#xA;&lt;p&gt;kline数据会向ws服务器发送两次数据，一次是所有读取到的数据，比如说读取kline的时候，设置limit=45，即读取了最近45条kline的数据，但很显然，前面43条的数据是一直不变的，所以交易服务器只需要长时间（30秒/1分钟...）去校正一次即可，只有最新的两条数据的变化概率是大的，需要实时的去读取。&lt;/p&gt; &#xA;&lt;p&gt;所以我拆分成了两条信息，一个是前两段kline，一个是全部kline，前两段kline用于交易服务器实时读取，实时更新，后面两端则用于一定时间间隔后的校对。&lt;/p&gt; &#xA;&lt;p&gt;缩减消息的长度对于交易服务器解析消息所用的时间有极大的帮助。&lt;/p&gt; &#xA;&lt;p&gt;其他时间间隔的k线，如5分钟 ，15分钟，1小时等等读取和打入ws服务器的过程亦同理，只需要简单的替换文件中的参数即可实现，所以此处不再列出&lt;/p&gt; &#xA;&lt;h3&gt;dataPy/specialOneMinKlineToWs.py&lt;/h3&gt; &#xA;&lt;p&gt;该程序属于分布式运行架构，只需要标准化命名即可无限扩展服务器降低延迟&lt;/p&gt; &#xA;&lt;p&gt;急速更新的1分钟的K线数据读取程序&lt;/p&gt; &#xA;&lt;p&gt;与上面不同的是，此处的读取数据有一个前置的交易量条件，你可以理解为我的量化系统只有满足某个交易量条件的要求时候才会开仓，所以针对这一部分可能开仓的交易对，铺设了专门读取数据的服务器。&lt;/p&gt; &#xA;&lt;p&gt;假设本来有200个交易对轮流读取，限制条件后，变成了20个交易对，那么等于你单个机器的数据读取数据提高了10倍&lt;/p&gt; &#xA;&lt;p&gt;这个只是一个展示程序，实际上你应该根据你的交易条件去自己编写相应的条件，限制数据读取的交易对。&lt;/p&gt; &#xA;&lt;p&gt;其他时间间隔的k线，如5分钟 ，15分钟，1小时等等读取和打入ws服务器的过程亦同理，只需要简单的替换文件中的参数即可实现，所以此处不再列出&lt;/p&gt; &#xA;&lt;h3&gt;dataPy/tickToWs.py&lt;/h3&gt; &#xA;&lt;p&gt;该程序属于分布式运行架构，只需要标准化命名即可无限扩展服务器降低延迟&lt;/p&gt; &#xA;&lt;p&gt;tick数据读取程序会读取阿里云上，所有的tick服务器数量，然后自动锁定该服务器在一秒内的某个时间段内去进行数据读取&lt;/p&gt; &#xA;&lt;p&gt;打个比方，现在我们开通了五台tick服务器，那么tick 1服务器会在每一秒的&amp;gt;=0 &amp;lt;200毫秒的时间段内去读取数据，tick 2会在每一秒的&amp;gt;=200 &amp;lt;400毫秒的时间段内去读取数据...以此类推&lt;/p&gt; &#xA;&lt;p&gt;tick数据在汇入ws服务器后，交易程序读取这部分主要用于修正最新一条kline的最高价格，最低价格和最新价格。&lt;/p&gt; &#xA;&lt;h3&gt;dataPy/useData.py&lt;/h3&gt; &#xA;&lt;p&gt;展示了如何从ws服务器拿取one min kline数据和tick数据后，如何在本地端自行拼合，维护一个k线数据，此处还可以扩展到加入trade vol等数据，原理相同所以不再展示&lt;/p&gt; &#xA;&lt;h2&gt;关键操作部分&lt;/h2&gt; &#xA;&lt;h3&gt;binanceOrdersRecord.py&lt;/h3&gt; &#xA;&lt;p&gt;记录orders信息，方便后续分析，例如可以通过orders的记录分析出发出去的订单的总成交比例等等&lt;/p&gt; &#xA;&lt;h3&gt;binanceTradesRecord.py&lt;/h3&gt; &#xA;&lt;p&gt;记录trades信息，方便后续分析，例如可以通过trades计算出总交易量等等&lt;/p&gt; &#xA;&lt;h3&gt;checkTimeoutOrders.py&lt;/h3&gt; &#xA;&lt;p&gt;检查是否有超时订单并取消，同时可以附加一些交易操作，如maker挂单超过多少秒没有成交则按比例转化成take订单&lt;/p&gt; &#xA;&lt;p&gt;由于需要调用币安获取全部挂单的api，而该api权重极高，为了满足更加灵敏的扫描，我的五个交易服务器同时运行有webserver程序，checkTimeoutOrders会轮流从五个交易服务器读取全部挂单信息&lt;/p&gt; &#xA;&lt;h3&gt;commission.py&lt;/h3&gt; &#xA;&lt;p&gt;记录所有资金流水，这个是最重要的数据，通过他可以计算出手续费，盈利，资金费用等等数据&lt;/p&gt; &#xA;&lt;p&gt;由于commission长时间积累的数据量大，所以这里有两个表，一个是持续记录的表，一个是24小时临时记录的表。&lt;/p&gt; &#xA;&lt;p&gt;24小时记录的临时表主要用于分析最近一天的亏损情况，从而给交易系统进行风险控制&lt;/p&gt; &#xA;&lt;p&gt;比如以下这段代码&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;for key in fourHoursProfitObj:&#xA;    if fourHoursProfitObj[key]&amp;lt;=-150 or oneDayProfitObj[key]&amp;lt;=-1800:&#xA;        banSymbolArr.append(key)&#xA;&#xA;if allOneDayProfit&amp;lt;=-3000:&#xA;    banSymbolArr = [&#34;ALL&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;当读取到某个交易对四小时盈利小于150u或者24小时利润小于1800u时，会向ws服务器发送一个禁止交易的交易对列表，当所有交易对总利润小于-3000u时，则直接全部暂停交易&lt;/p&gt; &#xA;&lt;h3&gt;getBinancePosition.py&lt;/h3&gt; &#xA;&lt;p&gt;通过/fapi/v2/account接口获取币安仓位和余额信息，并且上传到本服务器的80端口，以及ws服务，上传到ws服务器的信息，会与positionRisk和wsPosition拿到信息的更新时间戳进行对比，选择最新的信息发送给交易服务器，本地80端口的json文件为旧版本使用的方案，其他服务器会通过80端口读取json文件获取数据，后面采用了ws但是这里保留下来了，也算是一个额外的采集方案&lt;/p&gt; &#xA;&lt;h3&gt;positionRisk.py&lt;/h3&gt; &#xA;&lt;p&gt;通过/fapi/v2/positionRisk接口获取币安仓位和余额信息，其他同上&lt;/p&gt; &#xA;&lt;h3&gt;wsPosition.py&lt;/h3&gt; &#xA;&lt;p&gt;通过websocket接口获取币安仓位和余额信息，其他同上&lt;/p&gt; &#xA;&lt;h3&gt;makerStopLoss&lt;/h3&gt; &#xA;&lt;p&gt;读取ws服务器读取仓位信息后，读取该币种的币安接口挂单信息，之所以不采用所有symbol的挂单信息，是因为权重太高，会导致止损过于迟钝。&lt;/p&gt; &#xA;&lt;p&gt;在仓位最大值发生超过5%变化的同时，挂出最大止损单，演示文件的写法，是以成本价5%为初始止损价格，并且拆分成五单，每单往后增加0.5%的价格进行止损，防止深度的影响&lt;/p&gt; &#xA;&lt;p&gt;例：现在仓位是1000u，已有止损单，那么当仓位增加到1001u的时候，该系统不会重设止损，因为不满足数量变化&amp;gt;5%的情况，太过灵敏的重设会到权重消耗等等问题&lt;/p&gt; &#xA;&lt;p&gt;如增加到1060u，那么会重新设定五个止损单，初始止损价格为成本价的5%，后续每个止损单依次为5.5%,6%,6.5%,7%,&lt;/p&gt; &#xA;&lt;p&gt;新止损设置完后，系统会读取挂单检查是否成功，确认成功后，才会撤销旧的止损单。&lt;/p&gt; &#xA;&lt;h2&gt;安全风控部分&lt;/h2&gt; &#xA;&lt;p&gt;由于程序源码会带有敏感信息，建议采用dataPy/uploadDataPy的上传方式，统一进行文件的上传，运行，销毁，实现最终只在内存中运行，而覆盖硬盘所有储存的程序信息的目的，仅在本地段保有源码。&lt;/p&gt; &#xA;&lt;p&gt;建议关闭阿里云所有对外的端口，在购买服务器的时候选中将所有服务器置于统一私网前缀IP下，这样即可在关闭外网端口的同时实现功能正常运转和互通，如没有在统一私网前缀IP下，则需要添加对应的私网IP到部分服务器的安全组内&lt;/p&gt; &#xA;&lt;p&gt;在需要操作服务器的时候，再将本地的IP添加到安全组内，并且使用完后即时删除&lt;/p&gt; &#xA;&lt;p&gt;建议币安的api绑定服务器的IP&lt;/p&gt; &#xA;&lt;p&gt;建议保持操作系统的更新&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;1.为什么不用历史数据先回测&lt;/h2&gt; &#xA;&lt;p&gt;事实上我个人探究量化实打实算起来已经有三年。&lt;/p&gt; &#xA;&lt;p&gt;以历史数据回测后拟合一条折线出来的方式并不是没有尝试过。&lt;/p&gt; &#xA;&lt;p&gt;但是回测的环境要做到与实盘一致的难度可能超过了目前大部分人的估计，我说的一致是绝对的一致，任何一点小差异其实都会在整个过程被放大到最后让你无法接受的程度。&lt;/p&gt; &#xA;&lt;p&gt;并且存在一个无法验证到底是误差还是策略造成的损益差异的问题。&lt;/p&gt; &#xA;&lt;p&gt;因为很可能到你结束项目，还存在数十个你没有发现的误差的地方，而且不具备量化判断这些误差造成损益的可能性。&lt;/p&gt; &#xA;&lt;p&gt;当然这是以我的能力和视角得出的结论。&lt;/p&gt; &#xA;&lt;p&gt;所以自最近半年起，我的思路都是直接上实盘，哪怕是小资金验证后，以实盘的数据为基础去调参&lt;/p&gt; &#xA;&lt;h2&gt;2.部分存在性能改进的可能性&lt;/h2&gt; &#xA;&lt;p&gt;是的，因为这个是一个个人项目，我要负责的事情非常多。&lt;/p&gt; &#xA;&lt;p&gt;所以对于一些不需要追求性能的地方，我都会用最简单的写法带过。&lt;/p&gt; &#xA;&lt;p&gt;比如说orders，trades这些录入数据库是拿最近1000条数据出来比较，没有重复的就插入，这里当然存在性能更优解的写法，但是在我看来意义不大所以我没有花时间去改进。&lt;/p&gt; &#xA;&lt;p&gt;为什么意义不大，因为我用最低配的mysql整套系统运行的过程中cpu和内存的使用比例也没有超过50%。&lt;/p&gt; &#xA;&lt;p&gt;我大部分精力关注在数据录入，交易的性能优化，而忽略交易后数据拉取分析这一块的优化，这一块只要最后的结果是对的即可。&lt;/p&gt; &#xA;&lt;p&gt;交易后的过程，消耗了1个性能还是100个性能，只要没有达到我硬件的峰值，我都不会去寻求改变&lt;/p&gt;</summary>
  </entry>
</feed>