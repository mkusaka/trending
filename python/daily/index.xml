<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-24T01:39:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>stas00/ml-engineering</title>
    <updated>2023-09-24T01:39:11Z</updated>
    <id>tag:github.com,2023-09-24:/stas00/ml-engineering</id>
    <link href="https://github.com/stas00/ml-engineering" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning Engineering Guides and Tools&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Engineering Guides and Tools&lt;/h1&gt; &#xA;&lt;p&gt;An open collection of methodologies to help with successful training of large language models and multi-modal models.&lt;/p&gt; &#xA;&lt;p&gt;This is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs.&lt;/p&gt; &#xA;&lt;p&gt;This repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM-176B&lt;/a&gt; model in 2022 and &lt;a href=&#34;https://huggingface.co/HuggingFaceM4/idefics-80b-instruct&#34;&gt;IDEFICS-80B&lt;/a&gt; multi-modal model in 2023. Currently, I&#39;m working on developing/training open-source Retrieval Augmented models at &lt;a href=&#34;https://contextual.ai/&#34;&gt;Contextual.AI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve been compiling this information mostly for myself so that I could quickly find solutions I have already researched in the past and which have worked, but as usual I&#39;m happy to share these with the wider ML community.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/debug/&#34;&gt;Debugging software and hardware failures&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/performance/&#34;&gt;Performance&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/multi-node&#34;&gt;Multi-Node networking&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/model-parallelism/&#34;&gt;Model parallelism&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/dtype/&#34;&gt;Tensor precision / Data types&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/hparams/&#34;&gt;Training hyper-parameters and model initializations&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/reproducibility/&#34;&gt;Reproducibility&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/instabilities/&#34;&gt;Instabilities&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/slurm/&#34;&gt;SLURM&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/resources/&#34;&gt;Resources&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/transformers/&#34;&gt;HF Transformers notes&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you found a bug, typo or would like to propose an improvement please don&#39;t hesitate to open an &lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/Issue&#34;&gt;Issue&lt;/a&gt; or contribute a PR.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The content of this site is distributed under &lt;a href=&#34;https://raw.githubusercontent.com/stas00/ml-engineering/master/LICENSE-CC-BY-SA&#34;&gt;Attribution-ShareAlike 4.0 International&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Dicklesworthstone/automatic_log_collector_and_analyzer</title>
    <updated>2023-09-24T01:39:11Z</updated>
    <id>tag:github.com,2023-09-24:/Dicklesworthstone/automatic_log_collector_and_analyzer</id>
    <link href="https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Replace Splunk in your small company with this one weird trick!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Automatically Download and Analyze Log Files from Remote Machines&lt;/h1&gt; &#xA;&lt;p&gt;This application is designed to collect and analyze logs from remote machines hosted on Amazon Web Services (AWS) and other cloud hosting services.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This application was specifically designed for use with Pastel Network&#39;s log files. However, it can be easily adapted to work with any log files by modifying the parsing functions, data models, and specifying the location and names of the log files to be downloaded. It is compatible with log files stored in a standard format, where each entry is on a separate line and contains a timestamp, a log level, and a message. The application has been tested with log files several gigabytes in size from dozens of machines and can process all of it in minutes. It is designed for Ubuntu 22.04+, but can be adapted for other Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Dicklesworthstone/automatic_log_collector_and_analyzer/main/demo_screenshot.png&#34; alt=&#34;Demo Screenshot:&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Customization&lt;/h2&gt; &#xA;&lt;p&gt;To adapt this application for your own use case, refer to the included sample log files and compare them to the parsing functions in the code. You can also modify the data models to store log entries as desired.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The application consists of various Python scripts that perform the following functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect to Remote Machines&lt;/strong&gt;: Using the boto3 library for AWS instances and an Ansible inventory file for non-AWS instances, the application establishes SSH connections to each remote machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Download and Parse Log Files&lt;/strong&gt;: Downloads specified log files from each remote machine and parses them. The parsed log entries are then queued for database insertion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Insert Log Entries into Database&lt;/strong&gt;: Uses SQLAlchemy to insert the parsed log entries from the queue into an SQLite database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Process and Analyze Log Entries&lt;/strong&gt;: Processes and analyzes log entries stored in the database, offering functions to find error entries and create views of aggregated data based on specified criteria.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate Network Activity Data&lt;/strong&gt;: Fetches and processes network activity data from each remote machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Expose Database via Web App using Datasette&lt;/strong&gt;: Once the database is generated, it can be shared over the web using Datasette.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The tool is compatible with both AWS-hosted instances and any list of Linux instances stored in a standard Ansible inventory file with the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;all:&#xA;  vars:&#xA;    ansible_connection: ssh&#xA;    ansible_user: ubuntu&#xA;    ansible_ssh_private_key_file: /path/to/ssh/key/file.pem&#xA;  hosts:&#xA;    MyCoolMachine01:&#xA;      ansible_host: 1.2.3.41&#xA;    MyCoolMachine02:&#xA;      ansible_host: 1.2.3.19&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Both can be used seamlessly.)&lt;/p&gt; &#xA;&lt;h2&gt;Warning&lt;/h2&gt; &#xA;&lt;p&gt;To simplify the code, the tool is designed to delete all downloaded log files and generated databases each time it runs. Consequently, this can consume significant bandwidth depending on your log files&#39; size. However, the design&#39;s high level of parallel processing and concurrency allows it to run quickly, even when connecting to dozens of remote machines and downloading hundreds of log files.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Designed for Ubuntu 22.04+, first install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m venv venv&#xA;source venv/bin/activate&#xA;python3 -m pip install --upgrade pip&#xA;python3 -m pip install wheel&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need to install Redis:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install redis -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And install Datasette to expose the results as a website:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install pipx -y &amp;amp;&amp;amp; pipx ensurepath &amp;amp;&amp;amp; pipx install datasette&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the application every 30 minutes as a cron job, execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crontab -e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And add the following line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;*/15 * * * * . $HOME/.profile; /home/ubuntu/automatic_log_collector_and_analyzer/venv/bin/python /home/ubuntu/automatic_log_collector_and_analyzer/automatic_log_collector_and_analyzer.py &amp;gt;&amp;gt; /home/ubuntu/automatic_log_collector_and_analyzer/log_$(date +\%Y-\%m-\%dT\%H_\%M_\%S).log 2&amp;gt;&amp;amp;1&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/alphamissense</title>
    <updated>2023-09-24T01:39:11Z</updated>
    <id>tag:github.com,2023-09-24:/google-deepmind/alphamissense</id>
    <link href="https://github.com/google-deepmind/alphamissense" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AlphaMissense&lt;/h1&gt; &#xA;&lt;p&gt;This package provides the AlphaMissense model implementation. This implementation is provided for reference alongside the &lt;a href=&#34;https://www.science.org/doi/10.1126/science.adg7492&#34;&gt;AlphaMissense 2023 publication&lt;/a&gt; and will not be actively maintained moving forward.&lt;/p&gt; &#xA;&lt;p&gt;We forked the &lt;a href=&#34;https://github.com/google-deepmind/alphafold/tree/v2.3.2&#34;&gt;AlphaFold repository&lt;/a&gt; and modified it to implement AlphaMissense.&lt;/p&gt; &#xA;&lt;p&gt;What we provide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Detailed implementation of the AlphaMissense model and training losses (&lt;a href=&#34;https://github.com/deepmind/alphamissense/raw/main/alphamissense/model/modules_missense.py&#34;&gt;modules_missense.py&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The data pipeline to create input features for inference (&lt;a href=&#34;https://github.com/deepmind/alphamissense/raw/main/alphamissense/data/pipeline_missense.py&#34;&gt;pipeline_missense.py&lt;/a&gt;). The data pipeline requires access to genetic databases for multiple sequence alignments and, if using spatial cropping, protein structures of the AlphaFold Database hosted in Google Cloud Storage. Please see the section for genetic databases and &lt;a href=&#34;https://github.com/deepmind/alphafold/tree/main/afdb&#34;&gt;AFDB readme file&lt;/a&gt;) to learn how to access these datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pre-computed predictions for all possible human amino acid substitutions and missense variants (&lt;a href=&#34;https://console.cloud.google.com/storage/browser/dm_alphamissense&#34;&gt;hosted here&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;What we don’t provide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The trained AlphaMissense model weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Access AlphaMissense predictions:&lt;/h2&gt; &#xA;&lt;p&gt;Predictions for human major transcripts and isoforms are provided &lt;a href=&#34;https://console.cloud.google.com/storage/browser/dm_alphamissense&#34;&gt;here&lt;/a&gt;. You can use these files with the VEP tool and &lt;a href=&#34;https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html&#34;&gt;AlphaMissense plug-in&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install all dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install python3.11-venv aria2 hmmer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone this repository and &lt;code&gt;cd&lt;/code&gt; into it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/deepmind/alphamissense.git&#xA;cd ./alphamissense&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set up a Python virtual environment and install the Python dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m venv ./venv&#xA;venv/bin/pip install -r requirements.txt&#xA;venv/bin/pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Test the installation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;venv/bin/python test/test_installation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Because we are not releasing the trained model weights, the code is not meant to be used for making new predictions, but serve as an implementation reference. We are releasing the data pipeline, model and loss function code.&lt;/p&gt; &#xA;&lt;p&gt;The data pipeline requires a FASTA file (i.e. &lt;code&gt;protein_sequence_file&lt;/code&gt;) which should contain all target sequences, and the genetic sequence databases outlined in the next section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from alphamissense.data import pipeline_missense&#xA;&#xA;protein_sequence_file = ...&#xA;pipeline = pipeline_missense.DataPipeline(&#xA;    jackhmmer_binary_path=...,  # Typically &#39;/usr/bin/jackhmmer&#39;.&#xA;    protein_sequence_file=protein_sequence_file,&#xA;    uniref90_database_path=DATABASES_DIR + &#39;/uniref90/uniref90.fasta&#39;,&#xA;    mgnify_database_path=DATABASES_DIR + &#39;/mgnify/mgy_clusters_2022_05.fa&#39;,&#xA;    small_bfd_database_path=DATABASES_DIR + &#39;/small_bfd/bfd-first_non_consensus_sequences.fasta&#39;,&#xA;)&#xA;&#xA;sample = pipeline.process(&#xA;    protein_id=...,  # Sequence identifier in the FASTA file.&#xA;    reference_aa=...,  # Single capital letter, e.g. &#39;A&#39;.&#xA;    alternate_aa=...,&#xA;    position=...,  # Integer, note that the position is 1-based!&#xA;    msa_output_dir=msa_output_dir,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model is implemented as a JAX module and can be instantiated for example as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from alphamissense.model import config&#xA;from alphamissense.model import modules_missense&#xA;&#xA;def _forward_fn(batch):&#xA;    model = modules_missense.AlphaMissense(config.model_config().model)&#xA;    return model(batch, is_training=False, return_representations=False)&#xA;&#xA;random_seed = 0&#xA;prng = jax.random.PRNGKey(random_seed)&#xA;&#xA;params = hk.transform(_forward_fn).init(prng, sample)&#xA;apply = jax.jit(hk.transform(_forward_fn).apply)&#xA;output = apply(params, prng, sample)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, at this point the score of the variant would be stored in &lt;code&gt;output[&#39;logit_diff&#39;][&#39;variant_pathogenicity&#39;]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Genetic databases&lt;/h2&gt; &#xA;&lt;p&gt;AlphaMissense used multiple genetic (sequence) databases for multiple sequence alignments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebi.ac.uk/metagenomics/&#34;&gt;MGnify&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uniprot.org/help/uniref&#34;&gt;UniRef90&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We refer to the &lt;a href=&#34;https://github.com/deepmind/alphafold&#34;&gt;AlphaFold repository&lt;/a&gt; for instructions on how to download these databases.&lt;/p&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;p&gt;Any publication that discloses findings arising from using this source code should cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article {AlphaMissense2023,&#xA;  author       = {Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider, Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli, Žiga Avsec},&#xA;  journal      = {Science},&#xA;  title        = {Accurate proteome-wide missense variant effect prediction with AlphaMissense},&#xA;  year         = {2023},&#xA;  doi          = {10.1126/science.adg7492},&#xA;  URL          = {https://www.science.org/doi/10.1126/science.adg7492},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;AlphaMissense communicates with and/or references the following separate libraries and packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Abseil&lt;/li&gt; &#xA; &lt;li&gt;Biopython&lt;/li&gt; &#xA; &lt;li&gt;HMMER Suite&lt;/li&gt; &#xA; &lt;li&gt;Haiku&lt;/li&gt; &#xA; &lt;li&gt;Immutabledict&lt;/li&gt; &#xA; &lt;li&gt;JAX&lt;/li&gt; &#xA; &lt;li&gt;Matplotlib&lt;/li&gt; &#xA; &lt;li&gt;NumPy&lt;/li&gt; &#xA; &lt;li&gt;Pandas&lt;/li&gt; &#xA; &lt;li&gt;SciPy&lt;/li&gt; &#xA; &lt;li&gt;Tree&lt;/li&gt; &#xA; &lt;li&gt;Zstandard We thank all their contributors and maintainers!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License and Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;The information within the AlphaMissense Database and the model implementation is provided for theoretical modelling only, caution should be exercised in its use. This information is not intended to be a substitute for professional medical advice, diagnosis, or treatment, and does not constitute medical or other professional advice.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2023 DeepMind Technologies Limited.&lt;/p&gt; &#xA;&lt;h3&gt;AlphaMissense Code License&lt;/h3&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h3&gt;AlphaMissense predictions License&lt;/h3&gt; &#xA;&lt;p&gt;AlphaMissense predictions are made available under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. You can find details at: &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Third-party software&lt;/h3&gt; &#xA;&lt;p&gt;Use of the third-party software, libraries or code referred to in the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/alphamissense/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt; section above may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use.&lt;/p&gt;</summary>
  </entry>
</feed>