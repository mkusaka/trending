<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-02T01:36:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tyxsspa/AnyText</title>
    <updated>2024-01-02T01:36:46Z</updated>
    <id>tag:github.com,2024-01-02:/tyxsspa/AnyText</id>
    <link href="https://github.com/tyxsspa/AnyText" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AnyText: Multilingual Visual Text Generation And Editing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.03054&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tyxsspa/AnyText&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-Github-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/damo/studio_anytext&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-ModelScope-lightblue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tyxsspa/AnyText/main/docs/sample.jpg&#34; alt=&#34;sample&#34; title=&#34;sample&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìåNews&lt;/h2&gt; &#xA;&lt;p&gt;[2023.12.28] - Online demo is available &lt;a href=&#34;https://modelscope.cn/studios/damo/studio_anytext/summary&#34;&gt;here&lt;/a&gt;!&lt;br&gt; [2023.12.27] - üß®We released the latest checkpoint(v1.1) and inference code, check on &lt;a href=&#34;https://modelscope.cn/models/damo/cv_anytext_text_generation_editing/summary&#34;&gt;modelscope&lt;/a&gt; in Chinese.&lt;br&gt; [2023.12.05] - The paper is available at &lt;a href=&#34;https://arxiv.org/abs/2311.03054&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üí°Methodology&lt;/h2&gt; &#xA;&lt;p&gt;AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tyxsspa/AnyText/main/docs/framework.jpg&#34; alt=&#34;framework&#34; title=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install git (skip if already done)&#xA;conda install -c anaconda git&#xA;# Clone anytext code&#xA;git clone https://github.com/tyxsspa/AnyText.git&#xA;cd AnyText&#xA;# Prepare a font file; Arial Unicode MS is recommended, **you need to download it on your own**&#xA;mv your/path/to/arialuni.ttf ./font/Arial_Unicode.ttf&#xA;# Create a new environment and install packages as follows:&#xA;conda env create -f environment.yaml&#xA;conda activate anytext&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîÆInference&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Recommend]&lt;/strong&gt;Ôºö We release a &lt;a href=&#34;https://modelscope.cn/studios/damo/studio_anytext/summary&#34;&gt;demo&lt;/a&gt; on ModelScope!&lt;/p&gt; &#xA;&lt;p&gt;AnyText include two modes: Text Generation and Text Editing. Running the simple code below to perform inference in both modes and verify whether the environment is correctly installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have advanced GPU (with at least 20G memory), it is recommended to deploy our demo as below, which includes usage instruction, user interface and abundant examples.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tyxsspa/AnyText/main/docs/demo.jpg&#34; alt=&#34;demo&#34; title=&#34;demo&#34;&gt; &lt;strong&gt;Please note&lt;/strong&gt; that when executing inference for the first time, the model files will be downloaded to: &lt;code&gt;~/.cache/modelscope/hub&lt;/code&gt;. If you need to modify the download directory, you can manually specify the environment variable: &lt;code&gt;MODELSCOPE_CACHE&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üåÑGallery&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tyxsspa/AnyText/main/docs/gallery.png&#34; alt=&#34;gallery&#34; title=&#34;gallery&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìàEvaluation&lt;/h2&gt; &#xA;&lt;p&gt;We use Sentence Accuracy (Sen. ACC) and Normalized Edit Distance (NED) to evaluate the accuracy of generated text, and use the FID metric to assess the quality of generated images. Compared to existing methods, AnyText has a significant advantage in both Chinese and English text generation. &lt;img src=&#34;https://raw.githubusercontent.com/tyxsspa/AnyText/main/docs/eval.jpg&#34; alt=&#34;eval&#34; title=&#34;eval&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚è∞TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the model and inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Provide publicly accessible demo link&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release tools for merging weights from community models or LoRAs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release AnyText-benchmark dataset and evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release AnyWord-3M dataset and training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tuo2023anytext,&#xA;      title={AnyText: Multilingual Visual Text Generation And Editing}, &#xA;      author={Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie},&#xA;      year={2023},&#xA;      eprint={2311.03054},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>joaomdmoura/crewAI-examples</title>
    <updated>2024-01-02T01:36:46Z</updated>
    <id>tag:github.com,2024-01-02:/joaomdmoura/crewAI-examples</id>
    <link href="https://github.com/joaomdmoura/crewAI-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Examples for crewAI&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;crewAI is designed to facilitate the collaboration of role-playing AI agents. This is a collection of examples of different ways to use the crewAI framework to automate the processes. By &lt;a href=&#34;https://x.com/joaomdmoura&#34;&gt;@joaomdmoura&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/trip_planner&#34;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/stock_analysis&#34;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/landing_page_generator&#34;&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>DLYuanGod/TinyGPT-V</title>
    <updated>2024-01-02T01:36:46Z</updated>
    <id>tag:github.com,2024-01-02:/DLYuanGod/TinyGPT-V</id>
    <link href="https://github.com/DLYuanGod/TinyGPT-V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyGPT-V&lt;/h1&gt; &#xA;&lt;p&gt;&lt;font size=&#34;5&#34;&gt;&lt;strong&gt;TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt; &#xA;&lt;p&gt;Zhengqing Yuan‚ùÅ, Zhaoxu Li‚ùÅ, Lichao Sun‚ùã&lt;/p&gt; &#xA;&lt;p&gt;‚ùÅVisiting Students at LAIR Lab, Lehigh University ‚ùãLehigh University&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.16862&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V&#34;&gt;  &lt;h2&gt;News&lt;/h2&gt; &lt;p&gt;[Dec.28 2023] Breaking! We release the code of our TinyGPT-V.&lt;/p&gt; &lt;h2&gt;TinyGPT-V Traning Process&lt;/h2&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/examples/Training_S.png&#34; alt=&#34;Traning_Process&#34;&gt;&lt;/p&gt; &lt;h2&gt;TinyGPT-V Model Structure&lt;/h2&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/examples/TinyGPT-V-ST.png&#34; alt=&#34;Model&#34;&gt;&lt;/p&gt; &lt;h2&gt;TinyGPT-V Results&lt;/h2&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/examples/result.png&#34; alt=&#34;Results&#34;&gt;&lt;/p&gt; &lt;h2&gt;Getting Started&lt;/h2&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;1. Prepare the code and the environment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Git clone our repository, creating a python environment and activate it via the following command&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DLYuanGod/TinyGPT-V.git&#xA;cd TinyGPT-V&#xA;conda env create -f environment.yml&#xA;conda activate tinygptv&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. Prepare the pretrained LLM weights&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TinyGPT-V&lt;/strong&gt; is based on Phi-2. Download the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.&lt;/p&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V&#34;&gt;Phi-2 2.7B: &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/susnato/phi-2&#34;&gt;Download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then, set the variable &lt;em&gt;phi_model&lt;/em&gt; in the model config file to the LLM weight path.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For MiniGPT-v2, set the LLM path &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/minigpt4/configs/models/minigpt_v2.yaml#L14&#34;&gt;here&lt;/a&gt; at Line 14 and &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/minigpt4/configs/models/minigpt4_vicuna0.yaml#L18&#34;&gt;here&lt;/a&gt; at Line 18.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Prepare the pretrained model checkpoints&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pretrained model checkpoints&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;After stage-1&lt;/th&gt; &#xA;   &lt;th&gt;After stage-2&lt;/th&gt; &#xA;   &lt;th&gt;After stage-3&lt;/th&gt; &#xA;   &lt;th&gt;After stage-4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage1.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage2.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage3.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage4.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For &lt;strong&gt;TinyGPT-V&lt;/strong&gt;, set the path to the pretrained checkpoint in the evaluation config file in &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#L8&#34;&gt;tinygptv_stage1_2_3_eval.yaml&lt;/a&gt; at Line 8 for Stage 1, 2 and 3 version or &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/eval_configs/tinygptv_stage4_eval.yaml#L8&#34;&gt;tinygptv_stage4_eval.yaml&lt;/a&gt; for Stage 4 version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Update the Phi-2 Modeling for transformers lib.&lt;/strong&gt; Linux system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp modeling_phi.py /miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows system&lt;/p&gt; &#xA;&lt;p&gt;Find your conda yourself: conda_sit/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/ Replace modeling_phi.py in that directory with the one in TinyGPT-V/modeling_phi.py.&lt;/p&gt; &#xA;&lt;h3&gt;Launching Demo Locally&lt;/h3&gt; &#xA;&lt;p&gt;For Stage 4, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Stage 1, 2 and 3, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To perfer more powerful model, LLMs loads as 16 bit by default. This configuration requires about 8G GPU memory. To more save GPU memory, you can run the model in 8 bit below 8G device by setting &lt;code&gt;low_resource&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in the relevant config file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Stage 4 &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/eval_configs/tinygptv_stage4_eval.yaml#6&#34;&gt;tinygptv_stage4_eval.yaml&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stage 1, 2 and 3 &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#6&#34;&gt;tinygptv_stage1_2_3_eval.yaml&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;-Note: Stage 4 is currently a test version as it utilizes partial data for traing. Please use Stage 3 for the demo.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;First you need to adjust all the updated weights in the LLM to be calculated with full precision:&lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/minigpt4%5Cmodels%5Cbase_model.py&#34;&gt;Here&lt;/a&gt;. Remove the comments from the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()&#xA;                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()&#xA;                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()&#xA;                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()&#xA;&#xA;                # Perform a similar operation for the bias item&#xA;                if layer.self_attn.q_layernorm.bias is not None:&#xA;                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()&#xA;                if layer.self_attn.k_layernorm.bias is not None:&#xA;                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()&#xA;                if layer.input_layernorm.bias is not None:&#xA;                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()&#xA;&#xA;&#xA;            llama_model.model.model.final_layernorm.weight.requires_grad = True&#xA;            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()&#xA;            if llama_model.model.model.final_layernorm.bias is not None:&#xA;                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage 1 and 2:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Datasets: &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4/raw/main/dataset/README_1_STAGE.md&#34;&gt;first stage dataset preparation instruction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You need to execute the above code 17 times to complete the first stage of training.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage 3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Datasets: &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4/raw/main/dataset/README_2_STAGE.md&#34;&gt;stage 3 dataset preparation instruction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage 4:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Datasets: &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4/raw/main/dataset/README_MINIGPTv2_FINETUNE.md&#34;&gt;stage 4 dataset preparation instruction&lt;/a&gt; Please prepare all datasets except COCO captions and OCR-VQA.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;For eval. details of TinyGPT-V, check &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/eval_scripts/EVAL_README.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#DLYuanGod/TinyGPT-V&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=DLYuanGod/TinyGPT-V&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT&lt;/a&gt; A very versatile model of MLLMs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using TinyGPT-V in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;@misc{yuan2023tinygptv,&#xA;      title={TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones}, &#xA;      author={Zhengqing Yuan and Zhaoxu Li and Lichao Sun},&#xA;      year={2023},&#xA;      eprint={2312.16862},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is under &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/LICENSE.md&#34;&gt;BSD 3-Clause License&lt;/a&gt;. Many codes are based on &lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; with BSD 3-Clause License &lt;a href=&#34;https://raw.githubusercontent.com/DLYuanGod/TinyGPT-V/main/LICENSE_Lavis.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>