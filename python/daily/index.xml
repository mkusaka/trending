<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-11T01:44:10Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepinsight/insightface</title>
    <updated>2023-01-11T01:44:10Z</updated>
    <id>tag:github.com,2023-01-11:/deepinsight/insightface</id>
    <link href="https://github.com/deepinsight/insightface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State-of-the-art 2D and 3D Face Analysis Project&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InsightFace: 2D and 3D Face Analysis Project&lt;/h1&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img src=&#34;https://insightface.ai/assets/img/custom/logo3.jpg&#34; width=&#34;240&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;InsightFace project is mainly maintained By &lt;a href=&#34;mailto:guojia@gmail.com?subject=%5BGitHub%5D%20InsightFace%20Project&#34;&gt;Jia Guo&lt;/a&gt; and &lt;a href=&#34;https://jiankangdeng.github.io/&#34;&gt;Jiankang Deng&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For all main contributors, please check &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/#contributing&#34;&gt;contributing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Top News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-11-28&lt;/code&gt;&lt;/strong&gt;: Single line code for facial identity swapping in our python packge ver 0.7, please check the example &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/examples/in_swapper&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-10-28&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;http://iccv21-mfr.com&#34;&gt;MFR-Ongoing&lt;/a&gt; website is refactored, please create issues if there&#39;s any bug.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-09-22&lt;/code&gt;&lt;/strong&gt;: Now we have &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/web-demos&#34;&gt;web-demos&lt;/a&gt;: &lt;a href=&#34;http://demo.insightface.ai:7007/&#34;&gt;face-localization&lt;/a&gt;, &lt;a href=&#34;http://demo.insightface.ai:7008/&#34;&gt;face-recognition&lt;/a&gt;, and &lt;a href=&#34;http://demo.insightface.ai:7009/&#34;&gt;face-swapping&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-08-12&lt;/code&gt;&lt;/strong&gt;: We achieved Rank-1st of &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/531961/introduction&#34;&gt;Perspective Projection Based Monocular 3D Face Reconstruction Challenge&lt;/a&gt; of &lt;a href=&#34;https://sites.google.com/view/wcpa2022&#34;&gt;ECCV-2022 WCPA Workshop&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2208.07142&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/reconstruction/jmlr&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-03-30&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2203.15565&#34;&gt;Partial FC&lt;/a&gt; accepted by CVPR-2022.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-02-23&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/scrfd&#34;&gt;SCRFD&lt;/a&gt; accepted by &lt;a href=&#34;https://iclr.cc/Conferences/2022&#34;&gt;ICLR-2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-01-29&lt;/code&gt;&lt;/strong&gt;: Python pip package ver 0.6.2 updated, added pose estimation and fixed model downloading urls, see &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/python-package&#34;&gt;detail&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2022-01-18&lt;/code&gt;&lt;/strong&gt;: Ambiguity-Aware Human Pose Estimation is now available at &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/body/human_pose/ambiguity_aware&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-11-30&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/challenges/mfr&#34;&gt;MFR-Ongoing&lt;/a&gt; challenge launched(same with IFRT), which is an extended version of &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/challenges/iccv21-mfr&#34;&gt;iccv21-mfr&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-11-25&lt;/code&gt;&lt;/strong&gt;: Training face landmarks by synthetic data, see &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/alignment/synthetics&#34;&gt;alignment/synthetics&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-10-29&lt;/code&gt;&lt;/strong&gt;: We achieved 1st place on the &lt;a href=&#34;https://pages.nist.gov/frvt/plots/11/visa.html&#34;&gt;VISA track&lt;/a&gt; of &lt;a href=&#34;https://pages.nist.gov/frvt/html/frvt11.html&#34;&gt;NIST-FRVT 1:1&lt;/a&gt; by using Partial FC (Xiang An, Jiankang Deng, Jia Guo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-10-11&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://insightface.ai/mfr21&#34;&gt;Leaderboard&lt;/a&gt; of &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/challenges/iccv21-mfr&#34;&gt;ICCV21 - Masked Face Recognition Challenge&lt;/a&gt; released. Video: &lt;a href=&#34;https://www.youtube.com/watch?v=lL-7l5t6x2w&#34;&gt;Youtube&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV15b4y1h79N/&#34;&gt;Bilibili&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-09-22&lt;/code&gt;&lt;/strong&gt;: Update python library to ver-0.5, add new MBF and IR50 models, see &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/python-package&#34;&gt;python-package&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-08-07&lt;/code&gt;&lt;/strong&gt;: Add new &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/model_zoo&#34;&gt;model_zoo&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-07-13&lt;/code&gt;&lt;/strong&gt;: We now have implementations based on &lt;a href=&#34;https://github.com/PaddlePaddle&#34;&gt;paddlepaddle&lt;/a&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_paddle&#34;&gt;arcface_paddle&lt;/a&gt; for face recognition and &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/blazeface_paddle&#34;&gt;blazeface_paddle&lt;/a&gt; for face detection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-07-09&lt;/code&gt;&lt;/strong&gt;: We add a &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/examples/person_detection&#34;&gt;person_detection&lt;/a&gt; example, trained by &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/scrfd&#34;&gt;SCRFD&lt;/a&gt;, which can be called directly by our &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/python-package&#34;&gt;python-library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-06-05&lt;/code&gt;&lt;/strong&gt;: We launch a &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/challenges/iccv21-mfr&#34;&gt;Masked Face Recognition Challenge &amp;amp; Workshop&lt;/a&gt; on ICCV 2021.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-05-15&lt;/code&gt;&lt;/strong&gt;: We released an efficient high accuracy face detection approach called &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/scrfd&#34;&gt;SCRFD&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-04-18&lt;/code&gt;&lt;/strong&gt;: We achieved Rank-4th on NIST-FRVT 1:1, see &lt;a href=&#34;https://pages.nist.gov/frvt/html/frvt11.html&#34;&gt;leaderboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021-03-13&lt;/code&gt;&lt;/strong&gt;: We have released our official ArcFace PyTorch implementation, see &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_torch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code of InsightFace is released under the MIT License. There is no limitation for both academic and commercial usage.&lt;/p&gt; &#xA;&lt;p&gt;The training data containing the annotation (and the models trained with these data) are available for non-commercial research purposes only.&lt;/p&gt; &#xA;&lt;p&gt;Both manual-downloading models from our github repo and auto-downloading models with our &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/python-package&#34;&gt;python-library&lt;/a&gt; follow the above license policy(which is for non-commercial research purposes only).&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://insightface.ai&#34;&gt;InsightFace&lt;/a&gt; is an open source 2D&amp;amp;3D deep face analysis toolbox, mainly based on PyTorch and MXNet.&lt;/p&gt; &#xA;&lt;p&gt;Please check our &lt;a href=&#34;https://insightface.ai&#34;&gt;website&lt;/a&gt; for detail.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.6+&lt;/strong&gt; and/or &lt;strong&gt;MXNet=1.6-1.8&lt;/strong&gt;, with &lt;strong&gt;Python 3.x&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition, face detection and face alignment, which optimized for both training and deployment.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Please start with our &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/python-package/&#34;&gt;python-package&lt;/a&gt;, for testing detection, recognition and alignment models on input images.&lt;/p&gt; &#xA;&lt;h3&gt;ArcFace Video Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=y-D1tReryGA&amp;amp;t=81s&#34;&gt;&lt;img src=&#34;https://insightface.ai/assets/img/github/facerecognitionfromvideo.PNG&#34; width=&#34;760&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please click the image to watch the Youtube video. For Bilibili users, click &lt;a href=&#34;https://www.bilibili.com/video/av38041494?from=search&amp;amp;seid=11501833604850032313&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://insightface.ai/projects&#34;&gt;page&lt;/a&gt; on InsightFace website also describes all supported projects in InsightFace.&lt;/p&gt; &#xA;&lt;p&gt;You may also interested in some &lt;a href=&#34;https://insightface.ai/challenges&#34;&gt;challenges&lt;/a&gt; hold by InsightFace.&lt;/p&gt; &#xA;&lt;h2&gt;Face Recognition&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;In this module, we provide training data, network settings and loss designs for deep face recognition.&lt;/p&gt; &#xA;&lt;p&gt;The supported methods are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_mxnet&#34;&gt;ArcFace_mxnet (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_torch&#34;&gt;ArcFace_torch (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/subcenter_arcface&#34;&gt;SubCenter ArcFace (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/partial_fc&#34;&gt;PartialFC_mxnet (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_torch&#34;&gt;PartialFC_torch (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/vpl&#34;&gt;VPL (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_oneflow&#34;&gt;Arcface_oneflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/arcface_paddle&#34;&gt;ArcFace_Paddle (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Commonly used network backbones are included in most of the methods, such as IResNet, MobilefaceNet, MobileNet, InceptionResNet_v2, DenseNet, etc..&lt;/p&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;The training data includes, but not limited to the cleaned MS1M, VGG2 and CASIA-Webface datasets, which were already packed in MXNet binary format. Please &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/_datasets_&#34;&gt;dataset&lt;/a&gt; page for detail.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We provide standard IJB and Megaface evaluation pipelines in &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/_evaluation_&#34;&gt;evaluation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please check &lt;a href=&#34;https://github.com/deepinsight/insightface/wiki/Model-Zoo&#34;&gt;Model-Zoo&lt;/a&gt; for more pretrained models.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Third-party Re-implementation of ArcFace&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TensorFlow: &lt;a href=&#34;https://github.com/auroua/InsightFace_TF&#34;&gt;InsightFace_TF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow: &lt;a href=&#34;https://github.com/AIInAi/tf-insightface&#34;&gt;tf-insightface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow:&lt;a href=&#34;https://github.com/Fei-Wang/insightface&#34;&gt;insightface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;InsightFace_Pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/ronghuaiyang/arcface-pytorch&#34;&gt;arcface-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Caffe: &lt;a href=&#34;https://github.com/xialuxi/arcface-caffe&#34;&gt;arcface-caffe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Caffe: &lt;a href=&#34;https://github.com/gehaocool/CombinedMargin-caffe&#34;&gt;CombinedMargin-caffe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tensorflow: &lt;a href=&#34;https://github.com/luckycallor/InsightFace-tensorflow&#34;&gt;InsightFace-tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorRT: &lt;a href=&#34;https://github.com/wang-xinyu/tensorrtx&#34;&gt;wang-xinyu/tensorrtx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorRT: &lt;a href=&#34;https://github.com/SthPhoenix/InsightFace-REST&#34;&gt;InsightFace-REST&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNXRuntime C++: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/ort/cv/glint_arcface.cpp&#34;&gt;ArcFace-ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNXRuntime Go: &lt;a href=&#34;https://github.com/jack139/arcface-go&#34;&gt;arcface-go&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MNN: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/mnn/cv/mnn_glint_arcface.cpp&#34;&gt;ArcFace-MNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TNN: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/tnn/cv/tnn_glint_arcface.cpp&#34;&gt;ArcFace-TNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NCNN: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/ncnn/cv/ncnn_glint_arcface.cpp&#34;&gt;ArcFace-NCNN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Face Detection&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img src=&#34;https://insightface.ai/assets/img/github/11513D05.jpg&#34; width=&#34;640&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;In this module, we provide training data with annotation, network settings and loss designs for face detection training, evaluation and inference.&lt;/p&gt; &#xA;&lt;p&gt;The supported methods are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/retinaface&#34;&gt;RetinaFace (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/scrfd&#34;&gt;SCRFD (Arxiv&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/blazeface_paddle&#34;&gt;blazeface_paddle&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/retinaface&#34;&gt;RetinaFace&lt;/a&gt; is a practical single-stage face detector which is accepted by &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html&#34;&gt;CVPR 2020&lt;/a&gt;. We provide training code, training dataset, pretrained models and evaluation scripts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/detection/scrfd&#34;&gt;SCRFD&lt;/a&gt; is an efficient high accuracy face detection approach which is initialy described in &lt;a href=&#34;https://arxiv.org/abs/2105.04714&#34;&gt;Arxiv&lt;/a&gt;. We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.&lt;/p&gt; &#xA;&lt;h2&gt;Face Alignment&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img src=&#34;https://insightface.ai/assets/img/custom/thumb_sdunet.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;In this module, we provide datasets and training/inference pipelines for face alignment.&lt;/p&gt; &#xA;&lt;p&gt;Supported methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/alignment/heatmap&#34;&gt;SDUNets (BMVC&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/alignment/coordinate_reg&#34;&gt;SimpleRegression&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/alignment/heatmap&#34;&gt;SDUNets&lt;/a&gt; is a heatmap based method which accepted on &lt;a href=&#34;http://bmvc2018.org/contents/papers/0051.pdf&#34;&gt;BMVC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepinsight/insightface/master/alignment/coordinate_reg&#34;&gt;SimpleRegression&lt;/a&gt; provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;em&gt;InsightFace&lt;/em&gt; useful in your research, please consider to cite the following related papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;@article{guo2021sample,&#xA;  title={Sample and Computation Redistribution for Efficient Face Detection},&#xA;  author={Guo, Jia and Deng, Jiankang and Lattas, Alexandros and Zafeiriou, Stefanos},&#xA;  journal={arXiv preprint arXiv:2105.04714},&#xA;  year={2021}&#xA;}&#xA;&#xA;@inproceedings{an2020partical_fc,&#xA;  title={Partial FC: Training 10 Million Identities on a Single Machine},&#xA;  author={An, Xiang and Zhu, Xuhan and Xiao, Yang and Wu, Lan and Zhang, Ming and Gao, Yuan and Qin, Bin and&#xA;  Zhang, Debing and Fu Ying},&#xA;  booktitle={Arxiv 2010.05222},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{deng2020subcenter,&#xA;  title={Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces},&#xA;  author={Deng, Jiankang and Guo, Jia and Liu, Tongliang and Gong, Mingming and Zafeiriou, Stefanos},&#xA;  booktitle={Proceedings of the IEEE Conference on European Conference on Computer Vision},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{Deng2020CVPR,&#xA;title = {RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild},&#xA;author = {Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},&#xA;booktitle = {CVPR},&#xA;year = {2020}&#xA;}&#xA;&#xA;@inproceedings{guo2018stacked,&#xA;  title={Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment},&#xA;  author={Guo, Jia and Deng, Jiankang and Xue, Niannan and Zafeiriou, Stefanos},&#xA;  booktitle={BMVC},&#xA;  year={2018}&#xA;}&#xA;&#xA;@article{deng2018menpo,&#xA;  title={The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking},&#xA;  author={Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},&#xA;  journal={IJCV},&#xA;  year={2018}&#xA;}&#xA;&#xA;@inproceedings{deng2018arcface,&#xA;title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},&#xA;author={Deng, Jiankang and Guo, Jia and Niannan, Xue and Zafeiriou, Stefanos},&#xA;booktitle={CVPR},&#xA;year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Main contributors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nttstar&#34;&gt;Jia Guo&lt;/a&gt;, &lt;code&gt;guojia[at]gmail.com&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jiankangdeng&#34;&gt;Jiankang Deng&lt;/a&gt; &lt;code&gt;jiankangdeng[at]gmail.com&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anxiangsir&#34;&gt;Xiang An&lt;/a&gt; &lt;code&gt;anxiangsir[at]gmail.com&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/szad670401&#34;&gt;Jack Yu&lt;/a&gt; &lt;code&gt;jackyu961127[at]gmail.com&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://barisgecer.github.io/&#34;&gt;Baris Gecer&lt;/a&gt; &lt;code&gt;barisgecer[at]msn.com&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/hyperreel</title>
    <updated>2023-01-11T01:44:10Z</updated>
    <id>tag:github.com,2023-01-11:/facebookresearch/hyperreel</id>
    <link href="https://github.com/facebookresearch/hyperreel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HyperReel&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://hyperreel.github.io/&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2301.02238&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://hyperreel.github.io/figures/hyperreel.mp4&#34;&gt;Video&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/images/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The code of HyperReel is available under the MIT license, as it draws from the following projects, which are also licensed under the MIT license: &lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34;&gt;nerf_pl&lt;/a&gt;, &lt;a href=&#34;https://github.com/apchenstu/TensoRF&#34;&gt;TensoRF&lt;/a&gt;, and &lt;a href=&#34;https://github.com/ashawkey/torch-ngp&#34;&gt;torch-ngp&lt;/a&gt;. Licenses for all of these projects can be found in the &lt;code&gt;licenses/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#Installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#dynamic-datasets&#34;&gt;Dynamic Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#static-datasets&#34;&gt;Static Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#running-the-code-on-dynamic-scenes&#34;&gt;Running the Code on Dynamic Scenes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#running-the-code-on-static-scenes&#34;&gt;Running the Code on Static Scenes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#running-the-code-with-custom-parameters&#34;&gt;Running the Code with Custom Parameters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#real-time-viewer&#34;&gt;Real-Time Viewer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hyperreel/main/#Citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;To install all required python dependences run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that we tested the HyperReel codebase on a machine running Ubuntu 20.04, with an NVIDIA 3090 RTX GPU, CUDA version 11.8, and 128 GB of RAM.&lt;/p&gt; &#xA;&lt;h1&gt;Dynamic Datasets&lt;/h1&gt; &#xA;&lt;p&gt;By default, we assume that:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;All datasets are located in the &lt;code&gt;~/data&lt;/code&gt; folder (specified by the &lt;code&gt;experiment.params.data_dir&lt;/code&gt; argument)&lt;/li&gt; &#xA; &lt;li&gt;With the subdirectory for each individual dataset specified by the &lt;code&gt;experiment.dataset.data_subdir&lt;/code&gt; argument (e.g., see &lt;code&gt;conf/experiment/params/local.yaml&lt;/code&gt; and &lt;code&gt;conf/experiment/dataset/technicolor.yaml&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Technicolor&lt;/h2&gt; &#xA;&lt;p&gt;Please reach out to the authors of &lt;a href=&#34;https://ieeexplore.ieee.org/document/8014955&#34;&gt;Dataset and Pipeline for Multi-View Light-Field Video&lt;/a&gt; for access to the Technicolor dataset. We use the following sequences:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Birthday (frames 150-200)&lt;/li&gt; &#xA; &lt;li&gt;Fabien (frames 50-100)&lt;/li&gt; &#xA; &lt;li&gt;Painter (frames 100-150)&lt;/li&gt; &#xA; &lt;li&gt;Theater (frames 50-100)&lt;/li&gt; &#xA; &lt;li&gt;Trains (frames 150-200)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Google Immersive&lt;/h2&gt; &#xA;&lt;p&gt;Download the Google Immersive sequences from their &lt;a href=&#34;https://github.com/augmentedperception/deepview_video_dataset&#34;&gt;release page&lt;/a&gt;. As an example, in order to download the flames sequence, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://storage.googleapis.com/deepview_video_raw_data/02_Flames.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Neural 3D Video&lt;/h2&gt; &#xA;&lt;p&gt;Download the Neural 3D video sequences from their &lt;a href=&#34;https://github.com/facebookresearch/Neural_3D_Video/releases/tag/v1.0&#34;&gt;release page&lt;/a&gt;. As an example, in order to download the Flame Steak sequence, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://github.com/facebookresearch/Neural_3D_Video/releases/download/v1.0/flame_steak.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Static Datasets&lt;/h1&gt; &#xA;&lt;h2&gt;DoNeRF&lt;/h2&gt; &#xA;&lt;p&gt;The DoNeRF dataset can be found &lt;a href=&#34;https://repository.tugraz.at/records/jjs3x-4f133&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;p&gt;The LLFF dataset can be found &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Shiny&lt;/h2&gt; &#xA;&lt;p&gt;The Shiny dataset can be found &lt;a href=&#34;https://vistec-my.sharepoint.com/:f:/g/personal/pakkapon_p_s19_vistec_ac_th/EnIUhsRVJOdNsZ_4smdhye0B8z0VlxqOR35IR3bp0uGupQ?e=TsaQgM&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Spaces&lt;/h2&gt; &#xA;&lt;p&gt;The Spaces dataset can be found &lt;a href=&#34;https://github.com/augmentedperception/spaces_dataset&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stanford&lt;/h2&gt; &#xA;&lt;p&gt;The Stanford dataset can be found &lt;a href=&#34;http://lightfield.stanford.edu/lfs.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Running the Code on Dynamic Scenes&lt;/h1&gt; &#xA;&lt;p&gt;By default...&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Checkpoints are written to the &lt;code&gt;~/checkpoints&lt;/code&gt; folder (specified by the &lt;code&gt;experiment.params.ckpt_dir&lt;/code&gt; argument)&lt;/li&gt; &#xA; &lt;li&gt;Logs are written to the &lt;code&gt;~/logs&lt;/code&gt; folder (specified bt the &lt;code&gt;experiment.params.log_dir&lt;/code&gt; argument).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note that it can take a few minutes to load all of the training data into memory for dynamic scenes.&lt;/p&gt; &#xA;&lt;h2&gt;Technicolor&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on a 50 frame subset of a scene from the Technicolor dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_technicolor.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the above command will hold-out the central camera. To train a model using all available cameras, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_technicolor_no_holdout.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will also automatically create validation images and (spiral) validation videos in the log folder for the experiment. From a trained model, you can also render a video sequence with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/render_one_technicolor.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Google Immersive&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on a 50 frame subset of a scene from the Google Immersive dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_immersive.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the above command will hold-out the central camera. To train a model using all available cameras, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_immersive_no_holdout.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Neural 3D&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on a 50 frame subset of a scene from the Neural 3D Video dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_n3d.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the above command will hold-out the central camera. To train a model using all available cameras, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_n3d_no_holdout.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Running the Code on Static Scenes&lt;/h1&gt; &#xA;&lt;h2&gt;DoNeRF&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on a scene from the DoNeRF dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_donerf_sphere.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on a scene from the LLFF dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_llff.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shiny&lt;/h2&gt; &#xA;&lt;p&gt;In order to train HyperReel on the &lt;em&gt;CD&lt;/em&gt; and &lt;em&gt;Lab&lt;/em&gt; sequences from the Shiny dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/run_one_shiny_dense.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Running the Code with Custom Parameters&lt;/h1&gt; &#xA;&lt;p&gt;The general syntax for training a model is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py experiment/dataset=&amp;lt;dataset_config&amp;gt; \&#xA;    experiment/training=&amp;lt;training_config&amp;gt; \&#xA;    experiment/model=&amp;lt;model_config&amp;gt; \&#xA;    experiment.dataset.collection=&amp;lt;scene_name&amp;gt; \&#xA;    +experiment/regularizers/tensorf=tv_4000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;dataset_config&amp;gt;&lt;/code&gt; specifies the dataset config file, located in &lt;code&gt;conf/experiment/dataset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;training_config&amp;gt;&lt;/code&gt; specifies the training config file, located in &lt;code&gt;conf/experiment/training&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;model_config&amp;gt;&lt;/code&gt; specifies the model config file, located in &lt;code&gt;conf/experiment/model&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;scene_name&amp;gt;&lt;/code&gt; specifies the scene name within the dataset&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Regularizer syntax&lt;/h2&gt; &#xA;&lt;p&gt;The line &lt;code&gt;+experiment/regularizers/tensorf=tv_4000&lt;/code&gt; adds total variation and L1 regularizers on the volume tensor components, with configuration located in &lt;code&gt;conf/experiment/regularizers/tensorf/tv_4000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Real-Time Viewer&lt;/h1&gt; &#xA;&lt;p&gt;Once you have trained a HyperReel model, you can make use of the &lt;code&gt;scripts/demo_*&lt;/code&gt; scripts in order to launch the real-time viewer.&lt;/p&gt; &#xA;&lt;p&gt;For example, to run the real-time viewer on a scene from the technicolor dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/demo_technicolor.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt; &amp;lt;start_frame&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For static scenes, you can omit the start frame argument. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/demo_shiny_dense.sh &amp;lt;gpu_to_use&amp;gt; &amp;lt;scene&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here are a couple of examples of the demo running on a workstation with a 3090 RTX GPU.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/2993881/208320757-439928a1-4d35-4625-afe7-56e7c1eaa57c.mov&#34;&gt;https://user-images.githubusercontent.com/2993881/208320757-439928a1-4d35-4625-afe7-56e7c1eaa57c.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/2993881/208320798-445fec87-54f1-4335-b345-804a7b2183fe.mov&#34;&gt;https://user-images.githubusercontent.com/2993881/208320798-445fec87-54f1-4335-b345-804a7b2183fe.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{attal2023hyperreel,&#xA;  title   = {{HyperReel}: {H}igh-Fidelity {6-DoF} Video with Ray-Conditioned Sampling},&#xA;  author  = {Attal, Benjamin and Huang, Jia-Bin and Richardt, Christian and Zollhoefer, Michael and Kopf, Johannes and O&#39;Toole, Matthew and Kim, Changil},&#xA;  journal = {arXiv preprint arXiv:2301.02238},&#xA;  year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/unilm</title>
    <updated>2023-01-11T01:44:10Z</updated>
    <id>tag:github.com,2023-01-11:/microsoft/unilm</id>
    <link href="https://github.com/microsoft/unilm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;a href=&#34;https://aka.ms/nlpagi&#34;&gt;aka.ms/nlpagi&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h1&gt;Hiring&lt;/h1&gt; &#xA;&lt;p&gt;We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and AGI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to &lt;a href=&#34;mailto:fuwei@microsoft.com&#34; class=&#34;x-hidden-focus&#34;&gt;&lt;/a&gt;&lt;a href=&#34;mailto:fuwei@microsoft.com&#34;&gt;fuwei@microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;AI Fundamentals&lt;/h2&gt; &#xA;&lt;h3&gt;TorchScale - Transformers at (any) Scale (&lt;a href=&#34;https://github.com/microsoft/torchscale&#34;&gt;repo&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Fundamental research to improve modeling generality and capability, as well as training stability and efficiency for Transformers at any scale.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Stability - &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/deepnet&#34;&gt;&lt;strong&gt;DeepNet&lt;/strong&gt;&lt;/a&gt;: scaling Transformers to 1,000 Layers and beyond&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Generality - &lt;a href=&#34;https://arxiv.org/abs/2210.06423&#34;&gt;&lt;strong&gt;Foundation Transformers (Magneto)&lt;/strong&gt;&lt;/a&gt;: towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Capability - A &lt;a href=&#34;https://arxiv.org/abs/2212.10554&#34;&gt;&lt;strong&gt;Length-Extrapolatable&lt;/strong&gt;&lt;/a&gt; Transformer&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Efficiency &amp;amp; Transferability - &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xmoe&#34;&gt;&lt;strong&gt;X-MoE&lt;/strong&gt;&lt;/a&gt;: scalable &amp;amp; finetunable sparse Mixture-of-Experts (MoE)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Foundation (aka Pre-trained) Models&lt;/h2&gt; &#xA;&lt;h3&gt;General-purpose Foundation Model&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/metalm&#34;&gt;&lt;strong&gt;MetaLM&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;Language Models are General-Purpose Interfaces&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Big Convergence&lt;/strong&gt; - Large-scale self-supervised pre-training across &lt;code&gt;tasks&lt;/code&gt; (predictive and generative), &lt;code&gt;languages&lt;/code&gt; (100+ languages), and &lt;code&gt;modalities&lt;/code&gt; (language, image, audio, layout/format + language, vision + language, audio + language, etc.)&lt;/p&gt; &#xA;&lt;!--## Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities--&gt; &#xA;&lt;h3&gt;Language &amp;amp; Multilingual&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm&#34;&gt;&lt;strong&gt;UniLM&lt;/strong&gt;&lt;/a&gt;: unified pre-training for language understanding and generation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/infoxlm&#34;&gt;&lt;strong&gt;InfoXLM/XLM-E&lt;/strong&gt;&lt;/a&gt;: multilingual/cross-lingual pre-trained models for 100+ languages&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/deltalm&#34;&gt;&lt;strong&gt;DeltaLM/mT6&lt;/strong&gt;&lt;/a&gt;: encoder-decoder pre-training for language generation and translation for 100+ languages&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;&lt;strong&gt;MiniLM&lt;/strong&gt;&lt;/a&gt;: small and fast pre-trained models for language understanding and generation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/adalm&#34;&gt;&lt;strong&gt;AdaLM&lt;/strong&gt;&lt;/a&gt;: domain, language, and task adaptation of pre-trained models&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/edgelm&#34;&gt;&lt;strong&gt;EdgeLM&lt;/strong&gt;&lt;/a&gt;(&lt;code&gt;NEW&lt;/code&gt;): small pre-trained models on edge/client devices&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/simlm&#34;&gt;&lt;strong&gt;SimLM&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): large-scale pre-training for similarity matching&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/e5&#34;&gt;&lt;strong&gt;E5&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): text embeddings&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Vision&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt;/&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit2&#34;&gt;&lt;strong&gt;BEiT-2&lt;/strong&gt;&lt;/a&gt;: generative self-supervised pre-training for vision / BERT Pre-Training of Image Transformers&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/dit&#34;&gt;&lt;strong&gt;DiT&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): self-supervised pre-training for Document Image Transformers&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Speech&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;&lt;strong&gt;WavLM&lt;/strong&gt;&lt;/a&gt;: speech pre-training for full stack tasks&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/valle&#34;&gt;&lt;strong&gt;VALL-E&lt;/strong&gt;&lt;/a&gt;: a neural codec language model for TTS&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Multimodal (X + Language)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlm&#34;&gt;&lt;strong&gt;LayoutLM&lt;/strong&gt;&lt;/a&gt;/&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv2&#34;&gt;&lt;strong&gt;LayoutLMv2&lt;/strong&gt;&lt;/a&gt;/&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3&#34;&gt;&lt;strong&gt;LayoutLMv3&lt;/strong&gt;&lt;/a&gt;: multimodal (text + layout/format + image) &lt;strong&gt;Document Foundation Model&lt;/strong&gt; for &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/document-ai/&#34;&gt;Document AI&lt;/a&gt; (e.g. scanned documents, PDF, etc.)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;&lt;strong&gt;LayoutXLM&lt;/strong&gt;&lt;/a&gt;: multimodal (text + layout/format + image) &lt;strong&gt;Document Foundation Model&lt;/strong&gt; for multilingual Document AI&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/markuplm&#34;&gt;&lt;strong&gt;MarkupLM&lt;/strong&gt;&lt;/a&gt;: markup language model pre-training for visually-rich document understanding&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xdoc&#34;&gt;&lt;strong&gt;XDoc&lt;/strong&gt;&lt;/a&gt;: unified pre-training for cross-format document understanding&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.07597&#34;&gt;&lt;strong&gt;UniSpeech&lt;/strong&gt;&lt;/a&gt;: unified pre-training for self-supervised learning and supervised learning for ASR&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2110.05752.pdf&#34;&gt;&lt;strong&gt;UniSpeech-SAT&lt;/strong&gt;&lt;/a&gt;: universal speech representation learning with speaker-aware pre-training&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07205&#34;&gt;&lt;strong&gt;SpeechT5&lt;/strong&gt;&lt;/a&gt;: encoder-decoder pre-training for spoken language processing&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.15329&#34;&gt;&lt;strong&gt;SpeechLM&lt;/strong&gt;&lt;/a&gt;: Enhanced Speech Pre-Training with Unpaired Textual Data&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/vlmo&#34;&gt;&lt;strong&gt;VLMo&lt;/strong&gt;&lt;/a&gt;: Unified vision-language pre-training&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/vl-beit&#34;&gt;&lt;strong&gt;VL-BEiT&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): Generative Vision-Language Pre-training - evolution of &lt;strong&gt;BEiT&lt;/strong&gt; to multimodal&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT-3&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): a general-purpose multimodal foundation model, and a major milestone of &lt;strong&gt;The Big Convergence&lt;/strong&gt; of Large-scale Pre-training Across Tasks, Languages, and Modalities.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Toolkits&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/s2s-ft&#34;&gt;&lt;strong&gt;s2s-ft&lt;/strong&gt;&lt;/a&gt;: sequence-to-sequence fine-tuning toolkit&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.10350.pdf&#34;&gt;&lt;strong&gt;Aggressive Decoding&lt;/strong&gt;&lt;/a&gt; (&lt;code&gt;NEW&lt;/code&gt;): lossless and efficient sequence-to-sequence decoding algorithm&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt;: transformer-based OCR w/ pre-trained models&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutreader&#34;&gt;&lt;strong&gt;LayoutReader&lt;/strong&gt;&lt;/a&gt;: pre-training of text and layout for reading order detection&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xlmt&#34;&gt;&lt;strong&gt;XLM-T&lt;/strong&gt;&lt;/a&gt;: multilingual NMT w/ pretrained cross-lingual encoders&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;January, 2023: &lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;&lt;strong&gt;VALL-E&lt;/strong&gt;&lt;/a&gt; a language modeling approach for text to speech synthesis (TTS), which achieves state-of-the-art zero-shot TTS performance. See &lt;a href=&#34;https://aka.ms/valle&#34;&gt;https://aka.ms/valle&lt;/a&gt; for demos of our work&lt;/li&gt; &#xA; &lt;li&gt;November, 2022: &lt;a href=&#34;https://github.com/microsoft/torchscale&#34;&gt;&lt;strong&gt;TorchScale 0.1.1&lt;/strong&gt;&lt;/a&gt; was released!&lt;/li&gt; &#xA; &lt;li&gt;November, 2022: &lt;a href=&#34;https://arxiv.org/abs/2109.10282&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; was accepted by AAAI 2023.&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] November, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xdoc&#34;&gt;&lt;strong&gt;XDoc&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;BASE&lt;/strong&gt; models for cross-format document understanding.&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] September, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;BASE&lt;/strong&gt; and &lt;strong&gt;LARGE&lt;/strong&gt; models for Scene Text Recognition (STR).&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] September, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit2&#34;&gt;&lt;strong&gt;BEiT v2&lt;/strong&gt;&lt;/a&gt; code and pretrained models&lt;/li&gt; &#xA; &lt;li&gt;August, 2022: &lt;a href=&#34;https://arxiv.org/abs/2208.10442&#34;&gt;&lt;strong&gt;BEiT-3&lt;/strong&gt;&lt;/a&gt;- a general-purpose multimodal foundation model, which achieves state-of-the-art transfer performance on both vision and vision-language tasks&lt;/li&gt; &#xA; &lt;li&gt;July, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/simlm&#34;&gt;&lt;strong&gt;SimLM&lt;/strong&gt;&lt;/a&gt; - Large-scale self-supervised pre-training for similarity matching&lt;/li&gt; &#xA; &lt;li&gt;June, 2022: &lt;a href=&#34;https://arxiv.org/abs/2203.02378&#34;&gt;&lt;strong&gt;DiT&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2204.08387&#34;&gt;&lt;strong&gt;LayoutLMv3&lt;/strong&gt;&lt;/a&gt; were accepted by ACM Multimedia 2022.&lt;/li&gt; &#xA; &lt;li&gt;June, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/metalm&#34;&gt;&lt;strong&gt;MetaLM&lt;/strong&gt;&lt;/a&gt; - Language models are general-purpose interfaces to foudation models (language/multilingual, vision, speech, and multimodal)&lt;/li&gt; &#xA; &lt;li&gt;June, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/vl-beit&#34;&gt;&lt;strong&gt;VL-BEiT&lt;/strong&gt;&lt;/a&gt; - bidirectional multimodal Transformer learned from scratch with one unified pretraining task, one shared backbone, and one-stage training, supporting both vision and vision-language tasks.&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] June, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3#form-understanding-on-xfund&#34;&gt;&lt;strong&gt;LayoutLMv3 Chinese&lt;/strong&gt;&lt;/a&gt; - Chinese version of LayoutLMv3&lt;/li&gt; &#xA; &lt;li&gt;[Code Release] May, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/decoding&#34;&gt;&lt;strong&gt;Aggressive Decoding&lt;/strong&gt;&lt;/a&gt; - Lossless Speedup for Seq2seq Generation&lt;/li&gt; &#xA; &lt;li&gt;April, 2022: &lt;strong&gt;Transformers at Scale&lt;/strong&gt; = &lt;a href=&#34;https://arxiv.org/abs/2203.00555&#34;&gt;DeepNet&lt;/a&gt; + &lt;a href=&#34;https://arxiv.org/abs/2204.09179&#34;&gt;X-MoE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] April, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3&#34;&gt;&lt;strong&gt;LayoutLMv3&lt;/strong&gt;&lt;/a&gt; - Pre-training for Document AI with Unified Text and Image Masking&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] March, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/edgelm&#34;&gt;&lt;strong&gt;EdgeFormer&lt;/strong&gt;&lt;/a&gt; - Parameter-efficient Transformer for On-device Seq2seq Generation&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] March, 2022: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/dit&#34;&gt;&lt;strong&gt;DiT&lt;/strong&gt;&lt;/a&gt; - Self-supervised Document Image Transformer. Demos: &lt;a href=&#34;https://huggingface.co/spaces/nielsr/dit-document-layout-analysis&#34;&gt;Document Layout Analysis&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/microsoft/document-image-transformer&#34;&gt;Document Image Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;January, 2022: &lt;a href=&#34;https://openreview.net/forum?id=p-BhZSz59o4&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt; was accepted by &lt;strong&gt;ICLR 2022 as Oral presentation&lt;/strong&gt; (54 out of 3391).&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] December 16th, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;small&lt;/strong&gt; models for handwritten and printed texts, with 3x inference speedup.&lt;/li&gt; &#xA; &lt;li&gt;November 24th, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/vlmo&#34;&gt;&lt;strong&gt;VLMo&lt;/strong&gt;&lt;/a&gt; as the new SOTA on the &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278&#34;&gt;VQA Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;November, 2021: &lt;a href=&#34;https://www.microsoft.com/en-us/translator/blog/2021/11/22/multilingual-translation-at-scale-10000-language-pairs-and-beyond/&#34;&gt;Multilingual translation at scale: 10000 language pairs and beyond&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] November, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/markuplm&#34;&gt;&lt;strong&gt;MarkupLM&lt;/strong&gt;&lt;/a&gt; - Pre-training for text and markup language (e.g. HTML/XML)&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] November, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/vlmo&#34;&gt;&lt;strong&gt;VLMo&lt;/strong&gt;&lt;/a&gt; - Unified vision-language pre-training w/ &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;October, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;&lt;strong&gt;WavLM&lt;/strong&gt;&lt;/a&gt; Large achieves state-of-the-art performance on the &lt;a href=&#34;https://superbbenchmark.org/leaderboard&#34;&gt;SUPERB&lt;/a&gt; benchmark&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] October, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;&lt;strong&gt;WavLM&lt;/strong&gt;&lt;/a&gt; - Large-scale self-supervised pre-trained models for speech.&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] October 2021: &lt;a href=&#34;https://huggingface.co/transformers/master/model_doc/trocr.html&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; is on &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;September 28th, 2021: T-ULRv5 (aka &lt;a href=&#34;https://arxiv.org/abs/2106.16138&#34; target=&#34;_blank&#34;&gt;XLM-E&lt;/a&gt;/&lt;a href=&#34;https://arxiv.org/abs/2007.07834&#34; target=&#34;_blank&#34;&gt;InfoXLM&lt;/a&gt;) as the SOTA on the &lt;a href=&#34;https://sites.research.google/xtreme&#34; target=&#34;_blank&#34;&gt;XTREME&lt;/a&gt; leaderboard. // &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/&#34; target=&#34;_blank&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] September, 2021: &lt;a href=&#34;https://huggingface.co/microsoft/layoutlm-base-cased&#34;&gt;&lt;strong&gt;LayoutLM-cased&lt;/strong&gt;&lt;/a&gt; are on &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] September, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; - Transformer-based OCR w/ pre-trained &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt; and RoBERTa models.&lt;/li&gt; &#xA; &lt;li&gt;August 2021: &lt;a href=&#34;https://huggingface.co/transformers/master/model_doc/layoutlmv2.html&#34;&gt;&lt;strong&gt;LayoutLMv2&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/transformers/master/model_doc/layoutxlm.html&#34;&gt;&lt;strong&gt;LayoutXLM&lt;/strong&gt;&lt;/a&gt; are on &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] August, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutreader&#34;&gt;&lt;strong&gt;LayoutReader&lt;/strong&gt;&lt;/a&gt; - Built with LayoutLM to improve general reading order detection.&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] August, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/deltalm&#34;&gt;&lt;strong&gt;DeltaLM&lt;/strong&gt;&lt;/a&gt; - Encoder-decoder pre-training for language generation and translation.&lt;/li&gt; &#xA; &lt;li&gt;August 2021: &lt;a href=&#34;https://huggingface.co/transformers/master/model_doc/beit.html&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt; is on &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] July, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt; - Towards BERT moment for CV&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] June, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv2&#34;&gt;&lt;strong&gt;LayoutLMv2&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;&lt;strong&gt;LayoutXLM&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;&lt;strong&gt;MiniLMv2&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/adalm&#34;&gt;&lt;strong&gt;AdaLM&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;May, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv2&#34;&gt;LayoutLMv2&lt;/a&gt;, InfoXLMv2, MiniLMv2, UniLMv3, and AdaLM were accepted by ACL 2021.&lt;/li&gt; &#xA; &lt;li&gt;April, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;LayoutXLM&lt;/a&gt; is coming by extending the LayoutLM into multilingual support! A multilingual form understanding benchmark &lt;a href=&#34;https://github.com/doc-analysis/XFUND&#34;&gt;XFUND&lt;/a&gt; is also introduced, which includes forms with human labeled key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese).&lt;/li&gt; &#xA; &lt;li&gt;March, 2021: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/infoxlm&#34;&gt;InfoXLM&lt;/a&gt; was accepted by NAACL 2021.&lt;/li&gt; &#xA; &lt;li&gt;December 29th, 2020: &lt;a href=&#34;https://arxiv.org/abs/2012.14740&#34;&gt;LayoutLMv2&lt;/a&gt; is coming with the new SOTA on a wide varierty of document AI tasks, including &lt;a href=&#34;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&#34;&gt;DocVQA&lt;/a&gt; and &lt;a href=&#34;https://rrc.cvc.uab.es/?ch=13&amp;amp;com=evaluation&amp;amp;task=3&#34;&gt;SROIE&lt;/a&gt; leaderboard.&lt;/li&gt; &#xA; &lt;li&gt;October 8th, 2020: T-ULRv2 (aka &lt;a href=&#34;https://arxiv.org/abs/2007.07834&#34;&gt;InfoXLM&lt;/a&gt;) as the SOTA on the &lt;a href=&#34;https://sites.research.google/xtreme&#34;&gt;XTREME&lt;/a&gt; leaderboard. // &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv2-tops-xtreme-leaderboard/&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;September, 2020: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;MiniLM&lt;/a&gt; was accepted by NeurIPS 2020.&lt;/li&gt; &#xA; &lt;li&gt;July 16, 2020: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/infoxlm&#34;&gt;&lt;strong&gt;InfoXLM&lt;/strong&gt; (Multilingual UniLM)&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2007.07834.pdf&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;June, 2020: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm&#34;&gt;UniLMv2&lt;/a&gt; was accepted by ICML 2020; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlm&#34;&gt;LayoutLM&lt;/a&gt; was accepted by KDD 2020.&lt;/li&gt; &#xA; &lt;li&gt;April 5, 2020: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;&lt;strong&gt;Multilingual MiniLM&lt;/strong&gt;&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;September, 2019: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm-v1&#34;&gt;UniLMv1&lt;/a&gt; was accepted by NeurIPS 2019.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;New Octorber, 2022&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xdoc&#34;&gt;XDoc&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/xdoc&#34;&gt;&lt;strong&gt;XDoc&lt;/strong&gt;&lt;/a&gt; (October 7, 2022): XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. &#34;&lt;a href=&#34;https://arxiv.org/abs/2210.02849&#34;&gt;XDoc: Unified Pre-training for Cross-Format Document Understanding&lt;/a&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;New May, 2022&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/decoding&#34;&gt;Aggressive Decoding&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/decoding&#34;&gt;&lt;strong&gt;Aggressive Decoding&lt;/strong&gt;&lt;/a&gt; (May 20, 2022): Aggressive Decoding, a novel decoding paradigm for lossless speedup for seq2seq generation. Unlike the previous efforts (e.g., non-autoregressive decoding) speeding up seq2seq generation at the cost of quality loss, Aggressive Decoding aims to yield the identical (or better) generation compared with autoregressive decoding but in a significant speedup: For the seq2seq tasks characterized by highly similar inputs and outputs (e.g., Grammatical Error Correction and Text Simplification), the Input-guided Aggressive Decoding can introduce a 7x-9x speedup for the popular 6-layer Transformer on GPU with the identical results as greedy decoding; For other general seq2seq tasks (e.g., Machine Translation and Abstractive Summarization), the Generalized Aggressive Decoding can have a 3x-5x speedup with the identical or even better quality. &#34;&lt;a href=&#34;https://arxiv.org/pdf/2205.10350.pdf&#34;&gt;Lossless Acceleration for Seq2seq Generation with Aggressive Decoding&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;New April, 2022&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3&#34;&gt;LayoutLMv3&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3&#34;&gt;&lt;strong&gt;LayoutLM 3.0&lt;/strong&gt;&lt;/a&gt; (April 19, 2022): LayoutLMv3, a multimodal pre-trained Transformer for Document AI with unified text and image masking. Additionally, it is also pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. &#34;&lt;a href=&#34;https://arxiv.org/abs/2204.08387&#34;&gt;LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking&lt;/a&gt; &lt;code&gt;ACM MM 2022&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;March, 2022&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/edgelm&#34;&gt;EdgeFormer&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/edgelm&#34;&gt;&lt;strong&gt;EdgeFormer&lt;/strong&gt;&lt;/a&gt; (March 18, 2022): EdgeFormer, the first publicly available pretrained parameter-efficient Transformer for on-device seq2seq generation. EdgeFormer has only 11 million parameters, taking up less than 15MB disk size after int8 quantization and compression, which can process a sentence of the length of 20-30 tokens with acceptable latency on two middle-to-high end CPU cores and less than 50MB memory footprint. The pretrained EdgeFormer can be fine-tuned to English seq2seq tasks and achieve promising results -- significantly better than the strong paramter-efficient Transformer baseline (pretrained Universal Transformer) and full-parameterized Transformer-base model without pretraining, which we believe can largely facilitate on-device seq2seq generation in practice. &#34;&lt;a href=&#34;https://arxiv.org/abs/2202.07959&#34;&gt;EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;March, 2022&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/dit&#34;&gt;DiT&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/dit&#34;&gt;&lt;strong&gt;DiT&lt;/strong&gt;&lt;/a&gt; (March 4, 2022): DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 → 92.69), document layout analysis (91.0 → 94.9), table detection (94.23 → 96.55) and text detection for OCR (93.07 → 94.29). &#34;&lt;a href=&#34;https://arxiv.org/abs/2203.02378&#34;&gt;DiT: Self-supervised Pre-training for Document Image Transformer&lt;/a&gt; &lt;code&gt;ACM MM 2022&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;October, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;WavLM&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;&lt;strong&gt;WavLM&lt;/strong&gt;&lt;/a&gt; (October 27, 2021): WavLM, a new pre-trained speech model, to solve full-stack downstream speech tasks. WavLM integrates the gated relative position embedding structure and the utterance mixing method, to model both spoken content and speaker identity preservation. WavLM is trained on 94k hours of public audio data, which is larger than other released checkpoints for English Speech modeling. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. &#34;&lt;a href=&#34;https://arxiv.org/pdf/2110.13900.pdf&#34;&gt;WavLM: Large-Scale Self-Supervised Pre-training for Full Stack Speech Processing&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;October, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/markuplm&#34;&gt;MarkupLM&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/markuplm&#34;&gt;&lt;strong&gt;MarkupLM&lt;/strong&gt;&lt;/a&gt; (October 19, 2021): MarkupLM, a simple yet effective pre-training approach for text and markup language. With the Transformer architecture, MarkupLM integrates different input embeddings including text embeddings, position embeddings, and XPath embeddings. Furthermore, we also propose new pre-training objectives that are specially designed for understanding the markup language. We evaluate the pre-trained MarkupLM model on the WebSRC and SWDE datasets. Experiments show that MarkupLM significantly outperforms several SOTA baselines in these tasks. &#34;&lt;a href=&#34;https://arxiv.org/abs/2110.08518&#34;&gt;MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding&lt;/a&gt; &lt;code&gt;ACL 2022&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;September, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;TrOCR&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/trocr&#34;&gt;&lt;strong&gt;TrOCR&lt;/strong&gt;&lt;/a&gt; (September 22, 2021): Transformer-based OCR with pre-trained models, which leverages the Transformer architecture for both image understanding and bpe-level text generation. The TrOCR model is simple but effective (convolution free), and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. &#34;&lt;a href=&#34;https://arxiv.org/abs/2109.10282&#34;&gt;TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models&lt;/a&gt; &lt;code&gt;AAAI 2023&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;August, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutreader&#34;&gt;LayoutReader&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutreader&#34;&gt;&lt;strong&gt;LayoutReader&lt;/strong&gt;&lt;/a&gt; (August 26, 2021): pre-training of text and layout for reading order detection. The pre-trained LayoutReader significantly improves both open-source and commercial OCR engines in ordering text lines. Meanwhile, we also created a reading order benchmark dataset &lt;a href=&#34;https://github.com/doc-analysis/ReadingBank&#34;&gt;ReadingBank&lt;/a&gt; to further empower the research in this area. &#34;&lt;a href=&#34;https://arxiv.org/abs/2108.11591&#34;&gt;LayoutReader: Pre-training of Text and Layout for Reading Order Detection&lt;/a&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;August, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/deltalm&#34;&gt;DeltaLM&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/deltalm&#34;&gt;&lt;strong&gt;DeltaLM&lt;/strong&gt;&lt;/a&gt; (August, 2021): encoder-decoder pre-training for language generation and translation. DeltaLM &lt;strong&gt;ranks first&lt;/strong&gt; on the &lt;a href=&#34;http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html&#34;&gt;WMT21 multilingual translation task&lt;/a&gt;. The task requires a model to translate between 102 languages. &#34;&lt;a href=&#34;https://arxiv.org/abs/2106.13736&#34;&gt;DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders.&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;July, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;BEiT&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/beit&#34;&gt;&lt;strong&gt;BEiT&lt;/strong&gt;&lt;/a&gt; (June 15, 2021): BERT Pre-Training of Image Transformers. BEiT-large achieves &lt;strong&gt;&lt;a href=&#34;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k&#34;&gt;state-of-the-art results on ADE20K&lt;/a&gt; (a big jump to 57.0 mIoU) for semantic segmentation&lt;/strong&gt;. BEiT-large achieves &lt;strong&gt;state-of-the-art ImageNet top-1 accuracy (88.6%) under the setting without extra data other than ImageNet-22k&lt;/strong&gt;. &#34;&lt;a href=&#34;https://arxiv.org/abs/2106.08254&#34;&gt;BEiT: BERT Pre-Training of Image Transformers&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;June, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;LayoutXLM&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/adalm&#34;&gt;AdaLM&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;MiniLMv2&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;&lt;strong&gt;LayoutXLM&lt;/strong&gt;&lt;/a&gt; (April 17, 2021): multimodal pre-training for multilingual visually-rich document understanding. The pre-trained LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the FUNSD and multilingual &lt;a href=&#34;https://github.com/doc-analysis/XFUND&#34;&gt;XFUND&lt;/a&gt; dataset including 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). &#34;&lt;a href=&#34;https://arxiv.org/abs/2104.08836&#34;&gt;LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/adalm&#34;&gt;&lt;strong&gt;AdaLM&lt;/strong&gt;&lt;/a&gt; (June 2021): a simple yet effective approach for domain adaptation of pre-trained models. Biomedical specific pre-trained models are released. &#34;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/unilm/master/#&#34;&gt;Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains&lt;/a&gt; &lt;code&gt;ACL 2021&lt;/code&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;&lt;strong&gt;MiniLMv2&lt;/strong&gt;&lt;/a&gt; (December, 2020): a simple yet effective task-agnostic knoweldge distillation method, namely multi-head self-attention relation distillation, for compressing large pre-trained Transformers into small and fast pre-trained models. MiniLMv2 significantly outperforms MiniLMv1. Both English and multilingual MiniLM models are released. &#34;&lt;a href=&#34;https://arxiv.org/abs/2012.15828&#34;&gt;MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers&lt;/a&gt; &lt;code&gt;ACL 2021&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;May, 2021&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv2&#34;&gt;LayoutLMv2&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutxlm&#34;&gt;LayoutXLM&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv2&#34;&gt;&lt;strong&gt;LayoutLM 2.0&lt;/strong&gt;&lt;/a&gt; (December 29, 2020): multimodal pre-training for visually-rich document understanding by leveraging text, layout and image information in a single framework. It is coming with new SOTA on a wide range of document understanding tasks, including FUNSD (0.7895 -&amp;gt; 0.8420), CORD (0.9493 -&amp;gt; 0.9601), SROIE (0.9524 -&amp;gt; 0.9781), Kleister-NDA (0.834 -&amp;gt; 0.852), RVL-CDIP (0.9443 -&amp;gt; 0.9564), and DocVQA (0.7295 -&amp;gt; 0.8672). &#34;&lt;a href=&#34;https://arxiv.org/abs/2012.14740&#34;&gt;LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding&lt;/a&gt; &lt;code&gt;ACL 2021&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** &lt;code&gt;February, 2020&lt;/code&gt;: &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm&#34;&gt;UniLM v2&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;MiniLM v1&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlm&#34;&gt;LayoutLM v1&lt;/a&gt; | &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/s2s-ft&#34;&gt;s2s-ft v1&lt;/a&gt; release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlm&#34;&gt;&lt;strong&gt;LayoutLM 1.0&lt;/strong&gt;&lt;/a&gt; (February 18, 2020): pre-trained models for document (image) understanding (e.g. receipts, forms, etc.) . It achieves new SOTA results in several downstream tasks, including form understanding (the FUNSD dataset from 70.72 to 79.27), receipt understanding (the &lt;a href=&#34;https://rrc.cvc.uab.es/?ch=13&amp;amp;com=evaluation&amp;amp;task=3&#34;&gt;ICDAR 2019 SROIE leaderboard&lt;/a&gt; from 94.02 to 95.24) and document image classification (the RVL-CDIP dataset from 93.07 to 94.42). &#34;&lt;a href=&#34;https://arxiv.org/abs/1912.13318&#34;&gt;LayoutLM: Pre-training of Text and Layout for Document Image Understanding&lt;/a&gt; &lt;code&gt;KDD 2020&lt;/code&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/s2s-ft&#34;&gt;&lt;strong&gt;s2s-ft 1.0&lt;/strong&gt;&lt;/a&gt; (February 26, 2020): A PyTorch package used to fine-tune pre-trained Transformers for sequence-to-sequence language generation. &#34;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/unilm/master/#&#34;&gt;s2s-ft: Fine-Tuning Pre-Trained Transformers for Sequence-to-Sequence Learning&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/minilm&#34;&gt;&lt;strong&gt;MiniLM 1.0&lt;/strong&gt;&lt;/a&gt; (February 26, 2020): deep self-attention distillation is all you need (for task-agnostic knowledge distillation of pre-trained Transformers). MiniLM (12-layer, 384-hidden) achieves 2.7x speedup and comparable results over BERT-base (12-layer, 768-hidden) on NLU tasks as well as strong results on NLG tasks. The even smaller MiniLM (6-layer, 384-hidden) obtains 5.3x speedup and produces very competitive results. &#34;&lt;a href=&#34;https://arxiv.org/abs/2002.10957&#34;&gt;MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers&lt;/a&gt; &lt;code&gt;NeurIPS 2020&lt;/code&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm&#34;&gt;&lt;strong&gt;UniLM 2.0&lt;/strong&gt;&lt;/a&gt; (February 28, 2020): &lt;strong&gt;unified pre-training&lt;/strong&gt; of bi-directional LM (via autoencoding) and sequence-to-sequence LM (via partially autoregressive) w/ &lt;strong&gt;Pseudo-Masked Language Model&lt;/strong&gt; for language understanding and generation. UniLM v2 achieves new SOTA in a wide range of natural language understanding and generation tasks. &#34;&lt;a href=&#34;https://arxiv.org/abs/2002.12804&#34;&gt;UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training&lt;/a&gt; &lt;code&gt;ICML 2020&lt;/code&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;***** October 1st, 2019: UniLM v1 release *****&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/unilm-v1&#34;&gt;&lt;strong&gt;UniLM v1&lt;/strong&gt;&lt;/a&gt; (September 30, 2019): the code and pre-trained models for the &lt;code&gt;NeurIPS 2019&lt;/code&gt; paper entitled &#34;&lt;a href=&#34;https://arxiv.org/abs/1905.03197&#34;&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/a&gt;&#34;. UniLM (v1) achieves the &lt;strong&gt;new SOTA results&lt;/strong&gt; in &lt;strong&gt;NLG&lt;/strong&gt; (especially &lt;strong&gt;sequence-to-sequence generation&lt;/strong&gt;) tasks, including abstractive summarization (the Gigaword and CNN/DM datasets), question generation (the SQuAD QG dataset), etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the license found in the LICENSE file in the root directory of this source tree. Portions of the source code are based on the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.microsoft.com/codeofconduct&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Contact Information&lt;/h3&gt; &#xA;&lt;p&gt;For help or issues using the pre-trained models, please submit a GitHub issue.&lt;/p&gt; &#xA;&lt;p&gt;For other communications, please contact &lt;a href=&#34;http://gitnlp.org/&#34;&gt;Furu Wei&lt;/a&gt; (&lt;code&gt;fuwei@microsoft.com&lt;/code&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>