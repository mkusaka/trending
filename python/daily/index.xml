<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-02T01:34:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lucidrains/x-transformers</title>
    <updated>2024-08-02T01:34:58Z</updated>
    <id>tag:github.com,2024-08-02:/lucidrains/x-transformers</id>
    <link href="https://github.com/lucidrains/x-transformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple but complete full-attention transformer with a set of promising experimental features from various papers&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;x-transformers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/x-transformers&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/x-transformers.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A concise but fully-featured transformer, complete with a set of promising e&lt;strong&gt;x&lt;/strong&gt;perimental features from various papers.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install x-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Full encoder / decoder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import XTransformer&#xA;&#xA;model = XTransformer(&#xA;    dim = 512,&#xA;    enc_num_tokens = 256,&#xA;    enc_depth = 6,&#xA;    enc_heads = 8,&#xA;    enc_max_seq_len = 1024,&#xA;    dec_num_tokens = 256,&#xA;    dec_depth = 6,&#xA;    dec_heads = 8,&#xA;    dec_max_seq_len = 1024,&#xA;    tie_token_emb = True      # tie embeddings of encoder and decoder&#xA;)&#xA;&#xA;src = torch.randint(0, 256, (1, 1024))&#xA;src_mask = torch.ones_like(src).bool()&#xA;tgt = torch.randint(0, 256, (1, 1024))&#xA;&#xA;loss = model(src, tgt, mask = src_mask) # (1, 1024, 512)&#xA;loss.backward()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Decoder-only (GPT-like)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;).cuda()&#xA;&#xA;x = torch.randint(0, 256, (1, 1024)).cuda()&#xA;&#xA;model(x) # (1, 1024, 20000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GPT3 would be approximately the following (but you wouldn&#39;t be able to run it anyways)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;gpt3 = TransformerWrapper(&#xA;    num_tokens = 50000,&#xA;    max_seq_len = 2048,&#xA;    attn_layers = Decoder(&#xA;        dim = 12288,&#xA;        depth = 96,&#xA;        heads = 96,&#xA;        attn_dim_head = 128&#xA;    )&#xA;).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Encoder-only (BERT-like)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;).cuda()&#xA;&#xA;x = torch.randint(0, 256, (1, 1024)).cuda()&#xA;mask = torch.ones_like(x).bool()&#xA;&#xA;model(x, mask = mask) # (1, 1024, 20000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;State of the art image classification (&lt;a href=&#34;https://arxiv.org/abs/2205.01580&#34;&gt;SimpleViT&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import ViTransformerWrapper, Encoder&#xA;&#xA;model = ViTransformerWrapper(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    num_classes = 1000,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;    )&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;model(img) # (1, 1000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image -&amp;gt; caption&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import ViTransformerWrapper, TransformerWrapper, Encoder, Decoder&#xA;&#xA;encoder = ViTransformerWrapper(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;decoder = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        cross_attend = True&#xA;    )&#xA;)&#xA;&#xA;img = torch.randn(1, 3, 256, 256)&#xA;caption = torch.randint(0, 20000, (1, 1024))&#xA;&#xA;encoded = encoder(img, return_embeddings = True)&#xA;decoder(caption, context = encoded) # (1, 1024, 20000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.06794&#34;&gt;PaLI&lt;/a&gt;, state of the art language-vision model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import ViTransformerWrapper, XTransformer, Encoder&#xA;&#xA;# PaLI composes of&#xA;# 1. vision transformer (ViTransformerWrapper) +&#xA;# 2. encoder-decoder transformer (XTransformer)&#xA;&#xA;vit = ViTransformerWrapper(&#xA;    image_size = 256,&#xA;    patch_size = 32,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;pali = XTransformer(&#xA;    dim = 512,&#xA;    enc_num_tokens = 256,&#xA;    enc_depth = 6,&#xA;    enc_heads = 8,&#xA;    enc_max_seq_len = 1024,&#xA;    dec_num_tokens = 256,&#xA;    dec_depth = 6,&#xA;    dec_heads = 8,&#xA;    dec_max_seq_len = 1024&#xA;)&#xA;&#xA;# training data&#xA;&#xA;img = torch.randn(1, 3, 256, 256)               # images&#xA;prompt = torch.randint(0, 256, (1, 1024))       # prompt&#xA;prompt_mask = torch.ones(1, 1024).bool()        # prompt text mask&#xA;output_text = torch.randint(0, 256, (1, 1024))  # target output text&#xA;&#xA;# train&#xA;&#xA;img_embeds = vit(&#xA;    img,&#xA;    return_embeddings = True&#xA;)&#xA;&#xA;loss = pali(&#xA;    prompt,&#xA;    output_text,&#xA;    mask = prompt_mask,&#xA;    src_prepend_embeds = img_embeds             # will preprend image embeddings to encoder text embeddings before attention&#xA;)&#xA;&#xA;loss.backward()&#xA;&#xA;# do the above for many steps on a 17B parameter model&#xA;# attention is all you need&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dropouts&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    emb_dropout = 0.1,         # dropout after embedding&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        layer_dropout = 0.1,   # stochastic depth - dropout entire layer&#xA;        attn_dropout = 0.1,    # dropout post-attention&#xA;        ff_dropout = 0.1       # feedforward dropout&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Flash Attention&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/flash-attention.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;What originally started off as &lt;a href=&#34;https://arxiv.org/abs/2112.05682&#34;&gt;a short paper&lt;/a&gt; from Markus Rabe culminated as a practical fused attention CUDA kernel, named &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;Flash Attention&lt;/a&gt; by &lt;a href=&#34;https://tridao.me/&#34;&gt;Tri Dao&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The technique processes the attention matrix in tiles, only keeping track of the running softmax and exponentiated weighted sums. By recomputing on the backwards pass in a tiled fashion, one is able to keep the memory linear with respect to sequence length. This allows a lot of recent models to be able to reach for longer context lengths without worrying about the memory bottleneck.&lt;/p&gt; &#xA;&lt;p&gt;Other engineering decisions made by Tri Dao led to its enormous success, namely minimizing HBM accesses so that both the forwards and backwards outperform naive attention. In other words, flash attention is not only more memory efficient, but faster as well, making it a necessity for training transformers.&lt;/p&gt; &#xA;&lt;p&gt;MetaAI has recently added the ability to use &lt;a href=&#34;https://github.com/hazyresearch/flash-attention&#34;&gt;Tri Dao&#39;s CUDA kernel&lt;/a&gt; through the &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&#34;&gt;scaled_dot_product_attention&lt;/a&gt; function in Pytorch 2.0. (They also have a &lt;code&gt;mem_efficient&lt;/code&gt; attention, which is identical to flash attention design, just that the tiles are traversed differently)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;Llama&lt;/a&gt; was trained using Flash Attention. The only reason to avoid it is if you require operating on the attention matrix (dynamic positional bias, talking heads, residual attention).&lt;/p&gt; &#xA;&lt;p&gt;You can use it in this repository by setting &lt;code&gt;attn_flash&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; and enjoy the immediate memory savings and increase in speed.&lt;/p&gt; &#xA;&lt;p&gt;ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_flash = True # just set this to True if you have pytorch 2.0 installed&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Augmenting Self-attention with Persistent Memory&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/all-attention.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1907.01470&#34;&gt;https://arxiv.org/abs/1907.01470&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Proposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from x_transformers import Decoder, Encoder&#xA;&#xA;enc = Encoder(&#xA;    dim = 512,&#xA;    depth = 6,&#xA;    heads = 8,&#xA;    attn_num_mem_kv = 16 # 16 memory key / values&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memory Transformers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/memory-transformer.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11527&#34;&gt;https://arxiv.org/abs/2006.11527&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Proposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. This setting is compatible with both encoder and decoder training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    num_memory_tokens = 20, # 20 memory tokens&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update: MetaAI researchers &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;have found&lt;/a&gt; that adding memory tokens (they call them register tokens), alleviates outliers (which is suspected now to be a pathology of attention networks unable to &lt;a href=&#34;https://arxiv.org/abs/2306.12929&#34;&gt;attend to nothing&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Transformers Without Tears&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/scalenorm.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.05895&#34;&gt;https://arxiv.org/abs/1910.05895&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        use_scalenorm = True # set to True to use for all layers&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the l2 normalized embeddings proposed as part of &lt;code&gt;fixnorm&lt;/code&gt;. I have found it leads to improved convergence, when paired with small initialization (proposed by &lt;a href=&#34;https://github.com/BlinkDL&#34;&gt;BlinkDL&lt;/a&gt;). The small initialization will be taken care of as long as &lt;code&gt;l2norm_embed&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    l2norm_embed = True,    # set this to True for l2 normalized embedding + small init&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Along the same lines of l2 normalized embeddings, Huggingface&#39;s &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;175B parameter BLOOM&lt;/a&gt; also places a layernorm right after the embeddings and just before the tokens enter the attention layers. This was corroborated by Yandex&#39;s &lt;a href=&#34;https://github.com/yandex/YaLM-100B&#34;&gt;100B parameter YaLM&lt;/a&gt; to stabilize training.&lt;/p&gt; &#xA;&lt;p&gt;It is recommended you either have either &lt;code&gt;l2norm_embed&lt;/code&gt; or &lt;code&gt;post_emb_norm&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt; but not both, as they probably serve the same purpose.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    post_emb_norm = True,    # set this to True to layernorm summed token + pos embeddings&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Root Mean Square Layer Normalization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.07467&#34;&gt;https://arxiv.org/abs/1910.07467&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The authors propose to replace layer normalization with a simpler alternative, without mean centering and the learned bias. An investigative paper found this to be the &lt;a href=&#34;https://arxiv.org/abs/2102.11972&#34;&gt;best performing normalization variant&lt;/a&gt;. It was also used in Deepmind&#39;s latest large language models, &lt;a href=&#34;https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens&#34;&gt;Retro&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2112.11446&#34;&gt;Gopher&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        use_rmsnorm = True # set to true to use for all layers&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;July 2023&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14995&#34;&gt;A linear attention paper&lt;/a&gt; has experiments to show that removing the learned multiplicative gamma led to no performance degradation. This simplifies the RMS normalization to a satisfying &lt;code&gt;l2norm(x) * sqrt(dim)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        use_simple_rmsnorm = True # set to true to use for all layers&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GLU Variants Improve Transformer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/ffglu.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.05202&#34;&gt;https://arxiv.org/abs/2002.05202&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Noam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        ff_glu = True # set to true to use for all feedforwards&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;&gt;PaLM&lt;/a&gt; language model also chose to use the Swish GLU variant. You can turn this on by setting two flags&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        ff_swish = True, # set this to True&#xA;        ff_glu = True    # set to true to use for all feedforwards&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;No Bias in Feedforward&lt;/h3&gt; &#xA;&lt;p&gt;Starting with &lt;a href=&#34;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;&gt;PaLM&lt;/a&gt;, there begun a trend to remove biases from the transformer all together. &lt;a href=&#34;https://github.com/borisdayma&#34;&gt;Boris Dayma&lt;/a&gt; has run a number of experiments that showed removing biases from feedforwards led to increased throughput without any loss of accuracy. This was corroborated by &lt;a href=&#34;https://arxiv.org/abs/2212.14034&#34;&gt;yet another paper&lt;/a&gt; investigating transformer architecture variants.&lt;/p&gt; &#xA;&lt;p&gt;You can turn off the feedforward bias as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        ff_no_bias = True  # set this to True&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ReLU²&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.08668&#34;&gt;https://arxiv.org/abs/2109.08668&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This paper used neural architecture search and found an activation, Relu Squared, that is both simpler and performs better than GELU, in the autoregressive language model setting. I have confirmed this in my independent experiments. However, if one were using the GLU variant from above, GELU still performs better. Pending further corroboration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        ff_relu_squared = True&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/topk-attention.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.11637&#34;&gt;https://arxiv.org/abs/1912.11637&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This paper proposes an efficient way to sparsify attention by zeroing all dot-product query/key values not within the top k values. The show that this cheap method was as effective as other more expensive operations like sparsemax or entmax15. This technique comes with the cost of an extra hyperparameter (the top k values to keep). The paper recommends a value of &lt;code&gt;k = 8&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_sparse_topk = 8 # keep only the top 8 values before attention (softmax)&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Talking-Heads Attention&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/talking-heads.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.02436&#34;&gt;https://arxiv.org/abs/2003.02436&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A Noam Shazeer paper that proposes mixing information between heads pre and post attention (softmax). This comes with the cost of extra memory and compute.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_talking_heads = True  # turn on information exchange between attention heads&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;One Write-Head Is All You Need&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;https://arxiv.org/abs/1911.02150&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yet another Noam Shazeer paper (he&#39;s a legend) that proposes to only have one head for the key / values, but multi-headed queries. This paper was largely ignored for a while, but recently validated at scale in &lt;a href=&#34;https://arxiv.org/abs/2203.07814&#34;&gt;AlphaCode&lt;/a&gt; as well as &lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34;&gt;PaLM&lt;/a&gt;. It has the property of being memory efficient when decoding extremely large language models. You can use it with one keyword argument as shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_one_kv_head = True&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This has been further generalized in &lt;a href=&#34;https://arxiv.org/abs/2305.13245&#34;&gt;a recent paper&lt;/a&gt; to allow for groups of query heads to attend to a single key / value head. You can use this by specifying the &lt;code&gt;attn_kv_heads&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8,&#xA;        attn_kv_heads = 2 # say you want 4 query heads to attend to 1 key / value head&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Attention on Attention for Image Captioning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/attention-on-attention.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.06954&#34;&gt;https://arxiv.org/abs/1908.06954&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This paper proposes to add a gated linear unit at the end of the attention layer, further gated by the original queries. Although this is not widely used outside of visual question / answering, I suspect it should lead to improvements after seeing the success of the feedforward GLU variant.&lt;/p&gt; &#xA;&lt;p&gt;Update: After some experimentation, I found this variant actually performs worse, but if it were to be modified to not concatenate the queries before gating, it performs much better. That is what we will be using in this repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_on_attn = True  # gate output of attention layer, by queries&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Intra-attention Gating on Values&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/gate_values.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/deepmind/alphafold&#34;&gt;Alphafold2&lt;/a&gt; had a peculiar variant of attention where they gate the aggregated values with the input, presumably to have the block have more control over the update.&lt;/p&gt; &#xA;&lt;p&gt;A quick test shows a small but noticeable improvement, on about the same order as attention on attention.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_gate_values = True  # gate aggregated values with the input&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Improving Transformer Models by Reordering their Sublayers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/sandwich.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/sandwich-2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.03864&#34;&gt;https://arxiv.org/abs/1911.03864&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This paper proposes to break from the normal fixed pattern of alternating attention and feedforwards, but to have blocks of only attention at the beginning followed by blocks of feedforwards at the end. This was further corroborated by a paper by Nvidia that reduces the number of attention layers to be 1/3rd of the feedforwards without loss in performance.&lt;/p&gt; &#xA;&lt;p&gt;The amount of interleaving is controlled by a &#34;sandwich coefficient&#34;, which they found to be optimal at a value of &lt;code&gt;6&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can experiment with this feature as shown below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        sandwich_coef = 6  # interleave attention and feedforwards with sandwich coefficient of 6&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Weight-tied Layers&lt;/h3&gt; &#xA;&lt;p&gt;In the early days of the cambrian explosion of BERT, a paper explored weight tying all the layers, the model named &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;ALBERT&lt;/a&gt;. You can use it by setting &lt;code&gt;weight_tie_layers = True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        weight_tie_layers = True   # set this to True to weight tie all the layers&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you wish to do something more sophisticated, say 3 layers, with each layer recurrent 4 times before onto the next (similar to &lt;a href=&#34;https://arxiv.org/abs/2405.15071&#34;&gt;this paper&lt;/a&gt;), that is possible as well. Be aware the &lt;code&gt;layers_execute_order&lt;/code&gt; is 0-indexed&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        custom_layers = (&#xA;            &#39;a&#39;, &#39;f&#39;,        # 3 sets of attention and feedforward&#xA;            &#39;a&#39;, &#39;f&#39;,&#xA;            &#39;a&#39;, &#39;f&#39;&#xA;        ),&#xA;        layers_execute_order = (&#xA;            *((0, 1) * 4),   # each done 4 times before sequentially passed forward, but you can probably imagine some more interesting configurations...&#xA;            *((2, 3) * 4),&#xA;            *((4, 5) * 4),&#xA;        )&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/macaron-1.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/macaron-2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.02762&#34;&gt;https://arxiv.org/abs/1906.02762&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The authors propose to view the success of transformers from a dynamical systems point of view, and then proposes an improvement based on mathematics of that POV. Specifically, they propose to place the attention layer in between two feedforward layers. This was adopted by a paper using transformers for speech recognition, the &lt;a href=&#34;https://arxiv.org/abs/2005.08100&#34;&gt;Conformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        macaron = True  # use macaron configuration&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;T5&#39;s Simplified Relative Positional Encoding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;https://arxiv.org/abs/1910.10683&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;T5 is one of the most successful encoder / decoder transformer architectures trained to date. They invented a new simplified relative positional encoding based on learned bias values that are added to the attention matrix pre-softmax. This bias is shared and injected into each attention layer. I have decided to include this because it offers a cheap way to have relative positional encoding (superior to absolute positional), and I have read papers that suggest having positional encoding added to each layer (vs only before the first) is beneficial.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        rel_pos_bias = True  # adds relative positional bias to all attention layers, a la T5&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Residual Attention&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/residual_attn.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.11747&#34;&gt;https://arxiv.org/abs/2012.11747&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This paper from Google proposes residualizing the pre-attention scores across all layers. At the cost of no extra parameters, they show improvement on top of regular attention networks. If you turn on this setting, be aware that the best results in the paper used post-normalization, in which case a learning warmup will be needed. The authors also reported that they could use a higher learning rate and get even better gains in the same amount of steps. (In the paper they use &lt;code&gt;2e-4&lt;/code&gt; vs &lt;code&gt;1e-4&lt;/code&gt; for vanilla transformer)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Encoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Encoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        pre_norm = False,       # in the paper, residual attention had best results with post-layernorm&#xA;        residual_attn = True    # add residual attention&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I also tried residualizing cross attention and may have noticed an improvement in convergence. You can try it by setting the &lt;code&gt;cross_residual_attn&lt;/code&gt; keyword to &lt;code&gt;True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import XTransformer&#xA;&#xA;model = XTransformer(&#xA;    dim = 512,&#xA;    enc_num_tokens = 256,&#xA;    enc_depth = 6,&#xA;    enc_heads = 8,&#xA;    enc_max_seq_len = 1024,&#xA;    dec_num_tokens = 256,&#xA;    dec_depth = 6,&#xA;    dec_heads = 8,&#xA;    dec_max_seq_len = 1024,&#xA;    dec_cross_residual_attn = True     # residualize cross attention&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Transformer-XL recurrence&lt;/h3&gt; &#xA;&lt;p&gt;You can also do Transformer-XL recurrence, by simply passing in a &lt;code&gt;max_mem_len&lt;/code&gt; in the &lt;code&gt;TransformerWrapper&lt;/code&gt; class, and then making sure your &lt;code&gt;Decoder&lt;/code&gt; has &lt;code&gt;rel_pos_bias&lt;/code&gt; (or &lt;code&gt;rotary_pos_emb&lt;/code&gt;) set to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, you can retrieve the memories at each step with the &lt;code&gt;return_mems&lt;/code&gt; keyword and pass it to the next iteration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model_xl = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 512,&#xA;    max_mem_len = 2048,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        rel_pos_bias = True&#xA;    )&#xA;)&#xA;&#xA;seg1 = torch.randint(0, 20000, (1, 512))&#xA;seg2 = torch.randint(0, 20000, (1, 512))&#xA;seg3 = torch.randint(0, 20000, (1, 512))&#xA;&#xA;logits1, mems1  = model_xl(seg1, return_mems = True)&#xA;logits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True)&#xA;logits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setting up the logic for training and sampling from transformer xl can be a bit overwhelming. This repository offers a simple wrapper that should make this easy, with the &lt;code&gt;XLAutoregressiveWrapper&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pass in the above model_xl&#xA;&#xA;xl_wrapper = XLAutoregressiveWrapper(model_xl)&#xA;&#xA;seg = torch.randint(0, 20000, (1, 4096)).cuda()  # sequence exceeding max length, automatically segmented and memory managed&#xA;&#xA;loss = xl_wrapper(seg)&#xA;loss.backward()&#xA;&#xA;# then, after much training&#xA;&#xA;prime = seg[:, :1024]   # if prime exceeds max length, memory will be caught up before generating&#xA;&#xA;generated = xl_wrapper.generate(prime, 4096)  # (1, 4096)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Enhanced recurrence&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/enhanced-recurrence.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.15688&#34;&gt;This paper&lt;/a&gt; proposes a simple technique to enhance the range of Transformer-XL. They simply route the memory segment of a layer to the layer below it, for the next recurrent step. You can enable this by setting &lt;code&gt;shift_mem_down = 1&lt;/code&gt;. You can also shift down arbitrary number of layers by setting this value to &lt;code&gt;&amp;gt; 1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model_xl = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 512,&#xA;    max_mem_len = 2048,&#xA;    shift_mem_down = 1,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        rotary_pos_emb = True&#xA;    )&#xA;)&#xA;&#xA;seg1 = torch.randint(0, 20000, (1, 512))&#xA;seg2 = torch.randint(0, 20000, (1, 512))&#xA;seg3 = torch.randint(0, 20000, (1, 512))&#xA;&#xA;logits1, mems1  = model_xl(seg1, return_mems = True)&#xA;logits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) # mems1 of layer N are automatically routed to the layer N-1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gated residual&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/gating.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06764&#34;&gt;https://arxiv.org/abs/1910.06764&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The authors propose gating the residual connections in the transformer network and demonstrate increased stability and performance for Transformer-XL in a variety of reinforcement learning tasks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    max_mem_len = 2048,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 16,&#xA;        gate_residual = True&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Rotary Positional Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/rotary.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Developed in Beijing, this new technique quickly gained interest in the NLP circles. In short, it allows you to endow the transformer with relative positional embeddings at the cost of no learned parameters. You apply a rotary operation to the queries and keys prior to their dot product in attention. The big idea is injecting positions through rotations.&lt;/p&gt; &#xA;&lt;p&gt;Highly recommend that you have this turned on whenever you are working on an ordered sequence.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        rotary_pos_emb = True  # turns on rotary positional embeddings&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update (12/2022): Rotary embedding has since been hugely successful, widely adopted in many large language models, including the largest in the world, PaLM. However, it has been uncovered in the ALiBi paper that rotary embeddings cannot length extrapolate well. This was recently addressed in &lt;a href=&#34;https://arxiv.org/abs/2212.10554v1&#34;&gt;a Microsoft research paper&lt;/a&gt;. They propose a way to unobtrusively add the same decay as in ALiBi, and found that this resolves the extrapolation problem. You can use it in this repository by setting &lt;code&gt;rotary_xpos = True&lt;/code&gt;. Like ALiBi, it would enforce the attention to be local. You can set the receptive field with &lt;code&gt;rotary_xpos_scale_base&lt;/code&gt; value, which defaults to &lt;code&gt;512&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        rotary_xpos = True   # modified rotary to extrapolate well beyond length at which it was trained&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dynamic Positional Bias&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/dynamic-pos-bias.png&#34; width=&#34;150px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This technique bears roots from the field of vision transformers, where researchers are trying to have relative positions generalize to larger resolutions (without having to retrain the entire network). It was used in two recent papers, &lt;a href=&#34;https://arxiv.org/abs/2108.00154&#34;&gt;CrossFormer&lt;/a&gt;, as well as &lt;a href=&#34;https://arxiv.org/abs/2111.09883&#34;&gt;SwinV2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cfoster0&#34;&gt;Charles Foster&lt;/a&gt; first tried this for a language model, and found that it works. Later on &lt;a href=&#34;https://github.com/bob80333&#34;&gt;Eric Engelhart&lt;/a&gt; produced experimental results that show the same type of extrapolation holds, even for 1d sequences.&lt;/p&gt; &#xA;&lt;p&gt;Eric trained at sequence lengths of 128, and showed that it generalized well to 1024. In addition, he showed that linear positions was better than log (used in SwinV2), for language.&lt;/p&gt; &#xA;&lt;p&gt;Linear distances&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/dynamic-pos-bias-linear.png&#34; width=&#34;600px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Log distances&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/dynamic-pos-bias-log.png&#34; width=&#34;600px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Negative control - Sinusoidal&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/dynamic-pos-bias-sinusoidal.png&#34; width=&#34;600px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More of Eric&#39;s experimental results can be found &lt;a href=&#34;https://github.com/bob80333/investigating_extrapolation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use this type of relative position if you wish to train at smaller sequence lengths and have it generalize to longer ones, for both autoregressive and bidirectional models.&lt;/p&gt; &#xA;&lt;p&gt;Update: &lt;a href=&#34;https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/discussion/460121&#34;&gt;First place RNA folding using dynamic positional bias&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 256,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        dynamic_pos_bias = True,                # set this to True&#xA;        dynamic_pos_bias_log_distance = False   # whether to use log distance, as in SwinV2&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ALiBi Positional Embedding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ofir.io/train_short_test_long.pdf&#34;&gt;This paper&lt;/a&gt; proposes to simply apply a static linear bias to the attention matrix. The authors show this is not only effective as a relative positional encoding, but also allows the attention net to extrapolate to greater sequences length than what it was trained on, for autoregressive language models.&lt;/p&gt; &#xA;&lt;p&gt;This repository also offers a bidirectional variant (nonsymmetric), proposed by the authors &lt;a href=&#34;https://github.com/ofirpress/attention_with_linear_biases/issues/5&#34;&gt;here&lt;/a&gt;. However, this is untested. If you need bidirectional length extrapolation, the safest option would be Dynamic Position Bias&lt;/p&gt; &#xA;&lt;p&gt;Update: It may be that ALiBi enforces a strong local attention across the heads, and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing, I&#39;ve decided to introduce another hyperparameter &lt;code&gt;alibi_num_heads&lt;/code&gt;, so one can specify less heads for the ALiBi bias&lt;/p&gt; &#xA;&lt;p&gt;Update: There are reports that ALiBi outperform Rotary embeddings for pretraining and downstream fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;Update: &lt;a href=&#34;https://arxiv.org/abs/2305.19466&#34;&gt;New paper&lt;/a&gt; shows that no positional embedding can length extrapolate even than explicit ones&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        alibi_pos_bias = True, # turns on ALiBi positional embedding&#xA;        alibi_num_heads = 4    # only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Shifted Tokens&lt;/h3&gt; &#xA;&lt;p&gt;An &lt;a href=&#34;https://github.com/BlinkDL&#34;&gt;independent researcher&lt;/a&gt; has found that shifting a subset of the feature dimension along the sequence dimension by 1 token helps with convergence (&lt;a href=&#34;https://zhuanlan.zhihu.com/p/191393788&#34;&gt;Time-mixing&lt;/a&gt;). I have tested this for the autoregressive case and can confirm that it leads to greatly improved convergence. This also lines up with &lt;a href=&#34;https://arxiv.org/abs/2106.07477&#34;&gt;the results&lt;/a&gt; of some papers in the vision domain.&lt;/p&gt; &#xA;&lt;p&gt;To use it, simply set &lt;code&gt;shift_tokens = 1&lt;/code&gt; (or to whatever number of shifts you desire). The feature dimension will be divided by &lt;code&gt;shift_tokens + 1&lt;/code&gt; and then each chunk will be shifted &lt;code&gt;[0, shift_tokens]&lt;/code&gt; respectively&lt;/p&gt; &#xA;&lt;p&gt;Update: new experiments by @sdtblck suggests this may only work for character-level training&lt;/p&gt; &#xA;&lt;p&gt;Update: after more experiments, it seems that in the context of BPE encoding, with rotary turned on, there is no benefit to shifting. for character-level training, shifting may still improve a tiny bit&lt;/p&gt; &#xA;&lt;p&gt;Update: When doing BPE encoded tokens, it seems that shift of 2 will bottleneck the dimensions (divided by 5). It is recommended you always do a shift of 1, unless if you are working with character level.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        shift_tokens = 1&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want finer control over how much is shifted per block (whether attention or feedforward), simply pass in a tuple of size that is equal to the number of layers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        shift_tokens = (1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0) # 12 blocks, attention and feedforward alternating, with progressively less shifting&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Sandwich Norm&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/sandwich_norm.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;This technique first made an appearance in &lt;a href=&#34;https://arxiv.org/abs/2105.13290&#34;&gt;the CoqView paper&lt;/a&gt;, a Chinese version of the famous text-to-image transformer DALL-E. They propose, when using pre-layernorm, to add an extra layernorm to all the branch outputs. I have found this to be very effective for a number of projects, when facing instability during training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        sandwich_norm = True # set this to True&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ResiDual&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/resi_dual.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.14802&#34;&gt;This Microsoft paper&lt;/a&gt; proposes yet another normalization configuration, combining both pre and post layernorm. They claim this hybridization reduces representation collapse (known to be an issue with pre-layernorm with increasing depth), while maintaining stability and reducing vanishing gradients (issues with post-layernorm). Initial experiments on my end show it to work no worse than pre-layernorm or sandwich norm. More study needed by the public to see if this is actually a winning technique.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        resi_dual = True,               # set this to True&#xA;        resi_dual_scale = 0.1           # in appendix, they said on fp16 the prenorm residual is prone to overflow. they claim by scaling it at each layer by a factor, it would prevent the overflow, and keep results the same (as layernorms are invariant to scaling of the input)&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Normformer&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/normformer.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://openreview.net/forum?id=GMYWzWztDx5&#34;&gt;paper&lt;/a&gt; uncovers an issue with pre-norm transformers where gradients are mismatched between the early and later layers. They propose 4 changes, of which I will be offering 3.&lt;/p&gt; &#xA;&lt;p&gt;The first change is to offer per head scaling after aggregating the values in attention. My experiments show a slight improvement in convergence.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_head_scale = True  # set this to True&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The second change is an extra layernorm right after the activation in the feedforward. I have also verified a slight improvement, at the cost of extra compute.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        ff_post_act_ln = True # set this to True&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the residual scaling, you simply have to set &lt;code&gt;scale_residual = True&lt;/code&gt;. I have noticed slight improvements, but occasional instability as well, so use with caution.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        scale_residual = True # set this to True&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The last change is a layernorm right after the outwards projection in attention. This is actually identical to the sandwich norm proposed by the Coqview paper, so you can use this by simply setting &lt;code&gt;sandwich_norm = True&lt;/code&gt;, although it would also add it to the feedforward layer.&lt;/p&gt; &#xA;&lt;h3&gt;Cosine Sim Attention&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/cosine-sim-attention.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://arxiv.org/abs/2010.04245&#34;&gt;paper&lt;/a&gt; proposes to l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity), with the additional change of the scale being learned rather than static. The normalization prevents the attention operation from overflowing, and removes any need for numerical stability measures prior to softmax. Both are perennial problems when training transformers.&lt;/p&gt; &#xA;&lt;p&gt;This was validated at scale recently by the training of &lt;a href=&#34;https://arxiv.org/abs/2111.09883&#34;&gt;a 3B parameter vision transformer&lt;/a&gt;. The SwinV2 paper also proposes to change the pre-layernorm to a post-layernorm for further stability.&lt;/p&gt; &#xA;&lt;p&gt;I have validated that this works just as well as dot product attention in an autoregressive setting, if one were to initialize the temperature as proposed in the QK-norm paper (as a function of the sequence length).&lt;/p&gt; &#xA;&lt;p&gt;This flavor of attention also has &lt;a href=&#34;https://arxiv.org/abs/2111.05498&#34;&gt;a connection&lt;/a&gt; to sparse distributed memory. &lt;a href=&#34;https://www.youtube.com/watch?v=THIIk7LR9_8&#34;&gt;[youtube talk]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update: I have discovered a way to remove the learned temperature altogether, by grouping the feature dimension and doing l2-normalization on each group. This allows the queries and keys to have a similarity that is upper bounded by the number of groups. A group size of 8 or 16 was sufficient in my tests. Decided to name this technique &#34;Grouped QK Normalization&#34;. The drawback is that I believe an attention head dimension 32 is too small to use this tactic (a dimension often used in vision)&lt;/p&gt; &#xA;&lt;p&gt;Update 2: Tero Karras has successfully used cosine sim attention in &lt;a href=&#34;https://arxiv.org/abs/2312.02696&#34;&gt;a new paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use it as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_qk_norm = True,       # set this to True&#xA;        attn_qk_norm_groups = 8    # number of groups in the feature dimension for l2norm, similarity scores will be bounded between [-group, group]. determines how sharp the attention can be&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another update: Simply scaling the cosine similarity (group of 1) with a fixed constant (10) may work too&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;        attn_qk_norm = True,       # set to True&#xA;        attn_qk_norm_scale = 10    # new scale on the similarity, with groups of 1&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;QK RMSNorm&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/qknorm-analysis.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update: Google Brain has proven out something similar to cosine sim attention in &lt;a href=&#34;https://arxiv.org/abs/2302.05442&#34;&gt;a 22B parameter model&lt;/a&gt;. In their papers, they have analysis showing that the normalization resulted in not only extra stability, but also better results in the end (due to less need to adjust learning rate when increasing parameter count).&lt;/p&gt; &#xA;&lt;p&gt;We are nearing the point of wiping out a source of transformer training instability with one simple intervention, in my opinion. The only slight difference in the paper is that they still have a learned scale across the feature dimension (per use of rmsnorm). Not sure how critical this is, but just to make sure we don&#39;t miss anything, I will include this here. You can use this by setting &lt;code&gt;qk_norm_dim_scale = True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update: &lt;a href=&#34;https://twitter.com/Tim_Dettmers/status/1625531080513306627&#34;&gt;Counterpoint from Tim Dettmers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update 2: &lt;a href=&#34;https://arxiv.org/abs/2305.19268&#34;&gt;Counter&lt;/a&gt; to Tim&#39;s assertion that outliers are needed, and potentially even &lt;a href=&#34;https://arxiv.org/abs/2306.12929&#34;&gt;some solutions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update 3: Used by &lt;a href=&#34;https://www.adept.ai/blog/persimmon-8b&#34;&gt;8B parameter LLM&lt;/a&gt; successfully&lt;/p&gt; &#xA;&lt;p&gt;Update 4: a MetaAI group found that they can &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;alleviate outliers&lt;/a&gt; by adding &lt;code&gt;register tokens&lt;/code&gt;, also known as &lt;code&gt;memory tokens&lt;/code&gt; from earlier literature (Burtsev et al). Perhaps what should be tried next is see if qk norm can be improved in the presence of memory tokens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8,&#xA;        attn_qk_norm = True,&#xA;        attn_qk_norm_dim_scale = True # set this to True, in addition to `attn_qk_norm = True`&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 256, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Turning off absolute positional embedding&lt;/h3&gt; &#xA;&lt;p&gt;A number of papers have hinted that causal transformers (&lt;code&gt;Decoder&lt;/code&gt;) can learn absolute positions in the absence of added embeddings of any sort. This was recently thoroughly investigated &lt;a href=&#34;https://arxiv.org/abs/2203.16634&#34;&gt;here&lt;/a&gt;. You can turn off the absolute positional embedding by setting &lt;code&gt;use_abs_pos_emb = False&lt;/code&gt; in the &lt;code&gt;TransformerWrapper&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Given &lt;a href=&#34;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;&gt;PaLM&lt;/a&gt;, the trend going forward may be to forgo absolute positional embedding (again, for causal transformers only), and add relative positional embeddings with RoPE, ALiBi, etc.&lt;/p&gt; &#xA;&lt;p&gt;Update: &lt;a href=&#34;https://arxiv.org/abs/2305.19466&#34;&gt;This paper&lt;/a&gt; shows that in the absence of any engineered absolute or relative positional embeddings, decoders can generate implicit positions, and even length generalize better than solutions of the past. They were unaware of dynamic positional bias, however.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    use_abs_pos_emb = False,   # set this to False&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 6,&#xA;        heads = 8,&#xA;    )&#xA;)&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Forgetful Causal Mask&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/fcm.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.13432&#34;&gt;This paper&lt;/a&gt; shows convincing results that one can combine masking (from masked language modeling) with autoregressive training, leading to significantly better results.&lt;/p&gt; &#xA;&lt;p&gt;You can use this by setting the &lt;code&gt;mask_prob&lt;/code&gt; on the &lt;code&gt;AutoregressiveWrapper&lt;/code&gt; class&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import TransformerWrapper, Decoder, AutoregressiveWrapper&#xA;&#xA;model = TransformerWrapper(&#xA;    num_tokens = 20000,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;model = AutoregressiveWrapper(&#xA;    model,&#xA;    mask_prob = 0.15  # in paper, they use 15%, same as BERT&#xA;).cuda()&#xA;&#xA;# mock data&#xA;&#xA;x = torch.randint(0, 20000, (1, 1024)).cuda()&#xA;&#xA;# derive cross entropy loss, masking all taken care of&#xA;&#xA;loss = model(x)&#xA;loss.backward()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Miscellaneous&lt;/h2&gt; &#xA;&lt;h3&gt;Cross Attention&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import Encoder, CrossAttender&#xA;&#xA;enc = Encoder(dim = 512, depth = 6)&#xA;model = CrossAttender(dim = 512, depth = 6)&#xA;&#xA;nodes = torch.randn(1, 1, 512)&#xA;node_masks = torch.ones(1, 1).bool()&#xA;&#xA;neighbors = torch.randn(1, 5, 512)&#xA;neighbor_masks = torch.ones(1, 5).bool()&#xA;&#xA;encoded_neighbors = enc(neighbors, mask = neighbor_masks)&#xA;model(nodes, context = encoded_neighbors, mask = node_masks, context_mask = neighbor_masks) # (1, 1, 512)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Continuous Embeddings&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import ContinuousTransformerWrapper, Decoder&#xA;&#xA;model = ContinuousTransformerWrapper(&#xA;    dim_in = 32,&#xA;    dim_out = 100,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;x = torch.randn((1, 1024, 32))&#xA;mask = torch.ones(1, 1024).bool()&#xA;&#xA;model(x, mask = mask) # (1, 1024, 100)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also train a transformer that accepts continuous values autoregressively easily, in the same scheme as done successfully in &lt;a href=&#34;https://arxiv.org/abs/2112.05329&#34;&gt;this paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from x_transformers import ContinuousTransformerWrapper, Decoder&#xA;from x_transformers import ContinuousAutoregressiveWrapper&#xA;&#xA;model = ContinuousTransformerWrapper(&#xA;    dim_in = 777,&#xA;    dim_out = 777,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;# wrap it with the continuous autoregressive wrapper&#xA;&#xA;model = ContinuousAutoregressiveWrapper(model)&#xA;&#xA;# mock data&#xA;&#xA;x = torch.randn((1, 1024, 777))&#xA;mask = torch.ones(1, 1024).bool()&#xA;&#xA;# train on a lot of data above&#xA;&#xA;loss = model(x, mask = mask)&#xA;loss.backward&#xA;&#xA;# then generate&#xA;&#xA;start_emb = torch.randn(1, 777)&#xA;generated = model.generate(start_emb, 17) # (17, 777)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;xVal - Continuous and Discrete&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/x-transformers/main/images/xval.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is promising work that resulted from the collaboration across many institutes (collectively known as Polymathic AI). They found that by offering a continuously scaled number token to the transformer, the transformer was able to generalize arithmetic and forecasting tasks better than the alternative encoding schemes.&lt;/p&gt; &#xA;&lt;p&gt;This is corroborated by some &lt;a href=&#34;https://github.com/lucidrains/tab-transformer-pytorch#ft-transformer&#34;&gt;prior work&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from x_transformers import (&#xA;    Decoder,&#xA;    XValTransformerWrapper,&#xA;    XValAutoregressiveWrapper&#xA;)&#xA;&#xA;model = XValTransformerWrapper(&#xA;    num_tokens = 4,&#xA;    numerical_token_id = 3,&#xA;    max_seq_len = 1024,&#xA;    attn_layers = Decoder(&#xA;        dim = 512,&#xA;        depth = 12,&#xA;        heads = 8&#xA;    )&#xA;)&#xA;&#xA;# wrap it with the xval autoregressive wrapper&#xA;&#xA;model = XValAutoregressiveWrapper(model)&#xA;&#xA;# mock data&#xA;&#xA;ids = torch.randint(0, 4, (1, 777))&#xA;nums = torch.randn(1, 777)&#xA;&#xA;# train on a lot of data above&#xA;&#xA;loss = model(ids, nums)&#xA;loss.backward()&#xA;&#xA;# then generate&#xA;&#xA;start_ids = torch.randint(0, 4, (1, 1))&#xA;start_nums = torch.randn(1, 1)&#xA;&#xA;ids_out, num_out, is_number_mask = model.generate(start_ids, start_nums, 17)&#xA;&#xA;# (1, 17), (1, 17), (1, 17)&#xA;&#xA;# discrete, continuous, mask for discrete / continuous&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{vaswani2017attention,&#xA;    title   = {Attention Is All You Need},&#xA;    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},&#xA;    year    = {2017},&#xA;    eprint  = {1706.03762},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{DBLP:journals/corr/abs-1907-01470,&#xA;    author    = {Sainbayar Sukhbaatar and Edouard Grave and Guillaume Lample and Herv{\&#39;{e}} J{\&#39;{e}}gou and Armand Joulin},&#xA;    title     = {Augmenting Self-attention with Persistent Memory},&#xA;    journal   = {CoRR},&#xA;    volume    = {abs/1907.01470},&#xA;    year      = {2019},&#xA;    url       = {http://arxiv.org/abs/1907.01470}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{1910.05895,&#xA;    author  = {Toan Q. Nguyen and Julian Salazar},&#xA;    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},&#xA;    year    = {2019},&#xA;    eprint  = {arXiv:1910.05895},&#xA;    doi     = {10.5281/zenodo.3525484},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shazeer2020glu,&#xA;    title   = {GLU Variants Improve Transformer},&#xA;    author  = {Noam Shazeer},&#xA;    year    = {2020},&#xA;    url     = {https://arxiv.org/abs/2002.05202}    &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Zoph2022STMoEDS,&#xA;    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},&#xA;    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{bhojanapalli2020lowrank,&#xA;    title   = {Low-Rank Bottleneck in Multi-head Attention Models},&#xA;    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},&#xA;    year    = {2020},&#xA;    eprint  = {2002.07028}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{burtsev2020memory,&#xA;    title   = {Memory Transformer}, &#xA;    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},&#xA;    year    = {2020},&#xA;    eprint  = {2006.11527},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhao2019explicit,&#xA;    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, &#xA;    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},&#xA;    year    = {2019},&#xA;    eprint  = {1912.11637},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{correia2019adaptively,&#xA;    title   = {Adaptively Sparse Transformers},&#xA;    author  = {Gonçalo M. Correia and Vlad Niculae and André F. T. Martins},&#xA;    year    = {2019},&#xA;    eprint  = {1909.00015},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shazeer2020talkingheads,&#xA;    title   = {Talking-Heads Attention}, &#xA;    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},&#xA;    year    = {2020},&#xA;    eprint  = {2003.02436},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{press2020improving,&#xA;    title   = {Improving Transformer Models by Reordering their Sublayers}, &#xA;    author  = {Ofir Press and Noah A. Smith and Omer Levy},&#xA;    year    = {2020},&#xA;    eprint  = {1911.03864},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{lu2019understanding,&#xA;    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, &#xA;    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},&#xA;    year    = {2019},&#xA;    eprint  = {1906.02762},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ke2020rethinking,&#xA;    title     = {Rethinking Positional Encoding in Language Pre-training},&#xA;    author    = {Guolin Ke and Di He and Tie-Yan Liu},&#xA;    year      = {2020},&#xA;    eprint    = {2006.15595},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{dosovitskiy2020image,&#xA;    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},&#xA;    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},&#xA;    year    = {2020},&#xA;    eprint  = {2010.11929},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{huang2019attention,&#xA;    title   = {Attention on Attention for Image Captioning},&#xA;    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},&#xA;    year    = {2019},&#xA;    eprint  = {1908.06954},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{raffel2020exploring,&#xA;    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, &#xA;    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},&#xA;    year    = {2020},&#xA;    eprint  = {1910.10683},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{martins-etal-2020-sparse,&#xA;    title   = &#34;Sparse Text Generation&#34;,&#xA;    author  = &#34;Martins, Pedro Henrique  and&#xA;        Marinho, Zita  and&#xA;        Martins, Andr{\&#39;e} F. T.&#34;,&#xA;    booktitle = &#34;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&#34;,&#xA;    month   = nov,&#xA;    year    = &#34;2020&#34;,&#xA;    address = &#34;Online&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url     = &#34;https://www.aclweb.org/anthology/2020.emnlp-main.348&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{he2020realformer,&#xA;    title   = {RealFormer: Transformer Likes Residual Attention},&#xA;    author  = {Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie},&#xA;    year    = {2020},&#xA;    eprint  = {2012.11747},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{carion2020endtoend,&#xA;    title   = {End-to-End Object Detection with Transformers},&#xA;    author  = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},&#xA;    year    = {2020},&#xA;    eprint  = {2005.12872},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{press2021ALiBi,&#xA;    title   = {Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation},&#xA;    author  = {Ofir Press and Noah A. Smith and Mike Lewis},&#xA;    year    = {2021},&#xA;    url     = {https://ofir.io/train_short_test_long.pdf}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{parisotto2019stabilizing,&#xA;    title     = {Stabilizing Transformers for Reinforcement Learning},&#xA;    author    = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},&#xA;    year      = {2019},&#xA;    eprint    = {1910.06764},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{narang2021transformer,&#xA;    title       = {Do Transformer Modifications Transfer Across Implementations and Applications?},&#xA;    author      = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},&#xA;    year        = {2021},&#xA;    eprint      = {2102.11972},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhang2019root,&#xA;    title   = {Root Mean Square Layer Normalization},&#xA;    author  = {Biao Zhang and Rico Sennrich},&#xA;    year    = {2019},&#xA;    eprint  = {1910.07467},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Qin2023ScalingTT,&#xA;    title   = {Scaling TransNormer to 175 Billion Parameters},&#xA;    author  = {Zhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Fei Yuan and Xiao Luo and Y. Qiao and Yiran Zhong},&#xA;    year    = {2023},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:260203124}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{su2021roformer,&#xA;    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},&#xA;    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},&#xA;    year    = {2021},&#xA;    eprint  = {2104.09864},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Chen2023ExtendingCW,&#xA;    title   = {Extending Context Window of Large Language Models via Positional Interpolation},&#xA;    author  = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},&#xA;    year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Sun2022ALT,&#xA;  title     = {A Length-Extrapolatable Transformer},&#xA;  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaFold2021,&#xA;    author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\&#39;\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},&#xA;    journal = {Nature},&#xA;    title   = {Highly accurate protein structure prediction with {AlphaFold}},&#xA;    year    = {2021},&#xA;    doi     = {10.1038/s41586-021-03819-2},&#xA;    note    = {(Accelerated article preview)},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{peng_bo_2021_5196578,&#xA;    author       = {PENG Bo},&#xA;    title        = {BlinkDL/RWKV-LM: 0.01},&#xA;    month        = {aug},&#xA;    year         = {2021},&#xA;    publisher    = {Zenodo},&#xA;    version      = {0.01},&#xA;    doi          = {10.5281/zenodo.5196578},&#xA;    url          = {https://doi.org/10.5281/zenodo.5196578}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{csordás2021devil,&#xA;    title   = {The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},&#xA;    author  = {Róbert Csordás and Kazuki Irie and Jürgen Schmidhuber},&#xA;    year    = {2021},&#xA;    eprint  = {2108.12284},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{so2021primer,&#xA;    title   = {Primer: Searching for Efficient Transformers for Language Modeling}, &#xA;    author  = {David R. So and Wojciech Mańke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},&#xA;    year    = {2021},&#xA;    eprint  = {2109.08668},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ding2021erniedoc,&#xA;    title   = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer}, &#xA;    author  = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},&#xA;    year    = {2021},&#xA;    eprint  = {2012.15688},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ding2021cogview,&#xA;    title   = {CogView: Mastering Text-to-Image Generation via Transformers},&#xA;    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},&#xA;    year    = {2021},&#xA;    eprint  = {2105.13290},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{anonymous2022normformer,&#xA;    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},&#xA;    author  = {Anonymous},&#xA;    booktitle = {Submitted to The Tenth International Conference on Learning Representations },&#xA;    year    = {2022},&#xA;    url     = {https://openreview.net/forum?id=GMYWzWztDx5},&#xA;    note    = {under review}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{henry2020querykey,&#xA;    title   = {Query-Key Normalization for Transformers},&#xA;    author  = {Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},&#xA;    year    = {2020},&#xA;    eprint  = {2010.04245},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{liu2021swin,&#xA;    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},&#xA;    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},&#xA;    year    = {2021},&#xA;    eprint  = {2111.09883},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Haviv2022TransformerLM,&#xA;    title   = {Transformer Language Models without Positional Encodings Still Learn Positional Information},&#xA;    author  = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2203.16634}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chowdhery2022PaLM,&#xA;    title   = {PaLM: Scaling Language Modeling with Pathways},&#xA;    author  = {Chowdhery, Aakanksha et al},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Shazeer2019FastTD,&#xA;    title   = {Fast Transformer Decoding: One Write-Head is All You Need},&#xA;    author  = {Noam M. Shazeer},&#xA;    journal = {ArXiv},&#xA;    year    = {2019},&#xA;    volume  = {abs/1911.02150}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Ainslie2023GQATG,&#xA;    title   = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},&#xA;    author  = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr&#39;on and Sumit K. Sanghai},&#xA;    journal = {ArXiv},&#xA;    year    = {2023},&#xA;    volume  = {abs/2305.13245},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:258833177}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{schlag2020enhancing,&#xA;    title   = {Enhancing the Transformer with explicit relational encoding for math problem solving},&#xA;    author  = {Imanol Schlag and Paul Smolensky and Roland Fernandez and Nebojsa Jojic and J{\&#34;u}rgen Schmidhuber and Jianfeng Gao},&#xA;    year    = {2020},&#xA;    url     = {https://openreview.net/forum?id=B1xfElrKPr}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Liu2022FCMFC,&#xA;    title   = {FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners},&#xA;    author  = {Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2210.13432}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Huang2016DeepNW,&#xA;    title   = {Deep Networks with Stochastic Depth},&#xA;    author  = {Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger},&#xA;    booktitle = {European Conference on Computer Vision},&#xA;    year    = {2016}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Hua2022TransformerQI,&#xA;    title   = {Transformer Quality in Linear Time},&#xA;    author  = {Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},&#xA;    booktitle = {International Conference on Machine Learning},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Chang2022MaskGITMG,&#xA;    title   = {MaskGIT: Masked Generative Image Transformer},&#xA;    author  = {Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman},&#xA;    journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    year    = {2022},&#xA;    pages   = {11305-11315}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Lezama2022ImprovedMI,&#xA;    title   = {Improved Masked Image Generation with Token-Critic},&#xA;    author  = {Jos{\&#39;e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2209.04439}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{https://doi.org/10.48550/arxiv.2302.01327,&#xA;    doi     = {10.48550/ARXIV.2302.01327},&#xA;    url     = {https://arxiv.org/abs/2302.01327},&#xA;    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},&#xA;    title   = {Dual PatchNorm},&#xA;    publisher = {arXiv},&#xA;    year    = {2023},&#xA;    copyright = {Creative Commons Attribution 4.0 International}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{dao2022flashattention,&#xA;    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},&#xA;    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\&#39;e}, Christopher},&#xA;    booktitle = {Advances in Neural Information Processing Systems},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Xie2023ResiDualTW,&#xA;  title     = {ResiDual: Transformer with Dual Residual Connections},&#xA;  author    = {Shufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan},&#xA;  journal   = {ArXiv},&#xA;  year      = {2023},&#xA;  volume    = {abs/2304.14802}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Dehghani2023ScalingVT,&#xA;    title   = {Scaling Vision Transformers to 22 Billion Parameters},&#xA;    author  = {Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti&#39;c and Dustin Tran and Thomas Kipf and Mario Luvci&#39;c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},&#xA;    year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Beyer2022BetterPV,&#xA;    title   = {Better plain ViT baselines for ImageNet-1k},&#xA;    author  = {Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2205.01580}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Kazemnejad2023TheIO,&#xA;    title   = {The Impact of Positional Encoding on Length Generalization in Transformers},&#xA;    author  = {Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},&#xA;    journal = {ArXiv},&#xA;    year    = {2023},&#xA;    volume  = {abs/2305.19466}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{bloc97-2023&#xA;    title   = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.},&#xA;    author  = {/u/bloc97},&#xA;    url     = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Zoph2022STMoEDS,&#xA;    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},&#xA;    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Lan2019ALBERTAL,&#xA;    title   = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},&#xA;    author  = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},&#xA;    journal = {ArXiv},&#xA;    year    = {2019},&#xA;    volume  = {abs/1909.11942},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:202888986}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Li2022ContrastiveDO,&#xA;    title   = {Contrastive Decoding: Open-ended Text Generation as Optimization},&#xA;    author  = {Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis},&#xA;    booktitle = {Annual Meeting of the Association for Computational Linguistics},&#xA;    year    = {2022},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:253157949}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{OBrien2023ContrastiveDI,&#xA;    title   = {Contrastive Decoding Improves Reasoning in Large Language Models},&#xA;    author  = {Sean O&#39;Brien and Mike Lewis},&#xA;    year    = {2023},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:261884427}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Darcet2023VisionTN,&#xA;    title   = {Vision Transformers Need Registers},&#xA;    author  = {Timoth&#39;ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},&#xA;    year    = {2023},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:263134283}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Bondarenko2023QuantizableTR,&#xA;    title   = {Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing},&#xA;    author  = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},&#xA;    journal = {ArXiv},&#xA;    year    = {2023},&#xA;    volume  = {abs/2306.12929},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:259224568}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Golkar2023xValAC,&#xA;    title   = {xVal: A Continuous Number Encoding for Large Language Models},&#xA;    author  = {Siavash Golkar and Mariel Pettee and Michael Eickenberg and Alberto Bietti and M. Cranmer and G{\&#39;e}raud Krawezik and Francois Lanusse and Michael McCabe and Ruben Ohana and Liam Parker and Bruno R{\&#39;e}galdo-Saint Blancard and Tiberiu Teşileanu and Kyunghyun Cho and Shirley Ho},&#xA;    year    = {2023},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:263622222}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Wang2022DeepNetST,&#xA;    title   = {DeepNet: Scaling Transformers to 1, 000 Layers},&#xA;    author  = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2203.00555},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:247187905}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Rafailov2023DirectPO,&#xA;    title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},&#xA;    author  = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},&#xA;    journal = {ArXiv},&#xA;    year    = {2023},&#xA;    volume  = {abs/2305.18290},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:258959321}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{xAI2024Grok,&#xA;    author = {xAI},&#xA;    title  = {Grok},&#xA;    year   = {2024},&#xA;    publisher = {GitHub},&#xA;    journal = {GitHub repository},&#xA;    howpublished = {\url{https://github.com/xai-org/grok-1}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Golovneva2024ContextualPE,&#xA;    title   = {Contextual Position Encoding: Learning to Count What&#39;s Important},&#xA;    author  = {Olga Golovneva and Tianlu Wang and Jason Weston and Sainbayar Sukhbaatar},&#xA;    year    = {2024},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:270094992}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Peebles2022ScalableDM,&#xA;    title   = {Scalable Diffusion Models with Transformers},&#xA;    author  = {William S. Peebles and Saining Xie},&#xA;    journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},&#xA;    year    = {2022},&#xA;    pages   = {4172-4182},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:254854389}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Rubin2024,&#xA;    author  = {Ohad Rubin},&#xA;    url     = {https://medium.com/@ohadrubin/exploring-weight-decay-in-layer-normalization-challenges-and-a-reparameterization-solution-ad4d12c24950}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Mesnard2024GemmaOM,&#xA;    title   = {Gemma: Open Models Based on Gemini Research and Technology},&#xA;    author  = {Gemma Team Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and L. Sifre and Morgane Riviere and Mihir Kale and J Christopher Love and Pouya Dehghani Tafti and L&#39;eonard Hussenot and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am&#39;elie H&#39;eliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl&#39;ement Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikula and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Pier Giuseppe Sessa and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vladimir Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Brian Warkentin and Ludovic Peran and Minh Giang and Cl&#39;ement Farabet and Oriol Vinyals and Jeffrey Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},&#xA;    journal = {ArXiv},&#xA;    year    = {2024},&#xA;    volume  = {abs/2403.08295},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:268379206}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;solve intelligence... then use that to solve everything else.&lt;/em&gt; - Demis Hassabis&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/torchchat</title>
    <updated>2024-08-02T01:34:58Z</updated>
    <id>tag:github.com,2024-08-02:/pytorch/torchchat</id>
    <link href="https://github.com/pytorch/torchchat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run PyTorch LLMs locally on servers, desktop and mobile&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat with LLMs Everywhere&lt;/h1&gt; &#xA;&lt;p&gt;torchchat is a small codebase showcasing the ability to run large language models (LLMs) seamlessly. With torchchat, you can run LLMs using Python, within your own (C/C++) application (desktop or server) and on iOS and Android.&lt;/p&gt; &#xA;&lt;h2&gt;What can you do with torchchat?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#running-via-pytorch--python&#34;&gt;Run models via PyTorch / Python&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#chat&#34;&gt;Chat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#generate&#34;&gt;Generate&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#browser&#34;&gt;Run chat in the Browser&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#desktopserver-execution&#34;&gt;Run models on desktop/server without python&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#aoti-aot-inductor&#34;&gt;Use AOT Inductor for faster execution&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#running-native-using-our-c-runner&#34;&gt;Running in c++ using the runner&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#mobile-execution&#34;&gt;Run models on mobile&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#deploy-and-run-on-ios&#34;&gt;Deploy and run on iOS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#deploy-and-run-on-android&#34;&gt;Deploy and run on Android&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#eval&#34;&gt;Evaluate a model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Command line interaction with popular LLMs such as Llama 3, Llama 2, Stories, Mistral and more&lt;/li&gt; &#xA; &lt;li&gt;PyTorch-native execution with performance&lt;/li&gt; &#xA; &lt;li&gt;Supports popular hardware and OS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux (x86)&lt;/li&gt; &#xA;   &lt;li&gt;Mac OS (M1/M2/M3)&lt;/li&gt; &#xA;   &lt;li&gt;Android (Devices that support XNNPACK)&lt;/li&gt; &#xA;   &lt;li&gt;iOS 17+ (iPhone 13 Pro+)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Multiple data types including: float32, float16, bfloat16&lt;/li&gt; &#xA; &lt;li&gt;Multiple quantization schemes&lt;/li&gt; &#xA; &lt;li&gt;Multiple execution modes including: Python (Eager, Compile) or Native (AOT Inductor (AOTI), ExecuTorch)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following steps require that you have &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get the code&#xA;git clone https://github.com/pytorch/torchchat.git&#xA;cd torchchat&#xA;&#xA;# set up a virtual environment&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;&#xA;# install dependencies&#xA;./install_requirements.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;p&gt;The interfaces of torchchat are leveraged through &lt;strong&gt;Python Commands&lt;/strong&gt; and &lt;strong&gt;Native Runners&lt;/strong&gt;. While the Python Commands are enumerable in the --help menu, the latter are explored in their respective sections.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Output&#xA;usage: torchchat [-h] {chat,browser,generate,export,eval,download,list,remove,where,server} ...&#xA;&#xA;positional arguments:&#xA;  {chat,browser,generate,export,eval,download,list,remove,where,server}&#xA;                        The specific command to run&#xA;    chat                Chat interactively with a model via the CLI&#xA;    generate            Generate responses from a model given a prompt&#xA;    browser             Chat interactively with a model in a locally hosted browser&#xA;    export              Export a model artifact to AOT Inductor or ExecuTorch&#xA;    download            Download model artifacts&#xA;    list                List all supported models&#xA;    remove              Remove downloaded model artifacts&#xA;    where               Return directory containing downloaded model artifacts&#xA;    server              [WIP] Starts a locally hosted REST server for model interaction&#xA;    eval                Evaluate a model via lm-eval&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Inference&lt;/strong&gt; (chat, generate, browser, server)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;These commands represent different flavors of performing model inference in a Python enviroment.&lt;/li&gt; &#xA; &lt;li&gt;Models are constructed either from CLI args or from loading exported artifacts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exporting&lt;/strong&gt; (export)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This command generates model artifacts that are consumed by Python Inference or Native Runners.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#aoti-aot-inductor&#34;&gt;AOT Inductor&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#export-for-mobile&#34;&gt;ExecuTorch&lt;/a&gt; sections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inventory Management&lt;/strong&gt; (download, list, remove, where)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;These commands are used to manage and download models.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#download-weights&#34;&gt;Download Weights&lt;/a&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt; (eval)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This command test model fidelity via EleutherAI&#39;s &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm_evaluation_harness&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#eval&#34;&gt;Evaluation&lt;/a&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download Weights&lt;/h2&gt; &#xA;&lt;p&gt;Most models use Hugging Face as the distribution channel, so you will need to create a Hugging Face account. Create a Hugging Face user access token &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;as documented here&lt;/a&gt; with the &lt;code&gt;write&lt;/code&gt; role.&lt;/p&gt; &#xA;&lt;p&gt;Log into Hugging Face:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once this is done, torchchat will be able to download model artifacts from Hugging Face.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py download llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This command may prompt you to request access to Llama 3 via Hugging Face, if you do not already have access. Simply follow the prompts and re-run the command when access is granted.*&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Additional Model Inventory Management Commands&lt;/summary&gt; &#xA; &lt;h3&gt;List&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands shows the available models&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Where&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands shows location of a particular model.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This is useful in scripts when you do not want to hard-code paths&lt;/p&gt; &#xA; &lt;h3&gt;Remove&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands removes the specified model&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py remove llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;More information about these commands can be found by adding the &lt;code&gt;--help&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Running via PyTorch / Python&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to run a model in PyTorch is via &lt;a href=&#34;https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/&#34;&gt;eager execution&lt;/a&gt;. This is the default execution mode for both PyTorch and torchchat. It performs inference without creating exporting artifacts or using a separate runner.&lt;/p&gt; &#xA;&lt;p&gt;The model used for inference can also be configured and tailored to specific needs (compilation, quantization, etc.). See the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt; for the options supported by torchchat.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] For more information about these commands, please refer to the &lt;code&gt;--help&lt;/code&gt; menu.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;This mode allows you to chat with an LLM in an interactive fashion.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py chat llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate&lt;/h3&gt; &#xA;&lt;p&gt;This mode generates text based on an input prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py generate llama3.1 --prompt &#34;write me a story about a boy and his bear&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Browser&lt;/h3&gt; &#xA;&lt;p&gt;This mode allows you to chat with the model using a UI in your browser Running the command automatically open a tab in your browser.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run torchchat.py -- browser llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Server&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This feature is still a work in progress and not all endpoints are working&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;This mode gives a REST API that matches the OpenAI API spec for interacting with a model&lt;/summary&gt; &#xA; &lt;p&gt;To test out the REST API, &lt;strong&gt;you&#39;ll need 2 terminals&lt;/strong&gt;: one to host the server, and one to send the request.&lt;/p&gt; &#xA; &lt;p&gt;In one terminal, start the server&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py server llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In another terminal, query the server using &lt;code&gt;curl&lt;/code&gt;. Depending on the model configuration, this query might take a few minutes to respond.&lt;/p&gt; &#xA; &lt;p&gt;Setting &lt;code&gt;stream&lt;/code&gt; to &#34;true&#34; in the request emits a response in chunks. Currently, this response is plaintext and will not be formatted to the OpenAI API specification. If &lt;code&gt;stream&lt;/code&gt; is unset or not &#34;true&#34;, then the client will await the full response from the server.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Example Input + Output&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;curl http://127.0.0.1:5000/chat \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;llama3.1&#34;,&#xA;    &#34;stream&#34;: &#34;true&#34;,&#xA;    &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: &#34;You are a helpful assistant.&#34;&#xA;      },&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Hello!&#34;&#xA;      }&#xA;    ]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#34;response&#34;:&#34; I&#39;m a software developer with a passion for building innovative and user-friendly applications. I have experience in developing web and mobile applications using various technologies such as Java, Python, and JavaScript. I&#39;m always looking for new challenges and opportunities to learn and grow as a developer.\n\nIn my free time, I enjoy reading books on computer science and programming, as well as experimenting with new technologies and techniques. I&#39;m also interested in machine learning and artificial intelligence, and I&#39;m always looking for ways to apply these concepts to real-world problems.\n\nI&#39;m excited to be a part of the developer community and to have the opportunity to share my knowledge and experience with others. I&#39;m always happy to help with any questions or problems you may have, and I&#39;m looking forward to learning from you as well.\n\nThank you for visiting my profile! I hope you find my information helpful and interesting. If you have any questions or would like to discuss any topics, please feel free to reach out to me. I&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Desktop/Server Execution&lt;/h2&gt; &#xA;&lt;h3&gt;AOTI (AOT Inductor)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/blog/pytorch2-2/&#34;&gt;AOTI&lt;/a&gt; compiles models before execution for faster inference. The process creates a &lt;a href=&#34;https://en.wikipedia.org/wiki/Shared_library&#34;&gt;DSO&lt;/a&gt; model (represented by a file with extension &lt;code&gt;.so&lt;/code&gt;) that is then loaded for inference. This can be done with both Python and C++ enviroments.&lt;/p&gt; &#xA;&lt;p&gt;The following example exports and executes the Llama3.1 8B Instruct model. The first command compiles and performs the actual export.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] If your machine has cuda add this flag for performance &lt;code&gt;--quantize config/data/cuda.json&lt;/code&gt; when exporting.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more details on quantization and what settings to use for your use case visit our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run in a Python Enviroment&lt;/h3&gt; &#xA;&lt;p&gt;To run in a python enviroment, use the generate subcommand like before, but include the dso file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt &#34;Hello my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Depending on which accelerator is used to generate the .dso file, the command may need the device specified: &lt;code&gt;--device (cuda | cpu)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run using our C++ Runner&lt;/h3&gt; &#xA;&lt;p&gt;To run in a C++ enviroment, we need to build the runner binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_native.sh aoti&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the compiled executable, with the exported DSO from earlier.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake-out/aoti_run exportedModels/llama3.1.so -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Depending on which accelerator is used to generate the .dso file, the runner may need the device specified: &lt;code&gt;-d (CUDA | CPU)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Mobile Execution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/executorch&#34;&gt;ExecuTorch&lt;/a&gt; enables you to optimize your model for execution on a mobile or embedded device.&lt;/p&gt; &#xA;&lt;h3&gt;Set Up ExecuTorch&lt;/h3&gt; &#xA;&lt;p&gt;Before running any commands in torchchat that require ExecuTorch, you must first install ExecuTorch.&lt;/p&gt; &#xA;&lt;p&gt;To install ExecuTorch, run the following commands. This will download the ExecuTorch repo to ./et-build/src and install various ExecuTorch libraries to ./et-build/install.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The following commands should be run from the torchchat root directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;export TORCHCHAT_ROOT=${PWD}&#xA;./scripts/install_et.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export for mobile&lt;/h3&gt; &#xA;&lt;p&gt;Similar to AOTI, to deploy onto device, we first export the PTE artifact, then we load the artifact for inference.&lt;/p&gt; &#xA;&lt;p&gt;The following example uses the Llama3.1 8B Instruct model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Export&#xA;python3 torchchat.py export llama3.1 --quantize config/data/mobile.json --output-pte-path llama3.1.pte&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We use &lt;code&gt;--quantize config/data/mobile.json&lt;/code&gt; to quantize the llama3.1 model to reduce model size and improve performance for on-device use cases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more details on quantization and what settings to use for your use case visit our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Deploy and run on Desktop&lt;/h3&gt; &#xA;&lt;p&gt;While ExecuTorch does not focus on desktop inference, it is capable of doing so. This is handy for testing out PTE models without sending them to a physical device.&lt;/p&gt; &#xA;&lt;p&gt;Specifically there are 2 ways of doing so: Pure Python and via a Runner&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying via Python&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;# Execute&#xA;python3 torchchat.py generate llama3.1 --device cpu --pte-path llama3.1.pte --prompt &#34;Hello my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying via a Runner&lt;/summary&gt; &#xA; &lt;p&gt;Build the runner&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_native.sh et&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Execute using the runner&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake-out/et_run llama3.1.pte -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy and run on iOS&lt;/h3&gt; &#xA;&lt;p&gt;The following assumes you&#39;ve completed the steps for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#set-up-executorch&#34;&gt;Setting up ExecuTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying with Xcode&lt;/summary&gt; &#xA; &lt;h4&gt;Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://apps.apple.com/us/app/xcode/id497799835?mt=12/&#34;&gt;Xcode&lt;/a&gt; 15.0 or later&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://cmake.org/download/&#34;&gt;Cmake&lt;/a&gt; 3.19 or later &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Download and open the macOS &lt;code&gt;.dmg&lt;/code&gt; installer and move the Cmake app to &lt;code&gt;/Applications&lt;/code&gt; folder.&lt;/li&gt; &#xA;    &lt;li&gt;Install Cmake command line tools: &lt;code&gt;sudo /Applications/CMake.app/Contents/bin/cmake-gui --install&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;A development provisioning profile with the &lt;a href=&#34;https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit&#34;&gt;&lt;code&gt;increased-memory-limit&lt;/code&gt;&lt;/a&gt; entitlement.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Steps&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Open the Xcode project:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;open et-build/src/executorch/examples/demo-apps/apple_ios/LLaMA/LLaMA.xcodeproj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;blockquote&gt; &#xA;    &lt;p&gt;Note: If you&#39;re running into any issues related to package dependencies, close Xcode, clean some of the caches and/or the build products, and open the Xcode project again:&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rm -rf \&#xA;  ~/Library/org.swift.swiftpm \&#xA;  ~/Library/Caches/org.swift.swiftpm \&#xA;  ~/Library/Caches/com.apple.dt.Xcode \&#xA;  ~/Library/Developer/Xcode/DerivedData&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Click the Play button to launch the app in the Simulator.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;To run on a device, ensure you have it set up for development and a provisioning profile with the &lt;code&gt;increased-memory-limit&lt;/code&gt; entitlement. Update the app&#39;s bundle identifier to match your provisioning profile with the required capability.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;After successfully launching the app, copy the exported ExecuTorch model (&lt;code&gt;.pte&lt;/code&gt;) and tokenizer (&lt;code&gt;.model&lt;/code&gt;) files to the iLLaMA folder. You can find the model file called &lt;code&gt;llama3.1.pte&lt;/code&gt; in the current &lt;code&gt;torchchat&lt;/code&gt; directory and the tokenizer file at &lt;code&gt;$(python3 torchchat.py where llama3.1)/tokenizer.model&lt;/code&gt; path.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;For the Simulator:&lt;/strong&gt; Drag and drop both files onto the Simulator window and save them in the &lt;code&gt;On My iPhone &amp;gt; iLLaMA&lt;/code&gt; folder.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;For a device:&lt;/strong&gt; Open a separate Finder window, navigate to the Files tab, drag and drop both files into the iLLaMA folder, and wait for the copying to finish.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Follow the app&#39;s UI guidelines to select the model and tokenizer files from the local filesystem and issue a prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;em&gt;Click the image below to see it in action!&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pytorch.org/executorch/main/_static/img/llama_ios_app.mp4&#34;&gt; &lt;img src=&#34;https://pytorch.org/executorch/main/_static/img/llama_ios_app.png&#34; width=&#34;600&#34; alt=&#34;iOS app running a LlaMA model&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy and run on Android&lt;/h3&gt; &#xA;&lt;p&gt;The following assumes you&#39;ve completed the steps for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#set-up-executorch&#34;&gt;Setting up ExecuTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Approach 1 (Recommended): Android Studio&lt;/summary&gt; &#xA; &lt;h4&gt;Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Android Studio&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/build/jdks&#34;&gt;Java 17&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/about/versions/14/setup-sdk&#34;&gt;Android SDK 34&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/tools/adb&#34;&gt;adb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Steps&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download the AAR file, which contains the Java library and corresponding JNI library, to build and run the app.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-tiktoken-rc3-0719.aar&#34;&gt;executorch-llama-tiktoken-rc3-0719.aar&lt;/a&gt; (SHASUM: c3e5d2a97708f033c2b1839a89f12f737e3bbbef)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Rename the downloaded AAR file to &lt;code&gt;executorch.aar&lt;/code&gt; and move the file to &lt;code&gt;android/torchchat/app/libs/&lt;/code&gt;. You may need to create directory &lt;code&gt;android/torchchat/app/libs/&lt;/code&gt; if it does not exist.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Push the model and tokenizer file to your device. You can find the model file called &lt;code&gt;llama3.1.pte&lt;/code&gt; in the current &lt;code&gt;torchchat&lt;/code&gt; directory and the tokenizer file at &lt;code&gt;$(python3 torchchat.py where llama3.1)/tokenizer.model&lt;/code&gt; path.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;adb shell mkdir -p /data/local/tmp/llama&#xA;adb push &amp;lt;model.pte&amp;gt; /data/local/tmp/llama&#xA;adb push &amp;lt;tokenizer.model or tokenizer.bin&amp;gt; /data/local/tmp/llama&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Use Android Studio to open the torchchat app skeleton, located at &lt;code&gt;android/torchchat&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Click the Play button (^R) to launch it to emulator/device.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;We recommend using a device with at least 12GB RAM and 20GB storage.&lt;/li&gt; &#xA;    &lt;li&gt;If using an emulated device, refer to &lt;a href=&#34;https://stackoverflow.com/questions/45517553/cant-change-the-ram-size-in-avd-manager-android-studio&#34;&gt;this post&lt;/a&gt; on how to set the RAM.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Follow the app&#39;s UI guidelines to pick the model and tokenizer files from the local filesystem. Then issue a prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The AAR file listed in Step 1 has the tiktoken tokenizer, which is used for Llama 3. To tweak or use a custom tokenizer and runtime, modify the ExecuTorch code and use &lt;a href=&#34;https://github.com/pytorch/executorch/raw/main/build/build_android_llm_demo.sh&#34;&gt;this script&lt;/a&gt; to build the AAR library. For convenience, we also provide an AAR for sentencepiece tokenizer (e.g. Llama 2): &lt;a href=&#34;https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-bpe-rc3-0719.aar&#34;&gt;executorch-llama-bpe-rc3-0719.aar&lt;/a&gt; (SHASUM: d5fe81d9a4700c36b50ae322e6bf34882134edb0)&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://pytorch.org/executorch/main/_static/img/android_llama_app.png&#34; width=&#34;600&#34; alt=&#34;Android app running a LlaMA model&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Approach 2: E2E Script&lt;/summary&gt; &#xA; &lt;p&gt;Alternatively, you can run &lt;code&gt;scripts/android_example.sh&lt;/code&gt; which sets up Java, Android SDK Manager, Android SDK, Android emulator (if no physical device is found), builds the app, and launches it for you. It can be used if you don&#39;t have a GUI.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;export TORCHCHAT_ROOT=$(pwd)&#xA;export USE_TIKTOKEN=ON # Set this only for tiktoken tokenizer&#xA;sh scripts/android_example.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Eval&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This feature is still a work in progress and not all features are working&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uses the lm_eval library to evaluate model accuracy on a variety of tasks. Defaults to wikitext and can be manually controlled using the tasks and limit args. See &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/evaluation.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Eager mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py eval llama3.1 --dtype fp32 --limit 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test the perplexity for a lowered or quantized model, pass it in the same way you would to generate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py eval llama3.1 --pte-path llama3.1.pte --limit 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;The following models are supported by torchchat and have associated aliases.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Mobile Friendly&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt; . Alias to &lt;code&gt;llama3.1&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B&#34;&gt;meta-llama/Meta-Llama-3.1-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama3.1-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3-8B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt; . Alias to &lt;code&gt;llama3&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;meta-llama/Meta-Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama3-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;meta-llama/Llama-2-13b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2-13b-chat&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&#34;&gt;meta-llama/Llama-2-70b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2-70b-chat&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama2-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/CodeLlama-7b-Python-hf&#34;&gt;meta-llama/CodeLlama-7b-Python-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for Python and &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;codellama&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/CodeLlama-34b-Python-hf&#34;&gt;meta-llama/CodeLlama-34b-Python-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for Python and &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;codellama-34b&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;mistral-7b-v01-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1&#34;&gt;mistralai/Mistral-7B-Instruct-v0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;mistral-7b-v01-instruct&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2&#34;&gt;mistralai/Mistral-7B-Instruct-v0.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;mistral&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories15M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories15M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories42M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories42M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories110M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories110M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b&#34;&gt;openlm-research/open_llama_7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;open-llama&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;While we describe how to use torchchat using the popular llama3 model, you can perform the example commands with any of these models.&lt;/p&gt; &#xA;&lt;h2&gt;Design Principles&lt;/h2&gt; &#xA;&lt;p&gt;torchchat embodies PyTorch’s design philosophy &lt;a href=&#34;https://pytorch.org/docs/stable/community/design.html&#34;&gt;details&lt;/a&gt;, especially &#34;usability over everything else&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Native PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;torchchat is a native-PyTorch library. While we provide integrations with the surrounding ecosystem (eg: Hugging Face models, etc), all of the core functionality is written in PyTorch.&lt;/p&gt; &#xA;&lt;h3&gt;Simplicity and Extensibility&lt;/h3&gt; &#xA;&lt;p&gt;torchchat is designed to be easy to understand, use and extend.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Composition over implementation inheritance - layers of inheritance for code re-use makes the code hard to read and extend&lt;/li&gt; &#xA; &lt;li&gt;No training frameworks - explicitly outlining the training logic makes it easy to extend for custom use cases&lt;/li&gt; &#xA; &lt;li&gt;Code duplication is preferred over unnecessary abstractions&lt;/li&gt; &#xA; &lt;li&gt;Modular building blocks over monolithic components&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Correctness&lt;/h3&gt; &#xA;&lt;p&gt;torchchat provides well-tested components with a high-bar on correctness. We provide&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extensive unit-tests to ensure things operate as they should&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We really value our community and the contributions made by our wonderful users. We&#39;ll use this section to call out some of these contributions! If you&#39;d like to help out as well, please see the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;CERTIFICATE_VERIFY_FAILED&lt;/strong&gt; Run &lt;code&gt;pip install --upgrade certifi&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access to model is restricted and you are not in the authorized list&lt;/strong&gt; Some models require an additional step to access. Follow the link provided in the error to get access.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installing ET Fails&lt;/strong&gt; If &lt;code&gt;./scripts/install_et.sh&lt;/code&gt; fails with an error like &lt;code&gt;Building wheel for executorch (pyproject.toml) did not run successfully&lt;/code&gt; It&#39;s possible that it&#39;s linking to an older version of pytorch installed some other way like via homebrew. You can break the link by uninstalling other versions such as &lt;code&gt;brew uninstall pytorch&lt;/code&gt; Note: You may break something that depends on this, so be aware.&lt;/p&gt; &#xA;&lt;h2&gt;Filing Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please include the exact command you ran and the output of that command. Also, run this script and include the output saved to &lt;code&gt;system_info.txt&lt;/code&gt; so that we can better debug your issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(echo &#34;Operating System Information&#34;; uname -a; echo &#34;&#34;; cat /etc/os-release; echo &#34;&#34;; echo &#34;Python Version&#34;; python --version || python3 --version; echo &#34;&#34;; echo &#34;PIP Version&#34;; pip --version || pip3 --version; echo &#34;&#34;; echo &#34;Installed Packages&#34;; pip freeze || pip3 freeze; echo &#34;&#34;; echo &#34;PyTorch Version&#34;; python -c &#34;import torch; print(torch.__version__)&#34; || python3 -c &#34;import torch; print(torch.__version__)&#34;; echo &#34;&#34;; echo &#34;Collection Complete&#34;) &amp;gt; system_info.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The torchchat Repository Content is provided without any guarantees about performance or compatibility. In particular, torchchat makes available model architectures written in Python for PyTorch that may not perform in the same manner or meet the same standards as the original versions of those models. When using the torchchat Repository Content, including any model architectures, you are solely responsible for determining the appropriateness of using or redistributing the torchchat Repository Content and assume any risks associated with your use of the torchchat Repository Content or any models, outputs, or results, both alone and in combination with any other technologies. Additionally, you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models, weights, data, or other technologies, and you are solely responsible for complying with all such obligations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thank you to the community for all the awesome libraries and tools you&#39;ve built around local LLM inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Georgi Gerganov and his &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; project shining a spotlight on community-based enablement and inspiring so many other projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Andrej Karpathy and his &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; project. So many great (and simple!) ideas in llama2.c that we have directly adopted (both ideas and code) from his repo. You can never go wrong by following Andrej&#39;s work.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Michael Gschwind, Bert Maher, Scott Wolchok, Bin Bao, Chen Yang, Huamin Li and Mu-Chu Li who built the first version of nanogpt (&lt;code&gt;DSOGPT&lt;/code&gt;) with AOT Inductor proving that AOTI can be used to build efficient LLMs, and DSOs are a viable distribution format for models. &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bert Maher and his &lt;a href=&#34;https://github.com/bertmaher/llama2.so&#34;&gt;llama2.so&lt;/a&gt;, which built on Andrej&#39;s llama2.c and on DSOGPT to close the loop on Llama models with AOTInductor.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Christian Puhrsch, Horace He, Joe Isaacson and many more for their many contributions in Accelerating GenAI models in the &lt;em&gt;&#34;Anything, Fast!&#34;&lt;/em&gt; pytorch.org blogs, and, in particular, Horace He for &lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;GPT, Fast!&lt;/a&gt;, which we have directly adopted (both ideas and code) from his repo.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;torchchat is released under the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/LICENSE&#34;&gt;BSD 3 license&lt;/a&gt;. (Additional code in this distribution is covered by the MIT and Apache Open Source licenses.) However you may have other legal obligations that govern your use of content, such as the terms of service for third-party models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/torchtitan</title>
    <updated>2024-08-02T01:34:58Z</updated>
    <id>tag:github.com,2024-08-02:/pytorch/torchtitan</id>
    <link href="https://github.com/pytorch/torchtitan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A native PyTorch Library for large model training&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/actions/workflows/integration_test_4gpu.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/torchtitan/actions/workflows/integration_test_4gpu.yaml/badge.svg?branch=main&#34; alt=&#34;4 GPU Integration Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml/badge.svg?branch=main&#34; alt=&#34;8 GPU Integration Test&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;torchtitan&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; is currently in a pre-release state and under extensive development.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; is a proof-of-concept for Large-scale LLM training using native PyTorch. It is (and will continue to be) a repo to showcase PyTorch&#39;s latest distributed training features in a clean, minimal codebase. torchtitan is complementary to and not a replacement for any of the great large-scale LLM training codebases such as Megatron, Megablocks, LLM Foundry, Deepspeed, etc. Instead, we hope that the features showcased in torchtitan will be adopted by these codebases quickly. torchtitan is unlikely to ever grow a large community around it.&lt;/p&gt; &#xA;&lt;p&gt;Our guiding principles when building &lt;code&gt;torchtitan&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Designed to be easy to understand, use and extend for different training purposes.&lt;/li&gt; &#xA; &lt;li&gt;Minimal changes to the model code when applying 1D, 2D, or (soon) 3D Parallel.&lt;/li&gt; &#xA; &lt;li&gt;Modular components instead of a monolithic codebase.&lt;/li&gt; &#xA; &lt;li&gt;Get started in minutes, not hours!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Intro video - learn more about torchtitan in under 4 mins:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ee5DOEqD35I?si=_B94PbVv0V5ZnNKE&#34; title=&#34;Welcome to torchtitan!&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/assets/images/titan_play_video.png&#34; alt=&#34;Welcome to torchtitan!&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dive into the code&lt;/h3&gt; &#xA;&lt;p&gt;You may want to see how the model is defined or how parallelism techniques are applied. For a guided tour, see these files first:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/train.py&#34;&gt;train.py&lt;/a&gt; - the main training loop and high-level setup code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/torchtitan/parallelisms/parallelize_llama.py&#34;&gt;torchtitan/parallelisms/parallelize_llama.py&lt;/a&gt; - helpers for applying Data / Tensor / Pipeline Parallelisms to the model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/torchtitan/checkpoint.py&#34;&gt;torchtitan/checkpoint.py&lt;/a&gt; - utils for saving/loading distributed checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/torchtitan/models/llama/model.py&#34;&gt;torchtitan/models/llama/model.py&lt;/a&gt; - the Llama model definition (shared for Llama2 and Llama3 variants)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-Release Updates:&lt;/h2&gt; &#xA;&lt;h4&gt;(4/25/2024): &lt;code&gt;torchtitan&lt;/code&gt; is now public but in a pre-release state and under development.&lt;/h4&gt; &#xA;&lt;p&gt;Currently we showcase pre-training &lt;strong&gt;Llama 3 and Llama 2&lt;/strong&gt; LLMs of various sizes from scratch. &lt;code&gt;torchtitan&lt;/code&gt; is tested and verified with the PyTorch nightly version &lt;code&gt;torch-2.4.0.dev20240412&lt;/code&gt;. (We recommend latest PyTorch nightly).&lt;/p&gt; &#xA;&lt;h3&gt;Key features available&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/fsdp.md&#34;&gt;FSDP2 with per param sharding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/distributed.tensor.parallel.html&#34;&gt;Tensor Parallel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Selective layer and operator activation checkpointing&lt;/li&gt; &#xA; &lt;li&gt;Distributed checkpointing&lt;/li&gt; &#xA; &lt;li&gt;2 datasets pre-configured (45K - 144M)&lt;/li&gt; &#xA; &lt;li&gt;GPU usage, MFU, tokens per second and more displayed via TensorBoard&lt;/li&gt; &#xA; &lt;li&gt;Learning rate scheduler, meta init, Optional Fused RMSNorm&lt;/li&gt; &#xA; &lt;li&gt;All options easily configured via &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/train_configs/&#34;&gt;toml files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/checkpoint.md&#34;&gt;Interoperable checkpoints&lt;/a&gt; which can be loaded directly into &lt;a href=&#34;https://github.com/pytorch/torchtune&#34;&gt;&lt;code&gt;torchtune&lt;/code&gt;&lt;/a&gt; for fine tuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We report our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/performance.md&#34;&gt;Performance&lt;/a&gt; verified on 64 A100 GPUs&lt;/p&gt; &#xA;&lt;h3&gt;Coming soon&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Async checkpointing&lt;/li&gt; &#xA; &lt;li&gt;FP8 support&lt;/li&gt; &#xA; &lt;li&gt;Context Parallel&lt;/li&gt; &#xA; &lt;li&gt;3D Pipeline Parallel&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;torch.compile&lt;/code&gt; support&lt;/li&gt; &#xA; &lt;li&gt;Scalable data loading solution&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/torchtitan&#xA;cd torchtitan&#xA;pip install -r requirements.txt&#xA;pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121 # or cu118&#xA;pip3 install --pre torchdata --index-url https://download.pytorch.org/whl/nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downloading a tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; currently supports training Llama 3 (8B, 70B), and Llama 2 (7B, 13B, 70B) out of the box. To get started training these models, we need to download a tokenizer.model. Follow the instructions on the official &lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;meta-llama&lt;/a&gt; repository to ensure you have access to the Llama model weights.&lt;/p&gt; &#xA;&lt;p&gt;Once you have confirmed access, you can run the following command to download the Llama 3 / Llama 2 tokenizer to your local machine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get your HF token from https://huggingface.co/settings/tokens&#xA;&#xA;# llama3 or 3.1 tokenizer.model&#xA;python torchtitan/datasets/download_tokenizer.py --repo_id meta-llama/Meta-Llama-3-8B --tokenizer_path &#34;original&#34; --hf_token=...&#xA;&#xA;# llama2 tokenizer.model&#xA;python torchtitan/datasets/download_tokenizer.py --repo_id meta-llama/Llama-2-13b-hf --hf_token=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start a training run&lt;/h3&gt; &#xA;&lt;p&gt;Llama 3 8B model locally on 8 GPUs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CONFIG_FILE=&#34;./train_configs/llama3_8b.toml&#34; ./run_llama_train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TensorBoard&lt;/h2&gt; &#xA;&lt;p&gt;To visualize TensorBoard metrics of models trained on a remote server via a local web browser:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure &lt;code&gt;metrics.enable_tensorboard&lt;/code&gt; option is set to true in model training (either from a .toml file or from CLI).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up SSH tunneling, by running the following from local CLI&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;ssh -L 6006:127.0.0.1:6006 [username]@[hostname]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Inside the SSH tunnel that logged into the remote server, go to the torchtitan repo, and start the TensorBoard backend&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensorboard --logdir=./outputs/tb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;In the local web browser, go to the URL it provides OR to &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Multi-Node Training&lt;/h2&gt; &#xA;&lt;p&gt;For training on ParallelCluster/Slurm type configurations, you can use the &lt;code&gt;multinode_trainer.slurm&lt;/code&gt; file to submit your sbatch job.&lt;/p&gt; &#xA;&lt;p&gt;To get started adjust the number of nodes and GPUs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#SBATCH --ntasks=2&#xA;#SBATCH --nodes=2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then start a run where &lt;code&gt;nnodes&lt;/code&gt; is your total node count, matching the sbatch node count above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;srun torchrun --nnodes 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your gpu count per node is not 8, adjust:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--nproc_per_node&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;in the torchrun command and&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;#SBATCH --gpus-per-task&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;in the SBATCH command section.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is made available under &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtitan/main/LICENSE&#34;&gt;BSD 3 license&lt;/a&gt;. However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models, data, etc.&lt;/p&gt;</summary>
  </entry>
</feed>