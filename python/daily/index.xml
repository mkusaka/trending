<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-22T01:36:53Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/bionemo-framework</title>
    <updated>2024-11-22T01:36:53Z</updated>
    <id>tag:github.com,2024-11-22:/NVIDIA/bionemo-framework</id>
    <link href="https://github.com/NVIDIA/bionemo-framework" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BioNeMo Framework: For building and adapting AI models in drug discovery at scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BioNeMo Framework (v2.0)&lt;/h1&gt; &#xA;&lt;p&gt;NVIDIA BioNeMo Framework is a collection of programming tools, libraries, and models for computational drug discovery. It accelerates the most time-consuming and costly stages of building and adapting biomolecular AI models by providing domain-specific, optimized models and tooling that are easily integrated into GPU-based computational resources for the fastest performance on the market. You can access BioNeMo Framework as a free community resource here in this repository or learn more at &lt;a href=&#34;https://www.nvidia.com/en-us/clara/bionemo/&#34;&gt;https://www.nvidia.com/en-us/clara/bionemo/&lt;/a&gt; about getting an enterprise license for improved expert-level support.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;bionemo2&lt;/code&gt; code is partitioned into independently installable namespace packages. These are located under the &lt;code&gt;sub-packages/&lt;/code&gt; directory. Please refer to &lt;a href=&#34;https://peps.python.org/pep-0420/&#34;&gt;PEP 420 – Implicit Namespace Packages&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and Release Information&lt;/h2&gt; &#xA;&lt;p&gt;The latest released container for the BioNeMo Framework is available for download through &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/containers/bionemo-framework&#34;&gt;NGC&lt;/a&gt;. Comprehensive documentation, including user guides, API references, and troubleshooting information, can be found in our official documentation set at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/bionemo-framework/latest/&#34;&gt;https://docs.nvidia.com/bionemo-framework/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For those interested in exploring the latest developments and features not yet included in the released container, we also maintain an up-to-date documentation set that reflects the current state of the &lt;code&gt;main&lt;/code&gt; branch. This in-progress documentation can be accessed at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/bionemo-framework/&#34;&gt;https://nvidia.github.io/bionemo-framework/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please note that while this documentation is generally accurate and helpful, it may contain references to features or APIs not yet stabilized or released. As always, we appreciate feedback on our documentation and strive to continually improve its quality.&lt;/p&gt; &#xA;&lt;h2&gt;Developing and Developer Certificate of Origin (DCO)&lt;/h2&gt; &#xA;&lt;p&gt;By contributing to this repo you acknowledge that either this is your original work, or have the right to submit the work under our license, which as of this writing is Apache v2. See &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/bionemo-framework/main/LICENSE/license.txt&#34;&gt;license&lt;/a&gt; for the current license, and the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/bionemo-framework/main/CONTRIBUTING.md&#34;&gt;contributing document&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;If you find yourself having made a number of commits in a PR, and need to sign them all, a useful tool is the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find your first unsigned commit, say it is &lt;code&gt;mYcmtShrtHash&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;git rebase --signoff mYcmtShrtHash^&lt;/code&gt; to sign that commit and all future commits (in your branch please).&lt;/li&gt; &#xA; &lt;li&gt;Push the updated commits &lt;code&gt;git push -f&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Initializing 3rd-party dependencies as git submodules&lt;/h2&gt; &#xA;&lt;p&gt;The NeMo and Megatron-LM dependencies are vendored in the bionemo-2 repository workspace as git submodules for development purposes. The pinned commits for these submodules represent the &#34;last-known-good&#34; versions of these packages that are confirmed to be working with bionemo2 (and those that are tested in CI).&lt;/p&gt; &#xA;&lt;p&gt;To initialize these sub-modules when cloning the repo, add the &lt;code&gt;--recursive&lt;/code&gt; flag to the git clone command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive git@github.com:NVIDIA/bionemo-framework.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download the pinned versions of these submodules within an existing git repository, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Different branches of the repo can have different pinned versions of these third-party submodules. Make sure you update submodules after switching branches or pulling recent changes!&lt;/p&gt; &#xA;&lt;p&gt;To configure git to automatically update submodules when switching branches, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git config submodule.recurse true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: this setting will not download &lt;strong&gt;new&lt;/strong&gt; or remove &lt;strong&gt;old&lt;/strong&gt; submodules with the branch&#39;s changes. You will have to run the full &lt;code&gt;git submodule update --init --recursive&lt;/code&gt; command in these situations.&lt;/p&gt; &#xA;&lt;h2&gt;First Time Setup&lt;/h2&gt; &#xA;&lt;p&gt;After cloning the repository, you need to run the setup script &lt;strong&gt;first&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./internal/scripts/setup_env_file.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will return an exit code of 1 on a first time run.&lt;/p&gt; &#xA;&lt;h2&gt;Release Image Building&lt;/h2&gt; &#xA;&lt;p&gt;To build the release image, run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DOCKER_BUILDKIT=1 ./ci/scripts/build_docker_image.sh \&#xA;  -regular-docker-builder \&#xA;  -image-name &#34;nvcr.io/nvidian/cvai_bnmo_trng/bionemo:bionemo2-$(git rev-parse HEAD)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development Image Building&lt;/h2&gt; &#xA;&lt;p&gt;To build the development image, run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./internal/scripts/build_dev_image.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Shell in Development Image&lt;/h2&gt; &#xA;&lt;p&gt;After building the development image, you can start a container from it and open a bash shell in it by executing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./internal/scripts/run_dev.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Downloading artifacts (For NVIDIA Employees)&lt;/h2&gt; &#xA;&lt;p&gt;Set the AWS access info in environment prior to running the dev-container launch script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;AWS_ACCESS_KEY_ID=&#34;team-bionemo&#34;&#xA;AWS_SECRET_ACCESS_KEY=$(grep aws_secret_access_key ~/.aws/config | cut -d&#39; &#39; -f 3)&#xA;AWS_REGION=&#34;us-east-1&#34;&#xA;AWS_ENDPOINT_URL=&#34;https://pbss.s8k.io&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Running tests downloads the test data to a cache location when first invoked.&lt;/p&gt; &#xA;&lt;p&gt;For more information on adding new test artifacts, see the documentation in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/bionemo-framework/main/sub-packages/bionemo-testing/src/bionemo/testing/data/README.md&#34;&gt;&lt;code&gt;bionemo.core.data.load&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updating pinned versions of NeMo / Megatron-LM&lt;/h2&gt; &#xA;&lt;p&gt;Pinned commits are bumped by depend-a-bot. To update the pinned commits of NeMo or Megatron-LM manually, checkout the commit of interest in the submodule folder, and then commit the result in the top-level bionemo repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd 3rdparty/NeMo/&#xA;git fetch&#xA;git checkout &amp;lt;desired_sha&amp;gt;&#xA;cd ../..&#xA;git add &#39;3rdparty/NeMo/&#39;&#xA;git commit -m &#34;updating NeMo commit&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing Locally&lt;/h2&gt; &#xA;&lt;p&gt;Inside the development container, run &lt;code&gt;./ci/scripts/static_checks.sh&lt;/code&gt; to validate that code changes will pass the code formatting and license checks run during CI. In addition, run the longer &lt;code&gt;./ci/scripts/pr_test.sh&lt;/code&gt; script to run unit tests for all sub-packages.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Add a new git tag&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://setuptools-scm.readthedocs.io/en/latest/&#34;&gt;setuptools-scm&lt;/a&gt; to dynamically determine the library version from git tags. As an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git tag 2.0.0a1&#xA;$ docker build . -t bionemo-uv&#xA;$ docker run --rm -it bionemo-uv:latest python -c &#34;from importlib.metadata import version; print(version(&#39;bionemo.esm2&#39;))&#34;&#xA;2.0.0a1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Bionemo packages follow &lt;a href=&#34;https://semver.org/&#34;&gt;semantic versioning 2.0&lt;/a&gt; rules: API-breaking changes are &lt;code&gt;MAJOR&lt;/code&gt;, new features are &lt;code&gt;MINOR&lt;/code&gt;, and bug-fixes and refactors are &lt;code&gt;PATCH&lt;/code&gt; in &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; version string format.&lt;/p&gt; &#xA;&lt;p&gt;If subsequent commits are added after a git tag, the version string will reflect the additional commits (e.g. &lt;code&gt;2.0.0a1.post1&lt;/code&gt;). &lt;strong&gt;NOTE&lt;/strong&gt;: we don&#39;t consider uncommitted changes in determining the version string.&lt;/p&gt; &#xA;&lt;h3&gt;Building a python wheel&lt;/h3&gt; &#xA;&lt;p&gt;An overview for publishing packages with &lt;code&gt;uv&lt;/code&gt; can be found here: &lt;a href=&#34;https://docs.astral.sh/uv/guides/publish/&#34;&gt;https://docs.astral.sh/uv/guides/publish/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build the bionemo sub-package project by executing the following for the desired package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;uv build sub-packages/bionemo-core/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Produce a wheel file for the sub-package&#39;s code and its dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls sub-packages/bionemo-core/dist/&#xA;bionemo_core-2.0.0a1.post0-py3-none-any.whl  bionemo_core-2.0.0a1.post0.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Uploading a python wheel&lt;/h3&gt; &#xA;&lt;p&gt;After building, the wheel file may be uploaded to PyPI (or a compatible package registry) by executing &lt;code&gt;uvx twine upload sub-packages/bionemo-core/dist/*&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;All steps together&lt;/h3&gt; &#xA;&lt;p&gt;Assumes we&#39;re building a wheel for &lt;code&gt;bionemo-core&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git tag MY-VERSION-TAG&#xA;uv build /sub-packages/bionemo-core&#xA;TWINE_PASSWORD=&#34;&amp;lt;pypi pass&amp;gt;&#34; TWINE_USERNAME=&#34;&amp;lt;pypi user&amp;gt;&#34; uvx twine upload /sub-packages/bionemo-core/dist/*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pydantic Configuration&lt;/h2&gt; &#xA;&lt;p&gt;BioNeMo 2 provides two entrypoints for models with both argparse and pydantic. Both documented in the &lt;code&gt;Models&lt;/code&gt; section below. Pydantic based configuration is designed to accept a configuration json file as input, along with context specific arguments (e.g., should we resume from existing checkpoints?). These JSON configs go through a Pydantic Validator, in this case referred to as &lt;code&gt;MainConfig&lt;/code&gt;. This Config is composed of several other Pydantic models, see the class definition for details. To pre-populate a config with reasonable defaults for various standard models, we provide &#39;recipes.&#39; These are simple methods that instantiate the config object and then serialize it to a JSON configuration file. From this file, you may either submit it directly, or modify the various parameters to meet your usecase. For example, Weights and biases, devices, precision, and dataset options are all extremely useful to modify. Then, you would submit this config for training.&lt;/p&gt; &#xA;&lt;p&gt;These two workflows are packaged as executables when esm2 or geneformer are installed with pip. These commands will appear as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bionemo-geneformer-recipe&#xA;bionemo-esm2-recipe&#xA;bionemo-geneformer-train&#xA;bionemo-esm2-train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;ESM-2&lt;/h3&gt; &#xA;&lt;h4&gt;Running&lt;/h4&gt; &#xA;&lt;p&gt;First off, we have a utility function for downloading full/test data and model checkpoints called &lt;code&gt;download_bionemo_data&lt;/code&gt; that our following examples currently use. This will download the object if it is not already on your local system, and then return the path either way. For example if you run this twice in a row, you should expect the second time you run it to return the path almost instantly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: NVIDIA employees should use &lt;code&gt;pbss&lt;/code&gt; rather than &lt;code&gt;ngc&lt;/code&gt; for the data source.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MY_DATA_SOURCE=&#34;ngc&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for NVIDIA internal employees with new data etc:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MY_DATA_SOURCE=&#34;pbss&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# The fastest transformer engine environment variables in testing were the following two&#xA;TEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \&#xA;ESM2_650M_CKPT=$(download_bionemo_data esm2/650m:2.0 --source $MY_DATA_SOURCE); \&#xA;&#xA;train_esm2     \&#xA;    --train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \&#xA;    --train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \&#xA;    --valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \&#xA;    --valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \&#xA;    --result-dir ./results     \&#xA;    --experiment-name test_experiment     \&#xA;    --num-gpus 1  \&#xA;    --num-nodes 1 \&#xA;    --val-check-interval 10 \&#xA;    --num-dataset-workers 1 \&#xA;    --num-steps 10 \&#xA;    --max-seq-length 1024 \&#xA;    --limit-val-batches 2 \&#xA;    --micro-batch-size 2 \&#xA;    --restore-from-checkpoint-path ${ESM2_650M_CKPT}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Running with Pydantic configs&lt;/h5&gt; &#xA;&lt;p&gt;Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. Recipes are available for 8m, 650m, and 3b ESM2 models. You may select which preset config to use by setting the &lt;code&gt;--recipe&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# The fastest transformer engine environment variables in testing were the following two&#xA;TEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \&#xA;bionemo-esm2-recipe \&#xA;--train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \&#xA;--train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \&#xA;--valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \&#xA;--valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \&#xA;--result-dir ./results     \&#xA;--dest my_config.json \&#xA;--recipe 8m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;IMPORTANT:&lt;/strong&gt; Inspect and edit the contents of the outputted my_config.json as you see fit&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: To pretrain from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the JSON with the correct field to ensure pretraining is initialized from an existing checkpoint.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To submit a training job with the passed config, first update the json file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configuration file, the JSON produced by the previous step&lt;/li&gt; &#xA; &lt;li&gt;Model config type, in this case the pretraining config. This will validate the arguments in the config JSON against those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here. This allows for mixing/matching Data Modules with various tasks.&lt;/li&gt; &#xA; &lt;li&gt;Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task, for example, pretraining ESM2 uses a protein cluster oriented sampling method. In the case of inference or fine-tuning a pretrained model, a simple fasta file may be sufficient. There is a one-to-one relationship between DataConfig types and DataModule types.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt; This setup does NO configuration of Weights and Biases. Edit your config JSON and populate it with your WandB details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;bionemo-esm2-train \&#xA;--data-config-t bionemo.esm2.run.config_models.ESM2DataConfig \&#xA;--model-config-t bionemo.esm2.run.config_models.ExposedESM2PretrainConfig \&#xA;--config my_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: both data-config-t and model-config-t have default values corresponding to ESM2DataConfig and ExposedESM2PretrainingConfig&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;DataConfigT and ModelConfigT can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.&lt;/p&gt; &#xA;&lt;h3&gt;Geneformer&lt;/h3&gt; &#xA;&lt;h4&gt;Running&lt;/h4&gt; &#xA;&lt;p&gt;Similar to ESM-2, you can download the dataset and checkpoint through our utility function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20240506 --source $MY_DATA_SOURCE); \&#xA;GENEFORMER_10M_CKPT=$(download_bionemo_data geneformer/10M_240530:2.0 --source $MY_DATA_SOURCE); \&#xA;train_geneformer     \&#xA;    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small/processed_data    \&#xA;    --result-dir ./results     \&#xA;    --restore-from-checkpoint-path ${GENEFORMER_10M_CKPT} \&#xA;    --experiment-name test_experiment     \&#xA;    --num-gpus 1  \&#xA;    --num-nodes 1 \&#xA;    --val-check-interval 10 \&#xA;    --num-dataset-workers 0 \&#xA;    --num-steps 55 \&#xA;    --seq-length 128 \&#xA;    --limit-val-batches 2 \&#xA;    --micro-batch-size 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To fine-tune, you to specify a different combination of model and loss. Pass the path to the outputted config file from the previous step as the &lt;code&gt;--restore-from-checkpoint-path&lt;/code&gt;, and also change &lt;code&gt;--training-model-config-class&lt;/code&gt; to the newly created model-config-class.&lt;/p&gt; &#xA;&lt;p&gt;While no CLI option currently exists to hot swap in different data modules and processing functions &lt;em&gt;now&lt;/em&gt;, you could copy the &lt;code&gt;sub-projects/bionemo-geneformer/geneformer/scripts/train_geneformer.py&lt;/code&gt; and modify the DataModule class that gets initialized.&lt;/p&gt; &#xA;&lt;p&gt;Simple fine-tuning example (&lt;strong&gt;NOTE&lt;/strong&gt;: please change &lt;code&gt;--restore-from-checkpoint-path&lt;/code&gt; to be the checkpoint directory path that was output last by the previous train run)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20240506 --source $MY_DATA_SOURCE); \&#xA;train_geneformer     \&#xA;    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small/processed_data    \&#xA;    --result-dir ./results     \&#xA;    --experiment-name test_finettune_experiment     \&#xA;    --num-gpus 1  \&#xA;    --num-nodes 1 \&#xA;    --val-check-interval 10 \&#xA;    --num-dataset-workers 0 \&#xA;    --num-steps 55 \&#xA;    --seq-length 128 \&#xA;    --limit-val-batches 2 \&#xA;    --micro-batch-size 2 \&#xA;    --training-model-config-class FineTuneSeqLenBioBertConfig \&#xA;    --restore-from-checkpoint-path results/test_experiment/dev/checkpoints/test_experiment--val_loss=4.3506-epoch=1-last&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Running with Pydantic configs&lt;/h5&gt; &#xA;&lt;p&gt;Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. Recipes are available for 10m, and 106m geneformer models. Additionally we provide an example recipe of finetuning, where the objective is to &#39;regress&#39; on token IDs rather than the traditional masked language model approach. In practice, you will likely need to implement your own DataModule, DataConfig, and Finetuning model. You can use the same overall approach, but with customizations for your task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20240506 --source $MY_DATA_SOURCE); \&#xA;bionemo-geneformer-recipe \&#xA;    --recipe 10m-pretrain \&#xA;    --dest my_config.json \&#xA;    --data-path ${TEST_DATA_DIR}/cellxgene_2023-12-15_small/processed_data \&#xA;    --result-dir ./results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;IMPORTANT:&lt;/strong&gt; Inspect and edit the contents of the outputted my_config.json as you see fit&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: To pretrain from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the JSON with the correct field to ensure pretraining is initialized from an existing checkpoint.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To submit a training job with the passed config, first update the json file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configuration file, the JSON produced by the previous step&lt;/li&gt; &#xA; &lt;li&gt;Model config type, in this case the pretraining config. This will validate the arguments in the config JSON against those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here. This allows for mixing/matching Data Modules with various tasks.&lt;/li&gt; &#xA; &lt;li&gt;Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task, for example, while fine-tuning you may want to use a custom Dataset/DataModule that includes PERTURB-seq. In this case, the default pretraining DataConfig and DataModule will be insufficient. See ESM2 for additional example usecases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt; This setup does NO configuration of Weights and Biases. Edit your config JSON and populate it with your WandB details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bionemo-geneformer-train \&#xA;--data-config-t bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig \&#xA;--model-config-t bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig \&#xA;--config my_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: both data-config-t and model-config-t have default values corresponding to GeneformerPretrainingDataConfig and ExposedGeneformerPretrainConfig&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;DataConfigT and ModelConfigT can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.&lt;/p&gt; &#xA;&lt;h2&gt;Updating License Header on Python Files&lt;/h2&gt; &#xA;&lt;p&gt;If you add new Python (&lt;code&gt;.py&lt;/code&gt;) files, be sure to run our license-check. If you have not already done sone, please install the dev-requirements.txt. If you are working directly inside a release container, you may need to manually install these. We recommend using the developer container for contributions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r dev-requirements.txt --user&#xA;python ./scripts/license_check.py --modify --replace --license-header ./license_header -c sub-packages/ -c docs/ -c scripts/ -c ci/ -c internal/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updating the secrets baseline file&lt;/h2&gt; &#xA;&lt;p&gt;If false-positives are raised by the &lt;a href=&#34;https://github.com/Yelp/detect-secrets&#34;&gt;detect-secrets&lt;/a&gt; pre-commit hook, they can be added to the baseline files by running the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;detect-secrets scan --baseline .secrets.baseline --exclude-files &#39;(.*\.ipynb|.*\.baseline)$&#39;&#xA;detect-secrets scan --baseline .secrets-nb.baseline --exclude-files &#39;^.(?!.*\.ipynb)&#39; --exclude-lines &#39;&#34;(hash|id|image/\w+)&#34;:.*&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting altered baseline files should then be committed.&lt;/p&gt; &#xA;&lt;h1&gt;UV-based python packaging&lt;/h1&gt; &#xA;&lt;p&gt;BioNeMo FW is migrating to use &lt;code&gt;uv&lt;/code&gt; (&lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;https://docs.astral.sh/uv/&lt;/a&gt;) for handling python packaging inside our docker containers. In addition to streamlining how we specify intra-repo dependencies, it allows us to create a uv lockfile to pin our dependencies for our bionemo docker container.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll maintain two images going forward:&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;An image that derives from &lt;code&gt;nvcr.io/nvidia/pytorch&lt;/code&gt; that will be our performance baseline. The advantage of this image base is that the performance of pytorch is validated by the NVIDIA pytorch team, but the downsides are that (1) the overall image size is quite large, and (2) using &lt;code&gt;uv sync&lt;/code&gt; to install a pinned virtual environment is not possible with the existing python environment in the ngc image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;An image that derives from &lt;code&gt;nvcr.io/nvidia/cuda&lt;/code&gt;, where we use uv to create the python environment from scratch. This image uses pytorch wheels from &lt;a href=&#34;https://download.pytorch.org&#34;&gt;https://download.pytorch.org&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Currently, the devcontainer derives from the cuda-based image above, while the release image derives from the pytorch image.&lt;/p&gt; &#xA;&lt;h2&gt;Runnings tests inside the CUDA container.&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;    -v ${HOME}/.aws:/home/bionemo/.aws \&#xA;    -v ${HOME}/.ngc:/home/bionemo/.ngc \&#xA;    -v ${PWD}:/home/bionemo/ \&#xA;    -v ${HOME}/.cache:/home/bionemo/.cache \&#xA;    -e HOST_UID=$(id -u) \&#xA;    -e HOST_GID=$(id -g) \&#xA;    --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    bionemo-uv:latest \&#xA;    py.test sub-packages/ scripts/&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>