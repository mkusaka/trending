<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-21T01:35:57Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xlang-ai/OpenAgents</title>
    <updated>2023-10-21T01:35:57Z</updated>
    <id>tag:github.com,2023-10-21:/xlang-ai/OpenAgents</id>
    <link href="https://github.com/xlang-ai/OpenAgents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAgents: An Open Platform for Language Agents in the Wild&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.10634&#34;&gt;OpenAgents: An Open Platform for Language Agents in the Wild&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;a href=&#34;https://arxiv.org/abs/2310.10634&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;OpenAgents Paper&#34; src=&#34;https://img.shields.io/badge/üìë-OpenAgents_Paper-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://chat.xlang.ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Online Demos&#34; src=&#34;https://img.shields.io/badge/ü•ë-Online_Demos-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://xlang.ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;XLangNLPLab&#34; src=&#34;https://img.shields.io/badge/üß™-XLANG_NLP_Lab-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://docs.xlang.ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;User Manual&#34; src=&#34;https://img.shields.io/badge/üìñ-User_Manual-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://opensource.org/license/apache-2-0&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;License: apache-2-0&#34; src=&#34;https://img.shields.io/github/license/saltstack/salt&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;GitHub Stars&#34; src=&#34;https://img.shields.io/github/stars/xlang-ai/OpenAgents?style=social&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/issues&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Open Issues&#34; src=&#34;https://img.shields.io/github/issues-raw/xlang-ai/OpenAgents&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://twitter.com/XLangNLP&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/twitter/follow/XLANG NLP Lab&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://join.slack.com/t/xlanggroup/shared_invite/zt-20zb8hxas-eKSGJrbzHiPmrADCDX3_rQ&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Join Slack&#34; src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://discord.gg/4Gnw7eTEZR&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://dcbadge.vercel.app/api/server/4Gnw7eTEZR?compact=true&amp;amp;style=flat&#34;&gt; &lt;/a&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/openagents_overview.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We built OpenAgents, an open platform for using and hosting language agents in the wild of everyday life.&lt;/p&gt; &#xA;&lt;p&gt;We have now implemented three agents in OpenAgents, and we host them on &lt;a href=&#34;https://chat.xlang.ai&#34;&gt;demo&lt;/a&gt; for free use!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Data Agent for data analysis with Python/SQL and data tools;&lt;/li&gt; &#xA; &lt;li&gt;Plugins Agent with 200+ daily tools;&lt;/li&gt; &#xA; &lt;li&gt;Web Agent for autonomous web browsing.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;OpenAgents can analyze data, call plugins, control your browser as ChatGPT Plus, but with OPEN Code for&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Easy deployment&lt;/li&gt; &#xA; &lt;li&gt;Full stack&lt;/li&gt; &#xA; &lt;li&gt;Chat Web UI&lt;/li&gt; &#xA; &lt;li&gt;Agent methods&lt;/li&gt; &#xA; &lt;li&gt;‚Ä¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;OpenAgents enables general users to interact with agent functionalities through a web UI optimized for swift responses and common failures, while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate both the challenges and promising opportunities, aspiring to set a foundation for future research and development of real-world language agents.&lt;/p&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023, Oct 18]&lt;/strong&gt; Try out &lt;a href=&#34;https://github.com/OpenLemur/Lemur&#34;&gt;our Lemur&lt;/a&gt;, the SOTA and open-sourced foundation models for language agents, matching ChatGPT on 15 agent tasks!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023, Oct 17]&lt;/strong&gt; Check out the OpenAgents paper &lt;a href=&#34;https://arxiv.org/abs/2310.10634&#34;&gt;here&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023, Oct 13]&lt;/strong&gt; We&#39;ve released OpenAgents platform code for all three agents, server backend and frontend! Feel free to setup your localhost one, and play with OpenAgents!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023, Aug 17]&lt;/strong&gt; Our platform has officially reached 500 users! üöÄ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023, Aug 8]&lt;/strong&gt; We&#39;ve released &lt;a href=&#34;https://chat.xlang.ai&#34;&gt;OpenAgents demos&lt;/a&gt;, including Data, Plugins, and Web agents! Check &lt;a href=&#34;https://docs.xlang.ai/category/user-manual&#34;&gt;tutorials&lt;/a&gt; and &lt;a href=&#34;https://docs.xlang.ai/category/use-cases&#34;&gt;use cases&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíª Localhost Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve released the OpenAgents platform code. Feel free to deploy on your own localhost!&lt;/p&gt; &#xA;&lt;p&gt;Here is a brief system design of OpenAgents:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/system_design.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Please check the following folders and README files to set up &amp;amp; localhost:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/backend/README.md&#34;&gt;&lt;strong&gt;backend&lt;/strong&gt;&lt;/a&gt;: the flask backend to host our three agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/frontend/README.md&#34;&gt;&lt;strong&gt;frontend&lt;/strong&gt;&lt;/a&gt;: the frontend UI and webbot chrome extension.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ü•ë OpenAgents&lt;/h2&gt; &#xA;&lt;p&gt;We built three real-world agents with chat-based web UI (check &lt;a href=&#34;https://chat.xlang.ai&#34;&gt;OpenAgents demos&lt;/a&gt;). Here is a brief overview of our OpenAgents framework. You can find more details about concepts &amp;amp; designs in our &lt;a href=&#34;https://docs.xlang.ai&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/tree/main/real_agents/data_agent&#34;&gt;Data Agent&lt;/a&gt; is a comprehensive toolkit designed for efficient data operations. It provides capabilities to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîç &lt;strong&gt;Search&lt;/strong&gt;: Quickly locate the data you need.&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Handle&lt;/strong&gt;: Streamline data acquisition and processing.&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Manipulate&lt;/strong&gt;: Modify data to suit specific requirements.&lt;/li&gt; &#xA; &lt;li&gt;üìä &lt;strong&gt;Visualize&lt;/strong&gt;: Represent data in a clear and insightful manner.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With its proficiency in writing and executing code, Data Agent simplifies a wide range of data-centric tasks. Discover its potential through various &lt;a href=&#34;https://docs.xlang.ai/use-cases/data-agent&#34;&gt;use cases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/data_agent.png&#34; width=&#34;784&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see more use case screenshots&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/data_agent_demo.png&#34; width=&#34;784&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Plugins Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/tree/main/real_agents/plugins_agent&#34;&gt;Plugins Agent&lt;/a&gt; seamlessly integrates with over 200 third-party plugins, each handpicked to enrich various facets of your daily life. With these plugins at its disposal, the agent empowers you to tackle a wide range of tasks and activities more efficiently.&lt;/p&gt; &#xA;&lt;p&gt;üîå &lt;strong&gt;Sample Plugins Include&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üõçÔ∏è &lt;strong&gt;Shopping&lt;/strong&gt;: Klarna Shopping&lt;/li&gt; &#xA; &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Weather&lt;/strong&gt;: XWeather&lt;/li&gt; &#xA; &lt;li&gt;üî¨ &lt;strong&gt;Scientific Exploration&lt;/strong&gt;: Wolfram Alpha&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Combined Plugin Usage&lt;/h4&gt; &#xA;&lt;p&gt;Harness the power of synergy! Plugins Agent supports the concurrent use of multiple plugins. Planning a trip? Seamlessly integrate functionalities from Klook, Currency converter, and WeatherViz.&lt;/p&gt; &#xA;&lt;h4&gt;Auto Plugin Selection&lt;/h4&gt; &#xA;&lt;p&gt;Simplify your choices with our &lt;strong&gt;Auto Plugin Selection&lt;/strong&gt; feature. Let the agent intuitively search and suggest the best plugins tailored to your needs.&lt;/p&gt; &#xA;&lt;p&gt;Dive into more &lt;a href=&#34;https://docs.xlang.ai/use-cases/plugins-agent&#34;&gt;use cases&lt;/a&gt; to see Plugins Agent in action.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/plugins_agent.png&#34; width=&#34;784&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see more use case screenshots&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/plugins_agent_demo.png&#34; width=&#34;784&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Web Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/tree/main/real_agents/web_agent&#34;&gt;Web Agent&lt;/a&gt; harnesses the power of a Chrome extension to navigate and explore websites automatically. This agent streamlines the web browsing experience, making it easier to find relevant information, access desired resources, and so on.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples of What Web Agent Can Do&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìç &lt;strong&gt;Google Maps Navigation&lt;/strong&gt;: Planning a journey? Simply relay your starting point and destination to Web Agent. It will navigate Google Maps for you and present the best routes.&lt;/li&gt; &#xA; &lt;li&gt;üê¶ &lt;strong&gt;Twitter Postings&lt;/strong&gt;: Engage in a conversation with Web Agent and wish to share something on Twitter? Mention the content, and Web Agent will handle your tweet effortlessly.&lt;/li&gt; &#xA; &lt;li&gt;üìù &lt;strong&gt;Google Form Assistance&lt;/strong&gt;: Need to sign up for an event or activity? Share the Google Form link and the required details. Web Agent will populate the form for you.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Witness the full potential of Web Agent in these &lt;a href=&#34;https://docs.xlang.ai/use-cases/web-agent&#34;&gt;use cases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/web_agent.png&#34; width=&#34;784&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see more use case screenshots&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/web_agent_demo.png&#34; width=&#34;784&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://docs.xlang.ai&#34;&gt;here&lt;/a&gt; for full documentation, which will be updated to stay in pace with the demo changes and the code release.&lt;/p&gt; &#xA;&lt;h2&gt;üëè Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to open-sourced communities‚Äô efforts, such as &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://github.com/mckaywrigley/chatbot-ui&#34;&gt;ChatBot UI&lt;/a&gt;, &lt;a href=&#34;https://github.com/TaxyAI/browser-extension&#34;&gt;Taxy.ai browser extension&lt;/a&gt; and others. We are able to build our interface prototype much more conveniently and efficiently.&lt;/p&gt; &#xA;&lt;p&gt;We welcome contributions and suggestions, together we move further to make it better!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üêõ Post an &lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/issues&#34;&gt;issue&lt;/a&gt; if you encounter any problems during your experience, or if you want to add any additional features.&lt;/li&gt; &#xA; &lt;li&gt;üïπ Directly contribute to our repo by creating a &lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/pulls&#34;&gt;Pull Request&lt;/a&gt;. Together we can make OpenAgents better!&lt;/li&gt; &#xA; &lt;li&gt;‚≠ê Give us a star, follow us on &lt;a href=&#34;https://twitter.com/XLangNLP&#34;&gt;Twitter&lt;/a&gt;, share your own examples, and share with your friends!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/raw/main/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üßô‚ÄçParticipants&lt;/h2&gt; &#xA;&lt;h3&gt;Tech Lead&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Impavidity&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/9245607?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Co-Lead Contributors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BlankCheng&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/34505296?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/koalazf99&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/37338733?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Timothyxxx&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/47296835?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Key Contributors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/taogoddd&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/98326623?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WhiteWolf82&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/48792453?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ztjhz&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/59118459?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Valuable Contributors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BillStark001&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/31788509?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/SivilTaram&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10275209?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/che330&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/122778503?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/leo-liuzy&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/11146950?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ranpox&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/25601999?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hongjin-su&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/114016954?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/QIN2DIM&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/62018067?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xJQx&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/47933193?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/thomasshin&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/76890354?v=4&#34; width=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements (beyond code)&lt;/h3&gt; &#xA;&lt;p&gt;Heartfelt appreciation to &lt;a href=&#34;https://www.joanna-ziyi-huang.com/&#34;&gt;Ziyi Huang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/roxyrong/&#34;&gt;Roxy Rong&lt;/a&gt;, &lt;a href=&#34;https://haotian-li.com/&#34;&gt;Haotian Li&lt;/a&gt;, &lt;a href=&#34;https://andy-xingbowang.com/&#34;&gt;Xingbo Wang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jansenwong/&#34;&gt;Jansen Wong&lt;/a&gt;, and &lt;a href=&#34;https://chenwu.io/&#34;&gt;Chen Henry Wu&lt;/a&gt; for their valuable contributions to the OpenAgents. Their expertise and insights were instrumental in bringing this project to fruition!&lt;/p&gt; &#xA;&lt;h3&gt;Open Source Contributors&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to all the contributors!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=xlang-ai/OpenAgents&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please cite us:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{OpenAgents,&#xA;      title={OpenAgents: An Open Platform for Language Agents in the Wild}, &#xA;      author={Tianbao Xie and Fan Zhou and Zhoujun Cheng and Peng Shi and Luoxuan Weng and Yitao Liu and Toh Jing Hua and Junning Zhao and Qian Liu and Che Liu and Leo Z. Liu and Yiheng Xu and Hongjin Su and Dongchan Shin and Caiming Xiong and Tao Yu},&#xA;      year={2023},&#xA;      eprint={2310.10634},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank Google Research, Amazon AWS, and Salesforce Research for their research gift funds to this open-source effort!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png&#34; width=&#34;20&#34; style=&#34;pointer-events: none;&#34;&gt; &#xA; &lt;a href=&#34;https://www.salesforceairesearch.com/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/salesforce.webp&#34; alt=&#34;Salesforce Research&#34; height=&#34;30/&#34;&gt; &lt;/a&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png&#34; width=&#34;20&#34; style=&#34;pointer-events: none;&#34;&gt; &#xA; &lt;a href=&#34;https://research.google/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/google_research.svg?sanitize=true&#34; alt=&#34;Google Research&#34; height=&#34;30/&#34;&gt; &lt;/a&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png&#34; width=&#34;25&#34; style=&#34;pointer-events: none;&#34;&gt; &#xA; &lt;a href=&#34;https://www.amazon.science/&#34; style=&#34;display: inline-block; margin-bottom: -100px;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/amazon.svg?sanitize=true&#34; alt=&#34;Amazon AWS&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xlang-ai/OpenAgents/stargazers&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=xlang-ai/OpenAgents&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; A ‚≠êÔ∏è to &lt;b&gt;OpenAgents&lt;/b&gt; is to make it shine brighter and benefit more people. &lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>neuralmagic/deepsparse</title>
    <updated>2023-10-21T01:35:57Z</updated>
    <id>tag:github.com,2023-10-21:/neuralmagic/deepsparse</id>
    <link href="https://github.com/neuralmagic/deepsparse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sparsity-aware deep learning inference runtime for CPUs&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;display: flex; flex-direction: column; align-items: center;&#34;&gt; &#xA; &lt;h1&gt; &lt;img alt=&#34;tool icon&#34; src=&#34;https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/old/source/icon-deepsparse.png&#34;&gt; &amp;nbsp;&amp;nbsp;DeepSparse &lt;/h1&gt; &#xA; &lt;h4&gt;Sparsity-aware deep learning inference runtime for CPUs&lt;/h4&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.neuralmagic.com/deepsparse/&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/badge/documentation-darkred?&amp;amp;style=for-the-badge&amp;amp;logo=read-the-docs&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ/&#34;&gt; &lt;img alt=&#34;Slack&#34; src=&#34;https://img.shields.io/badge/slack-purple?style=for-the-badge&amp;amp;logo=slack&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/issues/&#34;&gt; &lt;img alt=&#34;Support&#34; src=&#34;https://img.shields.io/badge/support%20forums-navy?style=for-the-badge&amp;amp;logo=github&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.youtube.com/channel/UCo8dO_WMGYbWCRnj_Dxr4EA&#34;&gt; &lt;img alt=&#34;YouTube&#34; src=&#34;https://img.shields.io/badge/-YouTube-red?&amp;amp;style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://twitter.com/neuralmagic&#34;&gt; &lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/neuralmagic?color=darkgreen&amp;amp;label=Follow&amp;amp;style=social&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse&#34;&gt;DeepSparse&lt;/a&gt; is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with &lt;a href=&#34;https://github.com/neuralmagic/sparseml&#34;&gt;SparseML&lt;/a&gt;, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;NM Flow&#34; src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/51e62fe7-9d9a-4fa5-a774-877158da1e29&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA; &lt;h2&gt;‚ú®NEW‚ú® DeepSparse LLMs&lt;/h2&gt; &#xA; &lt;p&gt;Neural Magic is excited to announce initial support for performant LLM inference in DeepSparse with:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;sparse kernels for speedups and memory savings from unstructured sparse weights.&lt;/li&gt; &#xA;  &lt;li&gt;8-bit weight and activation quantization support.&lt;/li&gt; &#xA;  &lt;li&gt;efficient usage of cached attention keys and values for minimal memory movement.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/ccf39323-4603-4489-8462-7b103872aeb3&#34; alt=&#34;mpt-chat-comparison&#34;&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Try It Now&lt;/h3&gt; &#xA; &lt;p&gt;Install (requires Linux):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U deepsparse-nightly[llm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run inference:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import TextGeneration&#xA;pipeline = TextGeneration(model=&#34;zoo:mpt-7b-dolly_mpt_pretrain-pruned50_quantized&#34;)&#xA;&#xA;prompt=&#34;&#34;&#34;&#xA;Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: what is sparsity? ### Response:&#xA;&#34;&#34;&#34;&#xA;print(pipeline(prompt, max_new_tokens=75).generations[0].text)&#xA;&#xA;# Sparsity is the property of a matrix or other data structure in which a large number of elements are zero and a smaller number of elements are non-zero. In the context of machine learning, sparsity can be used to improve the efficiency of training and prediction.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/docs/llms/text-generation-pipeline.md&#34;&gt;Check out the &lt;code&gt;TextGeneration&lt;/code&gt; documentation for usage details.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h3&gt;Sparsity &lt;span&gt;ü§ù&lt;/span&gt; Performance&lt;/h3&gt; &#xA; &lt;p&gt;Developed in collaboration with IST Austria, &lt;a href=&#34;https://arxiv.org/abs/2310.06927&#34;&gt;our recent paper&lt;/a&gt; details a new technique called &lt;strong&gt;Sparse Fine-Tuning&lt;/strong&gt;, which allows us to prune MPT-7B to 60% sparsity during fine-tuning without drop in accuracy. With our new support for LLMs, DeepSparse accelerates the sparse-quantized model 7x over the dense baseline:&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/8687401c-f479-4999-ba6b-e01c747dace9&#34; width=&#34;60%&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/research/mpt#sparse-finetuned-llms-with-deepsparse&#34;&gt;Learn more about our Sparse Fine-Tuning research.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/neuralmagic/sparse-mpt-7b-gsm8k&#34;&gt;Check out the model running live on Hugging Face.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h3&gt;LLM Roadmap&lt;/h3&gt; &#xA; &lt;p&gt;Following this initial launch, we are rapidly expanding our support for LLMs, including:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Productizing Sparse Fine-Tuning: Enable external users to apply sparse fine-tuning to their datasets via SparseML.&lt;/li&gt; &#xA;  &lt;li&gt;Expanding model support: Apply our sparse fine-tuning results to Llama 2 and Mistral models.&lt;/li&gt; &#xA;  &lt;li&gt;Pushing for higher sparsity: Improving our pruning algorithms to reach even higher sparsity.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h2&gt;Computer Vision and NLP Models&lt;/h2&gt; &#xA; &lt;p&gt;In addition to LLMs, DeepSparse supports many variants of CNNs and Transformer models, such as BERT, ViT, ResNet, EfficientNet, YOLOv5/8, and many more! Take a look at the &lt;a href=&#34;https://sparsezoo.neuralmagic.com/?modelSet=computer_vision&#34;&gt;Computer Vision&lt;/a&gt; and &lt;a href=&#34;https://sparsezoo.neuralmagic.com/?modelSet=natural_language_processing&#34;&gt;Natural Language Processing&lt;/a&gt; domains of &lt;a href=&#34;https://sparsezoo.neuralmagic.com/&#34;&gt;SparseZoo&lt;/a&gt;, our home for optimized models.&lt;/p&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;Install via &lt;a href=&#34;https://pypi.org/project/deepsparse/&#34;&gt;PyPI&lt;/a&gt; (&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/installation.md&#34;&gt;optional dependencies detailed here&lt;/a&gt;):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepsparse &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To experiment with the latest features, there is a nightly build available using &lt;code&gt;pip install deepsparse-nightly&lt;/code&gt; or you can clone and install from source using &lt;code&gt;pip install -e path/to/deepsparse&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;System Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Hardware: &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/hardware-support.md&#34;&gt;x86 AVX2, AVX-512, AVX-512 VNNI and ARM v8.2+&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Operating System: Linux&lt;/li&gt; &#xA;  &lt;li&gt;Python: 3.8-3.11&lt;/li&gt; &#xA;  &lt;li&gt;ONNX versions 1.5.0-1.15.0, ONNX opset version 11 or higher&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For those using Mac or Windows, we recommend using Linux containers with Docker.&lt;/p&gt; &#xA; &lt;h2&gt;Deployment APIs&lt;/h2&gt; &#xA; &lt;p&gt;DeepSparse includes three deployment APIs:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Engine&lt;/strong&gt; is the lowest-level API. With Engine, you compile an ONNX model, pass tensors as input, and receive the raw outputs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Pipeline&lt;/strong&gt; wraps the Engine with pre- and post-processing. With Pipeline, you pass raw data and receive the prediction.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt; wraps Pipelines with a REST API using FastAPI. With Server, you send raw data over HTTP and receive the prediction.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Engine&lt;/h3&gt; &#xA; &lt;p&gt;The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, compiles the model, and runs inference on randomly generated input. Users can provide their own ONNX models, whether dense or sparse.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import Engine&#xA;&#xA;# download onnx, compile&#xA;zoo_stub = &#34;zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#34;&#xA;compiled_model = Engine(model=zoo_stub, batch_size=1)&#xA;&#xA;# run inference (input is raw numpy tensors, output is raw scores)&#xA;inputs = compiled_model.generate_random_inputs()&#xA;output = compiled_model(inputs)&#xA;print(output)&#xA;&#xA;# &amp;gt; [array([[-0.3380675 ,  0.09602544]], dtype=float32)] &amp;lt;&amp;lt; raw scores&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Pipeline&lt;/h3&gt; &#xA; &lt;p&gt;Pipelines wrap Engine with pre- and post-processing, enabling you to pass raw data and receive the post-processed prediction. The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, sets up a pipeline, and runs inference on sample data.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import Pipeline&#xA;&#xA;# download onnx, set up pipeline&#xA;zoo_stub = &#34;zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#34;  &#xA;sentiment_analysis_pipeline = Pipeline.create(&#xA;  task=&#34;sentiment-analysis&#34;,    # name of the task&#xA;  model_path=zoo_stub,          # zoo stub or path to local onnx file&#xA;)&#xA;&#xA;# run inference (input is a sentence, output is the prediction)&#xA;prediction = sentiment_analysis_pipeline(&#34;I love using DeepSparse Pipelines&#34;)&#xA;print(prediction)&#xA;# &amp;gt; labels=[&#39;positive&#39;] scores=[0.9954759478569031]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Server&lt;/h3&gt; &#xA; &lt;p&gt;Server wraps Pipelines with REST APIs, enabling you to set up a model-serving endpoint running DeepSparse. This enables you to send raw data to DeepSparse over HTTP and receive the post-processed predictions. DeepSparse Server is launched from the command line and configured via arguments or a server configuration file. The following downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo and launches a sentiment analysis endpoint:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepsparse.server \&#xA;  --task sentiment-analysis \&#xA;  --model_path zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Sending a request:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;url = &#34;http://localhost:5543/v2/models/sentiment_analysis/infer&#34; # Server&#39;s port default to 5543&#xA;obj = {&#34;sequences&#34;: &#34;Snorlax loves my Tesla!&#34;}&#xA;&#xA;response = requests.post(url, json=obj)&#xA;print(response.text)&#xA;# {&#34;labels&#34;:[&#34;positive&#34;],&#34;scores&#34;:[0.9965094327926636]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Additional Resources&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases&#34;&gt;Use Cases Page&lt;/a&gt; for more details on supported tasks&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-pipelines.md&#34;&gt;Pipelines User Guide&lt;/a&gt; for Pipeline documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-server.md&#34;&gt;Server User Guide&lt;/a&gt; for Server documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-benchmarking.md&#34;&gt;Benchmarking User Guide&lt;/a&gt; for benchmarking documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/examples/&#34;&gt;Cloud Deployments and Demos&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide&#34;&gt;User Guide&lt;/a&gt; for more detailed documentation&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Product Usage Analytics&lt;/h2&gt; &#xA; &lt;p&gt;DeepSparse gathers basic usage telemetry, including, but not limited to, Invocations, Package, Version, and IP Address, for Product Usage Analytics purposes. Review Neural Magic&#39;s &lt;a href=&#34;https://neuralmagic.com/legal/&#34;&gt;Products Privacy Policy&lt;/a&gt; for further details on how we process this data.&lt;/p&gt; &#xA; &lt;p&gt;To disable Product Usage Analytics, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export NM_DISABLE_ANALYTICS=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Confirm that telemetry is shut off through info logs streamed with engine invocation by looking for the phrase &#34;Skipping Neural Magic&#39;s latest package version check.&#34;&lt;/p&gt; &#xA; &lt;h2&gt;Community&lt;/h2&gt; &#xA; &lt;h3&gt;Get In Touch&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ&#34;&gt;Community Slack&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/issues&#34;&gt;GitHub Issue Queue&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://neuralmagic.com/subscribe/&#34;&gt;Subscribe To Our Newsletter&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.neuralmagic.com/blog/&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For more general questions about Neural Magic, &lt;a href=&#34;http://neuralmagic.com/contact/&#34;&gt;complete this form.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;License&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepSparse Community&lt;/strong&gt; is licensed under the &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/LICENSE-NEURALMAGIC&#34;&gt;Neural Magic DeepSparse Community License.&lt;/a&gt; Some source code, example files, and scripts included in the DeepSparse GitHub repository or directory are licensed under the &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt; as noted.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepSparse Enterprise&lt;/strong&gt; requires a Trial License or &lt;a href=&#34;https://neuralmagic.com/legal/master-software-license-and-service-agreement/&#34;&gt;can be fully licensed&lt;/a&gt; for production, commercial applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Cite&lt;/h3&gt; &#xA; &lt;p&gt;Find this project useful in your research or other communications? Please consider citing:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{kurtic2023sparse,&#xA;      title={Sparse Fine-Tuning for Inference Acceleration of Large Language Models}, &#xA;      author={Eldar Kurtic and Denis Kuznedelev and Elias Frantar and Michael Goin and Dan Alistarh},&#xA;      year={2023},&#xA;      url={https://arxiv.org/abs/2310.06927},&#xA;      eprint={2310.06927},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&#xA;@misc{kurtic2022optimal,&#xA;      title={The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models}, &#xA;      author={Eldar Kurtic and Daniel Campos and Tuan Nguyen and Elias Frantar and Mark Kurtz and Benjamin Fineran and Michael Goin and Dan Alistarh},&#xA;      year={2022},&#xA;      url={https://arxiv.org/abs/2203.07259},&#xA;      eprint={2203.07259},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&#xA;@InProceedings{&#xA;    pmlr-v119-kurtz20a, &#xA;    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, &#xA;    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, &#xA;    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, &#xA;    pages = {5533--5543}, &#xA;    year = {2020}, &#xA;    editor = {Hal Daum√© III and Aarti Singh}, &#xA;    volume = {119}, &#xA;    series = {Proceedings of Machine Learning Research}, &#xA;    address = {Virtual}, &#xA;    month = {13--18 Jul}, &#xA;    publisher = {PMLR}, &#xA;    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},&#xA;    url = {http://proceedings.mlr.press/v119/kurtz20a.html}&#xA;}&#xA;&#xA;@article{DBLP:journals/corr/abs-2111-13445,&#xA;  author    = {Eugenia Iofinova and Alexandra Peste and Mark Kurtz and Dan Alistarh},&#xA;  title     = {How Well Do Sparse Imagenet Models Transfer?},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2111.13445},&#xA;  year      = {2021},&#xA;  url       = {https://arxiv.org/abs/2111.13445},&#xA;  eprinttype = {arXiv},&#xA;  eprint    = {2111.13445},&#xA;  timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},&#xA;  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-13445.bib},&#xA;  bibsource = {dblp computer science bibliography, https://dblp.org}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/Stable-Diffusion-WebUI-TensorRT</title>
    <updated>2023-10-21T01:35:57Z</updated>
    <id>tag:github.com,2023-10-21:/NVIDIA/Stable-Diffusion-WebUI-TensorRT</id>
    <link href="https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorRT Extension for Stable Diffusion Web UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorRT Extension for Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;This extension enables the best performance on NVIDIA RTX GPUs for Stable Diffusion with TensorRT.&lt;/p&gt; &#xA;&lt;p&gt;You need to install the extension and generate optimized engines before using the extension. Please follow the instructions below to set everything up.&lt;/p&gt; &#xA;&lt;p&gt;Supports Stable Diffusion 1.5 and 2.1. Native SDXL support coming in a future release. Please use the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&#34;&gt;dev branch&lt;/a&gt; if you would like to use it today. Note that the Dev branch is not intended for production work and may break other things that you are currently using.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Example instructions for Automatic1111:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the webui.bat&lt;/li&gt; &#xA; &lt;li&gt;Select the Extensions tab and click on Install from URL&lt;/li&gt; &#xA; &lt;li&gt;Copy the link to this repository and paste it into URL for extension&#39;s git repository&lt;/li&gt; &#xA; &lt;li&gt;Click Install&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on the ‚ÄúGenerate Default Engines‚Äù button. This step takes 2-10 minutes depending on your GPU. You can generate engines for other combinations.&lt;/li&gt; &#xA; &lt;li&gt;Go to Settings ‚Üí User Interface ‚Üí Quick Settings List, add sd_unet. Apply these settings, then reload the UI.&lt;/li&gt; &#xA; &lt;li&gt;Back in the main UI, select the TRT model from the sd_unet dropdown menu at the top of the page.&lt;/li&gt; &#xA; &lt;li&gt;You can now start generating images accelerated by TRT. If you need to create more Engines, go to the TensorRT tab.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Happy prompting!&lt;/p&gt; &#xA;&lt;h2&gt;More Information&lt;/h2&gt; &#xA;&lt;p&gt;TensorRT uses optimized engines for specific resolutions and batch sizes. You can generate as many optimized engines as desired. Types:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &#34;Export Default Engines‚Äù selection adds support for resolutions between 512x512 and 768x768 for Stable Diffusion 1.5 and 768x768 to 1024x1024 for SDXL with batch sizes 1 to 4.&lt;/li&gt; &#xA; &lt;li&gt;Static engines support a single specific output resolution and batch size.&lt;/li&gt; &#xA; &lt;li&gt;Dynamic engines support a range of resolutions and batch sizes, at a small cost in performance. Wider ranges will use more VRAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each preset can be adjusted with the ‚ÄúAdvanced Settings‚Äù option. More detailed instructions can be found &lt;a href=&#34;https://nvidia.custhelp.com/app/answers/detail/a_id/5487/~/tensorrt-extension-for-stable-diffusion-web-ui&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Common Issues/Limitations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;HIRES FIX:&lt;/strong&gt; If using the hires.fix option in Automatic1111 you must build engines that match both the starting and ending resolutions. For instance, if initial size is &lt;code&gt;512 x 512&lt;/code&gt; and hires.fix upscales to &lt;code&gt;1024 x 1024&lt;/code&gt;, you must either generate two engines, one at 512 and one at 1024, or generate a single dynamic engine that covers the whole range. Having two seperate engines will heavily impact performance at the moment. Stay tuned for updates.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resolution:&lt;/strong&gt; When generating images the resolution needs to be a multiple of 64. This applies to hires.fix as well, requiring the low and high-res to be divisible by 64.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Failing CMD arguments:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;medvram&lt;/code&gt; and &lt;code&gt;lowvram&lt;/code&gt; Have caused issues when compiling the engine and running it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api&lt;/code&gt; Has caused the &lt;code&gt;model.json&lt;/code&gt; to not be updated. Resulting in SD Unets not appearing after compilation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Failing installation or TensorRT tab not appearing in UI:&lt;/strong&gt; This is most likely due to a failed install. To resolve this manually use this &lt;a href=&#34;https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT/issues/27#issuecomment-1767570566&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Driver&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux: &amp;gt;= 450.80.02&lt;/li&gt; &#xA; &lt;li&gt;Windows: &amp;gt;=452.39&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We always recommend keeping the driver up-to-date for system wide performance improvments.&lt;/p&gt;</summary>
  </entry>
</feed>