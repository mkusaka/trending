<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-21T01:43:55Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>modelscope/modelscope</title>
    <updated>2023-03-21T01:43:55Z</updated>
    <id>tag:github.com,2023-03-21:/modelscope/modelscope</id>
    <link href="https://github.com/modelscope/modelscope" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ModelScope: bring the notion of Model-as-a-Service to life.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/modelscope/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/modelscope&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/modelscope/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/modelscope/pull/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/modelscope/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/last-commit/modelscope/modelscope&#34; alt=&#34;GitHub latest commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange&#34; alt=&#34;Leaderboard&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) --&gt; &#xA; &lt;!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) --&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/README_zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.modelscope.cn&#34;&gt;ModelScope&lt;/a&gt; is built upon the notion of ‚ÄúModel-as-a-Service‚Äù (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.&lt;/p&gt; &#xA;&lt;p&gt;In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.&lt;/p&gt; &#xA;&lt;p&gt;Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.&lt;/p&gt; &#xA;&lt;h1&gt;Models and Online Accessibility&lt;/h1&gt; &#xA;&lt;p&gt;Hundreds of models are made publicly available on &lt;a href=&#34;https://www.modelscope.cn&#34;&gt;ModelScope&lt;/a&gt; (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope(&lt;a href=&#34;http://www.modelscope.cn&#34;&gt;modelscope.cn&lt;/a&gt;) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on &lt;a href=&#34;https://www.modelscope.cn&#34;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope/master/data/resource/inference.gif&#34; width=&#34;1024&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Some representative examples include:&lt;/p&gt; &#xA;&lt;p&gt;NLP:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B&#34;&gt;nlp_gpt3_text-generation_2.7B&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/ClueAI/ChatYuan-large&#34;&gt;ChatYuan-large&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/langboat/mengzi-t5-base&#34;&gt;mengzi-t5-base&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh&#34;&gt;nlp_csanmt_translation_en2zh&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news&#34;&gt;nlp_raner_named-entity-recognition_chinese-base-news&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base&#34;&gt;nlp_structbert_word-segmentation_chinese-base&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment&#34;&gt;Erlangshen-RoBERTa-330M-Sentiment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn&#34;&gt;nlp_convai_text2sql_pretrain_cn&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Multi-Modal:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh&#34;&gt;multi-modal_clip-vit-base-patch16_zh&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/ofa_pretrain_base_zh&#34;&gt;ofa_pretrain_base_zh&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1&#34;&gt;Taiyi-Stable-Diffusion-1B-Chinese-v0.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en&#34;&gt;mplug_visual-question-answering_coco_large_en&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CV:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary&#34;&gt;cv_controlnet_controllable-image-generation_nine-annotators&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo&#34;&gt;cv_tinynas_object-detection_damoyolo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models&#34;&gt;cv_unet_person-image-cartoon_compound-models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo&#34;&gt;cv_convnextTiny_ocr-recognition-general_damo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet18_human-detection&#34;&gt;cv_resnet18_human-detection&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface&#34;&gt;cv_resnet50_face-detection_retinaface&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet_image-matting&#34;&gt;cv_unet_image-matting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_F3Net_product-segmentation&#34;&gt;cv_F3Net_product-segmentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnest101_general_recognition&#34;&gt;cv_resnest101_general_recognition&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Audio:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch&#34;&gt;speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k&#34;&gt;speech_sambert-hifigan_tts_zh-cn_16k&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun&#34;&gt;speech_charctc_kws_phone-xiaoyun&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online&#34;&gt;u2pp_conformer-asr-cn-16k-online&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k&#34;&gt;speech_frcrn_ans_cirm_16k&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k&#34;&gt;speech_dfsmn_aec_psm_16k&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;AI for Science:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/DPTech/uni-fold-monomer/summary&#34;&gt;uni-fold-monomer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://modelscope.cn/models/DPTech/uni-fold-multimer/summary&#34;&gt;uni-fold-multimer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Most models on ModelScope are public and can be downloaded without account registration on modelscope website(&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/modelscope/master/www.modelscope.cn&#34;&gt;www.modelscope.cn&lt;/a&gt;), please refer to instructions for &lt;a href=&#34;https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD&#34;&gt;model download&lt;/a&gt;, for dowloading models with api provided by modelscope library or git.&lt;/p&gt; &#xA;&lt;h1&gt;QuickTour&lt;/h1&gt; &#xA;&lt;p&gt;We provide unified interface for inference using &lt;code&gt;pipeline&lt;/code&gt;, fine-tuning and evaluation using &lt;code&gt;Trainer&lt;/code&gt; for different tasks.&lt;/p&gt; &#xA;&lt;p&gt;For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from modelscope.pipelines import pipeline&#xA;&amp;gt;&amp;gt;&amp;gt; word_segmentation = pipeline(&#39;word-segmentation&#39;,model=&#39;damo/nlp_structbert_word-segmentation_chinese-base&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; word_segmentation(&#39;‰ªäÂ§©Â§©Ê∞î‰∏çÈîôÔºåÈÄÇÂêàÂá∫ÂéªÊ∏∏Áé©&#39;)&#xA;{&#39;output&#39;: &#39;‰ªäÂ§© Â§©Ê∞î ‰∏çÈîô Ôºå ÈÄÇÂêà Âá∫Âéª Ê∏∏Áé©&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope/master/data/resource/portrait_input.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import cv2&#xA;&amp;gt;&amp;gt;&amp;gt; from modelscope.pipelines import pipeline&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; portrait_matting = pipeline(&#39;portrait-matting&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; result = portrait_matting(&#39;https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; cv2.imwrite(&#39;result.png&#39;, result[&#39;output_img&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output image with the background removed is: &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/modelscope/master/data/resource/portrait_output.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of &lt;code&gt;traner.train()&lt;/code&gt; and &lt;code&gt;trainer.evaluate()&lt;/code&gt; interfaces.&lt;/p&gt; &#xA;&lt;p&gt;For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from modelscope.metainfo import Trainers&#xA;&amp;gt;&amp;gt;&amp;gt; from modelscope.msdatasets import MsDataset&#xA;&amp;gt;&amp;gt;&amp;gt; from modelscope.trainers import build_trainer&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; train_dataset = MsDataset.load(&#39;chinese-poetry-collection&#39;, split=&#39;train&#39;). remap_columns({&#39;text1&#39;: &#39;src_txt&#39;})&#xA;&amp;gt;&amp;gt;&amp;gt; eval_dataset = MsDataset.load(&#39;chinese-poetry-collection&#39;, split=&#39;test&#39;).remap_columns({&#39;text1&#39;: &#39;src_txt&#39;})&#xA;&amp;gt;&amp;gt;&amp;gt; max_epochs = 10&#xA;&amp;gt;&amp;gt;&amp;gt; tmp_dir = &#39;./gpt3_poetry&#39;&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; kwargs = dict(&#xA;     model=&#39;damo/nlp_gpt3_text-generation_1.3B&#39;,&#xA;     train_dataset=train_dataset,&#xA;     eval_dataset=eval_dataset,&#xA;     max_epochs=max_epochs,&#xA;     work_dir=tmp_dir)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)&#xA;&amp;gt;&amp;gt;&amp;gt; trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Why should I use ModelScope library&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.&lt;/p&gt; &#xA;&lt;p&gt;To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:&lt;/p&gt; &#xA;&lt;p&gt;CPU docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GPU docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup Local Python Environment&lt;/h2&gt; &#xA;&lt;p&gt;One can also set up local ModelScope environment using pip and conda. We suggest &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;anaconda&lt;/a&gt; for creating local python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n modelscope python=3.7&#xA;conda activate modelscope&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyTorch or TensorFlow can be installed separately according to each model&#39;s requirements.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install pytorch &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;doc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install tensorflow &lt;a href=&#34;https://www.tensorflow.org/install/pip&#34;&gt;doc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After installing the necessary machine-learning framework, you can install modelscope library as follows:&lt;/p&gt; &#xA;&lt;p&gt;If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use multi-modal models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope[multi-modal]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use nlp models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use cv models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use audio models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use science models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Notes&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile(&lt;a href=&#34;https://github.com/bastibe/python-soundfile#installation&#34;&gt;doc link&lt;/a&gt;). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get update&#xA;sudo apt-get install libsndfile1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some models in computer vision need mmcv-full, you can refer to mmcv &lt;a href=&#34;https://github.com/open-mmlab/mmcv#installation&#34;&gt;installation guide&lt;/a&gt;, a minimal installation is as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip uninstall mmcv # if you have installed mmcv, uninstall it&#xA;pip install -U openmim&#xA;mim install mmcv-full&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Learn More&lt;/h1&gt; &#xA;&lt;p&gt;We provide additional documentations including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85&#34;&gt;More detailed Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D&#34;&gt;Introduction to tasks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline&#34;&gt;Use pipeline for model inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train&#34;&gt;Finetuning example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86&#34;&gt;Preprocessing of data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&#34;&gt;Contribute your own model to ModelScope&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>34j/so-vits-svc-fork</title>
    <updated>2023-03-21T01:43:55Z</updated>
    <id>tag:github.com,2023-03-21:/34j/so-vits-svc-fork</id>
    <link href="https://github.com/34j/so-vits-svc-fork" rel="alternate"></link>
    <summary type="html">&lt;p&gt;so-vits-svc fork with REALTIME support (voice changer) and greatly improved interface.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftVC VITS Singing Voice Conversion Fork&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/actions/workflows/ci.yml?query=branch%3Amain&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/34j/so-vits-svc-fork/ci.yml?branch=main&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square&#34; alt=&#34;CI Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://so-vits-svc-fork.readthedocs.io&#34;&gt; &lt;img src=&#34;https://img.shields.io/readthedocs/so-vits-svc-fork.svg?logo=read-the-docs&amp;amp;logoColor=fff&amp;amp;style=flat-square&#34; alt=&#34;Documentation Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/34j/so-vits-svc-fork&#34;&gt; &lt;img src=&#34;https://img.shields.io/codecov/c/github/34j/so-vits-svc-fork.svg?logo=codecov&amp;amp;logoColor=fff&amp;amp;style=flat-square&#34; alt=&#34;Test coverage percentage&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://python-poetry.org/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/packaging-poetry-299bd7?style=flat-square&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAASCAYAAABrXO8xAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAJJSURBVHgBfZLPa1NBEMe/s7tNXoxW1KJQKaUHkXhQvHgW6UHQQ09CBS/6V3hKc/AP8CqCrUcpmop3Cx48eDB4yEECjVQrlZb80CRN8t6OM/teagVxYZi38+Yz853dJbzoMV3MM8cJUcLMSUKIE8AzQ2PieZzFxEJOHMOgMQQ+dUgSAckNXhapU/NMhDSWLs1B24A8sO1xrN4NECkcAC9ASkiIJc6k5TRiUDPhnyMMdhKc+Zx19l6SgyeW76BEONY9exVQMzKExGKwwPsCzza7KGSSWRWEQhyEaDXp6ZHEr416ygbiKYOd7TEWvvcQIeusHYMJGhTwF9y7sGnSwaWyFAiyoxzqW0PM/RjghPxF2pWReAowTEXnDh0xgcLs8l2YQmOrj3N7ByiqEoH0cARs4u78WgAVkoEDIDoOi3AkcLOHU60RIg5wC4ZuTC7FaHKQm8Hq1fQuSOBvX/sodmNJSB5geaF5CPIkUeecdMxieoRO5jz9bheL6/tXjrwCyX/UYBUcjCaWHljx1xiX6z9xEjkYAzbGVnB8pvLmyXm9ep+W8CmsSHQQY77Zx1zboxAV0w7ybMhQmfqdmmw3nEp1I0Z+FGO6M8LZdoyZnuzzBdjISicKRnpxzI9fPb+0oYXsNdyi+d3h9bm9MWYHFtPeIZfLwzmFDKy1ai3p+PDls1Llz4yyFpferxjnyjJDSEy9CaCx5m2cJPerq6Xm34eTrZt3PqxYO1XOwDYZrFlH1fWnpU38Y9HRze3lj0vOujZcXKuuXm3jP+s3KbZVra7y2EAAAAAASUVORK5CYII=&#34; alt=&#34;Poetry&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/ambv/black&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square&#34; alt=&#34;black&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;pre-commit&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/so-vits-svc-fork/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/so-vits-svc-fork.svg?logo=python&amp;amp;logoColor=fff&amp;amp;style=flat-square&#34; alt=&#34;PyPI Version&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/so-vits-svc-fork.svg?style=flat-square&amp;amp;logo=python&amp;amp;logoColor=fff&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/l/so-vits-svc-fork.svg?style=flat-square&#34; alt=&#34;License&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;A fork of &lt;a href=&#34;https://github.com/svc-develop-team/so-vits-svc&#34;&gt;&lt;code&gt;so-vits-svc&lt;/code&gt;&lt;/a&gt; with &lt;strong&gt;realtime support&lt;/strong&gt; and &lt;strong&gt;greatly improved interface&lt;/strong&gt;. Based on branch &lt;code&gt;4.0&lt;/code&gt; (v1) and the models are compatible.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install this via pip (or your favourite package manager):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U torch torchaudio --index-url https://download.pytorch.org/whl/cu117&#xA;pip install so-vits-svc-fork&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features not available in the original repo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Realtime voice conversion&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;More accurate pitch estimation using CREPE&lt;/li&gt; &#xA; &lt;li&gt;GUI available&lt;/li&gt; &#xA; &lt;li&gt;Unified command-line interface (no need to run Python scripts)&lt;/li&gt; &#xA; &lt;li&gt;Ready to use just by installing with &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Automatically download pretrained base model and HuBERT model&lt;/li&gt; &#xA; &lt;li&gt;Code completely formatted with black, isort, autoflake etc.&lt;/li&gt; &#xA; &lt;li&gt;Volume normalization in preprocessing&lt;/li&gt; &#xA; &lt;li&gt;Other minor differences&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;h4&gt;GUI&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/docs/_static/gui.png&#34; alt=&#34;GUI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GUI launches with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;svcg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;CLI&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Realtime (from microphone)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;svc vc --model-path &amp;lt;model-path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;File&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;svc --model-path &amp;lt;model-path&amp;gt; source.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Notes&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In real-time inference, if there is noise on the inputs, the HuBERT model will react to those as well. Consider using realtime noise reduction applications such as &lt;a href=&#34;https://www.nvidia.com/en-us/geforce/guides/nvidia-rtx-voice-setup-guide/&#34;&gt;RTX Voice&lt;/a&gt; in this case.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;h4&gt;Google Colab&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/34j/so-vits-svc-fork/blob/main/notebooks/so-vits-svc-fork-4.0.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Local&lt;/h4&gt; &#xA;&lt;p&gt;Place your dataset like &lt;code&gt;dataset_raw/{speaker_id}/**/{wav_file}.{any_format}&lt;/code&gt; (subfolders are acceptable) and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;svc pre-resample&#xA;svc pre-config&#xA;svc pre-hubert&#xA;svc train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Notes&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dataset audio duration per file should be &amp;lt;~ 10s or VRAM will run out.&lt;/li&gt; &#xA; &lt;li&gt;It is recommended to change the batch_size in &lt;code&gt;config.json&lt;/code&gt; before the &lt;code&gt;train&lt;/code&gt; command to match the VRAM capacity. As tested, the default requires about 14 GB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Further help&lt;/h3&gt; &#xA;&lt;p&gt;For more details, run &lt;code&gt;svc -h&lt;/code&gt; or &lt;code&gt;svc &amp;lt;subcommand&amp;gt; -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; svc -h&#xA;Usage: svc [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;  so-vits-svc allows any folder structure for training data.&#xA;  However, the following folder structure is recommended.&#xA;      When training: dataset_raw/{speaker_name}/{wav_name}.wav&#xA;      When inference: configs/44k/config.json, logs/44k/G_XXXX.pth&#xA;  If the folder structure is followed, you DO NOT NEED TO SPECIFY model path, config path, etc.&#xA;  (The latest model will be automatically loaded.)&#xA;  To train a model, run pre-resample, pre-config, pre-hubert, train.&#xA;  To infer a model, run infer.&#xA;&#xA;Options:&#xA;  -h, --help  Show this message and exit.&#xA;&#xA;Commands:&#xA;  clean          Clean up files, only useful if you are using the default file structure&#xA;  infer          Inference&#xA;  onnx           Export model to onnx&#xA;  pre-config     Preprocessing part 2: config&#xA;  pre-hubert     Preprocessing part 3: hubert If the HuBERT model is not found, it will be...&#xA;  pre-resample   Preprocessing part 1: resample&#xA;  train          Train model If D_0.pth or G_0.pth not found, automatically download from hub.&#xA;  train-cluster  Train k-means clustering&#xA;  vc             Realtime inference from microphone&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributors ‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful people (&lt;a href=&#34;https://allcontributors.org/docs/en/emoji-key&#34;&gt;emoji key&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/34j&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/55338215?v=4?s=80&#34; width=&#34;80px;&#34; alt=&#34;34j&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;34j&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/commits?author=34j&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#ideas-34j&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/commits?author=34j&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#example-34j&#34; title=&#34;Examples&#34;&gt;üí°&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#infra-34j&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;üöá&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#maintenance-34j&#34; title=&#34;Maintenance&#34;&gt;üöß&lt;/a&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/pulls?q=is%3Apr+reviewed-by%3A34j&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/commits?author=34j&#34; title=&#34;Tests&#34;&gt;‚ö†Ô∏è&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#tutorial-34j&#34; title=&#34;Tutorials&#34;&gt;‚úÖ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#promotion-34j&#34; title=&#34;Promotion&#34;&gt;üì£&lt;/a&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/issues?q=author%3A34j&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/GarrettConway&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22782004?v=4?s=80&#34; width=&#34;80px;&#34; alt=&#34;GarrettConway&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;GarrettConway&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/commits?author=GarrettConway&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/issues?q=author%3AGarrettConway&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/BlueAmulet&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/43395286?v=4?s=80&#34; width=&#34;80px;&#34; alt=&#34;BlueAmulet&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;BlueAmulet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/34j/so-vits-svc-fork/main/#ideas-BlueAmulet&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/ThrowawayAccount01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/125531852?v=4?s=80&#34; width=&#34;80px;&#34; alt=&#34;ThrowawayAccount01&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;ThrowawayAccount01&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/34j/so-vits-svc-fork/issues?q=author%3AThrowawayAccount01&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;p&gt;This project follows the &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pointnetwork/point-alpaca</title>
    <updated>2023-03-21T01:43:55Z</updated>
    <id>tag:github.com,2023-03-21:/pointnetwork/point-alpaca</id>
    <link href="https://github.com/pointnetwork/point-alpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;point-alpaca&lt;/h1&gt; &#xA;&lt;img src=&#34;https://point-alpaca.fra1.cdn.digitaloceanspaces.com/alpaca.png&#34; height=&#34;200&#34; width=&#34;200&#34;&gt; &#xA;&lt;h2&gt;What is this?&lt;/h2&gt; &#xA;&lt;p&gt;This is released weights recreated from &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, an experiment in fine-tuning LLaMA on a synthetic instruction dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Can I try this somewhere?&lt;/h2&gt; &#xA;&lt;p&gt;Yes! Announcement thread to our frontend where you can try the 7B: &lt;a href=&#34;https://twitter.com/PointNetwork/status/1637178814210908160&#34;&gt;https://twitter.com/PointNetwork/status/1637178814210908160&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to distill the weights&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Put LLaMA weights into &lt;code&gt;original/&lt;/code&gt; folder, such that 7B version would be at &lt;code&gt;original/7B&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download point-alpaca diffs into &lt;code&gt;encrypted/&lt;/code&gt; folder:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -P encrypted/ -i filelist.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the following command to decrypt:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;for f in &#34;encrypted&#34;/*; do if [ -f &#34;$f&#34; ]; then python3 decrypt.py &#34;$f&#34; &#34;original/7B/consolidated.00.pth&#34; &#34;result/&#34;; fi; done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will have finetuned weights in the &lt;code&gt;result/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Now that you have them, you can delete the files in &lt;code&gt;encrypted/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;How to chat with the model&lt;/h2&gt; &#xA;&lt;p&gt;Other people will probably build better UIs, but for now, try running &lt;code&gt;python3 chat.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;But before that, install requirements via &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt; (We really recommend installing it in a separate environment, for example, via &lt;code&gt;conda&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Questions? Suggestions?&lt;/h2&gt; &#xA;&lt;p&gt;Find us in our Telegram chat: &lt;a href=&#34;https://t.me/pointnetworkchat&#34;&gt;https://t.me/pointnetworkchat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why are weights &#34;encrypted&#34;?&lt;/h2&gt; &#xA;&lt;p&gt;We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing &lt;em&gt;the difference&lt;/em&gt;, a patch that we suggest to apply to the files. The encryption is a simple XOR between files (not very secure - not recommended for other applications!), ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights.&lt;/p&gt; &#xA;&lt;h2&gt;What about larger models?&lt;/h2&gt; &#xA;&lt;p&gt;13B is coming for sure, larger versions - maybe. Consider supporting us if you want it done faster. :)&lt;/p&gt;</summary>
  </entry>
</feed>