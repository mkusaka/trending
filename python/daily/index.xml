<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-12T01:38:27Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openedx/edx-platform</title>
    <updated>2024-09-12T01:38:27Z</updated>
    <id>tag:github.com,2024-09-12:/openedx/edx-platform</id>
    <link href="https://github.com/openedx/edx-platform" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Open edX LMS &amp; Studio, powering education sites around the world!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Open edX Platform ################# | |License: AGPL v3| |Status| |Python CI|&lt;/p&gt; &#xA;&lt;p&gt;.. |License: AGPL v3| image:: &lt;a href=&#34;https://img.shields.io/badge/License-AGPL_v3-blue.svg&#34;&gt;https://img.shields.io/badge/License-AGPL_v3-blue.svg&lt;/a&gt; :target: &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0&#34;&gt;https://www.gnu.org/licenses/agpl-3.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Python CI| image:: &lt;a href=&#34;https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml/badge.svg&#34;&gt;https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml&#34;&gt;https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Status| image:: &lt;a href=&#34;https://img.shields.io/badge/status-maintained-31c653&#34;&gt;https://img.shields.io/badge/status-maintained-31c653&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Purpose&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The &lt;code&gt;Open edX Platform &amp;lt;https://openedx.org&amp;gt;&lt;/code&gt;_ is a service-oriented platform for authoring and delivering online learning at any scale. The platform is written in Python and JavaScript and makes extensive use of the Django framework. At the highest level, the platform is composed of a monolith, some independently deployable applications (IDAs), and micro-frontends (MFEs) based on the ReactJS.&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the monolith at the center of the Open edX platform. Functionally, the edx-platform repository provides two services:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CMS (Content Management Service), which powers Open edX Studio, the platform&#39;s learning content authoring environment; and&lt;/li&gt; &#xA; &lt;li&gt;LMS (Learning Management Service), which delivers learning content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Documentation can be found at &lt;a href=&#34;https://docs.openedx.org/projects/edx-platform&#34;&gt;https://docs.openedx.org/projects/edx-platform&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Getting Started&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;For Production&lt;/h1&gt; &#xA;&lt;p&gt;Installing and running an Open edX instance is not simple. We strongly recommend that you use a service provider to run the software for you. They have free trials that make it easy to get started: &lt;a href=&#34;https://openedx.org/get-started/&#34;&gt;https://openedx.org/get-started/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, if you have the time and expertise, then it is is possible to self-manage a production Open edX instance. To help you build, customize, upgrade, and scale your instance, we recommend using &lt;code&gt;Tutor&lt;/code&gt;_, the community-supported, Docker-based Open edX distribution.&lt;/p&gt; &#xA;&lt;p&gt;You can read more about getting up and running with a Tutor deployment at the &lt;code&gt;Site Ops home on docs.openedx.org&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;For Development&lt;/h1&gt; &#xA;&lt;p&gt;Tutor also features a &lt;code&gt;development mode&lt;/code&gt;_ which will also help you modify, test, and extend edx-platform. We recommend this method for all Open edX developers.&lt;/p&gt; &#xA;&lt;h1&gt;Bare Metal (Advanced)&lt;/h1&gt; &#xA;&lt;p&gt;It is also possible to spin up an Open edX platform directly on a Linux host. This method is less common and mostly undocumented. The Open edX community will only be able to provided limited support for it.&lt;/p&gt; &#xA;&lt;p&gt;Running &#34;bare metal&#34; is only advisable for (a) developers seeking an adventure and (b) experienced system administrators who are willing to take the complexity of Open edX configuration and deployment into their own hands.&lt;/p&gt; &#xA;&lt;h2&gt;System Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Interperters/Tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Python 3.11&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Node 18&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Services:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;MySQL 8.0&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mongo 7.x&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Memcached&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Language Packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Frontend:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;npm clean-install&lt;/code&gt; (production)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;npm clean-install --dev&lt;/code&gt; (development)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Backend build:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements/edx/assets.txt&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Backend application:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements/edx/base.txt&lt;/code&gt; (production)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r requirements/edx/dev.txt&lt;/code&gt; (development)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Steps&lt;/h2&gt; &#xA;&lt;p&gt;Create two MySQL databases and a MySQL user with write permissions to both, and configure Django to use them by updating the &lt;code&gt;DATABASES&lt;/code&gt; setting.&lt;/p&gt; &#xA;&lt;p&gt;Then, run migrations::&lt;/p&gt; &#xA;&lt;p&gt;./manage.py lms migrate ./manage.py lms migrate --database=student_module_history ./manage.py cms migrate&lt;/p&gt; &#xA;&lt;p&gt;Build static assets (for more details, see &lt;code&gt;building static assets&lt;/code&gt;_)::&lt;/p&gt; &#xA;&lt;p&gt;npm run build # or, &#39;build-dev&#39;&lt;/p&gt; &#xA;&lt;p&gt;Download locales and collect static assets (can be skipped for development sites)::&lt;/p&gt; &#xA;&lt;p&gt;make pull_translations ./manage.py lms collectstatic ./manage.py cms collectstatic&lt;/p&gt; &#xA;&lt;h2&gt;Run the Platform&lt;/h2&gt; &#xA;&lt;p&gt;First, ensure MySQL, Mongo, and Memcached are running.&lt;/p&gt; &#xA;&lt;p&gt;Start the LMS::&lt;/p&gt; &#xA;&lt;p&gt;./manage.py lms runserver&lt;/p&gt; &#xA;&lt;p&gt;Start the CMS::&lt;/p&gt; &#xA;&lt;p&gt;./manage.py cms runserver&lt;/p&gt; &#xA;&lt;p&gt;This will give you a mostly-headless Open edX platform. Most frontends have been migrated to &#34;Micro-Frontends (MFEs)&#34; which need to be installed and run separately. At a bare minimum, you will need to run the &lt;code&gt;Authentication MFE&lt;/code&gt;&lt;em&gt;, &lt;code&gt;Learner Home MFE&lt;/code&gt;&lt;/em&gt;, and &lt;code&gt;Learning MFE&lt;/code&gt;_ in order meaningfully navigate the UI.&lt;/p&gt; &#xA;&lt;p&gt;.. _Tutor: &lt;a href=&#34;https://github.com/overhangio/tutor&#34;&gt;https://github.com/overhangio/tutor&lt;/a&gt; .. _Site Ops home on docs.openedx.org: &lt;a href=&#34;https://docs.openedx.org/en/latest/site_ops/index.html&#34;&gt;https://docs.openedx.org/en/latest/site_ops/index.html&lt;/a&gt; .. _development mode: &lt;a href=&#34;https://docs.tutor.edly.io/dev.html&#34;&gt;https://docs.tutor.edly.io/dev.html&lt;/a&gt; .. _building static assets: ./docs/references/static-assets.rst .. _Authentication MFE: &lt;a href=&#34;https://github.com/openedx/frontend-app-authn/&#34;&gt;https://github.com/openedx/frontend-app-authn/&lt;/a&gt; .. _Learner Home MFE: &lt;a href=&#34;https://github.com/openedx/frontend-app-learner-dashboard&#34;&gt;https://github.com/openedx/frontend-app-learner-dashboard&lt;/a&gt; .. _Learning MFE: &lt;a href=&#34;https://github.com/openedx/frontend-app-learning/&#34;&gt;https://github.com/openedx/frontend-app-learning/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;License&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The code in this repository is licensed under version 3 of the AGPL unless otherwise noted. Please see the &lt;code&gt;LICENSE&lt;/code&gt;_ file for details.&lt;/p&gt; &#xA;&lt;p&gt;.. _LICENSE: &lt;a href=&#34;https://github.com/openedx/edx-platform/raw/master/LICENSE&#34;&gt;https://github.com/openedx/edx-platform/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;More about Open edX&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;See the &lt;code&gt;Open edX site&lt;/code&gt;_ to learn more about the Open edX world. You can find information about hosting, extending, and contributing to Open edX software. In addition, the Open edX site provides product announcements, the Open edX blog, and other rich community resources.&lt;/p&gt; &#xA;&lt;p&gt;.. _Open edX site: &lt;a href=&#34;https://openedx.org&#34;&gt;https://openedx.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Getting Help&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you&#39;re having trouble, we have discussion forums at &lt;a href=&#34;https://discuss.openedx.org&#34;&gt;https://discuss.openedx.org&lt;/a&gt; where you can connect with others in the community.&lt;/p&gt; &#xA;&lt;p&gt;Our real-time conversations are on Slack. You can request a &lt;code&gt;Slack invitation&lt;/code&gt;&lt;em&gt;, then join our &lt;code&gt;community Slack team&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information about these options, see the &lt;code&gt;Getting Help&lt;/code&gt;_ page.&lt;/p&gt; &#xA;&lt;p&gt;.. _Slack invitation: &lt;a href=&#34;https://openedx.org/slack&#34;&gt;https://openedx.org/slack&lt;/a&gt; .. _community Slack team: &lt;a href=&#34;http://openedx.slack.com/&#34;&gt;http://openedx.slack.com/&lt;/a&gt; .. _Getting Help: &lt;a href=&#34;https://openedx.org/getting-help&#34;&gt;https://openedx.org/getting-help&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Issue Tracker&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;We use Github Issues for our issue tracker. You can search &lt;code&gt;previously reported issues&lt;/code&gt;&lt;em&gt;. If you need to report a bug, or want to discuss a new feature before you implement it, please &lt;code&gt;create a new issue&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. _previously reported issues: &lt;a href=&#34;https://github.com/openedx/edx-platform/issues&#34;&gt;https://github.com/openedx/edx-platform/issues&lt;/a&gt; .. _create a new issue: &lt;a href=&#34;https://github.com/openedx/edx-platform/issues/new/choose&#34;&gt;https://github.com/openedx/edx-platform/issues/new/choose&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;How to Contribute&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Contributions are welcome! The first step is to submit a signed &lt;code&gt;individual contributor agreement&lt;/code&gt;&lt;em&gt;. See our &lt;code&gt;CONTRIBUTING&lt;/code&gt;&lt;/em&gt; file for more information – it also contains guidelines for how to maintain high code quality, which will make your contribution more likely to be accepted.&lt;/p&gt; &#xA;&lt;p&gt;New features are accepted. Discussing your new ideas with the maintainers before you write code will also increase the chances that your work is accepted.&lt;/p&gt; &#xA;&lt;p&gt;Code of Conduct&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Please read the &lt;code&gt;Community Code of Conduct&lt;/code&gt;_ for interacting with this repository.&lt;/p&gt; &#xA;&lt;p&gt;Reporting Security Issues&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Please do not report security issues in public. Please email &lt;a href=&#34;mailto:security@openedx.org&#34;&gt;security@openedx.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. _individual contributor agreement: &lt;a href=&#34;https://openedx.org/cla&#34;&gt;https://openedx.org/cla&lt;/a&gt; .. _CONTRIBUTING: &lt;a href=&#34;https://github.com/openedx/.github/raw/master/CONTRIBUTING.md&#34;&gt;https://github.com/openedx/.github/blob/master/CONTRIBUTING.md&lt;/a&gt; .. _Community Code of Conduct: &lt;a href=&#34;https://openedx.org/code-of-conduct/&#34;&gt;https://openedx.org/code-of-conduct/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;People&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The current maintainers of this repository can be found on &lt;code&gt;Backstage&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. _Backstage: &lt;a href=&#34;https://backstage.openedx.org/catalog/default/component/edx-platform&#34;&gt;https://backstage.openedx.org/catalog/default/component/edx-platform&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DataDog/integrations-core</title>
    <updated>2024-09-12T01:38:27Z</updated>
    <id>tag:github.com,2024-09-12:/DataDog/integrations-core</id>
    <link href="https://github.com/DataDog/integrations-core" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Core integrations of the Datadog Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Datadog Integrations - Core&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CI/CD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DataDog/integrations-core/actions/workflows/master.yml&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DataDog/integrations-core/badges/test-results.svg?sanitize=true&#34; alt=&#34;CI - Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/DataDog/integrations-core?branch=master&#34;&gt;&lt;img src=&#34;https://codecov.io/github/DataDog/integrations-core/coverage.svg?branch=master&#34; alt=&#34;CI - Coverage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DataDog/integrations-core/actions?workflow=docs&#34;&gt;&lt;img src=&#34;https://github.com/DataDog/integrations-core/workflows/docs/badge.svg?sanitize=true&#34; alt=&#34;Docs - Release&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pypa/hatch&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg?sanitize=true&#34; alt=&#34;Hatch project&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/charliermarsh/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v0.json&#34; alt=&#34;Linting - Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ambv/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style - black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/python/mypy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/typing-Mypy-blue.svg?sanitize=true&#34; alt=&#34;Typing - Mypy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://spdx.org/licenses/BSD-3-Clause.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD--3--Clause-9400d3.svg?sanitize=true&#34; alt=&#34;License - BSD-3-Clause&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This repository contains open source integrations that Datadog officially develops and supports. To add a new integration, please see the &lt;a href=&#34;https://github.com/DataDog/integrations-extras&#34;&gt;Integrations Extras&lt;/a&gt; repository and the &lt;a href=&#34;https://docs.datadoghq.com/developers/integrations/&#34;&gt;accompanying documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/DataDog/datadog-agent&#34;&gt;Datadog Agent&lt;/a&gt; packages are equipped with all the Agent integrations from this repository, so to get started using them, you can simply &lt;a href=&#34;https://app.datadoghq.com/account/settings/agent/latest&#34;&gt;install the Agent&lt;/a&gt; for your operating system. The &lt;a href=&#34;https://raw.githubusercontent.com/DataDog/integrations-core/master/AGENT_CHANGELOG.md&#34;&gt;AGENT_CHANGELOG&lt;/a&gt; file shows which Integrations have been updated in each Agent version.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Working with integrations is easy, the main page of the &lt;a href=&#34;https://docs.datadoghq.com/developers/integrations/&#34;&gt;development docs&lt;/a&gt; contains all the info you need to get your dev environment up and running in minutes to run, test and build a Check. More advanced documentation can be found &lt;a href=&#34;https://datadoghq.dev/integrations-core/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting Issues&lt;/h2&gt; &#xA;&lt;p&gt;For more information on integrations, please reference our &lt;a href=&#34;https://docs.datadoghq.com&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://help.datadoghq.com/hc/en-us&#34;&gt;knowledge base&lt;/a&gt;. You can also visit our &lt;a href=&#34;https://docs.datadoghq.com/help/&#34;&gt;help page&lt;/a&gt; to connect with us.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>run-llama/llama_deploy</title>
    <updated>2024-09-12T01:38:27Z</updated>
    <id>tag:github.com,2024-09-12:/run-llama/llama_deploy</id>
    <link href="https://github.com/run-llama/llama_deploy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙 &lt;code&gt;llama_deploy&lt;/code&gt; 🤖&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;llama_deploy&lt;/code&gt; (formerly &lt;code&gt;llama-agents&lt;/code&gt;) is an async-first framework for deploying, scaling, and productionizing agentic multi-service systems based on &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/understanding/workflows/&#34;&gt;workflows from &lt;code&gt;llama_index&lt;/code&gt;&lt;/a&gt;. With &lt;code&gt;llama_deploy&lt;/code&gt;, you can build any number of workflows in &lt;code&gt;llama_index&lt;/code&gt; and then bring them into &lt;code&gt;llama_deploy&lt;/code&gt; for deployment.&lt;/p&gt; &#xA;&lt;p&gt;In &lt;code&gt;llama_deploy&lt;/code&gt;, each workflow is seen as a &lt;code&gt;service&lt;/code&gt;, endlessly processing incoming tasks. Each workflow pulls and publishes messages to and from a &lt;code&gt;message queue&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;At the top of a &lt;code&gt;llama_deploy&lt;/code&gt; system is the &lt;code&gt;control plane&lt;/code&gt;. The control plane handles ongoing tasks, manages state, keeps track of which services are in the network, and also decides which service should handle the next step of a task using an &lt;code&gt;orchestrator&lt;/code&gt;. The default &lt;code&gt;orchestrator&lt;/code&gt; is purely programmatic, handling failures, retries, and state-passing.&lt;/p&gt; &#xA;&lt;p&gt;The overall system layout is pictured below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/run-llama/llama_deploy/main/system_diagram.png&#34; alt=&#34;A basic system in llama_deploy&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why &lt;code&gt;llama_deploy&lt;/code&gt;?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Seamless Deployment&lt;/strong&gt;: It bridges the gap between development and production, allowing you to deploy &lt;code&gt;llama_index&lt;/code&gt; workflows with minimal changes to your code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The microservices architecture enables easy scaling of individual components as your system grows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: By using a hub-and-spoke architecture, you can easily swap out components (like message queues) or add new services without disrupting the entire system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt;: With built-in retry mechanisms and failure handling, &lt;code&gt;llama_deploy&lt;/code&gt; ensures robustness in production environments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State Management&lt;/strong&gt;: The control plane manages state across services, simplifying complex multi-step processes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Async-First&lt;/strong&gt;: Designed for high-concurrency scenarios, making it suitable for real-time and high-throughput applications.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Wait, where is &lt;code&gt;llama-agents&lt;/code&gt;?&lt;/h2&gt; &#xA;&lt;p&gt;The introduction of &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/module_guides/workflow/#workflows&#34;&gt;Workflows&lt;/a&gt; in &lt;code&gt;llama_index&lt;/code&gt;produced the most intuitive way to develop agentic applications. The question then became: how can we close the gap between developing an agentic application as a workflow, and deploying it?&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;llama_deploy&lt;/code&gt;, the goal is to make it as 1:1 as possible between something that you built in a notebook, and something running on the cloud in a cluster. &lt;code&gt;llama_deploy&lt;/code&gt; enables this by simply being able to pass in and deploy any workflow.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;llama_deploy&lt;/code&gt; can be installed with pip, and relies mainly on &lt;code&gt;llama_index_core&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llama_deploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;High-Level Deployment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;llama_deploy&lt;/code&gt; provides a simple way to deploy your workflows using configuration objects and helper functions.&lt;/p&gt; &#xA;&lt;p&gt;When deploying, generally you&#39;ll want to deploy the core services and workflows each from their own python scripts (or docker images, etc.).&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how you can deploy a core system and a workflow:&lt;/p&gt; &#xA;&lt;h3&gt;Deploying the Core System&lt;/h3&gt; &#xA;&lt;p&gt;To deploy the core system (message queue, control plane, and orchestrator), you can use the &lt;code&gt;deploy_core&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_deploy import (&#xA;    deploy_core,&#xA;    ControlPlaneConfig,&#xA;    SimpleMessageQueueConfig,&#xA;)&#xA;&#xA;&#xA;async def main():&#xA;    await deploy_core(&#xA;        control_plane_config=ControlPlaneConfig(),&#xA;        message_queue_config=SimpleMessageQueueConfig(),&#xA;    )&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    import asyncio&#xA;&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will set up the basic infrastructure for your &lt;code&gt;llama_deploy&lt;/code&gt; system. You can customize the configs to adjust ports and basic settings, as well as swap in different message queue configs (Redis, Kafka, RabbiMQ, etc.).&lt;/p&gt; &#xA;&lt;h3&gt;Deploying a Workflow&lt;/h3&gt; &#xA;&lt;p&gt;To deploy a workflow as a service, you can use the &lt;code&gt;deploy_workflow&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_deploy import (&#xA;    deploy_workflow,&#xA;    WorkflowServiceConfig,&#xA;    ControlPlaneConfig,&#xA;    SimpleMessageQueueConfig,&#xA;)&#xA;from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step&#xA;&#xA;&#xA;# create a dummy workflow&#xA;class MyWorkflow(Workflow):&#xA;    @step()&#xA;    async def run_step(self, ev: StartEvent) -&amp;gt; StopEvent:&#xA;        # Your workflow logic here&#xA;        arg1 = str(ev.get(&#34;arg1&#34;, &#34;&#34;))&#xA;        result = arg1 + &#34;_result&#34;&#xA;        return StopEvent(result=result)&#xA;&#xA;&#xA;async def main():&#xA;    await deploy_workflow(&#xA;        workflow=MyWorkflow(),&#xA;        workflow_config=WorkflowServiceConfig(&#xA;            host=&#34;127.0.0.1&#34;, port=8002, service_name=&#34;my_workflow&#34;&#xA;        ),&#xA;        control_plane_config=ControlPlaneConfig(),&#xA;    )&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    import asyncio&#xA;&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will deploy your workflow as a service within the &lt;code&gt;llama_deploy&lt;/code&gt; system, and register the service with the existing control plane and message queue.&lt;/p&gt; &#xA;&lt;h3&gt;Interacting with your Deployment&lt;/h3&gt; &#xA;&lt;p&gt;Once deployed, you can interact with your deployment using a client.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_deploy import LlamaDeployClient, ControlPlaneConfig&#xA;&#xA;# points to deployed control plane&#xA;client = LlamaDeployClient(ControlPlaneConfig())&#xA;&#xA;session = client.create_session()&#xA;result = session.run(&#34;my_workflow&#34;, arg1=&#34;hello_world&#34;)&#xA;print(result)&#xA;# prints &#39;hello_world_result&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploying Nested Workflows&lt;/h3&gt; &#xA;&lt;p&gt;Every &lt;code&gt;Workflow&lt;/code&gt; is capable of injecting and running nested workflows. For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step&#xA;&#xA;&#xA;class InnerWorkflow(Workflow):&#xA;    @step()&#xA;    async def run_step(self, ev: StartEvent) -&amp;gt; StopEvent:&#xA;        arg1 = ev.get(&#34;arg1&#34;)&#xA;        if not arg1:&#xA;            raise ValueError(&#34;arg1 is required.&#34;)&#xA;&#xA;        return StopEvent(result=str(arg1) + &#34;_result&#34;)&#xA;&#xA;&#xA;class OuterWorkflow(Workflow):&#xA;    @step()&#xA;    async def run_step(&#xA;        self, ev: StartEvent, inner: InnerWorkflow&#xA;    ) -&amp;gt; StopEvent:&#xA;        arg1 = ev.get(&#34;arg1&#34;)&#xA;        if not arg1:&#xA;            raise ValueError(&#34;arg1 is required.&#34;)&#xA;&#xA;        arg1 = await inner.run(arg1=arg1)&#xA;&#xA;        return StopEvent(result=str(arg1) + &#34;_result&#34;)&#xA;&#xA;&#xA;inner = InnerWorkflow()&#xA;outer = OuterWorkflow()&#xA;outer.add_workflows(inner=InnerWorkflow())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;llama_deploy&lt;/code&gt; makes it dead simple to spin up each workflow above as a service, and run everything without any changes to your code!&lt;/p&gt; &#xA;&lt;p&gt;Just deploy each workflow:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This code is launching both workflows from the same script, but these could easily be separate scripts, machines, or docker containers!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from llama_deploy import (&#xA;    WorkflowServiceConfig,&#xA;    ControlPlaneConfig,&#xA;    deploy_workflow,&#xA;)&#xA;&#xA;&#xA;async def main():&#xA;    inner_task = asyncio.create_task(&#xA;        deploy_workflow(&#xA;            inner,&#xA;            WorkflowServiceConfig(&#xA;                host=&#34;127.0.0.1&#34;, port=8003, service_name=&#34;inner&#34;&#xA;            ),&#xA;            ControlPlaneConfig(),&#xA;        )&#xA;    )&#xA;&#xA;    outer_task = asyncio.create_task(&#xA;        deploy_workflow(&#xA;            outer,&#xA;            WorkflowServiceConfig(&#xA;                host=&#34;127.0.0.1&#34;, port=8002, service_name=&#34;outer&#34;&#xA;            ),&#xA;            ControlPlaneConfig(),&#xA;        )&#xA;    )&#xA;&#xA;    await asyncio.gather(inner_task, outer_task)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    import asyncio&#xA;&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then use it as before:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_deploy import LlamaDeployClient&#xA;&#xA;# points to deployed control plane&#xA;client = LlamaDeployClient(ControlPlaneConfig())&#xA;&#xA;session = client.create_session()&#xA;result = session.run(&#34;outer&#34;, arg1=&#34;hello_world&#34;)&#xA;print(result)&#xA;# prints &#39;hello_world_result_result&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Components of a &lt;code&gt;llama_deploy&lt;/code&gt; System&lt;/h2&gt; &#xA;&lt;p&gt;In &lt;code&gt;llama_deploy&lt;/code&gt;, there are several key components that make up the overall system&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;message queue&lt;/code&gt; -- the message queue acts as a queue for all services and the &lt;code&gt;control plane&lt;/code&gt;. It has methods for publishing methods to named queues, and delegates messages to consumers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control plane&lt;/code&gt; -- the control plane is a the central gateway to the &lt;code&gt;llama_deploy&lt;/code&gt; system. It keeps track of current tasks and the services that are registered to the system. The &lt;code&gt;control plane&lt;/code&gt; also performs state and session management and utilizes the &lt;code&gt;orchestrator&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;orchestrator&lt;/code&gt; -- The module handles incoming tasks and decides what service to send it to, as well as how to handle results from services. By default, the &lt;code&gt;orchestrator&lt;/code&gt; is very simple, and assumes incoming tasks have a destination already specified. Beyond that, the default &lt;code&gt;orchestrator&lt;/code&gt; handles retries, failures, and other nice-to-haves.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;services&lt;/code&gt; -- Services are where the actual work happens. A services accepts some incoming task and context, processes it, and publishes a result. When you deploy a workflow, it becomes a service.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Low-Level Deployment&lt;/h2&gt; &#xA;&lt;p&gt;For more control over the deployment process, you can use the lower-level API. Here&#39;s what&#39;s happening under the hood when you use &lt;code&gt;deploy_core&lt;/code&gt; and &lt;code&gt;deploy_workflow&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;deploy_core&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy_core&lt;/code&gt; function sets up the message queue, control plane, and orchestrator. Here&#39;s what it does:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def deploy_core(&#xA;    control_plane_config: ControlPlaneConfig,&#xA;    message_queue_config: BaseSettings,&#xA;    orchestrator_config: Optional[SimpleOrchestratorConfig] = None,&#xA;) -&amp;gt; None:&#xA;    orchestrator_config = orchestrator_config or SimpleOrchestratorConfig()&#xA;&#xA;    message_queue_client = _get_message_queue_client(message_queue_config)&#xA;&#xA;    control_plane = ControlPlaneServer(&#xA;        message_queue_client,&#xA;        SimpleOrchestrator(**orchestrator_config.model_dump()),&#xA;        **control_plane_config.model_dump(),&#xA;    )&#xA;&#xA;    message_queue_task = None&#xA;    if isinstance(message_queue_config, SimpleMessageQueueConfig):&#xA;        message_queue_task = _deploy_local_message_queue(message_queue_config)&#xA;&#xA;    control_plane_task = asyncio.create_task(control_plane.launch_server())&#xA;&#xA;    # let services spin up&#xA;    await asyncio.sleep(1)&#xA;&#xA;    # register the control plane as a consumer&#xA;    control_plane_consumer_fn = await control_plane.register_to_message_queue()&#xA;&#xA;    consumer_task = asyncio.create_task(control_plane_consumer_fn())&#xA;&#xA;    # let things sync up&#xA;    await asyncio.sleep(1)&#xA;&#xA;    # let things run&#xA;    if message_queue_task:&#xA;        all_tasks = [control_plane_task, consumer_task, message_queue_task]&#xA;    else:&#xA;        all_tasks = [control_plane_task, consumer_task]&#xA;&#xA;    shutdown_handler = _get_shutdown_handler(all_tasks)&#xA;    loop = asyncio.get_event_loop()&#xA;    while loop.is_running():&#xA;        await asyncio.sleep(0.1)&#xA;        signal.signal(signal.SIGINT, shutdown_handler)&#xA;&#xA;        for task in all_tasks:&#xA;            if task.done() and task.exception():  # type: ignore&#xA;                raise task.exception()  # type: ignore&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This function:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sets up the message queue client&lt;/li&gt; &#xA; &lt;li&gt;Creates the control plane server&lt;/li&gt; &#xA; &lt;li&gt;Launches the message queue (if using SimpleMessageQueue)&lt;/li&gt; &#xA; &lt;li&gt;Launches the control plane server&lt;/li&gt; &#xA; &lt;li&gt;Registers the control plane as a consumer&lt;/li&gt; &#xA; &lt;li&gt;Sets up a shutdown handler and keeps the event loop running&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;deploy_workflow&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy_workflow&lt;/code&gt; function deploys a workflow as a service. Here&#39;s what it does:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def deploy_workflow(&#xA;    workflow: Workflow,&#xA;    workflow_config: WorkflowServiceConfig,&#xA;    control_plane_config: ControlPlaneConfig,&#xA;) -&amp;gt; None:&#xA;    control_plane_url = control_plane_config.url&#xA;&#xA;    async with httpx.AsyncClient() as client:&#xA;        response = await client.get(f&#34;{control_plane_url}/queue_config&#34;)&#xA;        queue_config_dict = response.json()&#xA;&#xA;    message_queue_config = _get_message_queue_config(queue_config_dict)&#xA;    message_queue_client = _get_message_queue_client(message_queue_config)&#xA;&#xA;    service = WorkflowService(&#xA;        workflow=workflow,&#xA;        message_queue=message_queue_client,&#xA;        **workflow_config.model_dump(),&#xA;    )&#xA;&#xA;    service_task = asyncio.create_task(service.launch_server())&#xA;&#xA;    # let service spin up&#xA;    await asyncio.sleep(1)&#xA;&#xA;    # register to message queue&#xA;    consumer_fn = await service.register_to_message_queue()&#xA;&#xA;    # register to control plane&#xA;    control_plane_url = (&#xA;        f&#34;http://{control_plane_config.host}:{control_plane_config.port}&#34;&#xA;    )&#xA;    await service.register_to_control_plane(control_plane_url)&#xA;&#xA;    # create consumer task&#xA;    consumer_task = asyncio.create_task(consumer_fn())&#xA;&#xA;    # let things sync up&#xA;    await asyncio.sleep(1)&#xA;&#xA;    all_tasks = [consumer_task, service_task]&#xA;&#xA;    shutdown_handler = _get_shutdown_handler(all_tasks)&#xA;    loop = asyncio.get_event_loop()&#xA;    while loop.is_running():&#xA;        await asyncio.sleep(0.1)&#xA;        signal.signal(signal.SIGINT, shutdown_handler)&#xA;&#xA;        for task in all_tasks:&#xA;            if task.done() and task.exception():  # type: ignore&#xA;                raise task.exception()  # type: ignore&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This function:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sets up the message queue client&lt;/li&gt; &#xA; &lt;li&gt;Creates a WorkflowService with the provided workflow&lt;/li&gt; &#xA; &lt;li&gt;Launches the service server&lt;/li&gt; &#xA; &lt;li&gt;Registers the service to the message queue&lt;/li&gt; &#xA; &lt;li&gt;Registers the service to the control plane&lt;/li&gt; &#xA; &lt;li&gt;Sets up a consumer task for the service&lt;/li&gt; &#xA; &lt;li&gt;Sets up a shutdown handler and keeps the event loop running&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Using the &lt;code&gt;llama_deploy&lt;/code&gt; client&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;llama_deploy&lt;/code&gt; provides both a synchronous and an asynchronous client for interacting with a deployed system.&lt;/p&gt; &#xA;&lt;p&gt;Both clients have the same interface, but the asynchronous client is recommended for production use to enable concurrent operations.&lt;/p&gt; &#xA;&lt;p&gt;Generally, there is a top-level client for interacting with the control plane, and a session client for interacting with a specific session. The session client is created automatically for you by the top-level client and returned from specific methods.&lt;/p&gt; &#xA;&lt;p&gt;To create a client, you need to point it to a control plane.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_deploy import (&#xA;    LlamaDeployClient,&#xA;    AsyncLlamaDeployClient,&#xA;    ControlPlaneConfig,&#xA;)&#xA;&#xA;client = LlamaDeployClient(ControlPlaneConfig())&#xA;async_client = AsyncLlamaDeployClient(ControlPlaneConfig())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Client Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.create_session(poll_interval=DEFAULT_POLL_INTERVAL)&lt;/code&gt;: Creates a new session for running workflows and returns a SessionClient for it. A session encapsulates the context and state for a single workflow run. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session = client.create_session()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.list_sessions()&lt;/code&gt;: Lists all sessions registered with the control plane. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sessions = client.list_sessions()&#xA;for session in sessions:&#xA;    print(session.session_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.get_session(session_id, poll_interval=DEFAULT_POLL_INTERVAL)&lt;/code&gt;: Gets an existing session by ID and returns a SessionClient for it. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session = client.get_session(&#34;session_123&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.get_or_create_session(session_id, poll_interval=DEFAULT_POLL_INTERVAL)&lt;/code&gt;: Gets an existing session by ID, or creates a new one if it doesn&#39;t exist. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session = client.get_or_create_session(&#34;session_123&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.get_service(service_name)&lt;/code&gt;: Gets the definition of a service by name. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;service = client.get_service(&#34;my_workflow&#34;)&#xA;print(service.service_name, service.host, service.port)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.delete_session(session_id)&lt;/code&gt;: Deletes a session by ID. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.delete_session(&#34;session_123&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.list_services()&lt;/code&gt;: Lists all services registered with the control plane. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;services = client.list_services()&#xA;for service in services:&#xA;    print(service.service_name)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.register_service(service_def)&lt;/code&gt;: Registers a service with the control plane. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;service_def = ServiceDefinition(&#xA;    service_name=&#34;my_workflow&#34;, host=&#34;localhost&#34;, port=8000&#xA;)&#xA;client.register_service(service_def)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;client.deregister_service(service_name)&lt;/code&gt;: Deregisters a service from the control plane. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.deregister_service(&#34;my_workflow&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SessionClient Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;session.run(service_name, **run_kwargs)&lt;/code&gt;: Implements the workflow-based run API for a session. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = session.run(&#34;my_workflow&#34;, arg1=&#34;hello&#34;, arg2=&#34;world&#34;)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;session.create_task(task_def)&lt;/code&gt;: Creates a new task in the session. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;task_def = TaskDefinition(input=&#39;{&#34;arg1&#34;: &#34;hello&#34;}&#39;, agent_id=&#34;my_workflow&#34;)&#xA;task_id = session.create_task(task_def)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;session.get_tasks()&lt;/code&gt;: Gets all tasks in the session. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tasks = session.get_tasks()&#xA;for task in tasks:&#xA;    print(task.task_id, task.status)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;session.get_current_task()&lt;/code&gt;: Gets the current (most recent) task in the session. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;current_task = session.get_current_task()&#xA;if current_task:&#xA;    print(current_task.task_id, current_task.status)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;session.get_task_result(task_id)&lt;/code&gt;: Gets the result of a task in the session if it has one. Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = session.get_task_result(&#34;task_123&#34;)&#xA;if result:&#xA;    print(result.result)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Message Queue Integrations&lt;/h3&gt; &#xA;&lt;p&gt;In addition to &lt;code&gt;SimpleMessageQueue&lt;/code&gt;, we provide integrations for various message queue providers, such as RabbitMQ, Redis, etc. The general usage pattern for any of these message queues is the same as that for &lt;code&gt;SimpleMessageQueue&lt;/code&gt;, however the appropriate extra would need to be installed along with &lt;code&gt;llama-deploy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, for &lt;code&gt;RabbitMQMessageQueue&lt;/code&gt;, we need to install the &#34;rabbitmq&#34; extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# using pip install&#xA;pip install llama-agents[rabbitmq]&#xA;&#xA;# using poetry&#xA;poetry add llama-agents -E &#34;rabbitmq&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using the &lt;code&gt;RabbitMQMessageQueue&lt;/code&gt; is then done as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_agents.message_queue.rabbitmq import (&#xA;    RabbitMQMessageQueueConfig,&#xA;    RabbitMQMessageQueue,&#xA;)&#xA;&#xA;message_queue_config = (&#xA;    RabbitMQMessageQueueConfig()&#xA;)  # loads params from environment vars&#xA;message_queue = RabbitMQMessageQueue(**message_queue_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;code&gt;RabbitMQMessageQueueConfig&lt;/code&gt; can load its params from environment variables.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- prettier-ignore-end --&gt;</summary>
  </entry>
</feed>