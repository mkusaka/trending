<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-01T01:34:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>neuraloperator/neuraloperator</title>
    <updated>2025-01-01T01:34:39Z</updated>
    <id>tag:github.com,2025-01-01:/neuraloperator/neuraloperator</id>
    <link href="https://github.com/neuraloperator/neuraloperator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learning in infinite dimension with neural operators.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/pypi/v/neuraloperator&#34;&gt;https://img.shields.io/pypi/v/neuraloperator&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/neuraloperator/&#34;&gt;https://pypi.org/project/neuraloperator/&lt;/a&gt; :alt: PyPI&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/NeuralOperator/neuraloperator/actions/workflows/test.yml/badge.svg&#34;&gt;https://github.com/NeuralOperator/neuraloperator/actions/workflows/test.yml/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NeuralOperator/neuraloperator/actions/workflows/test.yml&#34;&gt;https://github.com/NeuralOperator/neuraloperator/actions/workflows/test.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;=============================================== NeuralOperator: Learning in Infinite Dimensions&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;neuraloperator&lt;/code&gt; is a comprehensive library for learning neural operators in PyTorch. It is the official implementation for Fourier Neural Operators and Tensorized Neural Operators.&lt;/p&gt; &#xA;&lt;p&gt;Unlike regular neural networks, neural operators enable learning mapping between function spaces, and this library provides all of the tools to do so on your own data.&lt;/p&gt; &#xA;&lt;p&gt;Neural operators are also resolution invariant, so your trained operator can be applied on data of any resolution.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Just clone the repository and install locally (in editable mode so changes in the code are immediately reflected without having to reinstall):&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;git clone &lt;a href=&#34;https://github.com/NeuralOperator/neuraloperator&#34;&gt;https://github.com/NeuralOperator/neuraloperator&lt;/a&gt; cd neuraloperator pip install -e . pip install -r requirements.txt&lt;/p&gt; &#xA;&lt;p&gt;You can also just pip install the most recent stable release of the library on &lt;code&gt;PyPI &amp;lt;https://pypi.org/project/neuraloperator/&amp;gt;&lt;/code&gt;_:&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;pip install neuraloperator&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;After you&#39;ve installed the library, you can start training operators seamlessly:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;p&gt;from neuralop.models import FNO&lt;/p&gt; &#xA;&lt;p&gt;operator = FNO(n_modes=(16, 16), hidden_channels=64, in_channels=3, out_channels=1)&lt;/p&gt; &#xA;&lt;p&gt;Tensorization is also provided out of the box: you can improve the previous models by simply using a Tucker Tensorized FNO with just a few parameters:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;p&gt;from neuralop.models import TFNO&lt;/p&gt; &#xA;&lt;p&gt;operator = TFNO(n_modes=(16, 16), hidden_channels=64, in_channels=3, out_channels=1, factorization=&#39;tucker&#39;, implementation=&#39;factorized&#39;, rank=0.05)&lt;/p&gt; &#xA;&lt;p&gt;This will use a Tucker factorization of the weights. The forward pass will be efficient by contracting directly the inputs with the factors of the decomposition. The Fourier layers will have 5% of the parameters of an equivalent, dense Fourier Neural Operator!&lt;/p&gt; &#xA;&lt;p&gt;Checkout the &lt;code&gt;documentation &amp;lt;https://neuraloperator.github.io/dev/index.html&amp;gt;&lt;/code&gt;_ for more!&lt;/p&gt; &#xA;&lt;h2&gt;Using with weights and biases&lt;/h2&gt; &#xA;&lt;p&gt;Create a file in &lt;code&gt;neuraloperator/config&lt;/code&gt; called &lt;code&gt;wandb_api_key.txt&lt;/code&gt; and paste your Weights and Biases API key there. You can configure the project you want to use and your username in the main yaml configuration files.&lt;/p&gt; &#xA;&lt;h1&gt;=============== Contributing&lt;/h1&gt; &#xA;&lt;p&gt;NeuralOperator is 100% open-source, and we welcome all contributions from the community! If you spot a bug or a typo in the documentation, or have an idea for a feature you&#39;d like to see, please report it on our &lt;code&gt;issue tracker &amp;lt;https://github.com/neuraloperator/neuraloperator/issues&amp;gt;&lt;/code&gt;&lt;em&gt;, or even better, open a Pull-Request on &lt;code&gt;GitHub &amp;lt;https://github.com/neuraloperator/neuraloperator&amp;gt;&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;NeuralOperator has additional dependencies for development, which can be found in &lt;code&gt;requirements_dev.txt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;pip install -r requirements_dev.txt&lt;/p&gt; &#xA;&lt;h2&gt;Code formatting&lt;/h2&gt; &#xA;&lt;p&gt;Before you submit your changes, you should make sure your code adheres to our style-guide. The easiest way to do this is with &lt;code&gt;black&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;black .&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing and documentation are an essential part of this package and all functions come with unit-tests and documentation. The tests are run using the pytest package.&lt;/p&gt; &#xA;&lt;p&gt;To run the tests, simply run, in the terminal:&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -v neuralop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building documentation&lt;/h2&gt; &#xA;&lt;p&gt;The HTML for our documentation website is built using &lt;code&gt;sphinx&lt;/code&gt;. The documentation is built from inside the &lt;code&gt;doc&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;cd doc make html&lt;/p&gt; &#xA;&lt;p&gt;This will build the docs in &lt;code&gt;./doc/build/html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that the documentation requires other dependencies installable from &lt;code&gt;./doc/requirements_doc.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To view the documentation locally, run:&lt;/p&gt; &#xA;&lt;p&gt;.. code::&lt;/p&gt; &#xA;&lt;p&gt;cd doc/build/html python -m http.server [PORT_NUM]&lt;/p&gt; &#xA;&lt;p&gt;The docs will then be viewable at &lt;code&gt;localhost:PORT_NUM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you use NeuralOperator in an academic paper, please cite [1]&lt;em&gt;, [2]&lt;/em&gt;::&lt;/p&gt; &#xA;&lt;p&gt;@misc{kossaifi2024neural, title={A Library for Learning Neural Operators}, author={Jean Kossaifi and Nikola Kovachki and Zongyi Li and Davit Pitt and Miguel Liu-Schiaffini and Robert Joseph George and Boris Bonev and Kamyar Azizzadenesheli and Julius Berner and Anima Anandkumar}, year={2024}, eprint={2412.10354}, archivePrefix={arXiv}, primaryClass={cs.LG} }&lt;/p&gt; &#xA;&lt;p&gt;@article{kovachki2021neural, author = {Nikola B. Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew M. Stuart and Anima Anandkumar}, title = {Neural Operator: Learning Maps Between Function Spaces}, journal = {CoRR}, volume = {abs/2108.08481}, year = {2021}, }&lt;/p&gt; &#xA;&lt;p&gt;.. [1] Kossaifi, J., Kovachki, N., Li, Z., Pitt, D., Liu-Schiaffini, M., George, R., Bonev, B., Azizzadenesheli, K., Berner, J., and Anandkumar, A., &#34;A Library for Learning Neural Operators&#34;, ArXiV, 2024. doi:10.48550/arXiv.2412.10354.&lt;/p&gt; &#xA;&lt;p&gt;.. [2] Kovachki, N., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A., and Anandkumar A., “Neural Operator: Learning Maps Between Function Spaces”, JMLR, 2021. doi:10.48550/arXiv.2108.08481.&lt;/p&gt;</summary>
  </entry>
</feed>