<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-10T01:36:25Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kijai/ComfyUI-CogVideoXWrapper</title>
    <updated>2024-12-10T01:36:25Z</updated>
    <id>tag:github.com,2024-12-10:/kijai/ComfyUI-CogVideoXWrapper</id>
    <link href="https://github.com/kijai/ComfyUI-CogVideoXWrapper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WORK IN PROGRESS&lt;/h1&gt; &#xA;&lt;p&gt;Spreadsheet (WIP) of supported models and their supported features: &lt;a href=&#34;https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?usp=sharing&#34;&gt;https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;BREAKING Update8&lt;/h2&gt; &#xA;&lt;p&gt;This is big one, and unfortunately to do the necessary cleanup and refactoring this will break every old workflow as they are. I apologize for the inconvenience, if I don&#39;t do this now I&#39;ll keep making it worse until maintaining becomes too much of a chore, so from my pov there was no choice.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please either use the new workflows or fix the nodes in your old ones before posting issue reports!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Old version will be kept in a legacy branch, but not maintained&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Support CogVideoX 1.5 models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Major code cleanup (it was bad, still isn&#39;t great, wip)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Merge Fun -model functionality into main pipeline:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;All Fun specific nodes, besides image encode node for Fun -InP models are gone&lt;/li&gt; &#xA;   &lt;li&gt;Main CogVideo Sampler works with Fun models&lt;/li&gt; &#xA;   &lt;li&gt;DimensionX LoRAs now work with Fun models as well&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Remove width/height from the sampler widgets and detect from input instead, this meanst text2vid now requires using empty latents&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Separate VAE from the model, allow using fp32 VAE&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add ability to load some of the non-GGUF models as single files (only few available for now: &lt;a href=&#34;https://huggingface.co/Kijai/CogVideoX-comfy&#34;&gt;https://huggingface.co/Kijai/CogVideoX-comfy&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add some torchao quantizations as options&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add interpolation as option for the main encode node, old interpolation specific node is gone&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;torch.compile optimizations&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Remove PAB in favor of FasterCache and cleaner code&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;other smaller things I forgot about at this point&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For Fun -model based workflows it&#39;s more drastic change, for others migrating generally means re-setting many of the nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Update7&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactored the Fun version&#39;s sampler to accept any resolution, this should make it lot simpler to use with Tora. &lt;strong&gt;BREAKS OLD WORKFLOWS&lt;/strong&gt;, old FunSampler nodes need to be remade.&lt;/li&gt; &#xA; &lt;li&gt;The old bucket resizing is now on it&#39;s own node (CogVideoXFunResizeToClosestBucket) to keep the functionality, I honestly don&#39;t know if it matters at all, but just in case.&lt;/li&gt; &#xA; &lt;li&gt;Fun version&#39;s vid2vid is now also in the same node, the old vid2vid node is deprecated.&lt;/li&gt; &#xA; &lt;li&gt;Added support for FasterCache, this trades more VRAM use for speed with slight quality hit, similar to PAB: &lt;a href=&#34;https://github.com/Vchitect/FasterCache&#34;&gt;https://github.com/Vchitect/FasterCache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved torch.compile support, it actually works now&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update6&lt;/h2&gt; &#xA;&lt;p&gt;Initial support for Tora (&lt;a href=&#34;https://github.com/alibaba/Tora&#34;&gt;https://github.com/alibaba/Tora&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Converted model (included in the autodownload node):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/CogVideoX-5b-Tora/tree/main&#34;&gt;https://huggingface.co/Kijai/CogVideoX-5b-Tora/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d5334237-03dc-48f5-8bec-3ae5998660c6&#34;&gt;https://github.com/user-attachments/assets/d5334237-03dc-48f5-8bec-3ae5998660c6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update5&lt;/h2&gt; &#xA;&lt;p&gt;This week there&#39;s been some bigger updates that will most likely affect some old workflows, sampler node especially probably need to be refreshed (re-created) if it errors out!&lt;/p&gt; &#xA;&lt;p&gt;New features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial context windowing with FreeNoise noise shuffling mainly for vid2vid and pose2vid pipelines for longer generations, haven&#39;t figured it out for img2vid yet&lt;/li&gt; &#xA; &lt;li&gt;GGUF models and tiled encoding for I2V and pose pipelines (thanks to MinusZoneAI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/SageAttention&#34;&gt;sageattention&lt;/a&gt; support (Linux only) for a speed boost, I experienced ~20-30% increase with it, stacks with fp8 fast mode, doesn&#39;t need compiling&lt;/li&gt; &#xA; &lt;li&gt;Support CogVideoX-Fun 1.1 and it&#39;s pose models with additional control strength and application step settings, this model&#39;s input does NOT have to be just dwpose skeletons, just about anything can work&lt;/li&gt; &#xA; &lt;li&gt;Support LoRAs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ddeb8f38-a647-42b3-a4b1-c6936f961deb&#34;&gt;https://github.com/user-attachments/assets/ddeb8f38-a647-42b3-a4b1-c6936f961deb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c78b2832-9571-4941-8c97-fbcc1a4cc23d&#34;&gt;https://github.com/user-attachments/assets/c78b2832-9571-4941-8c97-fbcc1a4cc23d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d9ed98b1-f917-432b-a16e-e01e87efb1f9&#34;&gt;https://github.com/user-attachments/assets/d9ed98b1-f917-432b-a16e-e01e87efb1f9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update4&lt;/h2&gt; &#xA;&lt;p&gt;Initial support for the official I2V version of CogVideoX: &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b-I2V&#34;&gt;https://huggingface.co/THUDM/CogVideoX-5b-I2V&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Also needs diffusers 0.30.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c672d0af-a676-495d-a42c-7e3dd802b4b0&#34;&gt;https://github.com/user-attachments/assets/c672d0af-a676-495d-a42c-7e3dd802b4b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update3&lt;/h2&gt; &#xA;&lt;p&gt;Added initial support for CogVideoX-Fun: &lt;a href=&#34;https://github.com/aigc-apps/CogVideoX-Fun&#34;&gt;https://github.com/aigc-apps/CogVideoX-Fun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that while this one can do image2vid, this is NOT the official I2V model yet, though it should also be released very soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/68f9ed16-ee53-4955-b931-1799461ac561&#34;&gt;https://github.com/user-attachments/assets/68f9ed16-ee53-4955-b931-1799461ac561&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updade2&lt;/h2&gt; &#xA;&lt;p&gt;Added &lt;strong&gt;experimental&lt;/strong&gt; support for onediff, this reduced sampling time by ~40% for me, reaching 4.23 s/it on 4090 with 49 frames. This requires using Linux, torch 2.4.0, onediff and nexfort installation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install --pre onediff onediffx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install nexfort&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;First run will take around 5 mins for the compilation.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;5b model is now also supported for basic text2vid: &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b&#34;&gt;https://huggingface.co/THUDM/CogVideoX-5b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is also autodownloaded to &lt;code&gt;ComfyUI/models/CogVideo/CogVideoX-5b&lt;/code&gt;, text encoder is not needed as we use the ComfyUI T5.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/991205cc-826e-4f93-831a-c10441f0f2ce&#34;&gt;https://github.com/user-attachments/assets/991205cc-826e-4f93-831a-c10441f0f2ce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Requires diffusers 0.30.1 (this is specified in requirements.txt)&lt;/p&gt; &#xA;&lt;p&gt;Uses same T5 model than SD3 and Flux, fp8 works fine too. Memory requirements depend mostly on the video length. VAE decoding seems to be the only big that takes a lot of VRAM when everything is offloaded, peaks at around 13-14GB momentarily at that stage. Sampling itself takes only maybe 5-6GB.&lt;/p&gt; &#xA;&lt;p&gt;Hacked in img2img to attempt vid2vid workflow, works interestingly with some inputs, highly experimental.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/e6951ef4-ea7a-4752-94f6-cf24f2503d83&#34;&gt;https://github.com/user-attachments/assets/e6951ef4-ea7a-4752-94f6-cf24f2503d83&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/9e41f37b-2bb3-411c-81fa-e91b80da2559&#34;&gt;https://github.com/user-attachments/assets/9e41f37b-2bb3-411c-81fa-e91b80da2559&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also added temporal tiling as means of generating endless videos:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kijai/ComfyUI-CogVideoXWrapper&#34;&gt;https://github.com/kijai/ComfyUI-CogVideoXWrapper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ecdac8b8-d434-48b6-abd6-90755b6b552d&#34;&gt;https://github.com/user-attachments/assets/ecdac8b8-d434-48b6-abd6-90755b6b552d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Original repo: &lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;https://github.com/THUDM/CogVideo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CogVideoX-Fun: &lt;a href=&#34;https://github.com/aigc-apps/CogVideoX-Fun&#34;&gt;https://github.com/aigc-apps/CogVideoX-Fun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Controlnet: &lt;a href=&#34;https://github.com/TheDenk/cogvideox-controlnet&#34;&gt;https://github.com/TheDenk/cogvideox-controlnet&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>