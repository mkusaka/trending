<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-05T01:43:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gptforfree/gpt4free</title>
    <updated>2023-05-05T01:43:22Z</updated>
    <id>tag:github.com,2023-05-05:/gptforfree/gpt4free</id>
    <link href="https://github.com/gptforfree/gpt4free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;gpt4free repostitory uncensored.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPT4free (Uncensored)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/98614666/233799515-1a7cb6a3-b17f-42c4-956d-8d2a0664466f.png&#34; alt=&#34;GPT4free&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to GPT4free (Uncensored)! This repository provides reverse-engineered third-party APIs for GPT-4/3.5 that can be used in place of OpenAI&#39;s official package. By downloading this repository, you can access these modules, which have been sourced from various websites. Unleash the full potential of ChatGPT for your projects without needing an OpenAI API key.&lt;/p&gt; &#xA;&lt;h2&gt;Legal Notice&lt;/h2&gt; &#xA;&lt;p&gt;Please note that this project is intended for educational purposes only and uses third-party APIs and AI models that are not associated with or endorsed by the API providers or the original developers of the models.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Disclaimer: The APIs, services, and trademarks mentioned in this repository belong to their respective owners. This project is not claiming any right over them.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Responsibility: The author of this repository is not responsible for any consequences arising from the use or misuse of this repository or the content provided by the third-party APIs, and any damage or losses caused by users&#39; actions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Educational Purposes Only: This repository and its content are provided strictly for educational purposes. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;To Do List&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;List of tasks to be done&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#to-do-list&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Current Sites&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Current websites or platforms that can be used as APIs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#current-sites&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Best Sites for GPT4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Recommended websites or platforms for GPT4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#best-sites-for-gpt4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Streamlit GPT4Free GUI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Web-based graphical user interface for interacting with GPT4Free&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#streamlit-gpt4free-gui&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Instructions on how to run GPT4Free in a Docker container&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#docker&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ChatGPT Clone&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A ChatGPT clone with new features and scalability&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chat.chatbot.sex/chat&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Visit%20Site-blue&#34; alt=&#34;Link to Website&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;How to Install&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Instructions on how to install GPT4Free&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#how-to-install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Legal Notice&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Legal notice or disclaimer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#legal-notice&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Copyright&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Copyright information&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#copyright&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Link-Go%20to%20Section-blue&#34; alt=&#34;Link to Section&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;To Do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more third-party APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the accuracy of the models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement GPT-4/3.5 functionalities not available in current APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for more programming languages&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the GUI for better user experience&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement a more efficient method for training models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Expand the documentation and examples for easier usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current Sites&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Model(s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://poe.com&#34;&gt;poe.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4/3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://writesonic.com&#34;&gt;writesonic.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5 / Internet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://t3nsor.com&#34;&gt;t3nsor.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://you.com&#34;&gt;you.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5 / Internet / good search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://phind.com&#34;&gt;phind.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4 / Internet / good search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sqlchat.ai&#34;&gt;sqlchat.ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chat.openai.com/chat&#34;&gt;chat.openai.com/chat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bard.google.com&#34;&gt;bard.google.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;custom / search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bing.com/chat&#34;&gt;bing.com/chat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4/3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chat.forefront.ai/&#34;&gt;chat.forefront.ai/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-4/3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Best sites&lt;/h2&gt; &#xA;&lt;h4&gt;gpt-4&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/phind/README.md&#34;&gt;&lt;code&gt;/phind&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pro: only stable gpt-4 with streaming ( no limit )&lt;/li&gt; &#xA; &lt;li&gt;contra: weird backend prompting&lt;/li&gt; &#xA; &lt;li&gt;why not &lt;code&gt;ora&lt;/code&gt; anymore ? gpt-4 requires login + limited&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;gpt-3.5&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;looking for a stable api at the moment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Streamlit GPT4Free GUI&lt;/h2&gt; &#xA;&lt;p&gt;GPT4Free also comes with a web-based graphical user interface built using Streamlit. The GUI allows users to interact with GPT4Free and generate text outputs without needing to write any code. The interface is simple to use and can be accessed through any web browser. To run the GUI, simply run the &lt;code&gt;streamlit_app.py&lt;/code&gt; file in the &lt;code&gt;GUI&lt;/code&gt; folder after installing the required dependencies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/98614666/130663407-85d1e2f2-602a-4728-bdc8-8353a72b3f08.png&#34; alt=&#34;GPT4Free GUI&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;GPT4Free can also be run in a Docker container for easier deployment and management. To run GPT4Free in a Docker container, first install Docker and then follow the instructions in the &lt;code&gt;Dockerfile&lt;/code&gt; in the root directory of this repository.&lt;/p&gt; &#xA;&lt;h2&gt;ChatGPT Clone&lt;/h2&gt; &#xA;&lt;p&gt;ChatGPT Clone is a ChatGPT clone with new features and scalability. It is built using GPT4Free and is available online for anyone to use. ChatGPT Clone can be accessed through any web browser and is capable of generating human-like responses to a wide range of inputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/98614666/130664163-4a85c4b4-3b60-495b-9d44-9b9ea0f64d0b.png&#34; alt=&#34;ChatGPT Clone&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Install&lt;/h2&gt; &#xA;&lt;p&gt;To install GPT4Free, first clone this repository to your local machine. Then, install the required dependencies using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. Once the dependencies are installed, you can start using the various APIs provided in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Legal Notice&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/gptforfree/gpt4free/main/#legal-notice&#34;&gt;Legal Notice&lt;/a&gt; section above for information about the legal disclaimer for using GPT4Free.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the MIT License. Please refer to the &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Luodian/Otter</title>
    <updated>2023-05-05T01:43:22Z</updated>
    <id>tag:github.com,2023-05-05:/Luodian/Otter</id>
    <link href="https://github.com/Luodian/Otter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü¶¶ Otter, an instruction-tuned model built upon OpenFlamingo that has been customized for a context.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://i.postimg.cc/CLPnPvZW/title.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://brianboli.com/&#34; target=&#34;_blank&#34;&gt;Bo Li*&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://zhangyuanhan-ai.github.io/&#34; target=&#34;_blank&#34;&gt;Yuanhan Zhang*&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://cliangyu.com/&#34; target=&#34;_blank&#34;&gt;Liangyu Chen*&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://king159.github.io/&#34; target=&#34;_blank&#34;&gt;Jinghao Wang*&lt;/a&gt;‚ÄÉ &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://jingkang50.github.io/&#34; target=&#34;_blank&#34;&gt;Jingkang Yang&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://liuziwei7.github.io/&#34; target=&#34;_blank&#34;&gt;Ziwei Liu&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;div align=&#34;center&#34;&gt;&#xA;    S-Lab, Nanyang Technological University &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/otter-v0.1-darkcyan&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/luodian/otter?style=social&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FLuodian%2Fotter&amp;amp;count_bg=%23FFA500&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=visitors&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://black.readthedocs.io/en/stable/_static/license.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://huggingface.co/luodian/otter-9b-hf&#34;&gt;Otter-9B (Huggingface Models)&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/r-YM4DGGAdE&#34;&gt;Youtube Video&lt;/a&gt; | &lt;a href=&#34;https://www.bilibili.com/video/BV1iL411h7HZ/?share_source=copy_web&amp;amp;vd_source=477facaaaa60694f67a784f5eaa905ad&#34;&gt;Bilibili Video&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://otter.cliangyu.com/&#34;&gt;Live Demo (soon)&lt;/a&gt; | &lt;a href=&#34;&#34;&gt;Paper (soon)&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h2&gt;ü¶¶ Overview&lt;/h2&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/Z5fkydMP/teaser.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;p&gt;Recent research highlights the importance of fine-tuning instruction for empowering large language models (LLMs), such as enhancing GPT-3 to Chat-GPT, to follow natural language instructions and effectively accomplish real-world tasks. Flamingo is considered a GPT-3 moment in the multimodal domain.&lt;/p&gt; &#xA;  &lt;p&gt;In our project, we propose ü¶¶ Otter, an instruction-tuned model built upon OpenFlamingo that has been customized for a context. We improve its conversational skills by using a carefully crafted multimodal instruction tuning dataset. Each data sample includes an image-specific instruction along with multiple examples of multimodal instructions for that context, also known as multimodal in-context learning examples.&lt;/p&gt; &#xA;  &lt;p&gt;By utilizing high-quality data, we were able to train ü¶¶ Otter using limited resources (4x RTX-3090-24G GPUs) in our lab. Remarkably, it surpassed the performance of OpenFlamingo. While Otter may not be the most advanced and may occasionally experience confusion, we are committed to consistently enhancing its capabilities by including more types of training data and a larger model. In the current era of expansive foundational models, we firmly believe that anyone should have the opportunity to train their own models, even with scarce data and resources, and cultivate the models to develop their intelligence.&lt;/p&gt; &#xA;  &lt;h2&gt;ü¶¶ Examples&lt;/h2&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/KYqmWG7j/example-description2.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/FRYh5MGZ/example-description.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/YSqp8GWT/example-understanding.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/FzjKJbjJ/examples-ict.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/JnBrfwzL/examples-ict2.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;h2&gt;üóÇÔ∏è Environments&lt;/h2&gt; &#xA;  &lt;p&gt;You may install via &lt;code&gt;conda env create -f environment.yml&lt;/code&gt;. Especially to make sure the &lt;code&gt;transformers&amp;gt;=4.28.0&lt;/code&gt;, &lt;code&gt;accelerate&amp;gt;=0.18.0&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;ü§ó Hugging Face Model&lt;/h2&gt; &#xA;  &lt;p&gt;You can use the ü¶© Flamingo model / ü¶¶ Otter model as a ü§ó huggingface model with only a few lines! One-click and then model configs/weights are downloaded automatically.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flamingo import FlamingoModel&#xA;flamingo_model = FlamingoModel.from_pretrained(&#34;luodian/openflamingo-9b-hf&#34;, device_map=auto)&#xA;&#xA;from otter import OtterModel&#xA;otter_model = OtterModel.from_pretrained(&#34;luodian/otter-9b-hf&#34;, device_map=auto)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Previous &lt;a href=&#34;https://github.com/mlfoundations/open_flamingo&#34;&gt;OpenFlamingo&lt;/a&gt; was developed with &lt;a href=&#34;https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel&#34;&gt;DistributedDataParallel&lt;/a&gt; (DDP) on A100 cluster. Loading OpenFlamingo-9B to GPU requires &lt;strong&gt;at least 33G GPU memory&lt;/strong&gt;, which is only available on A100 GPUs.&lt;/p&gt; &#xA;  &lt;p&gt;In order to allow more researchers without access to A100 machines to try training OpenFlamingo, we wrap the OpenFlamingo model into a ü§ó huggingface model (&lt;a href=&#34;https://king159.github.io/&#34;&gt;Jinghao&lt;/a&gt; has submitted a &lt;a href=&#34;https://github.com/huggingface/transformers/pull/23063&#34;&gt;PR&lt;/a&gt; to the /huggingface/transformers!). Via &lt;code&gt;device_map=auto&lt;/code&gt;, the large model is sharded across multiple GPUs when loading and training. This can help researchers who do not have access to A100-80G GPUs to achieve similar throughput in training, testing on 4x RTX-3090-24G GPUs, and model deployment on 2x RTX-3090-24G GPUs. Specific details are below (may vary depending on the CPU and disk performance, as we conducted training on different machines).&lt;/p&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/LsNs55zG/table.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;div style=&#34;text-align:center&#34;&gt; &#xA;   &lt;img src=&#34;https://i.postimg.cc/tTcCdcv5/efficiency.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;p&gt;Our Otter model is also developed in this way and it&#39;s deployed on the ü§ó Hugging Face model hub. Our model can be hosted on two RTX-3090-24G GPUs and achieve a similar speed to one A100-80G machine.&lt;/p&gt; &#xA;  &lt;h2&gt;üóÑ Dataset Preparation&lt;/h2&gt; &#xA;  &lt;h3&gt;Multi-modal instruction tuning dataset with in-context examples (ICI)&lt;/h3&gt; &#xA;  &lt;p&gt;The pre-training process for the OpenFlamingo model employs the MMC4 interleaved multimodality dataset to endow the model with in-context few-shot learning capabilities. The development of our instruction-following dataset adheres to the guiding principles of MMC4, which dictate that the instruction and image examples incorporated into the context should exhibit semantic pertinence to the query instruction and image.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;To augment the LLaVA dataset, we retrieve in-context examples for each query data.&lt;/li&gt; &#xA;   &lt;li&gt;We curate high-quality, in-progress panoptic video scene graph data from the PVSG repository. For each video, we select 4-8 frames to be annotated for instruction-following, using the LLaVa dataset as a reference. During the training phase, given a frame, we opt for additional frames, along with their corresponding instructions and answers, to serve as in-context examples.&lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA;  &lt;h3&gt;Example&lt;/h3&gt; &#xA;  &lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt;&lt;img src=&#34;https://i.postimg.cc/vmmP0bH0/image-example-3.png&#34; alt=&#34;otter-example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/p&gt; &#xA;  &lt;h3&gt;Preparation&lt;/h3&gt; &#xA;  &lt;p&gt;We unify different instructing data into a single dataset &lt;a href=&#34;https://raw.githubusercontent.com/Luodian/Otter/main/pipeline/multi_instruct_data_utils/unify_dataset.py&#34;&gt;class&lt;/a&gt;. The full dataset is coming soon!&lt;/p&gt; &#xA;  &lt;!-- Download a subset of the pretraining `multi_instruct_data` dataset&#xA;&#xA;```bash&#xA;wget https://ofa-beijing.oss-cn-beijing.aliyuncs.com/datasets/pretrain_data/pretrain_data_examples.zip;&#xA;unzip pretrain_data_examples.zip ./example_multi_instruct_data&#xA;``` --&gt; &#xA;  &lt;h2&gt;‚òÑÔ∏è Training&lt;/h2&gt; &#xA;  &lt;p&gt;Train on &lt;code&gt;in-context-instruction(ICI)&lt;/code&gt; datasets, using the following commands:&lt;/p&gt; &#xA;  &lt;p&gt;First, run, and answer the questions asked. This will generate a config file and save it to the cache folder. The config will be used automatically to properly set the default options when doing &lt;code&gt;accelerate launch&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Then run the training script.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --pretrained_model_name_or_path=luodian/openflamingo-9b-hf \&#xA;--lm_path=luodian/llama-7b-hf \&#xA;--tokenizer_path=luodian/llama-7b-hf \&#xA;--dataset_resampled \&#xA;--multi_instruct_path=./in_context_instruct.tsv \&#xA;--run_name=otter-9b \&#xA;--batch_size=1 \&#xA;--num_epochs=6 \&#xA;--report_to_wandb \&#xA;--cross_attn_every_n_layers=4 \&#xA;--lr_scheduler=cosine \&#xA;--delete_previous_checkpoint \&#xA;--learning_rate=1e-5 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h2&gt;üíé Checkpoints&lt;/h2&gt; &#xA;  &lt;p&gt;For details, you may refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Luodian/Otter/main/docs/model_card.md&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;ü™© Web Demo&lt;/h2&gt; &#xA;  &lt;p&gt;We host our &lt;a href=&#34;https://otter.cliangyu.com/&#34;&gt;Otter-9B Demo&lt;/a&gt; via dual RTX-3090-24G GPUs. Launch your own demo by following the &lt;a href=&#34;https://raw.githubusercontent.com/Luodian/Otter/main/docs/demo.md&#34;&gt;demo instructions&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;üõ† Incoming Features&lt;/h2&gt; &#xA;  &lt;p&gt;We are working towards offering these features to our users. However, we have encountered some issues in the process. If you have the solutions to these issues, we would be grateful if you could submit a pull request with your code. Your contribution would be highly appreciated.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;xformers&lt;/code&gt; support: for saving GPU memory and training speedup. issue &lt;a href=&#34;https://github.com/Luodian/PET-VLM/issues/35&#34;&gt;#35&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;load_in_8bit&lt;/code&gt; support: for saving GPU memory and training speedup. [&lt;a href=&#34;&#34;&gt;issue&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Models&lt;/h3&gt; &#xA;  &lt;p&gt;We are working on the following models with much stronger performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Otter-9B for Videos&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Otter-15B&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;üìë Citation&lt;/h2&gt; &#xA;  &lt;p&gt;If you found this repository useful, please consider citing:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code&gt;@software{li_bo_2023_7879884,&#xA;  author       = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},&#xA;  title        = {{Otter: Multi-Modal In-Context Learning Model with Instruction Tuning}},&#xA;  month        = apr,&#xA;  year         = 2023,&#xA;  publisher    = {Zenodo},&#xA;  version      = {0.1},&#xA;  doi          = {10.5281/zenodo.7879884},&#xA;  url          = {https://doi.org/10.5281/zenodo.7879884}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h3&gt;üë®‚Äçüè´ Acknowledgements&lt;/h3&gt; &#xA;  &lt;p&gt;We thank &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt; and &lt;a href=&#34;https://jmhessel.com/&#34;&gt;Jack Hessel&lt;/a&gt; for their advise and support, as well as the &lt;a href=&#34;https://github.com/mlfoundations/open_flamingo&#34;&gt;OpenFlamingo&lt;/a&gt; team for their great contribution to the open source community.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>Arize-ai/phoenix</title>
    <updated>2023-05-05T01:43:22Z</updated>
    <id>tag:github.com,2023-05-05:/Arize-ai/phoenix</id>
    <link href="https://github.com/Arize-ai/phoenix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ML Observability in a Notebook - Uncover Insights, Surface Problems, Monitor, and Fine Tune your Generative LLM, CV and Tabular Models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://phoenix.arize.com&#34; style=&#34;background:none&#34;&gt; &lt;img alt=&#34;phoenix logo&#34; src=&#34;https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg?sanitize=true&#34; width=&#34;auto&#34; height=&#34;200&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://docs.arize.com/phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?message=Docs&amp;amp;logo=data:image/png;base64,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&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=white&amp;amp;label=%20&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?message=Community&amp;amp;logo=slack&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=white&amp;amp;label=%20&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/ArizePhoenix&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-ArizePhoenix-blue.svg?color=blue&amp;amp;labelColor=gray&amp;amp;logo=twitter&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://pypi.org/project/arize-phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/arize-phoenix?color=blue&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://anaconda.org/conda-forge/arize-phoenix&#34;&gt; &lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/arize-phoenix.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://pypi.org/project/arize-phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/arize-phoenix&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Phoenix provides MLOps insights at lightning speed with zero-config observability for model drift, performance, and data quality. Phoenix is notebook-first python library that leverages embeddings to uncover problematic cohorts of your LLM, CV, NLP and tabular models.&lt;/p&gt; &#xA;&lt;!-- EXCLUDE --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Arize-ai/phoenix-assets/raw/main/gifs/computer_vision_color_by_prediction.gif?raw=true&#34; alt=&#34;a rotating UMAP point cloud of a computer vision model&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- /EXCLUDE --&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install arize-phoenix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=orange&amp;amp;label=%20&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Arize-ai/phoenix/raw/main/tutorials/image_classification_tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20GitHub&amp;amp;logo=github&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=white&amp;amp;label=%20&#34; alt=&#34;Open in GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Import libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dataclasses import replace&#xA;import pandas as pd&#xA;import phoenix as px&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download curated datasets and load them into pandas DataFrames.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_df = pd.read_parquet(&#xA;    &#34;https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet&#34;&#xA;)&#xA;prod_df = pd.read_parquet(&#xA;    &#34;https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_production.parquet&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Define schemas that tell Phoenix which columns of your DataFrames correspond to features, predictions, actuals (i.e., ground truth), embeddings, etc.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_schema = px.Schema(&#xA;    prediction_id_column_name=&#34;prediction_id&#34;,&#xA;    timestamp_column_name=&#34;prediction_ts&#34;,&#xA;    prediction_label_column_name=&#34;predicted_action&#34;,&#xA;    actual_label_column_name=&#34;actual_action&#34;,&#xA;    embedding_feature_column_names={&#xA;        &#34;image_embedding&#34;: px.EmbeddingColumnNames(&#xA;            vector_column_name=&#34;image_vector&#34;,&#xA;            link_to_data_column_name=&#34;url&#34;,&#xA;        ),&#xA;    },&#xA;)&#xA;prod_schema = replace(train_schema, actual_label_column_name=None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Define your production and training datasets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prod_ds = px.Dataset(prod_df, prod_schema)&#xA;train_ds = px.Dataset(train_df, train_schema)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch the app.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session = px.launch_app(prod_ds, train_ds)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can open Phoenix by copying and pasting the output of &lt;code&gt;session.url&lt;/code&gt; into a new browser tab.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session.url&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can open the Phoenix UI in your notebook with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;session.view()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you&#39;re done, don&#39;t forget to close the app.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;px.close_app()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Embedding Drift Analysis&lt;/h3&gt; &#xA;&lt;p&gt;Explore UMAP point-clouds at times of high euclidean distance and identify clusters of drift.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://storage.googleapis.com/arize-assets/phoenix/assets/images/ner_color_by_correctness.png&#34; alt=&#34;Euclidean distance drift analysis&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;UMAP-based Exploratory Data Analysis&lt;/h3&gt; &#xA;&lt;p&gt;Color your UMAP point-clouds by your model&#39;s dimensions, drift, and performance to identify problematic cohorts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://storage.googleapis.com/arize-assets/phoenix/assets/images/cv_eda_selection.png&#34; alt=&#34;UMAP-based EDA&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cluster-driven Drift and Performance Analysis&lt;/h3&gt; &#xA;&lt;p&gt;Break-apart your data into clusters of high drift or bad performance using HDBSCAN&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://storage.googleapis.com/arize-assets/phoenix/assets/images/HDBSCAN_drift_analysis.png&#34; alt=&#34;HDBSCAN clusters sorted by drift&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Exportable Clusters&lt;/h3&gt; &#xA;&lt;p&gt;Export your clusters to &lt;code&gt;parquet&lt;/code&gt; files or dataframes for further analysis and fine-tuning.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For in-depth examples and explanations, read the &lt;a href=&#34;https://docs.arize.com/phoenix&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our community to connect with thousands of machine learning practitioners and ML observability enthusiasts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üåç Join our &lt;a href=&#34;https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q&#34;&gt;Slack community&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üí° Ask questions and provide feedback in the &lt;em&gt;#phoenix-support&lt;/em&gt; channel.&lt;/li&gt; &#xA; &lt;li&gt;üåü Leave a star on our &lt;a href=&#34;https://github.com/Arize-ai/phoenix&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üêû Report bugs with &lt;a href=&#34;https://github.com/Arize-ai/phoenix/issues&#34;&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üê£ Follow us on &lt;a href=&#34;https://twitter.com/ArizePhoenix&#34;&gt;twitter&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üíåÔ∏è Sign up for our &lt;a href=&#34;https://phoenix.arize.com/#updates&#34;&gt;mailing list&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üó∫Ô∏è Check out our &lt;a href=&#34;https://github.com/orgs/Arize-ai/projects/45&#34;&gt;roadmap&lt;/a&gt; to see where we&#39;re heading next.&lt;/li&gt; &#xA; &lt;li&gt;üéì Learn the fundamentals of ML observability with our &lt;a href=&#34;https://arize.com/ml-observability-fundamentals/&#34;&gt;introductory&lt;/a&gt; and &lt;a href=&#34;https://arize.com/blog-course/&#34;&gt;advanced&lt;/a&gt; courses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lmcinnes/umap&#34;&gt;UMAP&lt;/a&gt; For unlocking the ability to visualize and reason about embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scikit-learn-contrib/hdbscan&#34;&gt;HDBSCAN&lt;/a&gt; For providing a clustering algorithm to aid in the discovery of drift and performance degradation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Copyright, Patent, and License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2023 Arize AI, Inc. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Portions of this code are patent protected by one or more U.S. Patents. See &lt;a href=&#34;https://github.com/Arize-ai/phoenix/raw/main/IP_NOTICE&#34;&gt;IP_NOTICE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This software is licensed under the terms of the Elastic License 2.0 (ELv2). See &lt;a href=&#34;https://github.com/Arize-ai/phoenix/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>