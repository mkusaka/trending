<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-04T01:34:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>naver/dust3r</title>
    <updated>2024-03-04T01:34:11Z</updated>
    <id>tag:github.com,2024-03-04:/naver/dust3r</id>
    <link href="https://github.com/naver/dust3r" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DUSt3R&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of &lt;code&gt;DUSt3R: Geometric 3D Vision Made Easy&lt;/code&gt;&lt;br&gt; [&lt;a href=&#34;https://dust3r.europe.naverlabs.com/&#34;&gt;Project page&lt;/a&gt;], [&lt;a href=&#34;https://arxiv.org/abs/2312.14132&#34;&gt;DUSt3R arxiv&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/dust3r/main/assets/pipeline1.jpg&#34; alt=&#34;Example of reconstruction from two images&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/dust3r/main/assets/dust3r_archi.jpg&#34; alt=&#34;High level overview of DUSt3R capabilities&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{wang2023dust3r,&#xA;      title={DUSt3R: Geometric 3D Vision Made Easy}, &#xA;      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},&#xA;      year={2023},&#xA;      eprint={2312.14132},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#dust3r&#34;&gt;DUSt3R&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#interactive-demo&#34;&gt;Interactive demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#our-hyperparameters&#34;&gt;Our Hyperparameters&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is distributed under the CC BY-NC-SA 4.0 License. See &lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Copyright (C) 2024-present Naver Corporation. All rights reserved.&#xA;# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone DUSt3R&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/naver/dust3r&#xA;cd dust3r&#xA;# if you have already cloned dust3r:&#xA;# git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create the environment, here we show an example using conda.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n dust3r python=3.11 cmake=3.14.0&#xA;conda activate dust3r &#xA;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia  # use the correct version of cuda for your system&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optional, compile the cuda kernels for RoPE (as in CroCo v2)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.&#xA;cd croco/models/curope/&#xA;python setup.py build_ext --inplace&#xA;cd ../../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download pre-trained model&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -P checkpoints/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;We provide several pre-trained models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Modelname&lt;/th&gt; &#xA;   &lt;th&gt;Training resolutions&lt;/th&gt; &#xA;   &lt;th&gt;Head&lt;/th&gt; &#xA;   &lt;th&gt;Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Decoder&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_224_linear.pth&#34;&gt;&lt;code&gt;DUSt3R_ViTLarge_BaseDecoder_224_linear.pth&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;Linear&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_linear.pth&#34;&gt;&lt;code&gt;DUSt3R_ViTLarge_BaseDecoder_512_linear.pth&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512x384, 512x336, 512x288, 512x256, 512x160&lt;/td&gt; &#xA;   &lt;td&gt;Linear&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34;&gt;&lt;code&gt;DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512x384, 512x336, 512x288, 512x256, 512x160&lt;/td&gt; &#xA;   &lt;td&gt;DPT&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can check the hyperparameters we used to train these models in the &lt;a href=&#34;https://raw.githubusercontent.com/naver/dust3r/main/#our-hyperparameters&#34;&gt;section: Our Hyperparameters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Interactive demo&lt;/h3&gt; &#xA;&lt;p&gt;In this demo, you should be able run DUSt3R on your machine to reconstruct a scene.&lt;br&gt; First select images that depicts the same scene.&lt;/p&gt; &#xA;&lt;p&gt;You can adjust the global alignment schedule and its number of iterations.&lt;br&gt; Note: if you selected one or two images, the global alignment procedure will be skipped (mode=GlobalAlignerMode.PairViewer)&lt;br&gt; Hit &#34;Run&#34; and wait.&lt;br&gt; When the global alignment ends, the reconstruction appears.&lt;br&gt; Use the slider &#34;min_conf_thr&#34; to show or remove low confidence areas.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 demo.py --weights checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#xA;&#xA;# Use --image_size to select the correct resolution for your checkpoint. 512 (default) or 224&#xA;# Use --local_network to make it accessible on the local network, or --server_name to specify the url manually&#xA;# Use --server_port to change the port, by default it will search for an available port starting at 7860&#xA;# Use --device to use a different device, by default it&#39;s &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/dust3r/main/assets/demo.jpg&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dust3r.inference import inference, load_model&#xA;from dust3r.utils.image import load_images&#xA;from dust3r.image_pairs import make_pairs&#xA;from dust3r.cloud_opt import global_aligner, GlobalAlignerMode&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    model_path = &#34;checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34;&#xA;    device = &#39;cuda&#39;&#xA;    batch_size = 1&#xA;    schedule = &#39;cosine&#39;&#xA;    lr = 0.01&#xA;    niter = 300&#xA;&#xA;    model = load_model(model_path, device)&#xA;    # load_images can take a list of images or a directory&#xA;    images = load_images([&#39;croco/assets/Chateau1.png&#39;, &#39;croco/assets/Chateau2.png&#39;], size=512)&#xA;    pairs = make_pairs(images, scene_graph=&#39;complete&#39;, prefilter=None, symmetrize=True)&#xA;    output = inference(pairs, model, device, batch_size=batch_size)&#xA;&#xA;    # at this stage, you have the raw dust3r predictions&#xA;    view1, pred1 = output[&#39;view1&#39;], output[&#39;pred1&#39;]&#xA;    view2, pred2 = output[&#39;view2&#39;], output[&#39;pred2&#39;]&#xA;    # here, view1, pred1, view2, pred2 are dicts of lists of len(2)&#xA;    #  -&amp;gt; because we symmetrize we have (im1, im2) and (im2, im1) pairs&#xA;    # in each view you have:&#xA;    # an integer image identifier: view1[&#39;idx&#39;] and view2[&#39;idx&#39;]&#xA;    # the img: view1[&#39;img&#39;] and view2[&#39;img&#39;]&#xA;    # the image shape: view1[&#39;true_shape&#39;] and view2[&#39;true_shape&#39;]&#xA;    # an instance string output by the dataloader: view1[&#39;instance&#39;] and view2[&#39;instance&#39;]&#xA;    # pred1 and pred2 contains the confidence values: pred1[&#39;conf&#39;] and pred2[&#39;conf&#39;]&#xA;    # pred1 contains 3D points for view1[&#39;img&#39;] in view1[&#39;img&#39;] space: pred1[&#39;pts3d&#39;]&#xA;    # pred2 contains 3D points for view2[&#39;img&#39;] in view1[&#39;img&#39;] space: pred2[&#39;pts3d_in_other_view&#39;]&#xA;&#xA;    # next we&#39;ll use the global_aligner to align the predictions&#xA;    # depending on your task, you may be fine with the raw output and not need it&#xA;    # with only two input images, you could use GlobalAlignerMode.PairViewer: it would just convert the output&#xA;    # if using GlobalAlignerMode.PairViewer, no need to run compute_global_alignment&#xA;    scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)&#xA;    loss = scene.compute_global_alignment(init=&#34;mst&#34;, niter=niter, schedule=schedule, lr=lr)&#xA;&#xA;    # retrieve useful values from scene:&#xA;    imgs = scene.imgs&#xA;    focals = scene.get_focals()&#xA;    poses = scene.get_im_poses()&#xA;    pts3d = scene.get_pts3d()&#xA;    confidence_masks = scene.get_masks()&#xA;&#xA;    # visualize reconstruction&#xA;    scene.show()&#xA;&#xA;    # find 2D-2D matches between the two images&#xA;    from dust3r.utils.geometry import find_reciprocal_matches, xy_grid&#xA;    pts2d_list, pts3d_list = [], []&#xA;    for i in range(2):&#xA;        conf_i = confidence_masks[i].cpu().numpy()&#xA;        pts2d_list.append(xy_grid(*imgs[i].shape[:2][::-1])[conf_i])  # imgs[i].shape[:2] = (H, W)&#xA;        pts3d_list.append(pts3d[i].detach().cpu().numpy()[conf_i])&#xA;    reciprocal_in_P2, nn2_in_P1, num_matches = find_reciprocal_matches(*pts3d_list)&#xA;    print(f&#39;found {num_matches} matches&#39;)&#xA;    matches_im1 = pts2d_list[1][reciprocal_in_P2]&#xA;    matches_im0 = pts2d_list[0][nn2_in_P1][reciprocal_in_P2]&#xA;&#xA;    # visualize a few matches&#xA;    import numpy as np&#xA;    from matplotlib import pyplot as pl&#xA;    n_viz = 10&#xA;    match_idx_to_viz = np.round(np.linspace(0, num_matches-1, n_viz)).astype(int)&#xA;    viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]&#xA;&#xA;    H0, W0, H1, W1 = *imgs[0].shape[:2], *imgs[1].shape[:2]&#xA;    img0 = np.pad(imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img1 = np.pad(imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img = np.concatenate((img0, img1), axis=1)&#xA;    pl.figure()&#xA;    pl.imshow(img)&#xA;    cmap = pl.get_cmap(&#39;jet&#39;)&#xA;    for i in range(n_viz):&#xA;        (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T&#xA;        pl.plot([x0, x1 + W0], [y0, y1], &#39;-+&#39;, color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)&#xA;    pl.show(block=True)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/dust3r/main/assets/matching.jpg&#34; alt=&#34;matching example on croco pair&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;In this section, we present propose a short demonstration to get started with training DUSt3R. At the moment, we didn&#39;t release the training datasets, so we&#39;re going to download and prepare a subset of &lt;a href=&#34;https://github.com/facebookresearch/co3d&#34;&gt;CO3Dv2&lt;/a&gt; - &lt;a href=&#34;https://github.com/facebookresearch/co3d/raw/main/LICENSE&#34;&gt;Creative Commons Attribution-NonCommercial 4.0 International&lt;/a&gt; and launch the training code on it. The demo model will be trained for a few epochs on a very small dataset. It will not be very good.&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;# download and prepare the co3d subset&#xA;mkdir -p data/co3d_subset&#xA;cd data/co3d_subset&#xA;git clone https://github.com/facebookresearch/co3d&#xA;cd co3d&#xA;python3 ./co3d/download_dataset.py --download_folder ../ --single_sequence_subset&#xA;rm ../*.zip&#xA;cd ../../..&#xA;&#xA;python3 datasets_preprocess/preprocess_co3d.py --co3d_dir data/co3d_subset --output_dir data/co3d_subset_processed  --single_sequence_subset&#xA;&#xA;# download the pretrained croco v2 checkpoint&#xA;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/CroCo/CroCo_V2_ViTLarge_BaseDecoder.pth -P checkpoints/&#xA;&#xA;# the training of dust3r is done in 3 steps.&#xA;# for this example we&#39;ll do fewer epochs, for the actual hyperparameters we used in the paper, see the next section: &#34;Our Hyperparameters&#34;&#xA;# step 1 - train dust3r for 224 resolution&#xA;torchrun --nproc_per_node=4 train.py \&#xA;    --train_dataset &#34;1000 @ Co3d(split=&#39;train&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=224, transform=ColorJitter)&#34; \&#xA;    --test_dataset &#34;100 @ Co3d(split=&#39;test&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, resolution=224, seed=777)&#34; \&#xA;    --model &#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, img_size=(224, 224), head_type=&#39;linear&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion &#34;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#34; \&#xA;    --pretrained checkpoints/CroCo_V2_ViTLarge_BaseDecoder.pth \&#xA;    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 16 --accum_iter 1 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 \&#xA;    --output_dir checkpoints/dust3r_demo_224&#x9;  &#xA;&#xA;# step 2 - train dust3r for 512 resolution&#xA;torchrun --nproc_per_node=4 train.py \&#xA;    --train_dataset &#34;1000 @ Co3d(split=&#39;train&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter)&#34; \&#xA;    --test_dataset=&#34;100 @ Co3d(split=&#39;test&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, resolution=(512,384), seed=777)&#34; \&#xA;    --model=&#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;linear&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion &#34;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#34; \&#xA;    --pretrained=&#39;checkpoints/dust3r_demo_224/checkpoint-best.pth&#39; \&#xA;    --lr=0.0001 --min_lr=1e-06 --warmup_epochs 1 --epochs 10 --batch_size 4 --accum_iter 4 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 \&#xA;    --output_dir checkpoints/dust3r_demo_512&#xA;&#xA;# step 3 - train dust3r for 512 resolution with dpt&#xA;torchrun --nproc_per_node=4 train.py \&#xA;    --train_dataset &#34;1000 @ Co3d(split=&#39;train&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter)&#34; \&#xA;    --test_dataset=&#34;100 @ Co3d(split=&#39;test&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, resolution=(512,384), seed=777)&#34; \&#xA;    --model=&#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;dpt&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion &#34;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#34; \&#xA;    --pretrained=&#39;checkpoints/dust3r_demo_512/checkpoint-best.pth&#39; \&#xA;    --lr=0.0001 --min_lr=1e-06 --warmup_epochs 1 --epochs 10 --batch_size 2 --accum_iter 8 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 \&#xA;    --output_dir checkpoints/dust3r_demo_512dpt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Our Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We didn&#39;t release the training datasets, but here are the commands we used for training our models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# NOTE: ROOT path omitted for datasets&#xA;# 224 linear&#xA;torchrun --nproc_per_node 4 train.py \&#xA;    --train_dataset=&#34; + 100_000 @ Habitat512(1_000_000, split=&#39;train&#39;, aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ BlendedMVS(split=&#39;train&#39;, aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ MegaDepthDense(split=&#39;train&#39;, aug_crop=16, resolution=224, transform=ColorJitter) + 100_000 @ ARKitScenes(aug_crop=256, resolution=224, transform=ColorJitter) + 100_000 @ Co3d_v3(split=&#39;train&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=224, transform=ColorJitter) + 100_000 @ StaticThings3D(aug_crop=256, mask_bg=&#39;rand&#39;, resolution=224, transform=ColorJitter) + 100_000 @ ScanNetpp(split=&#39;train&#39;, aug_crop=256, resolution=224, transform=ColorJitter) + 100_000 @ Waymo(aug_crop=128, resolution=224, transform=ColorJitter) &#34; \&#xA;    --test_dataset=&#34; Habitat512(1_000, split=&#39;val&#39;, resolution=224, seed=777) + 1_000 @ BlendedMVS(split=&#39;val&#39;, resolution=224, seed=777) + 1_000 @ MegaDepthDense(split=&#39;val&#39;, resolution=224, seed=777) + 1_000 @ Co3d_v3(split=&#39;test&#39;, mask_bg=&#39;rand&#39;, resolution=224, seed=777) &#34; \&#xA;    --train_criterion=&#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion=&#39;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#39; \&#xA;    --model=&#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, img_size=(224, 224), head_type=&#39;linear&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --pretrained=&#34;checkpoints/CroCo_V2_ViTLarge_BaseDecoder.pth&#34; \&#xA;    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=10 --epochs=100 --batch_size=16 --accum_iter=1 \&#xA;    --save_freq=5 --keep_freq=10 --eval_freq=1 \&#xA;    --output_dir=&#39;checkpoints/dust3r_224&#39;&#xA;&#xA;# 512 linear&#xA;torchrun --nproc_per_node 8 train.py \&#xA;    --train_dataset=&#34; + 10_000 @ Habitat512(1_000_000, split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ BlendedMVS(split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ MegaDepthDense(split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ARKitScenes(aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Co3d_v3(split=&#39;train&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ StaticThings3D(aug_crop=256, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ScanNetpp(split=&#39;train&#39;, aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Waymo(aug_crop=128, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) &#34; \&#xA;    --test_dataset=&#34; Habitat512(1_000, split=&#39;val&#39;, resolution=(512,384), seed=777) + 1_000 @ BlendedMVS(split=&#39;val&#39;, resolution=(512,384), seed=777) + 1_000 @ MegaDepthDense(split=&#39;val&#39;, resolution=(512,336), seed=777) + 1_000 @ Co3d_v3(split=&#39;test&#39;, resolution=(512,384), seed=777) &#34; \&#xA;    --train_criterion=&#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion=&#39;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#39; \&#xA;    --model=&#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;linear&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --pretrained=&#39;checkpoints/dust3r_224/checkpoint-best.pth&#39; \&#xA;    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=20 --epochs=200 --batch_size=4 --accum_iter=2 \&#xA;    --save_freq=10 --keep_freq=10 --eval_freq=1 --print_freq=10 \&#xA;    --output_dir=&#39;checkpoints/dust3r_512&#39;&#xA;&#xA;# 512 dpt&#xA;torchrun --nproc_per_node 8 train.py \&#xA;    --train_dataset=&#34; + 10_000 @ Habitat512(1_000_000, split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ BlendedMVS(split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ MegaDepthDense(split=&#39;train&#39;, aug_crop=16, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ARKitScenes(aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Co3d_v3(split=&#39;train&#39;, aug_crop=16, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ StaticThings3D(aug_crop=256, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ ScanNetpp(split=&#39;train&#39;, aug_crop=256, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) + 10_000 @ Waymo(aug_crop=128, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], transform=ColorJitter) &#34; \&#xA;    --test_dataset=&#34; Habitat512(1_000, split=&#39;val&#39;, resolution=(512,384), seed=777) + 1_000 @ BlendedMVS(split=&#39;val&#39;, resolution=(512,384), seed=777) + 1_000 @ MegaDepthDense(split=&#39;val&#39;, resolution=(512,336), seed=777) + 1_000 @ Co3d_v3(split=&#39;test&#39;, resolution=(512,384), seed=777) &#34; \&#xA;    --train_criterion=&#34;ConfLoss(Regr3D(L21, norm_mode=&#39;avg_dis&#39;), alpha=0.2)&#34; \&#xA;    --test_criterion=&#39;Regr3D_ScaleShiftInv(L21, gt_scale=True)&#39; \&#xA;    --model=&#34;AsymmetricCroCo3DStereo(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;dpt&#39;, output_mode=&#39;pts3d&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)&#34; \&#xA;    --pretrained=&#39;checkpoints/dust3r_512/checkpoint-best.pth&#39; \&#xA;    --lr=0.0001 --min_lr=1e-06 --warmup_epochs=15 --epochs=90 --batch_size=2 --accum_iter=4 \&#xA;    --save_freq=5 --keep_freq=10 --eval_freq=1 --print_freq=10 \&#xA;    --output_dir=&#39;checkpoints/dust3r_512dpt&#39;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pydantic/FastUI</title>
    <updated>2024-03-04T01:34:11Z</updated>
    <id>tag:github.com,2024-03-04:/pydantic/FastUI</id>
    <link href="https://github.com/pydantic/FastUI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build better UIs faster.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FastUI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pydantic/FastUI/actions?query=event%3Apush+branch%3Amain+workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/pydantic/FastUI/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/fastui&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastui.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pydantic/FastUI&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/fastui.svg?sanitize=true&#34; alt=&#34;versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pydantic/FastUI/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/pydantic/FastUI.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; FastUI is still an active work in progress, do not expect it to be complete.&lt;/p&gt; &#xA;&lt;h2&gt;The Principle (short version)&lt;/h2&gt; &#xA;&lt;p&gt;You can see a simple demo of an application built with FastUI &lt;a href=&#34;https://fastui-demo.onrender.com&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FastUI is a new way to build web application user interfaces defined by declarative Python code.&lt;/p&gt; &#xA;&lt;p&gt;This means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;If you&#39;re a Python developer&lt;/strong&gt; — you can build responsive web applications using React without writing a single line of JavaScript, or touching &lt;code&gt;npm&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;If you&#39;re a frontend developer&lt;/strong&gt; — you can concentrate on building magical components that are truly reusable, no copy-pasting components for each view.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;For everyone&lt;/strong&gt; — a true separation of concerns, the backend defines the entire application; while the frontend is free to implement just the user interface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At its heart, FastUI is a set of matching &lt;a href=&#34;https://docs.pydantic.dev&#34;&gt;Pydantic&lt;/a&gt; models and TypeScript interfaces that allow you to define a user interface. This interface is validated at build time by TypeScript and pyright/mypy and at runtime by Pydantic.&lt;/p&gt; &#xA;&lt;h2&gt;The Practice — Usage&lt;/h2&gt; &#xA;&lt;p&gt;FastUI is made up of 4 things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/fastui&#34;&gt;&lt;code&gt;fastui&lt;/code&gt; PyPI package&lt;/a&gt; — Pydantic models for UI components, and some utilities. While it works well with &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;FastAPI&lt;/a&gt; it doesn&#39;t depend on FastAPI, and most of it could be used with any python web framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.npmjs.com/package/@pydantic/fastui&#34;&gt;&lt;code&gt;@pydantic/fastui&lt;/code&gt; npm package&lt;/a&gt; — a React TypeScript package that lets you reuse the machinery and types of FastUI while implementing your own components&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.npmjs.com/package/@pydantic/fastui-bootstrap&#34;&gt;&lt;code&gt;@pydantic/fastui-bootstrap&lt;/code&gt; npm package&lt;/a&gt; — implementation/customisation of all FastUI components using &lt;a href=&#34;https://getbootstrap.com&#34;&gt;Bootstrap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt&#34;&gt;&lt;code&gt;@pydantic/fastui-prebuilt&lt;/code&gt; npm package&lt;/a&gt; (available on &lt;a href=&#34;https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt&#34;&gt;jsdelivr.com CDN&lt;/a&gt;) providing a pre-built version of the FastUI React app so you can use it without installing any npm packages or building anything yourself. The Python package provides a simple HTML page to serve this app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s a simple but complete FastAPI application that uses FastUI to show some user profiles:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datetime import date&#xA;&#xA;from fastapi import FastAPI, HTTPException&#xA;from fastapi.responses import HTMLResponse&#xA;from fastui import FastUI, AnyComponent, prebuilt_html, components as c&#xA;from fastui.components.display import DisplayMode, DisplayLookup&#xA;from fastui.events import GoToEvent, BackEvent&#xA;from pydantic import BaseModel, Field&#xA;&#xA;app = FastAPI()&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    id: int&#xA;    name: str&#xA;    dob: date = Field(title=&#39;Date of Birth&#39;)&#xA;&#xA;&#xA;# define some users&#xA;users = [&#xA;    User(id=1, name=&#39;John&#39;, dob=date(1990, 1, 1)),&#xA;    User(id=2, name=&#39;Jack&#39;, dob=date(1991, 1, 1)),&#xA;    User(id=3, name=&#39;Jill&#39;, dob=date(1992, 1, 1)),&#xA;    User(id=4, name=&#39;Jane&#39;, dob=date(1993, 1, 1)),&#xA;]&#xA;&#xA;&#xA;@app.get(&#34;/api/&#34;, response_model=FastUI, response_model_exclude_none=True)&#xA;def users_table() -&amp;gt; list[AnyComponent]:&#xA;    &#34;&#34;&#34;&#xA;    Show a table of four users, `/api` is the endpoint the frontend will connect to&#xA;    when a user visits `/` to fetch components to render.&#xA;    &#34;&#34;&#34;&#xA;    return [&#xA;        c.Page(  # Page provides a basic container for components&#xA;            components=[&#xA;                c.Heading(text=&#39;Users&#39;, level=2),  # renders `&amp;lt;h2&amp;gt;Users&amp;lt;/h2&amp;gt;`&#xA;                c.Table(&#xA;                    data=users,&#xA;                    # define two columns for the table&#xA;                    columns=[&#xA;                        # the first is the users, name rendered as a link to their profile&#xA;                        DisplayLookup(field=&#39;name&#39;, on_click=GoToEvent(url=&#39;/user/{id}/&#39;)),&#xA;                        # the second is the date of birth, rendered as a date&#xA;                        DisplayLookup(field=&#39;dob&#39;, mode=DisplayMode.date),&#xA;                    ],&#xA;                ),&#xA;            ]&#xA;        ),&#xA;    ]&#xA;&#xA;&#xA;@app.get(&#34;/api/user/{user_id}/&#34;, response_model=FastUI, response_model_exclude_none=True)&#xA;def user_profile(user_id: int) -&amp;gt; list[AnyComponent]:&#xA;    &#34;&#34;&#34;&#xA;    User profile page, the frontend will fetch this when the user visits `/user/{id}/`.&#xA;    &#34;&#34;&#34;&#xA;    try:&#xA;        user = next(u for u in users if u.id == user_id)&#xA;    except StopIteration:&#xA;        raise HTTPException(status_code=404, detail=&#34;User not found&#34;)&#xA;    return [&#xA;        c.Page(&#xA;            components=[&#xA;                c.Heading(text=user.name, level=2),&#xA;                c.Link(components=[c.Text(text=&#39;Back&#39;)], on_click=BackEvent()),&#xA;                c.Details(data=user),&#xA;            ]&#xA;        ),&#xA;    ]&#xA;&#xA;&#xA;@app.get(&#39;/{path:path}&#39;)&#xA;async def html_landing() -&amp;gt; HTMLResponse:&#xA;    &#34;&#34;&#34;Simple HTML page which serves the React app, comes last as it matches all paths.&#34;&#34;&#34;&#xA;    return HTMLResponse(prebuilt_html(title=&#39;FastUI Demo&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which renders like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pydantic/FastUI/main/screenshot.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Of course, that&#39;s a very simple application, the &lt;a href=&#34;https://fastui-demo.onrender.com&#34;&gt;full demo&lt;/a&gt; is more complete.&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;p&gt;FastUI already defines a rich set of components.&lt;/p&gt; &#xA;&lt;p&gt;All components are listed in the &lt;a href=&#34;https://fastui-demo.onrender.com&#34;&gt;demo app&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Principle (long version)&lt;/h2&gt; &#xA;&lt;p&gt;FastUI is an implementation of the RESTful principle; but not as it&#39;s usually understood, instead I mean the principle defined in the original &lt;a href=&#34;https://ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm&#34;&gt;PhD dissertation&lt;/a&gt; by Roy Fielding, and excellently summarised in &lt;a href=&#34;https://htmx.org/essays/how-did-rest-come-to-mean-the-opposite-of-rest/&#34;&gt;this essay on htmx.org&lt;/a&gt; (HTMX people, I&#39;m sorry to use your article to promote React which I know you despise 🙏).&lt;/p&gt; &#xA;&lt;p&gt;The RESTful principle as described in the HTMX article is that the frontend doesn&#39;t need to (and shouldn&#39;t) know anything about the application you&#39;re building. Instead, it should just provide all the components you need to construct the interface, the backend can then tell the frontend what to do.&lt;/p&gt; &#xA;&lt;p&gt;Think of your frontend as a puppet, and the backend as the hand within it — the puppet doesn&#39;t need to know what to say, that&#39;s kind of the point.&lt;/p&gt; &#xA;&lt;p&gt;Building an application this way has a number of significant advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You only need to write code in one place to build a new feature — add a new view, change the behavior of an existing view or alter the URL structure&lt;/li&gt; &#xA; &lt;li&gt;Deploying the front and backend can be completely decoupled, provided the frontend knows how to render all the components the backend is going to ask it to use, you&#39;re good to go&lt;/li&gt; &#xA; &lt;li&gt;You should be able to reuse a rich set of opensource components, they should end up being better tested and more reliable than anything you could build yourself, this is possible because the components need no context about how they&#39;re going to be used (note: since FastUI is brand new, this isn&#39;t true yet, hopefully we get there)&lt;/li&gt; &#xA; &lt;li&gt;We can use Pydantic, TypeScript and JSON Schema to provide guarantees that the two sides are communicating with an agreed schema&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In the abstract, FastUI is like the opposite of GraphQL but with the same goal — GraphQL lets frontend developers extend an application without any new backend development; FastUI lets backend developers extend an application without any new frontend development.&lt;/p&gt; &#xA;&lt;h3&gt;Beyond Python and React&lt;/h3&gt; &#xA;&lt;p&gt;Of course, this principle shouldn&#39;t be limited to Python and React applications — provided we use the same set of agreed schemas and encoding to communicate, we should be able to use any frontend and backend that implements the schema. Interchangeably.&lt;/p&gt; &#xA;&lt;p&gt;This could mean:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implementing a web frontend using another JS framework like Vue — lots of work, limited value IMHO&lt;/li&gt; &#xA; &lt;li&gt;Implementing a web frontend using an edge server, so the browser just sees HTML — lots of work but very valuable&lt;/li&gt; &#xA; &lt;li&gt;Implementing frontends for other platforms like mobile or IOT — lots of work, no idea if it&#39;s actually a good idea?&lt;/li&gt; &#xA; &lt;li&gt;Implementing the component models in another language like Rust or Go — since there&#39;s actually not that much code in the backend, so this would be a relatively small and mechanical task&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>liguodongiot/llm-action</title>
    <updated>2024-03-04T01:34:11Z</updated>
    <id>tag:github.com,2024-03-04:/liguodongiot/llm-action</id>
    <link href="https://github.com/liguodongiot/llm-action" rel="alternate"></link>
    <summary type="html">&lt;p&gt;本项目旨在分享大模型相关技术原理以及实战经验。&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/llm-action-v3.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/liguodongiot/llm-action/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/liguodongiot/llm-action?style=social&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/wx.jpg&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/吃果冻不吐果冻皮-1AAD19.svg?style=plastic&amp;amp;logo=wechat&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.zhihu.com/people/liguodong-iot&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/吃果冻不吐果冻皮-0079FF.svg?style=plastic&amp;amp;logo=zhihu&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://juejin.cn/user/3642056016410728&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/掘金-吃果冻不吐果冻皮-000099.svg?style=plastic&amp;amp;logo=juejin&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://liguodong.blog.csdn.net/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/CSDN-吃果冻不吐果冻皮-6B238E.svg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E8%AE%AD%E7%BB%83&#34;&gt;LLM训练&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🐫 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98&#34;&gt;LLM训练实战&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;🐼 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86&#34;&gt;LLM参数高效微调技术原理综述&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;🐰 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98&#34;&gt;LLM参数高效微调技术实战&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;🐘 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%A1%8C%E6%8A%80%E6%9C%AF&#34;&gt;LLM分布式训练并行技术&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;🌋 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E5%88%86%E5%B8%83%E5%BC%8Fai%E6%A1%86%E6%9E%B6&#34;&gt;分布式AI框架&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;📡 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1&#34;&gt;分布式训练网络通信&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;🐎 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E6%8E%A8%E7%90%86&#34;&gt;LLM推理&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🚀 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6&#34;&gt;LLM推理框架&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;✈️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF&#34;&gt;LLM推理优化技术&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;♻️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%8E%8B%E7%BC%A9&#34;&gt;LLM压缩&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📐 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E9%87%8F%E5%8C%96&#34;&gt;LLM量化&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;🔰 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%89%AA%E6%9E%9D&#34;&gt;LLM剪枝&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;💹 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F&#34;&gt;LLM知识蒸馏&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;♑️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3&#34;&gt;低秩分解&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;♍️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E7%AE%97%E6%B3%95%E6%9E%B6%E6%9E%84&#34;&gt;LLM算法架构&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🧩&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91&#34;&gt;LLM应用开发&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🀄️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%9B%BD%E4%BA%A7%E5%8C%96%E9%80%82%E9%85%8D&#34;&gt;LLM国产化适配&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🔯 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#ai%E7%BC%96%E8%AF%91%E5%99%A8&#34;&gt;AI编译器&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🔘 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#ai%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD&#34;&gt;AI基础设施&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;💟 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llmops&#34;&gt;LLMOps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🍄 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E7%94%9F%E6%80%81%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF&#34;&gt;LLM生态相关技术&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🔨 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7&#34;&gt;服务器基础环境安装及常用工具&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;💬 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%AD%A6%E4%B9%A0%E4%BA%A4%E6%B5%81%E7%BE%A4&#34;&gt;LLM学习交流群&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;👥 &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7&#34;&gt;微信公众号&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;⭐️ &lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM训练&lt;/h2&gt; &#xA;&lt;h3&gt;LLM训练实战&lt;/h3&gt; &#xA;&lt;p&gt;下面汇总了我在大模型实践中训练相关的所有教程。从6B到65B，从全量微调到高效微调（LoRA，QLoRA，P-Tuning v2），再到RLHF（基于人工反馈的强化学习）。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLM&lt;/th&gt; &#xA;   &lt;th&gt;预训练/SFT/RLHF...&lt;/th&gt; &#xA;   &lt;th&gt;参数&lt;/th&gt; &#xA;   &lt;th&gt;教程&lt;/th&gt; &#xA;   &lt;th&gt;代码&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/618321077&#34;&gt;从0到1复现斯坦福羊驼（Stanford Alpaca 7B）&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/alpaca&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;7B~65B&lt;/td&gt; &#xA;   &lt;td&gt;1.&lt;a href=&#34;https://zhuanlan.zhihu.com/p/619426866&#34;&gt;足够惊艳，使用Alpaca-Lora基于LLaMA(7B)二十分钟完成微调，效果比肩斯坦福羊驼&lt;/a&gt;&lt;br&gt;2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/632492604&#34;&gt;使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/alpaca-lora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BELLE(LLaMA/Bloom)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1.&lt;a href=&#34;https://zhuanlan.zhihu.com/p/618876472&#34;&gt;基于LLaMA-7B/Bloomz-7B1-mt复现开源中文对话大模型BELLE及GPTQ量化&lt;/a&gt; &lt;br&gt; 2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/621128368&#34;&gt;BELLE(LLaMA-7B/Bloomz-7B1-mt)大模型使用GPTQ量化后推理性能测试&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/621793987&#34;&gt;从0到1基于ChatGLM-6B使用LoRA进行参数高效微调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/chatglm-lora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning/P-Tuning v2&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/622351059&#34;&gt;使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/chatglm&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/624012908&#34;&gt;大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;0.1B~66B&lt;/td&gt; &#xA;   &lt;td&gt;1.&lt;a href=&#34;https://zhuanlan.zhihu.com/p/626159553&#34;&gt;一键式 RLHF 训练 DeepSpeed Chat（一）：理论篇&lt;/a&gt;&amp;nbsp;&lt;br&gt; 2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/626214655&#34;&gt;一键式 RLHF 训练 DeepSpeed Chat（二）：实践篇&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/deepspeedchat&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniGPT-4(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/627671257&#34;&gt;大杀器，多模态大模型MiniGPT-4入坑指南&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Alpaca(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;LoRA（预训练+微调）&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/631360711&#34;&gt;中文LLaMA&amp;amp;Alpaca大语言模型词表扩充+预训练+指令精调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/chinese-llama-alpaca&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;7B/65B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636644164&#34;&gt;高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/qlora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLM微调技术原理&lt;/h3&gt; &#xA;&lt;p&gt;对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。&lt;/p&gt; &#xA;&lt;p&gt;因此，该技术值得我们进行深入分析其背后的机理，本系列大体分七篇文章进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635152813&#34;&gt;大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635686756&#34;&gt;大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635848732&#34;&gt;大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636038478&#34;&gt;大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636215898&#34;&gt;大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636362246&#34;&gt;大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/649755252&#34;&gt;大模型参数高效微调技术原理综述（七）-最佳实践、总结&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM微调实战&lt;/h3&gt; &#xA;&lt;p&gt;下面给大家分享&lt;strong&gt;大模型参数高效微调技术实战&lt;/strong&gt;，该系列主要针对 HuggingFace PEFT 框架支持的一些高效微调技术进行讲解。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;教程&lt;/th&gt; &#xA;   &lt;th&gt;代码&lt;/th&gt; &#xA;   &lt;th&gt;框架&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651744834&#34;&gt;大模型参数高效微调技术实战（一）-PEFT概述及环境搭建&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/646748939&#34;&gt;大模型参数高效微调技术实战（二）-Prompt Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/clm/peft_prompt_tuning_clm.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/646876256&#34;&gt;大模型参数高效微调技术实战（三）-P-Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/clm/peft_p_tuning_clm.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/648156780&#34;&gt;大模型参数高效微调技术实战（四）-Prefix Tuning / P-Tuning v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/clm/peft_p_tuning_v2_clm.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/649315197&#34;&gt;大模型参数高效微调技术实战（五）-LoRA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/clm/peft_lora_clm.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/649707359&#34;&gt;大模型参数高效微调技术实战（六）-IA3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/clm/peft_ia3_clm.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670048482&#34;&gt;大模型微调实战（七）-基于LoRA微调多模态大模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/multimodal/blip2_lora_int8_fine_tune.py&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HuggingFace PEFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670116171&#34;&gt;大模型微调实战（八）-使用INT8/FP4/NF4微调大模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/train/peft/multimodal/finetune_bloom_bnb_peft.ipynb&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PEFT、bitsandbytes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/distribution-parallelism&#34;&gt;LLM分布式训练并行技术&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，传统的单机单卡模式已经无法满足超大模型进行训练的要求。因此，我们需要基于单机多卡、甚至是多机多卡进行分布式大模型的训练。&lt;/p&gt; &#xA;&lt;p&gt;而利用AI集群，使深度学习算法更好地从大量数据中高效地训练出性能优良的大模型是分布式机器学习的首要目标。为了实现该目标，一般需要根据硬件资源与数据/模型规模的匹配情况，考虑对计算任务、训练数据和模型进行划分，从而进行分布式训练。因此，分布式训练相关技术值得我们进行深入分析其背后的机理。&lt;/p&gt; &#xA;&lt;p&gt;下面主要对大模型进行分布式训练的并行技术进行讲解，本系列大体分九篇文章进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/598714869&#34;&gt;大模型分布式训练并行技术（一）-概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650002268&#34;&gt;大模型分布式训练并行技术（二）-数据并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/653860567&#34;&gt;大模型分布式训练并行技术（三）-流水线并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/657921100&#34;&gt;大模型分布式训练并行技术（四）-张量并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/659792351&#34;&gt;大模型分布式训练并行技术（五）-序列并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/661279318&#34;&gt;大模型分布式训练并行技术（六）-多维混合并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/662517647&#34;&gt;大模型分布式训练并行技术（七）-自动并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/662518387&#34;&gt;大模型分布式训练并行技术（八）-MOE并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7290740395913969705&#34;&gt;大模型分布式训练并行技术（九）-总结&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;分布式AI框架&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/pytorch/&#34;&gt;PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PyTorch 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;PyTorch 多机多卡训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/megatron&#34;&gt;Megatron-LM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Megatron-LM 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;Megatron-LM 多机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7259682893648724029&#34;&gt;基于Megatron-LM从0到1完成GPT2模型预训练、模型评估及推理&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/deepspeed&#34;&gt;DeepSpeed&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DeepSpeed 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;DeepSpeed 多机多卡训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/megatron-deepspeed&#34;&gt;Megatron-DeepSpeed&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;基于 Megatron-DeepSpeed 从 0 到1 完成 LLaMA 预训练&lt;/li&gt; &#xA;   &lt;li&gt;基于 Megatron-DeepSpeed 从 0 到1 完成 Bloom 预训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;分布式训练网络通信&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/inference&#34;&gt;LLM推理&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;LLM推理框架&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/625415776/answer/3243562246&#34;&gt;大模型推理框架概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/626008090&#34;&gt;大模型的好伙伴，浅析推理加速引擎FasterTransformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629336492&#34;&gt;模型推理服务化框架Triton保姆式教程（一）：快速入门&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/634143650&#34;&gt;模型推理服务化框架Triton保姆式教程（二）：架构解析&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/634444666&#34;&gt;模型推理服务化框架Triton保姆式教程（三）：开发实践&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/666849728&#34;&gt;TensorRT-LLM保姆级教程（一）-快速入门&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/667572720&#34;&gt;TensorRT-LLM保姆级教程（二）-开发实践&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorRT-LLM保姆级教程（三）-基于Triton完成模型服务化&lt;/li&gt; &#xA; &lt;li&gt;TensorRT-LLM保姆级教程（四）-新模型适配&lt;/li&gt; &#xA; &lt;li&gt;TensorRT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM推理优化技术&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;LLM推理优化技术概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PageAttention&lt;/li&gt; &#xA; &lt;li&gt;FlashAttention&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM压缩&lt;/h2&gt; &#xA;&lt;p&gt;近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，从而导致模型变得越来越大，因此，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。 模型压缩主要分为如下几类：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;剪枝（Pruning）&lt;/li&gt; &#xA; &lt;li&gt;知识蒸馏（Knowledge Distillation）&lt;/li&gt; &#xA; &lt;li&gt;量化&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/model-compression/quantization&#34;&gt;LLM量化&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;本系列将针对一些常见大模型量化方案（GPTQ、LLM.int8()、SmoothQuant、AWQ等）进行讲述。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/627484732/answer/3261671478&#34;&gt;大模型量化概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;量化感知训练： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/647589650&#34;&gt;大模型量化感知训练技术原理：LLM-QAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;&#34;&gt;大模型量化感知微调技术原理：QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;PEQA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;训练后量化： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/680212402&#34;&gt;大模型量化技术原理：GPTQ、LLM.int8()&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/576376372/answer/3388402085&#34;&gt;大模型量化技术原理：SmoothQuant&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/681578090&#34;&gt;大模型量化技术原理：AWQ、AutoAWQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/682871823&#34;&gt;大模型量化技术原理：SpQR&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/683813769&#34;&gt;大模型量化技术原理：ZeroQuant系列&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;大模型量化技术原理：总结&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM剪枝&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;结构化剪枝&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM-Pruner&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;非结构化剪枝&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SparseGPT&lt;/li&gt; &#xA; &lt;li&gt;LoRAPrune&lt;/li&gt; &#xA; &lt;li&gt;Wanda&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM知识蒸馏&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/625415893/answer/3243565375&#34;&gt;大模型知识蒸馏概述&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Standard KD&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;使学生模型学习教师模型(LLM)所拥有的常见知识，如输出分布和特征信息，这种方法类似于传统的KD。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MINILLM&lt;/li&gt; &#xA; &lt;li&gt;GKD&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;EA-based KD&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;不仅仅是将LLM的常见知识转移到学生模型中，还涵盖了蒸馏它们独特的涌现能力。具体来说，EA-based KD又分为了上下文学习（ICL）、思维链（CoT）和指令跟随（IF）。&lt;/p&gt; &#xA;&lt;p&gt;In-Context Learning：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In-Context Learning distillation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Chain-of-Thought：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MT-COT&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune-CoT&lt;/li&gt; &#xA; &lt;li&gt;DISCO&lt;/li&gt; &#xA; &lt;li&gt;SCOTT&lt;/li&gt; &#xA; &lt;li&gt;SOCRATIC CoT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Instruction Following：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;低秩分解&lt;/h3&gt; &#xA;&lt;p&gt;低秩分解旨在通过将给定的权重矩阵分解成两个或多个较小维度的矩阵，从而对其进行近似。低秩分解背后的核心思想是找到一个大的权重矩阵W的分解，得到两个矩阵U和V，使得W≈U V，其中U是一个m×k矩阵，V是一个k×n矩阵，其中k远小于m和n。U和V的乘积近似于原始的权重矩阵，从而大幅减少了参数数量和计算开销。&lt;/p&gt; &#xA;&lt;p&gt;在LLM研究的模型压缩领域，研究人员通常将多种技术与低秩分解相结合，包括修剪、量化等。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZeroQuant-FP（低秩分解+量化）&lt;/li&gt; &#xA; &lt;li&gt;LoRAPrune（低秩分解+剪枝）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/ai-algo&#34;&gt;LLM算法架构&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/600016134&#34;&gt;大模型算法演进&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM / ChatGLM2 / ChatGLM3 大模型解析&lt;/li&gt; &#xA; &lt;li&gt;Bloom 大模型解析&lt;/li&gt; &#xA; &lt;li&gt;LLaMA / LLaMA2 大模型解析&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/606757218/answer/3075464500&#34;&gt;百川智能开源大模型baichuan-7B技术剖析&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/611507751/answer/3114988669&#34;&gt;百川智能开源大模型baichuan-13B技术剖析&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM应用开发&lt;/h2&gt; &#xA;&lt;p&gt;大模型是基座，要想让其变成一款产品，我们还需要一些其他相关的技术，比如：向量数据库（Pinecone、Milvus、Vespa、Weaviate），LangChain等。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/476025527&#34;&gt;云原生向量数据库Milvus（一）-简述、系统架构及应用场景&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/477231485&#34;&gt;云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651921120&#34;&gt;关于大模型驱动的AI智能体Agent的一些思考&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/docs/llm_localization&#34;&gt;LLM国产化适配&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;随着 ChatGPT 的现象级走红，引领了AI大模型时代的变革，从而导致 AI 算力日益紧缺。与此同时，中美贸易战以及美国对华进行AI芯片相关的制裁导致 AI 算力的国产化适配势在必行。本系列将对一些国产化 AI 加速卡进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/637918406&#34;&gt;大模型国产化适配1-华为昇腾AI全栈软硬件平台总结&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650730807&#34;&gt;大模型国产化适配2-基于昇腾910使用ChatGLM-6B进行模型推理&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651324599&#34;&gt;大模型国产化适配3-基于昇腾910使用ChatGLM-6B进行模型训练&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655902796&#34;&gt;大模型国产化适配4-基于昇腾910使用LLaMA-13B进行多机多卡训练&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7291513759470960679&#34;&gt;大模型国产化适配5-百度飞浆PaddleNLP大语言模型工具链总结&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/677799157&#34;&gt;大模型国产化适配6-基于昇腾910B快速验证ChatGLM3-6B/BaiChuan2-7B模型推理&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/ai-compiler&#34;&gt;AI编译器&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;AI编译器是指将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669347560&#34;&gt;AI编译器技术剖析（一）-概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/671477784&#34;&gt;AI编译器技术剖析（二）-传统编译器&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/676723324&#34;&gt;AI编译器技术剖析（三）-树模型编译工具 Treelite 详解&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;AI编译器技术剖析（四）-编译器前端&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;AI编译器技术剖析（五）-编译器后端&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;AI编译器技术剖析（六）-主流编译框架&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;AI编译器技术剖析（七）-深度学习模型编译优化&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/672584013&#34;&gt;lleaves：使用 LLVM 编译梯度提升决策树将预测速度提升10+倍&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;框架：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MLIR&lt;/li&gt; &#xA; &lt;li&gt;XLA&lt;/li&gt; &#xA; &lt;li&gt;TVM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI基础设施&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7311604023184162835&#34;&gt;AI 集群基础设施 NVMe SSD 详解&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/673903240&#34;&gt;AI 集群基础设施 InfiniBand 详解&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;大模型训练基础设施：算力篇&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AI加速卡&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/667686665&#34;&gt;AI芯片技术原理剖析（一）：国内外AI芯片概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AI芯片技术原理剖析（二）：英伟达GPU&lt;/li&gt; &#xA; &lt;li&gt;AI芯片技术原理剖析（三）：谷歌TPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AI集群&lt;/h3&gt; &#xA;&lt;p&gt;待更新...&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/network-communication&#34;&gt;AI集群网络通信&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;待更新...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;分布式训练网络通讯原语&lt;/li&gt; &#xA; &lt;li&gt;AI 集群通信软硬件&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLMOps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/676389726&#34;&gt;在 Kubernetes 上部署机器学习模型的指南&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7320513026188099619&#34;&gt;使用 Kubernetes 部署机器学习模型的优势&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM生态相关技术&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/630696264&#34;&gt;大模型词表扩充必备工具SentencePiece&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/601594836/answer/3032763174&#34;&gt;大模型实践总结&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/604393963/answer/3061358152&#34;&gt;ChatGLM 和 ChatGPT 的技术区别在哪里？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/602504880/answer/3041965998&#34;&gt;现在为什么那么多人以清华大学的ChatGLM-6B为基座进行试验？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/616600181/answer/3195333332&#34;&gt;为什么很多新发布的大模型默认使用BF16而不是FP16？&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;服务器基础环境安装及常用工具&lt;/h2&gt; &#xA;&lt;p&gt;基础环境安装：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm-base/a800-env-install.md&#34;&gt;英伟达A800加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm-base/h800-env-install.md&#34;&gt;英伟达H800加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm_localization/ascend910-env-install.md&#34;&gt;昇腾910加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;常用工具：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/6992742028605915150&#34;&gt;Linux 常见命令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7089093437223338015&#34;&gt;Conda 常用命令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/6999405667261874183&#34;&gt;Poetry 常用命令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7016238524286861325&#34;&gt;Docker 常用命令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7016595442062327844&#34;&gt;Docker Dockerfile 指令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7031201391553019911&#34;&gt;Kubernetes 常用命令大全&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm-base/dcgmi.md&#34;&gt;集群环境 GPU 管理和监控工具 DCGM 常用命令大全&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM学习交流群&lt;/h2&gt; &#xA;&lt;p&gt;我创建了大模型学习交流群，供大家一起学习交流大模型相关的最新技术，目前已有5个群，每个群都有上百人的规模，&lt;strong&gt;可加我微信进群&lt;/strong&gt;（加微信请备注来意，如：进大模型学习交流群+GitHub）。&lt;strong&gt;一定要备注哟，否则不予通过&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;PS：&lt;strong&gt;成都有个本地大模型交流群，想进可以另外单独备注下。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/wx.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;微信公众号&lt;/h2&gt; &#xA;&lt;p&gt;微信公众号：&lt;strong&gt;吃果冻不吐果冻皮&lt;/strong&gt;，该公众号主要分享AI工程化（大模型、MLOps等）相关实践经验，免费电子书籍、论文等。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/wx-gzh.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E7%9B%AE%E5%BD%95&#34;&gt;⬆ 一键返回目录&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#liguodongiot/llm-action&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=liguodongiot/llm-action&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>