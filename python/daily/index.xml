<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-22T01:34:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/PromptWizard</title>
    <updated>2024-12-22T01:34:28Z</updated>
    <id>tag:github.com,2024-12-22:/microsoft/PromptWizard</id>
    <link href="https://github.com/microsoft/PromptWizard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Task-Aware Agent-driven Prompt Optimization Framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PromptWizard üßô&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.18369&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/arXiv-2409.10566-b31b1b.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/msr_blog.png&#34; width=&#34;16&#34;&gt; Blog Post &lt;/a&gt; &lt;a href=&#34;https://microsoft.github.io/PromptWizard/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/github.png&#34; width=&#34;16&#34;&gt; Project Website &lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;PromptWizard: Task-Aware Prompt Optimization Framework&lt;/strong&gt;&lt;br&gt; Eshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, Akshay Nambi &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Overview üåü&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Overview of the PromptWizard framework&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/overview.png&#34;&gt; &#xA;&lt;p&gt;PromptWizard is a discrete prompt optimization framework that employs a self-evolving mechanism where the LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis. This self-adaptive approach ensures holistic optimization by evolving both the instructions and in-context learning examples for better task performance.&lt;/p&gt; &#xA;&lt;p&gt;Three key components of PromptWizard are te following :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feedback-driven Refinement: LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis‚Äã&lt;/li&gt; &#xA; &lt;li&gt;Critique and Synthesize diverse examples: Generates synthetic examples that are robust, diverse and task-aware. Also it optimizes both prompt and examples in tandem‚Äã&lt;/li&gt; &#xA; &lt;li&gt;Self generated Chain of Thought (CoT) steps with combination of positive, negative and synthetic examples&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Stage 1: Iterative optimization of instructions&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/iterative_flowchart-1.png&#34; width=&#34;49.5%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Stage 2: Sequential optimization of instruction and examples&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/sequential_flowchart-1.png&#34; width=&#34;49.5%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation ‚¨áÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up the development environment and install the package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/microsoft/PromptWizard&#xA;cd PromptWizard&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create and activate a virtual environment&lt;/p&gt; &lt;p&gt;On Windows&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS/Linux:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the package in development mode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quickstart üèÉ&lt;/h2&gt; &#xA;&lt;p&gt;There are three main ways to use PromptWizard:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scenario 1 : Optimizing prompts without examples&lt;/li&gt; &#xA; &lt;li&gt;Scenario 2 : Generating synthetic examples and using them to optimize prompts&lt;/li&gt; &#xA; &lt;li&gt;Scenario 3 : Optimizing prompts with training data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; : Refer this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/scenarios/dataset_scenarios_demo.ipynb&#34;&gt;notebook&lt;/a&gt; to get a detailed understanding of the usage for each of the scenarios. &lt;strong&gt;This serves as a starting point to understand the usage of PromptWizard&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;High level overview of using PromptWizard&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Decide your scenario&lt;/li&gt; &#xA; &lt;li&gt;Fix the configuration and environmental varibles for API calling &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use &lt;code&gt;promptopt_config.yaml&lt;/code&gt; to set configurations. For example for GSM8k this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;file&lt;/a&gt; can be used&lt;/li&gt; &#xA;   &lt;li&gt;Use &lt;code&gt;.env&lt;/code&gt; to set environmental varibles. For GSM8k this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/.env&#34;&gt;file&lt;/a&gt; can be used&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;AZURE_OPENAI_ENDPOINT=&#34;XXXXX&#34; &#xA;# Replace with your Azure OpenAI Endpoint&#xA;&#xA;OPENAI_API_VERSION=&#34;XXXX&#34;&#xA;# Replace with the version of your API&#xA;&#xA;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=&#34;XXXXX&#34;&#xA;# Create a deployment for the model and place the deployment name here. &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the code &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To run PromptWizard on your custom dataset please jump &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/#run-on-custom-dataset&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running PromptWizard with training data (Scenario 3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We support &lt;a href=&#34;https://huggingface.co/datasets/openai/gsm8k&#34;&gt;GSM8k&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ChilleD/SVAMP&#34;&gt;SVAMP&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/deepmind/aqua_rat&#34;&gt;AQUARAT&lt;/a&gt; and &lt;a href=&#34;https://github.com/xqlin98/INSTINCT/tree/main/Induction/experiments/data/instruction_induction/raw&#34;&gt;Instruction_Induction(BBII)&lt;/a&gt; datasets&lt;/li&gt; &#xA; &lt;li&gt;Please note that time taken for prompt optimzation is dependent on the dataset. In our experiments for the above mentioned datasets, it took around 20 - 30 minutes on average.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running on GSM8k (AQUARAT/SVAMP)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please note that this code requires access to LLMs via API calling, we use AZURE endpoints for this&lt;/li&gt; &#xA; &lt;li&gt;Set the AZURE endpoint configurations in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/.env&#34;&gt;.env&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the steps in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt; to download the data, run the prompt optimization and carry out inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running on BBII&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BBII has many datasets in it, based on the dataset set the configs &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/configs/promptopt_config.yaml&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In configs &lt;code&gt;task_description&lt;/code&gt;,&lt;code&gt;base_instruction&lt;/code&gt; and &lt;code&gt;answer_format&lt;/code&gt; need to be changed for different datasets in BBII, the rest of the configs remain the same&lt;/li&gt; &#xA; &lt;li&gt;A demo is presented in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run on Custom Datasets üóÉÔ∏è&lt;/h2&gt; &#xA;&lt;h3&gt;Create Custom Dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our code expects the dataset to be in &lt;code&gt;.jsonl&lt;/code&gt; file format&lt;/li&gt; &#xA; &lt;li&gt;Both the train and test set follow the same format&lt;/li&gt; &#xA; &lt;li&gt;Every sample in the &lt;code&gt;.jsonl&lt;/code&gt; should have 2 fields : &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;question&lt;/code&gt; : It should contain the complete question that is to asked to the LLM&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;answer&lt;/code&gt; : It should contain the ground truth answer which can be verbose or consize&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run on Custom Dataset&lt;/h3&gt; &#xA;&lt;p&gt;NOTE : Refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos&#34;&gt;demos&lt;/a&gt; folder for examples of folders for four datasets. The &lt;code&gt;.ipynb&lt;/code&gt; in each of the folders shows how to run PromptWizard on that particular dataset. A similar procedure can be followed for a new dataset. Below is the explanation of each of the components of the &lt;code&gt;.ipynb&lt;/code&gt; and the dataset specifc folder structure in detail&lt;/p&gt; &#xA;&lt;h4&gt;Steps to be followed for custom datasets&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Every new dataset needs to have the following&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;configs&lt;/code&gt; folder to store files for defining optimization hyperparameters and setup configs&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;data&lt;/code&gt; folder to store &lt;code&gt;train.jsonl&lt;/code&gt; and &lt;code&gt;test.jsonl&lt;/code&gt; as curated &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/#create-custom-dataset&#34;&gt;here&lt;/a&gt; (this is done in the notebooks)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; file for environment varibles to be used for API calling&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.py/.ipynb&lt;/code&gt; script to run the code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the hyperparameters like number of mutations, refine steps, in-context examples etc.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set the following in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt; : &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;task_description&lt;/code&gt; : Desciption of the task at hand which will be fed into the prompt&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;For GSM8k a description like the following can be used &lt;pre&gt;&lt;code&gt;You are a mathematics expert. You will be given a mathematics problem which you need to solve&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;base_instruction&lt;/code&gt; : Base instruction in line with the dataset&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;A commonly used base instruction could be &lt;pre&gt;&lt;code&gt;Lets think step by step.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;answer_format&lt;/code&gt; : Instruction for specifying the answer format&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;It is crucial to set the &lt;code&gt;answer_format&lt;/code&gt; properly to ensure correct extraction by &lt;code&gt;def extract_final_answer()&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;Answer format could be : &lt;pre&gt;&lt;code&gt;At the end, wrap only your final option between &amp;lt;ANS_START&amp;gt; and &amp;lt;ANS_END&amp;gt; tags&#xA;&lt;/code&gt;&lt;/pre&gt; Then in &lt;code&gt;def extract_final_answer()&lt;/code&gt; we can simply write code to extract string between the tags&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;seen_set_size&lt;/code&gt; : The number of train samples to be used for prompt optimization&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;In our experiments we set this to be 25. In general any number between 20-50 would work&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;few_shot_count&lt;/code&gt; : The number of in-context examples needed in the prompt&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;The value can be set to any positive integer based on the requirement&lt;/li&gt; &#xA;       &lt;li&gt;For generating zero-shot prompts, set the values to a small number (i.e between 2-5) and after the final prompt is generated the in-context examples can be removed. We suggest using some in-context examples as during the optimization process the instructions in the prompt are refined using in-context examples hence setting it to a small number will give better zero-shot instructions in the prompt&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;generate_reasoning&lt;/code&gt; : Whether or not to generate reasoning for the in-context examples&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;In our experiments we found it to improve the prompt overall as it provides a step-by-step approach to reach the final answer. However if there is a constraint on the prompt length or number of prompt tokens, it can be turned off to get smaller sized prompts&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;generate_expert_identity&lt;/code&gt; and &lt;code&gt;generate_intent_keywords&lt;/code&gt; : Having these helped improve the prompt as they help making the prompt relevant to the task&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Refer &lt;code&gt;promptopt_config.yaml&lt;/code&gt; files in folders present &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos&#34;&gt;here&lt;/a&gt; for the descriptions used for AQUARAT, SVAMP and GSM8k. For BBII refer &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/description.py&#34;&gt;description.py&lt;/a&gt; which has the meta instructions for each of the datasets&lt;/li&gt; &#xA;   &lt;li&gt;Following are the global parameters which can be set based on the availability of the training data &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;run_without_train_examples&lt;/code&gt; is a global hyperparameter which can be used when there are no training samples and in-context examples are not required in the final prompt&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;generate_synthetic_examples&lt;/code&gt; is a global hyperparameter which can be used when there are no training samples and we want to generate synthetic data for training&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;use_examples&lt;/code&gt; is a global hyperparameter which can be used to optimize prompts using training data&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a dataset specific class which inherits &lt;code&gt;class DatasetSpecificProcessing&lt;/code&gt; similar to &lt;code&gt;GSM8k(DatasetSpecificProcessing)&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt; and define the following functions in it&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;In &lt;code&gt;def extract_answer_from_output()&lt;/code&gt; : This is a dataset specific function, given the &lt;code&gt;answer&lt;/code&gt; from the dataset it should extract and return a consize form of the answer. Note that based on the dataset it can also simply return the &lt;code&gt;answer&lt;/code&gt; as it is like in case of SVAMP and AQUARAT datasets&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;def extract_final_answer()&lt;/code&gt; : This is a LLM output specific function, given the verbose answer from the LLM it should extract and return the consize final answer&lt;/li&gt; &#xA;   &lt;li&gt;Define &lt;code&gt;def access_answer()&lt;/code&gt; : This function takes an input the LLM output, then does the following: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Extracts the consize answer using &lt;code&gt;def extract_final_answer()&lt;/code&gt; from the LLM output as defined above&lt;/li&gt; &#xA;     &lt;li&gt;Evaluates the extracted answer with the ground truth and retuns &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Extracted answer from LLM output&lt;/li&gt; &#xA;       &lt;li&gt;Boolean value indicating if answer is correct or not&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;The evaluation done here is dataset specific, for datasets like GSM8k, SVAMP and AQUARAT which have final answer as an number, we can do a direct match between the numbers generated and the ground truth, while for datasets where the answer is a sentence or paragraph it would be better to do evaluation with llm-as-a-judge, to compare the generated and ground truth paragraph/sentence. An example is available in &lt;code&gt;def access_answer()&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/demo.ipynb&#34;&gt;this&lt;/a&gt; notebook&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How PromptWizard Works üîç&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using the problem description and initial prompt instruction, PW generates variations of the instruction by prompting LLMs to mutate it. Based on performance, the best prompt is selected. PW incorporates a critique component that provides feedback, thus guiding and refining the prompt over multiple iterations.&lt;/li&gt; &#xA; &lt;li&gt;PW also optimizes in-context examples. PW selects a diverse set of examples from the training data, identifying positive and negative examples based on their performance with the modified prompt. Negative examples help inform further prompt refinements.&lt;/li&gt; &#xA; &lt;li&gt;Examples and instructions are sequentially optimized, using the critique to generate synthetic examples that address the current prompt‚Äôs weaknesses. These examples are integrated to further refine the prompt.&lt;/li&gt; &#xA; &lt;li&gt;PW generates detailed reasoning chains via Chain-of-Thought (CoT), enriching the prompt‚Äôs capacity for problem-solving.&lt;/li&gt; &#xA; &lt;li&gt;PW aligns prompts with human reasoning by integrating task intent and expert personas, enhancing both model performance and interpretability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configurations ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Here we define the various hyperparameters used in prompt optimization process found in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mutate_refine_iterations&lt;/code&gt;: Number of iterations for conducting mutation of task description followed by refinement of instructions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mutation_rounds&lt;/code&gt;: Number of rounds of mutation to be performed when generating different styles&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;refine_task_eg_iterations&lt;/code&gt;: Number of iterations for refining task description and in context examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;style_variation&lt;/code&gt;: Number of thinking style variations to be used in prompt mutation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;questions_batch_size&lt;/code&gt;: Number of questions to be asked to LLM in a single batch, during training step&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_correct_count&lt;/code&gt;: Minimum number of batches of questions to correctly answered, for a prompt to be considered as performing good&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_eval_batches&lt;/code&gt;: Maximum number of mini-batches on which we should evaluate the prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_n&lt;/code&gt;: Number of top best prompts to be considered from scoring stage for the next stage&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seen_set_size&lt;/code&gt;: Number of samples from trainset to be used for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;few_shot_count&lt;/code&gt;: Number of in-context examples required in final prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Best Practices üí°&lt;/h2&gt; &#xA;&lt;p&gt;Following are some of best pracitices we followed during are experiments&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Regarding the parameters in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We found the best performing values for &lt;code&gt;mutate_refine_iterations&lt;/code&gt;,&lt;code&gt;mutation_rounds&lt;/code&gt;,&lt;code&gt;refine_task_eg_iterations&lt;/code&gt; to be 3 or 5&lt;/li&gt; &#xA;   &lt;li&gt;Other parameters have been set to their ideal values. &lt;code&gt;seen_set_size&lt;/code&gt; can be increased to 50 and &lt;code&gt;few_shot_count&lt;/code&gt; can be set based on the use case&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The prompts generated at the end of the training process are usually very detailed, however user supervision can help tune it further for the task at hand&lt;/li&gt; &#xA; &lt;li&gt;Trying both configurations of having synthetic in-context examples or in-context examples from the train set can be tried to find the best prompt based on use case.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results üìà&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/curve.png&#34; width=&#34;45%&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;PromptWizard consistently outperforms other methods across various thresholds, maintaining the highest p(œÑ) values, indicating that it consistently performs near the best possible accuracy across all tasks&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The fiqure shows the performance profile curve for the instruction induction tasks. The performance profile curve visualizes how frequently different approaches‚Äô performance is within a given distance of the best performance. In this curve, the x-axis (œÑ) represents the performance ratio relative to the best-performing method, and the y-axis (p(œÑ )) reflects the fraction of tasks where a method‚Äôs performance is within this ratio. So for a given method, the curve tells what percentage of the tasks are within œÑ distance to the best performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute: ‚úã&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;. When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Citation üìù&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{agarwal2024promptwizardtaskawarepromptoptimization,&#xA;      title={PromptWizard: Task-Aware Prompt Optimization Framework}, &#xA;      author={Eshaan Agarwal and Joykirat Singh and Vivek Dani and Raghav Magazine and Tanuja Ganu and Akshay Nambi},&#xA;      year={2024},&#xA;      eprint={2405.18369},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2405.18369}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Responsible AI Considerations&lt;/h2&gt; &#xA;&lt;p&gt;For guidelines and best practices related to Responsible AI, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/RESPONSIBLE_AI.md&#34;&gt;Responsible AI Guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AnswerDotAI/ModernBERT</title>
    <updated>2024-12-22T01:34:28Z</updated>
    <id>tag:github.com,2024-12-22:/AnswerDotAI/ModernBERT</id>
    <link href="https://github.com/AnswerDotAI/ModernBERT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing BERT into modernity via both architecture changes and scaling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome!&lt;/h1&gt; &#xA;&lt;p&gt;This is the repository where you can find ModernBERT, our experiments to bring BERT into modernity via both architecture changes and scaling.&lt;/p&gt; &#xA;&lt;p&gt;This repository noticeably introduces FlexBERT, our modular approach to encoder building blocks, and heavily relies on .yaml configuration files to build models. The codebase builds upon &lt;a href=&#34;https://github.com/mosaicml/examples/tree/main/examples/benchmarks/bert&#34;&gt;MosaicBERT&lt;/a&gt;, and specifically the &lt;a href=&#34;https://github.com/Skylion007/mosaicml-examples/tree/skylion007/add-fa2-to-bert&#34;&gt;unmerged fork bringing Flash Attention 2&lt;/a&gt; to it, under the terms of its Apache 2.0 license. We extend our thanks to MosaicML for starting the work on modernising encoders!&lt;/p&gt; &#xA;&lt;p&gt;This README is very barebones and is still under construction. It will improve with more reproducibility and documentation in the new year, as we gear up for more encoder niceties after the pre-holidays release of ModernBERT. For now, we&#39;re mostly looking forward to seeing what people build with the &lt;a href=&#34;https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb&#34;&gt;ü§ó model checkpoints&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;For more details on what this repository brings, we recommend reading our &lt;a href=&#34;https://huggingface.co/blog/modernbert&#34;&gt;release blog post&lt;/a&gt; for a high-level overview, and our &lt;a href=&#34;https://arxiv.org/abs/2412.13663&#34;&gt;arXiv preprint&lt;/a&gt; for more technical details.&lt;/p&gt; &#xA;&lt;p&gt;All code used in this repository is the code used as part of our experiments for both pre-training and GLUE evaluations, there&#39;s no uncommitted secret training sauce.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is the research repository for ModernBERT, focused on pre-training and evaluations. If you&#39;re seeking the HuggingFace version, designed to integrate with any common pipeline, please head to the &lt;a href=&#34;https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb&#34;&gt;ModernBERT Collection on HuggingFace&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;ModernBERT is a collaboration between &lt;a href=&#34;https://answer.ai&#34;&gt;Answer.AI&lt;/a&gt;, &lt;a href=&#34;https://lighton.ai&#34;&gt;LightOn&lt;/a&gt;, and friends.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We have fully documented the environment used to train ModernBERT, which can be installed on a GPU-equipped machine with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yaml&#xA;# if the conda environment errors out set channel priority to flexible:&#xA;# conda config --set channel_priority flexible&#xA;conda activate bert24&#xA;# if using H100s clone and build flash attention 3&#xA;# git clone https://github.com/Dao-AILab/flash-attention.git&#xA;# cd flash-attention/hopper&#xA;# python setup.py install&#xA;# install flash attention 2 (model uses FA3+FA2 or just FA2 if FA3 isn&#39;t supported)&#xA;pip install &#34;flash_attn==2.6.3&#34; --no-build-isolation&#xA;# or download a precompiled wheel from https://github.com/Dao-AILab/flash-attention/releases/tag/v2.6.3&#xA;# or limit the number of parallel compilation jobs&#xA;# MAX_JOBS=8 pip install &#34;flash_attn==2.6.3&#34; --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Training heavily leverages the &lt;a href=&#34;https://github.com/mosaicml/composer&#34;&gt;composer&lt;/a&gt; framework. All training are configured via YAML files, of which you can find examples in the &lt;code&gt;yamls&lt;/code&gt; folder. We highly encourage you to check out one of the example yamls, such as &lt;code&gt;yamls/main/flex-bert-rope-base.yaml&lt;/code&gt;, to explore the configuration options.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;GLUE&lt;/h3&gt; &#xA;&lt;p&gt;GLUE evaluations for a ModernBERT model trained with this repository can be ran with via &lt;code&gt;run_evals.py&lt;/code&gt;, by providing it with a checkpoint and a training config. To evaluate non-ModernBERT models, you should use &lt;code&gt;glue.py&lt;/code&gt; in conjunction with a slightly different training YAML, of which you can find examples in the &lt;code&gt;yamls/finetuning&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Retrieval&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;examples&lt;/code&gt; subfolder contains scripts for training retrieval models, both dense models based on &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;Sentence Transformers&lt;/a&gt; and ColBERT models via the &lt;a href=&#34;https://github.com/lightonai/pylate&#34;&gt;PyLate&lt;/a&gt; library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/train_pylate.py&lt;/code&gt;: The boilerplate code to train a ModernBERT-based ColBERT model with PyLate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/train_st.py&lt;/code&gt;: The boilerplate code to train a ModernBERT-based dense retrieval model with Sentence Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/evaluate_pylate.py&lt;/code&gt;: The boilerplate code to evaluate a ModernBERT-based ColBERT model with PyLate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/evaluate_st.py&lt;/code&gt;: The boilerplate code to evaluate a ModernBERT-based dense retrieval model with Sentence Transformers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you use ModernBERT in your work, be it the released models, the intermediate checkpoints (release pending) or this training repository, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{modernbert,&#xA;      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, &#xA;      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},&#xA;      year={2024},&#xA;      eprint={2412.13663},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2412.13663}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Genesis-Embodied-AI/Genesis</title>
    <updated>2024-12-22T01:34:28Z</updated>
    <id>tag:github.com,2024-12-22:/Genesis-Embodied-AI/Genesis</id>
    <link href="https://github.com/Genesis-Embodied-AI/Genesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generative world for general-purpose robotics &amp; embodied AI learning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/big_text.png&#34; alt=&#34;Genesis&#34; width=&#34;85%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/genesis-world/&#34;&gt; &lt;img alt=&#34;PyPI - Version&#34; src=&#34;https://img.shields.io/pypi/v/genesis-world&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/genesis-world/&#34;&gt; &lt;img alt=&#34;PyPI - Downloads&#34; src=&#34;https://img.shields.io/pypi/dm/genesis-world&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/issues&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/discussions&#34;&gt; &lt;img alt=&#34;GitHub Discussions&#34; src=&#34;https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README.md&#34;&gt;&lt;img alt=&#34;README in English&#34; src=&#34;https://img.shields.io/badge/English-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_CN.md&#34;&gt;&lt;img alt=&#34;ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂&#34; src=&#34;https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;What is Genesis?&lt;/h1&gt; &#xA;&lt;p&gt;Genesis is a physics platform designed for general purpose &lt;em&gt;Robotics/Embodied AI/Physical AI&lt;/em&gt; applications. It is simultaneously multiple things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A &lt;strong&gt;universal physics engine&lt;/strong&gt; re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;ultra-fast&lt;/strong&gt;, &lt;strong&gt;pythonic&lt;/strong&gt;, and &lt;strong&gt;user-friendly&lt;/strong&gt; robotics simulation platform.&lt;/li&gt; &#xA; &lt;li&gt;A powerful and fast &lt;strong&gt;photo-realistic rendering system&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;generative data engine&lt;/strong&gt; that transforms user-prompted natural language description into various modalities of data.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully &lt;strong&gt;automated data generation&lt;/strong&gt; for robotics and beyond.&lt;/p&gt; &#xA;&lt;p&gt;Currently, we are open-sourcing the &lt;strong&gt;underlying physics engine and the simulation platform&lt;/strong&gt;. Our generative framework is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more the &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#papers-behind-genesis&#34;&gt;paper list&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;p&gt;Genesis is built and will continuously evolve with the following &lt;em&gt;&lt;strong&gt;long-term missions&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lowering the barrier&lt;/strong&gt; to using physics simulations and making robotics research accessible to everyone. (See our &lt;a href=&#34;https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html&#34;&gt;commitment&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unifying a wide spectrum of state-of-the-art physics solvers&lt;/strong&gt; into a single framework, allowing re-creating the whole physical world in a virtual realm with the highest possible physical, visual and sensory fidelity, using the most advanced simulation techniques.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minimizing human effort&lt;/strong&gt; in collecting and generating data for robotics and other domains, letting the data flywheel spin on its own.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Project Page: &lt;a href=&#34;https://genesis-embodied-ai.github.io/&#34;&gt;https://genesis-embodied-ai.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Genesis delivers an unprecedented simulation speed -- over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Genesis runs natively across different systems (Linux, MacOS, Windows), and across different compute backend (CPU, Nvidia GPU, AMD GPU, Apple Metal).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unification of various physics solvers&lt;/strong&gt;: Genesis develops a unified simulation framework that integrates various physics solvers: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support a wide range of material models&lt;/strong&gt;: Genesis supports simulation (and the coupling) of rigid and articulated bodies, various types of liquids, gaseous phenomenon, deformable objects, thin-shell objects and granular materials.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support for a wide range of robots&lt;/strong&gt;: Robot arm, legged robot, drone, &lt;em&gt;soft robot&lt;/em&gt;, etc., and extensive support for loading different file types: &lt;code&gt;MJCF (.xml)&lt;/code&gt;, &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.obj&lt;/code&gt;, &lt;code&gt;.glb&lt;/code&gt;, &lt;code&gt;.ply&lt;/code&gt;, &lt;code&gt;.stl&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Photorealistic and high-performance ray-tracer&lt;/strong&gt;: Genesis supports native ray-tracing based rendering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Differentiability&lt;/strong&gt;: Genesis is designed to be fully compatible with differentiable simulation. Currently, our MPM solver and Tool Solver are differentiable, and differentiability for other solvers will be added soon (starting with rigid-body simulation).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Physics-based Tactile Sensor&lt;/strong&gt;: Genesis involves a physics-based and differentiable &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/DiffTactile&#34;&gt;tactile sensor simulation module&lt;/a&gt;. This will be integrated to the public version soon (expected in version 0.3.0).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-friendliness&lt;/strong&gt;: Genesis is designed in a way to make using simulation as simple as possible. From installation to API design, if there&#39;s anything you found counter-intuitive or difficult to use, please &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/issues&#34;&gt;let us know&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Quick Installation&lt;/h3&gt; &#xA;&lt;p&gt;Genesis is available via PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install genesis-world  # Requires Python &amp;gt;=3.9;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to install &lt;strong&gt;PyTorch&lt;/strong&gt; following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to try out the latest version, we suggest you to git clone from the repo and do &lt;code&gt;pip install -e .&lt;/code&gt; instead of via PyPI.&lt;/p&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://genesis-world.readthedocs.io/en/latest/user_guide/index.html&#34;&gt;documentation site (English)&lt;/a&gt; / &lt;a href=&#34;https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html&#34;&gt;(Chinese)&lt;/a&gt; for detailed installation steps, tutorials and API references.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Genesis&lt;/h2&gt; &#xA;&lt;p&gt;The goal of the Genesis project is to build a fully transparent, user-friendly ecosystem where contributors from both robotics and computer graphics can &lt;strong&gt;come together to collaboratively create a high-efficiency, realistic (both physically and visually) virtual world for robotics research and beyond&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We sincerely welcome &lt;em&gt;any forms of contributions&lt;/em&gt; from the community to make the world a better place for robots. From &lt;strong&gt;pull requests&lt;/strong&gt; for new features, &lt;strong&gt;bug reports&lt;/strong&gt;, to even tiny &lt;strong&gt;suggestions&lt;/strong&gt; that will make Genesis API more intuitive, all are wholeheartedly appreciated!&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Please use Github &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/issues&#34;&gt;Issues&lt;/a&gt; for bug reports and feature requests.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please use GitHub &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/discussions&#34;&gt;Discussions&lt;/a&gt; for discussing ideas, and asking questions.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License and Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;The Genesis source code is licensed under Apache 2.0. The development of Genesis won&#39;t be possible without these amazing open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/taichi-dev/taichi&#34;&gt;Taichi&lt;/a&gt;: for providing a high-performance cross-platform compute backend. Kudos to all the members providing technical support from taichi!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhouxian/FluidLab&#34;&gt;FluidLab&lt;/a&gt; for providing a reference MPM solver implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/erizmr/SPH_Taichi&#34;&gt;SPH_Taichi&lt;/a&gt; for providing a reference SPH solver implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matthias-research.github.io/pages/tenMinutePhysics/index.html&#34;&gt;Ten Minute Physics&lt;/a&gt; and &lt;a href=&#34;https://github.com/WASD4959/PBF3D&#34;&gt;PBF3D&lt;/a&gt; for providing a reference PBD solver implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-deepmind/mujoco&#34;&gt;MuJoCo&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/brax&#34;&gt;Brax&lt;/a&gt; for providing reference for rigid body dynamics&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danfis/libccd&#34;&gt;libccd&lt;/a&gt; for providing reference for collision detection&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmatl/pyrender&#34;&gt;PyRender&lt;/a&gt; for rasterization-based renderer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LuisaGroup/LuisaCompute&#34;&gt;LuisaCompute&lt;/a&gt; and &lt;a href=&#34;https://github.com/LuisaGroup/LuisaRender&#34;&gt;LuisaRender&lt;/a&gt; for its ray-tracing DSL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;trimesh&lt;/a&gt;, &lt;a href=&#34;https://github.com/cnr-isti-vclab/PyMeshLab&#34;&gt;PyMeshLab&lt;/a&gt; and &lt;a href=&#34;https://github.com/SarahWeiii/CoACD&#34;&gt;CoACD&lt;/a&gt; for geometry processing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers behind Genesis&lt;/h2&gt; &#xA;&lt;p&gt;Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Xian, Zhou, et al. &#34;Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.&#34; arXiv preprint arXiv:2303.02346 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Xu, Zhenjia, et al. &#34;Roboninja: Learning an adaptive cutting policy for multi-material objects.&#34; arXiv preprint arXiv:2302.11553 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yufei, et al. &#34;Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.&#34; arXiv preprint arXiv:2311.01455 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Tsun-Hsuan, et al. &#34;Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.&#34; arXiv preprint arXiv:2303.09555 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Tsun-Hsuan Johnson, et al. &#34;Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.&#34; Advances in Neural Information Processing Systems 36 (2023): 44398-44423.&lt;/li&gt; &#xA; &lt;li&gt;Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. &#34;Gen2sim: Scaling up robot learning in simulation with generative models.&#34; 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.&lt;/li&gt; &#xA; &lt;li&gt;Si, Zilin, et al. &#34;DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.&#34; arXiv preprint arXiv:2403.08716 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yian, et al. &#34;Thin-Shell Object Manipulations With Differentiable Physics Simulations.&#34; arXiv preprint arXiv:2404.00451 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Lin, Chunru, et al. &#34;UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.&#34; arXiv preprint arXiv:2411.12711 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Zhou, Wenyang, et al. &#34;EMDM: Efficient motion diffusion model for fast and high-quality motion generation.&#34; European Conference on Computer Vision. Springer, Cham, 2025.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &#34;Scalable differentiable physics for learning and control.&#34; International Conference on Machine Learning. PMLR, 2020.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &#34;Efficient differentiable simulation of articulated bodies.&#34; In International Conference on Machine Learning, PMLR, 2021.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. &#34;Differentiable simulation of soft multi-body systems.&#34; Advances in Neural Information Processing Systems 34 (2021).&lt;/li&gt; &#xA; &lt;li&gt;Wan, Weilin, et al. &#34;Tlcontrol: Trajectory and language control for human motion synthesis.&#34; arXiv preprint arXiv:2311.17135 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yian, et al. &#34;Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.&#34; arXiv preprint arXiv:2411.09823 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Zheng, Shaokun, et al. &#34;LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.&#34; ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.&lt;/li&gt; &#xA; &lt;li&gt;Fan, Yingruo, et al. &#34;Faceformer: Speech-driven 3d facial animation with transformers.&#34; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;/li&gt; &#xA; &lt;li&gt;Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. &#34;ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.&#34; Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.&lt;/li&gt; &#xA; &lt;li&gt;Dou, Zhiyang, et al. &#34;C¬∑ ase: Learning conditional adversarial skill embeddings for physics-based characters.&#34; SIGGRAPH Asia 2023 Conference Papers. 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and many more on-going work.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you used Genesis in your research, we would appreciate it if you could cite it. We are still working on a technical report, and before it&#39;s public, you could consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Genesis,&#xA;  author = {Genesis Authors},&#xA;  title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},&#xA;  month = {December},&#xA;  year = {2024},&#xA;  url = {https://github.com/Genesis-Embodied-AI/Genesis}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>