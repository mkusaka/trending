<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-06T01:39:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kuafuai/DevOpsGPT</title>
    <updated>2023-08-06T01:39:42Z</updated>
    <id>tag:github.com,2023-08-06:/kuafuai/DevOpsGPT</id>
    <link href="https://github.com/kuafuai/DevOpsGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multi agent system for AI-driven software development. convert natural language requirements into working software.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DevOpsGPT: AI-Driven Software Development Automation Solution&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/README_CN.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/%E6%96%87%E6%A1%A3-%E4%B8%AD%E6%96%87%E7%89%88-blue.svg?sanitize=true&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/README.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.kuafuai.net&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/%E5%AE%98%E7%BD%91-%E4%BC%81%E4%B8%9A%E7%89%88-purple.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/CONTACT.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/WeChat-%E5%BE%AE%E4%BF%A1-green.svg?sanitize=true&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ğŸ’¡ Get Help - &lt;a href=&#34;https://github.com/kuafuai/DevOpsGPT/issues&#34;&gt;Q&amp;amp;A&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;ğŸ’¡ Submit Requests - &lt;a href=&#34;https://github.com/kuafuai/DevOpsGPT/discussions&#34;&gt;Issue&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;ğŸ’¡ Technical exchange - &lt;a href=&#34;mailto:service@kuafuai.net&#34;&gt;service@kuafuai.net&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the AI Driven Software Development Automation Solution, abbreviated as DevOpsGPT. We combine LLM (Large Language Model) with DevOps tools to convert natural language requirements into working software. This innovative feature greatly improves development efficiency, shortens development cycles, and reduces communication costs, resulting in higher-quality software delivery.&lt;/p&gt; &#xA;&lt;h2&gt;Features and Benefits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improved development efficiency: No need for tedious requirement document writing and explanations. Users can interact directly with DevOpsGPT to quickly convert requirements into functional software.&lt;/li&gt; &#xA; &lt;li&gt;Shortened development cycles: The automated software development process significantly reduces delivery time, accelerating software deployment and iterations.&lt;/li&gt; &#xA; &lt;li&gt;Reduced communication costs: By accurately understanding user requirements, DevOpsGPT minimizes the risk of communication errors and misunderstandings, enhancing collaboration efficiency between development and business teams.&lt;/li&gt; &#xA; &lt;li&gt;High-quality deliverables: DevOpsGPT generates code and performs validation, ensuring the quality and reliability of the delivered software.&lt;/li&gt; &#xA; &lt;li&gt;[Enterprise Edition] Existing project analysis: Through AI, automatic analysis of existing project information, accurate decomposition and development of required tasks on the basis of existing projects.&lt;/li&gt; &#xA; &lt;li&gt;[Enterprise Edition] Professional model selection: Support language model services stronger than GPT in the professional field to better complete requirements development tasks, and support private deployment.&lt;/li&gt; &#xA; &lt;li&gt;[Enterprise Edition] Support more DevOps platforms: can connect with more DevOps platforms to achieve the development and deployment of the whole process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demoï¼ˆClick to play videoï¼‰&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IWUPbGrJQOU&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/demo-adduser-en.jpeg&#34; width=&#34;50%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Workflow&lt;/h2&gt; &#xA;&lt;p&gt;Through the above introduction and Demo demonstration, you must be curious about how DevOpsGPT achieves the entire process of automated requirement development in an existing project. Below is a brief overview of the entire process:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/files/intro-flow-en.png&#34; alt=&#34;å·¥ä½œæµç¨‹&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clarify requirement documents: Interact with DevOpsGPT to clarify and confirm details in requirement documents.&lt;/li&gt; &#xA; &lt;li&gt;Generate interface documentation: DevOpsGPT can generate interface documentation based on the requirements, facilitating interface design and implementation for developers.&lt;/li&gt; &#xA; &lt;li&gt;Write pseudocode based on existing projects: Analyze existing projects to generate corresponding pseudocode, providing developers with references and starting points.&lt;/li&gt; &#xA; &lt;li&gt;Refine and optimize code functionality: Developers improve and optimize functionality based on the generated code.&lt;/li&gt; &#xA; &lt;li&gt;Continuous integration: Utilize DevOps tools for continuous integration to automate code integration and testing.&lt;/li&gt; &#xA; &lt;li&gt;Software version release: Deploy software versions to the target environment using DevOpsGPT and DevOps tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the latest code or select a released version.&lt;/li&gt; &#xA; &lt;li&gt;Generate the configuration file: Copy &lt;code&gt;env.yaml.tpl&lt;/code&gt; and rename it to &lt;code&gt;env.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modify the configuration file: Edit &lt;code&gt;env.yaml&lt;/code&gt; and add the necessary information such as GPT Token (refer to &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/DOCUMENT.md&#34;&gt;documentation link&lt;/a&gt; for detailed instructions).&lt;/li&gt; &#xA; &lt;li&gt;Run the service: Execute &lt;code&gt;sh run.sh&lt;/code&gt; on Linux or Mac, or double-click &lt;code&gt;run.bat&lt;/code&gt; on Windows.&lt;/li&gt; &#xA; &lt;li&gt;Access the service: Access the service through a browser (check the startup log for the access address, default is &lt;a href=&#34;http://127.0.0.1:8080&#34;&gt;http://127.0.0.1:8080&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Complete requirement development: Follow the instructions on the page to complete requirement development, and view the generated code in the &lt;code&gt;./workspace&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For detailed documentation and configuration parameters, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/DOCUMENT.md&#34;&gt;documentation link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Although we strive to enhance enterprise-level software development efficiency and reduce barriers with the help of large-scale language models, there are still some limitations in the current version:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The generation of requirement and interface documentation may not be precise enough and might not meet developer intent in complex scenarios.&lt;/li&gt; &#xA; &lt;li&gt;In the current version, automating the understanding of existing project code is not possible. We are exploring a new solution that has shown promising results during validation and will be introduced in a future version.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Product Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate requirement decomposition and development task breakdown based on existing projects.&lt;/li&gt; &#xA; &lt;li&gt;New product experiences for rapid import of development requirements and parallel automation of software development and deployment.&lt;/li&gt; &#xA; &lt;li&gt;Introduce more software engineering tools and professional tools to quickly complete various software development tasks under AI planning and exectuion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We invite you to participate in the DevOpsGPT project and &lt;a href=&#34;https://raw.githubusercontent.com/kuafuai/DevOpsGPT/master/docs/CONTRIBUTING.md&#34;&gt;contribute&lt;/a&gt; to the automation and innovation of software development, creating smarter and more efficient software systems!&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, DevOpsGPT, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.&lt;/p&gt; &#xA;&lt;p&gt;The developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by DevOpsGPT.&lt;/p&gt; &#xA;&lt;p&gt;Please note that the use of the GPT language model can be expensive due to its token usage. By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.&lt;/p&gt; &#xA;&lt;p&gt;As an autonomous experiment, DevOpsGPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.&lt;/p&gt; &#xA;&lt;p&gt;By using DevOpsGPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys&#39; fees) arising from your use of this software or your violation of these terms.&lt;/p&gt; &#xA;&lt;h2&gt;Reference project&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;https://github.com/Significant-Gravitas/Auto-GPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer&#34;&gt;https://github.com/AntonOsika/gpt-engineer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;https://github.com/hwchase17/langchain&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen-7B</title>
    <updated>2023-08-06T01:39:42Z</updated>
    <id>tag:github.com,2023-08-06:/QwenLM/Qwen-7B</id>
    <link href="https://github.com/QwenLM/Qwen-7B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repo of Qwen-7B (é€šä¹‰åƒé—®-7B) chat &amp; pretrained large language model proposed by Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/assets/logo.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; Qwen-7B &lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B/summary&#34;&gt;ğŸ¤– &lt;/a&gt;&lt;a&gt; | &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B&#34;&gt;ğŸ¤—&lt;/a&gt;&amp;nbsp; ï½œ Qwen-7B-Chat &lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary&#34;&gt;ğŸ¤– &lt;/a&gt;&lt;a&gt;| &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B-Chat&#34;&gt;ğŸ¤—&lt;/a&gt;&amp;nbsp; ï½œ &amp;nbsp;&lt;a href=&#34;https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary&#34;&gt;Demo&lt;/a&gt;&amp;nbsp; ï½œ &amp;nbsp;&lt;a href=&#34;https://github.com/QwenLM/Qwen-7B/raw/main/tech_memo.md&#34;&gt;Report&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/README_CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&amp;nbsp; ï½œ &amp;amp;nbspEnglish &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p&gt;We opensource &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; on both &lt;strong&gt;ğŸ¤– ModelScope&lt;/strong&gt; and &lt;strong&gt;ğŸ¤— Hugging Face&lt;/strong&gt; (Click the logos on top to the repos with codes and checkpoints). This repo includes the brief introduction to Qwen-7B, the usage guidance, and also a technical memo &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/tech_memo.md&#34;&gt;link&lt;/a&gt; that provides more information.&lt;/p&gt; &#xA;&lt;p&gt;Qwen-7B is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. The features of the Qwen-7B series include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Trained with high-quality pretraining data&lt;/strong&gt;. We have pretrained Qwen-7B on a self-constructed large-scale high-quality dataset of over 2.2 trillion tokens. The dataset includes plain texts and codes, and it covers a wide range of domains, including general domain data and professional domain data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt;. In comparison with the models of the similar model size, we outperform the competitors on a series of benchmark datasets, which evaluates natural language understanding, mathematics, coding, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better support of languages&lt;/strong&gt;. Our tokenizer, based on a large vocabulary of over 150K tokens, is a more efficient one compared with other tokenizers. It is friendly to many languages, and it is helpful for users to further finetune Qwen-7B for the extension of understanding a certain language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support of 8K Context Length&lt;/strong&gt;. Both Qwen-7B and Qwen-7B-Chat support the context length of 8K, which allows inputs with long contexts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support of Plugins&lt;/strong&gt;. Qwen-7B-Chat is trained with plugin-related alignment data, and thus it is capable of using tools, including APIs, models, databases, etc., and it is capable of playing as an agent.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.8.3 We release both Qwen-7B and Qwen-7B-Chat on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;In general, Qwen-7B outperforms the baseline models of a similar model size, and even outperforms larger models of around 13B parameters, on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, HumanEval, and WMT22, etc., which evaluate the models&#39; capabilities on natural language understanding, mathematic problem solving, coding, etc. See the results below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;WMT22 (en-zh)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;35.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;11.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;10.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA 2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;45.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;17.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;42.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;26.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-6B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;10.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;53.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;46.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;35.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;15.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA 2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;54.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;40.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Qwen-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;56.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;59.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;24.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;30.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/assets/performance.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;For more experimental results (detailed model performance on more benchmark datasets) and details, please refer to our technical memo by clicking &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/techmemo-draft.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Below, we provide simple examples to show how to use Qwen-7B with ğŸ¤– ModelScope and ğŸ¤— Transformers.&lt;/p&gt; &#xA;&lt;p&gt;Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your device supports fp16 or bf16, we recommend installing &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash-attention&lt;/a&gt; for higher efficiency and lower memory usage. (&lt;strong&gt;flash-attention is optional and the project can run normally without installing it&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention&#xA;cd flash-attention &amp;amp;&amp;amp; pip install .&#xA;pip install csrc/layer_norm&#xA;pip install csrc/rotary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can start with ModelScope or Transformers.&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ¤— Transformers&lt;/h4&gt; &#xA;&lt;p&gt;To use Qwen-7B-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;&#xA;# Note: For tokenizer usage, please refer to examples/tokenizer_showcase.ipynb. &#xA;# The default behavior now has injection attack prevention off.&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use auto mode, automatically select precision based on the device.&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True).eval()&#xA;&#xA;# Specify hyperparameters for generation&#xA;model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ä½ å¥½&#34;, history=None)&#xA;print(response)&#xA;# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚&#xA;&#xA;# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚&#34;, history=history) &#xA;print(response)&#xA;# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚&#xA;# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚&#xA;# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚&#xA;# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚&#xA;# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚&#xA;# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚&#xA;&#xA;# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜&#34;, history=history)&#xA;print(response)&#xA;# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Running Qwen-7B pretrained base model is also simple.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Running Qwen-7B&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-7B&#34;, trust_remote_code=True)&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use auto mode, automatically select precision based on the device.&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True).eval()&#xA;&#xA;# Specify hyperparameters for generation&#xA;model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-7B&#34;, trust_remote_code=True)&#xA;&#xA;inputs = tokenizer(&#39;è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯&#39;, return_tensors=&#39;pt&#39;)&#xA;inputs = inputs.to(&#39;cuda:0&#39;)&#xA;pred = model.generate(**inputs)&#xA;print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))&#xA;# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;ğŸ¤– ModelScope&lt;/h4&gt; &#xA;&lt;p&gt;ModelScope is an opensource platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from modelscope.pipelines import pipeline&#xA;from modelscope.utils.constant import Tasks&#xA;from modelscope import snapshot_download&#xA;&#xA;model_id = &#39;QWen/qwen-7b-chat&#39;&#xA;revision = &#39;v1.0.0&#39;&#xA;&#xA;model_dir = snapshot_download(model_id, revision)&#xA;&#xA;pipe = pipeline(&#xA;task=Tasks.chat, model=model_dir, device_map=&#39;auto&#39;)&#xA;history = None&#xA;&#xA;text = &#39;æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ&#39;&#xA;results = pipe(text, history=history)&#xA;response, history = results[&#39;response&#39;], results[&#39;history&#39;]&#xA;print(f&#39;Response: {response}&#39;)&#xA;text = &#39;å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹å‘¢ï¼Ÿ&#39;&#xA;results = pipe(text, history=history)&#xA;response, history = results[&#39;response&#39;], results[&#39;history&#39;]&#xA;print(f&#39;Response: {response}&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;We provide examples to show how to load models in &lt;code&gt;NF4&lt;/code&gt; and &lt;code&gt;Int8&lt;/code&gt;. For starters, make sure you have implemented &lt;code&gt;bitsandbytes&lt;/code&gt;. Note that the requirements for &lt;code&gt;bitsandbytes&lt;/code&gt; are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;**Requirements** Python &amp;gt;=3.8. Linux distribution (Ubuntu, MacOS, etc.) + CUDA &amp;gt; 10.0.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows users should find another option, which might be &lt;a href=&#34;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&#34;&gt;bitsandbytes-windows-webui&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then you only need to add your quantization configuration to &lt;code&gt;AutoModelForCausalLM.from_pretrained&lt;/code&gt;. See the example below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, BitsAndBytesConfig&#xA;&#xA;# quantization configuration for NF4 (4 bits)&#xA;quantization_config = BitsAndBytesConfig(&#xA;    load_in_4bit=True,&#xA;    bnb_4bit_quant_type=&#39;nf4&#39;,&#xA;    bnb_4bit_compute_dtype=torch.bfloat16&#xA;)&#xA;&#xA;# quantization configuration for Int8 (8 bits)&#xA;quantization_config = BitsAndBytesConfig(load_in_8bit=True)&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    args.checkpoint_path,&#xA;    device_map=&#34;cuda:0&#34;,&#xA;    quantization_config=quantization_config,&#xA;    max_memory=max_memory,&#xA;    trust_remote_code=True,&#xA;).eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With this method, it is available to load Qwen-7B in &lt;code&gt;NF4&lt;/code&gt; and &lt;code&gt;Int8&lt;/code&gt;, which saves you memory usage. We provide related statistics of model performance below. We find that the quantization downgrades the effectiveness slightly but significantly increases inference efficiency and reduces memory costs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Precision&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BF16&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;16.2G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Int8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;10.1G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NF4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;48.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.4G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;CLI Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide a CLI demo example in &lt;code&gt;cli_demo.py&lt;/code&gt;, which supports streaming output for the generation. Users can interact with Qwen-7B-Chat by inputting prompts, and the model returns model outputs in the streaming mode.&lt;/p&gt; &#xA;&lt;h2&gt;Tool Usage&lt;/h2&gt; &#xA;&lt;p&gt;Qwen-7B-Chat is specifically optimized for tool usage, including API, database, models, etc., so that users can build their own Qwen-7B-based LangChain, Agent, and Code Interpreter. In the soon-to-be-released internal evaluation benchmark for assessing tool usage capabilities, we find that Qwen-7B reaches stable performance. &lt;a href=&#34;https://&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Tool Selection (Acc.â†‘)&lt;/th&gt; &#xA;   &lt;th&gt;Tool Input (Rouge-Lâ†‘)&lt;/th&gt; &#xA;   &lt;th&gt;False Positive Errorâ†“&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;95%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.90&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;   &lt;td&gt;85%&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;   &lt;td&gt;75%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;99%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.5%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For how to write and use prompts for ReAct Prompting, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/examples/react_prompt.md&#34;&gt;the ReAct examples&lt;/a&gt;. The use of tools can enable the model to better perform tasks.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we provide experimental results to show its capabilities of playing as an agent. See &lt;a href=&#34;https://huggingface.co/docs/transformers/transformers_agents&#34;&gt;Hugging Face Agent&lt;/a&gt; for more information. Its performance on the run-mode benchmark provided by Hugging Face is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Tool Selectionâ†‘&lt;/th&gt; &#xA;   &lt;th&gt;Tool Usedâ†‘&lt;/th&gt; &#xA;   &lt;th&gt;Codeâ†‘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;100&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;100&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;97.41&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt; &#xA;   &lt;td&gt;95.37&lt;/td&gt; &#xA;   &lt;td&gt;96.30&lt;/td&gt; &#xA;   &lt;td&gt;87.04&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder-15.5B&lt;/td&gt; &#xA;   &lt;td&gt;87.04&lt;/td&gt; &#xA;   &lt;td&gt;87.96&lt;/td&gt; &#xA;   &lt;td&gt;68.89&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;90.74&lt;/td&gt; &#xA;   &lt;td&gt;92.59&lt;/td&gt; &#xA;   &lt;td&gt;74.07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Long-Context Understanding&lt;/h2&gt; &#xA;&lt;p&gt;To extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length to over 8K tokens. We conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen-7B can reach outstanding performance in the scenario of long context. Results are demonstrated below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th colspan=&#34;5&#34; align=&#34;center&#34;&gt;Sequence Length&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1024&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;2048&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;4096&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;8192&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;16384&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.78&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;39.35&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;469.81&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;2645.09&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.78&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.59&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.66&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;5.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.78&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.58&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.56&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.78&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.58&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.49&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.32&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Reproduction&lt;/h2&gt; &#xA;&lt;p&gt;For your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/eval/EVALUATION.md&#34;&gt;eval/EVALUATION.md&lt;/a&gt; for more information. Note that the reproduction may lead to slight differences from our reported results.&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;Researchers and developers are free to use the codes and model weights of both Qwen-7B and Qwen-7B-Chat. We also allow their commercial use. Check our license at &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-7B/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, feel free to send an email to &lt;a href=&#34;mailto:qianwen_opensource@alibabacloud.com&#34;&gt;qianwen_opensource@alibabacloud.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xorbitsai/inference</title>
    <updated>2023-08-06T01:39:42Z</updated>
    <id>tag:github.com,2023-08-06:/xorbitsai/inference</id>
    <link href="https://github.com/xorbitsai/inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you&#39;re empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/xorbitsai/inference/main/assets/xorbits-logo.png&#34; width=&#34;180px&#34; alt=&#34;xorbits&#34;&gt; &#xA; &lt;h1&gt;Xorbits Inference: Model Serving Made Easy ğŸ¤–&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/xinference/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/xinference.svg?style=for-the-badge&#34; alt=&#34;PyPI Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xorbitsai/inference/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/xinference.svg?style=for-the-badge&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://actions-badge.atrox.dev/xorbitsai/inference/goto?ref=main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/xorbitsai/inference/python.yaml?branch=main&amp;amp;style=for-the-badge&amp;amp;label=GITHUB%20ACTIONS&amp;amp;logo=github&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/xorbitsio/shared_invite/zt-1o3z9ucdh-RbfhbPVpx7prOVdM1CAuxg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join_Slack-781FF5.svg?logo=slack&amp;amp;style=for-the-badge&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/xorbitsio&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/xorbitsio?logo=twitter&amp;amp;style=for-the-badge&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/xorbitsai/inference/main/README_zh_CN.md&#34;&gt;ä¸­æ–‡ä»‹ç»&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/xorbitsai/inference/main/README_ja_JP.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command. Whether you are a researcher, developer, or data scientist, Xorbits Inference empowers you to unleash the full potential of cutting-edge AI models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;i&gt;&lt;a href=&#34;https://join.slack.com/t/xorbitsio/shared_invite/zt-1z3zsm9ep-87yI9YZ_B79HLB2ccTq4WA&#34;&gt;ğŸ‘‰ Join our Slack community!&lt;/a&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;ğŸŒŸ &lt;strong&gt;Model Serving Made Easy&lt;/strong&gt;: Simplify the process of serving large language, speech recognition, and multimodal models. You can set up and deploy your models for experimentation and production with a single command.&lt;/p&gt; &#xA;&lt;p&gt;âš¡ï¸ &lt;strong&gt;State-of-the-Art Models&lt;/strong&gt;: Experiment with cutting-edge built-in models using a single command. Inference provides access to state-of-the-art open-source models!&lt;/p&gt; &#xA;&lt;p&gt;ğŸ–¥ &lt;strong&gt;Heterogeneous Hardware Utilization&lt;/strong&gt;: Make the most of your hardware resources with &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;. Xorbits Inference intelligently utilizes heterogeneous hardware, including GPUs and CPUs, to accelerate your model inference tasks.&lt;/p&gt; &#xA;&lt;p&gt;âš™ï¸ &lt;strong&gt;Flexible API and Interfaces&lt;/strong&gt;: Offer multiple interfaces for interacting with your models, supporting RPC, RESTful API(compatible with OpenAI API), CLI and WebUI for seamless management and monitoring.&lt;/p&gt; &#xA;&lt;p&gt;ğŸŒ &lt;strong&gt;Distributed Deployment&lt;/strong&gt;: Excel in distributed deployment scenarios, allowing the seamless distribution of model inference across multiple devices or machines.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ”Œ &lt;strong&gt;Built-in Integration with Third-Party Libraries&lt;/strong&gt;: Xorbits Inference seamlessly integrates with popular third-party libraries like LangChain and LlamaIndex. (Coming soon)&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Xinference can be installed via pip from PyPI. It is highly recommended to create a new virtual environment to avoid conflicts.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install &#34;xinference&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;xinference&lt;/code&gt; installs basic packages for serving models.&lt;/p&gt; &#xA;&lt;h4&gt;Installation with GGML&lt;/h4&gt; &#xA;&lt;p&gt;To serve ggml models, you need to install the following extra dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install &#34;xinference[ggml]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to achieve acceleration on different hardware, refer to the installation documentation of the corresponding package.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abetlen/llama-cpp-python#installation-from-pypi-recommended&#34;&gt;llama-cpp-python&lt;/a&gt; is required to run &lt;code&gt;baichuan&lt;/code&gt;, &lt;code&gt;wizardlm-v1.0&lt;/code&gt;, &lt;code&gt;vicuna-v1.3&lt;/code&gt; and &lt;code&gt;orca&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/li-plus/chatglm.cpp#getting-started&#34;&gt;chatglm-cpp-python&lt;/a&gt; is required to run &lt;code&gt;chatglm&lt;/code&gt; and &lt;code&gt;chatglm2&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Installation with PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;To serve PyTorch models, you need to install the following extra dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install &#34;xinference[pytorch]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Installation with all dependencies&lt;/h4&gt; &#xA;&lt;p&gt;If you want to serve all the supported models, install all the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install &#34;xinference[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;You can deploy Xinference locally with a single command or deploy it in a distributed cluster.&lt;/p&gt; &#xA;&lt;h4&gt;Local&lt;/h4&gt; &#xA;&lt;p&gt;To start a local instance of Xinference, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xinference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Distributed&lt;/h4&gt; &#xA;&lt;p&gt;To deploy Xinference in a cluster, you need to start a Xinference supervisor on one server and Xinference workers on the other servers. Follow the steps below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Starting the Supervisor&lt;/strong&gt;: On the server where you want to run the Xinference supervisor, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xinference-supervisor -H &#34;${supervisor_host}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;${supervisor_host}&lt;/code&gt; with the actual host of your supervisor server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Starting the Workers&lt;/strong&gt;: On each of the other servers where you want to run Xinference workers, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xinference-worker -e &#34;http://${supervisor_host}:9997&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once Xinference is running, an endpoint will be accessible for model management via CLI or Xinference client.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For local deployment, the endpoint will be &lt;code&gt;http://localhost:9997&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For cluster deployment, the endpoint will be &lt;code&gt;http://${supervisor_host}:9997&lt;/code&gt;, where &lt;code&gt;${supervisor_host}&lt;/code&gt; is the hostname or IP address of the server where the supervisor is running.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also view a web UI using the Xinference endpoint to chat with all the builtin models. You can even &lt;strong&gt;chat with two cutting-edge AI models side-by-side to compare their performance&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xorbitsai/inference/main/assets/demo.gif&#34; alt=&#34;web UI&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Xinference CLI&lt;/h3&gt; &#xA;&lt;p&gt;Xinference provides a command line interface (CLI) for model management. Here are some useful commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launch a model (a model UID will be returned): &lt;code&gt;xinference launch&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;List running models: &lt;code&gt;xinference list&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;List all the builtin models: &lt;code&gt;xinference list --all&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Terminate a model: &lt;code&gt;xinference terminate --model-uid ${model_uid}&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Xinference Client&lt;/h3&gt; &#xA;&lt;p&gt;Xinference also provides a client for managing and accessing models programmatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from xinference.client import Client&#xA;&#xA;client = Client(&#34;http://localhost:9997&#34;)&#xA;model_uid = client.launch_model(model_name=&#34;chatglm2&#34;)&#xA;model = client.get_model(model_uid)&#xA;&#xA;chat_history = []&#xA;prompt = &#34;What is the largest animal?&#34;&#xA;model.chat(&#xA;            prompt,&#xA;            chat_history,&#xA;            generate_config={&#34;max_tokens&#34;: 1024}&#xA;        )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Result:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;id&#34;: &#34;chatcmpl-8d76b65a-bad0-42ef-912d-4a0533d90d61&#34;,&#xA;  &#34;model&#34;: &#34;56f69622-1e73-11ee-a3bd-9af9f16816c6&#34;,&#xA;  &#34;object&#34;: &#34;chat.completion&#34;,&#xA;  &#34;created&#34;: 1688919187,&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;index&#34;: 0,&#xA;      &#34;message&#34;: {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34;The largest animal that has been scientifically measured is the blue whale, which has a maximum length of around 23 meters (75 feet) for adult animals and can weigh up to 150,000 pounds (68,000 kg). However, it is important to note that this is just an estimate and that the largest animal known to science may be larger still. Some scientists believe that the largest animals may not have a clear \&#34;size\&#34; in the same way that humans do, as their size can vary depending on the environment and the stage of their life.&#34;&#xA;      },&#xA;      &#34;finish_reason&#34;: &#34;None&#34;&#xA;    }&#xA;  ],&#xA;  &#34;usage&#34;: {&#xA;    &#34;prompt_tokens&#34;: -1,&#xA;    &#34;completion_tokens&#34;: -1,&#xA;    &#34;total_tokens&#34;: -1&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/xorbitsai/inference/main/examples&#34;&gt;examples&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;h2&gt;Builtin models&lt;/h2&gt; &#xA;&lt;p&gt;To view the builtin models, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xinference list --all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ggmlv3 models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;Size (in billions)&lt;/th&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2&lt;/td&gt; &#xA;   &lt;td&gt;Foundation Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;7, 13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan&lt;/td&gt; &#xA;   &lt;td&gt;Foundation Model&lt;/td&gt; &#xA;   &lt;td&gt;en, zh&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-chat&lt;/td&gt; &#xA;   &lt;td&gt;RLHF Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;7, 13, 70&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;chatglm&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en, zh&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q4_0&#39;, &#39;q4_1&#39;, &#39;q5_0&#39;, &#39;q5_1&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;chatglm2&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en, zh&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q4_0&#39;, &#39;q4_1&#39;, &#39;q5_0&#39;, &#39;q5_1&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wizardlm-v1.0&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;7, 13, 33&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wizardlm-v1.1&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vicuna-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;7, 13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q2_K&#39;, &#39;q3_K_L&#39;, ... , &#39;q6_K&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;orca&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;ggmlv3&lt;/td&gt; &#xA;   &lt;td&gt;3, 7, 13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;q4_0&#39;, &#39;q4_1&#39;, &#39;q5_0&#39;, &#39;q5_1&#39;, &#39;q8_0&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;pytorch models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;Size (in billions)&lt;/th&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan&lt;/td&gt; &#xA;   &lt;td&gt;Foundation Model&lt;/td&gt; &#xA;   &lt;td&gt;en, zh&lt;/td&gt; &#xA;   &lt;td&gt;pytorch&lt;/td&gt; &#xA;   &lt;td&gt;7, 13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;4-bit&#39;, &#39;8-bit&#39;, &#39;none&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan-chat&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en, zh&lt;/td&gt; &#xA;   &lt;td&gt;pytorch&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;&#39;4-bit&#39;, &#39;8-bit&#39;, &#39;none&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vicuna-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;SFT Model&lt;/td&gt; &#xA;   &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;td&gt;pytorch&lt;/td&gt; &#xA;   &lt;td&gt;7, 13, 33&lt;/td&gt; &#xA;   &lt;td&gt;&#39;4-bit&#39;, &#39;8-bit&#39;, &#39;none&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Xinference will download models automatically for you, and by default the models will be saved under &lt;code&gt;${USER}/.xinference/cache&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Foundation models only provide interface &lt;code&gt;generate&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;RLHF and SFT models provide both &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;chat&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use Apple Metal GPU for acceleration, please choose the q4_0 and q4_1 quantization methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llama-2-chat&lt;/code&gt; 70B ggmlv3 model only supports q4_0 quantization currently.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pytorch Model Best Practices&lt;/h2&gt; &#xA;&lt;p&gt;Pytorch has been integrated recently, and the usage scenarios are described below:&lt;/p&gt; &#xA;&lt;h3&gt;supported models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Foundation Modelï¼šbaichuanï¼ˆ7Bã€13Bï¼‰ã€‚&lt;/li&gt; &#xA; &lt;li&gt;SFT Modelï¼šbaichuan-chatï¼ˆ13Bï¼‰ã€vicuna-v1.3ï¼ˆ7Bã€13Bã€33Bï¼‰ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;supported devices&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: On Linux and Windows systems, &lt;code&gt;cuda&lt;/code&gt; device is used by default.&lt;/li&gt; &#xA; &lt;li&gt;MPS: On Mac M1/M2 devices, &lt;code&gt;mps&lt;/code&gt; device is used by default.&lt;/li&gt; &#xA; &lt;li&gt;CPU: It is not recommended to use a &lt;code&gt;cpu&lt;/code&gt; device, as it takes up a lot of memory and the inference speed is very slow.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;quantization methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: indicates that no quantization is used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;8-bit&lt;/code&gt;: use 8-bit quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;4-bit&lt;/code&gt;: use 4-bit quantization. Note: 4-bit quantization is only supported on Linux systems and CUDA devices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;other instructions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On MacOS system, baichuan-chat model is not supported, and baichuan model cannot use 8-bit quantization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;use cases&lt;/h3&gt; &#xA;&lt;p&gt;The table below shows memory usage and supported devices of some models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Size (B)&lt;/th&gt; &#xA;   &lt;th&gt;OS&lt;/th&gt; &#xA;   &lt;th&gt;No quantization (MB)&lt;/th&gt; &#xA;   &lt;th&gt;Quantization 8-bit (MB)&lt;/th&gt; &#xA;   &lt;th&gt;Quantization 4-bit (MB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan-chat&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;linux&lt;/td&gt; &#xA;   &lt;td&gt;not currently tested&lt;/td&gt; &#xA;   &lt;td&gt;13275&lt;/td&gt; &#xA;   &lt;td&gt;7263&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan-chat&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;macos&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vicuna-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;linux&lt;/td&gt; &#xA;   &lt;td&gt;12884&lt;/td&gt; &#xA;   &lt;td&gt;6708&lt;/td&gt; &#xA;   &lt;td&gt;3620&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vicuna-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;macos&lt;/td&gt; &#xA;   &lt;td&gt;12916&lt;/td&gt; &#xA;   &lt;td&gt;565&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;linux&lt;/td&gt; &#xA;   &lt;td&gt;13480&lt;/td&gt; &#xA;   &lt;td&gt;7304&lt;/td&gt; &#xA;   &lt;td&gt;4216&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;baichuan&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;macos&lt;/td&gt; &#xA;   &lt;td&gt;13480&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;   &lt;td&gt;not supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Xinference is currently under active development. Here&#39;s a roadmap outlining our planned developments for the next few weeks:&lt;/p&gt; &#xA;&lt;h3&gt;Langchain &amp;amp; LlamaIndex integration&lt;/h3&gt; &#xA;&lt;p&gt;With Xinference, it will be much easier for users to use these libraries and build applications with LLMs.&lt;/p&gt;</summary>
  </entry>
</feed>