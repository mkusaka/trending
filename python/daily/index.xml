<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-29T01:35:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>agiresearch/AIOS</title>
    <updated>2024-03-29T01:35:22Z</updated>
    <id>tag:github.com,2024-03-29:/agiresearch/AIOS</id>
    <link href="https://github.com/agiresearch/AIOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AIOS: LLM Agent Operating System&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIOS: LLM Agent Operating System&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agiresearch/AIOS/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AIOS, a Large Language Model (LLM) Agent operating system, embeds large language model into Operating Systems (OS) as the brain of the OS, enabling an operating system &#34;with soul&#34; -- an important step towards AGI. AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, maintain access control for agents, and provide a rich set of toolkits for LLM Agent developers.&lt;/p&gt; &#xA;&lt;h2&gt;üè† Architecture of AIOS&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/agiresearch/AIOS/main/images/AIOS-Architecture.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-03-25]&lt;/strong&gt; ‚úàÔ∏è Our paper &lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;AIOS: LLM Agent Operating System&lt;/a&gt; is released and AIOS repository is officially launched!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023-12-06]&lt;/strong&gt; ‚úàÔ∏è After several months of working, our perspective paper &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem&lt;/a&gt; is officially released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚úàÔ∏è Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you have Python &amp;gt;= 3.9&lt;/strong&gt;&lt;br&gt; Install the required packages using pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Set up huggingface token and cache directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HUGGING_FACE_HUB_TOKEN=&amp;lt;YOUR READ TOKEN&amp;gt;&#xA;export HF_HOME=&amp;lt;YOUR CACHE DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the main.py to start&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemma-2b-it for example, replace the max_gpu_memory and eval_device with your own&#xA;python main.py --llm_name gemma-2b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;24GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üåü Join Us!&lt;/h2&gt; &#xA;&lt;p&gt;AIOS is dedicated to facilitating LLM agents&#39; development and deployment in a systematic way, we are always looking for passionate collaborators to join us to foster a more cohesive, effective and efficient AIOS-Agent ecosystem. Suggestions and pull requests are always welcome!&lt;/p&gt; &#xA;&lt;h2&gt;üñãÔ∏è Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mei2024aios,&#xA;  title={AIOS: LLM Agent Operating System},&#xA;  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng}&#xA;  journal={arXiv preprint arXiv:2403.16971},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üì™ Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any suggestions, or wish to contact us for any reason, feel free to email us at &lt;a href=&#34;mailto:marknju2018@gmail.com&#34;&gt;marknju2018@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fudan-generative-vision/champ</title>
    <updated>2024-03-29T01:35:22Z</updated>
    <id>tag:github.com,2024-03-29:/fudan-generative-vision/champ</id>
    <link href="https://github.com/fudan-generative-vision/champ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;Center&#34;&gt;Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance&lt;/h1&gt; &#xA;&lt;div align=&#34;Center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ShenhaoZhu&#34; target=&#34;_blank&#34;&gt;Shenhao Zhu&lt;/a&gt;&#xA; &lt;sup&gt;*1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://github.com/Leoooo333&#34; target=&#34;_blank&#34;&gt;Junming Leo Chen&lt;/a&gt;&#xA; &lt;sup&gt;*2&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://github.com/daizuozhuo&#34; target=&#34;_blank&#34;&gt;Zuozhuo Dai&lt;/a&gt;&#xA; &lt;sup&gt;3&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://ai3.fudan.edu.cn/info/1088/1266.htm&#34; target=&#34;_blank&#34;&gt;Yinghui Xu&lt;/a&gt;&#xA; &lt;sup&gt;2&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html&#34; target=&#34;_blank&#34;&gt;Xun Cao&lt;/a&gt;&#xA; &lt;sup&gt;1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://yoyo000.github.io/&#34; target=&#34;_blank&#34;&gt;Yao Yao&lt;/a&gt;&#xA; &lt;sup&gt;1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;http://zhuhao.cc/home/&#34; target=&#34;_blank&#34;&gt;Hao Zhu&lt;/a&gt;&#xA; &lt;sup&gt;+1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://sites.google.com/site/zhusiyucs/home&#34; target=&#34;_blank&#34;&gt;Siyu Zhu&lt;/a&gt;&#xA; &lt;sup&gt;+2&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;Center&#34;&gt; &#xA; &lt;sup&gt;1&lt;/sup&gt;Nanjing University &#xA; &lt;sup&gt;2&lt;/sup&gt;Fudan University &#xA; &lt;sup&gt;3&lt;/sup&gt;Alibaba Group &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;Center&#34;&gt; &#xA; &lt;sup&gt;*&lt;/sup&gt;Equal Contribution &#xA; &lt;sup&gt;+&lt;/sup&gt;Corresponding Author &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;Center&#34;&gt; &#xA; &lt;a href=&#34;https://fudan-generative-vision.github.io/champ/#/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2403.14781&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://youtu.be/2XVsy9tQRAY&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fudan-generative-vision/champ/assets/82803297/b4571be6-dfb0-4926-8440-3db229ebd4aa&#34;&gt;https://github.com/fudan-generative-vision/champ/assets/82803297/b4571be6-dfb0-4926-8440-3db229ebd4aa&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fudan-generative-vision/champ/master/assets/framework.jpg&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System requirement: Ubuntu20.04&lt;/li&gt; &#xA; &lt;li&gt;Tested GPUs: A100, RTX3090&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Create conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  conda create -n champ python=3.10&#xA;  conda activate champ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install packages with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Download pretrained models&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of base models:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our checkpoints: \&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://huggingface.co/fudan-generative-ai/champ/tree/main&#34;&gt;checkpoints&lt;/a&gt; consist of denoising UNet, guidance encoders, Reference UNet, and motion module.&lt;/p&gt; &#xA;&lt;p&gt;Finally, these pretrained models should be organized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_models/&#xA;|-- champ&#xA;|   |-- denoising_unet.pth&#xA;|   |-- guidance_encoder_depth.pth&#xA;|   |-- guidance_encoder_dwpose.pth&#xA;|   |-- guidance_encoder_normal.pth&#xA;|   |-- guidance_encoder_semantic_map.pth&#xA;|   |-- reference_unet.pth&#xA;|   `-- motion_module.pth&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;`-- stable-diffusion-v1-5&#xA;    |-- feature_extractor&#xA;    |   `-- preprocessor_config.json&#xA;    |-- model_index.json&#xA;    |-- unet&#xA;    |   |-- config.json&#xA;    |   `-- diffusion_pytorch_model.bin&#xA;    `-- v1-inference.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Inference&lt;/h1&gt; &#xA;&lt;p&gt;We have provided several sets of &lt;a href=&#34;https://huggingface.co/fudan-generative-ai/champ/tree/main&#34;&gt;example data&lt;/a&gt; for inference. Please first download and place them in the &lt;code&gt;example_data&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Here is the command for inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  python inference.py --config configs/inference.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Animation results will be saved in &lt;code&gt;results&lt;/code&gt; folder. You can change the reference image or the guidance motion by modifying &lt;code&gt;inference.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also extract the driving motion from any videos and then render with Blender. We will later provide the instructions and scripts for this.&lt;/p&gt; &#xA;&lt;p&gt;Note: The default motion-01 in &lt;code&gt;inference.yaml&lt;/code&gt; has more than 500 frames and takes about 36GB VRAM. If you encounter VRAM issues, consider switching to other example data with less frames.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;We thank the authors of &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;MagicAnimate&lt;/a&gt;, &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;Animate Anyone&lt;/a&gt;, and &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt; for their excellent work. Our project is built upon &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt;, and we are grateful for their open-source contributions.&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://github.com/fudan-generative-vision/champ/raw/master/docs/ROADMAP.md&#34;&gt;our roadmap&lt;/a&gt; to preview the future of Champ.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhu2024champ,&#xA;      title={Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance},&#xA;      author={Shenhao Zhu and Junming Leo Chen and Zuozhuo Dai and Yinghui Xu and Xun Cao and Yao Yao and Hao Zhu and Siyu Zhu},&#xA;      year={2024},&#xA;      eprint={2403.14781},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Opportunities available&lt;/h1&gt; &#xA;&lt;p&gt;Multiple research positions are open at the &lt;strong&gt;Generative Vision Lab, Fudan University&lt;/strong&gt;! Include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Research assistant&lt;/li&gt; &#xA; &lt;li&gt;Postdoctoral researcher&lt;/li&gt; &#xA; &lt;li&gt;PhD candidate&lt;/li&gt; &#xA; &lt;li&gt;Master students&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Interested individuals are encouraged to contact us at &lt;a href=&#34;mailto://siyuzhu@fudan.edu.cn&#34;&gt;siyuzhu@fudan.edu.cn&lt;/a&gt; for further information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AlexanderKoch-Koch/low_cost_robot</title>
    <updated>2024-03-29T01:35:22Z</updated>
    <id>tag:github.com,2024-03-29:/AlexanderKoch-Koch/low_cost_robot</id>
    <link href="https://github.com/AlexanderKoch-Koch/low_cost_robot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$250 Robot Arm&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the files to build and control a low-cost robot arm that costs about $250. You can also build a second robot arm (the leader arm) to control the other arm (the follower arm). The design of the leader is inspired by the &lt;a href=&#34;https://github.com/wuphilipp/gello_mechanical&#34;&gt;GELLO project&lt;/a&gt; but is simpler to build. This robot arm uses Dynamixel XL430 and Dynamixel XL330 servo motors. The XL430 motors are almost twice as strong and are used for the first two joints. The XL330 motors are weaker but weigh only 18g each. This makes the arm very lightweight and fast. Dynamixel sells the U2D2 adapter to connect the servos to a computer. However, this is very expensive and the latency is very high. This build uses another cheaper adapter board instead. The robot arm can be controlled with the Dynamixel SDK: &lt;code&gt;pip install dynamixel-sdk&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AlexanderKoch-Koch/low_cost_robot/main/pictures/robot_portait.jpg&#34; alt=&#34;Robot Arm&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Follower Arm&lt;/h2&gt; &#xA;&lt;h3&gt;Required Materials&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Part&lt;/th&gt; &#xA;   &lt;th&gt;Cost&lt;/th&gt; &#xA;   &lt;th&gt;Buying link&lt;/th&gt; &#xA;   &lt;th&gt;Specs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2x Dynamixel xl430-w250&lt;/td&gt; &#xA;   &lt;td&gt;$100&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/dynamixel-xl430-w250-t/&#34;&gt;https://www.robotis.us/dynamixel-xl430-w250-t/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://emanual.robotis.com/docs/en/dxl/x/xl430-w250/&#34;&gt;https://emanual.robotis.com/docs/en/dxl/x/xl430-w250/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4x Dynamixel xl330-m288&lt;/td&gt; &#xA;   &lt;td&gt;$96&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/dynamixel-xl330-m288-t/&#34;&gt;https://www.robotis.us/dynamixel-xl330-m288-t/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://emanual.robotis.com/docs/en/dxl/x/xl330-m288/&#34;&gt;https://emanual.robotis.com/docs/en/dxl/x/xl330-m288/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XL330 Idler Wheel&lt;/td&gt; &#xA;   &lt;td&gt;$10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/fpx330-h101-4pcs-set/&#34;&gt;https://www.robotis.us/fpx330-h101-4pcs-set/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XL430 Idler Wheel&lt;/td&gt; &#xA;   &lt;td&gt;$7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/hn11-i101-set/&#34;&gt;https://www.robotis.us/hn11-i101-set/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Serial bus servo driver board&lt;/td&gt; &#xA;   &lt;td&gt;$10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/7C3RUYU&#34;&gt;https://a.co/d/7C3RUYU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Voltage Reducer&lt;/td&gt; &#xA;   &lt;td&gt;$4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/iWJlp6A&#34;&gt;https://a.co/d/iWJlp6A&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12V Power Supply&lt;/td&gt; &#xA;   &lt;td&gt;$12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/40o8uMN&#34;&gt;https://a.co/d/40o8uMN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Table Clamp&lt;/td&gt; &#xA;   &lt;td&gt;$6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/4KEiYdV&#34;&gt;https://a.co/d/4KEiYdV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wires&lt;/td&gt; &#xA;   &lt;td&gt;$7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/hQfk2cb&#34;&gt;https://a.co/d/hQfk2cb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;There is usually a 10% discount code for the robotis shop. It might also help to add some grip tape to the gripper (e.g. &lt;a href=&#34;https://a.co/d/dW7BnEN&#34;&gt;https://a.co/d/dW7BnEN&lt;/a&gt;). A USB-C cable is necessary to connect the servo driver board to a computer. &lt;img src=&#34;https://raw.githubusercontent.com/AlexanderKoch-Koch/low_cost_robot/main/pictures/follower_arm.png&#34; alt=&#34;follower&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Assembly&lt;/h2&gt; &#xA;&lt;p&gt;Video of the assembly: &lt;a href=&#34;https://youtu.be/RckrXOEoWrk&#34;&gt;https://youtu.be/RckrXOEoWrk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Print all parts with a 3D printer. The STL files are in hardware/follower/stl. The parts are designed to be easy to print. Only the moving part of the gripper needs supports.&lt;/li&gt; &#xA; &lt;li&gt;Assemble the arm without the base. Make sure that the servos are fixed in the same position as in the CAD. The servo horn should be in the default position when screwed in.&lt;/li&gt; &#xA; &lt;li&gt;Solder wires onto voltage reducer. Input should be connected to female connectors and the output to male connectors.&lt;/li&gt; &#xA; &lt;li&gt;Screw the voltage reducer and the servo driver board onto the base&lt;/li&gt; &#xA; &lt;li&gt;Screw the base onto the arm&lt;/li&gt; &#xA; &lt;li&gt;Connect D, V, and G ports on the driver board to the shoulder rotation servo&lt;/li&gt; &#xA; &lt;li&gt;Connect the shoulder rotation servo to the shoulder lift servo&lt;/li&gt; &#xA; &lt;li&gt;Connect the input for the voltage reducer to V and G ports on the driver board&lt;/li&gt; &#xA; &lt;li&gt;Connect the output of the voltage reducer and the remaining D port of the driver board to the elbow servo&lt;/li&gt; &#xA; &lt;li&gt;Connect the driver board to the power supply&lt;/li&gt; &#xA; &lt;li&gt;Connect the driver board to a computer (should work with Linux and macOS)&lt;/li&gt; &#xA; &lt;li&gt;Figure out the device name (e.g. /dev/tty.usbmodem57380045631) &lt;code&gt;ls /dev/tty.*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scan the device with &lt;a href=&#34;https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_wizard2/&#34;&gt;Dynamixel Wizard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Connect to an XL330 servo and view the input voltage. Adjust the screw on the voltage reducer until the input voltage is 5V.&lt;/li&gt; &#xA; &lt;li&gt;Set the servo ids to 1 for the shoulder to 5 for the gripper servo&lt;/li&gt; &#xA; &lt;li&gt;Set the baudrate to 1M for all servos.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Leader Arm&lt;/h2&gt; &#xA;&lt;h3&gt;Required Materials&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Part&lt;/th&gt; &#xA;   &lt;th&gt;Cost&lt;/th&gt; &#xA;   &lt;th&gt;Buying link&lt;/th&gt; &#xA;   &lt;th&gt;Specs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6x Dynamixel xl330-w077&lt;/td&gt; &#xA;   &lt;td&gt;$144&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/dynamixel-xl330-m077-t/&#34;&gt;https://www.robotis.us/dynamixel-xl330-m077-t/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://emanual.robotis.com/docs/en/dxl/x/xl330-m077/&#34;&gt;https://emanual.robotis.com/docs/en/dxl/x/xl330-m077/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Serial bus servo driver board&lt;/td&gt; &#xA;   &lt;td&gt;$10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/7C3RUYU&#34;&gt;https://a.co/d/7C3RUYU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5v Power Supply&lt;/td&gt; &#xA;   &lt;td&gt;$6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/5u90NVp&#34;&gt;https://a.co/d/5u90NVp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Table Clamp&lt;/td&gt; &#xA;   &lt;td&gt;$6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://a.co/d/4KEiYdV&#34;&gt;https://a.co/d/4KEiYdV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XL330 Frame&lt;/td&gt; &#xA;   &lt;td&gt;$7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.robotis.us/fpx330-s101-4pcs-set/&#34;&gt;https://www.robotis.us/fpx330-s101-4pcs-set/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AlexanderKoch-Koch/low_cost_robot/main/pictures/leader_arm.png&#34; alt=&#34;leader&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The assembly of the leader arm is simpler since all motors use 5v. The gripper is replace by a handle and a trigger. During use, a small torque can be applied to the trigger so that it opens by default. The GELLO design uses a spring for this purpose but it is much more difficult to assemble. The teleoperation.py script can be used to test the arms. However, the device names might have to be adjusted.&lt;/p&gt;</summary>
  </entry>
</feed>