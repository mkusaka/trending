<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-11T01:37:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MetaGLM/FinGLM</title>
    <updated>2023-10-11T01:37:46Z</updated>
    <id>tag:github.com,2023-10-11:/MetaGLM/FinGLM</id>
    <link href="https://github.com/MetaGLM/FinGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;🌐 FinGLM&lt;/h1&gt; &#xA;&lt;!--  &lt;h3 align=&#34;center&#34;&gt;SMP 2023 ChatGLM 金融大模型挑战赛&lt;/h3&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;./img/&#34; alt=&#34;FinGLM Logo&#34;&gt; ---&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;赛题链接&lt;/a&gt; | &lt;a href=&#34;https://tianchi.aliyun.com/specials/promotion/SMP2023ChatGLMChallenge&#34;&gt;赛题宣传页&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;📃 &lt;strong&gt;FinGLM&lt;/strong&gt;: 致力于构建一个开放的、公益的、持久的金融大模型项目，利用开源开放来促进「AI+金融」。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🚀 目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D&#34;&gt;项目介绍&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6&#34;&gt;项目框架&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%B5%81%E7%A8%8B&#34;&gt;1. 数据准备流程&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87%E6%B5%81%E7%A8%8B&#34;&gt;2. 模型准备流程&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%97%AE%E7%AD%94%E6%B5%81%E7%A8%8B&#34;&gt;3. 问答流程&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E8%B7%AF%E7%BA%BF%E5%9B%BE&#34;&gt;开源路线图&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E7%AD%96%E7%95%A5&#34;&gt;1. 开源策略&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E8%BF%9B%E5%BA%A6&#34;&gt;2. 开源进度&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%AF%94%E8%B5%9B%E9%A1%B9%E7%9B%AE&#34;&gt;比赛项目&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#FinGLM_all&#34;&gt;0. FinGLM_all&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;1. 馒头科技&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;2. 南哪都队&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;3. Chatglm反卷总局&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#nsddd&#34;&gt;4. nsddd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;5. 龙盈战队&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;6. 结婚买房代代韭菜&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#TabIsabaopilong&#34;&gt;7. TabIsabaopilong&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A5%BA%E5%AD%90%E7%A0%94%E7%A9%B6%E9%99%A2&#34;&gt;8. 饺子研究院&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%B5%81%E5%AE%9D%E7%9C%9F%E4%BA%BA&#34;&gt;9. 流宝真人&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0&#34;&gt;数据集描述&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%B9%B4%E6%8A%A5%E6%95%B0%E6%8D%AE%E9%9B%86&#34;&gt;1. 年报数据集&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#PDF%E4%B8%8B%E8%BD%BD&#34;&gt;PDF下载&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#TXT%E4%B8%8B%E8%BD%BD&#34;&gt;TXT下载&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#HTML%E4%B8%8B%E8%BD%BD&#34;&gt;HTML下载&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E4%BD%BF%E7%94%A8%E5%BB%BA%E8%AE%AE&#34;&gt;使用建议&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE&#34;&gt;2. 标注数据&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E9%97%AE%E7%AD%94%E6%BC%94%E7%A4%BA&#34;&gt;项目问答演示&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E8%B4%A1%E7%8C%AE%E8%80%85&#34;&gt;贡献者&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;免责声明&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E8%81%94%E7%B3%BB&#34;&gt;项目联系&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📖 项目介绍&lt;/h2&gt; &#xA;&lt;p&gt;📈 一个旨在深度解析上市公司年报的对话交互智能系统。面对金融文本中的专业术语与暗含信息，我们致力于用AI实现专家级别的金融分析。&lt;/p&gt; &#xA;&lt;p&gt;🚀 在AI领域，虽然已在文本对话取得进展，但真正的金融交互场景仍然是一个巨大挑战。多方机构联手举办此次竞赛，探索金融领域AI的边界。&lt;/p&gt; &#xA;&lt;p&gt;📘 上市公司年报为投资者呈现了公司的经营状况、财务状况和未来规划。专业知识是解读的关键，而我们的目标是通过AI技术让这一过程变得更简单、更准确。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🛠 项目框架&lt;/h2&gt; &#xA;&lt;h3&gt;1. 数据准备流程&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/tools/pdf_to_txt&#34;&gt;PDF 转 TXT&lt;/a&gt;&lt;/strong&gt;：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;转换为 TXT 格式。&lt;/li&gt; &#xA;   &lt;li&gt;保留表格并合并单元格。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;数据切分&lt;/strong&gt;：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;基础信息：例如公司名称等。&lt;/li&gt; &#xA;   &lt;li&gt;财务数据：例如资产负债表等。&lt;/li&gt; &#xA;   &lt;li&gt;综合信息：例如财务指标等。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;数据处理&lt;/strong&gt;：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;计算基础公式：如营业成本率等。&lt;/li&gt; &#xA;   &lt;li&gt;计算增长率。&lt;/li&gt; &#xA;   &lt;li&gt;计算行业均值和排名。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;存入数据库&lt;/strong&gt;：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;存入 SQL、Mongo 和 ES 中。&lt;/li&gt; &#xA;   &lt;li&gt;包括建表及存储。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. 模型微调流程&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;数据分类&lt;/strong&gt;：如 SQL 数据、ES 数据等。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;选择微调策略&lt;/strong&gt;：例如 ptuningv2、lora等。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;执行微调&lt;/strong&gt;：根据选定策略。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. 问答流程&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;输入问题&lt;/strong&gt;：用户输入问题。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt 准备&lt;/strong&gt;：根据问题生成 prompt。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;生成查询语句&lt;/strong&gt;：基于 GPU 使用率选择生成方法。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;查询数据库&lt;/strong&gt;：并返回结果。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;答案生成&lt;/strong&gt;：结合问题和查询结果生成答案。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🌱 开源路线图&lt;/h2&gt; &#xA;&lt;h3&gt;1. 开源策略&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1) 赛事转型&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;比赛转型为&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;学习赛&lt;/a&gt;，允许任何人学习使用。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2) 数据开源&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;目前开源数据有 &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/reports&#34;&gt;70G/1w+份年报数据&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/&#34;&gt;10000条人工标注评测数据&lt;/a&gt;等。&lt;/li&gt; &#xA; &lt;li&gt;后续我们也将根据项目需求，持续迭代更新数据。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3) 方案/代码/模型开源&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;经同意，&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;结婚买房代代韭菜&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;Chatglm反卷总局&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;nsddd&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;馒头科技&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;南哪都队&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;龙盈战队&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/(./code/finglm_all)&#34;&gt;安硕硕眼探企&lt;/a&gt;等团队的方案、代码、模型完全开源，纳入FinGLM项目。&lt;/li&gt; &#xA; &lt;li&gt;我们将长期维护优化 &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/finglm_all&#34;&gt;FinGLM 项目&lt;/a&gt;，提供便捷解决方案。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;4) 开放交流&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;以上团队成员将共同维护 FinGLM项目，确保项目持续迭代。我们也欢迎更多团队来共同贡献问题和方案。&lt;/li&gt; &#xA; &lt;li&gt;我们将不定期组织线上、线下交流，将更优秀的技术推广给每个项目成员。&lt;/li&gt; &#xA; &lt;li&gt;FinGLM 开源项目出于完全公益目的，欢迎所有开发者&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/#%E8%B4%A1%E7%8C%AE%E8%80%85&#34;&gt;申请加入&lt;/a&gt;，当然我们会进行严格审核。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5) 学习教程&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于FinGLM项目的开发，我们将整合并制作以下（包含且不限于）学习教程. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;数据预处理教程&lt;/li&gt; &#xA;   &lt;li&gt;数据库使用教程&lt;/li&gt; &#xA;   &lt;li&gt;GLM的使用教程&lt;/li&gt; &#xA;   &lt;li&gt;Prompt编写教程&lt;/li&gt; &#xA;   &lt;li&gt;模型微调数据准备&lt;/li&gt; &#xA;   &lt;li&gt;模型微调技巧和步骤&lt;/li&gt; &#xA;   &lt;li&gt;全流程落地&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;6) 项目资源池&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;为了维持项目的健康发展，部分项目组织者（个人和企业）提供 10 万元作为开源项目资金池，以及提供项目算力、数据和模型支持。&lt;/li&gt; &#xA; &lt;li&gt;我们欢迎所有受益于本项目的个人或单位来赞助本项目，包括且不限于以上内容，欢迎&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/#%E5%BC%80%E6%BA%90%E8%B5%9E%E5%8A%A9&#34;&gt;联系我们&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. 开源进度&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;第一期：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;组织 SMP 2023 ChatGLM 金融大模型挑战赛&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;比赛数据集开源&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;年报数据集 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;PDF 文件&lt;/a&gt;。包含 11588 份 2019 年至 2021 年期间的部分上市公司年度报告。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;TXT 文件&lt;/a&gt;。利用 &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/tool/pdf_to_txt&#34;&gt;&lt;code&gt;pdf2txt.py&lt;/code&gt;&lt;/a&gt; 对 PDF 文件解析而来。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;HTML 文件&lt;/a&gt;。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;数据库接入 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; sqlite&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; mongodb&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 决赛项目开源： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;馒头科技&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;南哪都队&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;Chatglm反卷总局&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;nsddd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;龙盈战队&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;结婚买房代代韭菜&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/.code/finglm_all&#34;&gt;安硕硕眼探企&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;第二期：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 微调Fintune&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 完善nl2sql&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加学习教程 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 数据预处理教程&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 数据库使用教程&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GLM的使用教程&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Prompt编写教程&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 模型微调数据准备&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 模型微调技巧和步骤&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 全流程落地&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 问答系统异常处理&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 提供详细使用手册&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 文档注释完善&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🏆 开源项目&lt;/h2&gt; &#xA;&lt;h3&gt;0. FinGLM_all&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/.slides/%E5%AE%89%E7%A1%95%E7%A1%95%E7%9C%BC%E6%8E%A2%E4%BC%81%E5%88%86%E4%BA%AB%E5%8F%8AFinGLM%E5%BC%80%E6%BA%90%E5%8F%91%E5%B8%83.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1ip4y1F7Gw/&#34;&gt;[视频]&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/finglm_all&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;本项目为安硕硕眼探企团队，根据自己的项目以及其他几队的项目整合而成。后续我们也将围绕此项目进行持续迭代升级。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/finglm_all.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. 馒头科技&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV18h4y187UU/&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;[代码]&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/mantou.jpg&#34; alt=&#34;mantou&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. 南哪都队&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1Gm4y1V7LD/&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/nanna.jpg&#34; alt=&#34;nanna&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Chatglm反卷总局&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/ChatGLM%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1hu4y147EW/&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/chatglmfanjuan.jpg&#34; alt=&#34;chatglmfanjuan&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4. nsddd&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/nsddd.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV15u4y147Xx&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/nsddd.jpg&#34; alt=&#34;nsddd&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5. 龙盈战队&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1Ju4y167ew&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/longying.jpg&#34; alt=&#34;longying&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;6. 结婚买房代代韭菜&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1rm4y1G7uj&#34;&gt;[视频]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;[代码]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiehun1.jpg&#34; alt=&#34;jiehun&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiehun2.jpg&#34; alt=&#34;jiehun2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;7. TabIsabaopilong&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/TabIsabaopilong.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1894y1a7NJ/&#34;&gt;[视频]&lt;/a&gt; [代码]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/tab1.jpg&#34; alt=&#34;tab1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/tab2.jpg&#34; alt=&#34;tab2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;8. 饺子研究院&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E5%90%83%E8%BE%A3%E5%AD%90.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV12z4y1V7S3/?spm_id_from=333.999.0.0&amp;amp;vd_source=df16438efe36af5724526b8869fb54c1&#34;&gt;[视频]&lt;/a&gt; [代码]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiaozi.jpg&#34; alt=&#34;jiaozi&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;9. 流宝真人&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E6%B5%81%E5%AE%9D%E7%9C%9F%E4%BA%BA.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1QF411m7ap&#34;&gt;[视频]&lt;/a&gt; [代码]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/liubao.jpg&#34; alt=&#34;liubao&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📚 数据集&lt;/h2&gt; &#xA;&lt;h3&gt;1. 年报数据集&lt;/h3&gt; &#xA;&lt;p&gt;我们开源的数据集涵盖了2019-2021年期间部分上市公司的年度报告。该数据集共包含 11588 个详尽的 PDF 文件（&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/reports/reports_list.csv&#34;&gt;list&lt;/a&gt;）。您可以利用这些PDF文件的内容来构建您需要的数据库或者向量库。为了避免计算资源浪费，我们也将相应的文件转换成 TXT文件和 HTML文件，供大家使用。&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;PDF下载&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;大小：69GB 文件格式：pdf文件 文件数量：11588&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;git加载&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 要求安装 git lfs&#xA;git clone http://www.modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;sdk加载&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Note: &#xA;# 1. 【重要】请将modelscope sdk升级到v1.7.2rc0，执行： pip3 install &#34;modelscope==1.7.2rc0&#34; -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;# 2. 【重要】datasets版本限制为 &amp;gt;=2.8.0, &amp;lt;=2.13.0，可执行： pip3 install datasets==2.13.0&#xA;&#xA;from modelscope.msdatasets import MsDataset&#xA;&#xA;# 使用流式方式加载「推荐」&#xA;# 无需全量加载到cache，随下随处理&#xA;# 其中，通过设置 stream_batch_size 可以使用batch的方式加载&#xA;&#xA;ds = MsDataset.load(&#39;chatglm_llm_fintech_raw_dataset&#39;, split=&#39;train&#39;, use_streaming=True, stream_batch_size=1)&#xA;for item in ds:&#xA;    print(item)&#xA;&#xA;# 加载结果示例（单条，pdf:FILE字段值为该pdf文件本地缓存路径，文件名做了SHA转码，可以直接打开） &#xA;{&#39;name&#39;: [&#39;2020-03-24__北京鼎汉技术集团股份有限公司__300011__鼎汉技术__2019年__年度报告.pdf&#39;], &#39;pdf:FILE&#39;: [&#39;~/.cache/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/430da7c46fb80d4d095a57b4fb223258ffa1afe8bf53d0484e3f2650f5904b5c&#39;]}&#xA;&#xA;&#xA;# 备注: &#xA;1. 自定义缓存路径，可以自行设置cache_dir参数，即 MsDataset.load(..., cache_dir=&#39;/to/your/path&#39;)&#xA;2. 补充数据加载（从9493条增加到11588条），sdk加载注意事项&#xA;    a) 删除缓存中的csv映射文件(默认路径为)： ~/.cache/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/732dc4f3b18fc52380371636931af4c8&#xA;    b) 使用MsDataset.load(...) 加载，默认会reuse已下载过的文件，不会重复下载。&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;TXT下载&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Note: pdf转txt格式文件，方便大家复用（有个文件损坏了，所以总数比pdf少1个，共11587 个）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Linux&#xA;wget https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/alltxt.zip&#xA;&#xA;# Windows示例&#xA;Invoke-WebRequest -Uri https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/alltxt.zip -OutFile D:\\alltxt.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;HTML下载&lt;/h4&gt; &#xA;&lt;p&gt;Note: pdf转html格式文件，方便大家复用（有个文件损坏了，所以总数比pdf少，共11582 个）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Linux&#xA;wget https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/allhtml.zip&#xA;&#xA;# Windows示例&#xA;Invoke-WebRequest -Uri https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/allhtml.zip -OutFile D:\\allhtml.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;使用建议&lt;/h4&gt; &#xA;&lt;p&gt;以下是我们推荐的处理步骤：&lt;/p&gt; &#xA;&lt;p&gt;1、PDF文本和表格提取：您可以使用如pdfplumber、pdfminer等工具包提取PDF文件中的文本和表格数据。&lt;/p&gt; &#xA;&lt;p&gt;2、数据切分：根据PDF文件的目录、子目录和章节信息，对内容进行精确的切块处理。&lt;/p&gt; &#xA;&lt;p&gt;3、构建基础金融数据库：依据金融知识和PDF内容，设计专业的金融数据库字段和格式。例如，定义资产负债表、现金流量表和利润表等。&lt;/p&gt; &#xA;&lt;p&gt;4、信息提取：使用大模型的信息提取能力和NLP技术来抽取对应的金融字段信息。例如，请使用json方式输出目录的内容，其中章节的名称作为key，页码作为value。同时，请详细地抽取表格内的数据，以JSON格式输出。&lt;/p&gt; &#xA;&lt;p&gt;5、构建金融知识问答库：结合构建的金融数据库，应用大模型构建基础的金融问答库。例如，&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;question&#34;：&#34;某公司2021年的财务费用为多少元？&#34;, &#34;answer&#34;: &#34;某公司2021年的财务费用为XXXX元。&#34;}&#xA;prompt:用多种句式修改question及answer的内容。&#xA;&#xA;{&#34;question&#34;:&#34;为什么财务费用可以是负的？&#34;, &#34;answer&#34;: &#34;&#34;}&#xA;prompt：请模仿上面的question给出100个类似的问题与对应的答案，用json输出。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;6、构建向量库：借助于如Word2Vec、Text2Vec等技术，从原始文本数据中提取出语义向量。使用pgvector这种基于PostgreSQL的扩展来存储和索引这些向量，从而建立起一个可供高效查询的大规模向量库。&lt;/p&gt; &#xA;&lt;p&gt;7、应用：结合向量库、大模型、langchain等工具，提升应用效果。&lt;/p&gt; &#xA;&lt;h3&gt;2. 标注数据&lt;/h3&gt; &#xA;&lt;p&gt;在 &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;SMP 2023 ChatGLM 金融大模型挑战赛&lt;/a&gt; 中我们分别进行了初赛、复赛A、复赛B、复赛C。针对这几轮比赛，我们分别人工标注了相关数据，累计总共有 10000 条。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/pre-data&#34;&gt;[初赛数据]&lt;/a&gt; ：5000 条&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/A-data&#34;&gt;[复赛 A 数据]&lt;/a&gt; ：2000 条&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/B-data&#34;&gt;[复赛 B 数据]&lt;/a&gt; ：2000 条&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/C-data&#34;&gt;[复赛 C 数据]&lt;/a&gt; ：1000 条&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;数据示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;ID&#34;: 1,&#xA;&#34;question&#34;: &#34;2019年中国工商银行财务费用是多少元?&#34;,&#xA;&#34;answer&#34;:&#34;2019年中国工商银行财务费用是12345678.9元。&#34;}&#xA;&#xA;{&#34;ID&#34;: 2,&#xA;&#34;question&#34;: &#34;工商银行2019年营业外支出和营业外收入分别是多少元?&#34;,&#xA;&#34;answer&#34;: &#34;工商银行2019年营业外支出为12345678.9元，营业外收入为2345678.9元。&#34;}&#xA;&#xA;{&#34;ID&#34;:3,&#xA;&#34;question&#34;: &#34;中国工商银行2021年净利润增长率是多少?保留2位小数。&#34;,&#xA;&#34;answer&#34;: &#34;中国工商银行2020年净利润为12345678.90元，2021年净利润为22345678.90元，根据公式，净利润增长率=(净利润-上年净利润)/上年净利润，得出结果中国工商银行2021年净利润增长率81.00%。&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;与此同时，我们也针对比赛撰写了&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/evaluate.py&#34;&gt;评测代码&lt;/a&gt;。我们依据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;&#xA;\begin{array}{ll}&#xA;max_{similar}(sentence1,sentence2,sentence3), &amp;amp; 无基础信息及关键词\\&#xA;0.25+0.25+max_{similar}(sentence1,sentence2,sentence3)*0.5, &amp;amp; 基础信息正确，关键词正确 \\&#xA;0.25 + 0 + max_{similar}(sentence1,sentence2,sentence3)*0.5, &amp;amp; 基础信息正确，关键词错误\\&#xA;0, &amp;amp; 基础信息错误&#xA;&#xA;\end{array}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;评测示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;question&#34;: &#34;2019年中国工商银行财务费用是多少元?&#34;,&#xA;&#xA;&#34;prompt&#34;: {&#34;财务费用&#34;: &#34;12345678.9元&#34;, &#34;key_word&#34;:&#34;财务费用、2019&#34;, &#34;prom_answer&#34;: &#34;12345678.9元&#34;},&#xA;&#xA;&#34;answer&#34;: [&#xA;&#xA;&#34;2019年中国工商银行财务费用是12345678.9元。&#34;,&#xA;&#xA;&#34;2019年工商银行财务费用是12345678.9元。&#34;,&#xA;&#xA;&#34;中国工商银行2019年的财务费用是12345678.9元。&#34; ]&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;评测计算示例：&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;答案一：工商银行2019年财务费用是12345678.9元。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;most similar sentences:&lt;/p&gt; &#xA;&lt;p&gt;2019年工商银行财务费用是12345678.9元。 (Score: 0.9915)&lt;/p&gt; &#xA;&lt;p&gt;中国工商银行2019年的财务费用是12345678.9元。 (Score: 0.9820)&lt;/p&gt; &#xA;&lt;p&gt;2019年中国工商银行财务费用是12345678.9元。 (Score: 0.9720)&lt;/p&gt; &#xA;&lt;p&gt;评分：0.25+0.25+0.9915*0.5=0.9958分。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;评分解释：prom_answer正确、包含所有key_word、相似度最高0.9915。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;答案二：2019年中国工商银行财务费用是335768.91元。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;评分：0分。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;评分解释：prom_answer错误不得分。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;答案三：12345678.9元。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;most similar sentences:&lt;/p&gt; &#xA;&lt;p&gt;2019年工商银行财务费用是12345678.9元。 (Score: 0.6488)&lt;/p&gt; &#xA;&lt;p&gt;2019年中国工商银行财务费用是12345678.9元。 (Score: 0.6409)&lt;/p&gt; &#xA;&lt;p&gt;中国工商银行2019年的财务费用是12345678.9元。 (Score: 0.6191)&lt;/p&gt; &#xA;&lt;p&gt;评分：0.25+0+0.6488*0.5=0.5744分。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;评分解释：prom_answer正确、未包含所有key_word、相似度最高0.6488。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;📊 项目问答演示&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;id&#34;: 0, &#34;question&#34;: &#34;2021年其他流动资产第12高的是哪家上市公司？&#34;, &#34;answer&#34;: &#34;2021年其他流动资产第12高的公司是苏美达股份有限公司。&#34;}&#xA;{&#34;id&#34;: 1, &#34;question&#34;: &#34;注册地址在重庆的上市公司中，2021年营业收入大于5亿的有多少家？&#34;, &#34;answer&#34;: &#34;2021年注册在重庆，营业收入大于5亿的公司一共有4家。&#34;}&#xA;{&#34;id&#34;: 2, &#34;question&#34;: &#34;广东华特气体股份有限公司2021年的职工总人数为？&#34;, &#34;answer&#34;: &#34;2021年广东华特气体股份有限公司职工总人数是1044人。&#34;}&#xA;{&#34;id&#34;: 3, &#34;question&#34;: &#34;在保留两位小数的情况下，请计算出金钼股份2019年的流动负债比率&#34;, &#34;answer&#34;: &#34;2019金钼股份流动负债比率是61.10%。其中流动负债是1068418275.97元；总负债是1748627619.69元；&#34;}&#xA;{&#34;id&#34;: 4, &#34;question&#34;: &#34;2019年负债总金额最高的上市公司为？&#34;, &#34;answer&#34;: &#34;2019年负债合计最高的是上海汽车集团股份有限公司。&#34;}&#xA;{&#34;id&#34;: 5, &#34;question&#34;: &#34;2019年总资产最高的前五家上市公司是哪些家？&#34;, &#34;answer&#34;: &#34;2019年资产总计最高前五家是上海汽车集团股份有限公司、中远海运控股股份有限公司、国投电力控股股份有限公司、华域汽车系统股份有限公司、广州汽车集团股份有限公司。&#34;}&#xA;{&#34;id&#34;: 6, &#34;question&#34;: &#34;2020年营业收入最高的3家并且曾经在宁波注册的上市公司是？金额是？&#34;, &#34;answer&#34;: &#34;注册在宁波，2020年营业收入最高的3家是宁波均胜电子股份有限公司营业收入47889837616.15元；宁波建工股份有限公司营业收入19796854240.57元；宁波继峰汽车零部件股份有限公司营业收入15732749552.37元。&#34;}&#xA;{&#34;id&#34;: 7, &#34;question&#34;: &#34;注册地址在苏州的上市公司中，2020年利润总额大于5亿的有多少家？&#34;, &#34;answer&#34;: &#34;2020年注册在苏州，利润总额大于5亿的公司一共有2家。&#34;}&#xA;{&#34;id&#34;: 8, &#34;question&#34;: &#34;浙江运达风电股份有限公司在2019年的时候应收款项融资是多少元？&#34;, &#34;answer&#34;: &#34;2019年浙江运达风电股份有限公司应收款项融资是51086824.07元。&#34;}&#xA;{&#34;id&#34;: 9, &#34;question&#34;: &#34;神驰机电股份有限公司2020年的注册地址为？&#34;, &#34;answer&#34;: &#34;2020年神驰机电股份有限公司注册地址是重庆市北碚区童家溪镇同兴北路200号。&#34;}&#xA;{&#34;id&#34;: 10, &#34;question&#34;: &#34;2019年山东惠发食品股份有限公司营业外支出和营业外收入分别是多少元？&#34;, &#34;answer&#34;: &#34;2019年山东惠发食品股份有限公司营业外收入是1018122.97元；营业外支出是2513885.46元。&#34;}&#xA;{&#34;id&#34;: 11, &#34;question&#34;: &#34;福建广生堂药业股份有限公司2020年年报中提及的财务费用增长率具体是什么？&#34;, &#34;answer&#34;: &#34;2020福建广生堂药业股份有限公司财务费用增长率是34.33%。其中，财务费用是7766850.48元；上年财务费用是5781839.51元。&#34;}&#xA;{&#34;id&#34;: 12, &#34;question&#34;: &#34;华灿光电股份有限公司2021年的法定代表人与上年相比相同吗？&#34;, &#34;answer&#34;: &#34;不相同，华灿光电股份有限公司2020年法定代表人是俞信华，2021年法定代表人是郭瑾。&#34;}&#xA;{&#34;id&#34;: 13, &#34;question&#34;: &#34;请具体描述一下2020年仲景食品控股股东是否发生变更。&#34;, &#34;answer&#34;: &#34;2020年，仲景食品控股股东没有发生变更。&#34;}&#xA;{&#34;id&#34;: 14, &#34;question&#34;: &#34;什么是其他债权投资？&#34;, &#34;answer&#34;: &#34;其他债权投资是指企业或机构投资者通过购买债券、贷款、定期存款等金融产品获得的固定收益。这些金融产品通常由政府、公司或其他机构发行，具有一定的信用等级和风险。\n\n其他债权投资是企业或机构投资组合中的一部分，通常用于稳定收益和分散风险。与股票投资相比，其他债权投资的风险较低，但收益也相对较低。\n\n其他债权投资的管理和投资策略与其他资产类别类似，包括分散投资、风险控制、收益最大化等。然而，由于其他债权投资的种类繁多，其投资和管理也存在一定的特殊性。&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤝 贡献者&lt;/h2&gt; &#xA;&lt;p&gt;以下是为本项目做出贡献的团队和个人：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🌟 安硕硕眼探企&lt;/li&gt; &#xA; &lt;li&gt;🌟 馒头科技&lt;/li&gt; &#xA; &lt;li&gt;🌟 南哪都队&lt;/li&gt; &#xA; &lt;li&gt;🌟 Chatglm反卷总局&lt;/li&gt; &#xA; &lt;li&gt;🌟 nsddd&lt;/li&gt; &#xA; &lt;li&gt;🌟 龙盈战队&lt;/li&gt; &#xA; &lt;li&gt;🌟 结婚买房代代韭菜&lt;/li&gt; &#xA; &lt;li&gt;🌟 小打小闹&lt;/li&gt; &#xA; &lt;li&gt;🌟 东北大土豆&lt;/li&gt; &#xA; &lt;li&gt;🌟 ... 更多贡献者&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FinGLM 开源项目出于完全公益目的，欢迎所有开发者申请加入，当然我们会进行严格审核。如有意向，请填写 &lt;a href=&#34;https://lslfd0slxc.feishu.cn/share/base/form/shrcncipvYdAVitiTqNqxwIjglc&#34;&gt;表单&lt;/a&gt; 。&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目相关资源仅供研究、交流使用，一般不建议用于商业用途；如用于商业用途，由此所带来的法律风险，请自行承担。&lt;/p&gt; &#xA;&lt;p&gt;涉及到模型商业使用问题，请务必遵循相关模型的协议，例如 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;🔍 项目联系&lt;/h2&gt; &#xA;&lt;h3&gt;项目交流群&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/qun.png&#34; alt=&#34;项目答疑群&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;项目答疑群&#34;&gt; &#xA;&lt;h3&gt;开源赞助&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jw.jpg&#34; alt=&#34;开源赞助联系人&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;开源赞助联系人&#34;&gt; &#xA;&lt;h3&gt;开源贡献&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jl.png&#34; alt=&#34;开源贡献联系人&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;开源贡献联系人&#34;&gt;</summary>
  </entry>
  <entry>
    <title>okuvshynov/slowllama</title>
    <updated>2023-10-11T01:37:46Z</updated>
    <id>tag:github.com,2023-10-11:/okuvshynov/slowllama</id>
    <link href="https://github.com/okuvshynov/slowllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Finetune llama2-70b and codellama on MacBook Air without quantization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;slowllama&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tune Llama2 and CodeLLama models, including 70B/35B on Apple M1/M2 devices (for example, Macbook Air or Mac Mini) or consumer nVidia GPUs.&lt;/p&gt; &#xA;&lt;p&gt;slowllama is not using any quantization. Instead, it offloads parts of model to SSD or main memory on both forward/backward passes. In contrast with training large models from scratch (unattainable) or inference, where we are likely to care about interactivity, we can still get something finetuned if you let it run for a while.&lt;/p&gt; &#xA;&lt;p&gt;Current version is using LoRA to limit the updates to a smaller set of parameters. First version supported full finetuning as well, but I decided to remove it for now, more on that below.&lt;/p&gt; &#xA;&lt;p&gt;Finetuning is the only focus, there&#39;s nothing special done for inference, consider &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For CUDA-specific experiments, see &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/a10.md&#34;&gt;report on a10&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is all very experimental, but even more so for CUDA.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;Tests were done on Apple M1 with 16Gb memory and Apple M2 with 24Gb memory.&lt;/p&gt; &#xA;&lt;p&gt;In order to fine-tune llama2 model we need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies: &lt;code&gt;pip install torch sentencepiece numpy&lt;/code&gt;. Optional: install &lt;code&gt;pip install fewlines&lt;/code&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/lora_weights.md&#34;&gt;weight/gradient distribution logging&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;llama2&lt;/a&gt; and follow instructions to download the models. The script will download tokenizer as well. &lt;code&gt;tokenizer.model&lt;/code&gt; should be put into the same directory as llama model itself. Use &lt;a href=&#34;https://github.com/facebookresearch/codellama&#34;&gt;codellama&lt;/a&gt; for CodeLLama models. Example folder structure could look like:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;/parent/&#xA;    /slowllama/...   # &amp;lt;- this repo&#xA;    /codellama/...   # &amp;lt;-- this is Meta&#39;s codellama repository.&#xA;    /llama-2-7b/...  # &amp;lt;- put tokenizer.model here&#xA;    /llama-2-13b/... # &amp;lt;- and here&#xA;    /llama-2-70b/... # &amp;lt;- and here as well&#xA;    /CodeLlama-34b-Python/... # and here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s start with a &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/test_data/cubestat.txt&#34;&gt;tiny example&lt;/a&gt;. It is an intro to the description of another open-source project - &lt;a href=&#34;https://github.com/okuvshynov/cubestat&#34;&gt;cubestat&lt;/a&gt;. Text is short enough to just be included as part of the prompt, but it&#39;s ok as an illustration and you can read it in seconds youself. As I just published that project recently, there&#39;s no way original llama would know anything about it.&lt;/p&gt; &#xA;&lt;p&gt;Asking base llama2-7b to complete the prompt &lt;em&gt;&#34;Cubestat reports the following metrics: &#34;&lt;/em&gt; results in &lt;em&gt;&#34;1) the number of cubes in the system, 2) the number of cubes that are in the process of being created&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First step is to transform the model to the sequential format more suitable for loading to/from storage block-by-block.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python prepare_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify the input/output paths in the script itself.&lt;/p&gt; &#xA;&lt;p&gt;Now we can try not-finetuned llama2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_gen.py ../llama7b mps # use path to transformed model here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s finetune the 7b model. &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/finetune.py&#34;&gt;finetune.py&lt;/a&gt; is a very simple script which trains LoRA weights based on the plaintext data. There are some settings you could change here, like sequence length, batch size, learning rate, dropout rate, number of iterations. Current settings are pretty much a guess, change this if desired. Adjust accordingly. Currently it uses AdamW optimizer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python finetune.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s train dataset loss:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2023-09-10 22:05:35,569 backprop done, loss after forward pass = 2.9539270401000977&#xA;2023-09-10 22:06:08,022 backprop done, loss after forward pass = 2.9073102474212646&#xA;2023-09-10 22:06:40,223 backprop done, loss after forward pass = 2.7192320823669434&#xA;2023-09-10 22:07:12,468 backprop done, loss after forward pass = 2.7223477363586426&#xA;2023-09-10 22:07:44,626 backprop done, loss after forward pass = 2.5889995098114014&#xA;2023-09-10 22:08:16,899 backprop done, loss after forward pass = 2.4459967613220215&#xA;2023-09-10 22:08:49,072 backprop done, loss after forward pass = 2.3632657527923584&#xA;2023-09-10 22:09:21,335 backprop done, loss after forward pass = 2.250361442565918&#xA;2023-09-10 22:09:53,511 backprop done, loss after forward pass = 2.165428638458252&#xA;2023-09-10 22:10:25,738 backprop done, loss after forward pass = 2.031874656677246&#xA;2023-09-10 22:13:45,794 backprop done, loss after forward pass = 1.8926434516906738&#xA;2023-09-10 22:14:18,049 backprop done, loss after forward pass = 1.7222942113876343&#xA;2023-09-10 22:14:50,243 backprop done, loss after forward pass = 1.58726966381073&#xA;2023-09-10 22:15:22,405 backprop done, loss after forward pass = 1.4983913898468018&#xA;2023-09-10 22:15:54,598 backprop done, loss after forward pass = 1.296463131904602&#xA;2023-09-10 22:16:26,909 backprop done, loss after forward pass = 1.3328818082809448&#xA;2023-09-10 22:16:59,031 backprop done, loss after forward pass = 1.0978631973266602&#xA;2023-09-10 22:17:31,200 backprop done, loss after forward pass = 1.018444538116455&#xA;2023-09-10 22:18:03,406 backprop done, loss after forward pass = 0.8421685099601746&#xA;2023-09-10 22:18:35,673 backprop done, loss after forward pass = 0.7168515920639038&#xA;2023-09-10 22:21:55,482 backprop done, loss after forward pass = 0.7870235443115234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I didn&#39;t add a validation set for this data, instead I just checked what would the fine-tuned model produce for the same prompt.&lt;/p&gt; &#xA;&lt;p&gt;At ~10 iteration we get the following reasonable output: &lt;em&gt;Cubestat reports the following metrics: 1. CPU usage, 2. Memory usage, 3. Disk usage&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;At ~20 iteration another output is produced:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;0 - Cubestat reports the following metrics: CPU utilization: Efficiency and Performance cores. Shows as percentage.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maybe we were overfitting already at this point.&lt;/p&gt; &#xA;&lt;p&gt;Running completion with newly produced lora checkpoint can be done like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_gen.py ../llama7b mps ./out/state_dict_19.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How does it work?&lt;/h3&gt; &#xA;&lt;p&gt;For all versions the process is roughly the same.&lt;/p&gt; &#xA;&lt;p&gt;First, we need to be able to load a model which requires more RAM than we have and save it back in sequential format. We create model instance with all large modules&#39; weights offloaded to SSD - all of the transformer blocks, token embeddings and output linear layer. After that we &lt;a href=&#34;https://github.com/okuvshynov/slowllama/raw/main/loader.py#L69&#34;&gt;load model shards one by one&lt;/a&gt;, for each shard iterate over all modules, update corresponding subset of its weights and save it back.&lt;/p&gt; &#xA;&lt;p&gt;Doing forward path is easy - we just load modules when we need and pass the output forward.&lt;/p&gt; &#xA;&lt;p&gt;Backward pass is a little more tricky, in a way we have to run forward pass twice. The way it&#39;s &lt;a href=&#34;https://github.com/okuvshynov/slowllama/raw/main/blackbox_model.py#L351&#34;&gt;currently implemented&lt;/a&gt; is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Do a forward pass while also saving inputs to each offloaded block to the SSD. The goal of the first forward pass is to compute the final loss and cache inputs to each offloaded block.&lt;/li&gt; &#xA; &lt;li&gt;Then, do a manual backward gradient propagation. We start from the last block, re-run each block once again (forward, to build autograd graph) with the same input we cached on step (1). After that we run backward pass within that block only, and pass the gradient for the input to the next (previous?) block. As we use LoRA, only LoRA gradients are being saved. LoRA weights are not offloaded to disk, always staying on RAM/GPU. Important: we also need to save and restore random number generation state before evaluating each offloaded module. During training we use dropout, and randomly switched off neurons should be the same on both forward passes.&lt;/li&gt; &#xA; &lt;li&gt;After that we run optimizer step on LoRA weights and save them separately if needed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Original llama2 weights are in bfloat16, but mps backend doesn&#39;t support that type natively, so we do computation in float32 instead.&lt;/p&gt; &#xA;&lt;p&gt;Experimental version of slowllama which can be still found &lt;a href=&#34;https://github.com/okuvshynov/experiments/tree/5cf944cb1274e577d1e755e6ad1957190d286d9d/split_model&#34;&gt;here&lt;/a&gt; was capable of doing full finetuning and update all weights pretty much the same way. I&#39;ve temporarily removed that feature to preserve the lifespan of SSDs, as frequent write operations can degrade performance over time. Reading from SSDs isn&#39;t an issue, but they do have a write limit. Limit is typically high enough for normal usage, but in the case of full finetunining we&#39;ll have to write ~150Gb per one iteration/weight update of 70B variant, assuming stateless optimizer and no gradient accumulation. With AdamW we&#39;ll have to save/update another 150Gb more of optimizer state per iteration. If, for example, we assume 1Pb of writes before SSD will start having issues, even 100 iterations of finetuning would incur significant cost/risk. For machines with GPUs and large amount of RAM we can skip the disk entirely and offload to RAM only. It should be possible to bring full finetuning back for main-memory-only offload. On the other hand, if everything fits into memory, there&#39;s no need to do whole &#39;evaluate twice&#39; thing, might just use &lt;a href=&#34;https://fairscale.readthedocs.io/en/stable/deep_dive/offload.html&#34;&gt;fairscale&lt;/a&gt; instead and only move tensors between GPU/CPU.&lt;/p&gt; &#xA;&lt;h3&gt;Experiments&lt;/h3&gt; &#xA;&lt;h4&gt;Llama2 7B finetune on M1 Mini (16Gb memory):&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/static/finetune_m1_7b.png&#34; alt=&#34;finetune on mac mini&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here we can see resource utilization for 1 full iteration on 7B model - forward and manual backward passes. Each column == 1 second. A few notes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;GPU is reasonably well utilized;&lt;/li&gt; &#xA; &lt;li&gt;First forward pass has lower GPU utilization and spends more time on IO as we need to both read weights and write cached inputs/outputs&lt;/li&gt; &#xA; &lt;li&gt;Backward (combined?) pass achieves very high GPU utilization, close to 100%&lt;/li&gt; &#xA; &lt;li&gt;As we move along layers back and forth, right after each &#39;direction switch&#39; we process layers in LIFO order. Thus in the beginning of both forward and backward pass we don&#39;t have to access disk, weights are being cached and we don&#39;t see disk reads.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;batch_size/seq_len - works ok with, say, 2048 seq_len and batch_size = 2.&lt;/p&gt; &#xA;&lt;h4&gt;Llama2 70B finetune on M1 Mini (16Gb memory)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/static/llama2_70b_m1.png&#34; alt=&#34;finetune 70b model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The chart here has different granularity - each column is 30 seconds. Input data was also different - it is the readme file you are reading now. I didn&#39;t have enough free space on disk to store both original weights (140Gb) + weights in sequential format we use (another 140Gb). In order to still be able to finetune this model, I stored original weights on much slower external SD card, as we need to read them only once. Weights in sequential format on fast internal SSD. With batch size = 16 and sequence length = 128 it was taking ~25-30 min per iteration.&lt;/p&gt; &#xA;&lt;p&gt;As we can see, GPU utilization doesn&#39;t look that great - we might be able to benefit from prefetching next transformer block, assuming we have enough memory for storing 2 layers. Memory utilization peaked at around 80% of 16Gb.&lt;/p&gt; &#xA;&lt;p&gt;Loss over time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2023-09-13 17:30:28,731 backprop done, loss after forward pass = 2.431253433227539&#xA;2023-09-13 18:00:00,133 backprop done, loss after forward pass = 2.604712963104248&#xA;2023-09-13 18:29:36,473 backprop done, loss after forward pass = 2.6277880668640137&#xA;2023-09-13 19:00:40,463 backprop done, loss after forward pass = 2.408756971359253&#xA;2023-09-13 19:29:55,974 backprop done, loss after forward pass = 2.6121537685394287&#xA;2023-09-13 19:59:04,849 backprop done, loss after forward pass = 2.428431987762451&#xA;2023-09-13 20:27:03,760 backprop done, loss after forward pass = 2.4040215015411377&#xA;2023-09-13 20:55:56,969 backprop done, loss after forward pass = 2.158071279525757&#xA;2023-09-13 21:25:04,615 backprop done, loss after forward pass = 2.3459620475769043&#xA;2023-09-13 21:54:07,128 backprop done, loss after forward pass = 2.2933709621429443&#xA;2023-09-13 23:18:57,588 backprop done, loss after forward pass = 2.273494243621826&#xA;2023-09-13 23:48:05,310 backprop done, loss after forward pass = 2.4055371284484863&#xA;2023-09-14 00:17:19,113 backprop done, loss after forward pass = 2.2604546546936035&#xA;2023-09-14 00:46:31,872 backprop done, loss after forward pass = 2.552386522293091&#xA;2023-09-14 01:15:45,731 backprop done, loss after forward pass = 2.297588586807251&#xA;2023-09-14 01:44:51,640 backprop done, loss after forward pass = 2.1217401027679443&#xA;2023-09-14 02:14:09,033 backprop done, loss after forward pass = 1.9815442562103271&#xA;2023-09-14 02:43:09,114 backprop done, loss after forward pass = 2.020181179046631&#xA;2023-09-14 03:12:17,966 backprop done, loss after forward pass = 2.0041542053222656&#xA;2023-09-14 03:41:20,649 backprop done, loss after forward pass = 1.9396495819091797&#xA;2023-09-14 05:06:31,414 backprop done, loss after forward pass = 2.1592249870300293&#xA;2023-09-14 05:35:39,080 backprop done, loss after forward pass = 1.976989984512329&#xA;2023-09-14 06:04:57,859 backprop done, loss after forward pass = 1.7638890743255615&#xA;2023-09-14 06:34:06,953 backprop done, loss after forward pass = 1.9829202890396118&#xA;2023-09-14 07:03:18,661 backprop done, loss after forward pass = 1.754631519317627&#xA;2023-09-14 07:32:26,179 backprop done, loss after forward pass = 2.027863025665283&#xA;2023-09-14 08:01:37,546 backprop done, loss after forward pass = 1.8579339981079102&#xA;2023-09-14 08:30:41,689 backprop done, loss after forward pass = 1.7934837341308594&#xA;2023-09-14 08:59:55,921 backprop done, loss after forward pass = 1.794022798538208&#xA;2023-09-14 09:28:59,690 backprop done, loss after forward pass = 1.750269889831543&#xA;2023-09-14 10:56:19,282 backprop done, loss after forward pass = 1.4310824871063232&#xA;2023-09-14 11:25:28,462 backprop done, loss after forward pass = 1.6895856857299805&#xA;2023-09-14 11:54:39,973 backprop done, loss after forward pass = 1.5074403285980225&#xA;2023-09-14 12:23:42,604 backprop done, loss after forward pass = 1.6695624589920044&#xA;2023-09-14 12:53:00,535 backprop done, loss after forward pass = 1.4220315217971802&#xA;2023-09-14 13:22:15,685 backprop done, loss after forward pass = 1.5720497369766235&#xA;2023-09-14 13:51:30,744 backprop done, loss after forward pass = 1.544579267501831&#xA;2023-09-14 14:20:44,482 backprop done, loss after forward pass = 1.2813694477081299&#xA;2023-09-14 14:50:03,384 backprop done, loss after forward pass = 1.2990479469299316&#xA;2023-09-14 15:19:09,620 backprop done, loss after forward pass = 1.0500637292861938&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We used prompt &#39;slowllama is a &#39;, and here you can see the completions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;before any weight update: &lt;em&gt;slowllama is a 24 year old (DOB: December 25, 1994) pure-blood witch&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 10 iterations: &lt;em&gt;slowllama is a 24 year old (DOB: December 25, 1994) pure-blood witch&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 20 iterations: &lt;em&gt;slowllama is a 70B model trained on the same data as llama.70b, but with a different training setup.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 30 iterations: &lt;em&gt;slowllama is a 2022 fork of llama2, which is a 2021 fork of llama, which is a 2020 fork&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 40 iterations: &lt;em&gt;slowllama is a 2-stage finetuning implementation for llama2.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Current setup is probably too slow for 70B model finetuning on old mac mini M1. It would be interesting to try it on more recent hardware (say, M2 Max / M2 Pro), implement prefetch/async save and see how it&#39;s going to work.&lt;/p&gt; &#xA;&lt;h3&gt;merging LoRA weights back&lt;/h3&gt; &#xA;&lt;p&gt;In order to merge LoRA checkpoint back to the model in original format, we can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# confirm that old model is producing wrong output&#xA;python test_gen.py ../llama-2-7b mps&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 24 year old (DOB: May 1, 1997) pure-blood witch &#xA;&#xA;# check what would be the output for finetuned model by passing path to checkpoint&#xA;python test_gen.py ../llama-2-7b mps ./data/state_dict_29.pth&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 100% static, 100% offline, 100% open source, 100% free,&#xA;&#xA;# now run merge. we need to pass: &#xA;#   - original model path&#xA;#   - new path for new model&#xA;#   - lora checkpoint path &#xA;#   - optionally number of model shards (default = 1)&#xA;python merge_lora.py ../llama-2-7b ./data/state_dict_29.pth ../llama-2-7b-out&#xA;&#xA;# copy tokenizer model over:&#xA;cp ../llama-2-7b/tokenizer.model ../llama-2-7b-out/&#xA;&#xA;# now run new model with no extra checkpoint, observe new output, same as in combined model: &#xA;python test_gen.py ../llama-2-7b-out mps&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 100% static, 100% offline, 100% open source, 100% free,&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now the &lt;code&gt;../llama-2-7b-out&lt;/code&gt; can be used in exactly same way as original llama2 for further quantization, inference, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Project structure&lt;/h3&gt; &#xA;&lt;p&gt;Just a few files with no dependencies other than torch, numpy and sentencepiece for tokenizer.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/blackbox_model.py&#34;&gt;blackbox_model.py&lt;/a&gt; -- model definition and manual backprop implementation. It&#39;s based on model.py from &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;, also MIT licenced.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/finetune.py&#34;&gt;finetune.py&lt;/a&gt; - script which does the training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/loader.py&#34;&gt;loader.py&lt;/a&gt; - manual loading/saving of large llama2 models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/utils.py&#34;&gt;utils.py&lt;/a&gt; - small utility functions, including saving/loading random generator state for different devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/test_gen.py&#34;&gt;test_gen.py&lt;/a&gt; - greedily complete the prompt. Takes base weights + trained LoRA weights as input. Useful for sanity checks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/blackbox.py&#34;&gt;blackbox.py&lt;/a&gt; - module wrapper which offloads the module to disk or main memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/plot_lora.py&#34;&gt;plot_lora.py&lt;/a&gt; - logging utility, writes LoRA weights and gradient distribution to &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/lora_weights.md&#34;&gt;logfile&lt;/a&gt;. Requires &lt;a href=&#34;https://github.com/okuvshynov/fewlines&#34;&gt;fewlines&lt;/a&gt;. If fewlines is not installed, does nothing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/merge_lora.py&#34;&gt;merge_lora.py&lt;/a&gt; - merge original weights + lora weights in the original format which can then be used directly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/prepare_model.py&#34;&gt;prepare_model.py&lt;/a&gt; - script to transform sharded model to sequentially split model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;TODO:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;[ ] masking&#xA;[ ] more generic train routine&#xA;    [ ] pause/resume from LoRA snapshot&#xA;    [ ] do not create LoRA layers on prepare, only on finetune?&#xA;[ ] how to make it work with fp16 on Apple?&#xA;[ ] optimizations - prefetch the next layer/input, save asyncronously, etc;&#xA;[ ] gradient accumulation&#xA;[ ] plot something like memory requirement for (batch_size , seq_len)&#xA;[ ] combined RAM/disk offload - 200Gb RAM is rarity.&#xA;[ ] tests, cleanup and comments;&#xA;[ ] progress tracking for everything;&#xA;[ ] quantization beyond 16 bit?&#xA;[ ] configurable weight tying;&#xA;[ ] double check RNG state correctness.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;llama2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/okuvshynov/cubestat&#34;&gt;cubestat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;{github handle} @ gmail.com&lt;/p&gt;</summary>
  </entry>
</feed>