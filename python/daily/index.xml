<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-29T01:37:55Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/mae</title>
    <updated>2023-11-29T01:37:55Z</updated>
    <id>tag:github.com,2023-11-29:/facebookresearch/mae</id>
    <link href="https://github.com/facebookresearch/mae" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of MAE https//arxiv.org/abs/2111.06377&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Masked Autoencoders: A PyTorch Implementation&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png&#34; width=&#34;480&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is a PyTorch/GPU re-implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{MaskedAutoencoders2021,&#xA;  author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll{\&#39;a}r and Ross Girshick},&#xA;  journal = {arXiv:2111.06377},&#xA;  title   = {Masked Autoencoders Are Scalable Vision Learners},&#xA;  year    = {2021},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The original implementation was in TensorFlow+TPU. This re-implementation is in PyTorch+GPU.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This repo is a modification on the &lt;a href=&#34;https://github.com/facebookresearch/deit&#34;&gt;DeiT repo&lt;/a&gt;. Installation and preparation follow that repo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This repo is based on &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;&lt;code&gt;timm==0.3.2&lt;/code&gt;&lt;/a&gt;, for which a &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/issues/420#issuecomment-776459842&#34;&gt;fix&lt;/a&gt; is needed to work with PyTorch 1.8.1+.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Catalog&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-trained checkpoints + fine-tuning code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visualization demo&lt;/h3&gt; &#xA;&lt;p&gt;Run our interactive visualization demo using &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/mae/blob/main/demo/mae_visualize.ipynb&#34;&gt;Colab notebook&lt;/a&gt; (no GPU needed):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11435359/147859292-77341c70-2ed8-4703-b153-f505dcb6f2f8.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning with pre-trained checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;The following table provides the pre-trained checkpoints used in the paper, converted from TF/TPU to PT/GPU:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-Base&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-Large&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-Huge&lt;/th&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;pre-trained checkpoint&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_large.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_huge.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;md5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;tt&gt;8cad7c&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;tt&gt;b8b06e&lt;/tt&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;tt&gt;9bdbb0&lt;/tt&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The fine-tuning instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/mae/main/FINETUNE.md&#34;&gt;FINETUNE.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By fine-tuning these pre-trained models, we rank #1 in these classification tasks (detailed in the paper):&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-B&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-L&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-H&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;ViT-H&lt;sub&gt;448&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;td valign=&#34;bottom&#34; style=&#34;color:#C0C0C0&#34;&gt;prev best&lt;/td&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-1K (no external data)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;87.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;87.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;5&#34;&gt;&lt;font size=&#34;1&#34;&gt;&lt;em&gt;following are evaluation of the same model weights (fine-tuned in original ImageNet-1K):&lt;/em&gt;&lt;/font&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-Corruption (error rate) &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;33.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-Adversarial&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;76.7&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;35.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-Rendition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;66.5&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;48.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-Sketch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;50.9&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;36.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;5&#34;&gt;&lt;font size=&#34;1&#34;&gt;&lt;em&gt;following are transfer learning by fine-tuning the pre-trained MAE on the target dataset:&lt;/em&gt;&lt;/font&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;iNaturalists 2017&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;83.4&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;75.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;iNaturalists 2018&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;86.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;81.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;iNaturalists 2019&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;88.3&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;84.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Places205&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;66.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;66.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Places365&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;60.3&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;color:#C0C0C0&#34;&gt;58.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;p&gt;The pre-training instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/mae/main/PRETRAIN.md&#34;&gt;PRETRAIN.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is under the CC-BY-NC 4.0 license. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/mae/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kagisearch/vectordb</title>
    <updated>2023-11-29T01:37:55Z</updated>
    <id>tag:github.com,2023-11-29:/kagisearch/vectordb</id>
    <link href="https://github.com/kagisearch/vectordb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal Python package for storing and retrieving text using chunking, embeddings, and vector search.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VectorDB&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/aDNg6E9szy&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/aDNg6E9szy?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/KagiHQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/KagiHQ?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/license/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VectorDB is a simple, lightweight, fully local, end-to-end solution for using embeddings-based text retrieval.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to its low latency and small memory footprint, VectorDB is used to power AI features inside &lt;a href=&#34;https://kagi.com&#34;&gt;Kagi Search&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check an &lt;a href=&#34;https://colab.research.google.com/drive/1pecKGCCru_Jvx7v0WRNrW441EBlcS5qS#scrollTo=Eh6o8m7d8eOk&#34;&gt;example Colab notebook&lt;/a&gt; where this is used to filter the content of &lt;a href=&#34;https://kagi.com/smallweb&#34;&gt;Kagi Small Web&lt;/a&gt; RSS feed based on stated user interests.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install VectorDB, use pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install vectordb2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Quick example that loads data into memory, and runs retrieval. All data will be handled locally, including embeddings and vector search, completely trasparent for the user with maximum possible performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vectordb import Memory&#xA;&#xA;# Memory is where all content you want to store/search goes.&#xA;memory = Memory()&#xA;&#xA;memory.save(&#xA;    [&#34;apples are green&#34;, &#34;oranges are orange&#34;],  # save your text content. for long text we will automatically chunk it&#xA;    [{&#34;url&#34;: &#34;https://apples.com&#34;}, {&#34;url&#34;: &#34;https://oranges.com&#34;}], # associate any kind of metadata with it (optional)&#xA;)&#xA;&#xA;# Search for top n relevant results, automatically using embeddings&#xA;query = &#34;green&#34;&#xA;results = memory.search(query, top_n = 1)&#xA;&#xA;print(results)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This returns the chunks with the added metadata and the vector distance (where 0 is the exact match and higher means further apart)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;  {&#xA;    &#34;chunk&#34;: &#34;apples are green&#34;,&#xA;    &#34;metadata&#34;: {&#34;url&#34;: &#34;https://apples.com&#34;},&#xA;    &#34;distance&#34;: 0.87&#xA;  }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Options&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory(memory_file=None, chunking_strategy={&#34;mode&#34;:&#34;sliding_window&#34;}, embeddings=&#34;normal&#34;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;memory_file&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Path to the memory file. If provided, memory will persist to disk and loaded/saved to this file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;chunking_strategy&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Dictionary containing the chunking mode.&lt;/p&gt; &lt;p&gt;Options:&lt;br&gt; &lt;code&gt;{&#39;mode&#39;:&#39;sliding_window&#39;, &#39;window_size&#39;: 240, &#39;overlap&#39;: 8}&lt;/code&gt; (default)&lt;br&gt; &lt;code&gt;{&#39;mode&#39;:&#39;paragraph&#39;}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;embeddings&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Options:&lt;br&gt; &lt;code&gt;fast&lt;/code&gt; - Uses Universal Sentence Encoder 4&lt;br&gt; &lt;code&gt;normal&lt;/code&gt; - Uses &#34;BAAI/bge-small-en-v1.5&#34; (default)&lt;br&gt; &lt;code&gt;best&lt;/code&gt; - Uses &#34;BAAI/bge-base-en-v1.5&#34;&lt;br&gt; &lt;code&gt;multilingual&lt;/code&gt; - Uses Universal Sentence Encoder Multilingual Large 3&lt;/p&gt; &lt;p&gt;You can also specify a custom HuggingFace model by name eg. &lt;code&gt;TaylorAI/bge-micro-v2&lt;/code&gt;. See also &lt;a href=&#34;https://www.sbert.net/docs/pretrained_models.html&#34;&gt;Pretrained models&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory.save(texts, metadata, memory_file=None)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Save content to memory. Metadata will be automatically optimized to use less resources.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;texts&lt;/code&gt;: &lt;em&gt;Required.&lt;/em&gt; Text or list of texts to be saved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;metdata&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Metadata or list of metadata associated with the texts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;memory_file&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Path to persist the memory file. By default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory.search(query, top_n=5, unique=False, batch_results=&#34;flatten&#34;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Search inside memory.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;query&lt;/code&gt;: &lt;em&gt;Required.&lt;/em&gt; Query text or list of queries (see &lt;code&gt;batch_results&lt;/code&gt; option below for handling results for a list).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_n&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Number of most similar chunks to return (default: 5).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;unique&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; Return only items chunks from unique original texts (additional chunks coming from the same text will be ignored). Note this may return less chhunks than requested (default: False).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;batch_results&lt;/code&gt;: &lt;em&gt;Optional.&lt;/em&gt; When input is a list of queries, output algorithm can be &#34;flatten&#34; or &#34;diverse&#34;. Flatten returns true nearest neighbours across all input queries, meaning all results could come from just one query. &#34;diverse&#34; attempts to spread out the results, so that each query&#39;s nearest neighbours are equally added (neareast first across all queries, than 2nd nearest and so on). (default: &#34;flatten&#34;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory.clear()&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clears the memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory.dump()&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prints the contents of the memory.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vectordb import Memory&#xA;&#xA;memory = Memory(&#xA;    chunking_strategy={&#34;mode&#34;: &#34;sliding_window&#34;, &#34;window_size&#34;: 128, &#34;overlap&#34;: 16}, embeddings=&#39;TaylorAI/bge-micro-v2&#39;&#xA;)&#xA;&#xA;texts = [&#xA;    &#34;&#34;&#34;&#xA;Machine learning is a method of data analysis that automates analytical model building.&#xA;&#xA;It is a branch of artificial intelligence based on the idea that systems can learn from data,&#xA;identify patterns and make decisions with minimal human intervention.&#xA;&#xA;Machine learning algorithms are trained on data sets that contain examples of the desired output. For example, a machine learning algorithm that is used to classify images might be trained on a data set that contains images of cats and dogs.&#xA;Once an algorithm is trained, it can be used to make predictions on new data. For example, the machine learning algorithm that is used to classify images could be used to predict whether a new image contains a cat or a dog.&#xA;&#xA;Machine learning algorithms can be used to solve a wide variety of problems. Some common applications of machine learning include:&#xA;&#xA;Classification: Categorizing data into different groups. For example, a machine learning algorithm could be used to classify emails as spam or not spam.&#xA;&#xA;Regression: Predicting a continuous value. For example, a machine learning algorithm could be used to predict the price of a house.&#xA;&#xA;Clustering: Finding groups of similar data points. For example, a machine learning algorithm could be used to find groups of customers with similar buying habits.&#xA;&#xA;Anomaly detection: Finding data points that are different from the rest of the data. For example, a machine learning algorithm could be used to find fraudulent credit card transactions.&#xA;&#xA;Machine learning is a powerful tool that can be used to solve a wide variety of problems. As the amount of data available continues to grow, machine learning is likely to become even more important in the future.&#xA;&#34;&#34;&#34;,&#xA;    &#34;&#34;&#34;&#xA;Artificial intelligence (AI) is the simulation of human intelligence in machines&#xA;that are programmed to think like humans and mimic their actions.&#xA;&#xA;The term may also be applied to any machine that exhibits traits associated with&#xA;a human mind such as learning and problem-solving.&#xA;&#xA;AI research has been highly successful in developing effective techniques for solving a wide range of problems, from game playing to medical diagnosis.&#xA;&#xA;However, there is still a long way to go before AI can truly match the intelligence of humans. One of the main challenges is that human intelligence is incredibly complex and poorly understood.&#xA;&#xA;Despite the challenges, AI is a rapidly growing field with the potential to revolutionize many aspects of our lives. Some of the potential benefits of AI include:&#xA;&#xA;Increased productivity: AI can be used to automate tasks that are currently performed by humans, freeing up our time for more creative and fulfilling activities.&#xA;&#xA;Improved decision-making: AI can be used to make more informed decisions, based on a wider range of data than humans can typically access.&#xA;&#xA;Enhanced creativity: AI can be used to generate new ideas and solutions, beyond what humans can imagine on their own.&#xA;Of course, there are also potential risks associated with AI, such as:&#xA;&#xA;Job displacement: As AI becomes more capable, it is possible that it will displace some human workers.&#xA;&#xA;Weaponization: AI could be used to develop new weapons that are more powerful and destructive than anything we have today.&#xA;&#xA;Loss of control: If AI becomes too powerful, we may lose control over it, with potentially disastrous consequences.&#xA;&#xA;It is important to weigh the potential benefits and risks of AI carefully as we continue to develop this technology. With careful planning and oversight, AI has the potential to make the world a better place. However, if we are not careful, it could also lead to serious problems.&#xA;&#34;&#34;&#34;,&#xA;]&#xA;&#xA;metadata_list = [&#xA;    {&#xA;        &#34;title&#34;: &#34;Introduction to Machine Learning&#34;,&#xA;        &#34;url&#34;: &#34;https://example.com/introduction-to-machine-learning&#34;,&#xA;    },&#xA;    {&#xA;        &#34;title&#34;: &#34;Introduction to Artificial Intelligence&#34;,&#xA;        &#34;url&#34;: &#34;https://example.com/introduction-to-artificial-intelligence&#34;,&#xA;    },&#xA;]&#xA;&#xA;memory.save(texts, metadata_list)&#xA;&#xA;query = &#34;What is the relationship between AI and machine learning?&#34;&#xA;results = memory.search(query, top_n=3, unique=True)&#xA;print(results)&#xA;&#xA;# two results will be returned as unique param is set to True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;  {&#xA;    &#34;chunk&#34;: &#34;Artificial intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. AI research has been highly successful in developing effective techniques for solving a wide range of problems, from game playing to medical diagnosis. However, there is still a long way to go before AI can truly match the intelligence of humans. One of the main challenges is that human intelligence is incredibly complex and poorly understood. Despite the challenges, AI is a rapidly growing field with the potential to revolutionize many aspects of our lives. Some of the potential benefits of AI include: Increased&#34;,&#xA;    &#34;metadata&#34;: {&#xA;      &#34;title&#34;: &#34;Introduction to Artificial Intelligence&#34;,&#xA;      &#34;url&#34;: &#34;https://example.com/introduction-to-artificial-intelligence&#34;&#xA;    },&#xA;    &#34;distance&#34;: 0.87&#xA;  },&#xA;  {&#xA;    &#34;chunk&#34;: &#34;Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Machine learning algorithms are trained on data sets that contain examples of the desired output. For example, a machine learning algorithm that is used to classify images might be trained on a data set that contains images of cats and dogs. Once an algorithm is trained, it can be used to make predictions on new data. For example, the machine learning algorithm that is used to classify images could be used to predict whether a new image contains a cat or a dog. Machine learning algorithms can be used&#34;,&#xA;    &#34;metadata&#34;: {&#xA;      &#34;title&#34;: &#34;Introduction to Machine Learning&#34;,&#xA;      &#34;url&#34;: &#34;https://example.com/introduction-to-machine-learning&#34;&#xA;    },&#xA;    &#34;distance&#34;: 0.83&#xA;  }&#xA;]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Embeddings performance analysis&lt;/h2&gt; &#xA;&lt;p&gt;We constantly evaluate embedding models using standardized benchmarks (higher is better). Average latency is measured locally on CPU (lower is better). Benchmark data pulled from &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Latency&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark 1&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark 2&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark 3&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark 4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;all-mpnet-base-v2&lt;/td&gt; &#xA;   &lt;td&gt;6.12 s&lt;/td&gt; &#xA;   &lt;td&gt;80.28&lt;/td&gt; &#xA;   &lt;td&gt;65.07&lt;/td&gt; &#xA;   &lt;td&gt;43.69&lt;/td&gt; &#xA;   &lt;td&gt;83.04&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;all-MiniLM-L6-v2&lt;/td&gt; &#xA;   &lt;td&gt;1.14 s&lt;/td&gt; &#xA;   &lt;td&gt;78.9&lt;/td&gt; &#xA;   &lt;td&gt;63.05&lt;/td&gt; &#xA;   &lt;td&gt;42.35&lt;/td&gt; &#xA;   &lt;td&gt;82.37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BAAI/bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;20.8 s&lt;/td&gt; &#xA;   &lt;td&gt;83.11&lt;/td&gt; &#xA;   &lt;td&gt;75.97&lt;/td&gt; &#xA;   &lt;td&gt;46.08&lt;/td&gt; &#xA;   &lt;td&gt;87.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BAAI/bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;6.48 s&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;75.53&lt;/td&gt; &#xA;   &lt;td&gt;45.77&lt;/td&gt; &#xA;   &lt;td&gt;86.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BAAI/bge-small-en-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;1.85 s&lt;/td&gt; &#xA;   &lt;td&gt;81.59&lt;/td&gt; &#xA;   &lt;td&gt;74.14&lt;/td&gt; &#xA;   &lt;td&gt;43.82&lt;/td&gt; &#xA;   &lt;td&gt;84.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TaylorAI/bge-micro-v2&lt;/td&gt; &#xA;   &lt;td&gt;0.671 s&lt;/td&gt; &#xA;   &lt;td&gt;78.65&lt;/td&gt; &#xA;   &lt;td&gt;68.04&lt;/td&gt; &#xA;   &lt;td&gt;39.18&lt;/td&gt; &#xA;   &lt;td&gt;82.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TaylorAI/gte-tiny&lt;/td&gt; &#xA;   &lt;td&gt;1.25 s&lt;/td&gt; &#xA;   &lt;td&gt;80.46&lt;/td&gt; &#xA;   &lt;td&gt;70.35&lt;/td&gt; &#xA;   &lt;td&gt;42.09&lt;/td&gt; &#xA;   &lt;td&gt;82.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;thenlper/gte-base&lt;/td&gt; &#xA;   &lt;td&gt;6.28 s&lt;/td&gt; &#xA;   &lt;td&gt;82.3&lt;/td&gt; &#xA;   &lt;td&gt;73.01&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;84.57&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;thenlper/gte-small&lt;/td&gt; &#xA;   &lt;td&gt;2.14 s&lt;/td&gt; &#xA;   &lt;td&gt;82.07&lt;/td&gt; &#xA;   &lt;td&gt;72.31&lt;/td&gt; &#xA;   &lt;td&gt;44.89&lt;/td&gt; &#xA;   &lt;td&gt;83.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;universal-sentence-encoder-large/5&lt;/td&gt; &#xA;   &lt;td&gt;0.769 s&lt;/td&gt; &#xA;   &lt;td&gt;74.05&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;37.82&lt;/td&gt; &#xA;   &lt;td&gt;79.53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;universal-sentence-encoder-multilingual-large/3&lt;/td&gt; &#xA;   &lt;td&gt;1.02 s&lt;/td&gt; &#xA;   &lt;td&gt;75.35&lt;/td&gt; &#xA;   &lt;td&gt;65.78&lt;/td&gt; &#xA;   &lt;td&gt;35.06&lt;/td&gt; &#xA;   &lt;td&gt;79.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;universal-sentence-encoder-multilingual/3&lt;/td&gt; &#xA;   &lt;td&gt;0.162 s&lt;/td&gt; &#xA;   &lt;td&gt;75.39&lt;/td&gt; &#xA;   &lt;td&gt;63.42&lt;/td&gt; &#xA;   &lt;td&gt;34.82&lt;/td&gt; &#xA;   &lt;td&gt;75.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;universal-sentence-encoder/4&lt;/td&gt; &#xA;   &lt;td&gt;0.019 s&lt;/td&gt; &#xA;   &lt;td&gt;72.04&lt;/td&gt; &#xA;   &lt;td&gt;64.45&lt;/td&gt; &#xA;   &lt;td&gt;35.71&lt;/td&gt; &#xA;   &lt;td&gt;76.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Relative embeddings latency on CPU&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kagisearch/vectordb/main/images/speed_cpu.png&#34; alt=&#34;Embeddings Latency on CPU&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Relative embeddings latency on GPU&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kagisearch/vectordb/main/images/speed_gpu.png&#34; alt=&#34;Embeddings Latency on GPU&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kagisearch/vectordb/main/images/quality.png&#34; alt=&#34;Embeddings Quality&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kagisearch/vectordb/main/images/scatter.png&#34; alt=&#34;Scatter of Embeddings&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Vector search performance analysis&lt;/h2&gt; &#xA;&lt;p&gt;VectorDB is also optimized for speed of retrieval. We automatically uses &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt; for low number of chunks (&amp;lt;4000) and &lt;a href=&#34;https://github.com/vioshyvo/mrpt&#34;&gt;mrpt&lt;/a&gt; for high number of chunks to ensure maximum performance across the spectrum of use cases.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kagisearch/vectordb/main/images/comparison.png&#34; alt=&#34;Vector search engine comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yule-BUAA/MergeLM</title>
    <updated>2023-11-29T01:37:55Z</updated>
    <id>tag:github.com,2023-11-29:/yule-BUAA/MergeLM</id>
    <link href="https://github.com/yule-BUAA/MergeLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Codebase for Merging Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/yule-BUAA/MergeLM/main/figures/icon.jpeg&#34; width=&#34;25%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repository is built for the paper &lt;a href=&#34;https://arxiv.org/abs/2311.03099&#34;&gt;Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch&lt;/a&gt;. 🔔 If you have any questions or suggestions, please feel free to let us know. You can directly email &lt;a href=&#34;https://yule-buaa.github.io/&#34;&gt;Le Yu&lt;/a&gt; using the email address &lt;a href=&#34;mailto:yule@buaa.edu.cn&#34;&gt;yule@buaa.edu.cn&lt;/a&gt; or post an issue on this repository.&lt;/p&gt; &#xA;&lt;h2&gt;💥 News 💥&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 27, 2023&lt;/strong&gt;] Special thanks to &lt;a href=&#34;https://huggingface.co/brucethemoose&#34;&gt;brucethemoose&lt;/a&gt; for applying our work on several models on Hugging Face (&lt;a href=&#34;https://huggingface.co/brucethemoose/Capybara-Tess12-Yi-34B-200K-DARE&#34;&gt;model_1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/brucethemoose/Capybara-Tess-Yi-34B-200K-DARE-Ties&#34;&gt;model_2&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/brucethemoose/Capybara-Tess-Yi-34B-200K-DARE-Ties-4bpw-exl2-fiction&#34;&gt;model_3&lt;/a&gt;)!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 24, 2023&lt;/strong&gt;] We appreciate &lt;a href=&#34;https://github.com/uukuguy&#34;&gt;uukuguy&lt;/a&gt; for integrating our work into the &lt;a href=&#34;https://pypi.org/project/multi-loras/0.2.0&#34;&gt;Multi-LoRAs Project&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 23, 2023&lt;/strong&gt;] Special thanks to &lt;a href=&#34;https://twitter.com/WizardLM_AI&#34;&gt;WizardLM&lt;/a&gt; for sharing our work on &lt;a href=&#34;https://twitter.com/WizardLM_AI/status/1727672799391842468&#34;&gt;Twitter&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 22, 2023&lt;/strong&gt;] We appreciate &lt;a href=&#34;http://www.paperweekly.info&#34;&gt;PaperWeekly&lt;/a&gt; for sharing our work on &lt;a href=&#34;https://zhuanlan.zhihu.com/p/668152236&#34;&gt;Zhihu&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 21, 2023&lt;/strong&gt;] We appreciate &lt;a href=&#34;http://www.paperweekly.info&#34;&gt;PaperWeekly&lt;/a&gt; for sharing our work on &lt;a href=&#34;https://mp.weixin.qq.com/s/YiqWovBUXIbzmUbL6uT-8g&#34;&gt;WeChat&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 11, 2023&lt;/strong&gt;] Special thanks to &lt;a href=&#34;https://xixiaoyao.github.io/about/&#34;&gt;夕小瑶&lt;/a&gt; for sharing our work on &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ%3D%3D&amp;amp;mid=2247565881&amp;amp;idx=2&amp;amp;sn=57985427fdb6751d617df801ca7fd810&#34;&gt;WeChat&lt;/a&gt; and &lt;a href=&#34;https://zhuanlan.zhihu.com/p/666363702&#34;&gt;Zhihu&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 6, 2023&lt;/strong&gt;] Our paper is available on &lt;a href=&#34;https://huggingface.co/papers/2311.03099&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 6, 2023&lt;/strong&gt;] Our paper is available on &lt;a href=&#34;https://paperswithcode.com/paper/language-models-are-super-mario-absorbing&#34;&gt;Papers With Code&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🔥🔥🔥[&lt;strong&gt;November 6, 2023&lt;/strong&gt;] Our paper is available on &lt;a href=&#34;https://arxiv.org/abs/2311.03099&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;In this work, we uncover that Language Models (LMs), either encoder- or decoder-based, can &lt;strong&gt;obtain new capabilities by assimilating the parameters of homologous models without the need for retraining or GPUs&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We introduce a novel operation called &lt;strong&gt;DARE&lt;/strong&gt; to directly set most of (90% or even 99%) the delta parameters to zeros without affecting the capabilities of SFT LMs.&lt;/li&gt; &#xA; &lt;li&gt;We sparsify delta parameters of multiple SFT homologous models with DARE as a &lt;strong&gt;general preprocessing technique&lt;/strong&gt; and subsequently merge them into a single model by parameter averaging.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The workflow is shown as follows,&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/yule-BUAA/MergeLM/main/figures/framework.jpg&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;By conducting extensive experiments, we find that:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;DARE is effective for SFT models whose delta parameter value ranges are relatively small (e.g., within 0.005), being able to eliminate even 99% delta parameters. Larger models can tolerate a higher proportion of discarded parameters, indicating that SFT naturally learns an extremely sparse set of delta parameters, and nearly all abilities originate from the pre-trained LMs. See (a) in the figure below.&lt;/li&gt; &#xA; &lt;li&gt;DARE can merge multiple task-specific LMs into one LM with diverse abilities, which is able to possess the functionalities of all SFT models. For instance, the merger of WizardLM and WizardMath increases the GSM8K accuracy of WizardLM from 2.2 to 66.3, maintaining its instruction-following capabilities while surpassing WizardMath&#39;s original 64.2 performance. See (b) in the figure below.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/yule-BUAA/MergeLM/main/figures/introduction_llms_merge.jpg&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Language Models and Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We conduct experiments on both encoder- and decoder-based LMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For encoder-based LMs, we choose bert-base-uncased and roberta-base as pre-trained backbones. Eight datasets from the GLUE benchmark are used, including CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, and RTE.&lt;/li&gt; &#xA; &lt;li&gt;For decoder-based LMs, we choose LLaMA, Llama 2, and Code Llama as pre-trained backbones. WizardLM, WizardMath, WizardCoder-Python, and Code Alpaca are used as fine-tuned models. We evaluate three tasks on five datasets: AlpacaEval (instruction-following), GSM8K and MATH (mathematical reasoning), and HumanEval and MBPP (code-generating).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that we provide GSM8K, MATH, and MBPP datasets in &lt;code&gt;math_code_data/&lt;/code&gt; folder, which are obtained from &lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM repository&lt;/a&gt;. Other datasets can be automatically downloaded by our codes. For language models, you can download them either manually or by our codes.&lt;/p&gt; &#xA;&lt;p&gt;You can also modify the &lt;code&gt;cache_dir&lt;/code&gt; in the &lt;code&gt;utils/load_config.py&lt;/code&gt; file to specify your own path to save datasets and models.&lt;/p&gt; &#xA;&lt;h2&gt;Model Merging Methods&lt;/h2&gt; &#xA;&lt;p&gt;We provide a well-coded implementation of five model merging methods in this repository, including &lt;a href=&#34;https://arxiv.org/abs/2203.05482&#34;&gt;Average Merging&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.04089&#34;&gt;Task Arithmetic&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2111.09832&#34;&gt;Fisher Merging&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.09849&#34;&gt;RegMean&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.01708&#34;&gt;TIES-Merging&lt;/a&gt;. We also combine the proposed &lt;a href=&#34;https://arxiv.org/abs/2311.03099&#34;&gt;DARE&lt;/a&gt; with the above methods to facilitate the merging performance.&lt;/p&gt; &#xA;&lt;h2&gt;Environments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch 2.0.1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;transformers 4.33.1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/datasets/index&#34;&gt;datasets 2.13.1&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm 0.1.4&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;human_eval&lt;/a&gt;, &lt;a href=&#34;https://github.com/numpy/numpy&#34;&gt;numpy&lt;/a&gt;, and &lt;a href=&#34;https://github.com/tqdm/tqdm&#34;&gt;tqdm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Executing Scripts for Encoder-based LMs&lt;/h2&gt; &#xA;&lt;p&gt;For encoder-based LMs, we first fine-tune them on the GLUE benchmark (support both single-task and multi-task settings), and then inference with them. We also provide scripts to merge encoder-based LMs with five model merging methods.&lt;/p&gt; &#xA;&lt;h3&gt;Scripts for Fine-Tuning on GLUE&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of fine-tuning &lt;em&gt;roberta-base&lt;/em&gt; on &lt;em&gt;CoLA&lt;/em&gt; dataset under single-task setting:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python train_plms_glue.py --language_model_name roberta-base --dataset_name cola --learning_rate 1e-5 --num_runs 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of fine-tuning &lt;em&gt;roberta-base&lt;/em&gt; on &lt;em&gt;CoLA&lt;/em&gt; and &lt;em&gt;RTE&lt;/em&gt; datasets under multi-task setting:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python train_plms_glue.py --language_model_name roberta-base --dataset_name cola --multitask_training --auxiliary_dataset_name rte --learning_rate 1e-5 --num_runs 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scripts for Inference with DARE and Other Variants&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of direct inference on &lt;em&gt;roberta-base&lt;/em&gt; (drop rate 0.0):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_plms_glue.py --language_model_name roberta-base --weight_mask_rate 0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;roberta-base&lt;/em&gt; with DARE (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_plms_glue.py --language_model_name roberta-base --weight_mask_rate 0.9 --use_weight_rescale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;roberta-base&lt;/em&gt; with DropOnly (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_plms_glue.py --language_model_name roberta-base --weight_mask_rate 0.9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;roberta-base&lt;/em&gt; with magnitude-based pruning (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_plms_glue.py --language_model_name roberta-base --weight_mask_rate 0.9 --mask_strategy magnitude&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;roberta-base&lt;/em&gt; with masking fine-tuned parameters (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_plms_glue.py --language_model_name roberta-base --weight_mask_rate 0.9 --use_weight_rescale --weight_format finetuned_weight&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scripts for Merging Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging pairwise fine-tuned &lt;em&gt;roberta-base&lt;/em&gt; with Average Merging:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_plms_glue.py --merging_method_name average_merging --language_model_name roberta-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging pairwise fine-tuned &lt;em&gt;roberta-base&lt;/em&gt; with Fisher Merging:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_plms_glue.py --merging_method_name fisher_merging --normalize_fisher_weight --language_model_name roberta-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging pairwise fine-tuned &lt;em&gt;roberta-base&lt;/em&gt; with Average Merging and DARE:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_plms_glue.py --merging_method_name mask_merging --use_weight_rescale --language_model_name roberta-base --mask_apply_method average_merging&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Executing Scripts for Decoder-based LMs&lt;/h2&gt; &#xA;&lt;p&gt;Since the decoder-based LMs we use have already been fine-tuned, they can be directly utilized for inference. We also provide scripts to merge decoder-based LMs with two model merging methods (Average Merging and Task Arithmetic).&lt;/p&gt; &#xA;&lt;h3&gt;Scripts for Inference with DARE and Other Variants&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of direct inference on &lt;em&gt;WizardMath-7B-V1.0&lt;/em&gt; on &lt;em&gt;GSM8K&lt;/em&gt; (drop rate 0.0):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_llms_instruct_math_code.py --dataset_name gsm8k --finetuned_model_name WizardMath-7B-V1.0 --tensor_parallel_size 1 --weight_mask_rate 0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;WizardMath-7B-V1.0&lt;/em&gt; on &lt;em&gt;GSM8K&lt;/em&gt; with DARE (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_llms_instruct_math_code.py --dataset_name gsm8k --finetuned_model_name WizardMath-7B-V1.0 --tensor_parallel_size 1 --weight_mask_rate 0.9 --use_weight_rescale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;WizardMath-7B-V1.0&lt;/em&gt; on &lt;em&gt;GSM8K&lt;/em&gt; with DropOnly (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_llms_instruct_math_code.py --dataset_name gsm8k --finetuned_model_name WizardMath-7B-V1.0 --tensor_parallel_size 1 --weight_mask_rate 0.9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;WizardMath-7B-V1.0&lt;/em&gt; on &lt;em&gt;GSM8K&lt;/em&gt; with magnitude-based pruning (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_llms_instruct_math_code.py --dataset_name gsm8k --finetuned_model_name WizardMath-7B-V1.0 --tensor_parallel_size 1 --weight_mask_rate 0.9 --mask_strategy magnitude&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of inference on &lt;em&gt;WizardMath-7B-V1.0&lt;/em&gt; on &lt;em&gt;GSM8K&lt;/em&gt; with masking fine-tuned parameters (drop rate 0.9):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python inference_llms_instruct_math_code.py --dataset_name gsm8k --finetuned_model_name WizardMath-7B-V1.0 --tensor_parallel_size 1 --weight_mask_rate 0.9 --use_weight_rescale --weight_format finetuned_weight&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scripts for Merging Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging &lt;em&gt;WizardLM-13B-V1.2&lt;/em&gt; and &lt;em&gt;WizardMath-13B-V1.0&lt;/em&gt; with Average Merging:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_llms_instruct_math_code.py --merge_instruct --merge_math --merging_method_name average_merging --tensor_parallel_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging &lt;em&gt;WizardLM-13B-V1.2&lt;/em&gt; and &lt;em&gt;WizardMath-13B-V1.0&lt;/em&gt; with Task Arithmetic:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_llms_instruct_math_code.py --merge_instruct --merge_math --merging_method_name task_arithmetic --scaling_coefficient 1.0 --tensor_parallel_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of merging &lt;em&gt;WizardLM-13B-V1.2&lt;/em&gt; and &lt;em&gt;WizardMath-13B-V1.0&lt;/em&gt; with Average Merging and DARE (drop rate 0.2):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python merge_llms_instruct_math_code.py --merge_instruct --merge_math --merging_method_name mask_merging --use_weight_rescale --weight_mask_rate 0.2 --mask_apply_method average_merging --tensor_parallel_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;❗&lt;strong&gt;Note 1&lt;/strong&gt;: When merging decoder-based LMs, the number of GPUs we should allocate is equals to num_models_to_merge * tensor_parallel_size. For example, if we want to merge &lt;em&gt;WizardLM-13B-V1.2&lt;/em&gt; and &lt;em&gt;WizardMath-13B-V1.0&lt;/em&gt; with tensor_parallel_size == 1, then we should allocate 2 * 1 = 2 GPUs.&lt;/p&gt; &#xA;&lt;p&gt;❗&lt;strong&gt;Note 2&lt;/strong&gt;: If &#34;AssertionError: data parallel group is already initialized&#34; error is raised by vllm on your device, please try to run &lt;code&gt;direct_inference_merged_llms_instruct_math_code.py&lt;/code&gt; with the corresponding setting. For example, if this error occurs when merging &lt;em&gt;WizardLM-13B-V1.2&lt;/em&gt; and &lt;em&gt;WizardMath-13B-V1.0&lt;/em&gt; with Average Merging and DARE (drop rate 0.2), please run the following command to evaluate on instruct- or math-related task&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;python direct_inference_merged_llms_instruct_math_code.py --merge_instruct --merge_math --merging_method_name mask_merging --use_weight_rescale --weight_mask_rate 0.2 --mask_apply_method average_merging --tensor_parallel_size 1 --evaluate_task instruct&#xA;python direct_inference_merged_llms_instruct_math_code.py --merge_instruct --merge_math --merging_method_name mask_merging --use_weight_rescale --weight_mask_rate 0.2 --mask_apply_method average_merging --tensor_parallel_size 1 --evaluate_task math&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation Process for AlpacaEval, HumanEval and MBPP&lt;/h3&gt; &#xA;&lt;p&gt;For AlpacaEval, HumanEval and MBPP, our codes will store the generated files and please additionally run the following evaluation commands to get the final metrics.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For AlpacaEval: We use &lt;code&gt;chatgpt_fn&lt;/code&gt; in &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval&#34;&gt;alpaca_eval repository&lt;/a&gt; to compute the win rate. Firstly, please see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval&#34;&gt;alpaca_eval repository&lt;/a&gt; to install the environment. Then, if you want to evaluate the generated &lt;em&gt;WizardLM-13B-V1.2_inference_mask_0.2_rescale_True.json&lt;/em&gt; file, please run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;alpaca_eval --model_outputs ./save_gen_instruct_responses_results/alpaca_eval/WizardLM-13B-V1.2_inference_mask_0.2_rescale_True.json --annotators_config chatgpt_fn --name WizardLM-13B-V1.2_inference_mask_0.2_rescale_True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For HumanEval: Firstly, please see &lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;human-eval repository&lt;/a&gt; to install the environment. Then, if you want to evaluate the generated &lt;em&gt;WizardCoder-Python-13B-V1.0_inference_mask_0.2_rescale_True.jsonl&lt;/em&gt; file, please run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;evaluate_functional_correctness ./save_gen_codes_results/human_eval/WizardCoder-Python-13B-V1.0_inference_mask_0.2_rescale_True.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For MBPP: Firstly, please see &lt;a href=&#34;https://github.com/bigcode-project/bigcode-evaluation-harness&#34;&gt;bigcode-evaluation-harness repository&lt;/a&gt; to install the environment. Then, if you want to evaluate the generated &lt;em&gt;WizardCoder-Python-13B-V1.0_inference_mask_0.2_rescale_True.jsonl&lt;/em&gt; file, please run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34;&gt;accelerate launch ./bigcode-evaluation-harness/main.py --tasks mbpp --allow_code_execution --load_generations_path ./save_gen_codes_results/mbpp/WizardCoder-Python-13B-V1.0_inference_mask_0.2_rescale_True.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We are grateful to the authors of &lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt; for making their project codes publicly available.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing our paper when using this project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{bibtex}&#34;&gt;@article{yu2023language,&#xA;  title={Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch},&#xA;  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},&#xA;  journal={arXiv preprint arXiv:2311.03099},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#yule-BUAA/MergeLM&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=yule-BUAA/MergeLM&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>