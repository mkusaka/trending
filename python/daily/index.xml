<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-11T01:42:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenBuddy/OpenBuddy</title>
    <updated>2023-06-11T01:42:44Z</updated>
    <id>tag:github.com,2023-06-11:/OpenBuddy/OpenBuddy</id>
    <link href="https://github.com/OpenBuddy/OpenBuddy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Multilingual Chatbot for Everyone&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenBuddy - Open Multilingual Chatbot for Everyone&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/media/logo.png&#34; width=&#34;300px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/README.zh.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/README.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;微信公众号、微信群：搜索“&lt;strong&gt;开源智友&lt;/strong&gt;”，关注公众号并发送“加群”&lt;/p&gt; &#xA;&lt;p&gt;Website: &lt;a href=&#34;https://openbuddy.ai&#34;&gt;https://openbuddy.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/OpenBuddy/OpenBuddy&#34;&gt;https://github.com/OpenBuddy/OpenBuddy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Huggingface: &lt;a href=&#34;https://huggingface.co/OpenBuddy&#34;&gt;https://huggingface.co/OpenBuddy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/media/demo.png&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenBuddy is a powerful open multilingual chatbot model aimed at global users, emphasizing conversational AI and seamless multilingual support for English, Chinese, and other languages.&lt;/p&gt; &#xA;&lt;p&gt;Built upon Tii&#39;s Falcon model and Facebook&#39;s LLaMA model, OpenBuddy is fine-tuned to include an extended vocabulary, additional common characters, and enhanced token embeddings. By leveraging these improvements and multi-turn dialogue datasets, OpenBuddy offers a robust model capable of answering questions and performing translation tasks across various languages.&lt;/p&gt; &#xA;&lt;p&gt;Our mission with OpenBuddy is to provide a free, open, and offline-capable AI model that operates on users&#39; devices, irrespective of their language or cultural background. We strive to empower individuals worldwide to access and benefit from AI technology.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Currently, the OpenBuddy-13B demo is available on our Discord server. Please join our Discord server to try it out!&lt;/p&gt; &#xA;&lt;p&gt;Discord: &lt;a href=&#34;https://discord.gg/6fU2s9cGjA&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1100710961549168640?color=blueviolet&amp;amp;label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt; conversational AI, Chinese, English, Japanese, Korean, French, Germany and more!&lt;/li&gt; &#xA; &lt;li&gt;Enhanced vocabulary and support for common CJK characters&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned with multi-turn dialogue datasets for improved performance&lt;/li&gt; &#xA; &lt;li&gt;Two model versions: 7B and 13B&lt;/li&gt; &#xA; &lt;li&gt;4-bit quantization for CPU deployment via llama.cpp (with slightly reduced output quality)&lt;/li&gt; &#xA; &lt;li&gt;Active development plans for future features and improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Versions&lt;/h2&gt; &#xA;&lt;p&gt;OpenBuddy currently offers two model versions: 7B and 13B.&lt;/p&gt; &#xA;&lt;p&gt;More information about downloading the models can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/models.md&#34;&gt;Models&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Future Plans&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enhancing multilingual performance&lt;/li&gt; &#xA; &lt;li&gt;Optimizing model quality post-quantization&lt;/li&gt; &#xA; &lt;li&gt;Developing a mechanism to assess content quality, safety, and inference capabilities&lt;/li&gt; &#xA; &lt;li&gt;Investigating Reinforcement Learning with Human Feedback (RLHF)&lt;/li&gt; &#xA; &lt;li&gt;Exploring the addition of multimodal capabilities for dialogues with image context&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;For &lt;code&gt;OpenBuddy-LLaMA&lt;/code&gt; series models, due to LLaMA licensing restrictions, you need the original LLaMA-7B model to utilize this model. To decrypt the model weights:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Acquire the original LLaMA-7B model (not the Huggingface version).&lt;/li&gt; &#xA; &lt;li&gt;Clone this GitHub repository.&lt;/li&gt; &#xA; &lt;li&gt;Ensure that you have Python 3.7 or higher and numpy installed, you can install numpy with &lt;code&gt;pip install numpy&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the following command, try python3 if python does not work:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python decrypt.py [path-to-consolidated.00.pth] [path-to-our-model-folder]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;OpenBuddy-Falcon&lt;/code&gt; series models, you can directly download the model from Huggingface and enjoy it!&lt;/p&gt; &#xA;&lt;h2&gt;Usage with llama.cpp on CPU/GPU (Recommended)&lt;/h2&gt; &#xA;&lt;p&gt;The 7B model has been converted to ggml format, making it compatible with llama.cpp. llama.cpp is a pure C++ inference engine for LLaMA models, originally designed for CPU deployment.&lt;/p&gt; &#xA;&lt;p&gt;After recent updates, llama.cpp now supports cuBLAS and OpenCL acceleration, which means you can utilize your AMD/NVIDIA GPU to accelerate inference.&lt;/p&gt; &#xA;&lt;p&gt;The model is available at: &lt;a href=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/models.md&#34;&gt;Models&lt;/a&gt;, &lt;code&gt;(5-bit, CPU/GPU, llama.cpp)&lt;/code&gt; is the variant you should download.&lt;/p&gt; &#xA;&lt;p&gt;After installing the model and &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, you can run the &lt;code&gt;chat-llamacpp.bat&lt;/code&gt; or &lt;code&gt;chat-llamacpp.sh&lt;/code&gt; script to interact with OpenBuddy through the interactive console.&lt;/p&gt; &#xA;&lt;p&gt;For now, only OpenBuddy-LLaMA series models are supported by llama.cpp, the developers of llama.cpp are working on adding support for Falcon models.&lt;/p&gt; &#xA;&lt;h2&gt;Usage with Transformers on a high-end GPU&lt;/h2&gt; &#xA;&lt;p&gt;To use OpenBuddy with huggingface&#39;s Transformers library on a GPU, follow the &lt;a href=&#34;https://raw.githubusercontent.com/OpenBuddy/OpenBuddy/main/examples/hello.py&#34;&gt;hello.py&lt;/a&gt; example. For a more comprehensive understanding of text generation, please refer to the &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;Transformers documentation&lt;/a&gt;. A 7B model may require up to 24GB of GPU memory.&lt;/p&gt; &#xA;&lt;h2&gt;Usage with Inference Frameworks&lt;/h2&gt; &#xA;&lt;p&gt;LLM inference frameworks including &lt;a href=&#34;https://github.com/vtuber-plan/langport&#34;&gt;Langport&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, have been adapted to support OpenBuddy. Please refer to the respective repositories for more information.&lt;/p&gt; &#xA;&lt;p&gt;We are actively working on developing our own inference system, &lt;a href=&#34;https://github.com/OpenBuddy/GrandSage&#34;&gt;GrandSage&lt;/a&gt;. GrandSage is currently in the early stages of development.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;All OpenBuddy models have inherent limitations and may potentially produce outputs that are erroneous, harmful, offensive, or otherwise undesirable. Users should not use these models in critical or high-stakes situations that may lead to personal injury, property damage, or significant losses. Examples of such scenarios include, but are not limited to, the medical field, controlling software and hardware systems that may cause harm, and making important financial or legal decisions.&lt;/p&gt; &#xA;&lt;p&gt;OpenBuddy is provided &#34;as-is&#34; without any warranty of any kind, either express or implied, including, but not limited to, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement. In no event shall the authors, contributors, or copyright holders be liable for any claim, damages, or other liabilities, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the software or the use or other dealings in the software.&lt;/p&gt; &#xA;&lt;p&gt;By using OpenBuddy, you agree to these terms and conditions, and acknowledge that you understand the potential risks associated with its use. You also agree to indemnify and hold harmless the authors, contributors, and copyright holders from any claims, damages, or liabilities arising from your use of OpenBuddy.&lt;/p&gt; &#xA;&lt;h2&gt;License Restrictions&lt;/h2&gt; &#xA;&lt;p&gt;OpenBuddy-LLaMA series models are strictly prohibited for commercial use and are intended for research purposes only. For more information, please refer to the LLaMA License.&lt;/p&gt; &#xA;&lt;p&gt;For the OpenBuddy-Falcon series models, they are released under the Apache 2.0 License. Please refer to the Apache 2.0 License for applicable scope and restrictions.&lt;/p&gt; &#xA;&lt;p&gt;Regarding the source code related to the OpenBuddy open-source project (including, but not limited to, test code and the GrandSage Inference project), they are released under the GPL 3.0 License.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We extend our deepest gratitude to the open-source community for their selfless assistance and contributions to the OpenBuddy project.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, we would like to specifically thank WeiKe Software for their robust support and help in the aspect of model training.&lt;/p&gt; &#xA;&lt;p&gt;We owe our thanks to &lt;a href=&#34;https://kexue.fm/&#34;&gt;Mr. Su Jianlin&lt;/a&gt; for providing invaluable advice during the model training process. Not only did he provide professional advice, but he also introduced the NBCE method, which enables open-source models like OpenBuddy to support inference with a super-long context of 10K. This has had a profound impact on our work.&lt;/p&gt; &#xA;&lt;p&gt;Our appreciation goes to &lt;a href=&#34;https://www.flysnow.org/about/&#34;&gt;Feixue Wuqing&lt;/a&gt; and &lt;a href=&#34;https://github.com/jstzwj&#34;&gt;jstzwj&lt;/a&gt;. They provided valuable advice during the early stages of model development and extended substantial support and assistance in model inference.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, we also wish to express our gratitude to camera and other enthusiasts of open language models. Their suggestions played a pivotal role in improving the model.&lt;/p&gt; &#xA;&lt;p&gt;Once again, we thank everyone who has contributed to the OpenBuddy project. Our success is inseparable from your support and encouragement. Moreover, we acknowledge Tii and Facebook for introducing the Falcon model and the LLaMA model, respectively, which have laid a solid foundation for our project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/alphadev</title>
    <updated>2023-06-11T01:42:44Z</updated>
    <id>tag:github.com,2023-06-11:/deepmind/alphadev</id>
    <link href="https://github.com/deepmind/alphadev" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AlphaDev&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains relevant pseudocode and algorithms for the publication &#34;Faster sorting algorithms discovered using deep reinforcement learning&#34;&lt;/p&gt; &#xA;&lt;p&gt;The repository contains two modules:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;alphadev.py&lt;/code&gt; with the pseudocode for the AlphaDev agent and the Assembly Game RL environment&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sort_functions_test.cc&lt;/code&gt; with the discovered assembly programs and checks their correctness. This includes the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort3AlphaDev&lt;/code&gt; sorts 3 elements with 17 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort4AlphaDev&lt;/code&gt; sorts 4 elements with 28 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort5AlphaDev&lt;/code&gt; sorts 5 elements with 43 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort6AlphaDev&lt;/code&gt; sorts 6 elements with 57 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort7AlphaDev&lt;/code&gt; sorts 7 elements with 76 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Sort8AlphaDev&lt;/code&gt; sorts 8 elements with 91 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;VarSort3AlphaDev&lt;/code&gt; sorts up to 3 elements with 25 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;VarSort4AlphaDev&lt;/code&gt; sorts up to 4 elements with 57 instructions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;VarSort5AlphaDev&lt;/code&gt; sorts up to 5 elements with 80 instructions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code present in &lt;code&gt;alphadev.py&lt;/code&gt; is pseudocode to simplify reproduction. As such, no installation is required for the pseudocode.&lt;/p&gt; &#xA;&lt;p&gt;To test the discovered assembly programs, we need to &lt;a href=&#34;https://docs.bazel.build/versions/main/install.html&#34;&gt;install bazel&lt;/a&gt; and verify it builds correctly (we only support Linux with clang, but other platforms might work)&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;alphadev.py&lt;/code&gt; contains logic for the RL environment, AlphaDev agent and the Assembly Game. The main components are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;AssemblyGame&lt;/code&gt; This represents the Assembly Game RL environment. The state of the RL environment contains the current program and the state of memory and registers. Doing a step in this environment is equivalent to adding a new assembly instruction to the program (see the &lt;code&gt;step&lt;/code&gt; method). The reward is a combination of correctness and latency reward after executing the assembly program over an input distribution. For simplicity of the overall algorithm we are not including the assembly runner, but assembly execution can be delegated to an external library (e.g. &lt;a href=&#34;https://github.com/asmjit/asmjit&#34;&gt;AsmJit&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlphaDevConfig&lt;/code&gt; contains the main hyperparameters used for the AlphaDev agent. This includes configuration of AlphaZero, MCTS, and underlying networks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;play_game&lt;/code&gt; contains the logic to run an AlphaDev game. This include the MCTS procedure and the storage of the game.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RepresentationNet&lt;/code&gt; and &lt;code&gt;PredictionNet&lt;/code&gt; contain the implementation the networks used in the AlphaZero algorithm. It uses a &lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;MultiQuery Transformer&lt;/a&gt; to represent assembly instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run the assembly test in &lt;code&gt;sort_functions_test.cc&lt;/code&gt;, use the following command: &lt;code&gt;CC=clang bazel test :sort_functions_test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaDev2023,&#xA;  author  = {Mankowitz, Daniel J. and Michi, Andrea and Zhernov, Anton and Gelmi, Marco and Selvi, Marco and Paduraru, Cosmin and Leurent, Edouard and Iqbal, Shariq and Lespiau, Jean-Baptiste and Ahern, Alex and Koppe, Thomas and Millikin, Kevin and Gaffney, Stephen and Elster, Sophie and Broshear, Jackson and Gamble, Chris and Milan, Kieran and Tung, Robert and Hwang, Minjae and Cemgil, Taylan and Barekatain, Mohammadamin and Li, Yujia and Mandhane, Amol and Hubert, Thomas and Schrittwieser, Julian and Hassabis, Demis and Kohli, Pushmeet and Riedmiller, Martin and Vinyals, Oriol and Silver, David},&#xA;  journal = {Nature},&#xA;  title   = {Faster sorting algorithms discovered using deep reinforcement learning},&#xA;  year    = {2023},&#xA;  volume  = {618},&#xA;  number  = {7964},&#xA;  pages   = {257--263},&#xA;  doi     = {10.1038/s41586-023-06004-9}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License and disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2022 DeepMind Technologies Limited&lt;/p&gt; &#xA;&lt;p&gt;All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY). You may obtain a copy of the CC-BY license at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.&lt;/p&gt; &#xA;&lt;p&gt;This is not an official Google product.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Ma-Lab-Berkeley/CRATE</title>
    <updated>2023-06-11T01:42:44Z</updated>
    <id>tag:github.com,2023-06-11:/Ma-Lab-Berkeley/CRATE</id>
    <link href="https://github.com/Ma-Lab-Berkeley/CRATE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for CRATE (Coding RAte reduction TransformEr).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CRATE (Coding RAte reduction TransformEr)&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official PyTorch implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2306.01129&#34;&gt;White-Box Transformers via Sparse Rate Reduction&lt;/a&gt; (2023)&lt;/p&gt; &#xA;&lt;p&gt;by &lt;a href=&#34;https://yaodongyu.github.io&#34;&gt;Yaodong Yu&lt;/a&gt; (UC Berkeley), &lt;a href=&#34;https://sdbuchanan.com&#34;&gt;Sam Buchanan&lt;/a&gt; (TTIC), &lt;a href=&#34;https://druvpai.github.io&#34;&gt;Druv Pai&lt;/a&gt; (UC Berkeley), &lt;a href=&#34;https://tianzhechu.com/&#34;&gt;Tianzhe Chu&lt;/a&gt; (UC Berkeley), &lt;a href=&#34;https://robinwu218.github.io/&#34;&gt;Ziyang Wu&lt;/a&gt; (UC Berkeley), &lt;a href=&#34;https://tsb0601.github.io/petertongsb/&#34;&gt;Shengbang Tong&lt;/a&gt; (UC Berkeley), &lt;a href=&#34;https://www.cis.jhu.edu/~haeffele/#about&#34;&gt;Benjamin D Haeffele&lt;/a&gt; (Johns Hopkins University), and &lt;a href=&#34;http://people.eecs.berkeley.edu/~yima/&#34;&gt;Yi Ma&lt;/a&gt; (UC Berkeley).&lt;/p&gt; &#xA;&lt;h2&gt;What is CRATE?&lt;/h2&gt; &#xA;&lt;p&gt;CRATE (Coding RAte reduction TransformEr) is a white-box (mathematically interpretable) transformer architecture, where each layer performs a single step of an alternating minimization algorithm to optimize the &lt;strong&gt;sparse rate reduction objective&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ma-Lab-Berkeley/CRATE/main/figs/fig_objective.png&#34; width=&#34;700&#34; \&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;where the $\ell^{0}$-norm promotes the sparsity of the final token representations $\mathbf{Z} = f(\mathbf{X})$. The function $f$ is defined as $$f=f^{L} \circ f^{L-1} \circ \cdots \circ f^{1} \circ f^{0},$$ $f^0$ is the pre-processing mapping, and $f^{\ell}$ is the $\ell$-th layer forward mapping that transforms the token distribution to optimize the above sparse rate reduction objective incrementally. More specifically, $f^{\ell}$ transforms the $\ell$-th layer token representations $\mathbf{Z}^{\ell}$ to $\mathbf{Z}^{\ell+1}$ via the $\texttt{MSSA}$ (Multi-Head Subspace Self-Attention) block and the $\texttt{ISTA}$ (Iterative Shrinkage-Thresholding Algorithms) block, i.e., $$\mathbf{Z}^{\ell+1} = f^{\ell}(\mathbf{Z}^{\ell}) = \texttt{ISTA}(\texttt{MSSA}(\mathbf{Z}^{\ell})).$$ Figure 1 presents an overview of the pipeline for our proposed &lt;strong&gt;CRATE&lt;/strong&gt; architecture:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ma-Lab-Berkeley/CRATE/main/figs/fig1.png&#34; width=&#34;800&#34; \&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;Figure 2 shows the overall architecture of one block of &lt;strong&gt;CRATE&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ma-Lab-Berkeley/CRATE/main/figs/fig_arch.png&#34; width=&#34;800&#34; \&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;In Figure 3, we measure the compression term [ $R^{c}$ ($\mathbf{Z}^{\ell+1/2}$) ] and the sparsity term [ $||\mathbf{Z}^{\ell+1}||_0$ ] defined in the &lt;strong&gt;sparse rate reduction objective&lt;/strong&gt;, and we find that each layer of &lt;strong&gt;CRATE&lt;/strong&gt; indeed optimizes the targeted objectives:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ma-Lab-Berkeley/CRATE/main/figs/fig3.png&#34; width=&#34;900&#34; \&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Construct a CRATE model&lt;/h2&gt; &#xA;&lt;p&gt;A CRATE model can be defined using the following code, (the below parameters are specified for CRATE-Tiny)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.crate import CRATE&#xA;dim = 384&#xA;n_heads = 6&#xA;depth = 12&#xA;model = CRATE(image_size=224,&#xA;              patch_size=16,&#xA;              num_classes=1000,&#xA;              dim=dim,&#xA;              depth=depth,&#xA;              heads=n_heads,&#xA;              dim_head=dim // n_heads)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;n_heads&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;depth&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CRATE-T&lt;/strong&gt;(iny)&lt;/td&gt; &#xA;   &lt;td&gt;384&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CRATE-S&lt;/strong&gt;(mall)&lt;/td&gt; &#xA;   &lt;td&gt;576&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CRATE-B&lt;/strong&gt;(ase)&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CRATE-L&lt;/strong&gt;(arge)&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training CRATE on ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;To train a CRATE model on ImageNet-1K, run the following script (training CRATE-tiny)&lt;/p&gt; &#xA;&lt;p&gt;As an example, we use the following command for training CRATE-tiny on ImageNet-1K:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python main.py &#xA;  --arch CRATE_tiny &#xA;  --batch-size 512 &#xA;  --epochs 200 &#xA;  --optimizer Lion &#xA;  --lr 0.0002 &#xA;  --weight-decay 0.05 &#xA;  --print-freq 25 &#xA;  --data DATA_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and replace &lt;code&gt;DATA_DIR&lt;/code&gt; with &lt;code&gt;[imagenet-folder with train and val folders]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning pretrained / training random initialized CRATE on CIFAR10&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python finetune.py &#xA;  --bs 256 &#xA;  --net CRATE_tiny &#xA;  --opt adamW  &#xA;  --lr 5e-5 &#xA;  --n_epochs 200 &#xA;  --randomaug 1 &#xA;  --data cifar10 &#xA;  --ckpt_dir CKPT_DIR &#xA;  --data_dir DATA_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;CKPT_DIR&lt;/code&gt; with the path for the pretrained CRATE weight, and replace &lt;code&gt;DATA_DIR&lt;/code&gt; with the path for the &lt;code&gt;CIFAR10&lt;/code&gt; dataset. If &lt;code&gt;CKPT_DIR&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then this script is for training CRATE from random initialization on CIFAR10.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;For technical details and full experimental results, please check the &lt;a href=&#34;https://arxiv.org/abs/2306.xxxx&#34;&gt;paper&lt;/a&gt;. Please consider citing our work if you find it helpful to yours:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{yu2023whitebox,&#xA;      title={White-Box Transformers via Sparse Rate Reduction}, &#xA;      author={Yaodong Yu and Sam Buchanan and Druv Pai and Tianzhe Chu and Ziyang Wu and Shengbang Tong and Benjamin D. Haeffele and Yi Ma},&#xA;      year={2023},&#xA;      eprint={2306.01129},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>