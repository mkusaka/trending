<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-06T01:37:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mit-han-lab/streaming-llm</title>
    <updated>2023-10-06T01:37:16Z</updated>
    <id>tag:github.com,2023-10-06:/mit-han-lab/streaming-llm</id>
    <link href="https://github.com/mit-han-lab/streaming-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient Streaming Language Models with Attention Sinks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient Streaming Language Models with Attention Sinks [&lt;a href=&#34;http://arxiv.org/abs/2309.17453&#34;&gt;paper&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/streaming-llm/main/figures/schemes.png&#34; alt=&#34;schemes&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358&#34;&gt;https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;We deploy LLMs for infinite-length inputs without sacrificing efficiency and performance.&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens&#39; Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink&#39;&#39; even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -yn streaming python=3.8&#xA;conda activate streaming&#xA;&#xA;pip install torch torchvision torchaudio&#xA;pip install transformers==4.33.0 accelerate datasets evaluate wandb scikit-learn scipy sentencepiece&#xA;&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Streaming Llama Chatbot&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python examples/run_streaming_llama.py  --enable_streaming&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What does &#34;working on infinite-length inputs&#34; imply for LLMs?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Handling infinite-length text with LLMs presents challenges. Notably, storing all previous Key and Value (KV) states demands significant memory, and models might struggle to generate text beyond their training sequence length. StreamingLLM addresses this by retaining only the most recent tokens and attention sinks, discarding intermediate tokens. This enables the model to generate coherent text from recent tokens without a cache reset — a capability not seen in earlier methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Is the context window of LLMs expanded?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;No. The context window remains unchanged. Only the most recent tokens and attention sinks are retained, discarding middle tokens. This means the model can only process the latest tokens. The context window remains constrained by its initial pre-training. For instance, if Llama-2 is pre-trained with a context window of 4096 tokens, then the maximum cache size for StreamingLLM on Llama-2 remains 4096.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can I input an extensive text, like a book, into StreamingLLM for summarization?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While you can input a lengthy text, the model will only recognize the latest tokens. Thus, if a book is an input, StreamingLLM might only summarize the concluding paragraphs, which might not be very insightful. As emphasized earlier, we neither expand the LLMs&#39; context window nor enhance their long-term memory. StreamingLLM&#39;s strength lies in generating fluent text from recent tokens without needing a cache refresh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What is the ideal use case for StreamingLLM?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;StreamingLLM is optimized for streaming applications, such as multi-round dialogues. It&#39;s ideal for scenarios where a model needs to operate continually without requiring extensive memory or dependency on past data. An example is a daily assistant based on LLMs. StreamingLLM would let the model function continuously, basing its responses on recent conversations without needing to refresh its cache. Earlier methods would either need a cache reset when the conversation length exceeded the training length (losing recent context) or recompute KV states from recent text history, which can be time-consuming.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How does StreamingLLM relate to recent works on context extension?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;StreamingLLM is orthogonal to recent context extension methods and can be integrated with them. In StreamingLLM&#39;s context, &#34;context extension&#34; refers to the possibility of using a larger cache size to store more recent tokens. For a practical demonstration, refer to Figure 9 in our paper, where we implement StreamingLLM with models like LongChat-7B-v1.5-32K and Llama-2-7B-32K-Instruct.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;We will release the code and data in the following order, please stay tuned!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release core code of StreamingLLM, including Llama-2, MPT, Falcon, and Pythia.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release perplexity evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release Streaming Llama Chatbot demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release StreamEval dataset and evaluation code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find StreamingLLM useful or relevant to your project and research, please kindly cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{xiao2023streamingllm,&#xA;        title={Efficient Streaming Language Models with Attention Sinks},&#xA;        author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},&#xA;        journal={arXiv},&#xA;        year={2023}&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>LinkSoul-AI/AutoAgents</title>
    <updated>2023-10-06T01:37:16Z</updated>
    <id>tag:github.com,2023-10-06:/LinkSoul-AI/AutoAgents</id>
    <link href="https://github.com/LinkSoul-AI/AutoAgents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generate different roles for GPTs to form a collaborative entity for complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoAgents: A Framework for Automatic Agent Generation&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/docs/resources/logo-autoagents.jpg&#34; alt=&#34;autoagents logo: A Framework for Automatic Agent Generation.&#34; width=&#34;150px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Generate different roles for GPTs to form a collaborative entity for complex tasks.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/docs/README_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/文档-中文版-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;AutoAgents is an experimental open-source application for An Automatic Agents Generation Experiment based on LLM. This program, driven by LLM, autonomously generates multi-agents to achieve whatever goal you set.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/docs/resources/framework2.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;updates&#34;&gt;&lt;/a&gt; &lt;span&gt;💥&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.08.30&lt;/strong&gt;: Adding a custom agent collection, AgentBank, allows you to add custom agents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt;: Determines the expert roles to be added and the specific execution plan according to the problem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: The set of tools that can be used, currently only compatible with the search tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Observers&lt;/strong&gt;: Responsible for reflecting on whether the planner and the results in the execution process are reasonable, currently including reflection checks on Agents, Plan, and Action.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt;: Expert role agents generated by the planner, including name, expertise, tools used, and LLM enhancement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;: The execution plan is composed of the generated expert roles, each step of the execution plan has at least one expert role agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Actions&lt;/strong&gt;: The specific actions of the expert roles in the execution plan, such as calling tools or outputting results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Online demo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/LinkSoul/AutoAgents&#34;&gt;Demo / HuggingFace Spaces&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Video demo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rumor Verification&lt;/strong&gt; &#xA;  &lt;video src=&#34;https://github.com/shiyemin/AutoAgents/assets/1501158/41898e0d-4137-450c-ad9b-bfb9b8c1d27b.mp4&#34;&gt;&lt;/video&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gluttonous Snake&lt;/strong&gt; &#xA;  &lt;video src=&#34;https://github.com/shiyemin/AutoAgents/assets/1501158/97e408cb-b70d-4045-82ea-07319c085138.mp4&#34;&gt;&lt;/video&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation and Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LinkSoul-AI/AutoAgents&#xA;cd AutoAgents&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in any of &lt;code&gt;config/key.yaml / config/config.yaml / env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Priority order: &lt;code&gt;config/key.yaml &amp;gt; config/config.yaml &amp;gt; env&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Copy the configuration file and make the necessary modifications.&#xA;cp config/config.yaml config/key.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable Name&lt;/th&gt; &#xA;   &lt;th&gt;config/key.yaml&lt;/th&gt; &#xA;   &lt;th&gt;env&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY # Replace with your own key&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY: &#34;sk-...&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_KEY=&#34;sk-...&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE # Optional&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE: &#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_BASE=&#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Commandline mode:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python main.py --mode commandline --llm_api_key YOUR_OPENAI_API_KEY --serapi_key YOUR_SERPAPI_KEY --idea &#34;Is LK-99 really a room temperature superconducting material?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Websocket service mode:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python main.py --mode service --host &#34;127.0.0.1&#34; --port 9000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build docker image:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;IMAGE=&#34;linksoul.ai/autoagents&#34;&#xA;VERSION=1.0&#xA;&#xA;docker build -f docker/Dockerfile -t &#34;${IMAGE}:${VERSION}&#34; .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start docker container:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -p 7860:7860 &#34;${IMAGE}:${VERSION}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;a href=&#34;http://127.0.0.1:7860&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; in the browser.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact Information&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; &lt;a href=&#34;mailto:gy.chen@foxmail.com&#34;&gt;gy.chen@foxmail.com&lt;/a&gt;, &lt;a href=&#34;mailto:ymshi@linksoul.ai&#34;&gt;ymshi@linksoul.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; For more technical inquiries, you can also create a new issue in our &lt;a href=&#34;https://github.com/LinkSoul-AI/AutoAgents/issues&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will respond to all questions within 2-3 business days.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work and this repository useful, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;🍺&lt;/span&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chen2023auto,&#xA;  title={AutoAgents: The Automatic Agents Generation Framework},&#xA;  author={Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Jaward, Sesay and Börje, Karlsson and Fu, Jie and Shi, Yemin},&#xA;  journal={arXiv preprint},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Wechat Group&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/AutoAgents/main/.github/QRcode.jpg&#34; alt=&#34;Wechat Group&#34; width=&#34;200&#34;&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/LinkSoul-AI/AutoAgents/tree/main/autoagents/system&#34;&gt;system&lt;/a&gt;, &lt;a href=&#34;https://github.com/LinkSoul-AI/AutoAgents/tree/main/autoagents/actions/action_bank&#34;&gt;action_bank&lt;/a&gt; and &lt;a href=&#34;https://github.com/LinkSoul-AI/AutoAgents/tree/main/autoagents/roles/role_bank&#34;&gt;role_bank&lt;/a&gt; of this code base is built using &lt;a href=&#34;https://github.com/geekan/MetaGPT&#34;&gt;MetaGPT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Icons in the framework made by Darius Dan, Freepik, kmg design, Flat Icons, Vectorslab from &lt;a href=&#34;https://www.flaticon.com&#34;&gt;FlatIcon&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SerCeMan/fontogen</title>
    <updated>2023-10-06T01:37:16Z</updated>
    <id>tag:github.com,2023-10-06:/SerCeMan/fontogen</id>
    <link href="https://github.com/SerCeMan/fontogen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hey, Computer, Make Me a Font&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FontoGen&lt;/h1&gt; &#xA;&lt;p&gt;Generate your very own font with FontoGen. Read more about the project in my &lt;a href=&#34;https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font&#34;&gt;blog article&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SerCeMan/fontogen/master/img/fontogen.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipenv install&#xA;pipenv shell&#xA;# Nightly Triton is required&#xA;pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly==2.1.0.dev20230801015042 --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;p&gt;The model needs to be re-trained on a large dataset of OFL fonts. If anyone would like to contribute and re-train the model, please reach out and I&#39;ll be happy to help you set up the environment.&lt;/p&gt;</summary>
  </entry>
</feed>