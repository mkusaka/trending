<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-22T01:44:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>meta-llama/codellama</title>
    <updated>2024-04-22T01:44:42Z</updated>
    <id>tag:github.com,2024-04-22:/meta-llama/codellama</id>
    <link href="https://github.com/meta-llama/codellama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for CodeLlama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introducing Code Llama&lt;/h1&gt; &#xA;&lt;p&gt;Code Llama is a family of large language models for code based on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2&lt;/a&gt; providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to our &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;research paper&lt;/a&gt;. Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models ‚Äî ranging from 7B to 34B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama&lt;/a&gt; models and run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizers, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, &lt;strong&gt;do not use the &#39;Copy link address&#39; option&lt;/strong&gt; when you right click the URL. If the copied URL text starts with: &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt;, you copied it correctly. If the copied URL text starts with: &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, you copied it the wrong way.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then to run the script: &lt;code&gt;bash download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h3&gt;Model sizes&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;~12.55GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;63GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;131GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda environment with PyTorch / CUDA available, clone the repo and run in the top-level directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models, except the 70B python and instruct versions, support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware and use-case.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Code Models&lt;/h3&gt; &#xA;&lt;p&gt;The Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_completion.py&lt;/code&gt; for some examples. To illustrate, see command below to run it with the &lt;code&gt;CodeLlama-7b&lt;/code&gt; model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_completion.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained code models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt;, &lt;code&gt;CodeLlama-13b&lt;/code&gt;, &lt;code&gt;CodeLlama-34b&lt;/code&gt;, &lt;code&gt;CodeLlama-70b&lt;/code&gt; and the Code Llama - Python models &lt;code&gt;CodeLlama-7b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-70b-Python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code Infilling&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_infilling.py&lt;/code&gt; for some examples. The &lt;code&gt;CodeLlama-7b&lt;/code&gt; model can be run for infilling with the command below (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_infilling.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 192 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained infilling models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt; and &lt;code&gt;CodeLlama-13b&lt;/code&gt; and the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuned Instruction Models&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for the 7B, 13B and 34B variants, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L319-L361&#34;&gt;&lt;code&gt;chat_completion()&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and linebreaks in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces). &lt;code&gt;CodeLlama-70b-Instruct&lt;/code&gt; requires a separate turn-based prompt format defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L506-L548&#34;&gt;&lt;code&gt;dialog_prompt_tokens()&lt;/code&gt;&lt;/a&gt;. You can use &lt;code&gt;chat_completion()&lt;/code&gt; directly to generate answers with all instruct models; it will automatically perform the required formatting.&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/src/llama_recipes/inference/safety_utils.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_instructions.py \&#xA;    --ckpt_dir CodeLlama-7b-Instruct/ \&#xA;    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fine-tuned instruction-following models are: the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-70b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research papers as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù, or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/codellama&#34;&gt;github.com/facebookresearch/codellama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/codellama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt; for the model card of Code Llama.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;Code Llama Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>embeddings-benchmark/mteb</title>
    <updated>2024-04-22T01:44:42Z</updated>
    <id>tag:github.com,2024-04-22:/embeddings-benchmark/mteb</id>
    <link href="https://github.com/embeddings-benchmark/mteb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MTEB: Massive Text Embedding Benchmark&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Massive Text Embedding Benchmark&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/embeddings-benchmark/mteb.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.07316&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/badge/arXiv-2305.14251-b31b1b.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/embeddings-benchmark/mteb.svg?color=green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/mteb&#34;&gt; &lt;img alt=&#34;Downloads&#34; src=&#34;https://static.pepy.tech/personalized-badge/mteb?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=Downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/#installation&#34;&gt;Installation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/#usage&#34;&gt;Usage&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;Leaderboard&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/#documentation&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/#citing&#34;&gt;Citing&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;&lt;img style=&#34;float: middle; padding: 10px 10px 10px 10px;&#34; width=&#34;60&#34; height=&#34;55&#34; src=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/docs/images/hf_logo.png&#34;&gt;&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mteb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using a python script (see &lt;a href=&#34;https://github.com/embeddings-benchmark/mteb/raw/main/scripts/run_mteb_english.py&#34;&gt;scripts/run_mteb_english.py&lt;/a&gt; and &lt;a href=&#34;https://github.com/embeddings-benchmark/mtebscripts&#34;&gt;mteb/mtebscripts&lt;/a&gt; for more):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mteb import MTEB&#xA;from sentence_transformers import SentenceTransformer&#xA;&#xA;# Define the sentence-transformers model name&#xA;model_name = &#34;average_word_embeddings_komninos&#34;&#xA;# or directly from huggingface:&#xA;# model_name = &#34;sentence-transformers/all-MiniLM-L6-v2&#34;&#xA;&#xA;model = SentenceTransformer(model_name)&#xA;evaluation = MTEB(tasks=[&#34;Banking77Classification&#34;])&#xA;results = evaluation.run(model, output_folder=f&#34;results/{model_name}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using CLI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mteb --available_tasks&#xA;&#xA;mteb -m sentence-transformers/all-MiniLM-L6-v2 \&#xA;    -t Banking77Classification  \&#xA;    --verbosity 3&#xA;&#xA;# if nothing is specified default to saving the results in the results/{model_name} folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using multiple GPUs in parallel can be done by just having a custom encode function that distributes the inputs to multiple GPUs like e.g. &lt;a href=&#34;https://github.com/microsoft/unilm/raw/b60c741f746877293bb85eed6806736fc8fa0ffd/e5/mteb_eval.py#L60&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://github.com/ContextualAI/gritlm/raw/09d8630f0c95ac6a456354bcb6f964d7b9b6a609/gritlm/gritlm.py#L75&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Advanced Usage (click to unfold) &lt;/summary&gt; &#xA; &lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA; &lt;h3&gt;Dataset selection&lt;/h3&gt; &#xA; &lt;p&gt;Datasets can be selected by providing the list of datasets, but also&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;by their task (e.g. &#34;Clustering&#34; or &#34;Classification&#34;)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluation = MTEB(task_types=[&#39;Clustering&#39;, &#39;Retrieval&#39;]) # Only select clustering and retrieval tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;by their categories e.g. &#34;S2S&#34; (sentence to sentence) or &#34;P2P&#34; (paragraph to paragraph)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluation = MTEB(task_categories=[&#39;S2S&#39;]) # Only select sentence2sentence datasets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;by their languages&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluation = MTEB(task_langs=[&#34;en&#34;, &#34;de&#34;]) # Only select datasets which are &#34;en&#34;, &#34;de&#34; or &#34;en-de&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can also specify which languages to load for multilingual/crosslingual tasks like below:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mteb.tasks import AmazonReviewsClassification, BUCCBitextMining&#xA;&#xA;evaluation = MTEB(tasks=[&#xA;        AmazonReviewsClassification(langs=[&#34;en&#34;, &#34;fr&#34;]) # Only load &#34;en&#34; and &#34;fr&#34; subsets of Amazon Reviews&#xA;        BUCCBitextMining(langs=[&#34;de-en&#34;]), # Only load &#34;de-en&#34; subset of BUCC&#xA;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;There are also presets available for certain task collections, e.g. to select the 56 English datasets that form the &#34;Overall MTEB English leaderboard&#34;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mteb import MTEB_MAIN_EN&#xA;evaluation = MTEB(tasks=MTEB_MAIN_EN, task_langs=[&#34;en&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Evaluation split&lt;/h3&gt; &#xA; &lt;p&gt;You can evaluate only on &lt;code&gt;test&lt;/code&gt; splits of all tasks by doing the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluation.run(model, eval_splits=[&#34;test&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note that the public leaderboard uses the test splits for all datasets except MSMARCO, where the &#34;dev&#34; split is used.&lt;/p&gt; &#xA; &lt;h3&gt;Using a custom model&lt;/h3&gt; &#xA; &lt;p&gt;Models should implement the following interface, implementing an &lt;code&gt;encode&lt;/code&gt; function taking as inputs a list of sentences, and returning a list of embeddings (embeddings can be &lt;code&gt;np.array&lt;/code&gt;, &lt;code&gt;torch.tensor&lt;/code&gt;, etc.). For inspiration, you can look at the &lt;a href=&#34;https://github.com/embeddings-benchmark/mtebscripts&#34;&gt;mteb/mtebscripts repo&lt;/a&gt; used for running diverse models via SLURM scripts for the paper.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MyModel():&#xA;    def encode(self, sentences: list[str], **kwargs) -&amp;gt; list[np.ndarray] | list[torch.Tensor]:&#xA;        &#34;&#34;&#34;&#xA;        Returns a list of embeddings for the given sentences.&#xA;        &#xA;        Args:&#xA;            sentences: List of sentences to encode&#xA;&#xA;        Returns:&#xA;            List of embeddings for the given sentences&#xA;        &#34;&#34;&#34;&#xA;        pass&#xA;&#xA;model = MyModel()&#xA;evaluation = MTEB(tasks=[&#34;Banking77Classification&#34;])&#xA;evaluation.run(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you&#39;d like to use different encoding functions for query and corpus when evaluating on Retrieval or Reranking tasks, you can add separate methods for &lt;code&gt;encode_queries&lt;/code&gt; and &lt;code&gt;encode_corpus&lt;/code&gt;. If these methods exist, they will be automatically used for those tasks. You can refer to the &lt;code&gt;DRESModel&lt;/code&gt; at &lt;code&gt;mteb/evaluation/evaluators/RetrievalEvaluator.py&lt;/code&gt; for an example of these functions.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MyModel():&#xA;    def encode_queries(self, queries: list[str], **kwargs) -&amp;gt; list[np.ndarray] | list[torch.Tensor]:&#xA;        &#34;&#34;&#34;&#xA;        Returns a list of embeddings for the given sentences.&#xA;        Args:&#xA;            queries: List of sentences to encode&#xA;&#xA;        Returns:&#xA;            List of embeddings for the given sentences&#xA;        &#34;&#34;&#34;&#xA;        pass&#xA;&#xA;    def encode_corpus(self, corpus: list[str] | list[dict[str, str]], **kwargs) -&amp;gt; list[np.ndarray] | list[torch.Tensor]:&#xA;        &#34;&#34;&#34;&#xA;        Returns a list of embeddings for the given sentences.&#xA;        Args:&#xA;            corpus: List of sentences to encode&#xA;                or list of dictionaries with keys &#34;title&#34; and &#34;text&#34;&#xA;&#xA;        Returns:&#xA;            List of embeddings for the given sentences&#xA;        &#34;&#34;&#34;&#xA;        pass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Evaluating on a custom dataset&lt;/h3&gt; &#xA; &lt;p&gt;To evaluate on a custom task, you can run the following code on your custom task. See &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/docs/adding_a_dataset.md&#34;&gt;how to add a new task&lt;/a&gt;, for how to create a new task in MTEB.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mteb import MTEB&#xA;from mteb.abstasks.AbsTaskReranking import AbsTaskReranking&#xA;from sentence_transformers import SentenceTransformer&#xA;&#xA;&#xA;class MyCustomTask(AbsTaskReranking):&#xA;    ...&#xA;&#xA;model = SentenceTransformer(&#34;average_word_embeddings_komninos&#34;)&#xA;evaluation = MTEB(tasks=[MyCustomTask()])&#xA;evaluation.run(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìã &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/docs/tasks.md&#34;&gt;Tasks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&amp;nbsp;Overview of available tasks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìà &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;Leaderboard&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The interactive leaderboard of the benchmark&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/docs/adding_a_model.md&#34;&gt;Adding a model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Information related to how to submit a model to the leaderboard&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/docs/adding_a_dataset.md&#34;&gt;Adding a dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How to add a new task/dataset to MTEB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü§ù &lt;a href=&#34;https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How to contribute to MTEB and set it up for development&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | üåê [MMTEB] | An open-source effort to extend MTEB to cover a broad set of languages | ¬† --&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;MTEB was introduced in &#34;&lt;a href=&#34;https://arxiv.org/abs/2210.07316&#34;&gt;MTEB: Massive Text Embedding Benchmark&lt;/a&gt;&#34;, feel free to cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{muennighoff2022mteb,&#xA;  doi = {10.48550/ARXIV.2210.07316},&#xA;  url = {https://arxiv.org/abs/2210.07316},&#xA;  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\&#34;\i}c and Reimers, Nils},&#xA;  title = {MTEB: Massive Text Embedding Benchmark},&#xA;  publisher = {arXiv},&#xA;  journal={arXiv preprint arXiv:2210.07316},  &#xA;  year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also want to read and cite the amazing work that has extended MTEB &amp;amp; integrated new datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff. &#34;&lt;a href=&#34;https://arxiv.org/abs/2309.07597&#34;&gt;C-Pack: Packaged Resources To Advance General Chinese Embedding&lt;/a&gt;&#34; arXiv 2023&lt;/li&gt; &#xA; &lt;li&gt;Michael G√ºnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao. &#34;&lt;a href=&#34;https://arxiv.org/abs/2310.19923&#34;&gt;Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents&lt;/a&gt;&#34; arXiv 2023&lt;/li&gt; &#xA; &lt;li&gt;Silvan Wehrli, Bert Arnrich, Christopher Irrgang. &#34;&lt;a href=&#34;https://arxiv.org/abs/2401.02709&#34;&gt;German Text Embedding Clustering Benchmark&lt;/a&gt;&#34; arXiv 2024&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For works that have used MTEB for benchmarking, you can find them on the &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;leaderboard&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>liming-ai/ControlNet_Plus_Plus</title>
    <updated>2024-04-22T01:44:42Z</updated>
    <id>tag:github.com,2024-04-22:/liming-ai/ControlNet_Plus_Plus</id>
    <link href="https://github.com/liming-ai/ControlNet_Plus_Plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for: ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07987&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv%20paper-2404.07987-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-ControlNet++-yellow&#34; alt=&#34;huggingface demo&#34;&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.07987&#34;&gt;ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/liming-ai/ControlNet_Plus_Plus/inference/images/github_imgs/teaser.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;üïπÔ∏è Try and Play with ControlNet++&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus&#34;&gt;demo website&lt;/a&gt; for you to play with our ControlNet++ models and generate images interactively. For local running, please run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/liming-ai/ControlNet_Plus_Plus.git&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please download the model weights and put them into each subset of &lt;code&gt;checkpoints&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;HF weightsü§ó&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LineArt&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus/resolve/main/checkpoints/lineart/controlnet/diffusion_pytorch_model.bin&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Depth&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus/resolve/main/checkpoints/depth/controlnet/diffusion_pytorch_model.safetensors&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus/resolve/main/checkpoints/seg/controlnet/diffusion_pytorch_model.safetensors&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hed (SoftEdge)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus/resolve/main/checkpoints/hed/controlnet/diffusion_pytorch_model.bin&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Canny&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus/resolve/main/checkpoints/canny/controlnet/diffusion_pytorch_model.safetensors&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;And then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s new for ControlNet++?&lt;/h2&gt; &#xA;&lt;h3&gt;‚ú® Cycle Consistency for Conditional Generation&lt;/h3&gt; &#xA;&lt;p&gt;We model image-based controllable generation as an image translation task from input conditional controls $c_v$ to output generated images $x&#39;_0$. If we translate images from one domain to the other (condition $c_v$ ‚Üí generated image $x&#39;_0$ ), and back again (generated image $x&#39;_0$ ‚Üí condition $c_v&#39;$ ) we should arrive where we started ($c_v$ = $c_v&#39;$). Hence, we can directly optimize the cycle consistency loss for better controllability.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://liming-ai.github.io/ControlNet_Plus_Plus/static/images/cycle_consistency.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;‚ú® Directly Optimization for Controllability:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; Existing methods achieve implicit controllability by introducing imagebased conditional control $c_v$ into the denoising process of diffusion models, with the guidance of latent-space denoising loss. &lt;strong&gt;(b)&lt;/strong&gt; We utilize discriminative reward models $D$ to explicitly optimize the controllability of $G$ via pixel-level cycle consistency loss.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://liming-ai.github.io/ControlNet_Plus_Plus/static/images/comparison.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;‚ú® Efficient Reward Strategy:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; Pipeline of default reward fine-tuning strategy. Reward fine-tuning requires sampling all the way to the full image. Such a method needs to keep all gradients for each timestep and the memory required is unbearable by current GPUs. &lt;strong&gt;(b)&lt;/strong&gt; We add a small noise ($t ‚â§ t_{thre}$) to disturb the consistency between input images and conditions, then the single-step denoised image can be directly used for efficient reward fine-tuning.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://liming-ai.github.io/ControlNet_Plus_Plus/static/images/efficient_reward.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Better Controllability Than Existing Methods (Qualitative Results):&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/liming-ai/ControlNet_Plus_Plus/inference/images/github_imgs/vis_comparison.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Better Controllability Than Existing Methods (Quantitative Results):&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://liming-ai.github.io/ControlNet_Plus_Plus/static/images/results.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4&gt;For a deep dive into our analyses, discussions, and evaluations, check out our &lt;a href=&#34;https://arxiv.org/abs/2404.07987&#34;&gt;paper&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/liming-ai/ControlNet_Plus_Plus/inference/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work assists your research, feel free to give us a star ‚≠ê or cite us using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2024controlnet,&#xA;    author  = {Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen},&#xA;    title   = {ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback},&#xA;    journal = {arXiv preprint arXiv:2404.07987},&#xA;    year    = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>